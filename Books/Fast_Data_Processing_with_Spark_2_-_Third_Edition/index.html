<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="">

    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0"> <!--320-->
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no, user-scalable=no">

    <link rel="icon" href="../../mapt/images/favicon.ico">

    <link rel="stylesheet" href="../../mapt/css/font-awesome.css">
    <link rel="stylesheet" href="../../mapt/css/google-fonts.css">
    <link rel="stylesheet" href="../../mapt/css/devicon.css">

    <link rel="stylesheet" href="../../mapt/css/bootstrap.css">
    <link rel="stylesheet" href="../../mapt/css/bootstrap-xl.css">
    <link rel="stylesheet" href="../../mapt/css/magnific-popup.css">
    <link rel="stylesheet" href="../../mapt/css/prism.css">
    <link rel="stylesheet" href="../../mapt/css/hljs-github.css">

    <link rel="stylesheet" href="../../mapt/css/mapt.css">
    <link rel="stylesheet" href="../../mapt/css/custom.css">

    <script src="../../mapt/js/jquery.js"></script>
    <script src="../../mapt/js/bootstrap.js"></script>
    <script src="../../mapt/js/jquery.magnific-popup.js"></script>
    <script src="../../mapt/js/highlight.min.js"></script>

    <script src="../../mapt/js/custom.js"></script>
    
    <title>Fast Data Processing with Spark 2 - Third Edition</title>
</head>

<body class="home-body">
    <div id="wrapper">
        <div id="sidebar-wrapper">    
            <ul class="sidebar-nav">
                <div class="list-group" id="sidebar-nav" role="tablist">
                    <li>
                        <a href="../../index.html" class="sidenav-menu-holder back-btn" id="back-link">
                            <span class="sidenav-menu">Book List</span>
                            <span class="pull-left mr5"><i class="fa fa-chevron-left"></i></span>
                        </a>
                    </li>
                    
                    <li class="book-info copyright">
                        <span class="info text-nowrap"><span class="copyleft">&copy;</span><span><strong>RuTracker</strong>.org</span></span>
                    </li>          
                    <li class="book-info copyright">
                        <span class="info text-nowrap">Pub date: <strong>24 Oct 2016</strong></span>
                    </li>         
                    <li class="book-info">
                        <span class="info text-nowrap">Price: €<strong>27.99</strong></span>
                        <span class="info text-nowrap">ISBN: <strong>9781785889271</strong></span>
                    </li>     
            
                    <li>
                        <a href="graphics/cover.jpg" class="sidenav-menu-holder cover-img">
                            <img src="graphics/cover.jpg" class="cover-image">
                        </a>
                    </li>        
            
                    <div class="book_navigation">
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse1">
                                <div class="section-name">1: Installing Spark and Setting Up Your Cluster</div>
                            </a>
                        </li>
                        <div id="collapse1" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="1" class="sub-nav">
                                <a href="#ch01">
                                    <div class="section-name">Chapter 1: Installing Spark and Setting Up Your Cluster</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec7" class="sub-nav">
                                <a href="#ch01lvl1sec7">                    
                                    <div class="section-name">Directory organization and convention</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec8" class="sub-nav">
                                <a href="#ch01lvl1sec8">                    
                                    <div class="section-name">Installing the prebuilt distribution</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec9" class="sub-nav">
                                <a href="#ch01lvl1sec9">                    
                                    <div class="section-name">Building Spark from source</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec10" class="sub-nav">
                                <a href="#ch01lvl1sec10">                    
                                    <div class="section-name">Spark topology</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec11" class="sub-nav">
                                <a href="#ch01lvl1sec11">                    
                                    <div class="section-name">A single machine</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec12" class="sub-nav">
                                <a href="#ch01lvl1sec12">                    
                                    <div class="section-name">Running Spark on EC2</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec13" class="sub-nav">
                                <a href="#ch01lvl1sec13">                    
                                    <div class="section-name">Deploying Spark with Chef (Opscode)</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec14" class="sub-nav">
                                <a href="#ch01lvl1sec14">                    
                                    <div class="section-name">Deploying Spark on Mesos</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec15" class="sub-nav">
                                <a href="#ch01lvl1sec15">                    
                                    <div class="section-name">Spark on YARN</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec16" class="sub-nav">
                                <a href="#ch01lvl1sec16">                    
                                    <div class="section-name">Spark standalone mode</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec17" class="sub-nav">
                                <a href="#ch01lvl1sec17">                    
                                    <div class="section-name">References</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec18" class="sub-nav">
                                <a href="#ch01lvl1sec18">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse2">
                                <div class="section-name">2: Using the Spark Shell</div>
                            </a>
                        </li>
                        <div id="collapse2" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="2" class="sub-nav">
                                <a href="#ch02">
                                    <div class="section-name">Chapter 2: Using the Spark Shell</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec19" class="sub-nav">
                                <a href="#ch02lvl1sec19">                    
                                    <div class="section-name">The Spark shell</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec20" class="sub-nav">
                                <a href="#ch02lvl1sec20">                    
                                    <div class="section-name">Loading a simple text file</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec21" class="sub-nav">
                                <a href="#ch02lvl1sec21">                    
                                    <div class="section-name">Interactively loading data from S3</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec22" class="sub-nav">
                                <a href="#ch02lvl1sec22">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse3">
                                <div class="section-name">3: Building and Running a Spark Application</div>
                            </a>
                        </li>
                        <div id="collapse3" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="3" class="sub-nav">
                                <a href="#ch03">
                                    <div class="section-name">Chapter 3: Building and Running a Spark Application</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec23" class="sub-nav">
                                <a href="#ch03lvl1sec23">                    
                                    <div class="section-name">Building Spark applications</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec24" class="sub-nav">
                                <a href="#ch03lvl1sec24">                    
                                    <div class="section-name">Data wrangling with iPython</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec25" class="sub-nav">
                                <a href="#ch03lvl1sec25">                    
                                    <div class="section-name">Developing Spark with Eclipse</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec26" class="sub-nav">
                                <a href="#ch03lvl1sec26">                    
                                    <div class="section-name">Developing Spark with other IDEs</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec27" class="sub-nav">
                                <a href="#ch03lvl1sec27">                    
                                    <div class="section-name">Building your Spark job with Maven</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec28" class="sub-nav">
                                <a href="#ch03lvl1sec28">                    
                                    <div class="section-name">Building your Spark job with something else</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec29" class="sub-nav">
                                <a href="#ch03lvl1sec29">                    
                                    <div class="section-name">References</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec30" class="sub-nav">
                                <a href="#ch03lvl1sec30">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse4">
                                <div class="section-name">4: Creating a SparkSession Object</div>
                            </a>
                        </li>
                        <div id="collapse4" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="4" class="sub-nav">
                                <a href="#ch04">
                                    <div class="section-name">Chapter 4: Creating a SparkSession Object</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec31" class="sub-nav">
                                <a href="#ch04lvl1sec31">                    
                                    <div class="section-name">SparkSession versus SparkContext</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec32" class="sub-nav">
                                <a href="#ch04lvl1sec32">                    
                                    <div class="section-name">Building a SparkSession object</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec33" class="sub-nav">
                                <a href="#ch04lvl1sec33">                    
                                    <div class="section-name">SparkContext - metadata</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec34" class="sub-nav">
                                <a href="#ch04lvl1sec34">                    
                                    <div class="section-name">Shared Java and Scala APIs</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec35" class="sub-nav">
                                <a href="#ch04lvl1sec35">                    
                                    <div class="section-name">Python</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec36" class="sub-nav">
                                <a href="#ch04lvl1sec36">                    
                                    <div class="section-name">iPython</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec37" class="sub-nav">
                                <a href="#ch04lvl1sec37">                    
                                    <div class="section-name">Reference</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec38" class="sub-nav">
                                <a href="#ch04lvl1sec38">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse5">
                                <div class="section-name">5: Loading and Saving Data in Spark</div>
                            </a>
                        </li>
                        <div id="collapse5" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="5" class="sub-nav">
                                <a href="#ch05">
                                    <div class="section-name">Chapter 5: Loading and Saving Data in Spark</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec39" class="sub-nav">
                                <a href="#ch05lvl1sec39">                    
                                    <div class="section-name">Spark abstractions</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec40" class="sub-nav">
                                <a href="#ch05lvl1sec40">                    
                                    <div class="section-name">Data modalities</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec41" class="sub-nav">
                                <a href="#ch05lvl1sec41">                    
                                    <div class="section-name">Data modalities and Datasets/DataFrames/RDDs</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec42" class="sub-nav">
                                <a href="#ch05lvl1sec42">                    
                                    <div class="section-name">Loading data into an RDD</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec43" class="sub-nav">
                                <a href="#ch05lvl1sec43">                    
                                    <div class="section-name">Saving your data</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec44" class="sub-nav">
                                <a href="#ch05lvl1sec44">                    
                                    <div class="section-name">References</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec45" class="sub-nav">
                                <a href="#ch05lvl1sec45">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse6">
                                <div class="section-name">6: Manipulating Your RDD</div>
                            </a>
                        </li>
                        <div id="collapse6" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="6" class="sub-nav">
                                <a href="#ch06">
                                    <div class="section-name">Chapter 6: Manipulating Your RDD</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec46" class="sub-nav">
                                <a href="#ch06lvl1sec46">                    
                                    <div class="section-name">Manipulating your RDD in Scala and Java</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec47" class="sub-nav">
                                <a href="#ch06lvl1sec47">                    
                                    <div class="section-name">Manipulating your RDD in Python</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec48" class="sub-nav">
                                <a href="#ch06lvl1sec48">                    
                                    <div class="section-name">References</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec49" class="sub-nav">
                                <a href="#ch06lvl1sec49">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse7">
                                <div class="section-name">7: Spark 2.0 Concepts</div>
                            </a>
                        </li>
                        <div id="collapse7" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="7" class="sub-nav">
                                <a href="#ch07">
                                    <div class="section-name">Chapter 7: Spark 2.0 Concepts</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec50" class="sub-nav">
                                <a href="#ch07lvl1sec50">                    
                                    <div class="section-name">Code and Datasets for the rest of the book</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec51" class="sub-nav">
                                <a href="#ch07lvl1sec51">                    
                                    <div class="section-name">The data scientist and Spark features</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec52" class="sub-nav">
                                <a href="#ch07lvl1sec52">                    
                                    <div class="section-name">Spark v2.0 and beyond</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec53" class="sub-nav">
                                <a href="#ch07lvl1sec53">                    
                                    <div class="section-name">Apache Spark - evolution</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec54" class="sub-nav">
                                <a href="#ch07lvl1sec54">                    
                                    <div class="section-name">Apache Spark - the full stack</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec55" class="sub-nav">
                                <a href="#ch07lvl1sec55">                    
                                    <div class="section-name">The art of a big data store - Parquet</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec56" class="sub-nav">
                                <a href="#ch07lvl1sec56">                    
                                    <div class="section-name">References</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec57" class="sub-nav">
                                <a href="#ch07lvl1sec57">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse8">
                                <div class="section-name">8: Spark SQL</div>
                            </a>
                        </li>
                        <div id="collapse8" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="8" class="sub-nav">
                                <a href="#ch08">
                                    <div class="section-name">Chapter 8: Spark SQL</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec58" class="sub-nav">
                                <a href="#ch08lvl1sec58">                    
                                    <div class="section-name">The Spark SQL architecture</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec59" class="sub-nav">
                                <a href="#ch08lvl1sec59">                    
                                    <div class="section-name">Spark SQL how-to in a nutshell</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec60" class="sub-nav">
                                <a href="#ch08lvl1sec60">                    
                                    <div class="section-name">Spark SQL programming</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec61" class="sub-nav">
                                <a href="#ch08lvl1sec61">                    
                                    <div class="section-name">References</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec62" class="sub-nav">
                                <a href="#ch08lvl1sec62">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse9">
                                <div class="section-name">9: Foundations of Datasets/DataFrames – The Proverbial Workhorse for DataScientists</div>
                            </a>
                        </li>
                        <div id="collapse9" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="9" class="sub-nav">
                                <a href="#ch09">
                                    <div class="section-name">Chapter 9: Foundations of Datasets/DataFrames – The Proverbial Workhorse for DataScientists</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec63" class="sub-nav">
                                <a href="#ch09lvl1sec63">                    
                                    <div class="section-name">Datasets - a quick introduction</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec64" class="sub-nav">
                                <a href="#ch09lvl1sec64">                    
                                    <div class="section-name">Dataset APIs - an overview</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec65" class="sub-nav">
                                <a href="#ch09lvl1sec65">                    
                                    <div class="section-name">Dataset interfaces and functions</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec66" class="sub-nav">
                                <a href="#ch09lvl1sec66">                    
                                    <div class="section-name">References</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec67" class="sub-nav">
                                <a href="#ch09lvl1sec67">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse10">
                                <div class="section-name">10: Spark with Big Data</div>
                            </a>
                        </li>
                        <div id="collapse10" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="10" class="sub-nav">
                                <a href="#ch10">
                                    <div class="section-name">Chapter 10: Spark with Big Data</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec68" class="sub-nav">
                                <a href="#ch10lvl1sec68">                    
                                    <div class="section-name">Parquet - an efficient and interoperable big data format</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec69" class="sub-nav">
                                <a href="#ch10lvl1sec69">                    
                                    <div class="section-name">HBase</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec70" class="sub-nav">
                                <a href="#ch10lvl1sec70">                    
                                    <div class="section-name">Reference</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec71" class="sub-nav">
                                <a href="#ch10lvl1sec71">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse11">
                                <div class="section-name">11: Machine Learning with Spark ML Pipelines</div>
                            </a>
                        </li>
                        <div id="collapse11" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="11" class="sub-nav">
                                <a href="#ch11">
                                    <div class="section-name">Chapter 11: Machine Learning with Spark ML Pipelines</div>
                                </a>
                            </li>
                            <li data-chapter="11" data-section-id="ch11lvl1sec72" class="sub-nav">
                                <a href="#ch11lvl1sec72">                    
                                    <div class="section-name">Spark&#x27;s machine learning algorithm table</div>
                                </a>
                            </li>
                            <li data-chapter="11" data-section-id="ch11lvl1sec73" class="sub-nav">
                                <a href="#ch11lvl1sec73">                    
                                    <div class="section-name">Spark machine learning APIs - ML pipelines and MLlib</div>
                                </a>
                            </li>
                            <li data-chapter="11" data-section-id="ch11lvl1sec74" class="sub-nav">
                                <a href="#ch11lvl1sec74">                    
                                    <div class="section-name">ML pipelines</div>
                                </a>
                            </li>
                            <li data-chapter="11" data-section-id="ch11lvl1sec75" class="sub-nav">
                                <a href="#ch11lvl1sec75">                    
                                    <div class="section-name">Spark ML examples</div>
                                </a>
                            </li>
                            <li data-chapter="11" data-section-id="ch11lvl1sec76" class="sub-nav">
                                <a href="#ch11lvl1sec76">                    
                                    <div class="section-name">The API organization</div>
                                </a>
                            </li>
                            <li data-chapter="11" data-section-id="ch11lvl1sec77" class="sub-nav">
                                <a href="#ch11lvl1sec77">                    
                                    <div class="section-name">Basic statistics</div>
                                </a>
                            </li>
                            <li data-chapter="11" data-section-id="ch11lvl1sec78" class="sub-nav">
                                <a href="#ch11lvl1sec78">                    
                                    <div class="section-name">Linear regression</div>
                                </a>
                            </li>
                            <li data-chapter="11" data-section-id="ch11lvl1sec79" class="sub-nav">
                                <a href="#ch11lvl1sec79">                    
                                    <div class="section-name">Classification</div>
                                </a>
                            </li>
                            <li data-chapter="11" data-section-id="ch11lvl1sec80" class="sub-nav">
                                <a href="#ch11lvl1sec80">                    
                                    <div class="section-name">Clustering</div>
                                </a>
                            </li>
                            <li data-chapter="11" data-section-id="ch11lvl1sec81" class="sub-nav">
                                <a href="#ch11lvl1sec81">                    
                                    <div class="section-name">Recommendation</div>
                                </a>
                            </li>
                            <li data-chapter="11" data-section-id="ch11lvl1sec82" class="sub-nav">
                                <a href="#ch11lvl1sec82">                    
                                    <div class="section-name">Hyper parameters</div>
                                </a>
                            </li>
                            <li data-chapter="11" data-section-id="ch11lvl1sec83" class="sub-nav">
                                <a href="#ch11lvl1sec83">                    
                                    <div class="section-name">The final thing</div>
                                </a>
                            </li>
                            <li data-chapter="11" data-section-id="ch11lvl1sec84" class="sub-nav">
                                <a href="#ch11lvl1sec84">                    
                                    <div class="section-name">References</div>
                                </a>
                            </li>
                            <li data-chapter="11" data-section-id="ch11lvl1sec85" class="sub-nav">
                                <a href="#ch11lvl1sec85">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse12">
                                <div class="section-name">12: GraphX</div>
                            </a>
                        </li>
                        <div id="collapse12" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="12" class="sub-nav">
                                <a href="#ch12">
                                    <div class="section-name">Chapter 12: GraphX</div>
                                </a>
                            </li>
                            <li data-chapter="12" data-section-id="ch12lvl1sec86" class="sub-nav">
                                <a href="#ch12lvl1sec86">                    
                                    <div class="section-name">Graphs and graph processing - an introduction</div>
                                </a>
                            </li>
                            <li data-chapter="12" data-section-id="ch12lvl1sec87" class="sub-nav">
                                <a href="#ch12lvl1sec87">                    
                                    <div class="section-name">Spark GraphX</div>
                                </a>
                            </li>
                            <li data-chapter="12" data-section-id="ch12lvl1sec88" class="sub-nav">
                                <a href="#ch12lvl1sec88">                    
                                    <div class="section-name">GraphX - computational model</div>
                                </a>
                            </li>
                            <li data-chapter="12" data-section-id="ch12lvl1sec89" class="sub-nav">
                                <a href="#ch12lvl1sec89">                    
                                    <div class="section-name">The first example - graph</div>
                                </a>
                            </li>
                            <li data-chapter="12" data-section-id="ch12lvl1sec90" class="sub-nav">
                                <a href="#ch12lvl1sec90">                    
                                    <div class="section-name">Building graphs</div>
                                </a>
                            </li>
                            <li data-chapter="12" data-section-id="ch12lvl1sec91" class="sub-nav">
                                <a href="#ch12lvl1sec91">                    
                                    <div class="section-name">The GraphX API landscape</div>
                                </a>
                            </li>
                            <li data-chapter="12" data-section-id="ch12lvl1sec92" class="sub-nav">
                                <a href="#ch12lvl1sec92">                    
                                    <div class="section-name">Structural APIs</div>
                                </a>
                            </li>
                            <li data-chapter="12" data-section-id="ch12lvl1sec93" class="sub-nav">
                                <a href="#ch12lvl1sec93">                    
                                    <div class="section-name">Community, affiliation, and strengths</div>
                                </a>
                            </li>
                            <li data-chapter="12" data-section-id="ch12lvl1sec94" class="sub-nav">
                                <a href="#ch12lvl1sec94">                    
                                    <div class="section-name">Algorithms</div>
                                </a>
                            </li>
                            <li data-chapter="12" data-section-id="ch12lvl1sec95" class="sub-nav">
                                <a href="#ch12lvl1sec95">                    
                                    <div class="section-name">Partition strategy</div>
                                </a>
                            </li>
                            <li data-chapter="12" data-section-id="ch12lvl1sec96" class="sub-nav">
                                <a href="#ch12lvl1sec96">                    
                                    <div class="section-name">Case study - AlphaGo tweets analytics</div>
                                </a>
                            </li>
                            <li data-chapter="12" data-section-id="ch12lvl1sec97" class="sub-nav">
                                <a href="#ch12lvl1sec97">                    
                                    <div class="section-name">References</div>
                                </a>
                            </li>
                            <li data-chapter="12" data-section-id="ch12lvl1sec98" class="sub-nav">
                                <a href="#ch12lvl1sec98">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                    </div>
                </div>
            </ul>
        </div>
        
        <div id="page-content-wrapper" class="book-page">
            <a href="#" id="menu-toggle" class="toggle-nav"><i class="fa fa-bars fa-2x mr5"></i></a>
            
            <a href="#" id="back_to_top" class="back-to-top"><img src="../../mapt/images/kopimi.svg"></a>
            
            <div class="col-sm-12 col-xl-offset-2 col-xl-8 col-lg-offset-1 col-lg-10">
                <div class="btn-group pull-right mt15 mb30" role="group">
                    <a href="#home" class="btn btn-default">
                        <i class="fa fa-share fa-lg no-text-padding"></i>
                        <span class="hidden-xs ml5">Book Home</span>
                    </a>
                    <button class="btn btn-default" data-nid="24705" id="code-download">
                        <i class="fa fa-file fa-lg"></i>
                        <span class="hidden-xs ml5">Download Code Files</span>
                    </button>
                </div>
            </div>
            <div class="clearfix"></div>
            
            <div id="book-wrapper" class="container-fluid">
                <div class="col-sm-12 col-xl-offset-2 col-xl-8 col-lg-offset-1 col-lg-10" id="home">
                    <h2 class="product-title">Fast Data Processing with Spark 2 - Third Edition</h2>
                    <hr>
                    <div class="row">
                        <div class="col-sm-12">
                            <h5 class="mt10">By Krishna Sankar</h5>
                            <div>
                                <p class="mb20"><b>Learn how to use Spark to process big data at speed and scale for sharper analytics. Put the principles into practice for faster, slicker big data projects.</b></p>
                                <a href="#ch01" class="btn btn-info btn-lg pull-right hidden-xs">
                                    Start Reading <i class="fa fa-chevron-right ml5"></i>
                                </a>
                                <a href="#ch01" class="btn btn-info btn-lg btn-block mt20 mb20 visible-xs">
                                    Start Reading <i class="fa fa-chevron-right ml5"></i>
                                </a>
                                <div class="clearfix"></div>
                                <div class="col-sm-12">
                                    <ul id="myTabs" class="nav nav-tabs nav-justified hidden-xs mt20" role="tablist">
                                        <li class="active">
                                            <a href="#info" role="tab" id="info-tab" data-toggle="tab">
                                                <h5>Info</h5>
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#content" role="tab" id="content-tab" data-toggle="tab">
                                                <h5>Contents</h5>
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#author" role="tab" id="author-tab" data-toggle="tab">
                                                <h5>Author</h5>
                                            </a>
                                        </li>
                                    </ul>
                
                                    <ul id="myTabsMobile" class="nav nav-pills text-center nav-stacked visible-xs mb60" role="tablist">
                                        <li class="active">
                                            <a href="#info" role="tab" id="info-tab-responsive" data-toggle="tab">
                                                Info
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#content" role="tab" id="content-tab-responsive" data-toggle="tab">
                                                Contents
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#author" role="tab" id="author-tab-responsive" data-toggle="tab">
                                                Author
                                            </a>
                                        </li>
                                    </ul>
                
                                    <div id="myTabContent" class="tab-content pt30">
                                    
                                        <div role="tabpanel" class="tab-pane active fade in" id="info">
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">Features</h5>
                                            </div>
                                            <div class="hidden-xs">
                                                <h5>Features</h5>
                                            </div>
                                            <hr>
                                            <div>
                                                <ul>
                <li><span style="line-height: 20.4px; background-color: transparent;">A quick way to get started with Spark – and reap the rewards</span></li>
                <li><span style="line-height: 20.4px; background-color: transparent;">From analytics to engineering your Big Data architecture, we’ve got it covered</span></li>
                <li><span style="line-height: 20.4px; background-color: transparent;">Bring your Scala and Java knowledge – and put it to work on new and exciting problems</span></li>
                </ul>
                                            </div>
                                            <br>
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">Learning</h5>
                                            </div>
                                            <div class="hidden-xs">
                                                <h5>Learning</h5>
                                            </div>
                                            <hr>
                                            <div>
                                                <ul>
                <li>Install and set up Spark in your cluster</li>
                <li>Prototype distributed applications with Spark's interactive shell</li>
                <li>Perform data wrangling using the new DataFrame APIs</li>
                <li>Get to know the different ways to interact with Spark's distributed representation of data (RDDs)</li>
                <li>Query Spark with a SQL-like query syntax</li>
                <li>See how Spark works with Big Data</li>
                <li>Implement machine learning systems with highly scalable algorithms</li>
                <li>Use R, the popular statistical language, to work with Spark</li>
                <li>Apply interesting graph algorithms and graph processing with GraphX</li>
                </ul>
                                            </div>
                                            <br>
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">About</h5>
                                            </div>
                                            <div class="hidden-xs">
                                                <h5>About</h5>
                                            </div>
                                            <hr>
                                            <div>
                                                <p>When people want a way to process Big Data at speed, Spark is invariably the solution. With its ease of development (in comparison to the relative complexity of Hadoop), it’s unsurprising that it’s becoming popular with data analysts and engineers everywhere.</p>
                <p>Beginning with the fundamentals, we’ll show you how to get set up with Spark with minimum fuss. You’ll then get to grips with some simple APIs before investigating machine learning and graph processing – throughout we’ll make sure you know exactly how to apply your knowledge.</p>
                <p>You will also learn how to use the Spark shell, how to load data before finding out how to build and run your own Spark applications. Discover how to manipulate your RDD and get stuck into a range of DataFrame APIs. As if that’s not enough, you’ll also learn some useful Machine Learning algorithms with the help of Spark MLlib and integrating Spark with R. We’ll also make sure you’re confident and prepared for graph processing, as you learn more about the GraphX API.</p>
                                            </div>
                                        </div>
                                        
                                        <div role="tabpanel" class="tab-pane fade in" id="content">
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">Contents</h5>
                                                <hr>
                                            </div>
                                            <ul>
                                                <div>
                                                    <li data-chapter="1">
                                                        <div class="section-name">1: Installing Spark and Setting Up Your Cluster</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="1" class="chapter-section">
                                                                    <a href="#ch01">        
                                                                        <div class="section-name">Chapter 1: Installing Spark and Setting Up Your Cluster</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec7" class="chapter-section">
                                                                    <a href="#ch01lvl1sec7">                    
                                                                        <div class="section-name">Directory organization and convention</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec8" class="chapter-section">
                                                                    <a href="#ch01lvl1sec8">                    
                                                                        <div class="section-name">Installing the prebuilt distribution</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec9" class="chapter-section">
                                                                    <a href="#ch01lvl1sec9">                    
                                                                        <div class="section-name">Building Spark from source</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec10" class="chapter-section">
                                                                    <a href="#ch01lvl1sec10">                    
                                                                        <div class="section-name">Spark topology</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec11" class="chapter-section">
                                                                    <a href="#ch01lvl1sec11">                    
                                                                        <div class="section-name">A single machine</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec12" class="chapter-section">
                                                                    <a href="#ch01lvl1sec12">                    
                                                                        <div class="section-name">Running Spark on EC2</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec13" class="chapter-section">
                                                                    <a href="#ch01lvl1sec13">                    
                                                                        <div class="section-name">Deploying Spark with Chef (Opscode)</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec14" class="chapter-section">
                                                                    <a href="#ch01lvl1sec14">                    
                                                                        <div class="section-name">Deploying Spark on Mesos</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec15" class="chapter-section">
                                                                    <a href="#ch01lvl1sec15">                    
                                                                        <div class="section-name">Spark on YARN</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec16" class="chapter-section">
                                                                    <a href="#ch01lvl1sec16">                    
                                                                        <div class="section-name">Spark standalone mode</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec17" class="chapter-section">
                                                                    <a href="#ch01lvl1sec17">                    
                                                                        <div class="section-name">References</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec18" class="chapter-section">
                                                                    <a href="#ch01lvl1sec18">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="2">
                                                        <div class="section-name">2: Using the Spark Shell</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="2" class="chapter-section">
                                                                    <a href="#ch02">        
                                                                        <div class="section-name">Chapter 2: Using the Spark Shell</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec19" class="chapter-section">
                                                                    <a href="#ch02lvl1sec19">                    
                                                                        <div class="section-name">The Spark shell</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec20" class="chapter-section">
                                                                    <a href="#ch02lvl1sec20">                    
                                                                        <div class="section-name">Loading a simple text file</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec21" class="chapter-section">
                                                                    <a href="#ch02lvl1sec21">                    
                                                                        <div class="section-name">Interactively loading data from S3</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec22" class="chapter-section">
                                                                    <a href="#ch02lvl1sec22">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="3">
                                                        <div class="section-name">3: Building and Running a Spark Application</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="3" class="chapter-section">
                                                                    <a href="#ch03">        
                                                                        <div class="section-name">Chapter 3: Building and Running a Spark Application</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec23" class="chapter-section">
                                                                    <a href="#ch03lvl1sec23">                    
                                                                        <div class="section-name">Building Spark applications</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec24" class="chapter-section">
                                                                    <a href="#ch03lvl1sec24">                    
                                                                        <div class="section-name">Data wrangling with iPython</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec25" class="chapter-section">
                                                                    <a href="#ch03lvl1sec25">                    
                                                                        <div class="section-name">Developing Spark with Eclipse</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec26" class="chapter-section">
                                                                    <a href="#ch03lvl1sec26">                    
                                                                        <div class="section-name">Developing Spark with other IDEs</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec27" class="chapter-section">
                                                                    <a href="#ch03lvl1sec27">                    
                                                                        <div class="section-name">Building your Spark job with Maven</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec28" class="chapter-section">
                                                                    <a href="#ch03lvl1sec28">                    
                                                                        <div class="section-name">Building your Spark job with something else</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec29" class="chapter-section">
                                                                    <a href="#ch03lvl1sec29">                    
                                                                        <div class="section-name">References</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec30" class="chapter-section">
                                                                    <a href="#ch03lvl1sec30">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="4">
                                                        <div class="section-name">4: Creating a SparkSession Object</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="4" class="chapter-section">
                                                                    <a href="#ch04">        
                                                                        <div class="section-name">Chapter 4: Creating a SparkSession Object</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec31" class="chapter-section">
                                                                    <a href="#ch04lvl1sec31">                    
                                                                        <div class="section-name">SparkSession versus SparkContext</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec32" class="chapter-section">
                                                                    <a href="#ch04lvl1sec32">                    
                                                                        <div class="section-name">Building a SparkSession object</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec33" class="chapter-section">
                                                                    <a href="#ch04lvl1sec33">                    
                                                                        <div class="section-name">SparkContext - metadata</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec34" class="chapter-section">
                                                                    <a href="#ch04lvl1sec34">                    
                                                                        <div class="section-name">Shared Java and Scala APIs</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec35" class="chapter-section">
                                                                    <a href="#ch04lvl1sec35">                    
                                                                        <div class="section-name">Python</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec36" class="chapter-section">
                                                                    <a href="#ch04lvl1sec36">                    
                                                                        <div class="section-name">iPython</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec37" class="chapter-section">
                                                                    <a href="#ch04lvl1sec37">                    
                                                                        <div class="section-name">Reference</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec38" class="chapter-section">
                                                                    <a href="#ch04lvl1sec38">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="5">
                                                        <div class="section-name">5: Loading and Saving Data in Spark</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="5" class="chapter-section">
                                                                    <a href="#ch05">        
                                                                        <div class="section-name">Chapter 5: Loading and Saving Data in Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec39" class="chapter-section">
                                                                    <a href="#ch05lvl1sec39">                    
                                                                        <div class="section-name">Spark abstractions</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec40" class="chapter-section">
                                                                    <a href="#ch05lvl1sec40">                    
                                                                        <div class="section-name">Data modalities</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec41" class="chapter-section">
                                                                    <a href="#ch05lvl1sec41">                    
                                                                        <div class="section-name">Data modalities and Datasets/DataFrames/RDDs</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec42" class="chapter-section">
                                                                    <a href="#ch05lvl1sec42">                    
                                                                        <div class="section-name">Loading data into an RDD</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec43" class="chapter-section">
                                                                    <a href="#ch05lvl1sec43">                    
                                                                        <div class="section-name">Saving your data</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec44" class="chapter-section">
                                                                    <a href="#ch05lvl1sec44">                    
                                                                        <div class="section-name">References</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec45" class="chapter-section">
                                                                    <a href="#ch05lvl1sec45">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="6">
                                                        <div class="section-name">6: Manipulating Your RDD</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="6" class="chapter-section">
                                                                    <a href="#ch06">        
                                                                        <div class="section-name">Chapter 6: Manipulating Your RDD</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec46" class="chapter-section">
                                                                    <a href="#ch06lvl1sec46">                    
                                                                        <div class="section-name">Manipulating your RDD in Scala and Java</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec47" class="chapter-section">
                                                                    <a href="#ch06lvl1sec47">                    
                                                                        <div class="section-name">Manipulating your RDD in Python</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec48" class="chapter-section">
                                                                    <a href="#ch06lvl1sec48">                    
                                                                        <div class="section-name">References</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec49" class="chapter-section">
                                                                    <a href="#ch06lvl1sec49">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="7">
                                                        <div class="section-name">7: Spark 2.0 Concepts</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="7" class="chapter-section">
                                                                    <a href="#ch07">        
                                                                        <div class="section-name">Chapter 7: Spark 2.0 Concepts</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec50" class="chapter-section">
                                                                    <a href="#ch07lvl1sec50">                    
                                                                        <div class="section-name">Code and Datasets for the rest of the book</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec51" class="chapter-section">
                                                                    <a href="#ch07lvl1sec51">                    
                                                                        <div class="section-name">The data scientist and Spark features</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec52" class="chapter-section">
                                                                    <a href="#ch07lvl1sec52">                    
                                                                        <div class="section-name">Spark v2.0 and beyond</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec53" class="chapter-section">
                                                                    <a href="#ch07lvl1sec53">                    
                                                                        <div class="section-name">Apache Spark - evolution</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec54" class="chapter-section">
                                                                    <a href="#ch07lvl1sec54">                    
                                                                        <div class="section-name">Apache Spark - the full stack</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec55" class="chapter-section">
                                                                    <a href="#ch07lvl1sec55">                    
                                                                        <div class="section-name">The art of a big data store - Parquet</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec56" class="chapter-section">
                                                                    <a href="#ch07lvl1sec56">                    
                                                                        <div class="section-name">References</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec57" class="chapter-section">
                                                                    <a href="#ch07lvl1sec57">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="8">
                                                        <div class="section-name">8: Spark SQL</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="8" class="chapter-section">
                                                                    <a href="#ch08">        
                                                                        <div class="section-name">Chapter 8: Spark SQL</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec58" class="chapter-section">
                                                                    <a href="#ch08lvl1sec58">                    
                                                                        <div class="section-name">The Spark SQL architecture</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec59" class="chapter-section">
                                                                    <a href="#ch08lvl1sec59">                    
                                                                        <div class="section-name">Spark SQL how-to in a nutshell</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec60" class="chapter-section">
                                                                    <a href="#ch08lvl1sec60">                    
                                                                        <div class="section-name">Spark SQL programming</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec61" class="chapter-section">
                                                                    <a href="#ch08lvl1sec61">                    
                                                                        <div class="section-name">References</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec62" class="chapter-section">
                                                                    <a href="#ch08lvl1sec62">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="9">
                                                        <div class="section-name">9: Foundations of Datasets/DataFrames – The Proverbial Workhorse for DataScientists</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="9" class="chapter-section">
                                                                    <a href="#ch09">        
                                                                        <div class="section-name">Chapter 9: Foundations of Datasets/DataFrames – The Proverbial Workhorse for DataScientists</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec63" class="chapter-section">
                                                                    <a href="#ch09lvl1sec63">                    
                                                                        <div class="section-name">Datasets - a quick introduction</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec64" class="chapter-section">
                                                                    <a href="#ch09lvl1sec64">                    
                                                                        <div class="section-name">Dataset APIs - an overview</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec65" class="chapter-section">
                                                                    <a href="#ch09lvl1sec65">                    
                                                                        <div class="section-name">Dataset interfaces and functions</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec66" class="chapter-section">
                                                                    <a href="#ch09lvl1sec66">                    
                                                                        <div class="section-name">References</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec67" class="chapter-section">
                                                                    <a href="#ch09lvl1sec67">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="10">
                                                        <div class="section-name">10: Spark with Big Data</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="10" class="chapter-section">
                                                                    <a href="#ch10">        
                                                                        <div class="section-name">Chapter 10: Spark with Big Data</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec68" class="chapter-section">
                                                                    <a href="#ch10lvl1sec68">                    
                                                                        <div class="section-name">Parquet - an efficient and interoperable big data format</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec69" class="chapter-section">
                                                                    <a href="#ch10lvl1sec69">                    
                                                                        <div class="section-name">HBase</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec70" class="chapter-section">
                                                                    <a href="#ch10lvl1sec70">                    
                                                                        <div class="section-name">Reference</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec71" class="chapter-section">
                                                                    <a href="#ch10lvl1sec71">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="11">
                                                        <div class="section-name">11: Machine Learning with Spark ML Pipelines</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="11" class="chapter-section">
                                                                    <a href="#ch11">        
                                                                        <div class="section-name">Chapter 11: Machine Learning with Spark ML Pipelines</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="11" data-section-id="ch11lvl1sec72" class="chapter-section">
                                                                    <a href="#ch11lvl1sec72">                    
                                                                        <div class="section-name">Spark&#x27;s machine learning algorithm table</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="11" data-section-id="ch11lvl1sec73" class="chapter-section">
                                                                    <a href="#ch11lvl1sec73">                    
                                                                        <div class="section-name">Spark machine learning APIs - ML pipelines and MLlib</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="11" data-section-id="ch11lvl1sec74" class="chapter-section">
                                                                    <a href="#ch11lvl1sec74">                    
                                                                        <div class="section-name">ML pipelines</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="11" data-section-id="ch11lvl1sec75" class="chapter-section">
                                                                    <a href="#ch11lvl1sec75">                    
                                                                        <div class="section-name">Spark ML examples</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="11" data-section-id="ch11lvl1sec76" class="chapter-section">
                                                                    <a href="#ch11lvl1sec76">                    
                                                                        <div class="section-name">The API organization</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="11" data-section-id="ch11lvl1sec77" class="chapter-section">
                                                                    <a href="#ch11lvl1sec77">                    
                                                                        <div class="section-name">Basic statistics</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="11" data-section-id="ch11lvl1sec78" class="chapter-section">
                                                                    <a href="#ch11lvl1sec78">                    
                                                                        <div class="section-name">Linear regression</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="11" data-section-id="ch11lvl1sec79" class="chapter-section">
                                                                    <a href="#ch11lvl1sec79">                    
                                                                        <div class="section-name">Classification</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="11" data-section-id="ch11lvl1sec80" class="chapter-section">
                                                                    <a href="#ch11lvl1sec80">                    
                                                                        <div class="section-name">Clustering</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="11" data-section-id="ch11lvl1sec81" class="chapter-section">
                                                                    <a href="#ch11lvl1sec81">                    
                                                                        <div class="section-name">Recommendation</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="11" data-section-id="ch11lvl1sec82" class="chapter-section">
                                                                    <a href="#ch11lvl1sec82">                    
                                                                        <div class="section-name">Hyper parameters</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="11" data-section-id="ch11lvl1sec83" class="chapter-section">
                                                                    <a href="#ch11lvl1sec83">                    
                                                                        <div class="section-name">The final thing</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="11" data-section-id="ch11lvl1sec84" class="chapter-section">
                                                                    <a href="#ch11lvl1sec84">                    
                                                                        <div class="section-name">References</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="11" data-section-id="ch11lvl1sec85" class="chapter-section">
                                                                    <a href="#ch11lvl1sec85">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="12">
                                                        <div class="section-name">12: GraphX</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="12" class="chapter-section">
                                                                    <a href="#ch12">        
                                                                        <div class="section-name">Chapter 12: GraphX</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="12" data-section-id="ch12lvl1sec86" class="chapter-section">
                                                                    <a href="#ch12lvl1sec86">                    
                                                                        <div class="section-name">Graphs and graph processing - an introduction</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="12" data-section-id="ch12lvl1sec87" class="chapter-section">
                                                                    <a href="#ch12lvl1sec87">                    
                                                                        <div class="section-name">Spark GraphX</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="12" data-section-id="ch12lvl1sec88" class="chapter-section">
                                                                    <a href="#ch12lvl1sec88">                    
                                                                        <div class="section-name">GraphX - computational model</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="12" data-section-id="ch12lvl1sec89" class="chapter-section">
                                                                    <a href="#ch12lvl1sec89">                    
                                                                        <div class="section-name">The first example - graph</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="12" data-section-id="ch12lvl1sec90" class="chapter-section">
                                                                    <a href="#ch12lvl1sec90">                    
                                                                        <div class="section-name">Building graphs</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="12" data-section-id="ch12lvl1sec91" class="chapter-section">
                                                                    <a href="#ch12lvl1sec91">                    
                                                                        <div class="section-name">The GraphX API landscape</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="12" data-section-id="ch12lvl1sec92" class="chapter-section">
                                                                    <a href="#ch12lvl1sec92">                    
                                                                        <div class="section-name">Structural APIs</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="12" data-section-id="ch12lvl1sec93" class="chapter-section">
                                                                    <a href="#ch12lvl1sec93">                    
                                                                        <div class="section-name">Community, affiliation, and strengths</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="12" data-section-id="ch12lvl1sec94" class="chapter-section">
                                                                    <a href="#ch12lvl1sec94">                    
                                                                        <div class="section-name">Algorithms</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="12" data-section-id="ch12lvl1sec95" class="chapter-section">
                                                                    <a href="#ch12lvl1sec95">                    
                                                                        <div class="section-name">Partition strategy</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="12" data-section-id="ch12lvl1sec96" class="chapter-section">
                                                                    <a href="#ch12lvl1sec96">                    
                                                                        <div class="section-name">Case study - AlphaGo tweets analytics</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="12" data-section-id="ch12lvl1sec97" class="chapter-section">
                                                                    <a href="#ch12lvl1sec97">                    
                                                                        <div class="section-name">References</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="12" data-section-id="ch12lvl1sec98" class="chapter-section">
                                                                    <a href="#ch12lvl1sec98">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                            </ul>
                                        </div>
                                        
                                        <div role="tabpanel" class="tab-pane fade" id="author">
                                            <div class="visible-xs">
                                                <h4 class="mobile-title">About the Author</h4>
                                                <hr>
                                            </div>
                                            <p><strong>Krishna Sankar</strong></p>
                                            <div>
                                                <p>Krishna Sankar is a Senior Specialist—AI Data Scientist with Volvo Cars focusing on Autonomous Vehicles. His earlier stints include Chief Data Scientist at <a href="http://cadenttech.tv/" target="_blank">http://cadenttech.tv/</a>, Principal Architect/Data Scientist at Tata America Intl. Corp., Director of Data Science at a bioinformatics startup, and as a Distinguished Engineer at Cisco. He has been speaking at various conferences including ML tutorials at Strata SJC and London 2016, Spark Summit [goo.gl/ab30lD], Strata-Spark Camp, OSCON, PyCon, and PyData, writes about Robots Rules of Order [goo.gl/5yyRv6], Big Data Analytics—Best of the Worst [goo.gl/ImWCaz], predicting NFL, Spark [<a href="http://goo.gl/E4kqMD" target="_blank">http://goo.gl/E4kqMD</a>], Data Science [<a href="http://goo.gl/9pyJMH" target="_blank">http://goo.gl/9pyJMH</a>], Machine Learning [<a href="http://goo.gl/SXF53n" target="_blank">http://goo.gl/SXF53n</a>], Social Media Analysis [<a href="http://goo.gl/D9YpVQ" target="_blank">http://goo.gl/D9YpVQ</a>] as well as has been a guest lecturer at the Naval Postgraduate School. His occasional blogs can be found at <a href="https://doubleclix.wordpress.com/" target="_blank">https://doubleclix.wordpress.com/</a>. His other passion is flying drones (working towards Drone Pilot License (FAA UAS Pilot) and Lego Robotics—you will find him at the St.Louis FLL World Competition as Robots Design Judge.</p>
                                            </div>
                                        </div>
                                        
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    
                    <div class="next-wrapper">
                        <div class="row ns">
                            <hr />
                            <span class="hidden-xs">
                                <h4 class="pull-left">
                                    <strong>Up Next: </strong><span class="section-title"></span>
                                </h4>
                                <a href="#" class="btn btn-primary pull-right btn-lg">
                                    Next Section
                                </a>
                            </span>
                            <span class="visible-xs">
                                <a href="#" class="btn btn-primary btn-block btn-lg">
                                    Next Section
                                </a>
                            </span>
                        </div>
                        <div class="row ns">
                            <hr>
                        </div>
                    </div>
                </div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch01"></a>Chapter 1. Installing Spark and Setting Up Your Cluster</h2></div></div></div><p>This chapter will detail some common methods to set up Spark. Spark on a single machine is excellent for testing or exploring small Datasets, but here you will also learn to use Spark's built-in deployment scripts with a dedicated cluster via <span class="strong"><strong>Secure Shell</strong></span> (<span class="strong"><strong>SSH</strong></span>). For Cloud deployments of Spark, this chapter will look at EC2 (both traditional and Elastic Map reduce). Feel free to skip this chapter if you already have your local Spark instance installed and want to get straight to programming. The best way to navigate through installation is to use this chapter as a guide and refer to the Spark installation documentation at <a class="ulink" href="http://spark.apache.org/docs/latest/cluster-overview.html" target="_blank">http://spark.apache.org/docs/latest/cluster-overview.html</a>.</p><p>Regardless of how you are going to deploy Spark, you will want to get the latest version of Spark from <a class="ulink" href="https://spark.apache.org/downloads.html" target="_blank">https://spark.apache.org/downloads.html</a> (Version 2.0.0 as of this writing). Spark currently releases every 90 days. For coders who want to work with the latest builds, try cloning the code directly from the repository at <a class="ulink" href="https://github.com/apache/spark" target="_blank">https://github.com/apache/spark</a>. The building instructions are available at <a class="ulink" href="https://spark.apache.org/docs/latest/building-spark.html" target="_blank">https://spark.apache.org/docs/latest/building-spark.html</a>. Both source code and prebuilt binaries are available at this link. To interact with <span class="strong"><strong>Hadoop Distributed File System</strong></span> (<span class="strong"><strong>HDFS</strong></span>), you need to use Spark, which is built against the same version of Hadoop as your cluster. For Version 2.0.0 of Spark, the prebuilt package is built against the available Hadoop Versions 2.3, 2.4, 2.6, and 2.7. If you are up for the challenge, it's recommended that you build against the source as it gives you the flexibility of choosing the HDFS version that you want to support as well as apply patches with. In this chapter, we will do both.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip3"></a>Tip</h3><p>As you explore the latest version of Spark, an essential task is to read the release notes and especially what has been changed and deprecated. For 2.0.0, the list is slightly long and is available at <a class="ulink" href="https://spark.apache.org/releases/spark-release-2-0-0.html#removals-behavior-changes-and-deprecations" target="_blank">https://spark.apache.org/releases/spark-release-2-0-0.html#removals-behavior-changes-and-deprecations</a>. For example, the note talks about where the EC2 scripts have moved to and support for Hadoop 2.1 and earlier.</p></div><p>To compile the Spark source, you will need the appropriate version of Scala and the matching JDK. The Spark source <code class="literal">tar</code> utility includes the required Scala components. The following discussion is only for information there is no need to install Scala.</p><p>The Spark developers have done a good job of managing the dependencies. Refer to the <a class="ulink" href="https://spark.apache.org/docs/latest/building-spark.html" target="_blank">https://spark.apache.org/docs/latest/building-spark.html</a> web page for the latest information on this. The website states that:</p><div class="blockquote"><blockquote class="blockquote"><p>
<span class="emphasis"><em>"Building Spark using Maven requires Maven 3.3.9 or newer and Java 7+."</em></span>
</p></blockquote></div><p>Scala gets pulled down as a dependency by Maven (currently Scala 2.11.8). Scala does not need to be installed separately; it is just a bundled dependency.</p><p>Just as a note, Spark 2.0.0 by default runs with Scala 2.11.8, but can be compiled to run with Scala 2.10. I have just seen e-mails in the Spark users' group on this.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip4"></a>Tip</h3><p>This brings up another interesting point about the Spark community. The two essential mailing lists are user@spark.apache.org and dev@spark.apache.org. More details about the Spark community are available at <a class="ulink" href="https://spark.apache.org/community.html" target="_blank">https://spark.apache.org/community.html</a>.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec7"></a>Directory organization and convention</h2></div></div><hr /></div><p>One convention that would be handy is to download and install software in the <code class="literal">/opt</code> directory. Also, have a generic soft link to Spark that points to the current version. For example, <code class="literal">/opt/spark</code> points to <code class="literal">/opt/spark-2.0.0</code> with the following command:</p><pre class="programlisting">
<span class="strong"><strong>sudo ln -f -s spark-2.0.0 spark</strong></span>
</pre><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip5"></a>Tip</h3><p>
<span class="strong"><strong>Downloading the example code</strong></span>
</p><p>You can download the example code files for all of the Packt books you have purchased from your account at <a class="ulink" href="http://www.packtpub.com" target="_blank">http://www.packtpub.com</a>. If you purchased this book elsewhere, you can visit <a class="ulink" href="http://www.packtpub.com/support" target="_blank">http://www.packtpub.com/support</a> and register to have the files e-mailed directly to you.</p></div><p>Later, if you upgrade, say to Spark 2.1, you can change the soft link.</p><p>However, remember to copy any configuration changes and old logs when you change to a new distribution. A more flexible way is to change the configuration directory to <code class="literal">/etc/opt/spark</code> and the log files to <code class="literal">/var/log/spark/</code>. In this way, these files will stay independent of the distribution updates. More details are available at <a class="ulink" href="https://spark.apache.org/docs/latest/configuration.html#overriding-configuration-directory" target="_blank">https://spark.apache.org/docs/latest/configuration.html#overriding-configuration-directory</a> and <a class="ulink" href="https://spark.apache.org/docs/latest/configuration.html#configuring-logging" target="_blank">https://spark.apache.org/docs/latest/configuration.html#configuring-logging</a>.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec8"></a>Installing the prebuilt distribution</h2></div></div><hr /></div><p>Let's download prebuilt Spark and install it. Later, we will also compile a version and build from the source. The download is straightforward. The download page is at <a class="ulink" href="http://spark.apache.org/downloads.html" target="_blank">http://spark.apache.org/downloads.html</a>. Select the options as shown in the following screenshot:</p><p>
</p><div class="mediaobject"><img src="graphics/image_01_001.jpg" /></div><p>
</p><p>We will use <code class="literal">wget</code> from the command line. You can do a direct download as well:</p><pre class="programlisting">
<span class="strong"><strong>cd /opt</strong></span>
<span class="strong"><strong>sudo wget http://www-us.apache.org/dist/spark/spark-2.0.0/spark-2.0.0-bin-hadoop2.7.tgz</strong></span>
</pre><p>We are downloading the prebuilt version for Apache Hadoop 2.7 from one of the possible mirrors. We could have easily downloaded other prebuilt versions as well, as shown in the following screenshot:</p><p>
</p><div class="mediaobject"><img src="graphics/image_01_002.jpg" /></div><p>
</p><p>To uncompress it, execute the following command:</p><pre class="programlisting">
<span class="strong"><strong>sudo tar xvf spark-2.0.0-bin-hadoop2.7.tgz</strong></span>
</pre><p>To test the installation, run the following command:</p><pre class="programlisting">
<span class="strong"><strong>/opt/spark-2.0.0-bin-hadoop2.7/bin/run-example SparkPi 10</strong></span>
</pre><p>It will fire up the Spark stack and calculate the value of Pi. The result will be as shown in the following screenshot:</p><p>
</p><div class="mediaobject"><img src="graphics/image.jpg" /></div><p>
</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec9"></a>Building Spark from source</h2></div></div><hr /></div><p>Let's compile Spark on a new AWS instance. In this way, you can clearly understand what all the requirements are to get a Spark stack compiled and installed. I am using the Amazon Linux AMI, which has Java and other base stacks installed by default. As this is a book on Spark, we can safely assume that you would have the base configurations covered. We will cover the incremental installs for the Spark stack here.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note6"></a>Note</h3><p>The latest instructions for building from the source are available at <a class="ulink" href="http://spark.apache.org/docs/latest/building-spark.html" target="_blank">http://spark.apache.org/docs/latest/building-spark.html</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec6"></a>Downloading the source</h3></div></div></div><p>The first order of business is to download the latest source from <a class="ulink" href="https://spark.apache.org/downloads.html" target="_blank">https://spark.apache.org/downloads.html</a>. Select <span class="strong"><strong>Source Code</strong></span> from option <span class="strong"><strong>2</strong></span>. Choose a package type and either download directly or select a mirror. The download page is shown in the following screenshot:</p><p>
</p><div class="mediaobject"><img src="graphics/image_01_004.jpg" /></div><p>
</p><p>We can either download from the web page or use <code class="literal">wget</code>.</p><p>
</p><div class="mediaobject"><img src="graphics/image_01_005.jpg" /></div><p>
</p><p>We will use <code class="literal">wget</code> from the first mirror shown in the preceding screenshot and download it to the <code class="literal">opt</code> subdirectory, as shown in the following command:</p><pre class="programlisting">
<span class="strong"><strong>cd /opt</strong></span>
<span class="strong"><strong>sudo wget http://www-eu.apache.org/dist/spark/spark-2.0.0/spark-2.0.0.tgz</strong></span>
<span class="strong"><strong>sudo tar -xzf spark-2.0.0.tgz</strong></span>
</pre><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip7"></a>Tip</h3><p>The latest development source is in GitHub, which is available at <a class="ulink" href="https://github.com/apache/spark" target="_blank">https://github.com/apache/spark</a>. The latest version can be checked out by the Git clone at <a class="ulink" href="https://github.com/apache/spark.git" target="_blank">https://github.com/apache/spark.git</a>. This should be done only when you want to see the developments for the next version or when you are contributing to the source.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec7"></a>Compiling the source with Maven</h3></div></div></div><p>Compilation by nature is uneventful, but a lot of information gets displayed on the screen:</p><pre class="programlisting">
<span class="strong"><strong>cd /opt/spark-2.0.0</strong></span>
<span class="strong"><strong>export MAVEN_OPTS="-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m"</strong></span>
<span class="strong"><strong>sudo mvn clean package -Pyarn -Phadoop-2.7 -DskipTests</strong></span>
</pre><p>In order for the preceding snippet to work, we will need Maven installed on our system. Check by typing <code class="literal">mvn -v</code>. You will see the output as shown in the following screenshot:</p><p>
</p><div class="mediaobject"><img src="graphics/image_01_006.jpg" /></div><p>
</p><p>In case Maven is not installed in your system, the commands to install the latest version of Maven are given here:</p><pre class="programlisting">
<span class="strong"><strong>wget http://mirror.cc.columbia.edu/pub/software/apache/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz</strong></span>
<span class="strong"><strong>sudo tar -xzf apache-maven-3.3.9-bin.tar.gz</strong></span>
<span class="strong"><strong>sudo ln -f -s apache-maven-3.3.9 maven</strong></span>
<span class="strong"><strong>export M2_HOME=/opt/maven</strong></span>
<span class="strong"><strong>export PATH=${M2_HOME}/bin:${PATH}</strong></span>
</pre><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip8"></a>Tip</h3><p>Detailed Maven installation instructions are available at <a class="ulink" href="http://maven.apache.org/download.cgi#Installation" target="_blank">http://maven.apache.org/download.cgi#Installation</a>.
Sometimes, you will have to debug Maven using the <code class="literal">-X</code> switch. When I ran Maven, the Amazon Linux AMI didn't have the Java compiler! I had to install <code class="literal">javac</code> for Amazon Linux AMI using the following command:
<code class="literal">
<span class="strong"><strong>sudo yum install java-1.7.0-openjdk-devel</strong></span>
</code>
</p></div><p>The compilation time varies. On my Mac, it took approximately 28 minutes. The Amazon Linux on a <code class="literal">t2-medium</code> instance took 38 minutes. The times could vary, depending on the Internet connection, what libraries are cached, and so forth.</p><p>In the end, you will see a build success message like the one shown in the following screenshot:</p><p>
</p><div class="mediaobject"><img src="graphics/image_01_007.jpg" /></div><p>
</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec8"></a>Compilation switches</h3></div></div></div><p>As an example, the switches for the compilation of <code class="literal">-Pyarn -Phadoop-2.7 -DskipTests</code> are explained in <a class="ulink" href="https://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version" target="_blank">https://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version</a>. The <code class="literal">-D</code> instance defines a system property and <code class="literal">-P</code> defines a profile.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip9"></a>Tip</h3><p>You can also compile the source code in IDEA, and then upload the built version to your cluster.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec9"></a>Testing the installation</h3></div></div></div><p>A quick way to test the installation is by calculating Pi:</p><pre class="programlisting">
<span class="strong"><strong>/opt/spark/bin/run-example SparkPi 10</strong></span>
</pre><p>The result will be a few debug messages, and then the value of <code class="literal">Pi</code>, as shown in the following screenshot:</p><p>
</p><div class="mediaobject"><img src="graphics/image_01_008.jpg" /></div><p>
</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec10"></a>Spark topology</h2></div></div><hr /></div><p>This is a good time to talk about the basic mechanics and mechanisms of Spark. We will progressively dig deeper, but for now let's take a quick look at the top level.</p><p>Essentially, Spark provides a framework to process the vast amounts of data, be it in gigabytes, terabytes, and occasionally petabytes. The two main ingredients are computation and scale. The size and effectiveness of the problems that we can solve depends on these two factors, that is, the ability to apply complex computations over large amounts of data in a timely fashion. If our monthly runs take 40 days, we have a problem.</p><p>The key, of course, is parallelism, massive parallelism to be exact. We can make our computational algorithm tasks work in parallel, that is, instead of doing the steps one after another, we can perform many steps at the same time, or carry out data parallelism. This means that we run the same algorithms over a partitioned Dataset in parallel. In my humble opinion, Spark is extremely effective in applying data parallelism in an elegant framework. As you will see in the rest of this book, the two components are <span class="strong"><strong>Resilient Distributed Dataset</strong></span> (<span class="strong"><strong>RDD</strong></span>) and cluster manager. The cluster manager distributes the code and manages the data that is represented in RDDs. RDDs with transformations and actions are the main programming abstractions and present parallelized collections. Behind the scenes, a cluster manager controls the distribution and interaction with RDDs, distributes code, and manages fault-tolerant execution. As you will see later in the book, Spark has more abstractions on RDDs, namely <span class="strong"><strong>DataFrames</strong></span> and <span class="strong"><strong>Datasets</strong></span>. These layers make it extremely efficient for a data engineer or a data scientist to work on distributed data. Spark works with three types of cluster managers-standalone, Apache Mesos, and Hadoop YARN. The Spark page at <a class="ulink" href="http://spark.apache.org/docs/latest/cluster-overview.html" target="_blank">http://spark.apache.org/docs/latest/cluster-overview.html</a> has a lot more details on this. I just gave you a quick introduction here.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip10"></a>Tip</h3><p>If you have installed Hadoop 2.0, it is recommended to install Spark on YARN. If you have installed Hadoop 1.0, the standalone version is recommended. If you want to try Mesos, you can choose to install Spark on Mesos. Users are not recommended to install both YARN and Mesos.</p></div><p>Refer to the following diagram:</p><p>
</p><div class="mediaobject"><img src="graphics/image_01_009.jpg" /></div><p>
</p><p>The Spark driver program takes the program classes and hands them over to a cluster manager. The cluster manager, in turn, starts executors in multiple worker nodes, each having a set of tasks. When we ran the example program earlier, all these actions happened transparently on your machine! Later, when we install in a cluster, the examples will run, again transparently, across multiple machines in the cluster. This is the magic of Spark and distributed computing!</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec11"></a>A single machine</h2></div></div><hr /></div><p>A single machine is the simplest use case for Spark. It is also a great way to sanity check your build. In <code class="literal">spark/bin</code>, there is a shell script called <code class="literal">run-example</code>, which can be used to launch a Spark job. The <code class="literal">run-example</code> script takes the name of a Spark class and some arguments. Earlier, we used the <code class="literal">run-example</code> script from the <code class="literal">/bin</code> directory to calculate the value of Pi. There is a collection of the sample Spark jobs in <code class="literal">examples/src/main/scala/org/apache/spark/examples/</code>.</p><p>All of the sample programs take the parameter, <code class="literal">master</code> (the cluster manager), which can be the URL of a distributed cluster or <code class="literal">local[N]</code>, where <code class="literal">N</code> is the number of threads.</p><p>Going back to our <code class="literal">run-example</code> script, it invokes the more general <code class="literal">bin/spark-submit</code> script. For now, let's stick with the <code class="literal">run-example</code> script.</p><p>To run <code class="literal">GroupByTest</code> locally, try running the following command:</p><pre class="programlisting">
<span class="strong"><strong>bin/run-example GroupByTest</strong></span>
</pre><p>This line will produce an output like this given here:</p><pre class="programlisting">
<span class="strong"><strong>14/11/15 06:28:40 INFO SparkContext: Job finished: count at  GroupByTest.scala:51, took 0.494519333 s</strong></span>
<span class="strong"><strong>2000</strong></span>
</pre><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note11"></a>Note</h3><p>All the examples in this book can be run on a Spark installation on a local machine. So you can read through the rest of the chapter for additional information after you have gotten some hands-on exposure to Spark running on your local machine.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec12"></a>Running Spark on EC2</h2></div></div><hr /></div><p>Till Spark 2.0.0, the <code class="literal">ec2</code> directory contained the script to run a Spark cluster in EC2. From 2.0.0, the <code class="literal">ec2</code> scripts have been moved to an external repository hosted by the UC Berkeley AMPLab. These scripts can be used to run multiple Spark clusters and even run on-the-spot instances. Spark can also be run on <span class="strong"><strong>Elastic MapReduce</strong></span> (<span class="strong"><strong>Amazon EMR</strong></span>), which is Amazon's solution for MapReduce cluster management, and it gives you more flexibility around scaling instances. The UCB AMPLab page at <a class="ulink" href="https://github.com/amplab/spark-ec2" target="_blank">https://github.com/amplab/spark-ec2</a> has the latest onrunning Spark on EC2.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip12"></a>Tip</h3><p>The Stack Overflow page at <a class="ulink" href="http://stackoverflow.com/questions/38611573/how-to-launch-spark-2-0-on-ec2" target="_blank">http://stackoverflow.com/questions/38611573/how-to-launch-spark-2-0-on-ec2</a> is a must-read before attempting to run Spark on EC2. The blog at <code class="literal">https://medium.com/@eyaldahari/how-to-set-apache-spark-cluster-on-amazon-ec2-in-a-few-simple-steps-d29f0d6f1a81#.8wfa4vqbl</code> also has some good tips for running Spark in EC2.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec10"></a>Downloading EC-scripts</h3></div></div></div><p>There are many ways you can get the scripts. The best way is to download the <code class="literal">.zip</code> file from the AMPLab GitHub, unzip it, and move it from the <code class="literal">ec2</code> directory to the <code class="literal">spark-2.0.0</code> directory. In this way, things will work as before and are contained in the <code class="literal">spark</code> directory.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip13"></a>Tip</h3><p>Remember to repeat this, that is, download the <code class="literal">.zip</code> file, and then move the <code class="literal">ec2</code> directory, when you download newer versions of spark, say <code class="literal">spark-2.1.0</code>.</p></div><p>You can download a <code class="literal">.zip</code> file from GitHub, as shown here:</p><p>
</p><div class="mediaobject"><img src="graphics/image_01_010.jpg" /></div><p>
</p><p>Perform the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Download the <code class="literal">.zip</code> file from GitHub to, say <code class="literal">~/Downloads</code> (or another equivalent directory).</p></li><li><p>Run this command to unzip the files:</p><pre class="programlisting">
<span class="strong"><strong>      unzip spark-ec2-branch-1.6.zip</strong></span>
</pre></li><li><p>Rename the subdirectory:</p><pre class="programlisting">
<span class="strong"><strong>mv spark-ec2-branch-1.6 ec2</strong></span>
</pre></li><li><p>Move the directory under <code class="literal">spark-2.0.0</code>:</p><pre class="programlisting">
<span class="strong"><strong>mv ~/Downloads/ec2 /opt/spark-2.0.0/</strong></span>
</pre></li><li><p>Viola! It is as if the <code class="literal">ec2</code> directory was there all along!</p></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec11"></a>Running Spark on EC2 with the scripts</h3></div></div></div><p>To get started, you should make sure you have EC2 enabled on your account by signing up at <code class="literal">https://portal.aws.amazon.com/gp/aws/manageYourAccount</code>. Then it is a good idea to generate a separate access key pair for your Spark cluster, which you can do at <code class="literal">https://portal.aws.amazon.com/gp/aws/securityCredentials</code>. You will also need to create an EC2 key pair so that the Spark script can SSH to the launched machines, which can be done at <code class="literal">https://console.aws.amazon.com/ec2/home</code> by selecting <span class="strong"><strong>Key Pairs</strong></span> under <span class="strong"><strong>NETWORK &amp; SECURITY</strong></span>. Remember that key pairs are created per region and so you need to make sure that you create your key pair in the same region as you intend to run your Spark instances. Make sure to give it a name that you can remember as you will need it for the scripts (this chapter will use <code class="literal">spark-keypair</code> as its example key pair name.). You can also choose to upload your public SSH key instead of generating a new key. These are sensitive; so make sure that you keep them private. You also need to set <code class="literal">AWS_ACCESS_KEY</code> and <code class="literal">AWS_SECRET_KEY</code> as environment variables for the Amazon EC2 scripts:</p><pre class="programlisting">
<span class="strong"><strong>chmod 400 spark-keypair.pem</strong></span>
<span class="strong"><strong>export AWS_ACCESS_KEY=AWSACcessKeyId</strong></span>
<span class="strong"><strong>export AWS_SECRET_KEY=AWSSecretKey</strong></span>
</pre><p>You will find it useful to download the EC2 scripts provided by Amazon from <code class="literal">http://docs.aws.amazon.com/AWSEC2/latest/CommandLineReference/set-up-ec2-cli-linux.html</code>. Once you unzip the resulting <code class="literal">.zip</code> file, you can add <code class="literal">bin</code> to <code class="literal">PATH</code> in a manner similar to the way you did with the Spark <code class="literal">bin</code> instance:</p><pre class="programlisting">
<span class="strong"><strong>wget http://s3.amazonaws.com/ec2-downloads/ec2-api-tools.zip</strong></span>
<span class="strong"><strong>unzip ec2-api-tools.zip</strong></span>
<span class="strong"><strong>cd ec2-api-tools-*</strong></span>
<span class="strong"><strong>export EC2_HOME='pwd'</strong></span>
<span class="strong"><strong>export PATH=$PATH:'pwd'/bin</strong></span>
</pre><p>In order to test whether this works, try the following command:</p><pre class="programlisting">
<span class="strong"><strong>$ec2-describe-regions</strong></span>
</pre><p>This command will display the output shown in the following screenshot:</p><p>
</p><div class="mediaobject"><img src="graphics/image_01_011.jpg" /></div><p>
</p><p>Finally, you can refer to the EC2 command-line tool reference page at <code class="literal">http://docs.aws.amazon.com/AWSEC2/latest/CommandLineReference/set-up-ec2-cli-linux.html</code> as it has all the gory details.</p><p>The Spark EC2 script automatically creates a separate security group and firewall rules for running the Spark cluster. By default, your Spark cluster will be universally accessible on port <code class="literal">8080</code>, which is somewhat poor. Sadly, the <code class="literal">spark_ec2.py</code> script does not currently provide an easy way to restrict access to just your host. If you have a static IP address, I strongly recommend limiting access in <code class="literal">spark_ec2.py;</code> simply replace all instances of <code class="literal">0.0.0.0/0</code> with <code class="literal">[yourip]/32</code>. This will not affect intra-cluster communication as all machines within a security group can talk to each other by default.</p><p>Next, try to launch a cluster on EC2:</p><pre class="programlisting">
<span class="strong"><strong>./ec2/spark-ec2 -k spark-keypair -i pk-[....].pem -s 1 launch  myfirstcluster</strong></span>
</pre><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip14"></a>Tip</h3><p>If you get an error message, such as <code class="literal">The requested Availability Zone is currently constrained and…</code>, you can specify a different zone by passing in the <code class="literal">--zone</code> flag.</p></div><p>The <code class="literal">-i</code> parameter (in the preceding command line) is provided for specifying the private key to log into the instance; <code class="literal">-i pk-[....].pem</code> represents the path to the private key.</p><p>If you get an error about not being able to SSH to the master, make sure that only you have the permission to read the private key, otherwise SSH will refuse to use it.</p><p>You may also encounter this error due to a race condition, when the hosts report themselves as alive but the <code class="literal">spark-ec2</code> script cannot yet SSH to them. A fix for this issue is pending in <a class="ulink" href="https://github.com/mesos/spark/pull/555" target="_blank">https://github.com/mesos/spark/pull/555</a>. For now, a temporary workaround until the fix is available in the version of Spark you are using is to simply sleep an extra 100 seconds at the start of <code class="literal">setup_cluster</code> using the <code class="literal">-w</code> parameter. The current script has 120 seconds of delay built in.</p><p>If you do get a transient error while launching a cluster, you can finish the launch process using the <code class="literal">resume</code> feature by running the following command:</p><pre class="programlisting">
<span class="strong"><strong>./ec2/spark-ec2 -i ~/spark-keypair.pem launch myfirstsparkcluster  --resume</strong></span>
</pre><p>Refer to the following screenshot:</p><p>
</p><div class="mediaobject"><img src="graphics/image_01_012.jpg" /></div><p>
</p><p>It will go through a bunch of scripts, thus setting up Spark, Hadoop, and so forth. If everything goes well, you will see something like the following screenshot:</p><p>
</p><div class="mediaobject"><img src="graphics/image_01_013.jpg" /></div><p>
</p><p>This will give you a barebones cluster with one master and one worker with all of the defaults on the default machine instance size. Next, verify that it started up and your firewall rules were applied by going to the master on port <code class="literal">8080</code>. You can see in the preceding screenshot that the UI for the master is the output at the end of the script with port at <code class="literal">8080</code> and ganglia at <code class="literal">5080</code>.</p><p>Your AWS EC2 dashboard will show the instances as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_01_014.jpg" /></div><p>
</p><p>The ganglia dashboard shown in the following screenshot is a good place to monitor the instances:</p><p>
</p><div class="mediaobject"><img src="graphics/image_01_015.jpg" /></div><p>
</p><p>Try running one of the example jobs on your new cluster to make sure everything is okay, as shown in the following screenshot:</p><p>
</p><div class="mediaobject"><img src="graphics/image_01_016.jpg" /></div><p>
</p><p>The JPS should show this:</p><pre class="programlisting">
<span class="strong"><strong>root@ip-172-31-45-56 ~]$ jps</strong></span>
<span class="strong"><strong>1904 NameNode</strong></span>
<span class="strong"><strong>2856 Jps</strong></span>
<span class="strong"><strong>2426 Master</strong></span>
<span class="strong"><strong>2078 SecondaryNameNode</strong></span>
</pre><p>The script has started the Spark master, the Hadoop name node, and data nodes (in slaves).</p><p>Let's run the two programs that we ran earlier on our local machine:</p><pre class="programlisting">
<span class="strong"><strong>cd spark</strong></span>
<span class="strong"><strong>bin/run-example GroupByTest</strong></span>
<span class="strong"><strong>bin/run-example SparkPi 10</strong></span>
</pre><p>The ease with which one can spin up a few nodes in the Cloud, install the Spark stack, and run the program in a distributed manner is interesting.</p><p>The <code class="literal">ec2/spark-ec2 destroy &lt;cluster name&gt;</code> command will terminate the instances.</p><p>
</p><div class="mediaobject"><img src="graphics/image_01_017.jpg" /></div><p>
</p><p>If you have a problem with the key pairs, I found the command, <code class="literal">~/aws/ec2-api-tools-1.7.5.1/bin/ec2-describe-keypairs</code> helpful to troubleshoot.</p><p>Now that you've run a simple job on our EC2 cluster, it's time to configure your EC2 cluster for our Spark jobs. There are a number of options you can use to configure with the <code class="literal">spark-ec2</code> script.</p><p>The <code class="literal">ec2/ spark-ec2 -help</code> command will display all the options available.</p><p>First, consider what instance types you may need. EC2 offers an ever-growing collection of instance types and you can choose a different instance type for the master and the workers. The instance type has the most obvious impact on the performance of your Spark cluster. If your work needs a lot of RAM, you should choose an instance with more RAM. You can specify the instance type with <code class="literal">--instance-type= (name of instance type)</code>. By default, the same instance type will be used for both the master and the workers; this can be wasteful if your computations are particularly intensive and the master isn't being heavily utilized. You can specify a different master instance type with <code class="literal">--master-instance-type= (name of instance)</code>.</p><p>Spark's EC2 scripts use <span class="strong"><strong>Amazon Machine Images</strong></span> (<span class="strong"><strong>AMI</strong></span>) provided by the Spark team. Usually, they are current and sufficient for most of the applications. You might need your own AMI in certain circumstances, such as custom patches (for example, using a different version of HDFS) for Spark, as they will not be included in the machine image.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec12"></a>Deploying Spark on Elastic MapReduce</h3></div></div></div><p>In addition to the Amazon basic EC2 machine offering, Amazon offers a hosted MapReduce solution called <span class="strong"><strong>Elastic MapReduce</strong></span> (<span class="strong"><strong>EMR</strong></span>). The blog at <code class="literal">http://blogs.aws.amazon.com/bigdata/post/Tx6J5RM20WPG5V/Building-a-Recommendation-Engine-with-Spark-ML-on-Amazon-EMR-using-Zeppelin</code> has lots of interesting details on how to start Spark in EMR.</p><p>Deploying a Spark-based EMR has become very easy, Spark is a first class entity in EMR. When you create an EMR cluster, you have the option to select Spark. The following screenshot shows the <span class="strong"><strong>Create Cluster-Quick Options</strong></span> of EMR:</p><p>
</p><div class="mediaobject"><img src="graphics/image_01_018.jpg" /></div><p>
</p><p>The advanced option has Spark as well as other stacks.</p><p>
</p><div class="mediaobject"><img src="graphics/image_01_019.jpg" /></div><p>
</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec13"></a>Deploying Spark with Chef (Opscode)</h2></div></div><hr /></div><p>Chef is an open source automation platform that has become increasingly popular for deploying and managing both small and large clusters of machines. Chef can be used to control a traditional static fleet of machines and can also be used with EC2 and other cloud providers. Chef uses cookbooks as the basic building blocks of configuration and can either be generic or site-specific. If you have not used Chef before, a good tutorial for getting started with Chef can be found at <a class="ulink" href="https://learnchef.opscode.com/" target="_blank">https://learnchef.opscode.com/</a>. You can use a generic Spark cookbook as the basis for setting up your cluster.</p><p>To get Spark working, you need to create a role for both the master and the workers as well as configure the workers to connect to the master. Start by getting the cookbook from <a class="ulink" href="https://github.com/holdenk/chef-cookbook-spark" target="_blank">https://github.com/holdenk/chef-cookbook-spark</a>. The bare minimum requirements are to set the master host name (as master) to enable worker nodes to connect, and the username so that Chef can be installed in the correct place. You will also need to either accept Sun's Java license or switch to an alternative JDK. Most of the settings that are available in <code class="literal">spark-env.sh</code> are also exposed through the cookbook settings. You can see an explanation of the settings in the C<span class="emphasis"><em>onfiguring multiple hosts over SSH</em></span> section. The settings can be set as per role or you can modify the global defaults.</p><p>Create a role for the master with a knife role; create <code class="literal">spark_master_role -e [editor]</code>. This will bring up a template role file that you can edit. For a simple master, set it to this code:</p><pre class="programlisting">{
  "name": "spark_master_role", "description": "", "json_class": "Chef::Role",   "default_attributes": {   }, "override_attributes": {
   "username":"spark", "group":"spark", "home":"/home/spark/sparkhome", "master_ip":"10.0.2.15", }, "chef_type": "role", "run_list": [ "recipe[spark::server]", "recipe[chef-client]", ], "env_run_lists": {
  }
}
</pre><p>Then, create a role for the client in the same manner except that instead of <code class="literal">spark::server</code>, you need to use the <code class="literal">spark::client</code> recipe. Deploy the roles to different hosts:</p><pre class="programlisting">
<span class="strong"><strong>knife node run_list add master role[spark_master_role]</strong></span>
<span class="strong"><strong>knife node run_list add worker role[spark_worker_role]</strong></span>
</pre><p>Then, run <code class="literal">chef-client</code> on your nodes to update. Congrats, you now have a Spark cluster running!</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec14"></a>Deploying Spark on Mesos</h2></div></div><hr /></div><p>Mesos is a cluster management platform for running multiple distributed applications or frameworks on a cluster. Mesos can intelligently schedule and run Spark, Hadoop, and other frameworks concurrently on the same cluster. Spark can be run on Mesos either by scheduling individual jobs as separate Mesos tasks or running all of the Spark code as a single Mesos task. Mesos can quickly scale up to handle large clusters beyond the size of which you would want to manage with the plain old SSH scripts. Mesos, written in C++, was originally created at UC Berkley as a research project; it is currently undergoing Apache incubation and is actively used by Twitter.</p><p>The Spark web page, <a class="ulink" href="http://spark.apache.org/docs/latest/running-on-mesos.html" target="_blank">http://spark.apache.org/docs/latest/running-on-mesos.html</a>, has detailed instructions on installing and running Spark on Mesos.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec15"></a>Spark on YARN</h2></div></div><hr /></div><p>YARN is Apache Hadoop's NextGen Resource Manager. The Spark project provides an easy way to schedule jobs on YARN once you have a Spark assembly built. The Spark web page, <a class="ulink" href="http://spark.apache.org/docs/latest/running-on-yarn.html" target="_blank">http://spark.apache.org/docs/latest/running-on-yarn.html</a>, has the configuration details for YARN, which we had built earlier for compiling with the <code class="literal">-Pyarn</code> switch.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec16"></a>Spark standalone mode</h2></div></div><hr /></div><p>If you have a set of machines without any existing cluster management software, you can deploy Spark over SSH with some handy scripts. This method is known as <span class="strong"><strong>standalone mode</strong></span> in the Spark documentation at <a class="ulink" href="http://spark.apache.org/docs/latest/spark-standalone.html" target="_blank">http://spark.apache.org/docs/latest/spark-standalone.html</a>. An individual master and worker can be started by <code class="literal">sbin/start-master.sh</code> and <code class="literal">sbin/start-slaves.sh</code>, respectively. The default port for the master is 8080. As you likely don't want to go to each of your machines and run these commands by hand, there are a number of helper scripts in <code class="literal">bin/</code> to help you run your servers.</p><p>A prerequisite for using any of the scripts is having password less SSH access set up from the master to all of the worker machines. You probably want to create a new user for running Spark on the machines and lock it down. This book uses the username <code class="literal">sparkuser</code>. On your master, you can run <code class="literal">ssh-keygen</code> to generate the SSH keys and make sure that you do not set a password. Once you have generated the key, add the public one (if you generated an RSA key, it would be stored in <code class="literal">~/.ssh/id_rsa.pub</code> by default) to <code class="literal">~/.ssh/authorized_keys2</code> on each of the hosts.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip15"></a>Tip</h3><p>The Spark administration scripts require that your usernames match. If this isn't the case, you can configure an alternative username in your <code class="literal">~/.ssh/config</code>.</p></div><p>Now that you have the SSH access to the machines set up, it is time to configure Spark. There is a simple template in <code class="literal">[filepath]conf/spark-env.sh.template[/filepath]</code>, which you should copy to <code class="literal">[filepath]conf/spark-env.sh[/filepath]</code>.</p><p>You may also find it useful to set some (or all) of the environment variables shown in the following table:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<span class="strong"><strong>Name</strong></span>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<span class="strong"><strong>Purpose</strong></span>
</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>
<span class="strong"><strong>Default</strong></span>
</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">MESOS_NATIVE_LIBRARY</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>This variable is used to point to math where Mesos lives.</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>None</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">SCALA_HOME</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>This variable is used to point to where you extracted Scala.</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>None, must be set</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">SPARK_MASTER_IP</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>This variable states the IP address for the master to listen on and the IP address for the workers to connect to.</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>The result of running hostname</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">SPARK_MASTER_PORT</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>This variable states the port <code class="literal">#</code> for the Spark master to listen on.</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">7077</code>
</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">SPARK_MASTER_WEBUI_PORT</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>This variable states the port <code class="literal">#</code> of the WEB UI on the master.</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">8080</code>
</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">SPARK_WORKER_CORES</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>This variable states the number of cores to use.</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>All of them</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">SPARK_WORKER_MEMORY</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>This variable states how much memory to use.</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Max of (system memory-1 GB, 512 MB)</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">SPARK_WORKER_PORT</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>This variable states what port <code class="literal">#</code> the worker runs on.</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">Rand</code>
</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">SPARK_WEBUI_PORT</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>This variable states what port <code class="literal">#</code> the worker WEB UI runs on.</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">8081</code>
</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; ">
<p>
<code class="literal">SPARK_WORKER_DIR</code>
</p>
</td><td style="border-right: 0.5pt solid ; ">
<p>This variable states where to store files from the worker.</p>
</td><td style="">
<p>
<code class="literal">SPARK_HOME/work_dir</code>
</p>
</td></tr></tbody></table></div><p>Once you have completed your configuration, it's time to get your cluster up and running. You will want to copy the version of Spark and the configuration you have built to all of your machines. You may find it useful to install <code class="literal">pssh</code>, a set of parallel SSH tools, including <code class="literal">pscp</code>. The <code class="literal">pscp</code> tool makes it easy to <code class="literal">scp</code> to a number of target hosts, although it will take a while, as shown here:</p><pre class="programlisting">
<span class="strong"><strong>pscp -v -r -h conf/slaves -l sparkuser ../opt/spark ~/</strong></span>
</pre><p>If you end up changing the configuration, you need to distribute the configuration to all of the workers, as shown here:</p><pre class="programlisting">
<span class="strong"><strong>pscp -v -r -h conf/slaves -l sparkuser conf/spark-env.sh  /opt/spark/conf/spark-env.sh</strong></span>
</pre><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip16"></a>Tip</h3><p>If you use a shared NFS on your cluster, while by default Spark names log files and similar with shared names, you should configure a separate worker directory; otherwise, they will be configured to write to the same place. If you want to have your worker directories on the shared NFS, consider adding <code class="literal">'hostname'</code>, for example <code class="literal">SPARK_WORKER_DIR=~/work-'hostname'</code>.</p><p>You should also consider having your log files go to a scratch directory for performance.</p></div><p>Now you are ready to start the cluster and you can use the <code class="literal">sbin/start-all.sh</code>, <code class="literal">sbin/start-master.sh</code>, and <code class="literal">sbin/start-slaves.sh</code> scripts. It is important to note that <code class="literal">start-all.sh</code> and <code class="literal">start-master.sh</code> both assume that they are being run on the node, which is the master for the cluster. The start scripts all daemonize, and so you don't have to worry about running them on a screen.</p><pre class="programlisting">
<span class="strong"><strong>ssh master bin/start-all.sh</strong></span>
</pre><p>If you get a class not found error stating <code class="literal">java.lang.NoClassDefFoundError: scala/ScalaObject</code>, check to make sure that you have Scala installed on that worker host and that the <code class="literal">SCALA_HOME</code> is set correctly.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip17"></a>Tip</h3><p>The Spark scripts assume that your master has Spark installed in the same directory as your workers. If this is not the case, you should edit <code class="literal">bin/spark-config.sh</code> and set it to the appropriate directories.</p></div><p>The commands provided by Spark to help you administer your cluster are given in the following table. More details are available on the Spark website at <a class="ulink" href="http://spark.apache.org/docs/latest/spark-standalone.html#cluster-launch-scripts" target="_blank">http://spark.apache.org/docs/latest/spark-standalone.html#cluster-launch-scripts</a>.</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<span class="strong"><strong>Command</strong></span>
</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>
<span class="strong"><strong>Use</strong></span>
</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">bin/slaves.sh &lt;command&gt;</code>
</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This command runs the provided command on all of the worker hosts. For example, <code class="literal">bin/slave.sh uptime</code> will show how long each of the worker hosts have been up.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">bin/start-all.sh</code>
</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This command starts the master and all of the worker hosts. This command must be run on the master.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">bin/start-master.sh</code>
</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This command starts the master host. This command must be run on the master.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">bin/start-slaves.sh</code>
</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This command starts the worker hosts.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">bin/start-slave.sh</code>
</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This command starts a specific worker.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">bin/stop-all.sh</code>
</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This command stops master and workers.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">bin/stop-master.sh</code>
</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This command stops the master.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; ">
<p>
<code class="literal">bin/stop-slaves.sh</code>
</p>
</td><td style="">
<p>This command stops all the workers.</p>
</td></tr></tbody></table></div><p>You now have a running Spark cluster. There is a handy Web UI on the master running on port 8080 you should go and visit, and on all of the workers on port 8081. The Web UI contains helpful information such as the current workers, and current and past jobs.</p><p>
</p><div class="mediaobject"><img src="graphics/image_01_020.jpg" /></div><p>
</p><p>Now that you have a cluster up and running, let's actually do something with it. As with the single host example, you can use the provided run script to run the Spark commands. All of the examples listed in <code class="literal">examples/src/main/scala/spark/org/apache/spark/examples/</code> take a parameter, <code class="literal">master</code>, which points them to the master. Assuming that you are on the master host, you could run them with this command:</p><pre class="programlisting">
<span class="strong"><strong>./run-example GroupByTest spark://'hostname':7077</strong></span>
</pre><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip18"></a>Tip</h3><p>If you run into an issue with <code class="literal">java.lang.UnsupportedClassVersionError</code>, you may need to update your JDK or recompile Spark if you grabbed the binary version. Version 1.1.0 was compiled with JDK 1.7 as the target. You can check the version of the JRE targeted by Spark with the following commands:
</p><pre class="programlisting">
<span class="strong"><strong>java -verbose -classpath ./core/target/scala- 2.9.2/classes/</strong></span>
<span class="strong"><strong>spark.SparkFiles |head -n 20</strong></span>
</pre><p>
Version 49 is JDK1.5, Version 50 is JDK1.6, and Version 60 is JDK1.7.</p></div><p>If you can't connect to <code class="literal">localhost</code>, make sure that you've configured your master (<code class="literal">spark.driver.port</code>) to listen to all of the IP addresses (or if you don't want to replace <code class="literal">localhost</code> with the IP address configured to listen to). More port configurations are listed at <a class="ulink" href="http://spark.apache.org/docs/latest/configuration.html#networking" target="_blank">http://spark.apache.org/docs/latest/configuration.html#networking</a>.</p><p>If everything has worked correctly, you will see the following log messages output to <code class="literal">stdout</code>:</p><pre class="programlisting">
<span class="strong"><strong>13/03/28 06:35:31 INFO spark.SparkContext: Job finished: count at GroupByTest.scala:35, took 2.482816756 s2000</strong></span>
</pre></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec17"></a>References</h2></div></div><hr /></div><p>The references are listed here:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<a class="ulink" href="http://archive09.linux.com/feature/151340" target="_blank">http://archive09.linux.com/feature/151340</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://spark-project.org/docs/latest/spark-standalone.html" target="_blank">http://spark-project.org/docs/latest/spark-standalone.html</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://bickson.blogspot.com/2012/10/deploying-graphlabsparkmesos-cluster-on.html" target="_blank">http://bickson.blogspot.com/2012/10/deploying-graphlabsparkmesos-cluster-on.html</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://www.ibm.com/developerworks/library/os-spark/" target="_blank">http://www.ibm.com/developerworks/library/os-spark/</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://mesos.apache.org/" target="_blank">http://mesos.apache.org/</a>
</p></li><li style="list-style-type: disc"><p>
<code class="literal">http://aws.amazon.com/articles/Elastic-MapReduce/4926593393724923</code>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://spark-project.org/docs/latest/ec2-scripts.html" target="_blank">http://spark-project.org/docs/latest/ec2-scripts.html</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://spark.apache.org/docs/latest/cluster-overview.html" target="_blank">http://spark.apache.org/docs/latest/cluster-overview.html</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf" target="_blank">https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://research.google.com/pubs/pub41378.html" target="_blank">http://research.google.com/pubs/pub41378.html</a>
</p></li><li style="list-style-type: disc"><p>
<code class="literal">http://aws.amazon.com/articles/4926593393724923</code>
</p></li><li style="list-style-type: disc"><p>
<code class="literal">http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-cli-install.html</code>
</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec18"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we have installed Spark on our machine for local development and set it up on our cluster, and so we are ready to run the applications that we write. While installing and maintaining a cluster is a good option, Spark is also available as a service option from Databricks. Databricks' Databricks Cloud for Spark, available at <a class="ulink" href="http://databricks.com/product" target="_blank">http://databricks.com/product</a>, is a very convenient offering for anyone who does not want to deal with the set up/maintenance of the cluster. They have the concept of a big data pipeline from ETL to analytics. This looks truly interesting to explore!</p><p>In the next chapter, you will learn to use the Spark shell.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch02"></a>Chapter 2. Using the Spark Shell</h2></div></div></div><p>In this chapter, we will cover the following topics related to the Spark shell:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Running the Spark shell</p></li><li style="list-style-type: disc"><p>Loading a simple text file</p></li><li style="list-style-type: disc"><p>Interactively loading data from S3</p></li><li style="list-style-type: disc"><p>Running the Spark shell in Python</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec19"></a>The Spark shell</h2></div></div><hr /></div><p>The Spark shell is an excellent tool for rapid prototyping with Spark. It works with Scala and Python. It allows you to interact with the Spark cluster and as a result of which, the full API is under your command. It can be great for debugging, just trying things out, or interactively exploring new Datasets or approaches.</p><p>The previous chapter should have gotten you to the point of having a Spark instance running; now all you need to do is start your Spark shell and point it at your running instance with the command given in the table we're soon going to check out.</p><p>For local mode, Spark will start an instance when you invoke the Spark shell or start a Spark program from an IDE. So, a local installation on a Mac or Linux PC/laptop is sufficient to start exploring the Spark shell. Not having to spin up a real cluster to do the prototyping is an important and useful feature of Spark. The <span class="strong"><strong>Quick Start</strong></span> guide at <a class="ulink" href="http://spark.apache.org/docs/latest/quick-start.html" target="_blank">http://spark.apache.org/docs/latest/quick-start.html</a> is a good reference.</p><p>Assuming that you have installed Spark in the <code class="literal">/opt</code> directory and also have a soft link to Spark, run the commands shown in the following table:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Let Spark shell start a Spark Instance</strong></span></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Start Spark shell on an already running instance (local or a Spark cluster)</strong></span></p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
</p><pre class="programlisting">scala</pre><p>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
</p><pre class="programlisting">
<span class="strong"><strong>cd /opt/spark </strong></span>
<span class="strong"><strong>bin/spark-shell</strong></span>
</pre><p>
</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>
</p><pre class="programlisting">
<span class="strong"><strong>cd /opt/spark</strong></span>
<span class="strong"><strong>export MASTER= spark://'hostname':7077   </strong></span>
<span class="strong"><strong>bin/spark-shell</strong></span>
</pre><p>
</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; ">
<p>
</p><pre class="programlisting">python</pre><p>
</p>
</td><td style="border-right: 0.5pt solid ; ">
<p>
</p><pre class="programlisting">
<span class="strong"><strong>cd /opt/spark</strong></span>
<span class="strong"><strong>bin/pyspark</strong></span>
</pre><p>
</p>
</td><td style="">
<p>
</p><pre class="programlisting">
<span class="strong"><strong>cd /opt/spark </strong></span>
<span class="strong"><strong>export MASTER= spark://'hostname':7077   </strong></span>
<span class="strong"><strong>bin/pyspark</strong></span>
</pre><p>
</p>
</td></tr></tbody></table></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip19"></a>Tip</h3><p>The documentation link <a class="ulink" href="http://spark.apache.org/docs/latest/programming-guide.html#using-the-shell" target="_blank">http://spark.apache.org/docs/latest/programming-guide.html#using-the-shell</a> has a list of Spark shell options. For example, <code class="literal">Bin/spark-shell -master local[2]</code> will start the Spark with two threads.</p></div><p>You will see the shell prompt as shown in the following screenshot:</p><p>
</p><div class="mediaobject"><img src="graphics/image_02_001.jpg" /></div><p>
</p><p>I have downloaded and compiled Spark in <code class="literal">~/Downloads/spark-2.0.0</code> and it is running in local mode.</p><p>A few points of interest are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The shell has instantiated a connection object (<code class="literal">SparkSession</code>) to the Spark instance in the <code class="literal">spark</code> variable. This is new to Spark 2.0.0. Earlier versions had <code class="literal">SparkContext</code>, <code class="literal">sqlContext</code>, and <code class="literal">hiveContext</code>. From Version 2.0.0 onward, all these subcontexts are consolidated under <code class="literal">SparkSession</code>, but are available from the <code class="literal">SparkSession</code> object. We will explore all these concepts in later chapters.</p></li><li style="list-style-type: disc"><p>The Spark monitor UI can be accessed at port 4040, as shown in the following screenshot:</p></li></ul></div><p>
</p><div class="mediaobject"><img src="graphics/image_02_002.jpg" /></div><p>
</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec13"></a>Exiting out of the shell</h3></div></div></div><p>When we start any program, the first thing we should know is how to exit. Exiting the shell is easy: use the <code class="literal">:quit</code> command and you will be dropped out of the <code class="literal">spark-shell</code> command.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec14"></a>Using Spark shell to run the book code</h3></div></div></div><p>As a convention that makes it easy to navigate directories, let's start the Spark shell from the directory in which you have downloaded the code and data for this book, which means from either the GitHub, <a class="ulink" href="https://github.com/xsankar/fdps-v3" target="_blank">https://github.com/xsankar/fdps-v3</a>, or the Packt support site.</p><p>Assuming the book code/data is at <code class="literal">~/fdps-v3</code> and Spark at <code class="literal">~/Downloads/spark-2.0.0</code>, start the Spark shell as follows:</p><pre class="programlisting">
<span class="strong"><strong>cd ~/fdps-v3</strong></span>
<span class="strong"><strong>~/Downloads/spark-2.0.0/bin/spark-shell</strong></span>
</pre><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip20"></a>Tip</h3><p>If you have used a different directory structure, please adjust accordingly, that is, change the directory to <code class="literal">fdps-v3</code> and start <code class="literal">spark-shell</code> from there.</p><p>The <code class="literal">fdps-v3/code</code> has the code and <code class="literal">fdps-v3/data</code> has the data.</p></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec20"></a>Loading a simple text file</h2></div></div><hr /></div><p>Let's download a Dataset and do some experimentation. One of the (if not the best) books for machine learning is <span class="emphasis"><em>The Elements of Statistical Learning</em></span>, <span class="emphasis"><em>Trevor Hastie</em></span>, <span class="emphasis"><em>Jerome H. Friedman</em></span>, <span class="emphasis"><em>Robert Tibshirani</em></span>, <span class="emphasis"><em>Springer</em></span>. The book site has an interesting set of Datasets. Let's grab the spam Dataset using the following command:</p><pre class="programlisting">
<span class="strong"><strong>wget http://www-stat.stanford.edu/~tibs/ElemStatLearn/ datasets/spam.data</strong></span>
</pre><p>Alternatively, you can find the spam Dataset from the GitHub link at <a class="ulink" href="https://github.com/xsankar/fdps-v3" target="_blank">https://github.com/xsankar/fdps-v3</a>.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note21"></a>Note</h3><p>All the examples assume that you have downloaded the repository in the <code class="literal">fdps-v3</code> directory in your home folder, that is, <code class="literal">~/fdps-v3/</code>. Please adjust the directory name if you have downloaded them somewhere else.</p></div><p>Now, load it as a text file into Spark with the following commands inside your Spark shell:</p><pre class="programlisting">
<span class="strong"><strong>scala&gt; val inFile = sc.textFile("data/spam.data")</strong></span>
<span class="strong"><strong>scala&gt; inFile.count()</strong></span>
</pre><p>This loads the <code class="literal">spam.data</code> file into Spark with each line being a separate entry in the <span class="strong"><strong>Resilient Distributed Datasets</strong></span> (<span class="strong"><strong>RDDs</strong></span>). You will learn about RDDs in the later chapters; however, RDD, in brief, is the basic data structure that Spark relies on. They are very versatile in terms of scaling, computation capabilities, and transformations.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip22"></a>Tip</h3><p>Spark uses the paradigm of lazy evaluation. The <code class="literal">sc.textFile</code> operation is a lazy operation in Spark, so it doesn't load anything until an action is invoked on the RDD. For example, <code class="literal">count()</code> is an action. So the command <code class="literal">sc.textfile</code> would succeed even when the user enters a bogus file directory; the RDD will still be created. However, when you type in the <code class="literal">action</code> command, it will fail.</p></div><p>The <code class="literal">sc</code> command in the command line is the Spark context. While applications would create a Spark context explicitly, the Spark shell creates something called <code class="literal">sc</code> for you and this is what we normally use.</p><p>You will see the result as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_02_003.jpg" /></div><p>
</p><p>The <code class="literal">count()</code> function gives the number of lines in a file.</p><p>Now, let's look at the first line. Type in the following command:</p><pre class="programlisting">
<span class="strong"><strong>scala&gt; inFile.first()</strong></span>
<span class="strong"><strong>And you will see a string like the screen shot below ! Excellent, you have written your first scala code !</strong></span>
</pre><p>Refer to the following screenshot:</p><p>
</p><div class="mediaobject"><img src="graphics/image_02_004.jpg" /></div><p>
</p><p>Note that if you're connected to a Spark master, it's possible that it will attempt to load the file on any one of the different machines in the cluster, so make sure that it can be accessed by all the worker nodes in a cluster. In general, you will need to put your data in HDFS, S3, or similar distributed file systems to avoid this problem. In local mode, you can just load the file directly (for example, <code class="literal">sc.textFile([filepath])</code>). You can also use the <code class="literal">addFile</code> function on the Spark context to make a file available across all of the machines like this:</p><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.SparkFiles</strong></span>
<span class="strong"><strong>scala&gt; val file = sc.addFile("data/spam.data")</strong></span>
<span class="strong"><strong>scala&gt; val inFile = sc.textFile(SparkFiles.get("spam.data"))</strong></span>
</pre><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip23"></a>Tip</h3><p>Like most shells, the Spark shell has a command history; you can press the up arrow key to get to the previous commands. Are you getting tired of typing or not being sure about what method you want to call on an object? Press <span class="emphasis"><em>Tab</em></span> and the Spark shell will autocomplete the line of code in the best way it can.</p></div><p>For this example, the RDD with each line as an individual string isn't super useful as our input data is actually space-separated numerical information. We can use the <code class="literal">map()</code> operation to iterate over the elements of the RDD and quickly convert it to a usable format. Note that <code class="literal">_.toDouble</code> is the Scala syntactic sugar for <code class="literal">x =&gt; x.toDouble</code>. The numbers are separated by the space. We use a map operation to convert the line to a set of numbers in string format (split the line using the space character) and then convert each of the numbers to a double, as shown next:</p><pre class="programlisting">
<span class="strong"><strong>scala&gt; val nums = inFile.map(line =&gt; line.split(' ').map(_.toDouble))</strong></span>
</pre><p>Verify that this is what we want by inspecting some elements in the <code class="literal">nums</code> RDD and comparing them against the original string RDD. Take a look at the first element of each of these by calling <code class="literal">.first()</code> on the RDDs:</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip24"></a>Tip</h3><p>Most of the output that will follow these commands will be extraneous <code class="literal">INFO</code> messages. No doubt, it will be informative to see what Spark does under the covers. However, if you want to keep the detailed messages out, you can copy <code class="literal">conf/log4j.properties.template as conf/ log4j.properties</code> and set <code class="literal">log4j.logger.org.apache.spark.repl.Main=WARN</code> instead of <code class="literal">INFO</code>. Once this is done, none of these messages will appear, and it will be possible for you to concentrate only on the commands and the output.</p></div><p>The screenshot is shown as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_02_005.jpg" /></div><p>
</p><pre class="programlisting">
<span class="strong"><strong>cp log4j.properties.template log4j.properties</strong></span>
</pre><p>Then, edit the <code class="literal">log4j.properties</code> file and change <code class="literal">log4j.logger.org.apache.spark.repl.Main</code> to <code class="literal">WARN</code>, as shown in the following screenshot:</p><p>
</p><div class="mediaobject"><img src="graphics/image_02_006.jpg" /></div><p>
</p><p>We inspect the first line with the <code class="literal">first()</code> method:</p><pre class="programlisting">
<span class="strong"><strong>scala&gt; inFile.first()</strong></span>
<span class="strong"><strong>res1: String = 0 0.64 0.64 0 0.32 0 0 0 0 0 0 0.64 0 0 0 0.32 0 1.29 1.93 0 0.96 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.778 0 0 3.756 61 278 1</strong></span>
<span class="strong"><strong>scala&gt; nums.first()</strong></span>
<span class="strong"><strong>res2: Array[Double] = Array(0.0, 0.64, 0.64, 0.0, 0.32, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.64, 0.0, 0.0, 0.0, 0.32, 0.0, 1.29, 1.93, 0.0, 0.96, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.778, 0.0, 0.0, 3.756, 61.0, 278.0, 1.0)</strong></span>
</pre><p>When you run a command and do not specify the left-hand side of the assignment (that is, leaving out the <code class="literal">val x</code> value of <code class="literal">val x = y</code>), the Spark shell will assign a default name (that is, <code class="literal">res[number]</code>) to the value.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip25"></a>Tip</h3><p>Operators in Spark are divided into transformations and actions. Transformations are evaluated lazily. Spark just creates the RDDs' lineage graph when you call a transformation, such as a map. No actual work is done until an action is invoked on the RDD. Creating the RDD and the map functions are transformations. The <code class="literal">.first()</code> function is an action that forces the execution.</p><p>So, when we created <code class="literal">inFile</code>, it really didn't do anything except create a variable and set up the pointers. Only when we call an action, such as <code class="literal">.first()</code>, does Spark evaluate the transformations. As a result, even if we point <code class="literal">inFile</code> to a nonexistent directory, Spark will take it. However, when we call <code class="literal">inFile.first()</code>, it will throw the <code class="literal">Input path does not exist:</code> error.</p></div><p>As you can see, the Spark shell is quite powerful. Much of the power comes from it being based on the Scala REPL (the Scala interactive shell) and so it inherits all of the power of the Scala REPL. That being said, most of the time you will probably prefer to work with more traditionally compiled code rather than in the REPL.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec21"></a>Interactively loading data from S3</h2></div></div><hr /></div><p>Now let's try another exercise with the Spark shell. As part of Amazon's EMR Spark support, they have handily provided some sample data of Wikipedia traffic statistics in S3, in the format that Spark can use. To access the data, you first need to set your AWS access credentials as shell params. For instructions on signing up for EC2 and setting up the shell parameters, see the <span class="emphasis"><em>Running Spark on EC2 with the scripts</em></span> section in <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Installing Spark and Setting Up Your Cluster</em></span> (S3 access requires additional keys such as <code class="literal">fs.s3n.awsAccessKeyId/awsSecretAccessKey</code> or the use of the <code class="literal">s3n://user:pw@</code> syntax). You can also set the shell parameters as <code class="literal">AWS_ACCESS_KEY_ID</code> and <code class="literal">AWS_SECRET_ACCESS_KEY</code>. We will leave the AWS configuration out of this discussion, but it needs to be completed.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip26"></a>Tip</h3><p>This is a slightly advanced topic and needs a few S3 configurations (which we won't cover here). The Stack Overflow has two good links on this, namely <a class="ulink" href="http://stackoverflow.com/questions/30385981/how-to-access-s3a-files-from-apache-spark" target="_blank">http://stackoverflow.com/questions/30385981/how-to-access-s3a-files-from-apache-spark</a> and <a class="ulink" href="http://stackoverflow.com/questions/28029134/how-can-i-access-s3-s3n-from-a-local-hadoop-2-6-installation" target="_blank">http://stackoverflow.com/questions/28029134/how-can-i-access-s3-s3n-from-a-local-hadoop-2-6-installation</a>.</p></div><p>Once this is done, load the S3 data and take a look at the first line:</p><pre class="programlisting">
<span class="strong"><strong>scala&gt; val file = sc.textFile("s3n://bigdatademo/sample/wiki/")</strong></span>
<span class="strong"><strong>14/11/16 00:02:43 INFO MemoryStore: ensureFreeSpace(34070) called with curMem=512470, maxMem=278302556</strong></span>
<span class="strong"><strong>14/11/16 00:02:43 INFO MemoryStore: Block broadcast_105 stored as values in memory (estimated size 33.3 KB, free 264.9 MB)</strong></span>
<span class="strong"><strong>file: org.apache.spark.rdd.RDD[String] = s3n://bigdatademo/sample/wiki/ MappedRDD[105] at textFile at &lt;console&gt;:17</strong></span>
<span class="strong"><strong>scala&gt; file.first()</strong></span>
<span class="strong"><strong>14/11/16 00:02:58 INFO BlockManager: Removing broadcast 104</strong></span>
<span class="strong"><strong>14/11/16 00:02:58 INFO BlockManager: Removing block broadcast_104</strong></span>
<span class="strong"><strong>[..]</strong></span>
<span class="strong"><strong>14/11/16 00:03:00 INFO SparkContext: Job finished: first at &lt;console&gt;:20, took 0.442788 s</strong></span>
<span class="strong"><strong>res6: String = aa.b Pecial:Listusers/sysop 1 4695</strong></span>
<span class="strong"><strong>scala&gt; file.take(1)</strong></span>
<span class="strong"><strong>14/11/16 00:05:06 INFO SparkContext: Starting job: take at &lt;console&gt;:20</strong></span>
<span class="strong"><strong>14/11/16 00:05:06 INFO DAGScheduler: Got job 105 (take at &lt;console&gt;:20) with 1 output partitions (allowLocal=true)</strong></span>
<span class="strong"><strong>14/11/16 00:05:06 INFO DAGScheduler: Final stage: Stage 105(take at &lt;console&gt;:20)</strong></span>
<span class="strong"><strong>[...]</strong></span>
<span class="strong"><strong>14/11/16 00:05:07 INFO SparkContext: Job finished: take at &lt;console&gt;:20, took 0.777104 s</strong></span>
<span class="strong"><strong>res7: Array[String] = Array(aa.b Pecial:Listusers/sysop 1 4695)</strong></span>
</pre><p>You don't need to set your AWS credentials as shell params; the general form of the S3 path is <code class="literal">s3n://&lt;AWS ACCESS ID&gt;:&lt;AWS SECRET&gt;@bucket/path</code>.</p><p>It is important to take a look at the first line of the data; the reason for this is that due to lazy evaluation, Spark won't actually bother to load the data unless we force it to materialize something with it. It is useful to note that Amazon has provided a small sample Dataset to get started with. This data is pulled from a much larger set available at <code class="literal">http://aws.amazon.com/datasets/4182</code>. This practice can be quite useful when developing in interactive mode as in this mode, you would want to receive fast feedback of the jobs that are getting completed quickly. If your sample data is too big and your runs are taking too long, you could quickly slim down the RDD by using the <code class="literal">sample</code> functionality built into the Spark shell:</p><pre class="programlisting">
<span class="strong"><strong>scala&gt; val seed  = (100*math.random).toInt</strong></span>
<span class="strong"><strong>seed: Int = 8</strong></span>
<span class="strong"><strong>scala&gt; val sample = file.sample(false,1/10.,seed)</strong></span>
<span class="strong"><strong>res10: spark.RDD[String] = SampledRDD[4] at sample at &lt;console&gt;:17</strong></span>
</pre><p>If you want to rerun on the sampled data later, you could write it back to S3:</p><pre class="programlisting">
<span class="strong"><strong>scala&gt; sample.saveAsTextFile("s3n://mysparkbucket/test")</strong></span>
<span class="strong"><strong>13/04/21 22:46:18 INFO spark.PairRDDFunctions: Saving as hadoop file  of type (NullWritable, Text)</strong></span>
<span class="strong"><strong>....</strong></span>
<span class="strong"><strong>13/04/21 22:47:46 INFO spark.SparkContext: Job finished:  saveAsTextFile at &lt;console&gt;:19, took 87.462236222 s</strong></span>
</pre><p>Now that you have the data loaded, you'll need to find the most popular articles in a sample. First, parse the data by separating it into the name and count fields. Then, reduce the count using the key function, as there can be multiple entries with the same name. Finally, swap the key/value pair so that when we sort by key, we get back the highest count item:</p><pre class="programlisting">
<span class="strong"><strong>scala&gt; val parsed = file.sample(false,1/10.,seed).map(x =&gt; x.split("  ")).map(x =&gt; (x(1), x(2).toInt))</strong></span>
<span class="strong"><strong>parsed: spark.RDD[(java.lang.String, Int)] = MappedRDD[5] at map at  &lt;console&gt;:16</strong></span>
<span class="strong"><strong>scala&gt; val reduced = parsed.reduceByKey(_+_)</strong></span>
<span class="strong"><strong>13/04/21 23:21:49 WARN util.NativeCodeLoader: Unable to load native- hadoop library for your platform... using builtin-java classes where  applicable</strong></span>
<span class="strong"><strong>13/04/21 23:21:49 WARN snappy.LoadSnappy: Snappy native library not  loaded</strong></span>
<span class="strong"><strong>13/04/21 23:21:50 INFO mapred.FileInputFormat: Total input paths to  process : 1</strong></span>
<span class="strong"><strong>reduced: spark.RDD[(java.lang.String, Int)] = MapPartitionsRDD[8] at  reduceByKey at &lt;console&gt;:18</strong></span>
<span class="strong"><strong>scala&gt; val countThenTitle = reduced.map(x =&gt; (x._2, x._1))</strong></span>
<span class="strong"><strong>countThenTitle: spark.RDD[(Int, java.lang.String)] = MappedRDD[9] at  map at &lt;console&gt;:20</strong></span>
<span class="strong"><strong>scala&gt; countThenTitle.sortByKey(false).take(10)</strong></span>
<span class="strong"><strong>13/04/21 23:22:08 INFO spark.SparkContext: Starting job: take at  &lt;console&gt;:23</strong></span>
<span class="strong"><strong>....</strong></span>
<span class="strong"><strong>13/04/21 23:23:15 INFO spark.SparkContext: Job finished: take at  &lt;console&gt;:23, took 66.815676564 s</strong></span>
<span class="strong"><strong>res1: Array[(Int, java.lang.String)] = Array((213652,Main_Page),  (14851,Special:Search), (9528,Special:Export/Can_You_Hear_Me),  (6454,Wikipedia:Hauptseite), (4189,Special:Watchlist),  (3520,%E7%89%B9%E5%88%A5:%E3%81%8A%E3%81%BE%E3%81%8B%E3%81%9B%E8%A1%A 8%E7%A4%BA), (2857,Special:AutoLogin), (2416,P%C3%A1gina_principal),  (1990,Survivor_(TV_series)), (1953,Asperger_syndrome))</strong></span>
</pre><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec15"></a>Running the Spark shell in Python</h3></div></div></div><p>If you are more comfortable with Python than Scala, you can work with Spark interactively in Python by running <code class="literal">[cmd]./pyspark[/cmd]</code>. In order to start working in the Python shell, let's perform the commands in quick start, as shown at <a class="ulink" href="http://spark.apache.org/docs/latest/quick-start.html" target="_blank">http://spark.apache.org/docs/latest/quick-start.html</a>. This is just a simple exercise. We will see more of Python in <a class="link" href="#" linkend="ch09">Chapter 9</a>, <span class="emphasis"><em>Foundations of Datasets/DataFrames – The Proverbial Workhorse for DataScientists</em></span>:</p><p>
</p><div class="mediaobject"><img src="graphics/image_02_007.jpg" /></div><p>
</p><p>The Spark community has done a good job of mapping the APIs so the Scala and Python APIs are very congruent, except when it comes to accommodating language differences. Therefore, if you have done the programming in this book with Scala, you can transfer the skills to Python very easily.</p><p>
</p><div class="mediaobject"><img src="graphics/image_02_008.jpg" /></div><p>
</p><p>Creating text files, <code class="literal">count()</code>, and <code class="literal">first()</code> all work in the same manner.</p><p>Type <code class="literal">exit()</code> to exit the session.</p><p>As you can see, Python operations are very similar to those in Scala.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec22"></a>Summary</h2></div></div><hr /></div><p>In this chapter, you learned how to start the Spark shell (Scala and Python) and load data.</p><p>Now that you've seen how Spark's interactive console works, it's time to see how to build Spark jobs in a more traditional and persistent environment in the subsequent chapters.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch03"></a>Chapter 3. Building and Running a Spark Application</h2></div></div></div><p>This chapter focuses on the mechanics of building Spark applications. There are many tool chains and IDEs with their own details for compiling and building applications. This chapter gives you a quick overview.</p><p>If you are a Spark beginner, all the programs in this book can be run from the Spark shell and you can skip this chapter.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec23"></a>Building Spark applications</h2></div></div><hr /></div><p>Using Spark in an interactive mode with the Spark shell is very good for quick prototyping; however for developing applications, we need an IDE. The choices for the Spark IDE have come a long way since the days of Spark 1.0. One can use an array of the Spark IDEs for developing algorithms, data wrangling (that is, exploring data), and modeling analytics applications. As a general rule of thumb, iPython and Zeppelin are used for data exploration IDEs. The language of choice for iPython is Python and Scala/Java for Zeppelin. This is a general observation; all of them can handle the major languages; Scala, Java, Python, and SQL. For developing Scala and Java, the preferred IDE is Eclipse and IntelliJ. We will mostly use the Spark shell (and occasionally iPython) in this book, as our focus is data wrangling and understanding the Spark APIs. Of course, deploying Spark applications require compiling for Java and Scala.</p><p>Building the Spark jobs is a bit trickier than building a normal application as all dependencies have to be available on all the machines that are in your cluster.</p><p>In this chapter, we will first look at iPython and Eclipse, and then cover the process of building a Java and Scala Spark job with Maven, and learn to build the Spark jobs with a non-Maven aware build system. A reference website for building Spark is at <a class="ulink" href="http://spark.apache.org/docs/latest/building-spark.html" target="_blank">http://spark.apache.org/docs/latest/building-spark.html</a>.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec24"></a>Data wrangling with iPython</h2></div></div><hr /></div><p>I found iPython to be the best way to learn Spark. It is also a very good choice for data scientists and data engineers to explore, model, and reason with data.</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The exploration step includes understanding the data, experimenting with multiple transformations, extracting features for aggregation, and machine learning as well as ETL strategies</p></li><li style="list-style-type: disc"><p>The modeling and reason (of relationships and distributions between the variables) steps require fast iteration over the data and extracted features with different algorithms, experimenting with different parameters and arriving at a set of ML algorithms to develop an analytics app</p></li></ul></div><p>The iPython installation for your system (depending on OS, CPU, and so on) is best described at the iPython site, <a class="ulink" href="http://ipython.org/install.html" target="_blank">http://ipython.org/install.html</a> and <a class="ulink" href="https://ipython.readthedocs.org/en/stable/install/install.html" target="_blank">https://ipython.readthedocs.org/en/stable/install/install.html</a>. The iPython command shell requires the Jupyter notebook system, and then the iPython libraries. Of course, you also would need to have Python installed in your system.</p><p>Once iPython is working, starting the Spark development with iPython is very easy. The iPython IDE hooks up to <code class="literal">pyspark</code> and the interface is via the web browser as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Use <code class="literal">cd</code> into the directory where your notebooks are; for example, assuming that you have downloaded GitHub's <code class="literal">fdps-v3</code> into your home directory, enter as follows:</p></li></ul></div><pre class="programlisting">
<span class="strong"><strong>      cd ~/fdps-v3</strong></span>
<span class="strong"><strong>PYSPARK_DRIVER_PYTHON=ipython
      PYSPARK_DRIVER_PYTHON_OPTS="notebook"
      ~/Downloads/spark-2.0.0-preview/bin/pyspark</strong></span>
</pre><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>I have <code class="literal">spark</code> in my <code class="literal">Downloads</code> directory. If you have <code class="literal">spark</code> in your <code class="literal">/opt</code> directory, the command would be as follows:</p></li></ul></div><pre class="programlisting">
<span class="strong"><strong>      PYSPARK_DRIVER_PYTHON=ipython PYSPARK_DRIVER_PYTHON_OPTS="notebook"
      /opt/spark/bin/pyspark</strong></span>
</pre><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>What you are doing is invoking <code class="literal">pyspark</code> via the iPython IDE.</p></li><li style="list-style-type: disc"><p>You will see the IDE on the browser as shown in the following screenshot:</p></li></ul></div><p>
</p><div class="mediaobject"><img src="graphics/image_03_001.jpg" /></div><p>
</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec25"></a>Developing Spark with Eclipse</h2></div></div><hr /></div><p>Eclipse is the IDE of choice for Java developers. The best way is to install the Scala version (2.11.8) of Eclipse from <a class="ulink" href="http://scala-ide.org/" target="_blank">http://scala-ide.org/</a>. This will enable one to develop the Scala and Java programs.</p><p>Let's first create a Scala project like so:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_002.jpg" /></div><p>
</p><p>From Spark 2.0.0 onwards, they have changed the packaging, so we have to include <code class="literal">spark-2.0.0/assembly/target/scala-2.11/jars</code> in <span class="strong"><strong>Add External Jars…</strong></span> as shown in the following screenshot:</p><p>
</p><div class="mediaobject"><img src="graphics/image_03_003.jpg" /></div><p>
</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec26"></a>Developing Spark with other IDEs</h2></div></div><hr /></div><p>IntelliJ is a very popular IDE, which a lot of engineers use for developing Spark applications. I also like the Zeppelin IDE, which is very interactive, with good visualization capabilities, and supports Python, Scala, Java, and SQL.</p><p>To keep it simple and focused on our goals of working with Spark, we will use the Spark shell most of the time in this book.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip27"></a>Tip</h3><p>We want to make sure you understand the essentials of Spark and get to work with all its features and not worry about IDEs. Once you are familiar with Spark, you can use your favorite language and IDE.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec27"></a>Building your Spark job with Maven</h2></div></div><hr /></div><p>Maven is an open source Apache project that builds the Spark jobs in Java or Scala. As of Version 2.0.0, the building Spark site states that Maven is the official recommendation for packaging Spark and is the "build of reference" too. As with <code class="literal">sbt</code>, you can include the Spark dependency through Maven Central, simplifying our build process. Also, similar to <code class="literal">sbt</code> is the ability of Spark and all of our dependencies to put everything in a single JAR file using a plugin or build Spark as a monolithic JAR file using the <code class="literal">sbt/sbt</code> assembly for inclusion.</p><p>To illustrate the build process for the Spark jobs with Maven, this section will use Java as an example, as Maven is more commonly used to build the Java tasks. As a first step, let's take a Spark job that already works and go through the process of creating a build file for it. We can start by copying the <code class="literal">GroupByTest</code> example into a new directory and generating the Maven template, as shown here:</p><pre class="programlisting">
<span class="strong"><strong>mkdir example-java-build/; cd example-java-build</strong></span>
<span class="strong"><strong>mvn archetype:generate \</strong></span>
<span class="strong"><strong>   -DarchetypeGroupId=org.apache.maven.archetypes \</strong></span>
<span class="strong"><strong>   -DgroupId=spark.examples \</strong></span>
<span class="strong"><strong>  -DartifactId=JavaWordCount \</strong></span>
<span class="strong"><strong>   -Dfilter=org.apache.maven.archetypes:maven-archetype-quickstart</strong></span>
<span class="strong"><strong>cp ../examples/src/main/java/spark/examples/JavaWordCount.java
    JavaWordCount/src/main/java/spark/examples/JavaWordCount.java</strong></span>
</pre><p>Next, update your Maven <code class="literal">example-java-build/JavaWordCount/pom.xml</code> path to include information on the version of Spark we are using. Also, since the example file we are working with requires a JDK version greater than 1.5, we will need to update the Java version that Maven is configured to use; the current version is 1.3. In between the project tags, we will need to add the following code:</p><pre class="programlisting">  &lt;dependencies&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;junit&lt;/groupId&gt;
      &lt;artifactId&gt;junit&lt;/artifactId&gt;
      &lt;version&gt;4.11&lt;/version&gt;
      &lt;scope&gt;test&lt;/scope&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;org.spark-project&lt;/groupId&gt;
      &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;
      &lt;version&gt;2.0.0&lt;/version&gt;
    &lt;/dependency&gt;
  &lt;/dependencies&gt;
  &lt;build&gt;
    &lt;plugins&gt;
      &lt;plugin&gt;
        &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
        &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
        &lt;configuration&gt;
          &lt;source&gt;1.7&lt;/source&gt;
          &lt;target&gt;1.7&lt;/target&gt;
        &lt;/configuration&gt;
      &lt;/plugin&gt;
    &lt;/plugins&gt;
  &lt;/build&gt;
</pre><p>We can now build our JAR file with the <code class="literal">mvn</code> package, run the following command:</p><pre class="programlisting">
<span class="strong"><strong>SPARK_HOME="../"  SPARK_EXAMPLES_JAR="./target/JavaWordCount-1.0- SNAPSHOT.jar"  java -cp ./target/JavaWordCount-1.0- SNAPSHOT.jar:../../core/target/spark-core-assembly-1.5.2.jar  spark.examples.JavaWordCount local[1] ../../README</strong></span>
</pre><p>We can use a plugin to include all of the dependencies in our JAR file. Between the <code class="literal">&lt;plugins&gt;</code> tags, add the following code:</p><pre class="programlisting">&lt;plugin&gt;
  &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
  &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;
  &lt;version&gt;2.3&lt;/version&gt;
  &lt;configuration&gt;
    &lt;!-- This transform is used so that merging of akka        configuration files works --&gt;
    &lt;transformers&gt;
      &lt;transformer          implementation="org.apache.maven.plugins.shade.resource.           ApacheLicenseResourceTransformer"&gt;
      &lt;/transformer&gt;
      &lt;transformer          implementation="org.apache.maven.plugins.shade.resource.           AppendingTransformer"&gt;
        &lt;resource&gt;reference.conf&lt;/resource&gt;
      &lt;/transformer&gt;
    &lt;/transformers&gt;
  &lt;/configuration&gt;
  &lt;executions&gt;
    &lt;execution&gt;
      &lt;phase&gt;package&lt;/phase&gt;
      &lt;goals&gt;
        &lt;goal&gt;shade&lt;/goal&gt;
      &lt;/goals&gt;
    &lt;/execution&gt;
  &lt;/executions&gt;
&lt;/plugin&gt;
</pre><p>Then, run the <code class="literal">mvn</code> assembly and the resulting JAR file can be run as shown in the preceding section; however, leave out the Spark assembly JAR file from the class path.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec28"></a>Building your Spark job with something else</h2></div></div><hr /></div><p>If neither <code class="literal">sbt</code> nor Maven suits your needs, you may decide to use another build system. Thankfully, Spark supports building a fat JAR file with all its dependencies, which makes it easy to include in the build system of your choice. Simply, run the <code class="literal">sbt/sbt</code> assembly in the Spark directory and copy the resulting assembly JAR file at <code class="literal">core/target/spark-core-assembly-1.5.2.jar</code> to your build dependencies, and you are good to go. It is more common to use the <code class="literal">spark-assembly-1.5.2-hadoop2.6.0.jar</code> file. These files exist in <code class="literal">$SPARK_HOME$/lib</code> (if users use a prebuilt version) or in <code class="literal">$SPARK_HOME$/ assembly/target/scala-2.10/</code> (if users build the source code with Maven or <code class="literal">sbt</code>).</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip28"></a>Tip</h3><p>No matter what your build system is, you may find yourself wanting to use a patched version of the Spark libraries. In this case, you can deploy your Spark library locally. I recommend giving it a different version number to ensure that <code class="literal">sbt/Maven</code> picks up the modified version. You can change the version by editing <code class="literal">project/SparkBuild.scala</code> and changing the <code class="literal">version:=</code> part according to the version you have installed. If you are using <code class="literal">sbt</code>, you should run the <code class="literal">sbt/sbt</code> update in the project that is importing the custom version. For other build systems, you just need to ensure that you use the new assembly JAR file as part of your build.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec29"></a>References</h2></div></div><hr /></div><p>The references are listed here:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<a class="ulink" href="http://spark.apache.org/docs/latest/building-spark.html" target="_blank">http://spark.apache.org/docs/latest/building-spark.html</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://www.scala-sbt.org/" target="_blank">http://www.scala-sbt.org/</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="https://github.com/sbt/sbt-assembly" target="_blank">https://github.com/sbt/sbt-assembly</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://spark-project.org/docs/latest/scala-programming-guide.html" target="_blank">http://spark-project.org/docs/latest/scala-programming-guide.html</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://maven.apache.org/guides/getting-started/" target="_blank">http://maven.apache.org/guides/getting-started/</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://maven.apache.org/plugins/maven-compiler-plugin/examples/set-compiler-source-and-target.html" target="_blank">http://maven.apache.org/plugins/maven-compiler-plugin/examples/set-compiler-source-and-target.html</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://maven.apache.org/plugins/maven-dependency-plugin/" target="_blank">http://maven.apache.org/plugins/maven-dependency-plugin/</a>
</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec30"></a>Summary</h2></div></div><hr /></div><p>So now you can wrangle with data using iPython and Eclipse, as well as build your Spark jobs with Maven or a build system of your choice. It's time to jump in and start learning how to do more fun and exciting things, such as learning how to create a Spark session in the subsequent chapter. For this book, you can use the Spark shell and all the code will work. We also have the iPython notebooks for machine learning and DataFrames.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch04"></a>Chapter 4. Creating a SparkSession Object</h2></div></div></div><p>This chapter will cover how to create a <code class="literal">SparkSession</code> object in your cluster. A <code class="literal">SparkSession</code> object represents the connection to a Spark cluster (local or remote) and provides the entry point to interact with Spark. We need to create <code class="literal">SparkSession</code> so that we can interact with Spark and distribute our jobs. In <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Using the Spark Shell</em></span>, we interacted with Spark through the Spark shell which helped us create a <code class="literal">SparkSession</code> object and a <code class="literal">SparkContext</code> object. Now you can create RDDs, broadcast variables, and counters, and actually do fun things with your data. The Spark shell serves as an example of how to interact with the Spark cluster through the <code class="literal">SparkSession</code> and <code class="literal">SparkContext</code> object.</p><p>For a client to establish a connection to the Spark cluster, the <code class="literal">SparkSession</code> object needs some basic information, which is given here:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Master URL</strong></span>: This URL can be <code class="literal">local[n]</code> for local mode, <code class="literal">Spark://[sparkip]</code> for the Spark server, or <code class="literal">mesos://path</code> for a Mesos cluster</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Application name</strong></span>: This information is a human-readable application name</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Spark home</strong></span>: This information is the path to Spark on the master/workers</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>JARs</strong></span>: This information is the path to the JARs required for your job</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec31"></a>SparkSession versus SparkContext</h2></div></div><hr /></div><p>You would have noticed that we are using <code class="literal">SparkSession</code> and <code class="literal">SparkContext</code>, and this is not an error. Let's revisit the annals of Spark history for a perspective. It is important to understand where we came from, as you will hear about these connection objects for some time to come.</p><p>Prior to Spark 2.0.0, the three main connection objects were <code class="literal">SparkContext</code>, <code class="literal">SqlContext</code>, and <code class="literal">HiveContext</code>. The <code class="literal">SparkContext</code> object was the connection to a Spark execution environment and created RDDs and others, <code class="literal">SQLContext</code> worked with SparkSQL in the background of <code class="literal">SparkContext</code>, and <code class="literal">HiveContext</code> interacted with the <code class="literal">Hive</code> stores.</p><p>Spark 2.0.0 introduced Datasets/DataFrames as the main distributed data abstraction interface and the <code class="literal">SparkSession</code> object as the entry point to a Spark execution environment. Appropriately, the <code class="literal">SparkSession</code> object is found in the namespace, <code class="literal">org.apache.spark.sql.SparkSession</code> (Scala), or <code class="literal">pyspark.sql.sparkSession</code>. A few points to note are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>In Scala and Java, Datasets form the main data abstraction as typed data; however, for Python and R (which do not have compile time type checking), the data abstraction is DataFrame. For all practical API purposes, the Datasets in Scala/Java are the same as DataFrames in Python/R.</p></li><li style="list-style-type: disc"><p>While Datasets/DataFrames are top-level interfaces, RDDs have not disappeared. In fact, the underlying structures are still RRDs. (You will see more on RDDs in <a class="link" href="#" linkend="ch06">Chapter 6</a>, <span class="emphasis"><em>Manipulating Your RDD</em></span>.) Also, to interact with RDDs, we still need a <code class="literal">SparkContext</code> object and we can get one from the <code class="literal">SparkSession</code> object.</p></li><li style="list-style-type: disc"><p>The <code class="literal">SparkSession</code> object encapsulates the <code class="literal">SparkContext</code> object. As of version 2.0.0, <code class="literal">SparkContext</code> is still the conduit to a Spark cluster (local or remote); therefore, you will need <code class="literal">SparkCLuster</code> for doing execution environment operations, such as accumulators, <code class="literal">addFile</code>, <code class="literal">addJars</code>, and so on.</p><p>However, for operations such as reading and creating Datasets, use the <code class="literal">SparkSession</code> object.</p></li></ul></div><p>The chapters in this book will deal with this evolutionary influence. The first few chapters will cover <code class="literal">SparkContext</code> and RDDs, and we'll then move on to <code class="literal">SparkSession</code>, Datasets, and DataFrames.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip29"></a>Tip</h3><p>Coincidently, as I was doing the author review for this chapter, Jules Damji of Databricks wrote a very relevant blog, <span class="emphasis"><em>How to use SparkSession in Spark 2.0.0</em></span> available at <a class="ulink" href="https://databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html" target="_blank">https://databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html</a>.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec32"></a>Building a SparkSession object</h2></div></div><hr /></div><p>In the Scala and Python programs, you build a <code class="literal">SparkSession</code> object with the following build pattern:</p><pre class="programlisting">val sparkSession = new SparkSession.builder.master(master_path).appName("application name").config("optional configuration parameters").getOrCreate()
</pre><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip30"></a>Tip</h3><p>While you can hardcode all these values, it's better to read them from the environment with reasonable defaults. This approach provides maximum flexibility to run the code in a changing environment without having to recompile. Using <code class="literal">local</code> as the default value for the master makes it easy to launch your application in a test environment locally. By carefully selecting the defaults, you can avoid having to overspecify this.</p></div><p>The <code class="literal">spark-shell/pyspark</code> creates the <code class="literal">SparkSession</code> object automatically and assigns to the <code class="literal">spark</code> variable.</p><p>The <code class="literal">SparkSession</code> object has the <code class="literal">SparkContext</code> object, which you can access with <code class="literal">spark.sparkContext</code>.</p><p>As we will see later, the <code class="literal">SparkSession</code> object unifies more than the context; it also unifies the process of reading data in different formats and creating Datasets/DataFrames as well as views to execute SQL statements.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip31"></a>Tip</h3><p>So, in short, the rules are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Build a <code class="literal">SparkSession</code> object (or using the one created in the case of <code class="literal">spark-shell/pyspark</code>)</p></li><li style="list-style-type: disc"><p>Use the <code class="literal">SparkSession</code> object for reads, creating views for SQL statements, and creating Datasets and DataFrames</p></li><li style="list-style-type: disc"><p>Get the <code class="literal">SparkContext</code> object from <code class="literal">SparkSession</code> for things such as accumulators, distributing cache files, and working with RDD.</p></li></ul></div><p>Now you understand why we have been talking about <code class="literal">SparkSession</code> and <code class="literal">SparkContext</code> as if they are the same.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec33"></a>SparkContext - metadata</h2></div></div><hr /></div><p>The <code class="literal">SparkContext</code> object has a set of metadata that I found useful. The version number, application name, and memory available are useful pieces of information. At the start of a Spark program, I usually display/log the version number.</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<span class="strong"><strong>Value</strong></span>
</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>
<span class="strong"><strong>Use</strong></span>
</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">appName</code>
</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This value is the application name. If you have established a convention, this field can be useful at runtime.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">getConf</code>
</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This value returns configuration information.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">getExecutorMemoryStatus</code>
</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This value retrieves memory details. It could be useful if you want to check memory details. As Spark is distributed, the values do not mean that you are out of memory.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">Master</code>
</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This value is the name of the master.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; ">
<p>
<code class="literal">Version</code>
</p>
</td><td style="">
<p>I found this value very useful, especially while testing with different versions.</p>
</td></tr></tbody></table></div><p>Execute the following command from the shell:</p><pre class="programlisting">
<span class="strong"><strong>cd ~/Downloads/spark-2.0.0  ( Or to wherever you have your spark installed)</strong></span>
<span class="strong"><strong>bin/spark-shell</strong></span>
</pre><p>Refer to the following screenshot:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_001.jpg" /></div><p>
</p><pre class="programlisting">
<span class="strong"><strong>scala&gt; spark.version</strong></span>
<span class="strong"><strong>res0: String = 2.0.0</strong></span>
<span class="strong"><strong>scala&gt; sc.appName</strong></span>
<span class="strong"><strong>res1: String = Spark shell</strong></span>
<span class="strong"><strong>scala&gt; sc.version</strong></span>
<span class="strong"><strong>res2: String = 2.0.0</strong></span>
<span class="strong"><strong>scala&gt; spark.</strong></span>
</pre><p>Press the <span class="emphasis"><em>Tab</em></span> key and you will see the commands as shown in the following screenshot:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_002.jpg" /></div><p>
</p><pre class="programlisting">
<span class="strong"><strong>scala&gt;sc.</strong></span>
</pre><p>Press the <span class="emphasis"><em>Tab</em></span> key and you will see a bigger list of commands, as shown in the following screenshot:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_003.jpg" /></div><p>
</p><pre class="programlisting">
<span class="strong"><strong>scala&gt; sc.getExecutorMemoryStatus</strong></span>
<span class="strong"><strong>res3: scala.collection.Map[String,(Long, Long)] = Map(10.0.1.2:54783 -&gt; (384093388,384093388))</strong></span>
</pre><p>The <code class="literal">localhost:54783</code> value is the address and the port number of the machine. The first value represents the maximum amount of memory allocated for the block manager (to buffer the intermediate data or cache RDDs), while the second value represents the amount of remaining memory:</p><pre class="programlisting">
<span class="strong"><strong>scala&gt; sc.getConf</strong></span>
<span class="strong"><strong>res5: org.apache.spark.SparkConf = org.apache.spark.SparkConf@7bc17541</strong></span>
<span class="strong"><strong>scala&gt; sc.getConf.toString()</strong></span>
<span class="strong"><strong>res6: String = org.apache.spark.SparkConf@48acaa84</strong></span>
<span class="strong"><strong>scala&gt; </strong></span>
</pre><p>A more informative call of this is given here:</p><pre class="programlisting">
<span class="strong"><strong>scala&gt; sc.getConf.toDebugString</strong></span>
<span class="strong"><strong>res5: String =</strong></span>
<span class="strong"><strong>hive.metastore.warehouse.dir=file:/Users/ksankar/fdps-v3/spark-warehouse</strong></span>
<span class="strong"><strong>spark.app.id=local-1471217311152</strong></span>
<span class="strong"><strong>spark.app.name=Spark shell</strong></span>
<span class="strong"><strong>spark.driver.host=10.0.1.2</strong></span>
<span class="strong"><strong>spark.driver.port=54782</strong></span>
<span class="strong"><strong>spark.executor.id=driver</strong></span>
<span class="strong"><strong>spark.home=/Users/ksankar/Downloads/spark-2.0.0</strong></span>
<span class="strong"><strong>spark.jars= </strong></span>
<span class="strong"><strong>spark.master=local[*]</strong></span>
<span class="strong"><strong>spark.repl.class.outputDir=/private/var/folders/gq/70vnnyfj6913b6lms_td7gb40000gn/T/spark-63174a71-e33f-4265-a427-bdc140553210/repl-35c5c348-cd19-482c-af47-3aff08a7fa42</strong></span>
<span class="strong"><strong>spark.repl.class.uri=spark://10.0.1.2:54782/classes</strong></span>
<span class="strong"><strong>spark.sql.catalogImplementation=in-memory</strong></span>
<span class="strong"><strong>spark.submit.deployMode=client.</strong></span>
<span class="strong"><strong>scala&gt; :quit (To exit the shell)</strong></span>
</pre></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec34"></a>Shared Java and Scala APIs</h2></div></div><hr /></div><p>Once you have a <code class="literal">SparkSession</code> object created, it will serve as your main entry point. In the next chapter, you will learn how to use the <code class="literal">SparkSession</code> object to load and save data. You can also use <code class="literal">SparkSession.SparkContext</code> to launch more Spark jobs and add or remove dependencies. Some of the non-data-driven methods you can use on the <code class="literal">SparkSession.SparkContext</code> object are shown here:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<span class="strong"><strong>Method</strong></span>
</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>
<span class="strong"><strong>Use</strong></span>
</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">addJar(path)</code>
</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This method adds the JAR file for all the future jobs that would run through the <code class="literal">SparkContext</code> object.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">addFile(path)</code>
</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This method downloads the file to all the nodes on the cluster.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">listFiles/listJars</code>
</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This method shows the list of all the currently added files/JARs.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">stop()</code>
</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This method shuts down <code class="literal">SparkContext</code>.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">clearFiles()</code>
</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This method removes the files so that new nodes will not download them.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; ">
<p>
<code class="literal">clearJars()</code>
</p>
</td><td style="">
<p>This method removes the JARs from being required for future jobs.</p>
</td></tr></tbody></table></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec35"></a>Python</h2></div></div><hr /></div><p>The Python <code class="literal">SparkSession</code> object behaves in the same way as Scala. We can almost run the same commands as shown in the previous section, within the constraints of language semantics:</p><pre class="programlisting">
<span class="strong"><strong>bin/pyspark</strong></span>
</pre><p>Refer to the following screenshot:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_004.jpg" /></div><p>
</p><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; spark.version</strong></span>
<span class="strong"><strong>u'2.0.0'</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; sc.version</strong></span>
<span class="strong"><strong>u'2.0.0'</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; sc.appName</strong></span>
<span class="strong"><strong>u'PySparkShell'</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; sc.master</strong></span>
<span class="strong"><strong>u'local[*]'</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; sc.getMemoryStatus</strong></span>
<span class="strong"><strong>Traceback (most recent call last):</strong></span>
<span class="strong"><strong>  File "&lt;stdin&gt;", line 1, in &lt;module&gt;</strong></span>
<span class="strong"><strong>AttributeError: 'SparkContext' object has no attribute 'getMemoryStatus'</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; from pyspark.conf import SparkConf</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; conf = SparkConf()</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; conf.toDebugString()</strong></span>
<span class="strong"><strong>u'spark.app.name=PySparkShell\nspark.master=local[*]\nspark.submit.deployMode=client'</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; </strong></span>
<span class="strong"><strong>&gt;&gt;&gt; exit() (To exit the pyspark shell)</strong></span>
</pre><p>The <code class="literal">PySpark</code> instance does not have the <code class="literal">getExecutorMemoryStatus</code> call yet, but we can get some information with the <code class="literal">.toDebugString</code> call.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec36"></a>iPython</h2></div></div><hr /></div><p>Finally, let's fire up iPython and interact with the <code class="literal">SparkContext</code> object. As mentioned in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Building and Running a Spark Application</em></span>, refer to the iPython site (<a class="ulink" href="http://jupyter.readthedocs.org/en/latest/install.html" target="_blank">http://jupyter.readthedocs.org/en/latest/install.html</a>) for installing the Jupyter and iPython system.</p><p>First, change the directory to <code class="literal">fdps-v3</code>, where you would have downloaded the code and data for this book:</p><pre class="programlisting">
<span class="strong"><strong>cd ~/fdps-v3</strong></span>
</pre><p>The command to start iPython is as follows:</p><pre class="programlisting">
<span class="strong"><strong>PYSPARK_DRIVER_PYTHON=ipython PYSPARK_DRIVER_PYTHON_OPTS="notebook" ~/Downloads/spark-2.0.0/bin/pyspark</strong></span>
</pre><p>The iPython notebook will be launched in the web browser, as shown in the following screenshot, and you will see a list of iPython notebooks:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_005.jpg" /></div><p>
</p><p>Click on the <code class="literal">000-PreFlightCheck.ipynb</code> notebook:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_006.jpg" /></div><p>
</p><p>Run the first cell using <span class="emphasis"><em>Shift</em></span> + <span class="emphasis"><em>Enter.</em></span> You will see the results, including the Python version, Spark version, and so on, as shown in the preceding screenshot. The notebook has more cells, which we will see in the next few chapters.</p><p>Now that you are able to create a connection with your Spark cluster, it's time to start loading data into Spark, which we will see in the next chapter.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec37"></a>Reference</h2></div></div><hr /></div><p>The references are listed here:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<a class="ulink" href="https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/6122906529858466/431554386690884/4814681571895601/latest.html" target="_blank">https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/6122906529858466/431554386690884/4814681571895601/latest.html</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://spark-project.org/docs/latest/quick-start.html" target="_blank">http://spark-project.org/docs/latest/quick-start.html</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://www-stat.stanford.edu/~tibs/ElemStatLearn/data.html" target="_blank">http://www-stat.stanford.edu/~tibs/ElemStatLearn/data.html</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="https://github.com/mesos/spark/blob/master/repl/src/main/scala/spark/repl/SparkILoop.scala" target="_blank">https://github.com/mesos/spark/blob/master/repl/src/main/scala/spark/repl/SparkILoop.scala</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext" target="_blank">http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://www.scala-lang.org/api/current/index.html#scala.util.Properties$" target="_blank">http://www.scala-lang.org/api/current/index.html#scala.util.Properties$</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html" target="_blank">http://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html</a>
</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec38"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we covered how to connect to our Spark cluster using a <code class="literal">SparkSession</code> and <code class="literal">SparkContext</code> object. We saw how the APIs are uniform across all the languages, such as Scala and Python. We also learned a bit about the interactive shell and iPython. Using this knowledge, we will look at the different data sources we can use to load data into Spark in the next chapter.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch05"></a>Chapter 5. Loading and Saving Data in Spark</h2></div></div></div><p>Until now, you have experimented with the Spark shell, figured out how to create a connection with the Spark cluster, and build jobs for deployment. Now to make these jobs useful, you will need to learn how to load and save data in Spark, which we'll do in this chapter.</p><p>Before we dive into data, we have a couple of background tasks to do. First we need to get a view of Spark abstractions, and second, have a quick discussion about the different modalities of data.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec39"></a>Spark abstractions</h2></div></div><hr /></div><p>The goal of this book is that you get a good understanding of Spark via hands-on programming. The best way to understand Spark is to work through operations iteratively. As we are still in the initial chapters, some of the things might not be very clear, but they should be clear enough for the current context. As you write code and read further chapters, you will gather more information and insight. With this in mind, let's move to a quick discussion on Spark abstractions. We will revisit the abstractions in more detail in the following chapters.</p><p>The main features of Apache Spark are distributed data representation and computation, thus achieving massive scaling of data operations. Spark's primary unit for representation of data is RDD, which allows for easy parallel operations on the data. Until 2.0.0, everyone worked with RDDs. However, they are low-level raw structures, which can be optimized for performance and scalability.</p><p>This is where Datasets/DataFrames come into the picture. Datasets/DataFrames are API-level abstractions, that is, the main programming interface. They provide most of the RDD operations but are layered over RDDs via optimized query plans. So, the underlying representation is still an RDD but accessed via the Dataset/DataFrame APIs.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip32"></a>Tip</h3><p>
</p><p>RDDs can be viewed as arrays of arrays with primitive data types, such as integers, floats, and strings. Datasets/DataFrames, on the other hand, are similar to a table or a spreadsheet with column headings-such as name, title, order number, order date, and movie rating-and the associated data types.</p><p>
</p><p>The best part is that there are a few patterns that are recommended with respect to Datasets.</p><p>
</p><p>Wherever possible, use Datasets and DataFrames. Use <code class="literal">SparkSession</code>. However, if you need low-level manipulations to implement complex operations or algorithms, use RDDs. Use the <code class="literal">sparkcontext</code> object encapsulated by <code class="literal">SparkSession</code>.</p><p>
</p><p>Datasets/DataFrames and RDDs can be converted back and forth using <code class="literal">dataset.rdd()</code> and <code class="literal">SparkSession.createDataset(rdd)/SparkSession.createDataFrame(rdd)</code>.</p><p>
</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec16"></a>RDDs</h3></div></div></div><p>Now let's quickly discuss RDDs. Later, we have individual chapters dedicated to Datasets/DataFrames as well as one chapter on RDD operations. Spark RDDs can be created from any supported Hadoop source. Native collections in Scala, Java, and Python can also serve as the basis for an RDD. Creating RDDs from a native collection is especially useful for testing.</p><p>As an RDD follows the principle of lazy evaluation, it evaluates an expression only when it is needed, that is, when an action is called for. This means that when you try to access the data in an RDD, it could fail. The computation to create the data in an RDD is only done when the data is referenced by caching or writing out the RDD. This also means that you can chain a large number of operations and will not have to worry about excessive blocking in a computational thread. It's important to note during application development that you can write code, compile it, and even run your job; however, unless you materialize the RDD, your code would not even try to load the original data.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip33"></a>Tip</h3><p>Each time you materialize an RDD, it is recomputed; therefore, if we are going to use something frequently, performance improvement can be achieved by caching the RDD.</p></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec40"></a>Data modalities</h2></div></div><hr /></div><p>From a modality perspective, all data can be grouped into three categories: structured, semi-structured, and unstructured. The modality is independent of the data source, organization, or storage technologies. In fact, different representations, organizations, and storage technologies perform well with, at the most, one modality. It is very difficult to efficiently support more than one modality.</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Structured data is usually stored in databases, Oracle, HBase, Cassandra, and so on. Relational tables are the most commonly used organization and storage mechanism. Usually, structured data formats, data types, and sizes are fixed and well known.</p></li><li style="list-style-type: disc"><p>Semi-structured data, as the name implies, has enough structure; however, there is also variability in its size, type, and format. The most common semi-structured formats are <code class="literal">csv</code>, <code class="literal">json</code>, and <code class="literal">parquet</code>.</p></li><li style="list-style-type: disc"><p>Unstructured data, of course, is about 85 percent of the data we encounter. Images, audio files, and social media data all are unstructured. A lot of the data that we eventually process starts out as unstructured, which is structured via ETL, transformations, and other techniques.</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec41"></a>Data modalities and Datasets/DataFrames/RDDs</h2></div></div><hr /></div><p>Now let's tie together the modalities with the Spark abstractions and see how we can read and write data. Before 2.0.0, things were conceptually simpler-we only needed to read data into RDDs and use <code class="literal">map()</code> to transform the data as required. However, data wrangling was harder. With Dataset/DataFrame, we have the ability to read directly into a table with headings, associate data types with domain semantics, and start working with data more effectively.</p><p>As a general rule of thumb, perform the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Use <code class="literal">SparkContext</code> and RDDs to handle unstructured data.</p></li><li><p>Use <code class="literal">SparkSession</code> and Datasets/DataFrames for semi-structured and structured data. As you will see in the later chapters, <code class="literal">SparkSession</code> has unified the read from various formats, such as the <code class="literal">.csv</code>, <code class="literal">.json</code>, <code class="literal">.parquet</code>, <code class="literal">.jdbc</code>, <code class="literal">.orc</code>, and <code class="literal">.text</code> files. Moreover, there is a pluggable architecture called DataSource API to access any type of structured data.</p></li></ol></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec42"></a>Loading data into an RDD</h2></div></div><hr /></div><p>In this chapter, we will examine the different sources you can use for your RDD. If you decide to run it through the examples in the Spark shell, you can call <code class="literal">.cache()</code> or <code class="literal">.first()</code> on the RDDs you generate to check whether it can be loaded. In <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Using the Spark Shell</em></span>, you learned how to load data text from a file and from S3. In this chapter, we will look at the different formats of data (text file and CSV) and the different sources (filesystem and HDFS) supported.</p><p>One of the easiest ways to create an RDD is taking an existing Scala collection and converting it into an RDD. The <code class="literal">SparkContext</code> object provides a function called <code class="literal">parallelize</code> that takes a Scala collection and converts it into an RDD of the same type as the input collection, as shown here.</p><p>As mentioned in the previous chapters, <code class="literal">cd</code> to the <code class="literal">fdps-v3</code> directory and run <code class="literal">spark-shell</code> or <code class="literal">pyspark</code>.</p><p>For Scala<span class="strong"><strong>,</strong></span> refer to the following screenshot:</p><p>
</p><div class="mediaobject"><img src="graphics/image_05_001.jpg" /></div><p>
</p><p>For Java, refer to the following code:</p><pre class="programlisting">import java.util.Arrays;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.*;
import org.apache.spark.api.java.function.Function;

public class LDSV01 {

  public static void main(String[] args) {

    SparkConf conf = new SparkConf().setAppName("Chapter 05").setMaster("local");
    JavaSparkContext ctx = new JavaSparkContext(conf);
    JavaRDD&lt;Integer&gt; dataRDD = ctx.parallelize(Arrays.asList(1,2,4));
    System.out.println(dataRDD.count());
    System.out.println(dataRDD.take(3));
  }

}
Using Spark's default log4j profile:org/apache/spark/log4j-defaults.properties
16/01/03 08:55:25 INFO SparkContext: Running Spark version 2.0.0
[..]
16/01/03 08:55:28 INFO DAGScheduler: Job 0 finished: count at LDSV01.java:13, took 0.380508 s
3
16/01/03 08:55:28 INFO SparkContext: Starting job: take at LDSV01.java:14
[..]
16/01/03 08:55:29 INFO DAGScheduler: Job 1 finished: take at LDSV01.java:14, took 0.020155 s
[1, 2, 4]
16/01/03 08:55:29 INFO SparkContext: Invoking stop() from shutdown hook
</pre><p>The reason for a full program in Java is that you can use the Scala and Python shell; however, for Java, you need to compile and run the program.</p><p>For Python, refer to the following screenshot:</p><p>
</p><div class="mediaobject"><img src="graphics/image_05_002.jpg" /></div><p>
</p><p>The simplest method to load external data is loading the text from a file. This has a requirement that the file should be available on all the nodes in the cluster, which isn't much of a problem for local mode. When you're in distributed mode, you will want to use the <code class="literal">addFile</code> functionality of Spark to copy the file to all the machines in your cluster. Assuming that your <code class="literal">SparkContext</code> object is called <code class="literal">sc</code>, we could load the text data from a file that we downloaded in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Using the Spark Shell</em></span>.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip34"></a>Tip</h3><p>The data and code are available on GitHub at <a class="ulink" href="https://github.com/xsankar/fdps-v3" target="_blank">https://github.com/xsankar/fdps-v3</a>, which you can download into <code class="literal">~/fdps-v3</code>.</p></div><p>For Scala, refer to the following code:</p><pre class="programlisting">import org.apache.spark.SparkFiles;
...
sc.addFile("data/spam.data")
val inFile = sc.textFile(SparkFiles.get("spam.data"))
inFile.first()
</pre><p>Here's a screenshot of this:</p><p>
</p><div class="mediaobject"><img src="graphics/image_05_003.jpg" /></div><p>
</p><p>For Java, refer to the following code:</p><pre class="programlisting">import org.apache.spark.SparkConf;
import org.apache.spark.api.java.*;
import org.apache.spark.SparkFiles;;

public class LDSV02 {

  public static void main(String[] args) {
    SparkConf conf = new SparkConf().setAppName("Chapter 05").setMaster("local");
    JavaSparkContext ctx = new JavaSparkContext(conf);
    System.out.println("Running Spark Version : "+ctx.version());
    ctx.addFile("/Users/ksankar/fpds-vii/data/spam.data");
    JavaRDD&lt;String&gt; lines = ctx.textFile(SparkFiles.get("spam.data"));
    System.out.println(lines.first());
  }
}

Using Spark's default log4j profile:org/apache/spark/log4j-defaults.properties
16/08/15 13:49:13 INFO SparkContext: Running Spark version 2.0.0
[..]
Running Spark Version : 2.0.0
[..]
16/08/15 13:49:16 INFO DAGScheduler: Job 0 finished: first at LDSV02.java:13, took 0.576352 s
0 0.64 0.64 0 0.32 0 0 0 0 0 0 0.64 0 0 0 0.32 0 1.29 1.93 0 0.96 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.778 0 0 3.756 61 278 1
[..]
</pre><p>For Python, refer to the following code:</p><pre class="programlisting">from pyspark.files import SparkFiles
sc.addFile("data/spam.data")
in_file = sc.textFile(SparkFiles.get("spam.data"))
in_file.take(1)
</pre><p>Here's a screenshot of this:</p><p>
</p><div class="mediaobject"><img src="graphics/image_05_004.jpg" /></div><p>
</p><p>The resulting RDD is of the string type with each line being a unique element in the RDD. The <code class="literal">take(1)</code> action picks the first element from the RDD.</p><p>Frequently, your input files will be CSV or TSV files, which you will want to read and parse before you create RDDs for processing. The two ways of reading CSV files are as follows: either read and parse them using your own functions or use a CSV library, such as <code class="literal">opencsv</code>.</p><p>First, let's look at parsing using our own functions.</p><p>For Scala, first invoke <code class="literal">spark-shell</code>:</p><p>
</p><div class="mediaobject"><img src="graphics/image_05_005.jpg" /></div><p>
</p><p>Here's the code for this:</p><pre class="programlisting">scala&gt; val inFile = sc.textFile("data/Line_of_numbers.csv")
scala&gt; val numbersRDD = inFile.map(line =&gt; line.split(','))
scala&gt; numbersRDD.take(10)
</pre><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip35"></a>Tip</h3><p>Note that one common error is <code class="literal">org.apache.hadoop.mapred.InvalidInputException: Input path does not exist</code>. This is because Spark looks for the file in the <code class="literal">spark-2.0.0</code> directory if you start from there. It needs the full path.</p><p>Modify the first line to add the full path (where you have downloaded the book files from GitHub, usually <code class="literal">fdps-v3</code>).</p><p>You also need to execute the second line to point the RDD to the right file.</p></div><p>Refer to the following code:</p><pre class="programlisting">scala&gt; val inFile = sc.textFile("data/Line_of_numbers.csv")
inFile: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[4] at textFile at &lt;console&gt;:27

scala&gt; val numbersRDD = inFile.map(line =&gt; line.split(','))
numbersRDD: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[5] at map at &lt;console&gt;:29

scala&gt; numbersRDD.take(10)
res2: Array[Array[String]] = Array(Array(42, 42, 55, 61,53, 49, 43, 47, 49, 60, 68, 54, 34, 35, 35, 39))
</pre><p>This is an array of strings. We need <code class="literal">float</code> or <code class="literal">double</code>:</p><pre class="programlisting">Scala&gt; val numbersRDD = inFile.map(line =&gt; line.split(',')).map(x =&gt; x.map(_.toDouble))
scala&gt; val numbersRDD = inFile.map(line =&gt; line.split(',')).map(_.toDouble)
&lt;console&gt;:15: error: value toDouble is not a member of Array[String]
val numbersRDD = inFile.map(line =&gt; line.split(',')).map(_.toDouble)
</pre><p>This will not work as we have an array of strings. This is where <code class="literal">flatMap</code> comes in handy! It flattens the structure and returns an array:</p><pre class="programlisting">scala&gt; val numbersRDD = inFile.flatMap(line =&gt;    line.split(',')).map(_.toDouble)
numbersRDD: org.apache.spark.rdd.RDD[Double] = MappedRDD[10] at map at &lt;console&gt;:15
scala&gt; numbersRDD.collect()
 [..]
res10: Array[Double] = Array(42.0, 42.0, 55.0, 61.0, 53.0, 49.0, 43.0, 47.0, 49.0, 60.0, 68.0, 54.0, 34.0, 35.0, 35.0, 39.0)
scala&gt; numbersRDD.sum()
res9: Double = 766.0
scala&gt;
</pre><p>For Python, first invoke the <code class="literal">pyspark</code> shell:</p><pre class="programlisting">
<span class="strong"><strong>USS-Defiant:~ ksankar$ cd ~/Downloads/spark-2.0.0/</strong></span>
<span class="strong"><strong>USS-Defiant:spark-2.0.0 ksankar$ bin/pyspark</strong></span>
</pre><p>
</p><div class="mediaobject"><img src="graphics/image_05_006.jpg" /></div><p>
</p><p>Refer to the following command:</p><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; inp_file = sc.textFile("data/Line_of_numbers.csv")</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; numbers_rdd = inp_file.map(lambda line: line.split(','))</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; numbers_rdd.take(10)</strong></span>
<span class="strong"><strong>[..]</strong></span>
<span class="strong"><strong> [[u'42', u'42', u'55', u'61', u'53', u'49', u'43', u'47', u'49', u'60',
       u'68', u'54', u'34', u'35', u'35', u'39']]</strong></span>
<span class="strong"><strong>&gt;&gt;&gt;</strong></span>
<span class="strong"><strong>But we want the values as integers or double</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; numbers_rdd = inp_file.flatMap(lambda line:
        line.split(',')).map(lambda x:float(x))</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; numbers_rdd.take(10)</strong></span>
<span class="strong"><strong>    [42.0, 42.0, 55.0, 61.0, 53.0, 49.0, 43.0, 47.0, 49.0, 60.0]</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; numbers_sum = numbers_rdd.sum()</strong></span>
<span class="strong"><strong>[..]</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; numbers_sum</strong></span>
<span class="strong"><strong>766.0</strong></span>
<span class="strong"><strong>&gt;&gt;&gt;</strong></span>
</pre><p>For Java, use Eclipse or your favorite Java IDE to run the program, <code class="literal">fdps-v3/code/LDSV03.java</code>:</p><pre class="programlisting">import java.util.Arrays;
import java.util.List;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.*;
import org.apache.spark.api.java.function.DoubleFunction;
import org.apache.spark.api.java.function.FlatMapFunction;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.api.java.function.Function2;
import org.apache.spark.SparkFiles;;

public class LDSV03 {

  public static void main(String[] args) {
    SparkConf conf = new SparkConf().setAppName("Chapter
       05").setMaster("local");
    JavaSparkContext ctx = new JavaSparkContext(conf);
    System.out.println("Running Spark Version : " +ctx.version());
    ctx.addFile("/Users/ksankar/fdps-vii/data/Line_of_numbers.csv");
    //
    JavaRDD&lt;String&gt; lines =
    ctx.textFile(SparkFiles.get("Line_of_numbers.csv"));
    //
    JavaRDD&lt;String[]&gt; numbersStrRDD = lines.map(new
    Function&lt;String,String[]&gt;() {
      public String[] call(String line) {return line.split(",");}
    });
    List&lt;String[]&gt; val = numbersStrRDD.take(1);
    for (String[] e : val) {
      for (String s : e) {
        System.out.print(s+" ");
      }
      System.out.println();
    }
    //
    JavaRDD&lt;String&gt; strFlatRDD = lines.flatMap(new
    FlatMapFunction&lt;String,String&gt;() {
      public Iterable&lt;String&gt; call(String line) {return
      Arrays.asList(line.split(","));}
    });
    List&lt;String&gt; val1 = strFlatRDD.collect();
    for (String s : val1) {
      System.out.print(s+" ");
      }
    System.out.println();
    //
    JavaRDD&lt;Integer&gt; numbersRDD = strFlatRDD.map(new
    Function&lt;String,Integer&gt;() {
      public Integer call(String s) {return Integer.parseInt(s);}
    });
    List&lt;Integer&gt; val2 = numbersRDD.collect();
    for (Integer s : val2) {
      System.out.print(s+" ");
      }
    System.out.println();
    //
    Integer sum = numbersRDD.reduce(new
    Function2&lt;Integer,Integer,Integer&gt;() {
      public Integer call(Integer a, Integer b) {return a+b;}
    });
    System.out.println("Sum = "+sum);
  }
}
</pre><p>The results are as expected:</p><pre class="programlisting">
<span class="strong"><strong>Using Spark's default log4j profile: org/apache/spark/log4j-
    defaults.properties</strong></span>
<span class="strong"><strong>16/01/03 09:54:19 INFO SparkContext: Running Spark version 2.0.0</strong></span>
<span class="strong"><strong>[..]</strong></span>
<span class="strong"><strong>Running Spark Version : 2.0.0</strong></span>
<span class="strong"><strong>16/01/03 09:54:22 INFO Utils: Copying /Volumes/sdxc-01/fdps-
    vii/data/Line_of_numbers.csv to
    /private/var/folders/gq/70vnnyfj6913b6lms_td7gb40000gn/T/
    spark-8cd10820-4a05-49a0-b01d-fc3771d78c21/userFiles-3c65ab59-0f4b-
    4de5-b719-db828be61f92/Line_of_numbers.csv</strong></span>
<span class="strong"><strong>16/01/03 09:54:22 INFO SparkContext: Added file /Volumes/sdxc-01/fdps-
    vii/data/Line_of_numbers.csv at file:/Volumes/sdxc-01/fdps-
    vii/data/Line_of_numbers.csv with timestamp 1451843662655</strong></span>
<span class="strong"><strong>16/01/03 09:54:23 INFO MemoryStore: Block broadcast_0 stored as values
    in memory (estimated size 225.2 KB, free 225.2 KB)</strong></span>
<span class="strong"><strong>16/01/03 09:54:23 INFO MemoryStore: Block broadcast_0_piece0 stored as
    bytes in memory (estimated size 19.3 KB, free 244.6 KB)</strong></span>
<span class="strong"><strong>16/01/03 09:54:23 INFO BlockManagerInfo: Added broadcast_0_piece0 in
    memory on localhost:50463 (size: 19.3 KB, free: 2.4 GB)</strong></span>
<span class="strong"><strong>16/01/03 09:54:23 INFO SparkContext: Created broadcast 0 from textFile
    at LDSV03.java:20</strong></span>
<span class="strong"><strong>16/01/03 09:54:23 INFO FileInputFormat: Total input paths to process
    : 1</strong></span>
<span class="strong"><strong>[..]</strong></span>
<span class="strong"><strong>16/01/03 09:54:23 INFO DAGScheduler: Job 0 finished: take at
    LDSV03.java:25,
    took 0.193101 s</strong></span>
<span class="strong"><strong>42 42 55 61 53 49 43 47 49 60 68 54 34 35 35 39 </strong></span>
<span class="strong"><strong>16/01/03 09:54:23 INFO SparkContext: Starting job: collect at
    LDSV03.java:36</strong></span>
<span class="strong"><strong>[..]</strong></span>
<span class="strong"><strong>16/01/03 09:54:23 INFO DAGScheduler: Job 1 finished: collect at
    LDSV03.java:36, took 0.025556 s</strong></span>
<span class="strong"><strong>42 42 55 61 53 49 43 47 49 60 68 54 34 35 35 39 </strong></span>
<span class="strong"><strong>16/01/03 09:54:23 INFO SparkContext: Starting job: collect at
    LDSV03.java:45</strong></span>
<span class="strong"><strong>[..]</strong></span>
<span class="strong"><strong>16/01/03 09:54:23 INFO DAGScheduler: Job 2 finished: collect at
    LDSV03.java:45, took 0.035017 s</strong></span>
<span class="strong"><strong>42 42 55 61 53 49 43 47 49 60 68 54 34 35 35 39 </strong></span>
<span class="strong"><strong>16/01/03 09:54:23 INFO SparkContext: Starting job: reduce at
    LDSV03.java:51</strong></span>
<span class="strong"><strong>[..]</strong></span>
<span class="strong"><strong>16/01/03 09:54:23 INFO DAGScheduler: Job 3 finished: reduce at
    LDSV03.java:51, took 0.025595 s</strong></span>
<span class="strong"><strong>Sum = 766</strong></span>
<span class="strong"><strong>16/01/03 09:54:23 INFO SparkContext: Invoking stop() from shutdown hook</strong></span>
</pre><p>This also illustrates one of the ways of getting the data out of Spark; you can transform it to a standard Scala array using the <code class="literal">collect()</code> function. The <code class="literal">collect()</code> function is especially useful for testing, in much the same way that the <code class="literal">parallelize()</code> function is. The <code class="literal">collect()</code> function collects the job's execution results, while <code class="literal">parallelize()</code> partitions the input data and makes it an RDD. The <code class="literal">collect</code> function only works if your data fits into the memory of a single host (where the driver runs), and even in that case, it adds to a bottleneck where everything has to come back to a single machine.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip36"></a>Tip</h3><p>The <code class="literal">collect()</code> function brings all of the data to the machine that runs the code so beware of accidentally using <code class="literal">collect()</code> on a large RDD!</p></div><p>The <code class="literal">split()</code> and <code class="literal">toDouble()</code> functions don't always work out so well for more complex CSV files. The <code class="literal">opencsv</code> library is a versatile library for Java and Scala. For Python, the CSV library does the trick. Let's use the <code class="literal">opencsv</code> library to parse the CSV files in Scala.</p><p>Here's the code for Scala:</p><pre class="programlisting">import au.com.bytecode.opencsv.CSVReader
import java.io.StringReader

val inFile = sc.textFile("/Users/ksankar//fdps-v3/data/Line_of_numbers.csv")
val splitLines = inFile.map(line =&gt; {
  val reader = new CSVReader(new StringReader(line))
  reader.readNext()
})
val numericData = splitLines.map(line =&gt; line.map(_.toDouble))
val summedData = numericData.map(row =&gt; row.sum)
println(summedData.collect().mkString(","))
766.0
</pre><p>While loading text files into Spark is certainly easy, having text files on local disks is often not the most convenient format to store large chunks of data. Spark supports loading from all the different Hadoop formats (sequence files, regular text files, and so on) and from all the supported Hadoop storage sources (HDFS, S3, HBase, and so on). You can also load your CSV file into HBase using some of their bulk-loading tools (such as <code class="literal">import TSV</code>) and get your CSV data.</p><p>Sequence files are binary flat files that consist of key-value pairs; they are one of the common ways of storing data for use with Hadoop. Loading a sequence file into Spark is similar to loading a text file, but you also need to let it know about the types of keys and values. The types must either be subclasses of Hadoop's <code class="literal">Writable</code> class or be implicitly convertible into such a type. For Scala users, some natives are convertible through implicits in <code class="literal">WritableConverter</code>. As of Version 2.0.0, the standard <code class="literal">WritableConverter</code> types are integer, long, double, float, Boolean, byte array, and string. Let's illustrate by looking at the process of loading a sequence file of string to an integer, as shown here:</p><p>For Scala, refer to the following code:</p><pre class="programlisting">val data = sc.sequenceFile[String, Int](inputFile)
</pre><p>For Java, refer to the following code:</p><pre class="programlisting">JavaPairRDD&lt;Text, IntWritable&gt; dataRDD = sc.sequenceFile(file, Text.class, IntWritable.class);
JavaPairRDD&lt;String, Integer&gt; cleanData = dataRDD.map(new PairFunction&lt;Tuple2&lt;Text, IntWritable&gt;, String, Integer&gt;() {
  @Override
  public Tuple2&lt;String, Integer&gt; call(Tuple2&lt;Text, IntWritable&gt; pair) {
    return new Tuple2&lt;String, Integer&gt;(pair._1().toString(),
    pair._2().get());
  }
});
</pre><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip37"></a>Tip</h3><p>Note that in the preceding cases, like with text input, the file need not be a traditional file; it can reside on S3, HDFS, and so on. Also note that for Java, you can't rely on implicit conversions between types.</p></div><p>HBase is a Hadoop-based database designed to support random read/write access to entries. Loading data from HBase is a bit different from text files and sequence in files with respect to how we tell Spark what types to use for the data.</p><p>For Scala, refer to the following code:</p><pre class="programlisting">import spark._
import org.apache.hadoop.hbase.{HBaseConfiguration, HTableDescriptor}
import org.apache.hadoop.hbase.client.HBaseAdmin
import org.apache.hadoop.hbase.mapreduce.TableInputFormat
....
val conf = HBaseConfiguration.create()
conf.set(TableInputFormat.INPUT_TABLE, input_table)
 // Initialize hBase table if necessary
val admin = new HBaseAdmin(conf)
if(!admin.isTableAvailable(input_table)) {
  val tableDesc = new HTableDescriptor(input_table)
  admin.createTable(tableDesc)
}
val hBaseRDD = sc.newAPIHadoopRDD(conf, classOf[TableInputFormat],      classOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable],          classOf[org.apache.hadoop.hbase.client.Result])
</pre><p>For Java, refer to the following code:</p><pre class="programlisting">import spark.api.java.JavaPairRDD;
import spark.api.java.JavaSparkContext;
import spark.api.java.function.FlatMapFunction;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.HTableDescriptor;
import org.apache.hadoop.hbase.client.HBaseAdmin;
import org.apache.hadoop.hbase.mapreduce.TableInputFormat;
import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
import org.apache.hadoop.hbase.client.Result;
...
JavaSparkContext sc = new JavaSparkContext(args[0], "sequence load", System.getenv("SPARK_HOME"), System.getenv("JARS"));
Configuration conf = HBaseConfiguration.create();
conf.set(TableInputFormat.INPUT_TABLE, args[1]);
// Initialize hBase table if necessary
HBaseAdmin admin = new HBaseAdmin(conf);
if(!admin.isTableAvailable(args[1])) {
  HTableDescriptor tableDesc = new HTableDescriptor(args[1]);
  admin.createTable(tableDesc);
}
JavaPairRDD&lt;ImmutableBytesWritable, Result&gt; hBaseRDD = sc.newAPIHadoopRDD( conf, TableInputFormat.class, ImmutableBytesWritable.class, Result.class);
</pre><p>The method that you used to load the HBase data can be generalized for loading all other sorts of Hadoop data. If a <code class="literal">helper</code> method in <code class="literal">SparkContext</code> does not already exist for loading the data, simply create a configuration specifying how to load the data and pass it into a <code class="literal">newAPIHadoopRDD</code> function. Different <code class="literal">helper</code> methods exist for plain text files and sequence files. A <code class="literal">helper</code> method also exists for Hadoop files similar to the sequence file API.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec43"></a>Saving your data</h2></div></div><hr /></div><p>While distributed computational jobs are a lot of fun, they are much more useful when the results are stored in a useful place. While the methods for loading an RDD are largely found in the <code class="literal">SparkContext</code> class, the methods for saving an RDD are defined on the RDD classes. In Scala, implicit conversions exist so that an RDD, which can be saved as a sequence file, could be converted to the appropriate type; in Java, explicit conversions must be used.</p><p>Here are the different ways to save an RDD.</p><p>Here's the code for Scala:</p><pre class="programlisting">rddOfStrings.saveAsTextFile("out.txt")
keyValueRdd.saveAsObjectFile("sequenceOut")
</pre><p>Here's the code for Java:</p><pre class="programlisting">rddOfStrings.saveAsTextFile("out.txt")
keyValueRdd.saveAsObjectFile("sequenceOut")
</pre><p>Here's the code for Python:</p><pre class="programlisting">rddOfStrings.saveAsTextFile("out.txt")
</pre><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip38"></a>Tip</h3><p>In addition, users can save the RDD as a compressed text file using the following function:
<code class="literal">saveAsTextFile(path: String, codec: Class[_ &lt;: CompressionCodec]) </code>
</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec44"></a>References</h2></div></div><hr /></div><p>The references are listed here:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<a class="ulink" href="http://spark-project.org/docs/latest/scala-programming-guide.html#hadoop-datasets" target="_blank">http://spark-project.org/docs/latest/scala-programming-guide.html#hadoop-datasets</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://opencsv.sourceforge.net/" target="_blank">http://opencsv.sourceforge.net/</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://commons.apache.org/proper/commons-csv/" target="_blank">http://commons.apache.org/proper/commons-csv/</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/SequenceFileInputFormat.html" target="_blank">http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/SequenceFileInputFormat.html</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/InputFormat.html" target="_blank">http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/InputFormat.html</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/" target="_blank">http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://spark.apache.org/docs/latest/api/python/" target="_blank">http://spark.apache.org/docs/latest/api/python/</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://wiki.apache.org/hadoop/SequenceFile" target="_blank">http://wiki.apache.org/hadoop/SequenceFile</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://hbase.apache.org/book/quickstart.html" target="_blank">http://hbase.apache.org/book/quickstart.html</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/TableInputFormat.html" target="_blank">http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/TableInputFormat.html</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="https://spark.apache.org/docs/latest/api/java/org/apache/spark/api/java/JavaPairRDD.html" target="_blank">https://spark.apache.org/docs/latest/api/java/org/apache/spark/api/java/JavaPairRDD.html</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="https://bzhangusc.wordpress.com/2014/06/18/csv-parser/" target="_blank">https://bzhangusc.wordpress.com/2014/06/18/csv-parser/</a>
</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec45"></a>Summary</h2></div></div><hr /></div><p>In this chapter, first you got an insight into Spark abstractions, data modalities, and how different data types can be read into a Spark environment. Then, you saw how to load data from a variety of different sources. We also looked at the basic parsing of data from text input files. Now that we can get our data loaded into a Spark RDD, it is time to explore the different operations we can perform on our data in the next chapter.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch06"></a>Chapter 6. Manipulating Your RDD</h2></div></div></div><p>The last few chapters have been the necessary groundwork to get Spark working. Now that you know how to load and save data in different ways, it's time for the big payoff, that is, manipulating data. The API you'll use to manipulate your RDD is similar among languages but not identical. Unlike the previous chapters, each language is covered in its own section here; should you wish, you could only read the one pertaining to the language you are interested in using.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec46"></a>Manipulating your RDD in Scala and Java</h2></div></div><hr /></div><p>RDD is the primary low-level abstraction in Spark. As we discussed in the last chapter, the main programming APIs will be Datasets/DataFrames. However, underneath it all, the data will be represented as RDDs. So, understanding and working with RDDs is important. From a structural view, RDDs are just a bunch of elements-elements that can be operated in parallel.</p><p>RDD stands for Resilient Distributed Dataset, that is, it is distributed over a set of machines and the transformations are captured so that an RDD can be recreated in case there is a machine failure or memory corruption. One important aspect of the distributed parallel data representation scheme is that RDDs are immutable, which means when you do an operation, it generates a new RDD. Manipulating your RDD in Scala is quite simple, especially if you are familiar with Scala's collection library. Many of the standard functions are available directly on Spark's RDDs with the primary catch being that they are immutable.</p><p>RDDs are created either by parallelizing a collection or by reading data from external data sources. Manipulating your RDD in Java is fairly simple but a little more awkward at times than it is in Scala. There are a couple of reasons for this. The main reason has to do with <span class="strong"><strong>type inference</strong></span> and also with the fact that Java doesn't have anonymous functions. In the following code snippets, sometimes the Java code is more unwieldy because Java lacks type inference and anonymous functions. Java 8 has <code class="literal">lambda</code>, which would make Java a lot more elegant with Spark. Secondly, as Java doesn't have implicit conversions, we have to be more explicit with our types. While the return types are Java-friendly, Spark requires the use of Scala's Tuple2 class for key-value pairs.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip39"></a>Tip</h3><p>In this book, we are using the Java 7 language and will not use any of the new Java 8 features.</p></div><p>The hallmark of a <code class="literal">MapReduce</code> system is this: <code class="literal">map</code> and <code class="literal">reduce</code>, the two primitives. We've seen the <code class="literal">map</code> function used in the earlier chapters. The <code class="literal">map</code> function works by taking in a function, which acts on each individual element in the input RDD and produces a new output element. For example, to produce a new RDD where you want to add one to every number, use <code class="literal">rdd.map(x =&gt; x+1)</code>.</p><p>Alternatively, in Java, you can use this code:</p><pre class="programlisting">rdd.map(new Function&lt;Integer, Integer&gt;() { public Integer    call(Integer x) { return x+1;} });
</pre><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note40"></a>Note</h3><p>There are two types of the <code class="literal">map</code> function: <code class="literal">map</code> and <code class="literal">flatMap</code>. You can easily get confused between them. The <code class="literal">map</code> function takes an element and returns another element. The element could be a single entity, a tuple, or a list; nevertheless, there is a one-to-one correspondence with the <code class="literal">map</code> function. The <code class="literal">flatMap</code> function, on the other hand, takes one element and returns zero or one or more elements. Actually, the <code class="literal">map</code> function in Hadoop, <code class="literal">MapReduce</code>, is <code class="literal">flatMap</code>. In fact, the Spark word count example is implemented using the <code class="literal">flatMap()</code>, <code class="literal">map()</code>, and <code class="literal">reduceByKey()</code> functions.</p></div><p>It is important to understand that the <code class="literal">map</code> function and the other Spark functions do not modify/update the existing elements; instead, they return a new RDD with new elements-RDDs are immutable. The <code class="literal">reduce</code> function takes a function that operates in pairs to combine all of the data. The <code class="literal">reduce</code> function you provide needs to be commutative and associative (that is, <code class="literal">f(a,b) == f(b,a)</code> and <code class="literal">f(a,f(b,c)) == f(f(a,b),c</code>). For example, to sum all the elements, you need to use <code class="literal">rdd.reduce(x,y =&gt; x+y)</code> or <code class="literal">rdd.reduce(new Function2&lt;Integer, Integer&gt;(){ public Integer call(Integer x, Integer y) { return x+y;} }</code>.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note41"></a>Note</h3><p>All the functions are not commutative. For example, while multiplication is commutative (<span class="emphasis"><em>2*3 = 3*2</em></span>), subtraction is not, that is, <span class="emphasis"><em>3-2</em></span> is not the same as <span class="emphasis"><em>2-3</em></span>; this is true for division as well, that is, <span class="emphasis"><em>4/2</em></span> is not the same as <span class="emphasis"><em>2/4</em></span>. The same applies for associativity; sum is associative, that is, <span class="emphasis"><em>2+3+4 = (2+3)+4</em></span> or <span class="emphasis"><em>2+(3+4)</em></span>, but average is not, that is, the average of <code class="literal">(2,3,4,5,6)</code> is not equal to <span class="emphasis"><em>average (2,3) + average (4,5,6)</em></span>.</p></div><p>The <code class="literal">flatMap</code> function is a useful utility function that lets you write a function. This function returns an iterable of the type you want and then flattens the results. A simple example of this is a case where you want to parse all of the data, but some of it might fail to do so. The <code class="literal">flatMap</code> function can be used to output an empty list if it fails or a list with success if it works. Another example when the output collection has a different size than the input collection can be observed when you parse a document and split it in words; here every line may contain one or more words.</p><p>In addition to the <code class="literal">reduce</code> function, there is a corresponding <code class="literal">reduceByKey</code> function that works on the <code class="literal">PairRDD</code> classes, which are key-value pairs to produce another RDD. Unlike when you're using map on a list in Scala, your function will run on a number of different machines, and so you can't depend on the shared state with this.</p><p>Before moving on to other wonderful functions that are available for manipulating your RDD, you need to read a bit about shared states. In the example given earlier, where we added one to every integer, we didn't really share states. However, for simpler tasks, such as distributed parsing of data-which we did when loading the CSV file-it can be quite handy to share counters for things such as keeping track of the number of rejected records.</p><p>Spark supports two types of shared immutable data, which it calls <span class="strong"><strong>broadcast</strong></span> and <span class="strong"><strong>accumulator</strong></span> (via accumulators). To be precise, broadcast is immutable but accumulator is mutable (albeit it can only be added to):</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>You can create a new broadcast by calling <code class="literal">sc.broadcast(value)</code>. You don't have to explicitly broadcast values as Spark does its magic in the background. Broadcasting ensures that the value is sent to each node only once. Broadcasts are often used for things such as side inputs (for example, a hashmap that you need to look up as part of the <code class="literal">map</code> function). This returns an object that can be used to reference the broadcast value.</p></li><li style="list-style-type: disc"><p>Another method for sharing states is the use of an accumulator. To create an accumulator, use <code class="literal">sc.accumulator(initialvalue)</code>. This returns an object you can add to in a distributed context and then get back the value by calling <code class="literal">.value()</code>. The <code class="literal">accumulableCollection</code> instance can be used to create a collection that is appended in a distributed fashion; however, if you find yourself using this, ask yourself whether you could use the results of a map output instead. If the predefined accumulators don't work for your use case, you can use <code class="literal">accumulable</code> to define your own accumulation type. A broadcast value can be read by all the workers, but an accumulator can be written by all the workers but read only by the driver. Consider this example: The workers add <code class="literal">1</code> to the error count whenever they encounter a bad record. Now if you want a count of the records with errors, the driver will just need to check the error value, which will be the bad records from all the workers.</p></li></ul></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip42"></a>Tip</h3><p>If you are writing the Scala code that interacts with a Java Spark process (say, for testing), you may find it useful to use the <code class="literal">int</code> accumulator and similar others in the Java Spark context; otherwise, your accumulator types might not quite match up.</p><p>If you find that your accumulator isn't increasing in value like you expect, remember that Spark follows the principle of lazy evaluation. This means that Spark won't actually perform the maps, reductions, or other computations on RDDs until the data has to be output by an action.</p></div><p>Look at the previous example, where we parsed CSV files; let's make it a bit more robust. In your previous work, you had assumed that the input was well-formatted, and if any error were to occur, your entire pipeline would fail. While this can be the correct behavior for some kind of work, we may want to accept a number of malformed records while dealing with data from third parties. On the other hand, we don't want to just throw out all the records and declare it a success; we might miss an important format change and produce meaningless results. Consider the following code (<code class="literal">LoadCsvWithCountersExample.scala</code>):</p><pre class="programlisting">import org.apache.spark.SparkConf;
import org.apache.spark.SparkContext;
import org.apache.spark.SparkFiles;
import org.apache.spark.api.java.JavaSparkContext;
import au.com.bytecode.opencsv.CSVReader;
import java.io.StringReader;

object LoadCsvWithCountersExample {
  def main(args: Array[String]) {
    val sc = new SparkContext("local","Chapter 6")
    println(s"Running Spark Version ${sc.version}")
    val invalidLineCounter = sc.accumulator(0);
    val invalidNumericLineCounter = sc.accumulator(0);
    val inFile = sc.textFile("/Volumes/sdxc-01/fdps-vii/data/
    Line_of_numbers.csv");
    val splitLines = inFile.flatMap(line =&gt; {
      try {
        val reader = new CSVReader(new StringReader(line))
        Some(reader.readNext())
      } catch {
        case _ =&gt; {
          invalidLineCounter += 1
          None
        }
      }
      })
      val numericData = splitLines.flatMap(line =&gt; {
        try {
          Some(line.map(_.toDouble))
        } catch {
        case _ =&gt; {
          invalidNumericLineCounter += 1
          None
        }
      }
    })
    val summedData = numericData.map(row =&gt; row.sum)
    println(summedData.collect().mkString(","))
    println("Errors: "+invalidLineCounter+","
    +invalidNumericLineCounter)
  }
}
</pre><p>You can run the code. It'll provide you with the following result:</p><pre class="programlisting">
<span class="strong"><strong>Using Spark's default log4j profile: org/apache/spark/log4j-
    defaults.properties</strong></span>
<span class="strong"><strong>16/08/15 16:18:13 INFO SparkContext: Running Spark version 2.0.0</strong></span>
<span class="strong"><strong>[..]</strong></span>
<span class="strong"><strong>Running Spark Version 2.0.0</strong></span>
<span class="strong"><strong>[..]</strong></span>
<span class="strong"><strong>16/08/15 16:18:16 INFO DAGScheduler: Job 0 finished: collect at
    LoadCsvWithCountersExample.scala:37, took 0.238827 s</strong></span>
<span class="strong"><strong>766.0</strong></span>
<span class="strong"><strong>Errors: 0,0</strong></span>
</pre><p>Alternatively, in Java, you can use the following code (<code class="literal">LoadCsvWithCountersJavaExample.java</code>):</p><pre class="programlisting">import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.FlatMapFunction;
import org.apache.spark.Accumulator;

import au.com.bytecode.opencsv.CSVReader;

import java.io.StringReader;
import java.util.List;
import java.util.ArrayList;

public class LoadCsvWithCountersJavaExample {
  public static void main(String[] args) throws Exception {
    SparkConf conf = new SparkConf().setAppName("Chapter
    06").setMaster("local");
    JavaSparkContext sc = new JavaSparkContext(conf);
    final Accumulator&lt;Integer&gt; errors = sc.accumulator(0);
    JavaRDD&lt;String&gt; inFile = sc.textFile("/Volumes/sdxc-01/
    fdps-vii/data/Line_of_numbers.csv");
    JavaRDD&lt;Integer[] &gt; splitLines = inFile.flatMap(new
    FlatMapFunction&lt;String, Integer[]&gt; (){
      public Iterable&lt;Integer[]&gt; call(String line) {
        ArrayList&lt;Integer[]&gt; result = new ArrayList&lt;Integer[]&gt;();
        try {
          CSVReader reader = new CSVReader(new StringReader
          (line));
          String[] parsedLine = reader.readNext();
          Integer[] intLine = new Integer[parsedLine.length];
          for (int i = 0; i &lt; parsedLine.length; i++) {
            intLine[i] = Integer.parseInt(parsedLine[i]);
          }
          result.add(intLine);
        } catch (Exception e) {
          errors.add(1);
        }
        return result;
      }
    }
    );
    List &lt;Integer[]&gt; res = splitLines.collect();
    System.out.print("Loaded data ");
    Integer sum = 0;
    for (Integer[] e : res) {
      for (Integer val:e) {
        System.out.print(val+" ");
        sum += val;
      }
      System.out.println();
    }
    System.out.println("Sum = "+sum);
    System.out.println("Error count "+errors.value());
  }
}
</pre><p>You can run the code with parameters, and it will provide you with the following result:</p><pre class="programlisting">
<span class="strong"><strong>16/01/11 20:32:43 INFO SparkContext: Running Spark version 2.0.0 </strong></span>
<span class="strong"><strong>[..]</strong></span>
<span class="strong"><strong>16/01/11 20:32:46 INFO DAGScheduler: Job 0 finished: collect at
    LoadCsvWithCountersJavaExample.java:39, took 0.240100 s</strong></span>
<span class="strong"><strong>Loaded data 42 42 55 61 53 49 43 47 49 60 68 54 34 35 35 39 </strong></span>
<span class="strong"><strong>Sum = 766</strong></span>
<span class="strong"><strong>Error count 0</strong></span>
</pre><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip43"></a>Tip</h3><p>The preceding code example illustrates the usefulness of <code class="literal">flatMap</code>. In general, <code class="literal">flatMap</code> can be used when the required output collection is of a different size than that of the input collection. You can do this because in general, there are nested collections or types involved, which need to be flattened. Because the options in Scala can be used as sequences through an implicit conversion, you can avoid having to explicitly filter out the <code class="literal">None</code> result and just use <code class="literal">flatMap</code>.</p></div><p>Summary statistics can be quite useful when examining large Datasets. In the preceding example, you loaded the data as <code class="literal">Doubles</code> to use Spark's existing summary statistics capabilities on the RDD. In Java, this requires explicitly using the <code class="literal">JavaDoubleRDD</code> type. For Java, it is important to use <code class="literal">DoubleFunction&lt;Integer[]&gt;</code> rather than <code class="literal">Function&lt;Integer[], Double&gt;</code> in the example, as the second option won't result in the <code class="literal">JavaDoubleRDD</code> type. No such consideration is required for Scala as implicit conversions deal with the details. Compute the mean and the variance or compute them together with the statistics. You can extend this by adding it to the end of the preceding function to print out the summary statistics as <code class="literal">println(summedData.stats())</code>.</p><p>To do this with Java, we would do it as follows:</p><pre class="programlisting">JavaDoubleRDD summedData = splitLines.map(new    DoubleFunction&lt;Integer[]&gt;() {
  public Double call(Integer[] in) {
    Double ret = 0.;
    for (int i = 0; i &lt; in.length; i++) {
      ret += in[i];
    }
    return ret;
  }
}
);
System.out.println(summedData.stats());
</pre><p>While working with key-value pair data, it can be quite useful to group data with the same key together (for example, if the key represents a user or a sample). The <code class="literal">groupByKey</code> function provides an easy way to group data together by a key. This function is a special case of <code class="literal">combineByKey</code>. There are several functions in the <code class="literal">PairRDD</code> class that are all implemented very closely on top of <code class="literal">combineByKey</code>. If you find yourself using <code class="literal">groupByKey</code> or one of the other functions derived from <code class="literal">combineByKey</code> and immediately transforming the result, you should check to see whether there is a function better suited to the task. A common thing to do while starting out is to perform <code class="literal">groupByKey</code> and then sum the results with <code class="literal">groupByKey().map({case (x,y) =&gt; (x,y.sum)})</code>. Alternatively, in Java, you can use the following code:</p><pre class="programlisting">pairData.groupByKey().mapValues(new Function&lt;List&lt;Integer&gt;,
  Integer &gt;(){
  public Integer call(List&lt;Integer&gt; x) {
    Integer sum = 0;
    for (Integer i : x) {
      sum += i;
    }
    return sum;
  }
}
); or in python .map(lambda (x,y): (x,sum(y))).collect()
</pre><p>By using <code class="literal">reduceByKey</code>, it could be simplified to <code class="literal">reduceByKey((x,y) =&gt; x+y)</code> or in Java as follows:</p><pre class="programlisting">pairData.groupByKey().mapValues(
  new Function&lt;Iterable&lt;Integer&gt;, Integer &gt;(){
    public Integer call(Iterable&lt;Integer&gt; x) {
      Integer sum = 0; for (Integer i : x) {
        sum += i;
      }
      return sum;
    }
  }
);
</pre><p>In fact, this is much more efficient. No big shuffle is needed, as is the case for the <code class="literal">groupBy</code> function. The only thing required is an aggregation of the values, which is important.</p><p>The <code class="literal">foldByKey(zeroValue)(function)</code> function is similar to a traditional fold operation, which works per key. In a traditional fold, a list that is provided would be called with the initial value and the first element of the list, then the resulting value and the next element of the list would be the input to the next call of the fold. Doing this requires sequentially processing the entire list, so <code class="literal">foldByKey</code> behaves slightly differently. There is a handy table of the functions of the <code class="literal">PairRDD</code> classes at the end of this section.</p><p>Sometimes, you will only want to update the values of a key-value pair data structure, such as a <code class="literal">PairRDD</code> class. You've learned about <code class="literal">foldByKey</code> and how it doesn't quite work as a traditional fold. If you're a Scala developer and you require the <span class="emphasis"><em>traditional</em></span> fold behavior, you can perform the <code class="literal">groupByKey</code> function and then map a fold by the value over the resulting RDD. This is an example of a case where you only want to change the value and we don't care about the key of the RDD; therefore, examine the following code:</p><pre class="programlisting">rdd.groupByKey().mapValues(x =&gt; {x.fold(0)((a,b) =&gt; a+b)})
</pre><p>The preceding code is interesting as it combines the Spark function, <code class="literal">groupByKey</code>, with a Scala function, <code class="literal">fold()</code>. The <code class="literal">groupBy()</code> function shuffles the data so that the values are <span class="emphasis"><em>together</em></span>. The fold mentioned is a <span class="emphasis"><em>local</em></span> Scala fold, run on each node. Bear in mind that performance-wise, reduce is <code class="literal">log(n)</code>, while fold is <code class="literal">O(n)</code>.</p><p>Often your data won't come in cleanly from a single source and you will want to join the data together for processing, which can be done with <code class="literal">coGroup</code>. This can be done when you are joining web access logs with transaction data or just joining two different computations on the same data. Provided that the RDDs have the same key, we can join two RDDs together with <code class="literal">rdd.coGroup(otherRdd)</code>. There are a number of different <code class="literal">join</code> functions for different purposes illustrated in the table at the end of this section.</p><p>The next task you will learn is distributing files among the cluster. We illustrate this by adding GeoIP support and mixing it together with the gradient descent example from the earlier chapter. Sometimes, the libraries you will use would need files distributed along with them. While it is possible to add them to the JAR file and access them as class objects, Spark provides a simple way to distribute the required files by calling <code class="literal">addFile()</code>, as shown here:</p><pre class="programlisting">import scala.math

import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkFiles;
import org.apache.spark.util.Vector

import au.com.bytecode.opencsv.CSVReader

import java.util.Random
import java.io.StringReader
import java.io.File

import com.snowplowanalytics.maxmind.geoip.IpGeo

case class DataPoint(x: Vector, y: Double)

object GeoIpExample {

  def main(args: Array[String]) {
    if (args.length != 2) {
      System.err.println("Usage: GeoIpExample &lt;master&gt; &lt;inputfile&gt;")
      System.exit(1)
    }
    val master = args(0)
    val inputFile = args(1)
    val iterations = 100
    val maxMindPath = "GeoLiteCity.dat"
    val sc = new SparkContext(master, "GeoIpExample",
    System.getenv("SPARK_HOME"), Seq(System.getenv("JARS")))
    val invalidLineCounter = sc.accumulator(0)
    val inFile = sc.textFile(inputFile)
    val parsedInput = inFile.flatMap(line =&gt; {
      try {
        val row = (new CSVReader(new StringReader (line))).readNext()
        Some((row(0),row.drop(1).map(_.toDouble)))
      } catch {
        case _ =&gt; {
          invalidLineCounter += 1
          None
        }
      }
    })
    val geoFile = sc.addFile(maxMindPath)
    // getLocation gives back an option so we use flatMap to only output
       if its a some type
    val ipCountries = parsedInput.flatMapWith(_ =&gt; IpGeo(dbFile =
    SparkFiles.get(maxMindPath) ))((pair, ipGeo) =&gt; {
      ipGeo.getLocation(pair._1).map(c =&gt; (pair._1, c.countryCode)).toSeq
    })
    ipCountries.cache()
    val countries = ipCountries.values.distinct().collect()
    val countriesBc = sc.broadcast(countries)
    val countriesSignal = ipCountries.mapValues(country =&gt;
    countriesBc.value.map(s =&gt; if (country == s) 1. else 0.))
    val dataPoints = parsedInput.join(countriesSignal).map(input =&gt; {
      input._2 match {
        case (countryData, originalData) =&gt; DataPoint(new
        Vector(countryData++originalData.slice(1,originalData.size-2)) ,
        originalData(originalData.size-1))
      }
    })
    countriesSignal.cache()
    dataPoints.cache()
    val rand = new Random(53)
    var w = Vector(dataPoints.first.x.length, _ =&gt; rand.nextDouble)
    for (i &lt;- 1 to iterations) {
      val gradient = dataPoints.map(p =&gt;
      (1 / (1 + math.exp(-p.y*(w dot p.x))) - 1) * p.y * p.x).reduce(_ + _)
      w -= gradient
    }
    println("Final w: "+w)
  }
}
</pre><p>In this example, you will see multiple Spark computations. The first computation is to determine all the countries where our data is so that we can map the country to a binary feature. The code then uses a public list of proxies and the reported latency to try and estimate the latency I measured. This also illustrates the use of <code class="literal">mapWith</code> (which is now <code class="literal">MapPartitions</code>). If you have a mapping job that needs to create a per-partition resource, <code class="literal">mapWith</code> (which is now <code class="literal">MapPartitions</code>) can be used.</p><p>This can be useful for connections to the backend or the creation of something similar to a PRNG. Some elements also can't be serialized over the wire (such as the <code class="literal">IpCountry</code> instance in the example), and so you will have to create them per shard. You can also see that we cache a number of our RDDs to keep them from having to be recomputed.</p><p>There are several options when working with multiple RDDs.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec17"></a>Scala RDD functions</h3></div></div></div><p>These are the <code class="literal">PairRDD</code> functions based on <code class="literal">combineByKey</code>. All operate on the RDDs of the type <code class="literal">[K,V]</code>:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Function</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Param options</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Explanation</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Return type</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">foldByKey</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
</p><p>(zeroValue)(func(V,V)=&gt;V)</p><p>
</p><p>(zeroValue, partitioner)(func(V,V=&gt;V)</p><p>
</p><p>(zeroValue, partitions)(func(V,V=&gt;V)</p><p>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>This function merges the values using the provided function. Unlike a traditional fold over a list, the <code class="literal">zeroValue</code> function can be added an arbitrary number of times.</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>RDD[K,V]</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">reduceByKey</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
</p><p>(func(V,V)=&gt;V)</p><p>
</p><p>(func(V,V)=&gt;V,numTasks)</p><p>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>This function is the parallel version of reduce that merges the values for each key using the provided function and returns an RDD.</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>RDD[K,V]</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; ">
<p>
<code class="literal">groupByKey</code>
</p>
</td><td style="border-right: 0.5pt solid ; ">
<p>
</p><p>()</p><p>
</p><p>(numPartitions)</p><p>
</p>
</td><td style="border-right: 0.5pt solid ; ">
<p>This function groups elements together by the key.</p>
</td><td style="">
<p>RDD[K,Seq[V]]</p>
</td></tr></tbody></table></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec18"></a>Functions for joining the PairRDD classes</h3></div></div></div><p>Often, while working with two or more key-value RDDs, it is useful to join them together. There are a few different methods to do this, depending on what your desired behavior is:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Function</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Param options</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Explanation</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Return type</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">coGroup</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>(otherRdd[K,W]...)</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Join two (or more) RDDs by the shared key. Note that if an element is found missing in one RDD but present in the other, the <code class="literal">Seq</code> value will simply be empty.</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>RDD[(K,(Seq[V],Seq[W]...))]</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">join</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
</p><p>(otherRdd[K,W])</p><p>
</p><p>(otherRdd[K,W], partitioner)</p><p>
</p><p>(otherRdd[K,W], numPartitions)</p><p>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Join an RDD with another RDD. The result is only present for elements where the key is present in both RDDs.</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>RDD[(K,(V,W))]</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; ">
<p>
<code class="literal">subtractKey</code>
</p>
</td><td style="border-right: 0.5pt solid ; ">
<p>
</p><p>(otherRdd[K,W])</p><p>
</p><p>(otherRdd[K,W], partitioner)</p><p>
</p><p>(otherRdd[K,W], numPartitions)</p><p>
</p>
</td><td style="border-right: 0.5pt solid ; ">
<p>This returns an RDD with only keys not present in the other RDD.</p>
</td><td style="">
<p>RDD[(K,V)]</p>
</td></tr></tbody></table></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec19"></a>Other PairRDD functions</h3></div></div></div><p>Some functions only make sense when working on key-value pairs, as follows:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /><col /><col /></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Function</p>
</th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Param options</p>
</th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Explanation</p>
</th><th style="border-bottom: 0.5pt solid ; ">
<p>Return type</p>
</th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">lookup</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>(key: K)</strong></span></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>This function looks up a specific element in the RDD. It uses the RDD's partitioner to figure out which shard(s) to look at.</strong></span></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Seq[V]</strong></span></p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">mapValues</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>(f: V =&gt; U)</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>This function is a specialized version of the map for the <code class="literal">PairRDD</code> classes when you only want to change the value of the key-value pair. It takes the provided <code class="literal">map</code> function and applies it to the value. If you need to make your change based on both the key and the value, you must use one of the normal RDD <code class="literal">map</code> functions.</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>RDD[(K,U)]</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">collectAsMap</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>() (No arguments)</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>This function takes an RDD and returns a concrete map. Your RDD must be able to fit in the memory.</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Map[K, V]</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">countByKey</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>()</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>This function counts the number of elements for each key.</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Map[K, Long]</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">partitionBy</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>(partitioner: Partitioner, mapSideCombine: Boolean)</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>This function returns a new RDD with the same data but partitioned by the new partitioner. The Boolean flag, <code class="literal">mapSideCombine</code>, controls whether Spark should group values with the same key together before repartitioning. It defaults to <code class="literal">false</code> and sets to <code class="literal">true</code> if you have a large percentage of duplicate keys.</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>RDD[(K,V)]</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; ">
<p>
<code class="literal">flatMapValues</code>
</p>
</td><td style="border-right: 0.5pt solid ; ">
<p>(f: V =&gt; TraversableOnce[U])</p>
</td><td style="border-right: 0.5pt solid ; ">
<p>This function is similar to <code class="literal">MapValues</code>. It's a specialized version of <code class="literal">flatMap</code> for the <code class="literal">PairRDD</code> classes when you only want to change the value of the key-value pair. It takes the provided <code class="literal">map</code> function and applies it to the value. The resulting sequence is then <span class="emphasis"><em>flattened</em></span>, that is, instead of getting <code class="literal">Seq[Seq[V]]</code>, you get <code class="literal">Seq[V]</code>. If you need to make your change based on both key and value, you must use one of the normal RDD map functions.</p>
</td><td style="">
<p>RDD[(K,U)]</p>
</td></tr></tbody></table></div><p>For information on how to save the <code class="literal">PairRDD</code> classes, refer to the previous chapter.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec20"></a>Double RDD functions</h3></div></div></div><p>Spark defines a number of convenience functions that work when your RDD is comprised of doubles, as follows:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Function</strong></span></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Arguments</strong></span></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Return value</strong></span></p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">Mean</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>()</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Average</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">sampleStdev</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>()</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Standard deviation for a sample rather than a population (as it divides by <span class="emphasis"><em>N-1</em></span> rather than <span class="emphasis"><em>N</em></span>)</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">Stats</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>()</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Mean, variance, and count as <code class="literal">StatCounter</code>
</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">Stdev</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>()</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Standard deviation (for population)</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">Sum</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>()</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>The sum of elements</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; ">
<p>
<code class="literal">variance</code>
</p>
</td><td style="border-right: 0.5pt solid ; ">
<p>()</p>
</td><td style="">
<p>Variance</p>
</td></tr></tbody></table></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec21"></a>General RDD functions</h3></div></div></div><p>The remaining RDD functions are defined on all RDDs:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /><col /></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Function</p>
</th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Arguments</p>
</th><th style="border-bottom: 0.5pt solid ; ">
<p>Returns</p>
</th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">aggregate</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>(zero: U)(seqOp: (U,T) =&gt; T, combOp (U, U) =&gt; U)</strong></span></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>This function aggregates all the elements of each partition of an RDD and then combines them using <code class="literal">combOp</code>. The <code class="literal">zero</code> value should be neutral (that is, <code class="literal">0</code> for <code class="literal">+</code> and <code class="literal">1</code> for <code class="literal">*</code>).</strong></span></p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">cache</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>()</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function caches an RDD reused without being recomputed. It's the same as <code class="literal">persist(StorageLevel.MEMORY_ONLY)</code>.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">collect</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>()</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function returns an array of all the elements in the RDD.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">count</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>()</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function returns the number of elements in an RDD.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">countByValue</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>()</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function returns a map of the value to the number of times that the value occurs.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">distinct</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
</p><p>()</p><p>
</p><p>(partitions: Int)</p><p>
</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function returns an RDD that contains only distinct elements.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">filter</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>(f: T =&gt; Boolean)</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function returns an RDD that contains only elements matching <code class="literal">f</code>.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">filterWith</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>(constructA: Int =&gt; A )(f: (T, A) =&gt; Boolean)</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function is similar to <code class="literal">filter</code>, but <code class="literal">f</code> takes an additional parameter generated by <code class="literal">constructA</code>, which is called per-partition. The original motivation for this came when we provided the PRNG generation for each shard.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">first</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>()</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function returns the first element of the RDD.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">flatMap</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>(f: T =&gt; TraversableOnce[U])</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function returns an RDD of type <code class="literal">U</code>.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">fold</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>(zeroValue: T)(op: (T,T) =&gt; T)</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function merges values using the provided operation, the first operation on each partition, and then merges the merged result.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">foreach</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>(f: T =&gt; Unit)</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function applies the function <code class="literal">f</code> to each element.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">groupBy</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
</p><p>(f: T =&gt; K)</p><p>
</p><p>(f: T =&gt; K, p: Partitioner)</p><p>
</p><p>(f: T =&gt; K, numPartitions:Int)</p><p>
</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function takes in an RDD and produces a <code class="literal">PairRDD</code> class of the type <code class="literal">(K,Seq[T])</code>, using the result of <code class="literal">f</code> for the key and for each element.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">keyBy</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
</p><p>(f: T =&gt; K)</p><p>
</p><p>(f: T =&gt; K, p: Partitioner)</p><p>
</p><p>(f: T =&gt; K, numPartitions:Int)</p><p>
</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function is the same as <code class="literal">groupBy</code> but does not group results together with duplicate keys. It returns an RDD of <code class="literal">(K,T)</code>.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">map</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>(f: T =&gt; U)</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function returns an RDD of the result of applying <code class="literal">f</code> to every element in the input RDD.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">mapPartitions</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>(f: Iterator[T] =&gt; Iterator[U])</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function is similar to <code class="literal">map</code> except that the provided function takes and returns an iterator and is applied to each partition.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">mapPartitionsWithIndex</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>(f: (Int, Iterator[T]) =&gt; Iterator[U], preservePartitions)</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function is the same as <code class="literal">mapPartitions</code> but also provides the index of the original partition.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">mapWith</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>(constructA: Int =&gt; A)(f: (T, A) =&gt; U)</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function is similar to <code class="literal">map</code>, but <code class="literal">f</code> takes an additional parameter generated by <code class="literal">constructA</code>, which is called per-partition. The original motivation for this came when we provided the PRNG generation for each shard.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">persist</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
</p><p>()</p><p>
</p><p>(newLevel: StorageLevel)</p><p>
</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function sets the RDD storage level, which can cause the RDD to be stored after it is computed. The different <code class="literal">StorageLevel</code> values can be seen in <code class="literal">StorageLevel.scala</code> (<code class="literal">NONE</code>, <code class="literal">DISK_ONLY</code>, <code class="literal">MEMORY_ONLY</code>, and <code class="literal">MEMORY_AND_DISK</code> are the common ones).</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">pipe</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
</p><p>(command: Seq[String])</p><p>
</p><p>(command: Seq[String], env: Map[String, String])</p><p>
</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function takes an RDD and calls the specified command with the optional environment. Then, it pipes each element through the command. This function results in an RDD of type string.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">sample</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>(withReplacement: Boolean, fraction: Double, seed: Int)</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function returns an RDD of that fraction.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">takeSample</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>(withReplacement: Boolean, num: Int, seed: Int)</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function returns an array of the requested number of elements. It works by oversampling the RDD and then grabbing a subset.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">toDebugString</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>()</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This is a handy function that outputs the recursive deps of the RDD.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">union</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>(other: RDD[T])</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function is an RDD containing the elements of both the RDDs. Here, duplicates are not removed.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">unpersist</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>()</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function removes all the blocks of the RDD from the memory/disk if they've persisted.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; ">
<p>
<code class="literal">zip</code>
</p>
</td><td style="border-right: 0.5pt solid ; ">
<p>(other: RDD[U])</p>
</td><td style="">
<p>This function is important to note as it requires that the RDDs have the same number of partitions and the same size of each partition. It returns an RDD of key-value pairs, <code class="literal">RDD[T,U]</code>.</p>
</td></tr></tbody></table></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec22"></a>Java RDD functions</h3></div></div></div><p>Many of the Java RDD functions are quite similar to the Scala RDD functions, but the type signatures are somewhat different.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec1"></a>Spark Java function classes</h4></div></div></div><p>For the Java RDD API, we need to extend one of the provided function classes while implementing our function:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /><col /></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Name</p>
</th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Params</p>
</th><th style="border-bottom: 0.5pt solid ; ">
<p>Purpose</p>
</th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Function&lt;T,R&gt;</strong></span></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>R call(T t)</strong></span></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>This function takes something of the type <code class="literal">T</code> and returns something of the type <code class="literal">R</code>. It is commonly used for maps.</strong></span></p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>DoubleFunction&lt;T&gt;</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Double call(T t)</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function is the same as <code class="literal">Function&lt;T, Double&gt;</code>, but the result of the map-like call returns <code class="literal">JavaDoubleRDD</code> (for summary statistics).</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>PairFunction&lt;T, K, V&gt;</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Tuple2&lt;K, V&gt; call(T t)</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function results in a <code class="literal">JavaPairRDD</code> class. If you're working on <code class="literal">JavaPairRDD&lt;A,B&gt;</code>, have <code class="literal">T</code> of the type <code class="literal">Tuple2&lt;A,B&gt;</code>.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>FlatMapFunction&lt;T, R&gt;</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Iterable&lt;R&gt; call(T t)</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function is used for producing an RDD through <code class="literal">flatMap</code>.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>PairFlatMapFunction&lt;T, K, V&gt;</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Iterable&lt;Tuple2&lt;K, V&gt;&gt; call(T t)</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function results in a <code class="literal">JavaPairRDD</code> class. If you're working on <code class="literal">JavaPairRDD&lt;A,B&gt;</code>, have <code class="literal">T</code> of the type <code class="literal">Tuple2&lt;A,B&gt;</code>.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>DoubleFlatMapFunction&lt;T&gt;</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Iterable&lt;Double&gt; call(T t)</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function is the same as <code class="literal">FlatMapFunction&lt;T, Double&gt;</code>, but the result of the map-like call returns <code class="literal">JavaDoubleRDD</code> (for summary statistics).</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; ">
<p>Function2&lt;T1, T2, R&gt;</p>
</td><td style="border-right: 0.5pt solid ; ">
<p>R call(T1 t1, T2 t2)</p>
</td><td style="">
<p>This function is for taking two input actions and returning an output. It is used by fold and similar.</p>
</td></tr></tbody></table></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec2"></a>Common Java RDD functions</h4></div></div></div><p>These RDD functions are available regardless of the type of RDD:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /><col /></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Name</p>
</th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Params</p>
</th><th style="border-bottom: 0.5pt solid ; ">
<p>Purpose</p>
</th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">cache</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>()</strong></span></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>This function makes an RDD persist in memory.</strong></span></p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">coalesce</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>numPartitions: Int</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function returns a new RDD with the <code class="literal">numPartitions</code> partitions.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">collect</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>()</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function returns the <code class="literal">List</code> representation of the entire RDD.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">count</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>()</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function returns the number of elements.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">countByValue</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>()</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function returns a map of each unique value to the number of times that the value shows up.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">distinct</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
</p><p>()</p><p>
</p><p>(Int numPartitions)</p><p>
</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function is an RDD consisting of all the distinct elements of the RDD optionally in the provided number of partitions.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">filter</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>(Function&lt;T, Boolean&gt; f)</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function is an RDD, consisting only of the elements for which the provided function returns <code class="literal">true</code>.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">first</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>()</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function is the first element of the RDD.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">flatMap</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
</p><p>(FlatMapFunction&lt;T, U&gt; f)</p><p>
</p><p>(DoubleFlatMapFunction&lt;T&gt; f)</p><p>
</p><p>(PairFlatMapFunction&lt;T, K, V&gt; f)</p><p>
</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function is an RDD of the specified types (<code class="literal">U</code>, <code class="literal">Double</code>, and <code class="literal">Pair&lt;K,V&gt;</code> respectively).</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">fold</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>(T zeroValue, Function2&lt;T, T, T&gt; f)</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function returns the result <code class="literal">T</code>. Each partition is folded individually with the zero value and then the results are folded.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">foreach</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>(VoidFunction&lt;T&gt; f)</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function applies the function to each element in the RDD.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">groupBy</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
</p><p>(Function&lt;T, K&gt; f)</p><p>
</p><p>(Function&lt;T, K&gt; f, Int numPartitions)</p><p>
</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function returns a <code class="literal">JavaPairRDD</code> class of grouped elements.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">map</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
</p><p>(DoubleFunction&lt;T&gt; f)</p><p>
</p><p>(PairFunction&lt;T, K2, V2&gt; f)</p><p>
</p><p>(Function&lt;T, U&gt; f)</p><p>
</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function returns an RDD of an appropriate type for the input function (see the previous table) by calling the provided function on each element in the input RDD.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">mapPartitions</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
</p><p>(DoubleFunction&lt;Iterator&lt;T&gt;&gt; f)</p><p>
</p><p>(PairFunction&lt;Iterator&lt;T&gt;, K2, V2&gt; f)</p><p>
</p><p>(Function&lt;Iterator&lt;T&gt;, U&gt; f)</p><p>
</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function is similar to <code class="literal">map</code>, but the provided function is called per-partition. This can be useful if you have done some setup work that is necessary for each partition.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">reduce</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>(Function2&lt;T, T, T&gt; f)</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function uses the provided function to reduce down all the elements.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; ">
<p>
<code class="literal">sample</code>
</p>
</td><td style="border-right: 0.5pt solid ; ">
<p>(Boolean withReplacement, Double fraction, Int seed)</p>
</td><td style="">
<p>This function returns a smaller RDD, consisting of only the requested fraction of the data.</p>
</td></tr></tbody></table></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec3"></a>Methods for combining JavaRDDs</h4></div></div></div><p>There are a number of different functions that we can use to combine RDDs:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">subtract</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
</p><p>(JavaRDD&lt;T&gt; other)</p><p>
</p><p>(JavaRDD&lt;T&gt; other, Partitioner p)</p><p>
</p><p>(JavaRDD&lt;T&gt; other, Int numPartitions)</p><p>
</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function returns an RDD with only the elements initially present in the first RDD and not present in the other RDD.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">union</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>(JavaRDD&lt;T&gt; other)</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function is the union of the two RDDs.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; ">
<p>
<code class="literal">zip</code>
</p>
</td><td style="border-right: 0.5pt solid ; ">
<p>(JavaRDD&lt;U&gt; other)</p>
</td><td style="">
<p>
</p><p>This function returns an RDD of key-value pairs <code class="literal">RDD[T,U]</code>.</p><p>
</p><p>It is important to note that it requires that the RDDs should have the same number of partitions and the size of each partition.</p><p>
</p>
</td></tr></tbody></table></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec4"></a>Functions on JavaPairRDDs</h4></div></div></div><p>Some functions are only defined on the key-value <code class="literal">PairRDD</code> classes:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /><col /></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Name</p>
</th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Params</p>
</th><th style="border-bottom: 0.5pt solid ; ">
<p>Purpose</p>
</th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">cogroup</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
</p><p>(JavaPairRDD&lt;K, W&gt; other)</p><p>
</p><p>(JavaPairRDD&lt;K, W&gt; other, Int numPartitions)</p><p>
</p><p>(JavaPairRDD&lt;K, W&gt; other1, JavaPairRDD&lt;K, W&gt; other2)</p><p>
</p><p>(JavaPairRDD&lt;K, W&gt; other1, JavaPairRDD&lt;K, W&gt; other2, Int numPartitions)</p><p>
</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>This function joins two (or more) RDDs by the shared key. Note that if an element is missing in one RDD but present in the other one, the list will simply be empty.</strong></span></p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">combineByKey</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
</p><p>(Function&lt;V, C&gt; createCombiner</p><p>
</p><p>Function2&lt;C, V, C&gt; mergeValue,</p><p>
</p><p>Function2&lt;C,C,C&gt; mergeCombiners)</p><p>
</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function is a generic function to combine elements by key. The <code class="literal">createCombiner</code> function turns something of the type <code class="literal">V</code> into something of the type <code class="literal">C</code>. The <code class="literal">mergeValue</code> function adds <code class="literal">V</code> to <code class="literal">C</code>, and <code class="literal">mergeCombiners</code> is used to combine two <code class="literal">C</code> values into a single <code class="literal">C</code> value.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">collectAsMap</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>()</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function returns a map of the key-value pairs.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">countByKey</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>()</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function returns a map of the key to the number of elements with that key.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">flatMapValues</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>(Function[T] f, Iterable[V] v)</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function returns an RDD of the type <code class="literal">V</code>.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">join</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
</p><p>(JavaPairRDD&lt;K, W&gt; other)</p><p>
</p><p>(JavaPairRDD&lt;K, W&gt; other, Int integers)</p><p>
</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function joins an RDD with another RDD. The result is only present for elements where the key is present in both the RDDs.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">keys</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>()</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function returns an RDD of only the keys.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">lookup</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>(Key k)</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function looks up a specific element in the RDD. It uses the RDD's partitioner to figure out which shard(s) to look at.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">reduceByKey</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>(Function2[V,V,V] f)</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>The <code class="literal">reduceByKey</code> function is the parallel version of reduce that merges the values for each key using the provided function and returns an RDD.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">sortByKey</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
</p><p>(Comparator[K] comp, Boolean ascending)</p><p>
</p><p>(Comparator[K] comp)</p><p>
</p><p>(Boolean ascending)</p><p>
</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function sorts the RDD by key; therefore, each partition contains a fixed range.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; ">
<p>
<code class="literal">values</code>
</p>
</td><td style="border-right: 0.5pt solid ; ">
<p>()</p>
</td><td style="">
<p>This function returns an RDD of only the values.</p>
</td></tr></tbody></table></div></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec47"></a>Manipulating your RDD in Python</h2></div></div><hr /></div><p>Spark has a more limited Python API than Java and Scala, but it supports most of the core functionality.</p><p>The hallmark of a <code class="literal">MapReduce</code> system lies in two commands: <code class="literal">map</code> and <code class="literal">reduce</code>. You've seen the <code class="literal">map</code> function used in the earlier chapters. The <code class="literal">map</code> function works by taking in a function that works on each individual element in the input RDD and produces a new output element. For example, to produce a new RDD where you have added one to every number, you would use <code class="literal">rdd.map(lambda x: x+1)</code>. It's important to understand that the <code class="literal">map</code> function and the other Spark functions do not transform the existing elements; instead, they return a new RDD with new elements. The <code class="literal">reduce</code> function takes a function that operates in pairs to combine all of the data. This is returned to the calling program. If you were to sum all the elements, you would use <code class="literal">rdd.reduce(lambda x, y: x+y)</code>. The <code class="literal">flatMap</code> function is a useful utility function that allows you to write a function that returns an iterable of the type you want and then flattens the results. A simple example of this is a case where you want to parse all of the data, but some of it fails to parse. The <code class="literal">flatMap</code> function outputs an empty list if it fails or a list with its success if it works. In addition to <code class="literal">reduce</code>, there is a corresponding <code class="literal">reduceByKey</code> function that works on RDDs, which are key-value pairs, and produces another RDD.</p><p>Many of the mapping operations are also defined with a partition's variant. In this case, the function you need to provide takes and returns an iterator, which represents all of the data on that partition, thus performing work on a per-partition level. The <code class="literal">mapPartitions(func)</code> function can be quite useful if the operation you need to perform has to do expensive work on each shard/partition. An example of this is establishing a connection to a backend server. Another reason for using <code class="literal">mapPartitions(func)</code> is to do setup work for your <code class="literal">map</code> function that can't be serialized across the network. A good example of this is parsing some expensive side input, as shown here:</p><pre class="programlisting">def f(iterator):
  // Expensive work goes here
  for i in iterator:
  yield per_element_function(i)
</pre><p>Often, your data can be expressed with key-value mappings. As such, many of the functions defined on Python's RDD class only work if your data is in a key-value mapping. The <code class="literal">mapValues</code> function is used when you only want to update the key-value pair you are working with.</p><p>In addition to performing simple operations on the data, Spark also provides support for broadcast values and accumulators. Broadcast values can be used to broadcast a read-only value to all the partitions, which can save the need to reserialize a given value multiple times. Accumulators allow all the shards to be added to the accumulator and the result can then be read on the master. You can create an accumulator by using <code class="literal">counter = sc.accumulator(initialValue)</code>. If you want the behavior to be customized, you can also provide <code class="literal">AccumulatorParam</code> to the accumulator. The return value can then be incremented as <code class="literal">counter += x</code> on any of the workers. The resulting value can then be read with <code class="literal">counter.value()</code>. The broadcast value is created with <code class="literal">bc = sc.broadcast(value)</code> and then accessed by <code class="literal">bc.value()</code> on any worker. The accumulator can only be read on the master, and the broadcast value can be read on all the shards.</p><p>Let's look at a quick Python example that shows multiple RDD operations. We have two text files, namely <code class="literal">2009-2014-BO.txt</code> and <code class="literal">1861-1864-AL.txt</code>. These are the <span class="emphasis"><em>State Of the Union</em></span> speeches by Presidents Barack Obama and Abraham Lincoln. We want to compare the mood of the nation by comparing the salient differences in the words used.</p><p>The first step is reading the files and creating the word frequency vector, that is, each word and the number of times it is used in the speech. I am sure you would recognize this as a canonical word count <code class="literal">MapReduce</code> example, and in traditional Hadoop <code class="literal">MapReduce</code> system, it takes around 100 lines of code. In Spark, as we shall see, it takes only five lines of code:</p><pre class="programlisting">from pyspark.context import SparkContext
print "Running Spark Version %s" % (sc.version)
from pyspark.conf import SparkConf
conf = SparkConf()
print conf.toDebugString()
</pre><p>The <code class="literal">MapReduce</code> code is shown here:</p><pre class="programlisting">from operator import add
lines = sc.textFile("sotu/2009-2014-BO.txt")
word_count_bo = lines.flatMap(lambda x: x.split(' ')).\
  map(lambda x: (x.lower().rstrip().     lstrip().rstrip(',').rstrip('.'), 1)).\
  reduceByKey(add)
word_count_bo.count()
#6658 without lower, 6299 with lower, rstrip,lstrip 4835
lines = sc.textFile("sotu/1861-1864-AL.txt")
word_count_al = lines.flatMap(lambda x: x.split(' ')).map(lambda    x: (x.lower().rstrip().lstrip().rstrip(',').rstrip('.'),      1)).reduceByKey(add)
word_count_al.count()
</pre><p>Sorting an RDD by any column is very easy, as shown next:</p><pre class="programlisting">word_count_bo_1 = word_count_bo.sortBy(lambda x:    x[1],ascending=False)
</pre><p>We can collect the word vector. However, don't print it! It is a long list:</p><pre class="programlisting">for x in word_count_bo_1.take(10):
  print x
</pre><p>Now, let's take out common words, as shown here:</p><pre class="programlisting">common_words = ["us","has","all", "they", "from", "who","what","on","by","more","as","not","their","can","new","it","but","be","are","--","i","have","this","will","for","with","is","that","in","our","we","a","of","to","and","the","that's","or","make","do","you","at","it's","than","if","know","last","about","no","just","now","an","because","&lt;p&gt;we","why","we'll","how","two","also","every","come","we've","year","over","get","take","one","them","we're","need","want","when","like","most","-","been","first","where","so","these","they're","good","would","there","should","--&gt;","&lt;!--","up","i'm","his","their","which","may","were","such","some","those","was","here","she","he","its","her","his","don't","i've","what's","didn't","shouldn't","(applause.)","let's","doesn't"]
</pre><p>Filtering out common words is also a single <code class="literal">filter</code> operation. Of course, as RDDs are immutable, we will create a new filtered RDD:</p><pre class="programlisting">word_count_bo_clean = word_count_bo_1.filter(lambda x: x[0] not in    common_words)
word_count_al_clean = word_count_al.filter(lambda x: x[0] not in    common_words)
</pre><p>Finding the words that were spoken by Obama and not by Lincoln is a single RDD operation. You need to use <code class="literal">subractByKey</code> and then use <code class="literal">sortBy</code> on the count to see the different but most frequent words, as shown here:</p><pre class="programlisting">for x in word_count_bo_clean.subtractByKey   (word_count_al_clean).sortBy(lambda x:      x[1],ascending=False).take(15): #collect():
  print x
</pre><p>The preceding program should give you a good grip on the RDD functions and how to use them in Python.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec23"></a>Standard RDD functions</h3></div></div></div><p>These functions are available on all RDDs in Python:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /><col /></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Name</p>
</th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Params</p>
</th><th style="border-bottom: 0.5pt solid ; ">
<p>Purpose</p>
</th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">flatMap</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>f, preserves Partitioning=False</strong></span></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>This function takes a function that returns an iterator of the type <code class="literal">U</code> for each input of the type <code class="literal">T</code> and returns a flattened RDD of the type <code class="literal">U</code>.</strong></span></p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">mapPartitions</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>f, preserves Partitioning=False</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function takes a function that takes in an iterator of the type <code class="literal">T</code> and returns an iterator of the type <code class="literal">U</code>, which then results in an RDD of the type <code class="literal">U</code>. It's useful for map operations with expensive per-machine setup work.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">filter</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>f</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function takes a function and returns an RDD with only the elements for which the function returns <code class="literal">true</code>.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">distinct</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>()</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function returns an RDD with distinct elements (for example, <code class="literal">1, 1, 2</code> gives the output as <code class="literal">1, 2</code>).</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">union</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>other</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function returns a union of two RDDs.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">intersection</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>other</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function returns the intersection as a set (that is, no duplicates).</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">cartesian</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>other</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function returns the Cartesian product of the RDD with the other RDD.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">groupBy</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>f, numPartitions=None</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function returns an RDD with the elements grouped together for the value that <code class="literal">f</code> outputs.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">pipe</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>command, env={}</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function pipes each element of the RDD to the provided command and returns an RDD of the result.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">foreach</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>f</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function applies the function <code class="literal">f</code> to each element in the RDD.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">reduce</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>f</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function reduces the elements using the provided function.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">fold</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>zeroValue, op</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>With this function, each partition is folded individually with the <code class="literal">zero</code> value; then, the results are folded.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">countByValue</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>()</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function returns a dictionary mapping of each distinct value to the number of times it is found in the RDD.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">take</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>num</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function returns a list of <code class="literal">num</code> elements. This can be slow for the large values of <code class="literal">num</code>; therefore, use <code class="literal">collect</code> if you want to get back to the entire RDD.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">Stats()</code>, <code class="literal">min()</code>, <code class="literal">max()</code>, <code class="literal">mean()</code>, <code class="literal">variance()</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>()</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>These are statistical functions.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; ">
<p>
<code class="literal">partitionBy</code>
</p>
</td><td style="border-right: 0.5pt solid ; ">
<p>numPartitions, partitionFunc=hash</p>
</td><td style="">
<p>This function makes a new RDD partitioned using the provided partitioning function. The <code class="literal">partitionFunc</code> function simply needs to map the input key to an integer number, and the <code class="literal">partitionBy</code> function calculates the partition by that number using <code class="literal">numPartitions</code>.</p>
</td></tr></tbody></table></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec24"></a>The PairRDD functions</h3></div></div></div><p>These functions are only available on key-value pair functions:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /><col /></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Name</p>
</th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Params</p>
</th><th style="border-bottom: 0.5pt solid ; ">
<p>Purpose</p>
</th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">collectAsMap</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>()</strong></span></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>This function returns a dictionary consisting of all the key-value pairs of the RDD.</strong></span></p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">reduceByKey</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>func, numPartitions=None</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function is the parallel version of <code class="literal">reduce</code>, which merges the values for each key using the provided function and returns an RDD.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">countByKey</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>()</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function returns a dictionary of the number of elements for each key.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">join</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>other, numPartitions=None</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function joins an RDD with another RDD. The result is only present for elements where the key is present in both RDDs. The value that gets stored for each key is a tuple of the values from each RDD.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">rightOuterJoin</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>other, numPartitions=None</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function joins an RDD with another RDD. It outputs a given key-value pair only if the key it's being joined to is present in the RDD. If the key is not present in the source RDD, the first value in the tuple will be <code class="literal">None</code>.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">leftOuterJoin</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>other, numPartitions=None</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function joins an RDD with another RDD. It outputs a given key-value pair only if the key is present in the source RDD. If the key is not present in the other RDD, the second value in the tuple will be <code class="literal">None</code>.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">combineByKey</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>createCombiner, mergeValues, mergeCombiners</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function combines elements using a key. It takes an RDD of the type <code class="literal">[K,V]</code> and returns an RDD of the type <code class="literal">[K,C]</code>. The <code class="literal">createCombiner</code> function turns something of the type <code class="literal">V</code> into something of the type <code class="literal">C</code>. The <code class="literal">mergeValue</code> function adds <code class="literal">V</code> to <code class="literal">C</code>, and <code class="literal">mergeCombiners</code> is used to combine the two <code class="literal">C</code> values into a single <code class="literal">C</code> value.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">zip</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>other</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function returns key-value pairs, pairing one element from each RDD. The first key-value pair would be the first element from this RDD, and the value would be the first element from the <span class="emphasis"><em>other</em></span> RDD; the second pair would be the respective second elements from each of the RDDs; and so on.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">groupByKey</code>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>numPartitions=None</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This function groups the values in the RDD using the key they have.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; ">
<p>
<code class="literal">cogroup</code>
</p>
</td><td style="border-right: 0.5pt solid ; ">
<p>other, numPartitions=None</p>
</td><td style="">
<p>This function joins two (or more) RDDs using the shared key. Note that if an element is missing in one RDD but present in the other one, the list will simply be empty.</p>
</td></tr></tbody></table></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec48"></a>References</h2></div></div><hr /></div><p>Some references are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<a class="ulink" href="http://www.scala-lang.org/api/current/index.html#scala.collection.immutable.List" target="_blank">http://www.scala-lang.org/api/current/index.html#scala.collection.immutable.List</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD" target="_blank">http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaPairRDD" target="_blank">http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaPairRDD</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaDoubleRDD" target="_blank">http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaDoubleRDD</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext" target="_blank">https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://abshinn.github.io/python/apache-spark/2014/10/11/using-combinebykey-in-apache-spark/" target="_blank">http://abshinn.github.io/python/apache-spark/2014/10/11/using-combinebykey-in-apache-spark/</a>
</p></li><li style="list-style-type: disc"><p>Good examples of RDD transformations: (<a class="ulink" href="https://github.com/JerryLead/SparkLearning/tree/master/src" target="_blank">https://github.com/JerryLead/SparkLearning/tree/master/src</a>)</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec49"></a>Summary</h2></div></div><hr /></div><p>This chapter looked at how to perform computations on data in a distributed fashion once it's loaded into an RDD. With our knowledge of how to load and save RDDs, we can now write distributed programs using Spark.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch07"></a>Chapter 7. Spark 2.0 Concepts</h2></div></div></div><p>Now that you have seen the fundamental underpinnings of Spark, let's take a broader look at the architecture, context, and ecosystem in which Spark operates. This is a catch-all chapter that captures a divergent set of essential topics that will help you get a broader understanding of Spark as a whole. Once you go through this, you will understand who is using Spark and how and where it is being used. This chapter will cover the following topics:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The Datasets accompanying this book and the IDEs for data wrangling</p></li><li style="list-style-type: disc"><p>A quick description of a data scientist's expectation from Spark</p></li><li style="list-style-type: disc"><p>The Data Lake architecture and the position of Spark</p></li><li style="list-style-type: disc"><p>The evolution and progression of Spark Architecture to 2.0</p></li><li style="list-style-type: disc"><p>The Parquet data storage mechanism</p></li></ul></div><p>So with good fundamental knowledge of the Spark framework, let's start focusing on these three topics: data scientist DevOps, data wrangling, and of course, the mechanisms in Apache Spark, including DataFrames, machine learning, and working with big data. In this chapter, we will look at the broader picture. In <a class="link" href="#" linkend="ch08">Chapter 8</a>, <span class="emphasis"><em>Spark SQL</em></span>, we will dive into DataFrames, APIs, and work on programs using DataFrame APIs. In <a class="link" href="#" linkend="ch09">Chapter 9</a>, <span class="emphasis"><em>Foundations of Datasets/DataFrames - The Proverbial Workhorse for Data Scientists</em></span>, we will visit Spark SQL. In <a class="link" href="#" linkend="ch10">Chapter 10</a>, <span class="emphasis"><em>Spark with Big Data</em></span>, we will see how Spark fits with the big data ecosystem. In <a class="link" href="#" linkend="ch11">Chapter 11</a>, <span class="emphasis"><em>Machine Learning with Spark ML Pipelines</em></span>, we'll get a chance to work with machine learning libraries. Spark R is another interesting area for data scientists to explore, and we have <a class="link" href="#" linkend="ch12">Chapter 12</a>, <span class="emphasis"><em>GraphX</em></span>, for this.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec50"></a>Code and Datasets for the rest of the book</h2></div></div><hr /></div><p>The first order of business is to look at the code and Datasets that we will be using for the rest of the chapters.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec25"></a>Code</h3></div></div></div><p>It is time for you to experiment with Spark APIs and wrangle with data. We have been using the Scala and Python shell in this book and you can continue to do so. You should also explore using an iPython notebook, which is an excellent way for data engineers and data scientists to experiment with data. The iPython notebooks and its Datasets are available at <a class="ulink" href="https://github.com/xsankar/fdps-v3" target="_blank">https://github.com/xsankar/fdps-v3</a>. You'll have to download some of the data yourselves due to the restrictions in distributing them. We have provided the appropriate URL as and when the need to download data arises.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec26"></a>IDE</h3></div></div></div><p>For this book, we will use <code class="literal">scala-shell</code> and <code class="literal">pyspark</code>. The Zeppelin IDE is another fine choice. Python is a better language for data scientists and has a tradition of strong scientific libraries. For those of you who prefer Scala, it is not that hard to map Python programs to Scala. If you are using Scala, my suggestion is the Zeppellin IDE.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec27"></a>iPython startup and test</h3></div></div></div><p>Before attempting the notebooks, install Jupyter and iPython. We will use Python 2.7. Make sure you can create a notebook.</p><p>Assuming that you have downloaded the GitHub to the <code class="literal">fdps-v3/</code> directory, you can start iPython like this:</p><pre class="programlisting">
<span class="strong"><strong>cd ~/fdps-v3/</strong></span>
<span class="strong"><strong>PYSPARK_DRIVER_PYTHON=ipython PYSPARK_DRIVER_PYTHON_OPTS="notebook" ~/Downloads/spark-2.0.0/bin/pyspark
</strong></span>
</pre><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note44"></a>Note</h3><p>
</p><p>
<span class="strong"><strong>A couple of things to note</strong></span>
</p><p>
</p><p>This attempt needs the location of <code class="literal">pyspark</code>. In the example, I have Spark in my <code class="literal">~/Downloads</code> directory.</p><p>
</p></div><p>You will see the iPython IDE in the <code class="literal">http://localhost:8888/</code> browser.</p><p>
</p><div class="mediaobject"><img src="graphics/image_07_001.jpg" /></div><p>
</p><p>Let's make sure our installation is working fine:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Click on <code class="literal">000-PreFlightCheck.ipynb</code>.</p></li><li><p>The notebook will start in a new tab. Run the first code block and it should print the version numbers, as shown in the following screenshot:</p><p>
</p><div class="mediaobject"><img src="graphics/image_07_002.jpg" /></div><p>
</p></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec28"></a>Datasets</h3></div></div></div><p>We will use lots of Datasets in this book. Some are small, while some are relatively bigger. The following sections will provide further details on each Dataset.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl3sec5"></a>Car-mileage</h4></div></div></div><p>This is a very old Dataset, going back to 70s. It contains engine characteristics, such as mileage, hp, torque, and rear axle ratio for 32 cars. It is a tiny Dataset, which is useful for the purpose of understanding statistical operations, aggregations, na, udf, and column-type conversion operations. It is also very easy to experiment with. The <code class="literal">csv</code> file has the header that describes the data elements.</p><p>The name of the file is <code class="literal">data/car-data/car-mileage.csv</code>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl3sec6"></a>Northwind industries sales data</h4></div></div></div><p>While writing other chapters, I was wondering what could be considered a good Dataset if I wish to bring out the various aspects of SQL. And, I hit upon an idea! Long time ago, the Northwind database was the canonical database to learn Microsoft Access, and later, the SQL server. And that would be a good Dataset for learning Spark SQL as well!</p><p>Let's use some of the tables and data to dig deeper into Spark SQL. The SQL scripts to create the Northwind database are available at <a class="ulink" href="https://northwinddatabase.codeplex.com/releases/view/71634" target="_blank">https://northwinddatabase.codeplex.com/releases/view/71634</a>. In our case, we will load data from a set of CSV files and create an appropriate Dataset in Spark. Then, we will fire off the SQL queries of increasing complexity. A good reference for this is the Spark SQL programming guide available at <a class="ulink" href="https://spark.apache.org/docs/latest/sql-programming-guide.html" target="_blank">https://spark.apache.org/docs/latest/sql-programming-guide.html</a>.</p><p>The Dataset has products, orders, order details, and salespeople from the fictitious company Northwind Industries-about 800 orders, 2150 order details, and 70 products. It's a tiny Dataset but easy to experiment with and very useful in understanding SQL operations, such as <code class="literal">Join</code>, as well as temporal partition and the aggregation of data, for example, year-wise aggregation.</p><p>The files are <code class="literal">data/NW/NW-products.csv</code>, <code class="literal">data/NW/NW-orders.csv</code>, <code class="literal">data/NW/NW-order-details.csv</code>, and <code class="literal">data/NW/NW-employees.csv</code>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl3sec7"></a>Titanic passenger list</h4></div></div></div><p>The Titanic passenger list includes data related to the passenger list of the Titanic and the survivor flag, interesting data wrangling, and classification exercise. We will use this data to understand DataFrames as well as MLlib.</p><p>The name of the file is <code class="literal">data/titanic/titanic3.csv</code>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl3sec8"></a>State of the Union speeches by POTUS</h4></div></div></div><p>This data includes the text files dating from 1760 to the present-State of the Union speeches by Washington, Lincoln, Bill Clinton, George Bush, and Barack Obama. We will use this data to understand how we can infer the mood of the nation using the ML pipelines.</p><p>The name of the file is <code class="literal">data/sotu/</code>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl3sec9"></a>Movie lens Dataset</h4></div></div></div><p>The movie lens Dataset includes data related to movie ratings by users, curated by the group lens project. There are multiple versions available, including small, medium, large, and XL. The small version has 100 K ratings, medium has 1 million, large with 10 million, and XL with 20 million. The main use of this is to study recommendation algorithms. We will also do some interesting data wrangling with Spark DataFrames.</p><p>The name of the file is <code class="literal">data/medium/</code>.</p></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec51"></a>The data scientist and Spark features</h2></div></div><hr /></div><p>One of the interesting questions relevant to this book is, "What do data scientists want?" It is a question that is being discussed and debated in many blogs. A short answer is as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The ability to explore, model, and reason data at scale-because many of their algorithms get asymptotically better with data, and so, a small Dataset sample is not enough for exploring different algorithms</p></li><li style="list-style-type: disc"><p>The ability to deploy without a lot of impedance</p></li><li style="list-style-type: disc"><p>The facility to evolve models once they are in production and the real world is using them</p></li></ul></div><p>In short, all we ask for is the shortest path from the lab to the factory, enabling a data scientist DevOps person! The following screenshot (combining talks from Josh Willis and Ian Buss), which displays <span class="strong"><strong>The Sense &amp; Sensibility of a Data Scientist DevOps</strong></span>, succinctly shows the value of Apache Spark to a data scientist by addressing three points:</p><p>
</p><div class="mediaobject"><img src="graphics/image_07_003.jpg" /></div><p>
</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec29"></a>Who is this data scientist DevOps person?</h3></div></div></div><p>Of course, we really do not want to start defining a data scientist here. There are many blogs (including mine) that explore this topic. It is enough to say that a data scientist DevOps person focuses on exploratory data analysis, visualization, lots of data wrangling, and tons of ideas. The work could span operational analytics, product analytics, decision data science, and product data science. That is, the inferences might be to answer business questions, say sales or marketing, or to embed intelligence in a product.</p><p>She (or he) would spend time equally with business/customers, algorithms, and data wrangling. The time might not be precisely one-third-for example, a person with more theoretical work might go deeper into algorithms, while a product data scientist might also dive into architecture and data modeling.</p><p>Their preferred programming language is R or Python, and they have enough statistical and programming background to work with both. The term data scientist DevOps can and will include developers who become proficient in statistics as well as statisticians who embrace programming. With this in mind, the preferred interface would be an IDE, such as iPython or Zeppelin, which can interact with a local or remote spark cluster. The focus is on agile and fast data wrangling-which is different from systems development, something that focuses on programming and the tools of choice are Java and IDEs, such as Eclipse.</p><p>Data scientists need to address reporting as well as analytics and span development-stage-production environments. Thus, they enable model development in the lab and model evolution in production.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec30"></a>The Data Lake architecture</h3></div></div></div><p>The following diagram shows a canonical Spark Data Lake architecture and stack with essential elements. It consists of the <span class="strong"><strong>Data Hub</strong></span>, <span class="strong"><strong>Analytics Hub</strong></span>, <span class="strong"><strong>Reporting Hub</strong></span>, <span class="strong"><strong>Visualization</strong></span>, and <span class="strong"><strong>ETL</strong></span> services.</p><p>
</p><div class="mediaobject"><img src="graphics/image_07_004.jpg" /></div><p>
</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl3sec10"></a>Data Hub</h4></div></div></div><p>Data Hub is the store where all the curated data lives, fed by ETL processes, Kafka, and so on. The data is heterogeneous in terms of the topics (for example, marketing data, transactional data, web logs, social media data, and unstructured data) as well as of the temporal fidelity nature (some of the data might relate to daily workings, some by the minute, and some with much more granularity). Data curation is what keeps the data hub usable. Also remember, the schemas would change over time, and the management of schema is essential.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl3sec11"></a>Reporting Hub</h4></div></div></div><p>Reporting Hub usually contains the data in a more structured and aggregated format suitable for daily consumption by reports and visualization dashboards. Spark is deployed here mainly for ETL and transformations. There could be matching and other in-memory requirements where Spark would work very well. Visualization tools, such as Tableau, Quilk, and Pentaho, can directly access the data in Spark via SparkSQL.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl3sec12"></a>Analytics Hub</h4></div></div></div><p>Analytics Hub is where the data scientists and the readers of this book spend most of their time. Analytics Hub would have access to the vast amount of data in Data Hub, and the analytics is expected to produce intermediate Datasets, feature extractions, as well as model Datasets. DataFrames, MLlib, GraphX, and ML pipelines are all very relevant here.</p></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec52"></a>Spark v2.0 and beyond</h2></div></div><hr /></div><p>Spark v2.0 and beyond has been the catalyst for a renaissance in data science! Datasets, DataFrames, ML pipelines, and new and improved algorithms in MLlib have paved the way for data wrangling at scale. I think Version 2.0 marks the spot where Spark turned into a mature framework. It could handle huge workloads in terms of the number of machines as well as the volume of data. The community update at the Spark Summit 2015 in San Francisco included a slide that showed the power of Spark:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The largest cluster-8,000 nodes (Tencent)</p></li><li style="list-style-type: disc"><p>The largest single job-1 petabyte and more (Alibaba and Tencent)</p></li><li style="list-style-type: disc"><p>The longest running job-1 petabyte and more for a week (Alibaba)</p></li><li style="list-style-type: disc"><p>The top streaming intake-1 terabyte/hour (Janelia farm)</p></li><li style="list-style-type: disc"><p>The largest shuffle-1 petabyte during sort benchmark (databricks)</p></li><li style="list-style-type: disc"><p>Netflix uses Spark for ad-hoc query and experimentation; they have 1,500 and more Spark nodes with 100 terabyte memory, chugging through 15 petabyte and more of S3 data and 7 petabyte of Parquet</p></li><li style="list-style-type: disc"><p>Tencent, probably the biggest known Spark installation, has 400 terabyte and more of memory in 8,000 and more machines and 150 petabyte data, running ETL and SQL</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec53"></a>Apache Spark - evolution</h2></div></div><hr /></div><p>It is interesting to trace the evolution of Apache Spark from an abstract perspective. Spark started out as a fast engine for big data processing-fast to run the code and write code as well. The original value proposition for Spark was that it offered faster in-memory computation graphs with compatibility with the Hadoop ecosystem, plus interesting and very usable APIs in Scala, Java, and Python. RDDs ruled the world. The focus was on iterative and interactive apps that operated on data multiple times, which was not a good use case for Hadoop.</p><p>The evolution didn't stop there. As Matei pointed out in his talk at MIT, users wanted more, and the Spark programming model evolved to include the following functionalities:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>More complex, multi-pass analytics (for example, ML pipelines and graph)</p></li><li style="list-style-type: disc"><p>More interactive ad-hoc queries</p></li><li style="list-style-type: disc"><p>More real-time stream processing</p></li><li style="list-style-type: disc"><p>More parallel machine learning algorithms beyond the basic RDDs</p></li><li style="list-style-type: disc"><p>More types of data sources as input and output</p></li><li style="list-style-type: disc"><p>More integration with R to span statistical computing beyond single-node tools</p></li><li style="list-style-type: disc"><p>More integration with apps such as visualization dashboards</p></li><li style="list-style-type: disc"><p>More performance with even larger Datasets and complex applications</p></li></ul></div><p>Spark Versions 1.3,1.4, 1.5, and 1.6 rose to the occasion and answered these requests as follows:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<span class="strong"><strong>Functionality</strong></span>
</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>
<span class="strong"><strong>Spark feature</strong></span>
</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>More complex multi-pass analytics</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>ML pipelines and GraphX</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>More interactive ad-hoc queries</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Datasets and DataFrames</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>More real-time stream processing</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Spark streaming</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>More parallel machine learning algorithms beyond the basic RDDs</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>MLlib</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>More types of data sources as input and output</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">DataFrameReader</code> and <code class="literal">DataFrameWriter</code>
</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>More integration with R to span statistical computing beyond single-node tools</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>SparkR</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>More integration with apps such as visualization dashboards</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">SparkSQL</code> and <code class="literal">jdbc</code> drivers</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; ">
<p>More performance with even larger Datasets and complex applications</p>
</td><td style="">
<p>Tungsten for optimized execution and unified execution across all languages</p>
</td></tr></tbody></table></div><p>Of course, Spark has added a lot more features than in the preceding table, for example, the interface with kafka, flume, and kinesis, which we will not get into in this book.</p><p>The various talks at the Spark Summit 2015 in San Francisco paint a clear picture of the trajectory and locus of Apache Spark:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>A very clear focus on Spark as a unified engine across diverse data sources, workloads, and environments</p></li><li style="list-style-type: disc"><p>Another important direction is Project Tungsten for optimizing execution, making Spark faster, and preparing for the next five years</p></li><li style="list-style-type: disc"><p>The third focus is the DAG visualization and debugging tools</p></li></ul></div><p>The following diagram shows how Spark's projection for the future maps across the actual features in the Spark platform:</p><p>
</p><div class="mediaobject"><img src="graphics/image_07_005.jpg" /></div><p>
</p><p>The interface to data sources is guided by the vision of high-level interfaces similar to the single-node tools. DataFrames are inspired by popular data transformation frameworks, such as Pandas in Python and the R DataFrames; the ML pipeline is inspired by Scikit-learn and Spark R interfaces with R.</p><p>The data source interface is far advanced than just having the ability to read MongoDB or Cassandra databases, or even the <code class="literal">jdbc</code> drivers. The algorithms, pushdown predicates, and the ability to leverage various smart optimizations in the storage native to the data sources are all part of the data source interface layers. The data would be returned as Datasets/DataFrames, which in turn can be consumed by the apps or the layers (refer to the diagram in the next section).</p><p>The engine improvements are taking priority at databricks because Spark is now handling more complex jobs, and CPUs are not getting any faster. Another main reason for the engine improvements is the JVM and the GC overhead-one of the goals of Project Tungsten is the off-heap memory management, which can optimize how memory is handled by the execution graphs and RDDs. Another interesting feature is the caching, leveraging data locality and cache-aware data structures. Caching improves performance because it avoids recomputation as well as data reads from the disk, as the data is available in the caches.</p><p>In addition to visible API improvements, each version of Spark achieves performance improvements as well. In fact, Spark 2.0, under the cover improvements, include shared optimization pipelines, efficient joins, and space efficiency. The Datasets do column-wise compression with four times less memory space and faster serialization performance.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec54"></a>Apache Spark - the full stack</h2></div></div><hr /></div><p>With all of this background information behind us, let's take a quick look at the full Spark stack (shown in the following diagram), which used to be a lot simpler, showing how the Spark ecosystem is continually evolving:</p><p>
</p><div class="mediaobject"><img src="graphics/image_07_006.jpg" /></div><p>
</p><p>The Spark stack currently includes the following features:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>It provides the Spark SQL feature. This feature uses SQL for data manipulation while maintaining the underlying Spark computations. It also provides the vital interface via exposing the Datasets to external systems through JDBC/ODBC, arguably the best value of Spark SQL.</p></li><li style="list-style-type: disc"><p>Advanced analytics, which is still evolving; look out for features such as parameter server and neural networks in the later versions of Spark.</p></li><li style="list-style-type: disc"><p>It provides the Dataset/DataFrame API, of course. It is one of parts we are focusing on in this book and we will see more of it in the following chapters.</p></li><li style="list-style-type: disc"><p>The catalyst optimizer is an interesting beast. It is the proverbial software layer that separates a declarative API/interface from efficient computation pragmatics. We will look at this layer a little closely in the next chapter when we talk about DataFrames.</p></li><li style="list-style-type: disc"><p>The data sources layer makes it possible to integrate Spark with other external systems. Of course, it is not a layer, but has the capabilities embedded in other layers. As we know, data exists in multiple heterogeneous systems, namely text files, relational databases, NOSQL systems, and so forth. Spark's focus is on abstracting the data sources and the focus on integration with as many data sources as possible as a conceptual layer (rather than specific point drivers).</p></li><li style="list-style-type: disc"><p>The Spark streaming and the GraphX are important parts of the system, but we will not focus on them in this book.</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec55"></a>The art of a big data store - Parquet</h2></div></div><hr /></div><p>For an efficient and performant computing stack, we also need an equally optimal storage mechanism. Parquet fits the bill and can be considered as a best practice. The pattern uses the HDFS file system, curates the Datasets, and stores them in the Parquet format.</p><p>Parquet is a very efficient columnar format for data storage, initially developed by contributors from Twitter, Cloudera, Criteo, Berkely AMP Lab, LinkedIn, and Stripe. The Google Dremel paper (Dremel, 2010) inspired the basic algorithms and design of Parquet. It is now a top-level Apache project, <code class="literal">parquet-apache</code>, and is the default format for reading and writing operations in Spark DataFrames. Almost all the big data products, from MPP databases to query engines to visualization tools, interface natively with Parquet. Let's take a quick look at the science of data storage in the big data domain, what we need from a store, and the capabilities of Parquet.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec31"></a>Column projection and data partition</h3></div></div></div><p>Column projection and data partition are used to enable you to read less data and only the data you need. Reading less data and optimizing it is a big part of the DataFrame story, especially distributed DataFrame at scale spanning 10s of machines, even 100s or 1000s. Parquet implements the concept of shredding columns and storing them in partitions. As a result, one only needs to read the columns needed, as opposed to row storage where the entire row is read and the columns not needed are ignored, but encounter the read penalty. Row read is expensive if we need only a few columns. Also, because of the partitions, one can surgically read subsets of columnar data without exhaustive seeks, thus adding another level of efficiency. Of course, column-oriented schemes would be slower for reading all columns.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec32"></a>Compression</h3></div></div></div><p>Compression is very important for performance, saving bandwidth as well as increasing storage efficiency. I/O is expensive, so is compression-column store compression is efficient because of multiple reasons. First, usually columns are more homogeneous, resulting in better compression; second, columns can be compressed with encoding schemes without heavyweight compression. In fact, Parquet achieves good compression ratio with schemes, such as bit packing, run length encoding, delta encoding, prefix coding, and dictionary encoding. An interesting side effect of the encoding is that many times, the scheme turns string comparison to integer comparison, thus increasing performance.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec33"></a>Smart data storage and predicate pushdown</h3></div></div></div><p>Predicate pushdown allows the store to skip data that don't match the selection predicates. Parquet stores the statistics about the data chunks that can be used by query planners and for predicate pushdown semantics. Thus, column pruning, skipping data blocks, happens at the Parquet drivel layer.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec34"></a>Support for evolving schema</h3></div></div></div><p>By nature, big data evolves, and we really cannot go back and rewrite older schemas as data structures change, especially when new columns are added. Of course, no system can create newer columns in old data and populate for us, but it can be flexible when we add more attributes. Parquet stores the schema and the design allows evolvability.</p><p>The Parquet schema merging capability makes it possible to evolve schema by having multiple Parquet files with different but compatible schemas. Naturally, one has to be careful to keep the schema compatibility in mind.</p><p>In short, the Parquet format provides a good balance with query optimization and storage efficiency. However, it has limitations as well:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>It doesn't perform well for single record additions; one gets the best performance when data is stored in chunks</p></li><li style="list-style-type: disc"><p>Like we discussed earlier, it would be relatively slower for reading all the columns</p></li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec35"></a>Performance</h3></div></div></div><p>Spark Version 2.0 improved Parquet throughput, from 11 M rows/second to 90 M rows/second, an eight-fold increase. One benchmark shows multiple operators throughput has increased from 14 M rows/second in Spark 1.6 to 125 M rows/second in Spark 2.0. We can definitely say that Spark and Parquet will achieve more performance and features in the future.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec56"></a>References</h2></div></div><hr /></div><p>The references are stated as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<a class="ulink" href="http://cdn.oreillystatic.com/en/assets/1/event/126/Apache%20Spark_%20What_s%20new_%20what_s%20coming%20Presentation.pdf" target="_blank">http://cdn.oreillystatic.com/en/assets/1/event/126/Apache%20Spark_%20What_s%20new_%20what_s%20coming%20Presentation.pdf</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="https://spark-summit.org/2015-east/wp-content/uploads/2015/03/SSE15-1-Matei-Zaharia.pdf" target="_blank">https://spark-summit.org/2015-east/wp-content/uploads/2015/03/SSE15-1-Matei-Zaharia.pdf</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://www.slideshare.net/databricks/spark-community-update-spark-summit-san-francisco-2015" target="_blank">http://www.slideshare.net/databricks/spark-community-update-spark-summit-san-francisco-2015</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="https://doubleclix.wordpress.com/2014/05/11/the-sense-sensibility-of-a-data-scientist-devops/" target="_blank">https://doubleclix.wordpress.com/2014/05/11/the-sense-sensibility-of-a-data-scientist-devops/</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://www.slideshare.net/databricks/spark-sql-deep-dive-melbroune" target="_blank">http://www.slideshare.net/databricks/spark-sql-deep-dive-melbroune</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://www.slideshare.net/databricks/introducing-dataframes-in-spark-for-large-scale-data-science" target="_blank">http://www.slideshare.net/databricks/introducing-dataframes-in-spark-for-large-scale-data-science</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://www.slideshare.net/databricks/bdtc2" target="_blank">http://www.slideshare.net/databricks/bdtc2</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://www.slideshare.net/SparkSummit/deep-dive-into-project-tungsten-josh-rosen" target="_blank">http://www.slideshare.net/SparkSummit/deep-dive-into-project-tungsten-josh-rosen</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="https://databricks.com/blog/2015/06/22/understanding-your-spark-application-through-visualization.html" target="_blank">https://databricks.com/blog/2015/06/22/understanding-your-spark-application-through-visualization.html</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://www.slideshare.net/databricks/dynamic-allocation-in-spark" target="_blank">http://www.slideshare.net/databricks/dynamic-allocation-in-spark</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://cdn.oreillystatic.com/en/assets/1/event/100/Parquet_%20An%20Open%20Columnar%20Storage%20for%20Hadoop%20Presentation%201.pdf" target="_blank">http://cdn.oreillystatic.com/en/assets/1/event/100/Parquet_%20An%20Open%20Columnar%20Storage%20for%20Hadoop%20Presentation%201.pdf</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://www.slideshare.net/julienledem/how-to-use-parquet-hadoopsummitsanjose2015" target="_blank">http://www.slideshare.net/julienledem/how-to-use-parquet-hadoopsummitsanjose2015</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://www.slideshare.net/alexlevenson/hadoop-summit-2015-performance-optimization-at-scale" target="_blank">http://www.slideshare.net/alexlevenson/hadoop-summit-2015-performance-optimization-at-scale</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="https://parquet.apache.org/" target="_blank">https://parquet.apache.org/</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://www.slideshare.net/SparkSummit/data-storage-tips-for-optimal-spark-performancevida-ha-databricks" target="_blank">http://www.slideshare.net/SparkSummit/data-storage-tips-for-optimal-spark-performancevida-ha-databricks</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces75" target="_blank">https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces75</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://yunus.hacettepe.edu.tr/~tonta/courses/spring2003/dok322/" target="_blank">http://yunus.hacettepe.edu.tr/~tonta/courses/spring2003/dok322/</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="https://code.google.com/p/northwindextended/" target="_blank">https://code.google.com/p/northwindextended/</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://www.slideshare.net/databricks/2015-0616-spark-summit" target="_blank">http://www.slideshare.net/databricks/2015-0616-spark-summit</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://research.google.com/pubs/pub36632.html" target="_blank">http://research.google.com/pubs/pub36632.html</a>
</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec57"></a>Summary</h2></div></div><hr /></div><p>This is an interesting chapter where we discussed the broader and wider picture of where Spark fits in the big data and analytics ecosystem. First, we looked at the Datasets that accompany this book as well as some interesting IDEs. We then discussed the role of data scientists and what they expect from a Spark stack, which led to our discussion to the Spark-based Data Lake architecture and then the Spark stack. We also looked at Parquest as an efficient storage format.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch08"></a>Chapter 8. Spark SQL</h2></div></div></div><p>Spark SQL provides an important feature in the Spark ecosystem, that is, integration with different data sources as well as the capability to interact with other subsystems, such as visualization. As we know, in modern data stacks, no stack is an island by itself, and in many ways, the versatility of integration with other components is an important capability. Obviously, the role of Spark SQL is not to replace SQL databases. We see it more as a versatile query interface for Spark data that complements the data wrangling and input capabilities of Spark. The ability to scale complex data operations makes sense only when one can utilize the results in flexible ways, and Spark SQL achieves that. We'll cover the following topics in this chapter:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The Spark SQL architecture</p></li><li style="list-style-type: disc"><p>Datasets/DataFrames</p></li><li style="list-style-type: disc"><p>SQL programming</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec58"></a>The Spark SQL architecture</h2></div></div><hr /></div><p>Interestingly, as I was writing this chapter, Michael Armbrust from Databricks wrote a blog about the data sources API and presented an architecture diagram; this is what inspired me to create the following diagram:</p><p>
</p><div class="mediaobject"><img src="graphics/image_08_001.jpg" /></div><p>
</p><p>The bottom layer is a flexible data access layer (and store) that works via multiple formats, usually a distributed filesystem such as HDFS. The computation layer is the place where we leverage the distributed-at-scale processing of the Spark engine, including the streaming data. The computation layer usually acts on RDDs. The <span class="strong"><strong>Dataset/DataFrame</strong></span> layer provides the API layer. The Spark SQL then overlays the Dataset/DataFrame layer and provides data access for applications, dashboards, BI tools, and so forth. There is a huge amount of SQL knowledge among various people, with roles ranging from data analysts and programmers to data engineers, who have developed interesting SQL queries over their data. Spark needs to leverage this knowledge of SQL queries, and it does this via Spark SQL.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec59"></a>Spark SQL how-to in a nutshell</h2></div></div><hr /></div><p>Prior to Spark 2.0.0, the heart of Spark SQL was SchemaRDD, which, as you can guess, associates a schema with an RDD. Of course, internally it does a lot of magic by leveraging the ability to scale and distribute processing and providing flexible storage.</p><p>In many ways, data access via Spark SQL is deceptively simple; we mean the process of creating one or more appropriate RDDs by paying attention to the layout, data types, and so on, and then accessing them via SchemaRDDs. We get to use all the interesting features of Spark to create the RDDs: structured data from Hive or Parquet, unstructured data from any source, and the ability to apply RDD operations at scale. Then, you need to overlay the respective schemas to the RDDs by creating SchemaRDDs. Voilà! You now have the ability to run SQL over RDDs. You can see the SchemaRDDs being created in the log entries.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec36"></a>Spark SQL with Spark 2.0</h3></div></div></div><p>The preceding section was true until Spark 2.0 (actually Datasets have been available since Spark 1.6.0)! The transition had been in progress since Spark 1.3.0, when DataFrames were introduced. Finally in Spark 2.0, all the elegant designs came together and Datasets became the way to wrangle with data. The basic workflow and semantics still exist; we have a Dataset in the disk in some form, say comma-separated values (<code class="literal">csv</code>), <code class="literal">tab</code>, or <code class="literal">pipe </code>delimited. We need to read the data and tell Spark the organization and the types; we also need a way to represent the data in memory at scale in a distributed fashion and find interesting ways to do operations on the data. While pre-Spark 2.0 had RDDs and schemaRDD to express data semantics and tables, Spark 2.0 unified the interface to Datasets/DataFrames.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip45"></a>Tip</h3><p>Even how we print data elements has become very easy now. Earlier, we had to use either <code class="literal">result.take(10).foreach(println)</code> or <code class="literal">result.take(30).foreach(e=&gt;println("%15s | %9.2f |".format(e(0),e(1))))</code> to get a reasonably formatted output. These are still good patterns for printing RDDs, but DataFrames/Datasets can be printed using the <code class="literal">result.show()</code> method. As you will see, it does an excellent job. Sometimes, we will use the <code class="literal">head()</code> call to see more details, such as the underlying object; most of the time it is <code class="literal">sql.row</code>, but still it gives us the layout, which comes in handy when we want to filter or use map operations.</p></div><p>In the next few sections, we will see how all of these come together. In this chapter, we will look at SQL statements, and in later chapters, we will explore the rich data wrangling capabilities of Datasets/DataFrames.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note46"></a>Note</h3><p>Because of the transition to 2.0 and the changes, we now have both versions of the code. For example, <code class="literal">sparkSQL.scala</code> is the pre-2.0 way of doing things; it will still work with 2.0 but will display deprecated messages. You should always start the Spark shell to show the deprecated messages like this:
<code class="literal">
<span class="strong"><strong>spark-shell -deprecation
</strong></span>
</code>
</p><p>This command will display a message when it encounters a deprecated method, and you can then change the call. This is how I created the <code class="literal">sparkSQL2.scala</code> file, which uses the 2.0 code.</p></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec60"></a>Spark SQL programming</h2></div></div><hr /></div><p>Let's now get our hands dirty and work through various examples. We will start with a simple Dataset and then progressively perform more sophisticated SQL statements. We will use the NorthWind Dataset.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec37"></a>Datasets/DataFrames</h3></div></div></div><p>In short, Datasets are semantic domain-specific objects, which means they are very rich in terms of typing and they possess all the functions of RDDs. In short, the best of both worlds! A DataFrame is an untyped view into a Dataset, basically a collection of rows. This is useful for doing abstract generic operations on a Dataset, that is, operations that depend only on the positions of elements in a row and other factors. We will learn more in later sections.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip47"></a>Tip</h3><p>As languages, such as Python and R, do not have compile-time type checking, Datasets and DataFrames are collapsed and called DataFrames.</p></div><p>Another change in 2.0 is <code class="literal">sparksession</code>, which replaces <code class="literal">sqlcontext</code>, <code class="literal">hivecontext</code>, and others. The <code class="literal">sparksession</code> instance has a very rich and flexible <code class="literal">read</code> method that can handle different data formats, such as <code class="literal">.csv</code>, <code class="literal">.parquet</code>, <code class="literal">.json</code>, and <code class="literal">.jdbc</code>. The <code class="literal">read()</code> method has options that reflect format-related characteristics, such as header, delimiter, and others.</p><p>Combining all the new interfaces and capabilities, we will create Datasets from the <code class="literal">read</code> operations available from <code class="literal">sparksession</code> and see how we can apply the <code class="literal">sql</code> statements.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec38"></a>SQL access to a simple data table</h3></div></div></div><p>Let's load a small CSV file into the employee Dataset, as shown here:</p><pre class="programlisting">//
// sparkSQL2.scala
//
// Code for Spark 2.0 way of doing things
//
// register case class external to main
//
[..]
val filePath = "/Users/ksankar/fdps-v3/"
println(s"Running Spark Version ${sc.version}")
//
val employees = spark.read.option("header","true").
csv(filePath + "data/NW-Employees.csv").as[Employee]
println("Employees has "+employees.count()+" rows")
employees.show(5)
employees.head()
</pre><p>The code is straightforward. We create a <code class="literal">case</code> class that represents the employee table. We then parse the CSV file and create a Dataset that has the <code class="literal">Employee</code> class as its elements.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note48"></a>Note</h3><p>The data files are available at <a class="ulink" href="https://github.com/xsankar/fdps-v3" target="_blank">https://github.com/xsankar/fdps-v3</a>. After you download the data, change <code class="literal">filePath</code> (the preceding code) to the directory where you have <code class="literal">fdps-v3</code>.</p></div><p>The screenshot of the process and the output from running the code from the Spark shell are shown here.</p><p>We start the Spark shell with the following command:</p><pre class="programlisting">
<span class="strong"><strong>/Volumes/sdxc-01/spark-2.0.0-preview/bin/spark-shell -deprecation</strong></span>
</pre><p>Refer to the following screenshot:</p><p>
</p><div class="mediaobject"><img src="graphics/image_08_002.jpg" /></div><p>
</p><p>Note a couple of things. As in older versions, the Spark context is available as <code class="literal">sc</code>; however, the new <code class="literal">sparkSession</code> instance is also available as Spark and that is what we will use.</p><p>Let's load the file <code class="literal">sparkSql2.scala</code>. It will run all the commands; we will parse the output and the code to understand the various operations.</p><p>
</p><div class="mediaobject"><img src="graphics/image_08_003.jpg" /></div><p>
</p><p>We declare a <code class="literal">case</code> class and <code class="literal">Employee</code> and then parse the file to the Dataset of the employees, as shown here:</p><pre class="programlisting">case class Employee(EmployeeID : String,
  LastName : String, FirstName : String, Title : String,
  BirthDate : String, HireDate : String,
  City : String, State : String, Zip : String, Country : String,
  ReportsTo : String)
//
case class Order(OrderID : String, CustomerID : String, EmployeeID : String,
  OrderDate : String, ShipCountry : String)
//
case class OrderDetails(OrderID : String, ProductID : String, UnitPrice : Double, Qty : Int, Discount : Double)
</pre><p>All the magic is done in the <code class="literal">read</code> method of the <code class="literal">sparkSession</code> variable, <code class="literal">spark</code>. We use the <code class="literal">csv</code> method, giving it a filename as well as an option to say we have the headers. The file looks like the following screenshot:</p><p>
</p><div class="mediaobject"><img src="graphics/image_08_004.jpg" /></div><p>
</p><p>The column titles in the header row correspond to the names in our <code class="literal">case</code> class and <code class="literal">Employee</code>, and the file is a <code class="literal">.csv</code> file. Spark is intelligent enough to match the data and our <code class="literal">case</code> class, and it subsequently creates the Dataset. We can see that the Dataset has nine rows and the <code class="literal">show(5)</code> command displays the first five rows in a tabular format.</p><p>The next step is to create a view in the Dataset that can be used for queries using SQL.</p><p>The code is as follows:</p><pre class="programlisting">employees.createOrReplaceTempView("EmployeesTable")
var result = spark.sql("SELECT * from EmployeesTable")
result.show(5)
result.head(3)
//
employees.explain(true)
</pre><p>We create a view,  <code class="literal">EmployeesTable</code>, over which an SQL statement is applied. The result is a DataFrame, which we verify using the <code class="literal">show()</code> command. Also, <code class="literal">explain</code> shows us the query plan as expected.</p><p>
</p><div class="mediaobject"><img src="graphics/image_08_005.jpg" /></div><p>
</p><p>Let's try a filter query, namely <code class="literal">SELECT * from Employees WHERE State = 'WA'</code>, and see how it works. Here is the code for this query:</p><pre class="programlisting">result = spark.sql("SELECT * from EmployeesTable WHERE State = 'WA'")
result.show(5)
result.head(3)
//
result.explain(true)
</pre><p>Here's the screenshot for this query:</p><p>
</p><div class="mediaobject"><img src="graphics/image_08_006.jpg" /></div><p>
</p><p>The query plan is slightly more detailed:</p><p>
</p><div class="mediaobject"><img src="graphics/image_08_007.jpg" /></div><p>
</p><p>Great! It worked as expected. You can see that the filter did get into the query plan.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl3sec13"></a>Handling multiple tables with Spark SQL</h4></div></div></div><p>Now that we have mastered the art of Spark SQL, let's try multiple Datasets and slightly larger Datasets. The <code class="literal">Orders</code> table's Dataset has 830 records and the <code class="literal">Order Details</code> table has approximately 2,000 records. These tables will give us a good representation of a few queries with joins that span the two tables.</p><p>Let's start by loading the <code class="literal">Orders</code> table, as shown here:</p><pre class="programlisting">case class Order(OrderID : String, CustomerID : String, EmployeeID : String, OrderDate : String, ShipCountry : String)
//
...
...
val orders = spark.read.option("header","true").
csv(filePath + "data/NW-Orders.csv").as[Order]
println("Orders has "+orders.count()+" rows")
orders.show(5)
orders.head()
orders.dtypes
</pre><p>Here, I ran into a few interesting miscues, which were very revealing but at the same time showed the work Spark is doing for us beneath the hood.</p><p>The first error was a little puzzling:</p><p>
</p><div class="mediaobject"><img src="graphics/image_08_008.jpg" /></div><p>
</p><p>I realized that the header of the fourth column in the <code class="literal">data</code> file had a spelling mistake: <code class="literal">EmpliyeeId</code> instead of <code class="literal">EmployeeID</code>! Once I corrected the error in the <code class="literal">data</code> file, I ran into the second error:</p><p>
</p><div class="mediaobject"><img src="graphics/image_08_009.jpg" /></div><p>
</p><p>This was easy to fix. There was another spelling mistake related to <code class="literal">ShipCountry</code>, the last column. After correcting the column title in the data file, I ran the code again; then the next error popped up:</p><p>
</p><div class="mediaobject"><img src="graphics/image_08_010.jpg" /></div><p>
</p><p>I had declared the <code class="literal">case</code> class as follows:</p><pre class="programlisting">case class OrderDetails(OrderID : String, ProductID : String, UnitPrice : Float, Qty : Int, Discount : Float)</pre><p>The <code class="literal">UnitPrice</code> instance needs to be changed to <code class="literal">Double</code>. The error message was detailed enough. So I changed the <code class="literal">case</code> class definition and ran the code again:</p><pre class="programlisting">case class OrderDetails(OrderID : String, ProductID : String, UnitPrice : Double, Qty : Int, Discount : Float)</pre><p>Here's a screenshot with the changes introduced:</p><p>
</p><div class="mediaobject"><img src="graphics/image_08_011.jpg" /></div><p>
</p><p>Then the next error popped up; this time the type of discount was changed to <code class="literal">Double</code>:</p><p>
</p><div class="mediaobject"><img src="graphics/image_08_012.jpg" /></div><p>
</p><p>Once the errors were fixed, the code ran fine and the orders Dataset and the view were created fine:</p><p>
</p><div class="mediaobject"><img src="graphics/image_08_013.jpg" /></div><p>
</p><p>The <code class="literal">read</code> method has an option to infer a schema automatically. I think this is a very good idea. I have been doing this in R for a long time. So let's try this option.</p><pre class="programlisting">val orders = spark.read.option("header","true").
option("inferSchema","true").
csv(filePath + "data/NW-Orders.csv").as[Order]
println("Orders has "+orders.count()+" rows")
orders.show(5)
orders.head()
orders.dtypes // verify column types
</pre><p>This code worked out fine. Notice the slight change in the types for <code class="literal">OrderID</code> and <code class="literal">EmployeeID</code>. They changed from <code class="literal">String</code> to <code class="literal">Integer</code>. Here's a screenshot of this:</p><p>
</p><div class="mediaobject"><img src="graphics/image_08_014.jpg" /></div><p>
</p><p>We apply a similar pattern for order details. By now, we are an old hand at doing this. The following code is for the loading process of the table:</p><pre class="programlisting">val orderDetails = spark.read.option("header","true").
option("inferSchema","true").
csv(filePath + "data/NW-Order-Details.csv").as[OrderDetails]
println("Order Details has "+orderDetails.count()+" rows")
orderDetails.show(5)
orderDetails.head()
orderDetails.dtypes // verify column types
//
//orders.createTempView("OrdersTable")
orders.createOrReplaceTempView("OrdersTable")
result = spark.sql("SELECT * from OrdersTable")
result.show(10)
result.head(3)
//
orderDetails.createOrReplaceTempView("OrderDetailsTable")
var result = spark.sql("SELECT * from OrderDetailsTable")
result.show(10)
result.head(3)
</pre><p>When we create the view, if the view already exists you will get the <code class="literal">TempTableAlreadyExists</code> exception:</p><p>
</p><div class="mediaobject"><img src="graphics/image_08_015.jpg" /></div><p>
</p><p>The solution is to use <code class="literal">createOrReplaceView</code> instead of <code class="literal">cretaeTempView</code>. Once I had changed <code class="literal">orders.createTempView("OrderTable")</code> to <code class="literal">orders.createOrReplaceTempView("OrderTable")</code>, the error disappeared.</p><p>The output of this is shown in the following screenshot. This is no different from our earlier work. We have 830 orders in our orders table and 2,155 order details.</p><p>
</p><div class="mediaobject"><img src="graphics/image_08_016.jpg" /></div><p>
</p><p>In this chapter, we are trying to create a few queries. So, we really do not need hundreds of records. However, the Dataset has enough records for you to try out various queries on your own. The Dataset is big enough to do meaningful queries but too small to work on a laptop with limited resources. This would be a good exercise for you to experiment with Spark SQL.</p><p>Now comes the interesting part. Let's join the two tables and see how that query works:</p><pre class="programlisting">//
// Now the interesting part
//
result = spark.sql("SELECT OrderDetailsTable.OrderID, ShipCountry, UnitPrice, Qty, Discount FROM OrdersTable INNER JOIN OrderDetailsTable ON OrdersTable.OrderID = OrderDetailsTable.OrderID")
result.show(10)
result.head(3)
//
// Sales By Country
//
result = spark.sql("SELECT ShipCountry, SUM(OrderDetailsTable.UnitPrice * Qty * Discount) AS ProductSales FROM OrdersTable INNER JOIN OrderDetailsTable ON OrdersTable.OrderID = OrderDetailsTable.OrderID GROUP BY ShipCountry")
result.count()
result.show(10)
result.head(3)
result.orderBy($"ProductSales".desc).show(10) // Top 10 by Sales
</pre><p>Refer to the following screenshot:</p><p>
</p><div class="mediaobject"><img src="graphics/image_08_017.jpg" /></div><p>
</p><p>It works fine! Good stuff! The second query, <code class="literal">Sales by Country</code>, is more interesting.</p><p>The output looks as expected. We can also use <code class="literal">orderBy</code> and see the top 10 countries by sales:</p><p>
</p><div class="mediaobject"><img src="graphics/image_08_018.jpg" /></div><p>
</p><p>Interestingly, this worked on the first try, the credit for which goes to the Spark developers. We could also format the printout with currency as well. I leave that as an exercise to be done by you! Try and run different queries and verify that the results are as expected.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl3sec14"></a>Aftermath</h4></div></div></div><p>As seen in the preceding screenshot, this was a good exercise. Also, Spark 2.0 has made the interfaces simpler and powerful. We are thoroughly impressed! We just created the last query and it ran fine. The Spark developers have done a good job. Good work, guys!</p><p>The Dataset also includes the <code class="literal">product</code> table, which I leave to you as an exercise. For example, you can work on a query that returns sales by product or one that shows the products that are selling more. The Dataset also has date fields, such as <code class="literal">order dates</code>, which you can use to query sales by quarters, and reports, such as <code class="literal">Product sales for 1997</code>. The dates are now read in as strings. They need to be converted to the <code class="literal">TIMESTAMP</code> data type.</p></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec61"></a>References</h2></div></div><hr /></div><p>The references are listed here:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<a class="ulink" href="https://northwinddatabase.codeplex.com/releases/view/71634" target="_blank">https://northwinddatabase.codeplex.com/releases/view/71634</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="https://databricks.com/blog/2015/01/09/spark-sql-data-sources-api-unified-data-access-for-the-spark-platform.html" target="_blank">https://databricks.com/blog/2015/01/09/spark-sql-data-sources-api-unified-data-access-for-the-spark-platform.html</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="https://spark.apache.org/docs/latest/sql-programming-guide.html" target="_blank">https://spark.apache.org/docs/latest/sql-programming-guide.html</a>
</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec62"></a>Summary</h2></div></div><hr /></div><p>This was an important chapter that discussed the integration aspects of Spark and SQL. We have covered the main parts, namely Datasets and programmatic access. However, there are more capabilities, such as the JDBC/ODBC server for direct SQL queries. On the integration side, you will see more integration capabilities in <a class="link" href="#" linkend="ch10">Chapter 10</a>, <span class="emphasis"><em>Spark with Big Data</em></span>. Spark SQL will be getting more features in the future versions, and I think this will be one of the areas that will grow at a much faster pace.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch09"></a>Chapter 9. Foundations of Datasets/DataFrames – The Proverbial Workhorse for DataScientists</h2></div></div></div><p>From a data wrangling perspective, Datasets are the most important feature of Spark 2.0.0. In this chapter, we will first look at Datasets from a stack perspective, including layering, optimizations, and so forth. Then we will delve more deeply into the actual Dataset APIs and cover the various operations, starting from reading various formats to creating Datasets and finally covering the rich capabilities for queries, aggregations, and scientific operations. We will use the car and orders Datasets for our examples.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec63"></a>Datasets - a quick introduction</h2></div></div><hr /></div><p>A Spark Dataset is a group of specified heterogeneous columns, akin to a spreadsheet or a relational database table. RDDs have always been the basic building blocks of Spark and they still are. But RDDs deal with objects; we might know what the objects are but the framework doesn't. So things such as type checking and semantic queries are not possible with RDDs. Then came DataFrames, which added schemas; we can associate schemas with an RDD. DataFrames also added SQL and SQL-like capabilities.</p><p>Spark 2.0.0 added Datasets, which have all the original DataFrame APIs as well as compile-time type checking, thus making our interfaces richer and more robust. So now we have three mechanisms:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Our preferred mechanism is the semantic-rich Datasets</p></li><li style="list-style-type: disc"><p>Our second option is the use of DataFrames as untyped views in a Dataset</p></li><li style="list-style-type: disc"><p>For low-level operations, we'll use RDDs as the underlying basic distributed objects</p></li></ul></div><p>In short, we should always use the Dataset APIs and abstractions. RDDs are akin to low-level APIs, which we do go to, occasionally. In fact, in this book, I had a few such occasions/places where I couldn't find appropriate Dataset APIs. Maybe there are Dataset options that I don't know about. So familiar RDD operations do come to the rescue. The advantage is that you can move from Datasets to RDDs and vice versa. So when needed, the pattern to follow is this: convert to RDD, do the low-level operation, and then convert back to a Dataset.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip49"></a>Tip</h3><p>
</p><p>A few points to remember:</p><p>
</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>For Python and R, the class is still DataFrame, but with all the Dataset APIs. So you can think as of it as this: Datasets in Python and R are called DataFrames. The APIs map exactly, so it is easy to keep the mental model straight.</p></li><li style="list-style-type: disc"><p>In Scala and Java, Datasets are the main interface and there is no DataFrame class.</p></li><li style="list-style-type: disc"><p>You will still see the name DataFrame, as in DataFrame Reader, DataFrame Writer, and so on. For all practical purposes, the words DataFrame and Dataset are interchangeable. And you will know when the difference matters.</p></li></ul></div><p>
</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec64"></a>Dataset APIs - an overview</h2></div></div><hr /></div><p>Before we delve into Datasets and data wrangling, let's take a broader view of the APIs; we will focus on the relevant functions we need. This will give us a firm foundation when we wrangle with data later in this chapter. Refer to the following diagram:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_001.jpg" /></div><p>
</p><p>The preceding diagram shows the broader hierarchy of the <code class="literal">org.apache.spark.sql</code> classes. Interestingly, <code class="literal">pyspark.sql</code> mirrors this hierarchy, except for DataFrame, which is basically the Scala Dataset. What I like about the PySpark interface is that it is very succinct and crisp, offering the same power, performance, and functionality as Scala or Java. But Scala has more elaborate hierarchies and more abstractions. One of the tricks to learn more about its functions is to refer to the Scala documentation, which I found to be a lot more detailed.</p><p>Each of these classes is rich with a lot of functions. The diagram shows only the most common ones we need in this chapter. You should refer to either <a class="ulink" href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.package" target="_blank">https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.package</a> or <a class="ulink" href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html" target="_blank">https://spark.apache.org/docs/latest/api/python/pyspark.sql.html</a> to get full coverage.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip50"></a>Tip</h3><p>In this book, and more so in this chapter, we encourage polyglot programming, moving between a compiled language, such as Scala/Java, and an interpreted one, such as Python/R. For data wrangling, the interpreted languages Python/R are more suitable. And the Scala Dataset/Python DataFrame makes it very easy to be in both languages, without any performance penalty.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec39"></a>org.apache.spark.sql.SparkSession/pyspark.sql.SparkSession</h3></div></div></div><p>SparkSession is the single entry point, and it has the create, read, and SQL functions that are relevant to us:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The <code class="literal">createDataset()</code>is a good way to create a Dataset from an RDD.</p></li><li style="list-style-type: disc"><p>The <code class="literal">SparkSession.read</code> library is very rich in semantics. You can read CSV files (<code class="literal">SparkSession.read.csv()</code>), text files (<code class="literal">SparkSession.read.text()</code>), Parquet (<code class="literal">SparkSession.read.parquet()</code>), JDBC(<code class="literal">SparkSession.read.jdbc()</code>), JSON (<code class="literal">SparkSession.read.json()</code>), and other formats. Each format has its own options appropriate to the respective format so that you can have good control over how the files are read and Datasets created.</p></li><li style="list-style-type: disc"><p>The <code class="literal">write()</code>method is also equally capable, and it is available as a function in the Dataset, for example, <code class="literal">df.write.csv()</code>.</p></li><li style="list-style-type: disc"><p>The <code class="literal">SparkSession.sql() </code>method takes a SQL query. We have covered this in <a class="link" href="#" linkend="ch08">Chapter 8</a>, <span class="emphasis"><em>Spark SQL</em></span>.</p></li><li style="list-style-type: disc"><p>If for any reason, you need SparkContext, it is encapsulated. So the <code class="literal">SparkSession.sparkContext </code>method returns the underlying SparkContext, which you can use to create RDDs as well as manage cluster resources.</p></li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec40"></a>org.apache.spark.sql.Dataset/pyspark.sql.DataFrame</h3></div></div></div><p>The <code class="literal">sql.Dataset/sql.DataFrame</code> class is the primary choice for data wrangling in Scala/Python. It has more than 100 functions. Some of the functions, such as <code class="literal">na.*</code> and <code class="literal">stat.*</code>, are mapped from the <code class="literal">sql.DataFrameNaFunctions</code> and <code class="literal">sql.DataFrameStatFunctions </code>packages. This is for convenience, as the <code class="literal">na</code> and <code class="literal">stat</code> functions are used extensively for data exploration.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec41"></a>org.apache.spark.sql.{Column,Row}/pyspark.sql.(Column,Row)</h3></div></div></div><p>While there is no need to use the <code class="literal">Column</code> and <code class="literal">Row</code> objects, operations, such as <code class="literal">map()</code>, are done at the row level and aggregation functions, such as sum, mean, and variance, are done at the column level. These are done transparently, but it is good to be aware that the underlying objects are <code class="literal">Row</code> and <code class="literal">Column</code>. Sometimes, low-level operations require us to recognize that we are dealing with a <code class="literal">Row</code>/<code class="literal">Column </code>object, and so we must index the values appropriately.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl3sec15"></a>org.apache.spark.sql.Column</h4></div></div></div><p>The <code class="literal">sql.Column </code>class is very simple and elegant in what it does.</p><p>First of all, it enables you to select a column by either the <code class="literal">df("&lt;columnName&gt;")</code>or <code class="literal">df.&lt;columnName&gt;</code> notation.</p><p>As we saw, there are two ways of accessing a column, either the dotted way (<code class="literal">df.&lt;columnName&gt;</code>, for example, <code class="literal">orders.date</code>) or the quoted way (<code class="literal">df["columnName"]</code>, for example, <code class="literal">orders["date"]</code>). You're recommended to use <code class="literal">df("&lt;columnName&gt;")</code> because a column name can collide with a DataFrame method if we use <code class="literal">df.&lt;columnName&gt;</code>. Also, the dotted method won't work when we want to create a new column or if the column name has embedded spaces.</p><p>Second, we are able to perform column-wise operations such as <code class="literal">+</code>,<code class="literal">-</code>, <code class="literal">*</code>,<code class="literal">/</code>,<code class="literal">%</code> (modulo),<code class="literal">&amp;&amp;</code>,<code class="literal">||</code>, <code class="literal">&lt;</code>,<code class="literal">&lt;=</code>,<code class="literal">&gt;</code>, and <code class="literal">&gt;=</code>. This way, we can perform an operation of this nature: <code class="literal">df("total") = df("price") * df("qty")</code>. We will see the total price example at the end of this subsection. The <code class="literal">sql.Column </code>class has some interesting operations: the inequality operator is <code class="literal">!==</code>, the usual equal to operator is <code class="literal">===</code>, and an equality test that is safe for null values is <code class="literal">&lt;=&gt;</code>. We will check out a snippet of the inequality test to calculate the accuracy of a classification algorithm.</p><p>Third, we are able to perform meta operations, such as type conversion, alias, not null, and so forth. Column types can be converted using a cast operator like this: <code class="literal">df_cars.mpg.cast("double")</code>. Column names can be changed using the alias operator. Scala has the <code class="literal">as()</code> operator, and Python has the alias method, <code class="literal">df_cars.mpg.cast("double").alias('mpg')</code>.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip51"></a>Tip</h3><p>It is a good practice to not change column names in the middle of a program, unless it is essential. Comment on what is being done and why. You will thank yourself later!</p></div><p>Finally, it provides you with operations that affect the Dataset/DataFrame organization. You can drop a column with <code class="literal">df.drop(&lt;columnName&gt;)</code>; of course, as Datasets/DataFrames are immutable, you will get a new Dataset/DataFrame minus the column.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl3sec16"></a>org.apache.spark.sql.Row</h4></div></div></div><p>The <code class="literal">sql.Row </code>class is a lot simpler than <code class="literal">sql.Column</code>. The <code class="literal">Row </code>object by itself is usually transparent and implicit; one example is, operations such as Map, which operates over a DataFrame row by row. Thus, the parameter type for a lambda is <code class="literal">Row</code>. The elements of <code class="literal">Row</code> can be accessed by an ordinal, that is, <code class="literal">row(0)</code>, or by a name <code class="literal">(row("&lt;name of field&gt;");</code> the length/size gives the number of elements.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec42"></a>org.apache.spark.sql.functions/pyspark.sql.functions</h3></div></div></div><p>Of all the classes, I think <code class="literal">sql.functions</code> class is the richest and provides the power that a Dataset has. In a nutshell, <code class="literal">sql.functions</code> have methods such as <code class="literal">split()</code>and <code class="literal">dateDiff()</code>, which work at the column level. The normal versions of these functions, whether they are in Scala or Python, act on single variables. The <code class="literal">sql.functions</code> class enables us to apply these functions to the Dataset columns in a distributed and optimized fashion.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec65"></a>Dataset interfaces and functions</h2></div></div><hr /></div><p>Now let's work out a few interesting examples, starting out with a simple one and then moving on to progressively complex operations.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip52"></a>Tip</h3><p>The code files are in <code class="literal">fdps-v3/code</code>, and the data files are in <code class="literal">fdps-v3/data</code>. You can run the code either from a Scala IDE or just from the Spark Shell.</p></div><p>Start Spark Shell from the bin directory where you have installed the spark:</p><pre class="programlisting">
<span class="strong"><strong>/Volumes/sdxc-01/spark-2.0.0/bin/spark-shell </strong></span>
</pre><p>Inside the shell, the following command will load the source:</p><pre class="programlisting">
<span class="strong"><strong>:load /Users/ksankar/fdps-v3/code/DS01.scala</strong></span>
</pre><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec43"></a>Read/write operations</h3></div></div></div><p>As we saw earlier, <code class="literal">SparkSession.read.* </code>gives us a rich set of features to read different types of data with flexible control over the options. <code class="literal">Dataset.write.*</code> does the same for writing data:</p><pre class="programlisting">val spark = SparkSession.builder
      .master("local")
      .appName("Chapter 9")
      .config("spark.logConf","true")
      .config("spark.logLevel","ERROR")
      .getOrCreate()
println("Running Spark Version ${spark.version}")
//
val startTime = System.nanoTime()
//
// Read Data
//
val filePath = "/Users/ksankar/fdps-v3/"
val cars = spark.read.option("header","true").
option("inferSchema","true").
csv(filePath + "data/spark-csv/cars.csv")
println("Cars has "+cars.count()+" rows")
cars.show(5)
cars.printSchema()
//
// Write data
// csv format with headers
cars.write.mode("overwrite").option("header","true").csv(filePath + "data/cars-out-csv.csv")
    // Parquet format
cars.write.mode("overwrite").partitionBy("year").parquet(filePath + "data/cars-out-pqt")
</pre><p>We start the Spark Shell, load the <code class="literal">DS01.scala </code>file, and run the <code class="literal">DS01.main()</code> function.</p><p>The data read completes successfully and we can see the data and the schema as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_002.jpg" /></div><p>
</p><p>The write also completes successfully. We can see the files and the directories as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_003.jpg" /></div><p>
</p><p>Because we used <code class="literal">partitionBy("year")</code>, Spark created four subdirectories. The partitioning is useful for push-down queries; for example, if a query has <span class="strong"><strong>year=1997</strong></span>, then the data for only that year will need to be selected while ignoring all of the other data. Remember that our data comprises only four records, but it might very well be a million records and part of a big data lake. In that case, the partitioning will save us a lot of reading.</p><p>We can also see that the compression is snappy, by default.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip53"></a>Tip</h3><p>To read the file back, we could use the <code class="literal">SparkSession.read.parquet(filePath + "data/cars-out-pqt") </code>call to create a new Dataset.</p></div><p>The CSV write also succeeds and we can inspect the file to check whether it has the headers. It does! The mode parameter in write controls what happens when the file, exists as follows:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Mode Parameter</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Description</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">SaveMode.Overwrite</code> or overwrite</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This is used to overwrite existing files</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">SaveMode.Append</code> or append</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This is used to append to an existing file</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<code class="literal">SaveMode.Ignore</code> or ignore</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This is used to ignore the <code class="literal">Data Exists</code> error silently and not save. Use this parameter with caution</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; ">
<p>
<code class="literal">SaveMode.Error</code>if exists or error</p>
</td><td style="">
<p>This is used to raise an exception if the file exists (default). How do we reset this?</p>
</td></tr></tbody></table></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip54"></a>Tip</h3><p>A little bit of history: prior to 1.4, SQLContext had lots of data source functions, including JSON. In 1.4, these functions were moved to the <code class="literal">pyspark.sql.DataFrameReader </code>class and specifically the <code class="literal">read()</code> function. In 2.0, the <code class="literal">read()</code> functions are consolidated under the <code class="literal">SparkSession</code> class. The read/write framework is also very extensible in 2.0, and we can add support for new data storage formats with the driver classes.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec44"></a>Aggregate functions</h3></div></div></div><p>One of the first things a data scientist would do to explore new data is calculate general functions such as min, max, mean, and standard deviation. The next step is to find the numbers by groups, which gives one an idea about how the groups are laid out in the sample data. In this section, we will explore the car mileage data:</p><pre class="programlisting">    val filePath = "/Users/ksankar/fdps-v3/"
    val cars = spark.read.option("header","true").
      option("inferSchema","true").
    csv(filePath + "data/car-data/car-mileage.csv")
    println("Cars has "+cars.count()+" rows")
    cars.show(5)
    cars.printSchema()
    //
    cars.describe("mpg","hp","weight","automatic").show()
    //
    cars.groupBy("automatic").avg("mpg","torque").show()
    //
    cars.groupBy().avg("mpg","torque").show()
    cars.agg(avg(cars("mpg")), mean(cars("torque")) ).show()
</pre><p>Here, the <code class="literal">describe </code>method shows the min, max, mean, and standard deviation. We can also find the numbers by groups, say what is the mpg for automatic versus manual transmissions? Interestingly, if we want to get the numbers outside describe, we can use the <code class="literal">agg</code> function and give it with the function and the column. Remember the function comes from the hundreds of functions available in <code class="literal">sql.functions</code>. We import the required function like this:</p><pre class="programlisting">import org.apache.spark.sql.functions.{avg,mean}
</pre><p>The code file is <code class="literal">DS02.scala</code>. We load the code and run the <code class="literal">DS02</code> object. The output is as expected:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_004.jpg" /></div><p>
</p><p>We get the summary of the whole Dataset. Also, the aggregation functions work well. So long as you understand <code class="literal">groupBy()</code> and use <code class="literal">sql.functions</code> with <code class="literal">agg()</code>, you have enough power to explore the data.</p><p>For example, the preceding mpg cross-table might lead us to the conclusion that manual automobiles have better mileage. Let's take a closer look:</p><pre class="programlisting">cars.groupBy("automatic").avg("mpg","torque","hp","weight").show()</pre><p>The code gives following output in response:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_005.jpg" /></div><p>
</p><p>Looking at the mean of <span class="strong"><strong>torque</strong></span> and <span class="strong"><strong>hp</strong></span>, we realize that, for this Dataset, the cars with automatic transmission also have larger torque and higher hp. And, they also weigh more. So we cannot make a generalization that manual transmission provides better mileage.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip55"></a>Tip</h3><p>Interestingly, <code class="literal">groupBy()</code> has an alias: <code class="literal">groupby()</code>. I prefer the camel case, but lowercase is not a mistake!</p></div><p>There are two types of aggregations: one on column values and the other on subsets of column values, that is, grouped values of some other columns. Full column aggregation functions act on all the values on a column (actually shorthand for the null group by, that is, <code class="literal">df.groupBy().agg</code>...); for example, if you want the average sales for the last three years combined, you can use <code class="literal">sql.functions.avg("sales")</code>. But if you want the average sales by year, you will need to use<code class="literal">sql.functions.groupby("year").agg("sales":"avg")</code>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec45"></a>Statistical functions</h3></div></div></div><p>Next, let's explore statistical functions. As we saw in the API diagram in the <span class="emphasis"><em>Dataset APIs - an overview </em></span>section, the functions are available under <code class="literal">sql.stat.*</code>:</p><pre class="programlisting">val cor = cars.stat.corr("hp","weight")
    println("hp to weight : Correlation = %.4f".format(cor))
val cov = cars.stat.cov("hp","weight")
    println("hp to weight : Covariance = %.4f".format(cov))
    //
    cars.stat.crosstab("automatic","NoOfSpeed").show()
   //
</pre><p>The code is in <code class="literal">DS03.scala</code>. We load and run the <code class="literal">DS03</code> object. The output is is as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_006.jpg" /></div><p>
</p><p>The output looks good. The correlation provides an understandable number rather than covariance. The cross tab is an interesting and very useful function. The <code class="literal">CrossTabulation</code> function provides a table of the frequency distribution for a set of variables. We can see that the automatic transmission engines have only three speed values, while the manual have 3, 4, and 5 speed values.</p><p>Let's use the <code class="literal">crosstab</code> function to explore the Titanic Dataset. This Dataset is a list of passengers with their attributes, such as name and age, and information related to whether they are traveling with spouse/siblings and so forth. And most importantly, it provides information on whether they have survived. A good Dataset to explore and ask questions is as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>How many of each gender survived?</p></li><li style="list-style-type: disc"><p>Did age make any difference?</p></li><li style="list-style-type: disc"><p>Did passengers traveling with spouses/siblings have a better chance of survival?</p></li></ul></div><p>In <a class="link" href="#" linkend="ch11">Chapter 11</a>, <span class="emphasis"><em>Machine Learning with Spark ML Pipelines</em></span>, we will use machine learning libraries to predict the survival. Here, we want to get a feel for the data and see whether there are any trends. And this would be a good measure of the data science capabilities of Spark. Refer to the following code:</p><pre class="programlisting">val filePath = "/Users/ksankar/fdps-v3/"
  val passengers = spark.read.option("header","true").
  option("inferSchema","true").
  csv(filePath + "data/titanic3_02.csv")
  println("Passengers has "+passengers.count()+" rows")
      //passengers.show(5)
      //passengers.printSchema()
      //
  val passengers1 =    passengers.select(passengers("Pclass"),passengers("Survived"),passengers("Gender"),passengers("Age"),passengers("SibSp"),passengers("Parch"),passengers("Fare"))
  passengers1.show(5)
  passengers1.printSchema()
    //
  passengers1.groupBy("Gender").count().show()
  passengers1.stat.crosstab("Survived","Gender").show()
    //
  passengers1.stat.crosstab("Survived","SibSp").show()
    //
    // passengers1.stat.crosstab("Survived","Age").show()
  val ageDist =  passengers1.select(passengers1("Survived"),   (passengers1("age") - passengers1("age") %  10).cast("int").as("AgeBracket"))
  ageDist.show(3)
  ageDist.stat.crosstab("Survived","AgeBracket").show()
</pre><p>The <code class="literal">read.csv </code>file loads the data. We select a few columns that are of interest to us and apply aggregate functions such as count and crosstab to get our answers.</p><p>The code is in <code class="literal">DS04.scala</code>. We load and run the <code class="literal">DS04</code> object. The output is interesting:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_007.jpg" /></div><p>
</p><p>The data load and the schema look good as shown in the preceding screenshot. Now check this out:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_008.jpg" /></div><p>
</p><p>In the preceding screenshot, we see that most of the males perished and the females survived:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_009.jpg" /></div><p>
</p><p>The SibSp flag (sibling/spouse) is an indication, but not fully conclusive. Maybe we could try SibSp and gender. I leave that as an exercise for you to pursue.</p><p>If we do a crosstab on the age, we'll get a long table for each number, as shown here:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_010.jpg" /></div><p>
</p><p>So we need to find a better scheme. And of course, this is easy with the following code:</p><pre class="programlisting">val ageDist =  passengers1.select(passengers1("Survived"), (passengers1("age") - passengers1("age") % 10).cast("int").as("AgeBracket"))
ageDist.show(3)
ageDist.stat.crosstab("Survived","AgeBracket").show()
</pre><p>If you look at the code closely, you will see that operations such as <code class="literal">%</code> and <code class="literal">-</code> are not normal Scala operations, but rather those from <code class="literal">sql.Column</code>. This is where the <code class="literal">Column </code>object and the operations defined there come in handy.</p><p>So let's create a new <code class="literal">ColumnAgeBracket </code>and a new Dataset with <code class="literal">AgeBracket</code> and the <code class="literal">Survived </code>columns, then crosstab <code class="literal">AgeBracket</code> with <code class="literal">Survived</code>. The output looks much better:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_011.jpg" /></div><p>
</p><p>Lots of people in the 20s and 30s, which means about 50 percent of the people survived. There are lots of people whose age is null.</p><p>Anyway, in short, the Spark 2.0.0 Dataset did pass the data wrangling test with flying colors! With a few lines of code, we were able to get some answers. In <a class="link" href="#" linkend="ch11">Chapter 11</a>, <span class="emphasis"><em>Machine Learning with Spark ML Pipelines</em></span>, we will explore whether we can predict survival using the decision tree machine learning algorithm.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec46"></a>Scientific functions</h3></div></div></div><p>Unlike the <code class="literal">na</code> and <code class="literal">stat</code> functions, scientific functions are not mapped. So we have to import them from <code class="literal">sql.functions</code> and then invoke them. Scientific functions include <code class="literal">log()</code>, <code class="literal">log10()</code>, <code class="literal">sqrt()</code>, <code class="literal">cbrt()</code>,<code class="literal">exp()</code>, <code class="literal">pow()</code>, <code class="literal">sin()</code>, <code class="literal">cos()</code>, <code class="literal">tan()</code>, <code class="literal">acos()</code>, <code class="literal">asin()</code>, <code class="literal">atan()</code>, <code class="literal">toDegrees()</code>, and <code class="literal">toRadians()</code>. Some interesting functions include <code class="literal">expm1(&lt;columnName&gt;)</code>, which computes the exponent of value-1, <code class="literal">log10p()</code> , the log of value plus 1 (presumably to accommodate when the value is 0), and <code class="literal">hypot()</code>, the hypotenuse. All these functions take column names or a column object, which is very versatile in that sense. Let's see some quick examples.</p><p>We will use the <code class="literal">log</code>, <code class="literal">log10</code>, <code class="literal">sqrt</code>, and <code class="literal">hypo</code> functions. We need to import them:</p><pre class="programlisting">importorg.apache.spark.sql.functions.{log,log10,sqrt,hypot}
</pre><p>Next, create an RDD and then a Dataset out of it:</p><pre class="programlisting">val aList : List[Int] = List(10,100,1000)
varaRDD = spark.sparkContext.parallelize(aList)
val sqlContext = spark.sqlContext
importsqlContext.implicits._
val ds = spark.createDataset(aRDD)
ds.show()
</pre><p>Then, run the functions on the Dataset:</p><pre class="programlisting">ds.select( ds("value"), log(ds("value")).as("ln")).show()
ds.select( ds("value"), log10(ds("value")).as("log10")).show()
ds.select( ds("value"), sqrt(ds("value")).as("sqrt")).show()
</pre><p>Of course, these are functions from the SQL package, not from normal Scala functions.</p><p>The code is <code class="literal">DS05.scala</code>. We load the file and run the object <code class="literal">DS05</code>. The result looks like this:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_012.jpg" /></div><p>
</p><p>The log will indicate null if the value of a row is <span class="strong"><strong>0</strong></span>:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_013.jpg" /></div><p>
</p><p>In such cases, <code class="literal">log1p()</code> comes in handy. This is good for machine learning formulas. Many formulas add the value 1 before the log, that is, <code class="literal">log(x+1)</code>, to take care of when <code class="literal">x = 0</code>:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_014.jpg" /></div><p>
</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip56"></a>Tip</h3><p>For example, the NFL ELO Algorithm's [NFL-ELO, 2015] Margin Of Victory Multiplier is calculated as <span class="emphasis"><em>ln(abs(PointDifference)+1)</em></span>, which becomes <span class="strong"><strong>0</strong></span> when Pd=0. This is fine because when score the difference between the two teams is 0, a draw, their respective ELOs don't change (maybe the weaker team should get a small bump up of their ELO).</p></div><p>Next, we read a file that has two sides of triangles and then run the <code class="literal">hypot</code> function:</p><pre class="programlisting">val filePath = "/Users/ksankar/fdps-v3/"
val data = spark.read.option("header","true").
option("inferSchema","true").
csv(filePath + "data/hypot.csv")
println("Data has "+data.count()+" rows")
data.show(5)
data.printSchema()
    //
data.select( data("X"),data("Y"),hypot(data("X"),data("Y")).as("hypot") ).show()
</pre><p>The result is as expected. We have six perfect Pythagorean triples:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_015.jpg" /></div><p>
</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec47"></a>Data wrangling with Datasets</h3></div></div></div><p>Now that we have covered Dataset API and how to use them in the abstract, let's take a small Dataset, apply our new skills, and extend the techniques we have learned. The <span class="strong"><strong>Northwind Sales</strong></span> data has orders, order details, and products. Let's ask some questions and see how succinctly the Spark Dataset can answer them. The questions we need answers for are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>How many orders were placed by each customer?</p></li><li style="list-style-type: disc"><p>How many orders were placed in each country?</p></li><li style="list-style-type: disc"><p>How many orders were placed for each month/year?</p></li><li style="list-style-type: disc"><p>What is the total number of sales for each customer, year-wise?</p></li><li style="list-style-type: disc"><p>What is the average order by customer, year-wise?</p></li></ul></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl3sec17"></a>Reading data into the respective Datasets</h4></div></div></div><p>Our first mission, if we have to choose one, will be to read data into the respective Datasets. This is probably the easiest part; we have the versatile <code class="literal">read.csv()</code>file to do this.</p><p>The code is in <code class="literal">DS06.scala</code>. It has the code for all the steps, but we will work through the details step by step.</p><p>The read is easy, so we will just inspect the results. We have 830 orders and 2,155 order details. The schema looks fine for both the Datasets:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_016.jpg" /></div><p>
</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl3sec18"></a>Aggregate and sort</h4></div></div></div><p>If we go back to the questions we saw earlier, the first two are not that hard. The <code class="literal">groupBy()</code>, <code class="literal">count()</code>, and <code class="literal">sort()</code>method will do the trick. So let's get the easy part over and done with:</p><pre class="programlisting">val orderByCustomer= orders.groupBy("CustomerID").count()
orderByCustomer.sort(orderByCustomer("count").desc).show(5) // We have out ans#1
//
val orderByCountry= orders.groupBy("ShipCountry").count()
orderByCountry.sort(orderByCountry("count").desc).show(5) // ans#2
</pre><p>As you can see, we use the column object's<code class="literal"> desc </code>property to sort by descending order. The results are as expected.</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_017.jpg" /></div><p>
</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl3sec19"></a>Date columns, totals, and aggregations</h4></div></div></div><p>Now for the rest of the questions, we need to do a few aggregations and also create a couple of new columns. The steps are as follows:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Add the <code class="literal">OrderTotal</code> column to the Orders Dataset. Here are the steps to do this:
</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Add the line total to the order details.</p></li><li><p>Aggregate the total using the <code class="literal">orderID</code>.</p></li><li><p>Join the order details and orders to add the order total.</p></li><li><p>Check whether there are any null columns.</p></li></ol></div><p>
</p></li><li><p>Add a date column.</p></li><li><p>Add the month and year.</p></li></ol></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch09lvl4sec0"></a>The OrderTotal column</h5></div></div></div><p>We have orders and order lines with the amount, quantity, and discount. We need to add an order total column to the orders table.</p><p>First, calculate the total for each line:</p><pre class="programlisting">val orderDetails1 = orderDetails.select(orderDetails("OrderID"),
((orderDetails("UnitPrice") * orderDetails("Qty")) -
((orderDetails("UnitPrice") * orderDetails("Qty")) * orderDetails("Discount")
)).as("OrderPrice"))
orderDetails1.show(5)
</pre><p>There are a lot of parentheses, but other than this the code is straightforward. Remember that the operations are actually performed on the columns. The output is as expected:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_018.jpg" /></div><p>
</p><p>Now we can aggregate by the <code class="literal">orderID</code>, resulting in a Dataset that has the total for each order:</p><pre class="programlisting">val orderTot = orderDetails1.groupBy("OrderID").sum("OrderPrice").alias("OrderTotal")
orderTot.sort("OrderID").show(5)
</pre><p>The output is fine:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_019.jpg" /></div><p>
</p><p>It is easy to add the order total to the orders Dataset by joining the aggregated Dataset:</p><pre class="programlisting">val orders1 = orders.join(orderTot, orders("OrderID").equalTo(orderTot("OrderID")), "inner")
      .select(orders("OrderID"),
orders("CustomerID"),
orders("OrderDate"),
orders("ShipCountry").alias("ShipCountry"),
orderTot("sum(OrderPrice)").alias("Total"))
    //
orders1.sort("CustomerID").show()
    //
    // # 1.4. Check if there are any null columns
orders1.filter(orders1("Total").isNull).show()
    //
</pre><p>The join does the magic, and there are no nulls:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_020.jpg" /></div><p>
</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch09lvl4sec1"></a>Date operations</h5></div></div></div><p>Date manipulation is much easier with 2.0.0 Datasets; they have very versatile conversion functions, which act on a column. We convert the order date to the date type and then extract the <code class="literal">Year</code> and <code class="literal">Month</code> columns:</p><pre class="programlisting">val orders2 =     orders1.withColumn("Date",to_date(orders1("OrderDate")))
  orders2.show(2)
  orders2.printSchema()
    //
    // # 3. Add month and year
val orders3 =  orders2.withColumn("Month",month(orders2("OrderDate"))).withColumn("Year",year(orders2("OrderDate")))
  orders3.show(2)
</pre><p>The <code class="literal">withColumn </code>function is very useful. You get a new Dataset with all the original columns plus the new column(s).</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip57"></a>Tip</h3><p>Be careful with the <code class="literal">to_date()</code> function. It takes the date in the yy-mm-dd format. Anything else doesn't work. Hopefully they will add a from-to format to specify the format of the string that we want to convert from.</p></div><p>The output is as expected:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_021.jpg" /></div><p>
</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl3sec20"></a>Final aggregations for the answers we want</h4></div></div></div><p>Now that we have the Dataset with all the required columns, it is a question of applying the right aggregations:</p><pre class="programlisting">// Q 3. How many orders by month/year ?
val ordersByYM = orders3.groupBy("Year","Month").sum("Total").as("Total")
  ordersByYM.sort(ordersByYM("Year"),ordersByYM("Month")).show()
</pre><p>Because we want year-wise and month-wise data, we group the data using two columns and sum the order total. To display the information, we can sort the data using two columns:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_022.jpg" /></div><p>
</p><p>The yearly total sales for each customer are a simple aggregation:</p><pre class="programlisting">// Q 4. Total Sales for each customer by year
var ordersByCY = orders3.groupBy("CustomerID","Year").sum("Total").as("Total")
ordersByCY.sort(ordersByCY("CustomerID"),ordersByCY("Year")).show()
</pre><p>Once we know the trick, aggregation by another set of columns is just routine:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_023.jpg" /></div><p>
</p><p>The average total sales for each customer is another simple symmetric aggregation operation:</p><pre class="programlisting">// Q 5. Average order by customer by year
ordersByCY = orders3.groupBy("CustomerID","Year").avg("Total").as("Total")
ordersByCY.sort(ordersByCY("CustomerID"),ordersByCY("Year")).show()
</pre><p>The fifth question is very similar to the fourth question, but we want the average, not the total. Again, this can be done using an easy column operation:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_024.jpg" /></div><p>
</p><p>Finally, the sixth question can be solved by just dropping the year and calculating the average for each customer:</p><pre class="programlisting">// Q 6. Average order by customer
val ordersCA = orders3.groupBy("CustomerID").avg("Total").as("Total")
ordersCA.sort(ordersCA("avg(Total)").desc).show()
</pre><p>The output is as expected:</p><p>
</p><div class="mediaobject"><img src="graphics/image_09_025.jpg" /></div><p>
</p></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec66"></a>References</h2></div></div><hr /></div><p>Here are some links you can refer to for more information:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<a class="ulink" href="https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html" target="_blank">https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html</a>, which provides information on three Apache Spark APIs, RDDs, DataFrames, and Datasets; when to use them; and why to use them.</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://www.slideshare.net/databricks/spark-summit-san-francisco-2016-matei-zaharia-keynote-apache-spark-20" target="_blank">http://www.slideshare.net/databricks/spark-summit-san-francisco-2016-matei-zaharia-keynote-apache-spark-20</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="https://databricks.com/blog/2016/05/11/spark-2-0-technical-preview-easier-faster-and-smarter.html" target="_blank">https://databricks.com/blog/2016/05/11/spark-2-0-technical-preview-easier-faster-and-smarter.html</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="https://doubleclix.wordpress.com/2015/01/20/the-art-of-nfl-ranking-the-elo-algorithm-and-fivethirtyeight/" target="_blank">https://doubleclix.wordpress.com/2015/01/20/the-art-of-nfl-ranking-the-elo-algorithm-and-fivethirtyeight/</a>
</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec67"></a>Summary</h2></div></div><hr /></div><p>This was an interesting chapter. Finally, we got to work with Dataset APIs, using real data. We also got a glimpse of API organization. Datasets and their associated classes have a lot of interesting functions for you to explore. Python APIs are very much similar to Scala APIs and sometimes a little easier. The IPython notebook is available at <a class="ulink" href="https://github.com/xsankar/fdps-v3/blob/master/extras/003-DataFrame-For-DS.ipynb" target="_blank">https://github.com/xsankar/fdps-v3/blob/master/extras/003-DataFrame-For-DS.ipynb</a>. Data wrangling with Python, and especially with Python notebooks, is the preferred way for data scientists.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch10"></a>Chapter 10. Spark with Big Data</h2></div></div></div><p>As we mentioned in <a class="link" href="#" linkend="ch08">Chapter 8</a>, <span class="emphasis"><em>Spark SQL</em></span>, the big data compute stack doesn't work in isolation. Integration points across multiple stacks and technologies are essential. In this chapter, we will look at how Spark works with some of the big data technologies that are part of the Hadoop ecosystem. We will cover the following topics in this chapter:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Parquet</strong></span>: This is an efficient storage format</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>HBase</strong></span>: This is the database in the Hadoop ecosystem</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec68"></a>Parquet - an efficient and interoperable big data format</h2></div></div><hr /></div><p>We explored the Parquet format in <a class="link" href="#" linkend="ch07">Chapter 7</a>,<span class="emphasis"><em> Spark 2.0 Concepts</em></span>. To recap, Parquet is essentially an interoperable storage format. Its main goals are space efficiency and query efficiency. Parquet's origin is based on Google's Dremel and was developed by Twitter and Cloudera. It is now an Apache incubator project. The nested storage format from Google Dremel is implemented in Parquet. It stores data in a columnar format and has an evolvable schema. This enables you to optimize queries (it can restrict the columns that you need to access and so you need not bring all the columns into the memory and discard the ones not needed), and it allows storage optimization (by decoding at the column level, which gives a much higher compression ratio). Another interesting feature is that Parquet can store nested Datasets. This feature can be leveraged in curated data lakes to store subject-based data. In addition to the ability to restrict column fetches during queries, Parquet 2.0 could implement push-down predicates. At the time of writing, the current Parquet version is 1.6.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec48"></a>Saving files in the Parquet format</h3></div></div></div><p>In <a class="link" href="#" linkend="ch08">Chapter 9</a>, <span class="emphasis"><em>Spark SQL</em></span>, we loaded the <code class="literal">Orders</code> tables from the <code class="literal">.csv</code> format. Let's save the data in the Parquet format. Usually, one would take a <code class="literal">.csv</code> file, do transformations, and then store it in the Parquet format (for example, the <code class="literal">Sales By Country</code> Dataset that we created). The relevant part of the code is shown as follows. The full source is available at <code class="literal">fdps-v3/code/BigData01v2-sshell.scala</code>:</p><pre class="programlisting">//
// Parquet Operations
//
orders.write.parquet(filePath + "Orders_Parquet")
</pre><p>It is that simple! With Spark 2.0.0, all of the write is consolidated and so we use the <code class="literal">write.parquet()</code> method for writing Parquet files. The methods, such as <code class="literal">write.csv()</code>, <code class="literal">write.json()</code>, and <code class="literal">write.jdbc()</code> help us save data in other formats.</p><p>
</p><div class="mediaobject"><img src="graphics/image_10_001.jpg" /></div><p>
</p><p>
<code class="literal">INFO</code> messages show us some of the inner details:</p><p>
</p><div class="mediaobject"><img src="graphics/image_10_002.jpg" /></div><p>
</p><p>Even though we store the Parquet file in the local filesystem in this example, in an actual production system we would need to use HDFS to store the files. We can inspect the log entries and see that it has started a job with the <code class="literal">ParquetTableOperations</code> class. The scheme used to save this is <span class="strong"><strong>Run Length Encoding</strong></span> (<span class="strong"><strong>RLE</strong></span>). As you can see, we only need a couple of lines of code, and Spark does all of the hard work under the covers. It creates a directory, data, and metadata files underneath the main directory. It has created one (compressed) file, corresponding to one job for one partition, as shown next:</p><p>
</p><div class="mediaobject"><img src="graphics/image_10_003.jpg" /></div><p>
</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec49"></a>Loading Parquet files</h3></div></div></div><p>Now let's load the <code class="literal">Orders</code> Parquet files and see whether the data got saved correctly. The code, again, is deceptively simple, as shown here:</p><pre class="programlisting">val parquetOrders = spark.read.parquet(filePath + "Orders_Parquet")</pre><p>We use the <code class="literal">read.parquet()</code> method in the <code class="literal">SparkSession</code> object:</p><p>
</p><div class="mediaobject"><img src="graphics/image_10_004.jpg" /></div><p>
</p><p>As you can see, the first few lines create all of the scaffolding and required definitions. The lazy evaluation does not do anything unless we ask for some action, such as <code class="literal">show(3)</code>. Spark does all the work reading the data. You can see that Spark figured out that there is a single file to process along with the field names and their types.</p><p>Note that you cannot overwrite a Parquet file, as shown here:</p><p>
</p><div class="mediaobject"><img src="graphics/image_10_005.jpg" /></div><p>
</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec50"></a>Saving processed RDDs in the Parquet format</h3></div></div></div><p>Now let's save our <code class="literal">SalesByCountry</code> report in the Parquet format. As we saw earlier, it is very simple and streamlined. We create views to the orders and order detail Datasets, run a SQL query, and write the resulting Dataset:</p><pre class="programlisting">//
// Create views for tables
//
orders.createOrReplaceTempView("OrdersTable")
orderDetails.createOrReplaceTempView("OrderDetailsTable")
val result = spark.sql("SELECT ShipCountry, SUM(OrderDetailsTable.UnitPrice * Qty * Discount) AS ProductSales FROM OrdersTable INNER JOIN OrderDetailsTable ON OrdersTable.OrderID = OrderDetailsTable.OrderID GROUP BY ShipCountry")
result.show(3)
result.write.parquet(filePath + "SalesByCountry_Parquet")
</pre><p>By now, we know the drill; as expected, the files are created, as shown next:</p><p>
</p><div class="mediaobject"><img src="graphics/image_10_006.jpg" /></div><p>
</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec69"></a>HBase</h2></div></div><hr /></div><p>HBase is the NoSQL datastore in the Hadoop ecosystem. Integration with a database is essential for Spark. It can read data from an HBase table or write to one. In fact, Spark supports HBase very well via the <code class="literal">HadoopdataSet</code> calls.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip58"></a>Tip</h3><p>If you want to experiment with HBase, you can install a standalone local version of HBase, as described in <a class="ulink" href="http://hbase.apache.org/book.html#quickstart" target="_blank">http://hbase.apache.org/book.html#quickstart</a>.</p></div><p>Before working through the examples, let's create a table and three records in HBase. For testing, you can install a local standalone version of HBase that works from the local filesystem. So there's no need for Hadoop or HDFS. However, this won't be suitable for production.</p><p>I created a <code class="literal">test</code> table with three records via the HBase shell, as shown in the following screenshot:</p><p>
</p><div class="mediaobject"><img src="graphics/image_10_007.jpg" /></div><p>
</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec51"></a>Loading from HBase</h3></div></div></div><p>The HBase test code in the Apache Spark examples is a good start to testing our HBase connectivity and loading data. The code is not that difficult, but we do need to keep track of the data types, that is, keys as bytes, values as strings, and so on. The relevant part of the test code is given here (the full source file is <code class="literal">fdps-v3/code/BigData02.scala</code>):</p><pre class="programlisting">val sc = new SparkContext("local","Chapter 10")
println(s"Running Spark Version ${sc.version}")
//
val conf = HBaseConfiguration.create()
conf.set(TableInputFormat.INPUT_TABLE, "test")

val admin = new HBaseAdmin(conf)
println(admin.isTableAvailable("test"))

val hBaseRDD = sc.newAPIHadoopRDD(conf, classOf[TableInputFormat],
classOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable],
classOf[org.apache.hadoop.hbase.client.Result])
  println(hBaseRDD.count())
  //
  hBaseRDD.foreach(println) // will print bytes
  hBaseRDD.foreach(e=&gt; ( println("%s | %s |".format( Bytes.toString(e._1.get()),e._2) ) ) )
  //
println("** Read Done **")
</pre><p>The output of this code is shown in the following screenshot:</p><p>
</p><div class="mediaobject"><img src="graphics/image_10_008.jpg" /></div><p>
</p><p>This is just the starting point. You need to convert the bytes from HBase to the actual data types of your data structures. You need to experiment a bit to get it right.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec52"></a>Saving to HBase</h3></div></div></div><p>Now let's store a new record in our test table: key as <code class="literal">row4</code> and value as <code class="literal">value4</code>. It does require a few more classes and manipulations but nothing fancy, as shown here:</p><pre class="programlisting">//
// create a pair RDD "row4":"value4"
// save it in column family "d"
//
val testMap = Map("row4" -&gt; "value4")
val pairs = sc.parallelize(List(("row4","value4")))
pairs.foreach(println)
//
//Function to convert our RDD to the required format for HBase
//
def convert(triple: (String, String)) = {
  val p = new Put(Bytes.toBytes(triple._1))
  p.add(Bytes.toBytes("cf"), Bytes.toBytes("d"), Bytes.toBytes(triple._2))
  (new org.apache.hadoop.hbase.io.ImmutableBytesWritable, p)
}
//
val jobConfig: JobConf = new JobConf(conf, this.getClass)
jobConfig.setOutputFormat(classOf[TableOutputFormat])
jobConfig.set(TableOutputFormat.OUTPUT_TABLE, "test")
//
new PairRDDFunctions(pairs.map(convert)).saveAsHadoopDataset (jobConfig)
//
println("** Write Done **")
</pre><p>The program runs and prints the output, as shown in the following screenshot:</p><p>
</p><div class="mediaobject"><img src="graphics/image_10_009.jpg" /></div><p>
</p><p>Now let's go back to the HBase shell and verify that the fourth record is added, as shown in the following screenshot:</p><p>
</p><div class="mediaobject"><img src="graphics/image_10_010.jpg" /></div><p>
</p><p>Good! We can see the fourth record and a later timestamp.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec53"></a>Other HBase operations</h3></div></div></div><p>We can also get metadata about the HBase server and the environment, as shown here:</p><pre class="programlisting">val status = admin.getClusterStatus();
println("HBase Version : " +status.getHBaseVersion())
println("Average Load : "+status.getAverageLoad())
println("Backup Master Size : " + status.getBackupMastersSize())
println("Balancer On : " + status.getBalancerOn())
println("Cluster ID : "+ status.getClusterId())
println("Server Info : " + status.getServerInfo())
</pre><p>The output prints out the details, as you can see in the following screenshot:</p><p>
</p><div class="mediaobject"><img src="graphics/image_10_011.jpg" /></div><p>
</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec70"></a>Reference</h2></div></div><hr /></div><p>The references are listed here:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<a class="ulink" href="https://github.com/apache/spark/tree/master/examples/src/main/scala/org/apache/spark/examples" target="_blank">https://github.com/apache/spark/tree/master/examples/src/main/scala/org/apache/spark/examples</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://parquet.incubator.apache.org/documentation/latest/" target="_blank">http://parquet.incubator.apache.org/documentation/latest/</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://www.slideshare.net/cloudera/hadoop-summit-36479635?ref=http://parquet.incubator.apache.org/presentations/" target="_blank">http://www.slideshare.net/cloudera/hadoop-summit-36479635?ref=http://parquet.incubator.apache.org/presentations/</a>
</p></li><li style="list-style-type: disc"><p>Google Dremel paper at <a class="ulink" href="http://research.google.com/pubs/pub36632.html" target="_blank">http://research.google.com/pubs/pub36632.html</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="https://blog.twitter.com/2013/dremel-made-simple-with-parquet" target="_blank">https://blog.twitter.com/2013/dremel-made-simple-with-parquet</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://planetcassandra.org/getting-started-with-apache-spark-and-cassandra/" target="_blank">http://planetcassandra.org/getting-started-with-apache-spark-and-cassandra/</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://blog.cloudera.com/blog/2014/12/new-in-cloudera-labs-sparkonhbase/" target="_blank">http://blog.cloudera.com/blog/2014/12/new-in-cloudera-labs-sparkonhbase/</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://www.vidyasource.com/blog/Programming/Scala/Java/Data/Hadoop/Analytics/2014/01/25/lighting-a-spark-with-hbase" target="_blank">http://www.vidyasource.com/blog/Programming/Scala/Java/Data/Hadoop/Analytics/2014/01/25/lighting-a-spark-with-hbase</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/HBaseTest.scala" target="_blank">https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/HBaseTest.scala</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="https://federicodayan.wordpress.com/2010/09/28/hbase-textgetbytes-and-immutablebyteswritabletostring/" target="_blank">https://federicodayan.wordpress.com/2010/09/28/hbase-textgetbytes-and-immutablebyteswritabletostring/</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="https://github.com/apache/parquet-format" target="_blank">https://github.com/apache/parquet-format</a>
</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec71"></a>Summary</h2></div></div><hr /></div><p>This chapter focused on the integration of Spark with other big data technologies. The Parquet format is an excellent way to expose the data processed by Spark to external systems, and Impala makes this very easy. The advantage of the Parquet format is that it is very efficient in terms of storage and expressive enough to capture the schema. We also looked at the process of interfacing with HBase. Thus, we can have our cake and eat it too! This means that we can leverage Spark for distributed scalable data processing, without losing the capability to integrate with other big data technologies. The next chapter, probably my favorite, is about machine learning. We will explore ML pipelines.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch11"></a>Chapter 11. Machine Learning with Spark ML Pipelines</h2></div></div></div><p>One of the major attractions of Spark is its ability to scale computations massively, and this is exactly what you need for machine learning algorithms. But the caveat is that all machine learning algorithms cannot be effectively parallelized. Each algorithm has its own challenges for parallelization, whether it is task parallelism or data parallelism. Having said that, Spark is becoming the de-facto platform for building machine learning algorithms and applications. Spark 2.0.0 has come a long way since version 1.1.0, with more algorithms and interesting APIs. For the latest information on this, you can refer to the Spark site at <a class="ulink" href="https://spark.apache.org/docs/latest/ml-guide.html" target="_blank">https://spark.apache.org/docs/latest/ml-guide.html</a>, which is the authoritative source.</p><p>In this chapter, we will first cover machine learning interfaces and organization, including the new ml pipeline, which has become mainstream in 2.0.0. Then, we will delve into the following machine learning algorithms:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Basic statistics</p></li><li style="list-style-type: disc"><p>Linear regression</p></li><li style="list-style-type: disc"><p>Classification</p></li><li style="list-style-type: disc"><p>Clustering</p></li><li style="list-style-type: disc"><p>Recommendation</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch11lvl1sec72"></a>Spark's machine learning algorithm table</h2></div></div><hr /></div><p>Apache Spark covers a wide spectrum of machine learning algorithms. The algorithms implemented in Spark 2.0.0 consist of packages: <code class="literal">org.apache.spark.ml</code> for Scala and Java and <code class="literal">pyspark.ml</code> for Python.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip59"></a>Tip</h3><p>Prior to 1.6.0, the libraries were in the <code class="literal">org.apache.spark.mllib</code> and <code class="literal">pyspark.mllib</code> packages, but from 2.0, the MLlib APIs are in maintenance mode. So you should use the ML APIs. In this chapter, we will do so, with clarifying notes wherever needed.</p></div><p>The following table summarizes the machine learning algorithms and data transformation features available in Spark 2.0.0:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<span class="strong"><strong>Algorithm</strong></span>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<span class="strong"><strong>Feature</strong></span>
</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>
<span class="strong"><strong>Notes</strong></span>
</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Basic statistics</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Summary statistics</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Here mean, stdev, count, max, min, and numNonZeros are all part of <code class="literal">dataframe.count()</code>, <code class="literal">dataframe.describe()</code>, and sql.functions</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Correlations and covariance</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Here, sql.functions are invoked as <code class="literal">dataframe.stat.corr</code>(0 and cov)</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Stratified sampling</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This provides two methods, <code class="literal">sampleBykey</code> and <code class="literal">sampleByKeyExact</code>, with and without replacement</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Hypothesis testing</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This is to test the statistical significance, Pearson's <span class="emphasis"><em>chi-squared</em></span> test for goodness of fit and independence tests</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Tests of data streams for A/B testing</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Since 1.6.0, these are used to check the capability of both Welch's and Student's respective two-sample t-test over streams</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Random data generation</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Generate data using functions such as Normal, Poisson, Exponential, gamma, and lognormal</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Regression</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Linear models</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This includes linear regression: Least Square, Lasso, and the Ridge regression</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Classification</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Binary classification</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This includes logistic regression, SVM, decision trees, random forests, gradient boosted trees, and naïve Bayes</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Multiclass classification</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This includes logistic, decision trees, naïve Bayes, and random forests</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Recommendation</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Collaborative filtering</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This includes alternating between least squares</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Clustering</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>K-means</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This includes K-means, Gaussian Mixture, and Streaming K-means</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Dimensionality Reduction</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>SVD and PCA</p>
</td><td style="border-bottom: 0.5pt solid ; ">
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Feature Extraction and Transformation</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
</p><p>TF-IDF, Word2Vec</p><p>
</p><p>StandardScaler, andNormalizer</p><p>
</p>
</td><td style="border-bottom: 0.5pt solid ; ">
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Optimization</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Gradient Descent, SGD, and L-BFGS</p>
</td><td style="border-bottom: 0.5pt solid ; ">
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Frequent pattern mining</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>The FP-growth Algorithm</p>
</td><td style="border-bottom: 0.5pt solid ; ">
</td></tr><tr><td style="border-right: 0.5pt solid ; ">
<p>Model import/export</p>
</td><td style="border-right: 0.5pt solid ; ">
</td><td style="">
</td></tr></tbody></table></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch11lvl1sec73"></a>Spark machine learning APIs - ML pipelines and MLlib</h2></div></div><hr /></div><p>Until around 1.6.0, the north-facing data abstraction method was RDD, and the MLlib APIs implemented machine learning on RDDs. MLlib was introduced in Spark 0.8 and, for the most part, were straightforward library calls to ML algorithms; however, this didn't reflect the data pipelines inherent in machine learning. With the advent of DataFrames and Datasets, MLlib transformed as well with more capabilities, and the resulting framework is the ML pipeline.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip60"></a>Tip</h3><p>MLlib APIs are in maintenance mode from 2.0.0 and will be deprecated in 3.0.0. But be aware that there are still some APIs that are not migrated to the ML world; for example, the random generator still outputs an RDD. So you will have to use MLlib to generate a random normal, poisson, or other distributions, then convert the RDD into a DataFrame, and finally use ML for transformation and machine learning algorithms.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch11lvl1sec74"></a>ML pipelines</h2></div></div><hr /></div><p>ML pipelines were developed to address the fact that machine learning is not just a bunch of algorithms, such as classification and regression, but a pipeline of actions performed over a Dataset. Let's take a quick look at the tasks involved in a typical machine learning process. The following figure shows the top-level activities:</p><p>
</p><div class="mediaobject"><img src="graphics/image_11_001.jpg" /></div><p>
</p><p>The first step is to get some data for the data science work. If you are using internal data, the data should be made anonymous and all PII information purged.</p><p>Once we have the data, we'll transform it: for example, we can convert a comma-separated CSV format into a DataFrame consisting of strings and numbers.</p><p>Then we extract the features that can be used to train our machine learning models. The feature extraction can be as simple as separating lines into words or normalizing words, such as deleting special characters and converting words to lowercase. This might also involve turning columns into categories, for example, Yes/No to 1/0 or Survived/Dead to 1/0.</p><p>Once we have the features, the next step is to split them into training and testing sets. Usually, it is a 80-20 random split; that is, we train the model using 80 percent of the data and evaluate the model with the remaining 20 percent of the data. But it might not be so easy. If our data is related to time series, a random split will, not work. If the data has a high-class imbalance (that is, 99% of the data belongs to one class-as in the case of data about rare diseases), a different strategy will be required, something that takes the class distribution into consideration.</p><p>The training itself has multiple steps:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>First we try different models and fit the training data into the algorithms to develop the models.</p></li><li><p>Different algorithms have parameters: batch size, number of runs, tolerances, and so forth. So even while developing the models, we need to tune the hyper parameters.</p></li><li><p>Finally, we might try different algorithms and choose the best one that fits the problem.</p></li></ol></div><p>The tuning and model selection are done via validation.</p><p>Once the model is selected with the optimized parameters, the test data is run with the model, which gives the performance of the model.</p><p>The model is then deployed into production where it uses the actual runtime data for predictions.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip61"></a>Tip</h3><p>Bear in mind that the preceding paragraph is a generalization of a vast body of work with underlying mathematics, statistics, and heuristics from practical experience.</p></div><p>MLlib mainly focuses on the fit-models box, and the new ML pipelines address all parts of the workflow. Let's take a quick table view of the ML pipeline capabilities with respect to the machine learning workflow.</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<span class="strong"><strong>Workflow Stage</strong></span>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<span class="strong"><strong>ML Feature</strong></span>
</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>
<span class="strong"><strong>Notes</strong></span>
</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<span class="strong"><strong>Transfom</strong></span>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>org.apache.spark.ml.feature</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Uses SQL functions such <span class="strong"><strong>assplit</strong></span>
</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<span class="strong"><strong>Extract Features</strong></span>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>org.apache.spark.ml.feature</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Uses Binarizer, Bucketizer, Normalizer, PAC, oneHotEncoder, Tokenizer, StandardScaler, StopWordsRemover, and Word2Vec</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<span class="strong"><strong>Split Data</strong></span>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>org.apache.spark.ml.tuning
org.apache.spark.sql.functions.split</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Uses TrainValidationSplit</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<span class="strong"><strong>Fit Model</strong></span>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>org.apache.spark.ml.regression, org.apache.spark.ml.recommendation</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>The machine learning algorithms live here</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<span class="strong"><strong>Model Optimization</strong></span>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>org.apache.spark.ml.tuning</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This is done through CrossValidator, and ParamGridBuilder</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>
<span class="strong"><strong>Model Evaluation</strong></span>
</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>org.apache.spark.ml.evaluation.RegressionEvaluator</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This calculates RMSE and MSE</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; ">
<p>
<span class="strong"><strong>Model Deployment</strong></span>
</p>
</td><td style="border-right: 0.5pt solid ; ">
<p>org.apache.spark.ml.util</p>
</td><td style="">
<p>Uses MLReader and MLWriter</p>
</td></tr></tbody></table></div><p>As you can see, ML pipelines have extensive capabilities that address the machine learning workflow.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip62"></a>Tip</h3><p>As the goal of this chapter is to provide a basic introduction, we won't go deeper into pipeline construction and other aspects. But we will use the ML patterns, transformer, feature extraction, model evaluation, and others, as we work through the various algorithms. We will point out the patterns as we encounter them in our examples.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch11lvl1sec75"></a>Spark ML examples</h2></div></div><hr /></div><p>Now, let's look at how to develop machine learning applications. Naturally, we need interesting Datasets to implement the algorithms; we will use appropriate Datasets for the algorithms shown in the next section. In this book, we will use Scala, but I have included <span class="strong"><strong>iPython</strong></span> notebooks for the algorithm examples in Python.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note63"></a>Note</h3><p>The code and data files are available in the GitHub repository at <a class="ulink" href="https://github.com/xsankar/fdps-v3" target="_blank">https://github.com/xsankar/fdps-v3</a>. Well keep it updated with the corrections.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch11lvl1sec76"></a>The API organization</h2></div></div><hr /></div><p>As an introduction, the following figure gives you a bird's eye view of the classes and methods that are relevant. Sometimes, one gets lost in the numerous classes and deep hierarchies.</p><p>
</p><div class="mediaobject"><img src="graphics/image_11_002.jpg" /></div><p>
</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip64"></a>Tip</h3><p>This organization is definitely something you should get used to. The MLlib library was more straightforward, but with far fewer capabilities. ML is definitely better, once you get the hang of it. It took me a few days to convert the examples; I had to visit/revisit the documentation multiple times. That is why I created the preceding diagram; this should make it easier for you to get a quick roadmap of the process while learning. Once done, it becomes easier to navigate the API documentation and find what one needs. Mastering the pipeline correctly will take a little while.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch11lvl1sec77"></a>Basic statistics</h2></div></div><hr /></div><p>Let's read the car mileage data and then compute some basic statistics. In Spark 2.0.0, DataFrameReader has the capability to read CSV files and create Datasets. And the Dataset has the <code class="literal">describe()</code> function, which calculates the count, mean, standard deviation, min, and max values. For correlation and covariance, we use the <code class="literal">stat.corr()</code> and <code class="literal">stat.cov()</code> methods. Spark 2.0.0 Datasets have made our statistics work a lot easier.</p><p>Now let's run the program, parse the code, and compare the results.</p><p>The code files are in <code class="literal">fdps-v3/code</code> and the data files in <code class="literal">fdps-v3/data</code>. You can run the code either from a Scala IDE or just from the Spark shell startup.</p><p>Start the Spark shell from the <code class="literal">bin</code> directory where you have installed Spark:</p><pre class="programlisting">
<span class="strong"><strong>/Volumes/sdxc-01/spark-2.0.0/bin/spark-shell</strong></span>
</pre><p>Inside the shell, you'll find this command:</p><pre class="programlisting">
<span class="strong"><strong>load /Users/ksankar/fdps-v3/code/ML01v2.scala</strong></span>
</pre><p>This command loads the source as follows:</p><p>
</p><div class="mediaobject"><img src="graphics/image_11_003.jpg" /></div><p>
</p><p>It creates the <code class="literal">ML01v2</code> object. To run the object, use the following command:</p><pre class="programlisting">
<span class="strong"><strong>ML01v2.main(Array("Hello","World"))</strong></span>
</pre><p>The result will be like this:</p><p>
</p><div class="mediaobject"><img src="graphics/image_11_004.jpg" /></div><p>
</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec54"></a>Loading data</h3></div></div></div><p>Loading data is a lot easier with the <code class="literal">read.csv</code> file and Datasets:</p><pre class="programlisting">  val spark = SparkSession.builder
      .master("local")
      .appName("Chapter 11")
      .config("spark.logConf","true")
      .config("spark.logLevel","ERROR")
      .getOrCreate()
  println(s"Running Spark Version ${spark.version}")
  //
  valfilePath = "/Users/ksankar/fdps-v3/"
  val cars = spark.read.option("header","true").
option("inferSchema","true").
  csv(filePath + "data/car-data/car-milage.csv")
println("Cars has "+cars.count()+" rows")
cars.show(5)
cars.printSchema()
</pre><p>We use the options <code class="literal">header = true</code> and <code class="literal">inferschema = true</code>. The first line in the Dataset (<code class="literal">car-milage.csv</code>) has the column name. The <code class="literal">show()</code> method prints out the first few rows nicely. The <code class="literal">printSchema()</code> method shows the data and the types, as inferred from the data file. Looks fine for us!</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec55"></a>Computing statistics</h3></div></div></div><p>This work is also a lot easier with Spark 2.0.0 and Datasets. The reason the statistics are encapsulated in an easy function is because data scientists look at the basic statistics as the first step in data exploration.</p><p>The code is simple and the results as follows:</p><pre class="programlisting">  //
  // Let us find summary statistics
  //
  cars.describe("mpg","hp","weight","automatic").show()
  //
  // correlations
  //
  varcor = cars.stat.corr("hp","weight")
  println("hp to weight : Correlation = %2.4f".format(cor))
  varcov = cars.stat.cov("hp","weight")
  println("hp to weight : Covariance = %2.4f".format(cov))
  //
  cor = cars.stat.corr("RARatio","width")
  println("Rear Axle Ratio to width : Correlation = %2.4f".format(cor))
  cov = cars.stat.cov("RARatio","width")
  println("Rear Axle Ratio to width : Covariance = %2.4f".format(cov))
</pre><p>The output of the preceding code is as follows, and it looks OK:</p><p>
</p><div class="mediaobject"><img src="graphics/image_11_005.jpg" /></div><p>
</p><p>While it might seem too much work to calculate the correlation of a tiny Dataset, remember that this will scale to Datasets consisting of 1,000,000 rows or even a billion rows!</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch11lvl1sec78"></a>Linear regression</h2></div></div><hr /></div><p>Linear regression involves a little more work than statistics. We need the data in a vector form along with a few more parameters; such as the learning rate, that is, the step size. We will also split the Dataset into <code class="literal">training</code> and <code class="literal">test</code>, as shown in the later part of this chapter.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec56"></a>Data transformation and feature extraction</h3></div></div></div><p>The <code class="literal">ml.feature</code> library has a class vector assembler that transforms the data into a vector of features:</p><pre class="programlisting">    //
    // Linear Regression
    //
    // Transformation to a labeled data that Linear Regression Can use
val cars1 = cars.na.drop()
val assembler = new VectorAssembler()
assembler.setInputCols(Array("displacement","hp","torque","CRatio","RARatio","CarbBarrells","NoOfSpeed","length","width","weight","automatic"))
assembler.setOutputCol("features")
val cars2 = assembler.transform(cars1)
cars2.show(40)
</pre><p>The result is a Dataset with a new column <code class="literal">features</code>, which contains vectorized features:</p><p>
</p><div class="mediaobject"><img src="graphics/image_11_006.jpg" /></div><p>
</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec57"></a>Data split</h3></div></div></div><p>Here, we split the data based on weight. In the next section, we will use the <code class="literal">randomSplit()</code> function for the classification:</p><pre class="programlisting">    //
    // Split into training &amp; test
    //
val train = cars2.filter(cars1("weight") &lt;= 4000)
val test = cars2.filter(cars1("weight") &gt; 4000)
test.show()
println("Train = "+train.count()+" Test = "+test.count())
</pre><p>The results are as expected:</p><p>
</p><div class="mediaobject"><img src="graphics/image_11_007.jpg" /></div><p>
</p><p>In the regression model, we create an algorithm object, set the appropriate parameters, and fit the training data to the algorithm to get the model. We will now inspect the various attributes of the model:</p><pre class="programlisting">    val algLR = new LinearRegression()
    algLR.setMaxIter(100)
    algLR.setRegParam(0.3)
    algLR.setElasticNetParam(0.8)
    algLR.setLabelCol("mpg")
    //
    val mdlLR = algLR.fit(train)
    //
    println(s"Coefficients: ${mdlLR.coefficients} Intercept: ${mdlLR.intercept}")
    val trSummary = mdlLR.summary
println(s"numIterations: ${trSummary.totalIterations}")
println(s"Iteration Summary History: ${trSummary.objectiveHistory.toList}")
trSummary.residuals.show()
println(s"RMSE: ${trSummary.rootMeanSquaredError}")
println(s"r2: ${trSummary.r2}")
</pre><p>The output is interesting in the sense that the model has APIs to expose the attributes. Our <code class="literal">R2</code> is .86, not that bad:</p><p>
</p><div class="mediaobject"><img src="graphics/image_11_008.jpg" /></div><p>
</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec58"></a>Predictions using the model</h3></div></div></div><p>We use the <code class="literal">transform()</code> method and the <code class="literal">test</code> Dataset to predict. The call adds a new column to our Dataset (actually a new Dataset with the new column added):</p><pre class="programlisting">//
// Now let us use the model to predict our test set
//
val predictions = mdlLR.transform(test)
predictions.show()
</pre><p>You can see the new column named predictions:</p><p>
</p><div class="mediaobject"><img src="graphics/image_11_009.jpg" /></div><p>
</p><p>The prediction is not that impressive. There are a couple of reasons for this. There might be quadratic effects or some of the variables might be correlated (for example, length, width, and weight, and we might not need all the three to predict the <code class="literal">mpg</code> value). Finally, we might not need all 10 features anyway. I leave it to you to try different combinations of the features.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec59"></a>Model evaluation</h3></div></div></div><p>Prior to Spark 2.0.0, we had our own code to calculate the error. The <code class="literal">spark.ml</code> library has an evaluation class with the required evaluators. The pattern is similar: we create the appropriate object, set the required parameters, and then call the <code class="literal">evaluate()</code> method:</p><pre class="programlisting">// Calculate RMSE&amp;MSE
val evaluator = new RegressionEvaluator()
evaluator.setLabelCol("mpg")
valrmse = evaluator.evaluate(predictions)
println("Root Mean Squared Error = "+"%6.3f".format(rmse))
//
evaluator.setMetricName("mse")
valmse = evaluator.evaluate(predictions)
println("Mean Squared Error = "+"%6.3f".format(mse))
//
</pre><p>The result is as expected:</p><p>
</p><div class="mediaobject"><img src="graphics/image_11_010.jpg" /></div><p>
</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch11lvl1sec79"></a>Classification</h2></div></div><hr /></div><p>Classification is very similar to linear regression. The algorithms take vectors, and the algorithm object has various parameters to tweak the algorithm in order to fit the needs of an application. The returned model can be used to predict the class invoking the transform method. We will use the Titanic Dataset and predict who will survive. The Dataset has 15 fields, including age, gender, whether they have siblings/a spouse, parents sailing with them, the class they are in, and so forth.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec60"></a>Loading data</h3></div></div></div><p>Similar to regression, we load the CSV data using the <code class="literal">read.csv()</code> method. The code file is <code class="literal">ML02v2.scala</code>. We load the code and run the <code class="literal">ML02v2</code> object. The CSV data is loaded and we print the schema to verify:</p><pre class="programlisting">val filePath = "/Users/ksankar/fdps-v3/"
  val passengers = spark.read.option("header","true").
    option("inferSchema","true").
    csv(filePath + "data/titanic3_02.csv")
  println("Passengers has "+passengers.count()+" rows")
  passengers.show(5)
  passengers.printSchema()
  //
</pre><p>The output is as expected:</p><p>
</p><div class="mediaobject"><img src="graphics/image_11_011.jpg" /></div><p>
</p><p>The schema is a little more elaborate than the regression example:</p><p>
</p><div class="mediaobject"><img src="graphics/image_11_012.jpg" /></div><p>
</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec61"></a>Data transformation and feature extraction</h3></div></div></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip65"></a>Tip</h3><p>For our model, we extract five attributes. The gender is a string Male/Female, which needs to be converted to 1/0. Here we use the ML transformation function <code class="literal">StringIndexer</code>. As you can see, <code class="literal">StringIndexer</code> itself acts as a machine learning algorithm with its own fit and <code class="literal">transform</code> method. It is this symmetry that makes it easy to create a pipeline.</p></div><p>A couple of records do not have the age criteria on and so we drop the records using the <code class="literal">na.drop()</code> method.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip66"></a>Tip</h3><p>
</p><p>Even though we drop the records with missing columns, in real life we should try to impute the missing data. Just because data for a column is missing, it doesn't mean that we should toss important data in other columns. There are many strategies we can use in our example:</p><p>
</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>We could replace the missing age with the average age</p></li><li style="list-style-type: disc"><p>We could be cleverer and infer the average age from the title in the name (Mr., Mrs., Miss, Jr., Master, and others)</p></li></ul></div><p>
</p></div><p>Then, like regression, we use the vector assembler to create a column of vectors called <code class="literal">features</code>, which will be ready for our algorithms to consume:</p><pre class="programlisting">val passengers1 = passengers.select(passengers("Pclass"),passengers("Survived").cast(DoubleType).as("Survived"),passengers("Gender"),passengers("Age"),passengers("SibSp"),passengers("Parch"),passengers("Fare"))
passengers1.show(5)
    //
    // VectorAssembler does not support the StringType type. So convert Gender to numeric
    //
val indexer = new StringIndexer()
indexer.setInputCol("Gender")
indexer.setOutputCol("GenderCat")
val passengers2 = indexer.fit(passengers1).transform(passengers1)
passengers2.show(5)
    //
val passengers3 = passengers2.na.drop()
println("Orig = "+passengers2.count()+" Final = "+ passengers3.count() + " Dropped = "+ (passengers2.count() - passengers3.count()))
    //
val assembler = new VectorAssembler()
    assembler.setInputCols(Array("Pclass","GenderCat","Age","SibSp","Parch","Fare"))
assembler.setOutputCol("features")
val passengers4 = assembler.transform(passengers3)
passengers4.show(5)
</pre><p>The output is as expected, but it's worth a look. The following features get extracted:</p><p>
</p><div class="mediaobject"><img src="graphics/image_11_013.jpg" /></div><p>
</p><p>The <code class="literal">GenderCat</code> column gets created:</p><p>
</p><div class="mediaobject"><img src="graphics/image_11_014.jpg" /></div><p>
</p><p>We drop the rows with missing data and create the column with a vector of features. We drop 264 records, 20%  of our Dataset! So we should really look at imputing the following strategies:</p><p>
</p><div class="mediaobject"><img src="graphics/image_11_015.jpg" /></div><p>
</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec62"></a>Data split</h3></div></div></div><p>Unlike the last time, here we will use the <code class="literal">randomSplit()</code> function, which is part of the Spark framework. Because we have a small Dataset, we will use the 90-10 split between the training and the test Dataset. We have 939 rows in the training set and 106 in the test set:</p><pre class="programlisting">    // split data
    //
val Array(train, test) = passengers4.randomSplit(Array(0.9, 0.1))
println("Train = "+train.count()+" Test = "+test.count())
</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec63"></a>The regression model</h3></div></div></div><p>By now, you must have become an expert. For the last time, we will use the same mechanics: create an algorithm object, set the appropriate parameters, fit the training data to the algorithm to get the model, and finally inspect the various attributes of the model:</p><pre class="programlisting">    //
    // Train a DecisionTree model.
val algTree = new DecisionTreeClassifier()
algTree.setLabelCol("Survived")
algTree.setImpurity("gini") // could be "entropy"
algTree.setMaxBins(32)algTree.setMaxDepth(5)
    //
val mdlTree = algTree.fit(train)
println("The tree has %d nodes.".format(mdlTree.numNodes))
println(mdlTree.toDebugString)
println(mdlTree.toString)
println(mdlTree.featureImportances)
</pre><p>The output is a tree with 57 nodes. We can print the model and inspect the tree. The tree is interesting:</p><p>
</p><div class="mediaobject"><img src="graphics/image_11_016.jpg" /></div><p>
</p><p>The importance of the attributes is interesting:</p><p>
</p><div class="mediaobject"><img src="graphics/image_11_017.jpg" /></div><p>
</p><p>This shows the influence of the variables in the prediction. The output shows the variable order and the percentage fraction. The variable <code class="literal">Gender</code> explains 57 percent of the survivors (1,0.56579), with the class (0,0.19927) adding another 20%.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec64"></a>Prediction using the model</h3></div></div></div><p>Again, as before, we use the <code class="literal">transform()</code> method and the test Dataset to make a prediction:</p><pre class="programlisting">val predictions = mdlTree.transform(test)
predictions.show(5)
</pre><p>We get a new Dataset with the predictions column added:</p><p>
</p><div class="mediaobject"><img src="graphics/image_11_018.jpg" /></div><p>
</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec65"></a>Model evaluation</h3></div></div></div><p>Like regression, we use the evaluator that is available with the ML framework. In this case, we use the <code class="literal">MulticlassClassificationEvaluator</code> method. We instantiate the object and set the required parameters:</p><pre class="programlisting">val evaluator = new MulticlassClassificationEvaluator()
evaluator.setLabelCol("Survived")
evaluator.setMetricName("accuracy") // could be f1, "weightedPrecision" or "weightedRecall"
    //
val accuracy = evaluator.evaluate(predictions)
println("Test Accuracy = %.2f%%".format(accuracy*100))
    //
val elapsedTime = (System.nanoTime() - startTime) / 1e9
println("Elapsed time: %.2fseconds".format(elapsedTime))
</pre><p>The output is as expected (a test accuracy of 78.3 percent), and we feel good because we have cracked the ML gene and are ready for the clustering algorithm!</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip67"></a>Tip</h3><p>Now that we have created a model using the decision tree algorithm, we can follow the same pattern to try out algorithms such as <span class="strong"><strong>RandomForests</strong></span>, <span class="strong"><strong>Gradient</strong></span>
<span class="strong"><strong>Boosted Trees</strong></span>, and <span class="strong"><strong>Neural Network-based classification</strong></span>. Create an algorithm object with the appropriate class and set the parameters required for each algorithm. The classes are <code class="literal">org.apache.spark.ml.classification.RandomForestClassifier</code>, <code class="literal">org.apache.spark.ml.classification</code>, <code class="literal">GBTClassifier</code>, and <code class="literal">org.apache.spark.ml.classification.MultilayerPerceptronClassifier</code>.</p></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch11lvl1sec80"></a>Clustering</h2></div></div><hr /></div><p>Spark MLlib has implemented the K-means clustering algorithm. The model training and prediction interfaces are similar to other machine learning algorithms. Let's see how it works by going through an example.</p><p>Let's use a sample data that has two dimensions: <span class="emphasis"><em>x</em></span> and <span class="emphasis"><em>y</em></span>. The plot of the points looks like the following screenshot:</p><p>
</p><div class="mediaobject"><img src="graphics/image_11_019.jpg" /></div><p>
</p><p>From the preceding graph, we can see that four clusters form one solution. Let's try <span class="emphasis"><em>k = 2</em></span> and <span class="emphasis"><em>k=4</em></span>. Let's see how the Spark clustering algorithm handles this Dataset and the groupings.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec66"></a>Loading data</h3></div></div></div><p>By now, we know very well how to load data using the <code class="literal">read.csv()</code> method. The following code file is <code class="literal">ML03v2.scala</code>:</p><p>
</p><div class="mediaobject"><img src="graphics/image_11_020.jpg" /></div><p>
</p><p>We run the <code class="literal">ML03v2</code> object:</p><p>
</p><div class="mediaobject"><img src="graphics/image_11_021.jpg" /></div><p>
</p><p>The code to read data is very similar to the Regression and Classification examples. So we will skip it here; you can look at the source code anyway. After reading the data in, we have the Dataset with two columns: <code class="literal">X</code> and <code class="literal">Y</code>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec67"></a>Data transformation and feature extraction</h3></div></div></div><p>For this example, the transformation is very simple. Create the features column with the <code class="literal">VectorAssembler</code> like this:</p><pre class="programlisting">val assembler = new VectorAssembler()
assembler.setInputCols(Array("X","Y"))
assembler.setOutputCol("features")
valdata1 = assembler.transform(data)
data1.show(5)
</pre><p>The output is as expected:</p><p>
</p><div class="mediaobject"><img src="graphics/image_11_022.jpg" /></div><p>
</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec68"></a>Data split</h3></div></div></div><p>Unlike our Regression and Classification models, clustering is not a supervised algorithm. That is, we have no training data, and we don't need to learn how to cluster data. The algorithm is unsupervised, which means it applies the K-means algorithm and finds which cluster each data point belongs to, all on its own. In short, there is no need to split the data. And there is no measure of accuracy, so a test set is of no use to us in this context. Later, we will see what all this means for clustering.</p><p>In the clustering model, we can use the patterns we learned from Regression and Classification here as well. We create an algorithm object, set the parameters, and run the data through. There is one twist: we do need to tell the algorithm how many clusters we need. This is the <code class="literal">k</code> variable we were talking about earlier. In our example here, we will try with <span class="emphasis"><em>k=2</em></span> and then <span class="emphasis"><em>k=4</em></span>; after the clustering is done, we will discuss the implications. Following is simple code for this model:</p><pre class="programlisting">var algKMeans = new KMeans().setK(2)
var mdlKMeans = algKMeans.fit(data1)
</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec69"></a>Predicting using the model</h3></div></div></div><p>The mechanics of predicting are equally simple, but of course the interpretation and evaluation are a little more involved. To predict, we use the <code class="literal">transform()</code> method and we get the new predictions column:</p><pre class="programlisting">var predictions = mdlKMeans.transform(data1)
predictions.show(3)
    //
predictions.write.mode("overwrite").option("header","true").csv(filePath + "data/cluster-2K.csv")
</pre><p>In this case, we write the data as a CSV file. We will use the data to plot the centers:</p><p>
</p><div class="mediaobject"><img src="graphics/image_11_023.jpg" /></div><p>
</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec70"></a>Model evaluation and interpretation</h3></div></div></div><p>Now we have to face the music and talk about what clustering means and how to make use of the results. First, the metric that we can use to evaluate between models is <span class="strong"><strong>WSSE</strong></span>. This is the sum of errors within a cluster; it is really not an error but the distance of the points in each cluster from its center. Every group has a center (called the <span class="strong"><strong>centroid</strong></span>) and we can calculate the square distance of the points in a group from the group center.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip68"></a>Tip</h3><p>Think of it this way: if we had only one cluster for our data, then there would be one center and there would be one WSSE. Now if we cluster the data around four centers and then calculate the WSSE for each cluster and sum that up, that would be WSSE<sub>4</sub>. Because we have four centers, the points in each cluster will be closer to its centroid, and the WSSE<sub>4</sub> would be smaller than WSSE<sub>1</sub>. So theoretically, a smaller WSSE means we have better cohesion between the points and <code class="literal">k</code> variable.</p></div><p>Calculating WSSE is not that hard; we invoke the <code class="literal">computeCost()</code> method. Let's compute the WSSE for <span class="emphasis"><em>k=2</em></span> and <span class="emphasis"><em>k=4</em></span> and save the data:</p><pre class="programlisting">var algKMeans = newKMeans().setK(2)
var mdlKMeans = algKMeans.fit(data1)
    // Evaluate clustering by computing Within Set Sum of Squared Errors.
var WSSSE = mdlKMeans.computeCost(data1)
println(s"Within Set Sum of Squared Errors (K=2) = %.3f".format(WSSSE))
    // Shows the result.
println("Cluster Centers (K=2) : " + mdlKMeans.clusterCenters.mkString("&lt;", ",", "&gt;"))
println("Cluster Sizes (K=2) : " +  mdlKMeans.summary.clusterSizes.mkString("&lt;", ",", "&gt;"))
    //
var predictions = mdlKMeans.transform(data1)
predictions.show(30)
    //
predictions.write.mode("overwrite").option("header","true").csv(filePath + "data/cluster-2K.csv")
    //
    //
    // Now let us try 4 centers
    //
algKMeans = new KMeans().setK(4)
mdlKMeans = algKMeans.fit(data1)
    // Evaluate clustering by computing Within Set Sum of Squared Errors.
WSSSE = mdlKMeans.computeCost(data1)
println(s"Within Set Sum of Squared Errors (K=4) = %.3f".format(WSSSE))
    // Shows the result.
println("Cluster Centers (K=4) : " + mdlKMeans.clusterCenters.mkString("&lt;", ",", "&gt;"))
println("Cluster Sizes (K=4) : " +  mdlKMeans.summary.clusterSizes.mkString("&lt;", ",", "&gt;"))
    //
predictions = mdlKMeans.transform(data1)
predictions.show(30)
    //
predictions.write.mode("overwrite").option("header","true").csv(filePath + "data/cluster-4K.csv")
    //
</pre><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip69"></a>Tip</h3><p>The <code class="literal">write.mode.overwrite</code> library avoids the path file: <code class="literal">/Users/ksankar/fdps-v3/data/cluster-2K.csv</code> already exists exception. But mode is a member of write not CSV, so it has to come first.</p></div><p>The output of the program is as expected:</p><p>
</p><div class="mediaobject"><img src="graphics/image_11_024.jpg" /></div><p>
</p><p>With two clusters (<span class="emphasis"><em>k=2</em></span>), as shown in the following graph, we have WSSE of 357.22 and two centers with points 17 and 4, respectively. We can see that the centers are at the points (7.5,5.5) and (13.75,14.0):</p><p>
</p><div class="mediaobject"><img src="graphics/image_11_025.jpg" /></div><p>
</p><p>With four clusters (<span class="emphasis"><em>k=4</em></span>), as seen in the following graph, the WSSE is reduced to 59.25. Each center has 8,4,4, and 5 points. The graph shows that the cluster with the center at (13.75,14.0) hasn't changed. The other cluster is now broken into three clusters with appropriate centers:</p><p>
</p><div class="mediaobject"><img src="graphics/image_11_026.jpg" /></div><p>
</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip70"></a>Tip</h3><p>Bear in mind that the results could vary a little between runs because the clustering algorithm picks the centers randomly and grows from there. With <span class="emphasis"><em>k=4</em></span>, the results are stable; however, with <span class="emphasis"><em>k=2</em></span>, there is room for partitioning the points in different ways. Try it out a few times and see whether the results change. They might not in this case, but can for other Datasets.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec71"></a>Clustering model interpretation</h3></div></div></div><p>The question remains: how do we select <code class="literal">k</code>, especially when we have data with multiple dimensions, that is, 10, 20, or even 100 attributes/columns? The selection of <code class="literal">k</code> is also a business question, that is, it depends on the application. For example, if you are an airline and are clustering your frequent flyers based on a set of attributes (say, miles travelled, credit card purchase, bonus miles, and so forth) to market different promotions for different clusters, you would want 5 or 6 clusters; creating 100s of different promotions wouldn't make sense here.</p><p>On the other hand, if you are LinkedIn and want to cluster users to recommend potential connections in interesting unexpected ways, even 10 million clusters (thus recommending 50 people per member, that is, 433 million members/10 million clusters ~ 43 per cluster) might be appropriate. Once the clusters are created, you can recommend users in each cluster to each other. Of course, you have to filter members who are already connected, so maybe 5 million or 1 million clusters might be appropriate.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch11lvl1sec81"></a>Recommendation</h2></div></div><hr /></div><p>Recommendation systems are one of the most visible and popular machine learning applications on the Web, from Amazon to LinkedIn to Walmart. The algorithms behind recommendations systems are very interesting. Recommendation algorithms fall into roughly five general mechanisms: knowledge-based, demographic-based, content-based, collaborative filtering (item-based or user-based), and latent factor-based. Of these, collaborative filtering is the most widely used and unfortunately very computationally intensive.</p><p>Spark implements a scalable variation, the <span class="strong"><strong>Alternating Least Square</strong></span> (<span class="strong"><strong>ALS</strong></span>) algorithm authored by Yehuda Koren, available at <a class="ulink" href="http://dl.acm.org/citation.cfm?id=1608614" target="_blank">http://dl.acm.org/citation.cfm?id=1608614</a>. It is a user-based collaborative filtering mechanism that uses the <span class="emphasis"><em>latent factors</em></span> method of learning, which can scale to a large Dataset. Let's quickly use the <code class="literal">movielens</code> medium Dataset to implement a recommendation using Spark. While the model development patterns follow what we have seen so far, there are some interesting waypoints at different stages, as we will see. Apart from that, the code is not that complex.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec72"></a>Loading data</h3></div></div></div><p>The code file is <code class="literal">ML04v2.scala</code>. In the Scala shell, we load and run the object <code class="literal">ML04v2</code>:</p><p>
</p><div class="mediaobject"><img src="graphics/image_11_027.jpg" /></div><p>
</p><p>The first thing we realize is that the <code class="literal">movielens</code> data fields are separated by <code class="literal">::</code> operator, so we can't use <code class="literal">read.csv</code>. We could of course write a small program to convert the data file into CSV format. Instead, let's take this opportunity to see how we can read text files and process them using RDDs.</p><p>One trick is to go down to RDD and use map to parse the files. There are many other ways, but this is good for us as it enables us to see how we can go back and forth between RDDs and Datasets. Once we have this method, the rest is relatively easy to read data in:</p><pre class="programlisting">def parseRating(row:Row) : Rating = {
val aList = row.getList[String](0)
  Rating(aList.get(0).toInt,aList.get(1).toInt,aList.get(2).toDouble) //.getInt(0), row.getInt(1), row.getDouble(2))
  }
  //
def rowSqDiff(row:Row) : Double = {
  math.pow( (row.getDouble(2) - row.getFloat(3).toDouble),2)
  }
  //
def main(args: Array[String]): Unit = {
println(getCurrentDirectory)
  val spark = SparkSession.builder
      .master("local")
      .appName("Chapter 11")
      .config("spark.logConf","true")
      .config("spark.logLevel","ERROR")
      .getOrCreate()
println(s"Running Spark Version ${spark.version}")
    //
    // To turn off INFO messages
    //
  val rootLogger = Logger.getRootLogger()
  rootLogger.setLevel(Level.ERROR)           // INFO, TRACE,...
  val startTime = System.nanoTime()
  //
  val filePath = "/Users/ksankar/fdps-v3/"
  val movies = spark.read.text(filePath + "data/medium/movies.dat")
  movies.show(5,truncate=false)
  movies.printSchema()
  val ratings = spark.read.text(filePath + "data/medium/ratings.dat")
  ratings.show(5,truncate=false)
  val users = spark.read.text(filePath + "data/medium/users.dat")
  users.show(5,truncate=false)
  //
  println("Got %d ratings from %d users on %d movies.".format(ratings.count(), users.count(), movies.count()))
</pre><p>We use a couple of techniques to read the data, parse it to the values, and then get a Dataset back. The results are as expected:</p><p>
</p><div class="mediaobject"><img src="graphics/image_11_028.jpg" /></div><p>
</p><p>While we read in the three files, we will use only the rating data file.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec73"></a>Data transformation and feature extraction</h3></div></div></div><p>To transform the ratings data, we have to downshift to RDD and then create a DataFrame back. This is one method; there are many other ways:</p><pre class="programlisting">    // Transformation
    // This is a kludge. Let me know if there is a better way
    //
val ratings1 = ratings.select(split(ratings("value"),"::")).as("values")
ratings1.show(5)
val ratings2 = ratings1.rdd.map(row =&gt; parseRating(row))
ratings2.take(3).foreach(println)
    //
val ratings3 = spark.createDataFrame(ratings2)
ratings3.show(5)
</pre><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip71"></a>Tip</h3><p>The map uses the <code class="literal">parseRating()</code> method; for it to work in the shell, we need to make our main object serializable.</p></div><p>The following is the output; we can see that the numbers look good:</p><p>
</p><div class="mediaobject"><img src="graphics/image_11_029.jpg" /></div><p>
</p><p>The spark recommendation algorithm takes a Dataset and looks for items and rating; we just need to explain the columns. So we don't need to perform <code class="literal">VectorAssembler</code> like we did last time. One of the tasks in the transformation stage is to understand the implementation and the data format it needs before you apply transformations as required.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec74"></a>Data splitting</h3></div></div></div><p>We will use <code class="literal">RandomSplit</code> to split the data. Comparing MLlib with the <span class="emphasis"><em>second edition</em></span> of this book, you'll find that, in the second addition, we used the last digit of the timestamp to split the data:</p><pre class="programlisting">val Array(train, test) = ratings3.randomSplit(Array(0.8, 0.2))
println("Train = "+train.count()+" Test = "+test.count())
</pre><p>The split works out well. We have training data of 800,625 rows and a test Dataset with 199,584 rows. As we have lots of data, we used the 80-20 split.</p><p>In the recommendation model as usual, we create the algorithm object and fit the training data. The algorithm expects a rating column, a user ID column, and an item ID column with the default names <code class="literal">rating</code>, <code class="literal">user</code>, and <code class="literal">item</code>, respectively. In our case, the <code class="literal">item</code> column is named product, so we use the <code class="literal">setItemCol()</code> method. We also set parameters such as the regularization parameter, the maximum number of iterations, and the rank:</p><pre class="programlisting">val algALS = new ALS()
algALS.setItemCol("product") // Otherwise will get exception "Field "item" does not exist"
algALS.setRank(12)
algALS.setRegParam(0.1) // was regularization parameter, was lambda in MLlib
algALS.setMaxIter(20)
val mdlReco = algALS.fit(train)
</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec75"></a>Predicting using the model</h3></div></div></div><p>The mechanics of prediction are the same as the other algorithms:</p><pre class="programlisting">val predictions = mdlReco.transform(test)
predictions.show(5)
predictions.printSchema()
</pre><p>We get the new predictions column:</p><p>
</p><div class="mediaobject"><img src="graphics/image_11_030.jpg" /></div><p>
</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec76"></a>Model evaluation and interpretation</h3></div></div></div><p>As the recommendation algorithm is supervised learning, we can calculate RSME and MSE. But there is one small kink: we might get NaN for either MSE or RMSE. We will run into the issue <code class="literal">RegressionEvaluator</code> returns NaN for ALS in <code class="literal">Spark.ml</code>; more information on this is available at https://issues.apache.org/jira/browse/SPARK-14489. What happens is that, when we split training and test data randomly, it might happen that the test data has users that are not in the training data. This is the cold start problem: how can we recommend something to a user who hasn't rated anything before, basically a new user? We have no way of calibrating the tastes of this user and so the recommendation algorithm returns <code class="literal">NaN</code>. This is fine as the output of the recommendation, but our RMSE calculation can't work with NaN. The solution, of course, is to drop the test records with NaN. So we need this extra step before we calculate RMSE:</p><pre class="programlisting">// Running into https://issues.apache.org/jira/browse/SPARK-14489 = cold Start
// So filter them out before calculating MSE et al
//
val pred = predictions.na.drop()
println("Orig = "+predictions.count()+" Final = "+ pred.count() + " Dropped = "+ (predictions.count() - pred.count()))
// Calculate RMSE&amp;MSE
val evaluator = new RegressionEvaluator()
evaluator.setLabelCol("rating")
var rmse = evaluator.evaluate(pred)
println("Root Mean Squared Error = "+"%.3f".format(rmse))
//
evaluator.setMetricName("mse")
var mse = evaluator.evaluate(pred)
println("Mean Squared Error = "+"%.3f".format(mse))
mse = pred.rdd.map(r =&gt; rowSqDiff(r)).reduce(_+_) / predictions.count().toDouble
println("Mean Squared Error (Calculated) = "+"%.3f".format(mse))
//
//
val elapsedTime = (System.nanoTime() - startTime) / 1e9
println("Elapsed time: %.2fseconds".format(elapsedTime))
//
println("*** That's All Folks ! ***")
</pre><p>Once we drop the NaNs, things are fine. Just to show the calculations, we also calculate MSE by hand:</p><p>
</p><div class="mediaobject"><img src="graphics/image_11_031.jpg" /></div><p>
</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch11lvl1sec82"></a>Hyper parameters</h2></div></div><hr /></div><p>We have glossed over an important aspect: model tuning. As you can see, there are many parameters that can be tuned, depending on the algorithm. And we have been setting the parameters once. For example, in the case of the recommender, we set <code class="literal">rank=12</code>, <code class="literal">regularizationParameter=0.1</code>, and <code class="literal">maxIterations=20</code>. In reality, the rank could be 8 or 12; the regularization parameter 0.1,1.0, or 10; and the iterations 10 or 20. So now we need to try 12 runs with these different values, calculate the accuracy, and then select the one with the best value. This is a simple case; we might have more than 100 runs and many parameters. This is where cross validation comes into the picture. To keep this book within its boundaries, I will leave this part for you to explore. Two places to go are the documentation for <code class="literal">org.apache.spark.ml.tuning</code> class and the examples code at <a class="ulink" href="https://github.com/apache/spark/tree/master/examples/src/main/java/org/apache/spark/examples/ml" target="_blank">https://github.com/apache/spark/tree/master/examples/src/main/java/org/apache/spark/examples/ml</a>.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch11lvl1sec83"></a>The final thing</h2></div></div><hr /></div><p>As we mentioned earlier, one of the interesting additions to spark 2.0.0 is the ML pipeline. A pipeline is nothing but a linear graph of transformers and estimators. If we look at the classes we have been using, they are either transformers or estimators. We had a decent pipeline for our classification example, as follows:</p><p>We started with Passengers, which was the Dataset that we read in.</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Passengers1 was after the feature extraction.</p></li><li style="list-style-type: disc"><p>Passenders2 was after <code class="literal">StringIndexer</code>.</p></li><li style="list-style-type: disc"><p>Passengers3 was after the <code class="literal">na.drop()</code> function.</p></li><li style="list-style-type: disc"><p>Passengers4 was after the <code class="literal">VectorAssembler()</code> function.</p></li><li style="list-style-type: disc"><p>The <code class="literal">algTree</code> object was the algorithm object.</p></li></ul></div><p>We would have created a pipeline:</p><pre class="programlisting">valtreePipeline = new Pipeline().setStages(Array(indexer, assembler, algTree))
</pre><p>Then, we would have created a model:</p><pre class="programlisting">valmdlTree = treePipeline.fit(trainData)
</pre><p>Finally, we would have predicted as usual:</p><pre class="programlisting">val predictions = mdlTree.transform(testData)
</pre><p>Of course, our original sequence won't work. We have to do <code class="literal">na.drop()</code> on <code class="literal">passenger1</code> and split it into <code class="literal">trainData</code> and <code class="literal">testData</code>. But there is no need to do either <code class="literal">StringIndexer</code> or <code class="literal">VectorAssembler</code>, as the pipeline has encapsulated those stages. The pipeline will call the <code class="literal">fit()</code> and <code class="literal">transform()</code> methods as appropriate.</p><p>We will leave this as a topic for you to explore now that you are a master at using the Spark ML classes!</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch11lvl1sec84"></a>References</h2></div></div><hr /></div><p>During the writing of this chapter, I came across many useful and relevant references. I have listed them here:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The <span class="emphasis"><em>GoodbyMapReduce</em></span> article from Mahout News (<a class="ulink" href="https://mahout.apache.org/" target="_blank">https://mahout.apache.org/</a>)</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="https://spark.apache.org/docs/latest/mllib-guide.html" target="_blank">https://spark.apache.org/docs/latest/mllib-guide.html</a>
</p></li><li style="list-style-type: disc"><p>The <span class="emphasis"><em>Collaborative Filtering ALS</em></span> paper (<a class="ulink" href="http://dl.acm.org/citation.cfm?id=1608614" target="_blank">http://dl.acm.org/citation.cfm?id=1608614</a>)</p></li><li style="list-style-type: disc"><p>Good presentation on decision trees (<a class="ulink" href="http://spark-summit.org/wp-content/uploads/2014/07/Scalable-Distributed-Decision-Trees-in-Spark-Made-Das-Sparks-Talwalkar.pdf" target="_blank">http://spark-summit.org/wp-content/uploads/2014/07/Scalable-Distributed-Decision-Trees-in-Spark-Made-Das-Sparks-Talwalkar.pdf</a>)</p></li><li style="list-style-type: disc"><p>Recommendation hands-on exercise from Spark Summit 2014 (<code class="literal">https://databricks-training.s3.amazonaws.com/movie-recommendation-with-mllib.html</code>)</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="https://amplab.cs.berkeley.edu/ml-pipelines/" target="_blank">https://amplab.cs.berkeley.edu/ml-pipelines/</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html" target="_blank">https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html</a>
</p></li><li style="list-style-type: disc"><p>ML Pipeline Design Doc <a class="ulink" href="https://docs.google.com/document/d/1rVwXRjWKfIb-7PI6b86ipytwbUH7irSNLF1_6dLmh8o/" target="_blank">https://docs.google.com/document/d/1rVwXRjWKfIb-7PI6b86ipytwbUH7irSNLF1_6dLmh8o/</a>
</p></li><li style="list-style-type: disc"><p>Examples at <a class="ulink" href="https://github.com/apache/spark/tree/master/examples/src/main/scala/org/apache/spark/examples/ml" target="_blank">https://github.com/apache/spark/tree/master/examples/src/main/scala/org/apache/spark/examples/ml</a>
</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch11lvl1sec85"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we covered a large surface area of Spark: machine learning with its associated capabilities. We covered core model development and prediction along with feature extraction and model evaluation. Machine learning is a vast subject and requires a lot more study, experimentation, and practical experience with interesting data science problems. Two books that are relevant to Spark Machine Learning are Packt's own book <span class="emphasis"><em>Machine Learning with Spark</em></span>, Nick Pentreath, and O'Reilly's <span class="emphasis"><em>Advanced Analytics with Spark</em></span>, Sandy Ryza, Uri Laserson, Sean Owen, and Josh Wills. Both are excellent books that you can refer to. In the next chapter, we will look at another interesting topic: the processing graphs and graph algorithms using the GraphX APIs.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch12"></a>Chapter 12. GraphX</h2></div></div></div><p>In this chapter, we will dive into the graph-processing capabilities of Spark, the GraphX package-very interesting, useful, and relevant. You will see things such as PageRank, connections, and communities. We will start with an introduction to graph processing and then progress to code the GraphX APIs on a simple, yet interesting giraffe graph. We will explore the organization and structure of the APIs and objects and then dive into algorithms that explore the community, PageRank, and so forth. Finally, we will explore the retweet network of the <span class="strong"><strong>#alphago</strong></span> community, exploring the data pipeline, the map attributes of properties, and vertices and edges. We'll then create a graph and run algorithms. Should be an interesting chapter!</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch12lvl1sec86"></a>Graphs and graph processing - an introduction</h2></div></div><hr /></div><p>Before diving into GraphX, let's take a quick look at the domain of graph processing. Graphs and algorithms on graph structures were always the core of specific industries, such as logistics, transportation, routing, as well as social networking. These industries, among many others, relied on very special algorithms to optimize their businesses, irrespective of whether they were routing trucks or packets. Then came <span class="strong"><strong>PageRank</strong></span> and <span class="strong"><strong>social media</strong></span>, and the domain of graph processing has been growing ever since.</p><p>While I was preparing the materials for this book three interesting graph applications crossed my desk, all of which were very representative of the new applications of graph processing. Here's a bit about these applications.</p><p>The EU has initiatives such as <span class="strong"><strong>fp6</strong></span> and <span class="strong"><strong>fp7</strong></span>, which consist of projects in multiple domains that are being worked on by a host of companies and educational institutions. And they found that they can measure the impact of a project by applying social network analysis on collaboration structures.</p><p>An article an behavioral ecology points out that the infection risk for gastrointestinal helminth parasites is more influenced by weak ties with individuals outside one's clique than by repeated contact with a core set of associates. They talk about a Giraffe graph that ties two cliques and members that are part of the weak tie. We will work with a Giraffe graph in the first example - graph section. Giraffe graphs have interesting properties.</p><p>Very recently, an analysis of the graph structures underlying 11.5 million documents in 2.6 TB of Panama papers exposed how celebrities, executives, and politicians invested in questionable tax haven schemes. This 2.6 TB of data (in approximately 4.8 million e-mails, 3 million database-formatted documents, and 2.1 million PDFs) was the largest ever journalistic big data analytics endeavor.</p><p>As datasets get larger and graphs become more complex (be they web pages, social networks, structures representing nefarious investments, or social graphs), graph processing becomes more challenging. This is where Spark GraphX and other analytics tools come in handy.</p><p>Graph-based systems can be viewed as two major categories: graph processing and graph databases. Graph-processing systems (such as GraphX, <span class="strong"><strong>Pregel BSP</strong></span>, and <span class="strong"><strong>GraphLab</strong></span>) are very good at running complex graph-based algorithms on large datasets, while graph databases (such as <span class="strong"><strong>AllegroGraph</strong></span>, <span class="strong"><strong>Titan</strong></span>, <span class="strong"><strong>Neo4j</strong></span>, and <span class="strong"><strong>RDF</strong></span> stores) are better at graph-based queries. Usually, if one has lots of graph-based applications, there would be a graph database as well as a graph-processing stack. Moreover, graph processing would be part of a larger workflow with processing pipelines such as ETL, machine learning, and other processing logic. Graph processing can consist of standalone functions, namely large-scale systems such as the Web or PageRank or high-scale social media applications such as Facebook, where you recommend people whom you may know.</p><p>Now let's focus on graph-processing frameworks. What makes them challenging is the nature of their computations—the computations are iterative and recursive, spanning the graph. This makes record-based relational systems unsuitable for running complex graph algorithms. Once the data is large enough to span multiple systems, the partitioning that is normally employed in database systems is not optimal for graph algorithms, especially for long-tail graphs that are the norm in the Internet world. Long-tail graphs have a lot of sparsely connected nodes, where a few of the nodes have most of the edges.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip72"></a>Tip</h3><p>Interestingly, as we were doing AR, Version 0.2 of the graphFrames package was released. Graphframes, as the name implies, leverages Datasets/DataFrames for much easier and powerful graph queries.</p></div><p>Even with frameworks such as MapReduce, data parallelism-based on disk-based partition schemes-has an impedance-related mismatch with graph representations. The processing of graphs is proportional to the edge cuts, that is, the number of edges cut by a partition scheme. Remember that with long-tail graphs, there will be lots of edge cuts for popular nodes because all the nodes won't fit in one system; therefore, the nodes would be stored in a distributed fashion.</p><p>This is where Spark GraphX comes into the picture. Spark offers graph parallelism over data parallelism and data-distributed RDD mechanism, almost the best of both the worlds! As we will see later, Graph offers different partition and storage schemes that can optimize complex graph algorithms. Of course, Spark can't eliminate the domain complexity, and some of the graph processing would need tuning and careful selection of partitioning and algorithms. Spark does provide enough primitives in terms of representing and processing APIs such that it makes many of the complex algorithms possible and fast and that too in most of the cases.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch12lvl1sec87"></a>Spark GraphX</h2></div></div><hr /></div><p>GraphX is a layer over Spark, thus it leverages all the interesting things about Spark-distributed processing, the algorithms, the versioned computation graph, and so forth. Interestingly, a couple of ML algorithms are written using GraphX APIs. Now refer to the following figure:</p><p>
</p><div class="mediaobject"><img src="graphics/image_12_001.jpg" /></div><p>
</p><p>This diagram shows the layers and the relationships between GraphX, Spark, and the algorithms. GraphX is truly a distributed graph-processing component at scale with powerful partitioning mechanisms, and of course, the in-memory representation that makes iterative processing faster than normal. The programming is much more succinct and very powerful. I went back to the source code and counted the lines of implementations of graph-related algorithms. PageRank is composed of 60 lines using the aggregateMessages API (which, as you will see later, is a very powerful abstraction) and 60 lines for <code class="literal">runUntilConvergence</code> using the Pregel API. The triangle count is 50 lines of code, and SVD++ is 150 lines of code. The Pregel API itself is approximately 35 lines of code. The LDA, which is part of the ML library, is approximately 400 lines of GraphX APIs, where the documents and terms are represented as vertices and the document-term matrix is captured as GraphX edges. There is some very elegant and compact code here—it's worth investing your time in trying to understand the code.</p><p>Spark GraphX has a rich computational model, built-in algorithms, as well as APIs for implementing powerful algorithms on your own. The current focus is on computation rather than query, and the new GraphFrames API combines the DataFrame and the graph to offer a rich query interface. The GraphX APIs include graph creation, structure queries, attribute transformers, structure transformers, connection mining primitives, and algorithms. In this chapter, we will focus on GraphX, and you will work with the APIs.</p><p>There are lots of interesting graph Datasets one can work with. Airline data, co-occurrence and co-citation from papers, and Wikipedia PageRank Analysis are all types of data that one can use to learn graph processing. We will use an example from social media, particularly the retweet network of the AlphaGo community, as a case study for this chapter.</p><p>As of Spark 2.0, GraphX has only Scala APIs, so we will do all of our programming via the Spark Shell and Scala. Actually, this is good; you will get a chance to work with Scala in the context of this book.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch12lvl1sec88"></a>GraphX - computational model</h2></div></div><hr /></div><p>Before we dive into creating a graph and applying algorithms, we need to understand the computational model. Needless to say, GraphX has a rich yet simple computational model:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>It consists of vertices connected by edges.</p></li><li><p>It is a property graph, which means the vertices and edges can have arbitrary objects as properties, and most importantly, these properties are visible to the APIs.</p></li></ol></div><p>It is also a directed multigraph, meaning the edges have a direction and there can be any number of edges between the vertices. This is important to note down, because some of the algorithms can be tricky when faced with loops and cyclic graphs. GraphX has APIs that will come in handy, for example, the <code class="literal">removeSelfEdges</code> method will be helpful when you want to remove loops.</p><p>With this model, many kinds of graphs can be created, including bipartite and tripartite graphs (for example, the Users-Tags-Web pages). The following diagram illustrates the computational model of GraphX:</p><p>
</p><div class="mediaobject"><img src="graphics/image_12_002.jpg" /></div><p>
</p><p>A vertex consists of a vertexID(64-bit long int) and a property object. The property object Is required, but you can always have an arbitrary value if you are not using the property. An edge consists of a source vertexID, a destination vertexID, and a property object. The APIs, as you will see, have the vertex and edge parameterized over object types. You can think of the properties as methods that help you attach user-defined objects to edges and vertices (ED/VD).</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch12lvl1sec89"></a>The first example - graph</h2></div></div><hr /></div><p>As our first example, let's create a simple graph. Remember the Giraffe Graph we talked about? Let's create one that is shown in the following figure:</p><p>
</p><div class="mediaobject"><img src="graphics/image_12_003.jpg" /></div><p>
</p><p>This is a graph from the lecture given by Prof. Jeffrey D. Ullman at Stanford University on graphs and social Networks (<a class="ulink" href="http://web.stanford.edu/class/cs246/handouts.html" target="_blank">http://web.stanford.edu/class/cs246/handouts.html</a>). It is a giraffe graph with two strong cliques connected by a weak edge. The numbers are the <span class="emphasis"><em>betweenness centrality</em></span> of each node, as calculated by the <span class="strong"><strong>G-N</strong></span> algorithm.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note73"></a>Note</h3><p>Fun facts about betweenness centrality: It shows how many paths an edge is part of; that is, its relevancy. High betweenness centrality is the sign of a bottleneck, a point of single failure; such edges need HA and probably alternate paths for rerouting, and they are susceptible to parasite infections and good candidates for a cut!</p></div><p>From the GraphX computational model, you can see that betweenness centrality would be represented as an edge property.</p><p>The nodes <span class="strong"><strong>A</strong></span>, <span class="strong"><strong>B</strong></span>,... are people, and we will have the age as the vertex property. That way, we have an interesting, realistic graph to apply GraphX APIs to.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip74"></a>Tip</h3><p>In designing graph apps, a lot of work goes in to figuring out the vertices, the edges, and their properties. These are not abstract; they depend on the application. For example, if you are looking at a PageRank type of application, then the vertices would be people, topics, or web pages and edges would be followers/followees or links. The directions are important. You should also consider what attributes you want to work on, such as locations, hashtags, retweets, categories, and so on.</p></div><p>Our node table and the edge table is as follows. Later, when we work on the retweet use case, you will see how we figure out the vertices and edges, then extract the appropriate data and apply transformations.</p><p>
</p><div class="mediaobject"><img src="graphics/image_12_004.jpg" /></div><p>
</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch12lvl1sec90"></a>Building graphs</h2></div></div><hr /></div><p>Now that we have our data, the next step is to create a graph. Let's fire up the Spark Shell. Run it from the directory where you have installed Spark:</p><p>
</p><div class="mediaobject"><img src="graphics/image_12_005.jpg" /></div><p>
</p><p>We create the graph in two steps: first we create an RDD list of the vertices and edges, then we create RDDs and eventually the graph. You don't need to type the code; the <code class="literal">graphx-0x.scala</code> files have the programs to create a graph and do the rest of the API stuff.</p><p>First we create the lists:</p><p>
</p><div class="mediaobject"><img src="graphics/image_12_006.jpg" /></div><p>
</p><p>Let's quickly run through the code:</p><pre class="programlisting">case class Person(name:String,age:Int)

val defaultPerson = Person("NA",0)

val vertexList = List( (1L, Person("Alice", 18)), (2L, Person("Bernie", 17)), (3L, Person("Cruz", 15)), (4L, Person("Donald", 12)), (5L, Person("Ed", 15)), (6L, Person("Fran", 10)), (7L, Person("Genghis",854)) )

val edgeList = List( Edge(1L, 2L, 5), Edge(1L, 3L, 1), Edge(3L, 2L, 5), Edge(2L, 4L, 12), Edge(4L, 5L, 4), Edge(5L, 6L, 2), Edge(6L, 7L, 2), Edge(7L, 4L, 5), Edge(6L, 4L, 4) )</pre><p>Remember the GraphX vertex is a vertexID plus a user-defined object as the property; the edge is the sourceID and destinationID plus a user-defined object as an <code class="literal">edge</code> property. In addition, as you will see later, the graph will create a default object for vertices that are not defined in the vertex list but are present in the edge list.</p><p>Our vertex object is a person with a name and an age. Our edge object is the inbetweenness of an integer. So the vertex list consists of elements that have an ID (integer) and a person object with a name (string) and age (integer). The edge list is actually three Integers, the ID of the source vertex, the destination vertexID and the inbetweenness centrality. As you can see from the output, Spark creates two lists, as expected.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip75"></a>Tip</h3><p>As we discussed earlier, the property object for the vertex and edge is an important design consideration. How they would look, what attributes would they contain, and the object hierarchy are all determined by your app and the algorithms you use.</p></div><p>The next step is to create RDDs, and the graph-<code class="literal">fdps-v3/code/graphX-02.scala</code> helps you do that:</p><p>
</p><div class="mediaobject"><img src="graphics/image_12_007.jpg" /></div><p>
</p><p>The code is deceptively simple. It is so, because we have done all the background work to architect the data and the objects:</p><pre class="programlisting">val vertexRDD = sc.parallelize(vertexList)
val edgeRDD = sc.parallelize(edgeList)
val graph = Graph(vertexRDD, edgeRDD,defaultPerson)
</pre><p>We create two RDDs, which have the required structure and elements, and create a graph with them. Note that we also give the default object, that is, <code class="literal">defaultPerson</code> to the object creator code. Then, if it finds a new vertexID in the edge list, it will automatically create a vertex with <code class="literal">defaultPerson</code> as the user-defined property of the vertex.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip76"></a>Tip</h3><p>In GraphX, there are four ways to create a graph.</p><p>You can load an edge list file using the <code class="literal">GraphLoader.edgeListFile(...)</code> call. The file would be of the form source id &lt;tab&gt; target id. You can also load the file using RDDs, a set of edge tuples of IDs using the <code class="literal">fromEdgeTuples()</code> call, and a list of edges using the <code class="literal">fromEdges()</code> call, all in the similar fashion.</p><p>I like to create it using the edge and vertex RDDs because they are flexible and very powerful, especially when it comes to manipulating the user-defined objects for the vertices and edges.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch12lvl1sec91"></a>The GraphX API landscape</h2></div></div><hr /></div><p>The next task in our to-do list is to take a quick look at the GraphX APIs. For the most part, the organization is the same, except for a couple of twists. The following figure shows the organization and categories of the APIs:</p><p>
</p><div class="mediaobject"><img src="graphics/image_12_008.jpg" /></div><p>
</p><p>Objects such as Edge, EdgeRDD, and others are under <code class="literal">org.apache.spark.graphx</code>. The graph object has APIs such as triplets, persist, subgraph and so on. But the graph algorithms are separated under ops, which is a GraphOps object, to separate the algorithms from the graph implementation. Another quirk is <code class="literal">lib</code>, which has analytic functions such as SVD++, ShortestPath, and others. So navigate around the GraphX classes and you will find all the methods and classes.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip77"></a>Tip</h3><p>One interesting thing to note is that the objects have a few methods that the class might not have. So take a look at the object methods if you cannot find them under the class:</p><div class="mediaobject"><img src="graphics/image_12_009.jpg" /></div><p>
</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch12lvl1sec92"></a>Structural APIs</h2></div></div><hr /></div><p>Armed with the knowledge of how to create a graph and the knowledge of how to navigate the APIs, let's focus on the structural APIs. The code is available in <code class="literal">graph-03.scala</code>, <code class="literal">graph-04.scala</code>, and <code class="literal">graph-05.scala</code> files:</p><p>
</p><div class="mediaobject"><img src="graphics/image_12_010.jpg" /></div><p>
</p><p>
</p><div class="mediaobject"><img src="graphics/image_12_011.jpg" /></div><p>
</p><p>You can get the number of edges and vertices by <code class="literal">numEdges</code> and <code class="literal">numVertices</code>.</p><pre class="programlisting">graph.numEdges
graph.numVertices
//
val vertices = graph.vertices
vertices.collect.foreach(println)
//
val edges = graph.edges
edges.collect.foreach(println)
//
val triplets = graph.triplets
triplets.take(3)
triplets.map(t=&gt;t.toString).collect().foreach(println)
</pre><p>The triplets are interesting. They actually encompass an edge and two vertices that the edge connects to and all the user-defined objects in one neat object. They are very useful when we want to write algorithms:</p><p>
</p><div class="mediaobject"><img src="graphics/image_12_012.jpg" /></div><p>
</p><p>Let's look at the code. It is not that complex:</p><pre class="programlisting">val inDeg = graph.inDegrees // Followers
inDeg.collect()
val outDeg = graph.outDegrees // Follows
outDeg.collect()
val allDeg = graph.degrees
allDeg.collect()
//
val g1 = graph.subgraph(epred = (edge) =&gt; edge.attr &gt; 4)
g1.triplets.collect.foreach(println)
//
// What is wrong ?
//
val g2 = graph.subgraph(vpred = (id, person) =&gt; person.age &gt; 21)
g2.triplets.collect.foreach(println)
</pre><p>The <code class="literal">inDegrees</code> are the edges pointing toward the vertices (followers, links referring to a web page, and others) and <code class="literal">outDegrees</code> are outward edges (things one follows, links that a web page has to other pages, and others). We extract a <code class="literal">subgraph</code> by giving a filter predicate on the property object of the edge (the edge predicate) or the property object of the vertices (the vertex predicate). This is where the versatility of GraphX shines. As we saw earlier, we can attach user-defined objects and use them in our graph-processing algorithms. In this code, for <code class="literal">g1</code>, we filtered edges that have more than four centralities.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lvl2sec77"></a>What's wrong with the output?</h3></div></div></div><p>When we do a subgraph <code class="literal">g2</code>, filtering the vertices with age &gt; 21, and then print the triplets of the subgraph, we get nothing. Is there something wrong? Actually, things are fine, as we can see from running <code class="literal">graph-05.scala</code> file:</p><p>
</p><div class="mediaobject"><img src="graphics/image_12_013.jpg" /></div><p>
</p><p>As earlier, the code is simple. The subgraph query takes a predicate, in our case, age &gt;= 18:</p><pre class="programlisting">// Look ma, no edges !
g2.vertices.collect.foreach(println)
g2.edges.collect.foreach(println)
//
val g3 = graph.subgraph(vpred = (id, person) =&gt; person.age &gt;= 18)
g3.triplets.collect
//
// Just two disjoint vertices
// If there are no edges, is it really a graph ?
g3.vertices.collect.foreach(println)
//
</pre><p>The reason the triplets do not print anything is because there are no edges in <code class="literal">g2</code>, just one vertex. A similar issue arises in <code class="literal">g3</code>: this has two vertices, but they are not joined by an edge. (you can check the results by looking at our Giraffe graph and correlating with the data table.) So again, no triplets are printed. The reason we were looking at these situations is to show that at times, graph processing can be a little tricky and our algorithms need to consider all the corner cases.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch12lvl1sec93"></a>Community, affiliation, and strengths</h2></div></div><hr /></div><p>Let's now look at the network connections and others. These algorithms are applied widely for fraud detection and security applications. Triangular spamming is a well-known technique that can be detected using the triangle count and community algorithms. Another interesting application of the triangle count is to estimate and rank communities. The age of a community is related to the density of the triangles; new communities will have fewer triangles, and as the communities mature, triangles start to form. Another interesting application is the concept of a heavy hitter in a community, defined as any vertex that has more than <code class="literal">sqrt(n)</code> degrees. Finding heavy hitter triangles would be like finding influential people in a community. Connected communities and strongly connected communities expose the structure in an underlying graph, akin to the Panama papers. And all these are APIs in GraphX. No wonder GraphX is part of the processing stack for Linkedin:</p><p>
</p><div class="mediaobject"><img src="graphics/image_12_014.jpg" /></div><p>
</p><p>Let's apply some of the connection and community algorithms to our graph and explore the results:</p><p>
</p><div class="mediaobject"><img src="graphics/image_12_015.jpg" /></div><p>
</p><p>The APIs are relatively simple, but the results are semantically richer and we can get interesting inferences:</p><pre class="programlisting">val cc = graph.connectedComponents() // returns another graph; costly operation
cc.triplets.collect
graph.connectedComponents.vertices.map(_.swap).groupByKey.map(_._2).collect
cc.vertices.map(_._2).collect.distinct.size // No. of connected components
//
// list the components and its number of nodes in the descending order
cc.vertices.groupBy(_._2).map(p=&gt;(p._1,p._2.size)).
    sortBy(x=&gt;x._2,false). // sortBy(keyFunc,ascending)
    collect()
//
// stronglyConnectedComponents
//     Compute the strongly connected component (SCC) of each vertex and
//     return a graph with the vertex value containing the lowest vertex id
//     in the SCC containing that vertex.
//
val ccS = graph.stronglyConnectedComponents(10)
ccS.triplets.collect
ccS.vertices.map(_.swap).groupByKey.map(_._2).collect
ccS.vertices.map(_._2).collect.distinct.size // No. of connected components
//
val triCounts = graph.triangleCount()
val triangleCounts = triCounts.vertices.collect
//
</pre><p>The <code class="literal">connectedComponents</code> call returns an interesting graph structure: the node IDs and the ID of the community that it belongs to. Looking at the graph, we can see that all the nodes are part of a single connected community. But if we apply <code class="literal">stronglyConnectedComponents</code>, we get four communities. This makes sense as strong connection requires bi-directed edges, and we can see that that is there only in the D-E-F-G network, and A, B, and C are strong communities by themselves. The triangle count shows that 4/D and 6/F are part of two triangles, and the others are part of one triangle. This can be verified by a visual analysis of our Giraffe graph. Algorithms such as <code class="literal">connectedComponents</code> and <code class="literal">stronglyConnectedComponents</code> are iterative and would take time and computing resources, depending on the size and complexity of the graph that you are processing.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch12lvl1sec94"></a>Algorithms</h2></div></div><hr /></div><p>Now we dive into the most interesting part of GraphX: algorithms and the graph parallel computation APIs to implement more algorithms. The following table shows a bird's eye view of the algorithms:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Type</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>GraphX method/example</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Graph-Parallel Computation</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>
</p><p>The method is <code class="literal">aggregateMessages()</code>, Function</p><p>
</p><p>
<code class="literal">Pregel()</code>. Refer to https://issues.apache.org/jira/browse/SPARK-5062 for examples.</p><p>
</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>PageRank</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>
</p><p>The method is <code class="literal">PageRank()</code>. As an example, refer to the influential papers in a citation network, Influencer in retweet. You can specifically check out the following:</p><p>
</p><p>
<span class="strong"><strong>staticPageRank()</strong></span>: This provides a static no of iterations and dynamic tolerance; see the parameters (tol versus numIter)</p><p>
</p><p>
<span class="strong"><strong>personalizedPageRank()</strong></span>: This is a variation of PageRank that gives a rank relative to a specified "source" vertex in the graph-People</p><p>
</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>You May Know ShortestPaths and SVD++</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>The methods are <code class="literal">ShortestPaths()</code> and SVD++. As an example, consider the fact that SDV++ takes an RDD of edges.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; ">
<p>LabelPropagation (LPA)</p>
</td><td style="">
<p>The method is <code class="literal">LabelPropagation()</code>.</p>
</td></tr></tbody></table></div><p>PageRank is probably the most talked about algorithm available in GraphX. In fact, it has three flavors: first, the PageRank algorithm, which is a dynamic version that takes a tolerance and runs until it converges to that tolerance. Second, there is a <code class="literal">staticPageRank</code> algorithm that takes a number of iterations and runs that many times, rather than seeking convergence. Finally, there is <code class="literal">personalizedPageRank</code>, which calculates PageRank with respect to a given vertex, probably a good algorithm to implement the "people you may know" or "find popular people in a specified subcommunity" functions.</p><p>Other interesting algorithms are <code class="literal">LabelPropogation</code> to find communities, <code class="literal">ShortestPaths</code>, and <code class="literal">SVD++</code>.</p><p>Calling the algorithms is straightforward. Let's see how we can run the page rank in our small graph. We will run <code class="literal">fdps-v3/code/graphX-07.scala</code> and inspect the output step by step, comparing with the program code:</p><p>
</p><div class="mediaobject"><img src="graphics/image_12_016.jpg" /></div><p>
</p><p>Getting PageRank is very straightforward. We call it for the graph like this:</p><pre class="programlisting">val ranks = graph.pageRank(0.1).vertices
ranks.collect().foreach(println)
val topVertices = ranks.sortBy(_._2,false).collect.foreach(println)
</pre><p>This returns the vertexID and the PageRank. We can then sort it and print the array or even print only the first few. Looking at our Giraffe graph, it makes sense that 4/D has the highest PageRank. We would have expected 5/E to be next (as 4/D points to 5/E), but for some reason, 6/F has a larger PageRank than 5/E. Note that 1/A has the lowest PageRank (0.15), which is the default damping factor, even though nothing points to it.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip78"></a>Tip</h3><p>PageRank is an algorithm that determines the importance/popularity of a web page based on the number of links that are pointing to it. So the PageRank of a page being pointed to by 100 web pages would normally be higher than one that is pointed to by only 10 pages. It also considers the popularity of the web pages that are pointing to it. For example, if you have a page that is linked by Google, the PageRank would be much higher.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lvl2sec78"></a>Graph parallel computation APIs</h3></div></div></div><p>Now let's switch gears and look at the APIs available for implementing algorithms: <code class="literal">aggregateMessages()</code> and <code class="literal">Pregel()</code>. These are basic graph parallel computation primitives that can be used to implement all kinds of interesting algorithms. We will dig deeper into <code class="literal">aggregateMessages()</code> in this chapter and will leave the <code class="literal">Pregel()</code> API for you to explore. Interestingly, the Pregel API is a superset of <code class="literal">aggregateMessages()</code>, and it is in fact implemented using the <code class="literal">aggregateMessages()</code> API. Now it will probably make more sense to read the second section and appreciate the elegance of the GraphX APIs—the <code class="literal">staticPageRank</code> API consists of 60 lines of code and uses the <code class="literal">aggregateMessages()</code> call, while <code class="literal">pageRank()</code> (the converging version) consists of 60 lines and uses the <code class="literal">Pregel()</code> call.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch12lvl3sec21"></a>The aggregateMessages() API</h4></div></div></div><p>The <code class="literal">aggregateMessages()</code> API is somewhat daunting if you just look at the signature, but it gets easier as one works with the API. So in this section, let's work on a few simple problems using the API to really understand the mechanism. The signature is as follows:</p><p>
<code class="literal">def aggregateMessages[A](sendMsg: (EdgeContext[VD, ED, A]) ⇒ Unit, mergeMsg: (A, A) ⇒ A, tripletFields: TripletFields = TripletFields.All)(implicit arg0: ClassTag[A]): VertexRDD[A]</code>
</p><p>Aggregates values from the neighboring edges and vertices of each vertex. The user-supplied <code class="literal">sendMsg</code> function is invoked on each edge of the graph, generating 0 or more messages to be sent to either vertex in the edge. The <code class="literal">mergeMsg</code> function is then used to combine all messages destined to the same vertex.</p><p>Let's parse the call signature and then apply it to a few situations. It has four elements, as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Message type</strong></span>: Note that <code class="literal">aggregateMessages()</code> is parameterized by the message type [A]. It is the value that gets passed around. It will be an integer in our examples.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>EdgeContext</strong></span>: This is an interesting one. It is a superset of <code class="literal">EdgeTriplet</code>, which we have seen earlier (that is, the source and destination vertices, the edge, and all the user objects attached to the vertices and the edge). Plus, it also has the capability to send a message to the destination vertex or the source vertex. Remember that our graph is directed, which means the edges have a source vertex and a destination vertex. And the message would be the <code class="literal">messageType</code> object we saw just a moment ago.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>sendMsg</strong></span>: This basically means a message sent in the context of EdgeContext, either to the source vertex or to the destination vertex, and as we discussed, it contains an object of the message type.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>mergeMsg</strong></span>: A routine/code run at each vertex that will aggregate all the messages received by that vertex.</p></li></ul></div><p>You can also look at <code class="literal">aggregateMessages()</code> as a MapReduce paradigm over the graph, the map being <code class="literal">sendMsg</code> and reduce being <code class="literal">mergeMsg</code>. Interestingly, in its previous incarnation, <code class="literal">aggregateMessages()</code> was called <code class="literal">mapReduceTriplets()</code>—you can still see it in the Pregel implementation code. JIRA has an interesting diagram on this, which will appear soon.</p><p>Of course, our abstract discussion wouldn't be complete without a few pragmatic examples.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch12lvl4sec2"></a>The first example - the oldest follower</h5></div></div></div><p>In our Giraffe graph, let's find the oldest follower. Remember that the edges are directed and we have the age as a property. Use the template for the four elements of the <code class="literal">aggregateMessages()</code> signature as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>messageType</strong></span>: Because we are working with age, we'll use an integer.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>edgeContext</strong></span>: From all the pieces of information available, we will use the age and send it to the source through <code class="literal">sendToDst</code>.</p></li></ul></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip79"></a>Tip</h3><p>As A is following B, A is the source and B is the destination; A sends its age to B. This might take a little thinking, but make sure you internalize the direction. Don't worry! We have more examples to practice this.</p></div><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>sendMsg</strong></span>: This is <code class="literal">sendtoSrc</code>, the age at each vertex.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>mergeMsg</strong></span>: We want the oldest follower, so the reduce operation must be max.</p></li></ul></div><p>In fact, our code nicely translates from what we just discussed:</p><pre class="programlisting">val oldestFollower = graph.aggregateMessages[Int](
edgeContext =&gt; edgeContext.sendToDst(edgeContext.srcAttr.age),//sendMsg
(x,y) =&gt; math.max(x,y) //mergeMsg
)
oldestFollower.collect()
</pre><p>And the output looks fine:</p><p>
</p><div class="mediaobject"><img src="graphics/image_12_017.jpg" /></div><p>
</p><p>We can compare the output with the following graph:</p><p>
</p><div class="mediaobject"><img src="graphics/image_12_018.jpg" /></div><p>
</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch12lvl4sec3"></a>The second example - the oldest followee</h5></div></div></div><p>Now what if we want to find the oldest followee, that is, the oldest person a vertex is following? Everything else remains the same: the age as the message, integer as the message type, and max as the reduce operation; the only difference is that <code class="literal">sendToSrc</code> should be changed to <code class="literal">dstAttr</code>.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip80"></a>Tip</h3><p>As A is following B, A is the source and B is the destination; as we want to get the age of the followee (which B knows), B sends its age to A, that is, <code class="literal">sendtoSrc</code> the <code class="literal">dstAttr.</code> An easy way to understand this is to see who has the information; find that in the <code class="literal">edgeContext</code> object and the direction to send to.</p></div><p>Our code looks very similar to the first example, except the direction:</p><pre class="programlisting">//  How do we get the oldest followee ?
val oldestFollowee = graph.aggregateMessages[Int](
    edgeContext =&gt; edgeContext.sendToSrc(edgeContext.dstAttr.age),//sendMsg
    (x,y) =&gt; math.max(x,y) //mergeMsg
    )
oldestFollowee.collect()
</pre><p>And the output is as expected:</p><p>
</p><div class="mediaobject"><img src="graphics/image_12_019.jpg" /></div><p>
</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch12lvl4sec4"></a>The third example - the youngest follower/followee</h5></div></div></div><p>Now it is easier. Our signature template (from the first and second examples) remains the same, except for the reduce operation, that is, <code class="literal">mergeMsg</code>. It will become <code class="literal">min (x,y)</code> instead of <code class="literal">max (x,y)</code>.</p><p>We make the changes in both of the code and run it:</p><pre class="programlisting">// #1 : What if we want the youngest follower?
val youngestFollower = graph.aggregateMessages[Int](
  edgeContext =&gt; edgeContext.sendToDst(edgeContext.srcAttr.age),//sendMsg
  (x,y) =&gt; math.min(x,y) //mergeMsg
  )
youngestFollower.collect()
//
// #2 : What if we want the youngest of the nodes folowee ?
val youngestFollowee = graph.aggregateMessages[Int](
  edgeContext =&gt; edgeContext.sendToSrc(edgeContext.dstAttr.age),//sendMsg
  (x,y) =&gt; math.min(x,y) //mergeMsg
  )
youngestFollowee.collect()
</pre><p>The output looks fine. You should check the graph and convince yourself that it is right:</p><p>
</p><div class="mediaobject"><img src="graphics/image_12_020.jpg" /></div><p>
</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch12lvl4sec5"></a>The fourth example - inDegree/outDegree</h5></div></div></div><p>Remember the <code class="literal">inDegree</code> and <code class="literal">outDegree</code> methods? Let's see whether we can implement them using our favorite call, <code class="literal">aggregateMessages()</code>.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip81"></a>Tip</h3><p>Thinking about the <code class="literal">inDegree</code> operation as a MapReduce reminds us of the <code class="literal">count</code> in the map reduce, where we send a tuple with the word and the number 1. In our case, we do the reduce at each vertex, so we only need the 1.</p></div><p>With that tip in mind, our signature template becomes easier:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>MessageType</strong></span> - This is just the number 1, which is an integer.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>edgeContext</strong></span>: We really don't need anything from <code class="literal">edgeContext</code>. Just the presense of a vertex is enough. So our message is just the number 1, as we do in the mapReduce word count.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>sendMsg</strong></span>: For <code class="literal">inDegree</code>, this is <code class="literal">sendToDst</code>, and for <code class="literal">outDegree</code>, it is <code class="literal">sendToSrc</code>.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>mergeMsg</strong></span>: Our reduce operation is the classic "add" we use in the word count.</p></li></ul></div><p>In fact, our code exactly looks like this, and the output values are the same as the inDegrees and <code class="literal">outDegrees</code> call:</p><pre class="programlisting">// #3 : Can we get inDegree with aggregateMessages ?
var iDegree = graph.aggregateMessages[Int](
edgeContext =&gt; edgeContext.sendToDst(1),//sendMsg
(x,y) =&gt; x+y //mergeMsg
)
iDegree.collect()
graph.inDegrees.collect()
//
val oDegree = graph.aggregateMessages[Int](
edgeContext =&gt; edgeContext.sendToSrc(1),//sendMsg
(x,y) =&gt; x+y //mergeMsg
)
oDegree.collect()
graph.outDegrees.collect()
</pre><p>The output values are as expected:</p><p>
</p><div class="mediaobject"><img src="graphics/image_12_021.jpg" /></div><p>
</p><p>The preceding discussions are summarized in the following diagram for clarity:</p><p>
</p><div class="mediaobject"><img src="graphics/image_12_022.jpg" /></div><p>
</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip82"></a>Tip</h3><p>Interestingly, <code class="literal">inDegrees</code> and <code class="literal">outDegrees</code> are actually implemented using <code class="literal">aggregateMessages()</code>. You can see the code in the Spark source file, namely <code class="literal">src/main/scala/org/apache/spark/graphx/GraphOps.scala</code>, method <code class="literal">private def degreesRDD</code>.</p></div></div></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch12lvl1sec95"></a>Partition strategy</h2></div></div><hr /></div><p>As we had mentioned earlier, graph processing becomes challenging when we use disk-partitioning strategies employed in MapReduce and others. Let's elaborate on this topic a little; we won't go into too much detail.</p><p>The problem is when we have millions of vertices and edges that do not fit into one machine, which means we need a distributed storage scheme. Naturally, we will have to store vertices and edges in many machines. Then the challenge is running iterative algorithms that would need back and forth communication between the machines. Interestingly, a Giraffe graph lends itself to efficient partitioning—one can cut the graph at the neck. So in our example, we can store the vertices A, B, and C in one machine and D, E, F, and G in another machine and still have optimum communication. This is called the edge cut. Unfortunately, the large graphs we encounter are all long-tail-based, that is, a few vertices are very popular and have lots of connections. In such cases, the communication when performing graph algorithms is not divided equally among all the nodes.</p><p>In short, unlike a relational table, graph processing is contextual with regard to a neighborhood; therefore, maintaining the locality and equal-sized partitioning becomes a challenge. Check out the following diagram:</p><p>
</p><div class="mediaobject"><img src="graphics/image_12_023.jpg" /></div><p>
</p><p>The diagram illustrates two schemes: the Edge cut and Vertex cut. Following is the description for both:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Edge cut</strong></span>: The scheme partitions the vertices into different machines. The communication and storage overhead of an edge cut is directly proportional to the number of edges that are cut. For vertices that are very popular, this will be high.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Vertex cut</strong></span>: The scheme partitions the edges, which means there would be duplication of the vertices. The communication and storage overhead of a vertex cut is directly proportional to the sum of the number of machines spanned by each vertex. The more the duplication, the higher the cost.</p></li></ul></div><p>In GraphX, the vertex cut strategy is set by default (to balance the hotspot issue due to the power law / Zipf's Law) with min replication of edges. But you can select a different scheme, depending on your application and characteristics of data. The <code class="literal">org.apache.spark.graphx</code>.<code class="literal">Graph</code> file has the partition API to repartition a graph. It takes a partition strategy, and optionally, the number of partitions:</p><pre class="programlisting">partitionBy(partitionStrategy: PartitionStrategy, numPartitions: Int)
partitionBy(partitionStrategy: PartitionStrategy)
</pre><p>The GraphX has four partition strategies, described as following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>RandomVertexCut</strong></span>:(usually the best) This collocates all the edges with the same direction edges between two vertices (hashing the source and destination vertex IDs).</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>CanonicalRandomVertexCut</strong></span>: This is a random vertex cut that collocates all the edges between two vertices, regardless of the direction (hashing the source and destination vertex IDs in a canonical direction). ... <span class="emphasis"><em>remember GraphX is multi graph</em></span>
</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>EdgePartition1D</strong></span>: This assigns edges to partitions using only the source vertex ID, collocating edges with the same source.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>EdgePartition2D</strong></span>: This provides 2D partitioning of the sparse edge adjacency matrix.</p></li></ul></div><p>An in-depth discussion is outside the scope of this book, but it is important to know that GraphX has a partition strategy. In fact, some of the algorithms require a partition strategy to run. For example, <span class="strong"><strong>groupEdges</strong></span> mention that the graph must have a partition strategy:</p><p>
</p><div class="mediaobject"><img src="graphics/image_12_024.jpg" /></div><p>
</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch12lvl1sec96"></a>Case study - AlphaGo tweets analytics</h2></div></div><hr /></div><p>Now that we have a good understanding of GraphX, let's apply our newly gained knowledge to analyze a retweet network. Like any big data project, the first task is to define a pipeline, figure out the data elements, the source, transformations, mapping, and processing.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lvl2sec79"></a>Data pipeline</h3></div></div></div><p>For this case study, I collected Twitter data pertaining to the AlphaGo project:</p><p>
</p><div class="mediaobject"><img src="graphics/image_12_025.jpg" /></div><p>
</p><p>While the full mechanics of data collection from Twitter is out of scope, I will quickly mention the main steps:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Using Python and the <span class="strong"><strong>tweepy</strong></span> framework, you can download the tweets mentioning the hashtag #alphago. Initially, pull all the tweets that Twitter will give and then use the since ID to incrementally get the tweets.</p></li><li><p>Then use application authentication for a higher rate. Twitter implements rate limiting, so the amount of tweets one can get without their firehose subscription is limited. Even then, I had collected approximately 300K tweets and 2 GB worth of data.</p></li><li><p>Store the data in <span class="strong"><strong>MongoDB</strong></span>. Twitter has a very rich Dataset with hundreds of attributes.</p></li><li><p>I wanted to create a graph model of the retweet network. So the transformations and feature extraction was aimed at that smaller task.</p></li><li><p>Lastly, store the extracted data in the <code class="literal">reTweetNetwork-small.psv</code> file.</p></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lvl2sec80"></a>GraphX modeling</h3></div></div></div><p>The interesting and challenging part is to model the vertices, edges, and objects. In this case, we want to find out the rank of users based on their retweet characteristics. We also want to understand the locations, time zones, and also the follower-followee characteristics. In this chapter, we will work up to PageRank calculation, but I have the location and time zone data for you to play with on your own. The following figure shows the data model and how it is created from the tweet record. The <code class="literal">tweetID</code>, count, and text go as part of the edge object; the details of the person who is tweeting go to the destination vertex, and the details of the person who is retweeting go to the source vertex:</p><p>
</p><div class="mediaobject"><img src="graphics/image_12_026.jpg" /></div><p>
</p><p>Let's look at the data model:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Our vertices are the users. Interestingly, Twitter has a 64-bit user ID, which we can use as the vertex ID. As you will see in a minute, this makes our mapping easier.</p></li><li style="list-style-type: disc"><p>As we are modeling the retweet domain, the edges represent the retweets, the source being the person who is retweeting, and the destination being the person who wrote the original tweet.</p></li><li style="list-style-type: disc"><p>Once we have the vertices and edges, defining the objects becomes a little easier:</p></li><li style="list-style-type: disc"><p>We store the username, location, time zone, and the number of friends and followers as the user object</p></li><li style="list-style-type: disc"><p>The <span class="strong"><strong>tweetID</strong></span>, <span class="strong"><strong>tweet text</strong></span>, and others become the object that is associated with each edge</p></li><li style="list-style-type: disc"><p>Interestingly, the <span class="strong"><strong>PSV</strong></span> file is inverted, that is, each record has the <span class="strong"><strong>tweetID</strong></span>, text, the details of the retweet user, and the details of the person who wrote the original tweet</p></li><li style="list-style-type: disc"><p>The preceding figure shows how the elements map to the graph.</p></li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lvl2sec81"></a>GraphX processing and algorithms</h3></div></div></div><p>Now we will start going through the code (<code class="literal">AlphaGo-01.scala</code>) and running it. The first step is to construct the vertex list and the edge list. To read the file, we use the <span class="strong"><strong>spark-csv</strong></span> package. So the Spark Shell needs to be started with the package option as shown here:</p><pre class="programlisting">
<span class="strong"><strong>/Volumes/sdxc-01/spark-1.6.0/bin/spark-shell --packages com.databricks:spark-csv_2.10:1.4.0</strong></span>
</pre><p>Refer to the following figure:</p><p>
</p><div class="mediaobject"><img src="graphics/image_12_027.jpg" /></div><p>
</p><p>First we load the data and create a DataFrame:</p><pre class="programlisting">import org.apache.spark.SparkContext
import org.apache.spark.SparkConf
import org.apache.spark.graphx._

println(new java.io.File( "." ).getCanonicalPath)
println(s"Running Spark Version ${sc.version}")
//
val sqlContext = new org.apache.spark.sql.SQLContext(sc)
val df = sqlContext.read.format("com.databricks.spark.csv").option("header", "false").option("inferSchema", "true").option("delimiter","|").load("file:/Users/ksankar/fdps-v3/data/reTweetNetwork-small.psv")
df.show(5)
df.count()
//
case class User(name:String, location:String, tz : String, fr:Int,fol:Int)
case class Tweet(id:String,count:Int)

val graphData = df.rdd
println("--- The Graph Data ---")
graphData.take(2).foreach(println)
</pre><p>We run the code by loading the file: <code class="literal">scala&gt; :load /Users/ksankar/fdps-v3/code/AlphaGo-01.scala</code>.</p><p>The DataFrame corresponds closely to the data model in the preceding figure. The run output shows the rows with 14 fields:</p><p>
</p><div class="mediaobject"><img src="graphics/image_12_028.jpg" /></div><p>
</p><p>The next step is to map each row to the vertex and edge, with appropriate data elements in the objects:</p><pre class="programlisting">val vert1 = graphData.map(row =&gt; (row(3).toString.toLong,User(row(4).toString,row(5).toString,row(6).toString,row(7).toString.toInt,row(8).toString.toInt)))
println("--- Vertices-1 ---")
vert1.count()
vert1.take(3).foreach(println)

val vert2 = graphData.map(row =&gt; (row(9).toString.toLong,User(row(10).toString,row(11).toString,row(12).toString,row(13).toString.toInt,row(14).toString.toInt)))
println("--- Vertices-2 ---")
vert2.count()
vert2.take(3).foreach(println)

val vertX = vert1.++(vert2)
println("--- Vertices-combined ---")
vertX.count()

val edgX = graphData.map(row =&gt; (Edge(row(3).toString.toLong,row(9).toString.toLong,Tweet(row(0).toString,row(1).toString.toInt))))
println("--- Edges ---")
edgX.take(3).foreach(println)
</pre><p>The output is as expected. We can see that we have 10001 vertices for each group, that is, the users who tweeted and the users who retweeted:</p><p>
</p><div class="mediaobject"><img src="graphics/image_12_029.jpg" /></div><p>

</p><p>Once we have the vertices and the edges in place, creating the graph and running the algorithms is easier:</p><pre class="programlisting">val rtGraph = Graph(vertX,edgX)
//
val ranks = rtGraph.pageRank(0.1).vertices
println("--- Page Rank ---")
ranks.take(2)
println("--- Top Users ---")
val topUsers = ranks.sortBy(_._2,false).take(3).foreach(println)
val topUsersWNames = ranks.join(rtGraph.vertices).sortBy(_._2._1,false).take(3).foreach(println)
//
//How big ?
println("--- How Big ? ---")
rtGraph.vertices.count
rtGraph.edges.count
//
// How many retweets ?
//
println("--- How many retweets ? ---")
val iDeg = rtGraph.inDegrees
val oDeg = rtGraph.outDegrees
//
iDeg.take(3)
iDeg.sortBy(_._2,false).take(3).foreach(println)
//
oDeg.take(3)
oDeg.sortBy(_._2,false).take(3).foreach(println)
//
// max retweets
println("--- Max retweets ---")
val topRT = iDeg.join(rtGraph.vertices).sortBy(_._2._1,false).take(3).foreach(println)
val topRT1 = oDeg.join(rtGraph.vertices).sortBy(_._2._1,false).take(3).foreach(println)
</pre><p>The output is interesting to study:</p><p>
</p><div class="mediaobject"><img src="graphics/image_12_030.jpg" /></div><p>
</p><p>The top retweeted users are Bruno, Hassabis, and Ken. Hassabis is the CEO of DeepMind, the company that created the AlphaGo program. Our data has 20,002 vertices, but the resulting graph has only 9,743 vertices and 10,001 edges. This makes sense as duplicate vertices are collapsed.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch12lvl1sec97"></a>References</h2></div></div><hr /></div><p>For more information, refer to the following links. They will further add to your knowledge:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<a class="ulink" href="http://neo4j.com/blog/icij-neo4j-unravel-panama-papers/" target="_blank">http://neo4j.com/blog/icij-neo4j-unravel-panama-papers/</a>
</p></li><li style="list-style-type: disc"><p>The GraphX paper at <a class="ulink" href="https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-gonzalez.pdf" target="_blank">https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-gonzalez.pdf</a>
</p></li><li style="list-style-type: disc"><p>The Pragel paper at <a class="ulink" href="http://kowshik.github.io/JPregel/pregel_paper.pdf" target="_blank">http://kowshik.github.io/JPregel/pregel_paper.pdf</a>
</p></li><li style="list-style-type: disc"><p>Scala/Python support at <a class="ulink" href="https://issues.apache.org/jira/browse/SPARK-3789" target="_blank">https://issues.apache.org/jira/browse/SPARK-3789</a>
</p></li><li style="list-style-type: disc"><p>The Java API for GraphX at <a class="ulink" href="https://issues.apache.org/jira/browse/SPARK-3665" target="_blank">https://issues.apache.org/jira/browse/SPARK-3665</a>
</p></li><li style="list-style-type: disc"><p>LDA at <a class="ulink" href="https://issues.apache.org/jira/browse/SPARK-1405" target="_blank">https://issues.apache.org/jira/browse/SPARK-1405</a>
</p></li><li style="list-style-type: disc"><p>Some good exercises at <a class="ulink" href="https://www.sics.se/~amir/files/download/dic/answers6.pdf" target="_blank">https://www.sics.se/~amir/files/download/dic/answers6.pdf</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://ampcamp.berkeley.edu/big-data-mini-course/graph-analytics-with-graphx.html" target="_blank">http://ampcamp.berkeley.edu/big-data-mini-course/graph-analytics-with-graphx.html</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="https://www.quora.com/What-are-the-main-concepts-behind-Googles-Pregel" target="_blank">https://www.quora.com/What-are-the-main-concepts-behind-Googles-Pregel</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://www.istc-cc.cmu.edu/publications/papers/2013/grades-graphx_with_fonts.pdf" target="_blank">http://www.istc-cc.cmu.edu/publications/papers/2013/grades-graphx_with_fonts.pdf</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="https://www.sics.se/~amir/files/download/papers/jabeja-vc.pdf" target="_blank">https://www.sics.se/~amir/files/download/papers/jabeja-vc.pdf</a>
</p></li><li style="list-style-type: disc"><p>Paco Nathan, Scala Days 2015, <a class="ulink" href="https://www.youtube.com/watch?v=P_V71n-gtDs" target="_blank">https://www.youtube.com/watch?v=P_V71n-gtDs</a>
</p></li><li style="list-style-type: disc"><p>Apache Spark Graph Processing by Packt at <a class="ulink" href="https://www.packtpub.com/big-data-and-business-intelligence/apache-spark-graph-processing" target="_blank">https://www.packtpub.com/big-data-and-business-intelligence/apache-spark-graph-processing</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://hortonworks.com/blog/introduction-to-data-science-with-apache-spark/" target="_blank">http://hortonworks.com/blog/introduction-to-data-science-with-apache-spark/</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://stanford.edu/~rezab/nips2014workshop/slides/ankur.pdf" target="_blank">http://stanford.edu/~rezab/nips2014workshop/slides/ankur.pdf</a>
</p></li><li style="list-style-type: disc"><p>Mining Massive Datasets book v2, <a class="ulink" href="http://infolab.stanford.edu/~ullman/mmds/ch10.pdf" target="_blank">http://infolab.stanford.edu/~ullman/mmds/ch10.pdf</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://web.stanford.edu/class/cs246/handouts.html" target="_blank">http://web.stanford.edu/class/cs246/handouts.html</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://www.cs.princeton.edu/~chazelle/courses/BIB/pagerank.htm" target="_blank">http://www.cs.princeton.edu/~chazelle/courses/BIB/pagerank.htm</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://kukuruku.co/hub/algorithms/social-network-analysis-spark-graphx" target="_blank">http://kukuruku.co/hub/algorithms/social-network-analysis-spark-graphx</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://sparktutorials.net/setup-your-zeppelin-notebook-for-data-science-in-apache-spark" target="_blank">http://sparktutorials.net/setup-your-zeppelin-notebook-for-data-science-in-apache-spark</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236&amp;DB_Short_Name=On-Time" target="_blank">http://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236&amp;DB_Short_Name=On-Time</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://openflights.org/data.html" target="_blank">http://openflights.org/data.html</a>
</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch12lvl1sec98"></a>Summary</h2></div></div><hr /></div><p>This was a slightly longer chapter, but I am sure you have progressed to be experts in Spark by now. We started by looking at graph processing and then moved on to GraphX APIs and finally to a case study. Keep a look out for more GraphX APIs and also the new GraphFrame API, which is being developed for querying. We also have come to the end of this book. You started by installing Spark and understanding Spark from the basics, then you progressed to RDDs, Datasets, SQL, big data, and machine learning. In the process, we also discussed how Spark has matured from 1.x to 2.x, what data scientists would look for in a framework such as Spark, and the Spark architecture. We (the authors, editors, reviewers, and the rest of the gang at Packt) enjoyed writing this book, and we hope you were able to get a good start on your journey to distributed computing with Apache Spark.</p></div></div></div></div>
</div></div></div></body></html>
