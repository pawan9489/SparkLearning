<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="">

    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0"> <!--320-->
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no, user-scalable=no">

    <link rel="icon" href="../../mapt/images/favicon.ico">

    <link rel="stylesheet" href="../../mapt/css/font-awesome.css">
    <link rel="stylesheet" href="../../mapt/css/google-fonts.css">
    <link rel="stylesheet" href="../../mapt/css/devicon.css">

    <link rel="stylesheet" href="../../mapt/css/bootstrap.css">
    <link rel="stylesheet" href="../../mapt/css/bootstrap-xl.css">
    <link rel="stylesheet" href="../../mapt/css/magnific-popup.css">
    <link rel="stylesheet" href="../../mapt/css/prism.css">
    <link rel="stylesheet" href="../../mapt/css/hljs-github.css">

    <link rel="stylesheet" href="../../mapt/css/mapt.css">
    <link rel="stylesheet" href="../../mapt/css/custom.css">

    <script src="../../mapt/js/jquery.js"></script>
    <script src="../../mapt/js/bootstrap.js"></script>
    <script src="../../mapt/js/jquery.magnific-popup.js"></script>
    <script src="../../mapt/js/highlight.min.js"></script>

    <script src="../../mapt/js/custom.js"></script>
    
    <title>Spark Cookbook</title>
</head>

<body class="home-body">
    <div id="wrapper">
        <div id="sidebar-wrapper">    
            <ul class="sidebar-nav">
                <div class="list-group" id="sidebar-nav" role="tablist">
                    <li>
                        <a href="../../index.html" class="sidenav-menu-holder back-btn" id="back-link">
                            <span class="sidenav-menu">Book List</span>
                            <span class="pull-left mr5"><i class="fa fa-chevron-left"></i></span>
                        </a>
                    </li>
                    
                    <li class="book-info copyright">
                        <span class="info text-nowrap"><span class="copyleft">&copy;</span><span><strong>RuTracker</strong>.org</span></span>
                    </li>          
                    <li class="book-info copyright">
                        <span class="info text-nowrap">Pub date: <strong>27 Jul 2015</strong></span>
                    </li>         
                    <li class="book-info">
                        <span class="info text-nowrap">Price: €<strong>29.99</strong></span>
                        <span class="info text-nowrap">ISBN: <strong>9781783987061</strong></span>
                    </li>     
            
                    <li>
                        <a href="graphics/cover.jpg" class="sidenav-menu-holder cover-img">
                            <img src="graphics/cover.jpg" class="cover-image">
                        </a>
                    </li>        
            
                    <div class="book_navigation">
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse1">
                                <div class="section-name">1: Getting Started with Apache Spark</div>
                            </a>
                        </li>
                        <div id="collapse1" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="1" class="sub-nav">
                                <a href="#ch01">
                                    <div class="section-name">Chapter 1: Getting Started with Apache Spark</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec09" class="sub-nav">
                                <a href="#ch01lvl1sec09">                    
                                    <div class="section-name">Introduction</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec10" class="sub-nav">
                                <a href="#ch01lvl1sec10">                    
                                    <div class="section-name">Installing Spark from binaries</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec11" class="sub-nav">
                                <a href="#ch01lvl1sec11">                    
                                    <div class="section-name">Building the Spark source code with Maven</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec12" class="sub-nav">
                                <a href="#ch01lvl1sec12">                    
                                    <div class="section-name">Launching Spark on Amazon EC2</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec13" class="sub-nav">
                                <a href="#ch01lvl1sec13">                    
                                    <div class="section-name">Deploying on a cluster in standalone mode</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec14" class="sub-nav">
                                <a href="#ch01lvl1sec14">                    
                                    <div class="section-name">Deploying on a cluster with Mesos</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec15" class="sub-nav">
                                <a href="#ch01lvl1sec15">                    
                                    <div class="section-name">Deploying on a cluster with YARN</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec16" class="sub-nav">
                                <a href="#ch01lvl1sec16">                    
                                    <div class="section-name">Using Tachyon as an off-heap storage layer</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse2">
                                <div class="section-name">2: Developing Applications with Spark</div>
                            </a>
                        </li>
                        <div id="collapse2" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="2" class="sub-nav">
                                <a href="#ch02">
                                    <div class="section-name">Chapter 2: Developing Applications with Spark</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec17" class="sub-nav">
                                <a href="#ch02lvl1sec17">                    
                                    <div class="section-name">Introduction</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec18" class="sub-nav">
                                <a href="#ch02lvl1sec18">                    
                                    <div class="section-name">Exploring the Spark shell</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec19" class="sub-nav">
                                <a href="#ch02lvl1sec19">                    
                                    <div class="section-name">Developing Spark applications in Eclipse with Maven</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec20" class="sub-nav">
                                <a href="#ch02lvl1sec20">                    
                                    <div class="section-name">Developing Spark applications in Eclipse with SBT</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec21" class="sub-nav">
                                <a href="#ch02lvl1sec21">                    
                                    <div class="section-name">Developing a Spark application in IntelliJ IDEA with Maven</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec22" class="sub-nav">
                                <a href="#ch02lvl1sec22">                    
                                    <div class="section-name">Developing a Spark application in IntelliJ IDEA with SBT</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse3">
                                <div class="section-name">3: External Data Sources</div>
                            </a>
                        </li>
                        <div id="collapse3" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="3" class="sub-nav">
                                <a href="#ch03">
                                    <div class="section-name">Chapter 3: External Data Sources</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec23" class="sub-nav">
                                <a href="#ch03lvl1sec23">                    
                                    <div class="section-name">Introduction</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec24" class="sub-nav">
                                <a href="#ch03lvl1sec24">                    
                                    <div class="section-name">Loading data from the local filesystem</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec25" class="sub-nav">
                                <a href="#ch03lvl1sec25">                    
                                    <div class="section-name">Loading data from HDFS</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec26" class="sub-nav">
                                <a href="#ch03lvl1sec26">                    
                                    <div class="section-name">Loading data from HDFS using a custom InputFormat</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec27" class="sub-nav">
                                <a href="#ch03lvl1sec27">                    
                                    <div class="section-name">Loading data from Amazon S3</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec28" class="sub-nav">
                                <a href="#ch03lvl1sec28">                    
                                    <div class="section-name">Loading data from Apache Cassandra</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec29" class="sub-nav">
                                <a href="#ch03lvl1sec29">                    
                                    <div class="section-name">Loading data from relational databases</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse4">
                                <div class="section-name">4: Spark SQL</div>
                            </a>
                        </li>
                        <div id="collapse4" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="4" class="sub-nav">
                                <a href="#ch04">
                                    <div class="section-name">Chapter 4: Spark SQL</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec30" class="sub-nav">
                                <a href="#ch04lvl1sec30">                    
                                    <div class="section-name">Introduction</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec31" class="sub-nav">
                                <a href="#ch04lvl1sec31">                    
                                    <div class="section-name">Understanding the Catalyst optimizer</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec32" class="sub-nav">
                                <a href="#ch04lvl1sec32">                    
                                    <div class="section-name">Creating HiveContext</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec33" class="sub-nav">
                                <a href="#ch04lvl1sec33">                    
                                    <div class="section-name">Inferring schema using case classes</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec34" class="sub-nav">
                                <a href="#ch04lvl1sec34">                    
                                    <div class="section-name">Programmatically specifying the schema</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec35" class="sub-nav">
                                <a href="#ch04lvl1sec35">                    
                                    <div class="section-name">Loading and saving data using the Parquet format</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec36" class="sub-nav">
                                <a href="#ch04lvl1sec36">                    
                                    <div class="section-name">Loading and saving data using the JSON format</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec37" class="sub-nav">
                                <a href="#ch04lvl1sec37">                    
                                    <div class="section-name">Loading and saving data from relational databases</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec38" class="sub-nav">
                                <a href="#ch04lvl1sec38">                    
                                    <div class="section-name">Loading and saving data from an arbitrary source</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse5">
                                <div class="section-name">5: Spark Streaming</div>
                            </a>
                        </li>
                        <div id="collapse5" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="5" class="sub-nav">
                                <a href="#ch05">
                                    <div class="section-name">Chapter 5: Spark Streaming</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec39" class="sub-nav">
                                <a href="#ch05lvl1sec39">                    
                                    <div class="section-name">Introduction</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec40" class="sub-nav">
                                <a href="#ch05lvl1sec40">                    
                                    <div class="section-name">Word count using Streaming</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec41" class="sub-nav">
                                <a href="#ch05lvl1sec41">                    
                                    <div class="section-name">Streaming Twitter data</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec42" class="sub-nav">
                                <a href="#ch05lvl1sec42">                    
                                    <div class="section-name">Streaming using Kafka</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse6">
                                <div class="section-name">6: Getting Started with Machine Learning Using MLlib</div>
                            </a>
                        </li>
                        <div id="collapse6" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="6" class="sub-nav">
                                <a href="#ch06">
                                    <div class="section-name">Chapter 6: Getting Started with Machine Learning Using MLlib</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec43" class="sub-nav">
                                <a href="#ch06lvl1sec43">                    
                                    <div class="section-name">Introduction</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec44" class="sub-nav">
                                <a href="#ch06lvl1sec44">                    
                                    <div class="section-name">Creating vectors</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec45" class="sub-nav">
                                <a href="#ch06lvl1sec45">                    
                                    <div class="section-name">Creating a labeled point</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec46" class="sub-nav">
                                <a href="#ch06lvl1sec46">                    
                                    <div class="section-name">Creating matrices</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec47" class="sub-nav">
                                <a href="#ch06lvl1sec47">                    
                                    <div class="section-name">Calculating summary statistics</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec48" class="sub-nav">
                                <a href="#ch06lvl1sec48">                    
                                    <div class="section-name">Calculating correlation</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec49" class="sub-nav">
                                <a href="#ch06lvl1sec49">                    
                                    <div class="section-name">Doing hypothesis testing</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec50" class="sub-nav">
                                <a href="#ch06lvl1sec50">                    
                                    <div class="section-name">Creating machine learning pipelines using ML</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse7">
                                <div class="section-name">7: Supervised Learning with MLlib – Regression</div>
                            </a>
                        </li>
                        <div id="collapse7" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="7" class="sub-nav">
                                <a href="#ch07">
                                    <div class="section-name">Chapter 7: Supervised Learning with MLlib – Regression</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec51" class="sub-nav">
                                <a href="#ch07lvl1sec51">                    
                                    <div class="section-name">Introduction</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec52" class="sub-nav">
                                <a href="#ch07lvl1sec52">                    
                                    <div class="section-name">Using linear regression</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec53" class="sub-nav">
                                <a href="#ch07lvl1sec53">                    
                                    <div class="section-name">Understanding cost function</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec54" class="sub-nav">
                                <a href="#ch07lvl1sec54">                    
                                    <div class="section-name">Doing linear regression with lasso</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec55" class="sub-nav">
                                <a href="#ch07lvl1sec55">                    
                                    <div class="section-name">Doing ridge regression</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse8">
                                <div class="section-name">8: Supervised Learning with MLlib – Classification</div>
                            </a>
                        </li>
                        <div id="collapse8" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="8" class="sub-nav">
                                <a href="#ch08">
                                    <div class="section-name">Chapter 8: Supervised Learning with MLlib – Classification</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec56" class="sub-nav">
                                <a href="#ch08lvl1sec56">                    
                                    <div class="section-name">Introduction</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec57" class="sub-nav">
                                <a href="#ch08lvl1sec57">                    
                                    <div class="section-name">Doing classification using logistic regression</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec58" class="sub-nav">
                                <a href="#ch08lvl1sec58">                    
                                    <div class="section-name">Doing binary classification using SVM</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec59" class="sub-nav">
                                <a href="#ch08lvl1sec59">                    
                                    <div class="section-name">Doing classification using decision trees</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec60" class="sub-nav">
                                <a href="#ch08lvl1sec60">                    
                                    <div class="section-name">Doing classification using Random Forests</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec61" class="sub-nav">
                                <a href="#ch08lvl1sec61">                    
                                    <div class="section-name">Doing classification using Gradient Boosted Trees</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec62" class="sub-nav">
                                <a href="#ch08lvl1sec62">                    
                                    <div class="section-name">Doing classification with Nave Bayes</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse9">
                                <div class="section-name">9: Unsupervised Learning with MLlib</div>
                            </a>
                        </li>
                        <div id="collapse9" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="9" class="sub-nav">
                                <a href="#ch09">
                                    <div class="section-name">Chapter 9: Unsupervised Learning with MLlib</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec63" class="sub-nav">
                                <a href="#ch09lvl1sec63">                    
                                    <div class="section-name">Introduction</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec64" class="sub-nav">
                                <a href="#ch09lvl1sec64">                    
                                    <div class="section-name">Clustering using k-means</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec65" class="sub-nav">
                                <a href="#ch09lvl1sec65">                    
                                    <div class="section-name">Dimensionality reduction with principal component analysis</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec66" class="sub-nav">
                                <a href="#ch09lvl1sec66">                    
                                    <div class="section-name">Dimensionality reduction with singular value decomposition</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse10">
                                <div class="section-name">10: Recommender Systems</div>
                            </a>
                        </li>
                        <div id="collapse10" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="10" class="sub-nav">
                                <a href="#ch10">
                                    <div class="section-name">Chapter 10: Recommender Systems</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec67" class="sub-nav">
                                <a href="#ch10lvl1sec67">                    
                                    <div class="section-name">Introduction</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec68" class="sub-nav">
                                <a href="#ch10lvl1sec68">                    
                                    <div class="section-name">Collaborative filtering using explicit feedback</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec69" class="sub-nav">
                                <a href="#ch10lvl1sec69">                    
                                    <div class="section-name">Collaborative filtering using implicit feedback</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse11">
                                <div class="section-name">11: Graph Processing Using GraphX</div>
                            </a>
                        </li>
                        <div id="collapse11" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="11" class="sub-nav">
                                <a href="#ch11">
                                    <div class="section-name">Chapter 11: Graph Processing Using GraphX</div>
                                </a>
                            </li>
                            <li data-chapter="11" data-section-id="ch11lvl1sec70" class="sub-nav">
                                <a href="#ch11lvl1sec70">                    
                                    <div class="section-name">Introduction</div>
                                </a>
                            </li>
                            <li data-chapter="11" data-section-id="ch11lvl1sec71" class="sub-nav">
                                <a href="#ch11lvl1sec71">                    
                                    <div class="section-name">Fundamental operations on graphs</div>
                                </a>
                            </li>
                            <li data-chapter="11" data-section-id="ch11lvl1sec72" class="sub-nav">
                                <a href="#ch11lvl1sec72">                    
                                    <div class="section-name">Using PageRank</div>
                                </a>
                            </li>
                            <li data-chapter="11" data-section-id="ch11lvl1sec73" class="sub-nav">
                                <a href="#ch11lvl1sec73">                    
                                    <div class="section-name">Finding connected components</div>
                                </a>
                            </li>
                            <li data-chapter="11" data-section-id="ch11lvl1sec74" class="sub-nav">
                                <a href="#ch11lvl1sec74">                    
                                    <div class="section-name">Performing neighborhood aggregation</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse12">
                                <div class="section-name">12: Optimizations and Performance Tuning</div>
                            </a>
                        </li>
                        <div id="collapse12" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="12" class="sub-nav">
                                <a href="#ch12">
                                    <div class="section-name">Chapter 12: Optimizations and Performance Tuning</div>
                                </a>
                            </li>
                            <li data-chapter="12" data-section-id="ch12lvl1sec75" class="sub-nav">
                                <a href="#ch12lvl1sec75">                    
                                    <div class="section-name">Introduction</div>
                                </a>
                            </li>
                            <li data-chapter="12" data-section-id="ch12lvl1sec76" class="sub-nav">
                                <a href="#ch12lvl1sec76">                    
                                    <div class="section-name">Optimizing memory</div>
                                </a>
                            </li>
                            <li data-chapter="12" data-section-id="ch12lvl1sec77" class="sub-nav">
                                <a href="#ch12lvl1sec77">                    
                                    <div class="section-name">Using compression to improve performance</div>
                                </a>
                            </li>
                            <li data-chapter="12" data-section-id="ch12lvl1sec78" class="sub-nav">
                                <a href="#ch12lvl1sec78">                    
                                    <div class="section-name">Using serialization to improve performance</div>
                                </a>
                            </li>
                            <li data-chapter="12" data-section-id="ch12lvl1sec79" class="sub-nav">
                                <a href="#ch12lvl1sec79">                    
                                    <div class="section-name">Optimizing garbage collection</div>
                                </a>
                            </li>
                            <li data-chapter="12" data-section-id="ch12lvl1sec80" class="sub-nav">
                                <a href="#ch12lvl1sec80">                    
                                    <div class="section-name">Optimizing the level of parallelism</div>
                                </a>
                            </li>
                            <li data-chapter="12" data-section-id="ch12lvl1sec81" class="sub-nav">
                                <a href="#ch12lvl1sec81">                    
                                    <div class="section-name">Understanding the future of optimization project Tungsten</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapsebackindex">
                                <div class="section-name">Appendix A: Index</div>
                            </a>
                        </li>
                        <div id="collapsebackindex" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="backindex" class="sub-nav">
                                <a href="#backindex">
                                    <div class="section-name">Chapter Appendix A: Index</div>
                                </a>
                            </li>
                        </div>
                    </div>
                </div>
            </ul>
        </div>
        
        <div id="page-content-wrapper" class="book-page">
            <a href="#" id="menu-toggle" class="toggle-nav"><i class="fa fa-bars fa-2x mr5"></i></a>
            
            <a href="#" id="back_to_top" class="back-to-top"><img src="../../mapt/images/kopimi.svg"></a>
            
            <div class="col-sm-12 col-xl-offset-2 col-xl-8 col-lg-offset-1 col-lg-10">
                <div class="btn-group pull-right mt15 mb30" role="group">
                    <a href="#home" class="btn btn-default">
                        <i class="fa fa-share fa-lg no-text-padding"></i>
                        <span class="hidden-xs ml5">Book Home</span>
                    </a>
                    <button class="btn btn-default disabled" id="code-download">
                        <i class="fa fa-file fa-lg"></i>
                        <span class="hidden-xs ml5">Download Code Files</span>
                    </button>
                </div>
            </div>
            <div class="clearfix"></div>
            
            <div id="book-wrapper" class="container-fluid">
                <div class="col-sm-12 col-xl-offset-2 col-xl-8 col-lg-offset-1 col-lg-10" id="home">
                    <h2 class="product-title">Spark Cookbook</h2>
                    <hr>
                    <div class="row">
                        <div class="col-sm-12">
                            <h5 class="mt10">By Rishi Yadav</h5>
                            <div>
                                <p class="mb20"><b>With over 60 recipes on Spark, covering Spark Core, Spark SQL, Spark Streaming, MLlib, and GraphX libraries this is the perfect Spark book to always have by your side</b></p>
                                <a href="#ch01" class="btn btn-info btn-lg pull-right hidden-xs">
                                    Start Reading <i class="fa fa-chevron-right ml5"></i>
                                </a>
                                <a href="#ch01" class="btn btn-info btn-lg btn-block mt20 mb20 visible-xs">
                                    Start Reading <i class="fa fa-chevron-right ml5"></i>
                                </a>
                                <div class="clearfix"></div>
                                <div class="col-sm-12">
                                    <ul id="myTabs" class="nav nav-tabs nav-justified hidden-xs mt20" role="tablist">
                                        <li class="active">
                                            <a href="#info" role="tab" id="info-tab" data-toggle="tab">
                                                <h5>Info</h5>
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#content" role="tab" id="content-tab" data-toggle="tab">
                                                <h5>Contents</h5>
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#author" role="tab" id="author-tab" data-toggle="tab">
                                                <h5>Author</h5>
                                            </a>
                                        </li>
                                    </ul>
                
                                    <ul id="myTabsMobile" class="nav nav-pills text-center nav-stacked visible-xs mb60" role="tablist">
                                        <li class="active">
                                            <a href="#info" role="tab" id="info-tab-responsive" data-toggle="tab">
                                                Info
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#content" role="tab" id="content-tab-responsive" data-toggle="tab">
                                                Contents
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#author" role="tab" id="author-tab-responsive" data-toggle="tab">
                                                Author
                                            </a>
                                        </li>
                                    </ul>
                
                                    <div id="myTabContent" class="tab-content pt30">
                                    
                                        <div role="tabpanel" class="tab-pane active fade in" id="info">
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">Features</h5>
                                            </div>
                                            <div class="hidden-xs">
                                                <h5>Features</h5>
                                            </div>
                                            <hr>
                                            <div>
                                                <ul>
                <li>Become an expert at graph processing using GraphX</li>
                <li>Use Apache Spark as your single big data compute platform and master its libraries</li>
                <li>Learn with recipes that can be run on a single machine as well as on a production cluster of thousands of machines</li>
                </ul>
                                            </div>
                                            <br>
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">Learning</h5>
                                            </div>
                                            <div class="hidden-xs">
                                                <h5>Learning</h5>
                                            </div>
                                            <hr>
                                            <div>
                                                <ul>
                <li>Install and configure Apache Spark with various cluster managers</li>
                <li>Set up development environments</li>
                <li>Perform interactive queries using Spark SQL</li>
                <li>Get to grips with real-time streaming analytics using Spark Streaming</li>
                <li>Master supervised learning and unsupervised learning using MLlib</li>
                <li>Build a recommendation engine using MLlib</li>
                <li>Develop a set of common applications or project types, and solutions that solve complex big data problems</li>
                <li>Use Apache Spark as your single big data compute platform and master its libraries</li>
                </ul>
                                            </div>
                                            <br>
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">About</h5>
                                            </div>
                                            <div class="hidden-xs">
                                                <h5>About</h5>
                                            </div>
                                            <hr>
                                            <div>
                                                <p>By introducing in-memory persistent storage, Apache Spark eliminates the need to store intermediate data in filesystems, thereby increasing processing speed by up to 100 times.</p>
                <p>This book will focus on how to analyze large and complex sets of data. Starting with installing and configuring Apache Spark with various cluster managers, you will cover setting up development environments. You will then cover various recipes to perform interactive queries using Spark SQL and real-time streaming with various sources such as Twitter Stream and Apache Kafka. You will then focus on machine learning, including supervised learning, unsupervised learning, and recommendation engine algorithms. After mastering graph processing using GraphX, you will cover various recipes for cluster optimization and troubleshooting.</p>
                                            </div>
                                        </div>
                                        
                                        <div role="tabpanel" class="tab-pane fade in" id="content">
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">Contents</h5>
                                                <hr>
                                            </div>
                                            <ul>
                                                <div>
                                                    <li data-chapter="1">
                                                        <div class="section-name">1: Getting Started with Apache Spark</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="1" class="chapter-section">
                                                                    <a href="#ch01">        
                                                                        <div class="section-name">Chapter 1: Getting Started with Apache Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec09" class="chapter-section">
                                                                    <a href="#ch01lvl1sec09">                    
                                                                        <div class="section-name">Introduction</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec10" class="chapter-section">
                                                                    <a href="#ch01lvl1sec10">                    
                                                                        <div class="section-name">Installing Spark from binaries</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec11" class="chapter-section">
                                                                    <a href="#ch01lvl1sec11">                    
                                                                        <div class="section-name">Building the Spark source code with Maven</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec12" class="chapter-section">
                                                                    <a href="#ch01lvl1sec12">                    
                                                                        <div class="section-name">Launching Spark on Amazon EC2</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec13" class="chapter-section">
                                                                    <a href="#ch01lvl1sec13">                    
                                                                        <div class="section-name">Deploying on a cluster in standalone mode</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec14" class="chapter-section">
                                                                    <a href="#ch01lvl1sec14">                    
                                                                        <div class="section-name">Deploying on a cluster with Mesos</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec15" class="chapter-section">
                                                                    <a href="#ch01lvl1sec15">                    
                                                                        <div class="section-name">Deploying on a cluster with YARN</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec16" class="chapter-section">
                                                                    <a href="#ch01lvl1sec16">                    
                                                                        <div class="section-name">Using Tachyon as an off-heap storage layer</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="2">
                                                        <div class="section-name">2: Developing Applications with Spark</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="2" class="chapter-section">
                                                                    <a href="#ch02">        
                                                                        <div class="section-name">Chapter 2: Developing Applications with Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec17" class="chapter-section">
                                                                    <a href="#ch02lvl1sec17">                    
                                                                        <div class="section-name">Introduction</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec18" class="chapter-section">
                                                                    <a href="#ch02lvl1sec18">                    
                                                                        <div class="section-name">Exploring the Spark shell</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec19" class="chapter-section">
                                                                    <a href="#ch02lvl1sec19">                    
                                                                        <div class="section-name">Developing Spark applications in Eclipse with Maven</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec20" class="chapter-section">
                                                                    <a href="#ch02lvl1sec20">                    
                                                                        <div class="section-name">Developing Spark applications in Eclipse with SBT</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec21" class="chapter-section">
                                                                    <a href="#ch02lvl1sec21">                    
                                                                        <div class="section-name">Developing a Spark application in IntelliJ IDEA with Maven</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec22" class="chapter-section">
                                                                    <a href="#ch02lvl1sec22">                    
                                                                        <div class="section-name">Developing a Spark application in IntelliJ IDEA with SBT</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="3">
                                                        <div class="section-name">3: External Data Sources</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="3" class="chapter-section">
                                                                    <a href="#ch03">        
                                                                        <div class="section-name">Chapter 3: External Data Sources</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec23" class="chapter-section">
                                                                    <a href="#ch03lvl1sec23">                    
                                                                        <div class="section-name">Introduction</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec24" class="chapter-section">
                                                                    <a href="#ch03lvl1sec24">                    
                                                                        <div class="section-name">Loading data from the local filesystem</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec25" class="chapter-section">
                                                                    <a href="#ch03lvl1sec25">                    
                                                                        <div class="section-name">Loading data from HDFS</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec26" class="chapter-section">
                                                                    <a href="#ch03lvl1sec26">                    
                                                                        <div class="section-name">Loading data from HDFS using a custom InputFormat</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec27" class="chapter-section">
                                                                    <a href="#ch03lvl1sec27">                    
                                                                        <div class="section-name">Loading data from Amazon S3</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec28" class="chapter-section">
                                                                    <a href="#ch03lvl1sec28">                    
                                                                        <div class="section-name">Loading data from Apache Cassandra</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec29" class="chapter-section">
                                                                    <a href="#ch03lvl1sec29">                    
                                                                        <div class="section-name">Loading data from relational databases</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="4">
                                                        <div class="section-name">4: Spark SQL</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="4" class="chapter-section">
                                                                    <a href="#ch04">        
                                                                        <div class="section-name">Chapter 4: Spark SQL</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec30" class="chapter-section">
                                                                    <a href="#ch04lvl1sec30">                    
                                                                        <div class="section-name">Introduction</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec31" class="chapter-section">
                                                                    <a href="#ch04lvl1sec31">                    
                                                                        <div class="section-name">Understanding the Catalyst optimizer</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec32" class="chapter-section">
                                                                    <a href="#ch04lvl1sec32">                    
                                                                        <div class="section-name">Creating HiveContext</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec33" class="chapter-section">
                                                                    <a href="#ch04lvl1sec33">                    
                                                                        <div class="section-name">Inferring schema using case classes</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec34" class="chapter-section">
                                                                    <a href="#ch04lvl1sec34">                    
                                                                        <div class="section-name">Programmatically specifying the schema</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec35" class="chapter-section">
                                                                    <a href="#ch04lvl1sec35">                    
                                                                        <div class="section-name">Loading and saving data using the Parquet format</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec36" class="chapter-section">
                                                                    <a href="#ch04lvl1sec36">                    
                                                                        <div class="section-name">Loading and saving data using the JSON format</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec37" class="chapter-section">
                                                                    <a href="#ch04lvl1sec37">                    
                                                                        <div class="section-name">Loading and saving data from relational databases</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec38" class="chapter-section">
                                                                    <a href="#ch04lvl1sec38">                    
                                                                        <div class="section-name">Loading and saving data from an arbitrary source</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="5">
                                                        <div class="section-name">5: Spark Streaming</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="5" class="chapter-section">
                                                                    <a href="#ch05">        
                                                                        <div class="section-name">Chapter 5: Spark Streaming</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec39" class="chapter-section">
                                                                    <a href="#ch05lvl1sec39">                    
                                                                        <div class="section-name">Introduction</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec40" class="chapter-section">
                                                                    <a href="#ch05lvl1sec40">                    
                                                                        <div class="section-name">Word count using Streaming</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec41" class="chapter-section">
                                                                    <a href="#ch05lvl1sec41">                    
                                                                        <div class="section-name">Streaming Twitter data</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec42" class="chapter-section">
                                                                    <a href="#ch05lvl1sec42">                    
                                                                        <div class="section-name">Streaming using Kafka</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="6">
                                                        <div class="section-name">6: Getting Started with Machine Learning Using MLlib</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="6" class="chapter-section">
                                                                    <a href="#ch06">        
                                                                        <div class="section-name">Chapter 6: Getting Started with Machine Learning Using MLlib</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec43" class="chapter-section">
                                                                    <a href="#ch06lvl1sec43">                    
                                                                        <div class="section-name">Introduction</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec44" class="chapter-section">
                                                                    <a href="#ch06lvl1sec44">                    
                                                                        <div class="section-name">Creating vectors</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec45" class="chapter-section">
                                                                    <a href="#ch06lvl1sec45">                    
                                                                        <div class="section-name">Creating a labeled point</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec46" class="chapter-section">
                                                                    <a href="#ch06lvl1sec46">                    
                                                                        <div class="section-name">Creating matrices</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec47" class="chapter-section">
                                                                    <a href="#ch06lvl1sec47">                    
                                                                        <div class="section-name">Calculating summary statistics</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec48" class="chapter-section">
                                                                    <a href="#ch06lvl1sec48">                    
                                                                        <div class="section-name">Calculating correlation</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec49" class="chapter-section">
                                                                    <a href="#ch06lvl1sec49">                    
                                                                        <div class="section-name">Doing hypothesis testing</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec50" class="chapter-section">
                                                                    <a href="#ch06lvl1sec50">                    
                                                                        <div class="section-name">Creating machine learning pipelines using ML</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="7">
                                                        <div class="section-name">7: Supervised Learning with MLlib – Regression</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="7" class="chapter-section">
                                                                    <a href="#ch07">        
                                                                        <div class="section-name">Chapter 7: Supervised Learning with MLlib – Regression</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec51" class="chapter-section">
                                                                    <a href="#ch07lvl1sec51">                    
                                                                        <div class="section-name">Introduction</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec52" class="chapter-section">
                                                                    <a href="#ch07lvl1sec52">                    
                                                                        <div class="section-name">Using linear regression</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec53" class="chapter-section">
                                                                    <a href="#ch07lvl1sec53">                    
                                                                        <div class="section-name">Understanding cost function</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec54" class="chapter-section">
                                                                    <a href="#ch07lvl1sec54">                    
                                                                        <div class="section-name">Doing linear regression with lasso</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec55" class="chapter-section">
                                                                    <a href="#ch07lvl1sec55">                    
                                                                        <div class="section-name">Doing ridge regression</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="8">
                                                        <div class="section-name">8: Supervised Learning with MLlib – Classification</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="8" class="chapter-section">
                                                                    <a href="#ch08">        
                                                                        <div class="section-name">Chapter 8: Supervised Learning with MLlib – Classification</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec56" class="chapter-section">
                                                                    <a href="#ch08lvl1sec56">                    
                                                                        <div class="section-name">Introduction</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec57" class="chapter-section">
                                                                    <a href="#ch08lvl1sec57">                    
                                                                        <div class="section-name">Doing classification using logistic regression</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec58" class="chapter-section">
                                                                    <a href="#ch08lvl1sec58">                    
                                                                        <div class="section-name">Doing binary classification using SVM</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec59" class="chapter-section">
                                                                    <a href="#ch08lvl1sec59">                    
                                                                        <div class="section-name">Doing classification using decision trees</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec60" class="chapter-section">
                                                                    <a href="#ch08lvl1sec60">                    
                                                                        <div class="section-name">Doing classification using Random Forests</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec61" class="chapter-section">
                                                                    <a href="#ch08lvl1sec61">                    
                                                                        <div class="section-name">Doing classification using Gradient Boosted Trees</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec62" class="chapter-section">
                                                                    <a href="#ch08lvl1sec62">                    
                                                                        <div class="section-name">Doing classification with Nave Bayes</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="9">
                                                        <div class="section-name">9: Unsupervised Learning with MLlib</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="9" class="chapter-section">
                                                                    <a href="#ch09">        
                                                                        <div class="section-name">Chapter 9: Unsupervised Learning with MLlib</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec63" class="chapter-section">
                                                                    <a href="#ch09lvl1sec63">                    
                                                                        <div class="section-name">Introduction</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec64" class="chapter-section">
                                                                    <a href="#ch09lvl1sec64">                    
                                                                        <div class="section-name">Clustering using k-means</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec65" class="chapter-section">
                                                                    <a href="#ch09lvl1sec65">                    
                                                                        <div class="section-name">Dimensionality reduction with principal component analysis</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec66" class="chapter-section">
                                                                    <a href="#ch09lvl1sec66">                    
                                                                        <div class="section-name">Dimensionality reduction with singular value decomposition</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="10">
                                                        <div class="section-name">10: Recommender Systems</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="10" class="chapter-section">
                                                                    <a href="#ch10">        
                                                                        <div class="section-name">Chapter 10: Recommender Systems</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec67" class="chapter-section">
                                                                    <a href="#ch10lvl1sec67">                    
                                                                        <div class="section-name">Introduction</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec68" class="chapter-section">
                                                                    <a href="#ch10lvl1sec68">                    
                                                                        <div class="section-name">Collaborative filtering using explicit feedback</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec69" class="chapter-section">
                                                                    <a href="#ch10lvl1sec69">                    
                                                                        <div class="section-name">Collaborative filtering using implicit feedback</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="11">
                                                        <div class="section-name">11: Graph Processing Using GraphX</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="11" class="chapter-section">
                                                                    <a href="#ch11">        
                                                                        <div class="section-name">Chapter 11: Graph Processing Using GraphX</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="11" data-section-id="ch11lvl1sec70" class="chapter-section">
                                                                    <a href="#ch11lvl1sec70">                    
                                                                        <div class="section-name">Introduction</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="11" data-section-id="ch11lvl1sec71" class="chapter-section">
                                                                    <a href="#ch11lvl1sec71">                    
                                                                        <div class="section-name">Fundamental operations on graphs</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="11" data-section-id="ch11lvl1sec72" class="chapter-section">
                                                                    <a href="#ch11lvl1sec72">                    
                                                                        <div class="section-name">Using PageRank</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="11" data-section-id="ch11lvl1sec73" class="chapter-section">
                                                                    <a href="#ch11lvl1sec73">                    
                                                                        <div class="section-name">Finding connected components</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="11" data-section-id="ch11lvl1sec74" class="chapter-section">
                                                                    <a href="#ch11lvl1sec74">                    
                                                                        <div class="section-name">Performing neighborhood aggregation</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="12">
                                                        <div class="section-name">12: Optimizations and Performance Tuning</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="12" class="chapter-section">
                                                                    <a href="#ch12">        
                                                                        <div class="section-name">Chapter 12: Optimizations and Performance Tuning</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="12" data-section-id="ch12lvl1sec75" class="chapter-section">
                                                                    <a href="#ch12lvl1sec75">                    
                                                                        <div class="section-name">Introduction</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="12" data-section-id="ch12lvl1sec76" class="chapter-section">
                                                                    <a href="#ch12lvl1sec76">                    
                                                                        <div class="section-name">Optimizing memory</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="12" data-section-id="ch12lvl1sec77" class="chapter-section">
                                                                    <a href="#ch12lvl1sec77">                    
                                                                        <div class="section-name">Using compression to improve performance</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="12" data-section-id="ch12lvl1sec78" class="chapter-section">
                                                                    <a href="#ch12lvl1sec78">                    
                                                                        <div class="section-name">Using serialization to improve performance</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="12" data-section-id="ch12lvl1sec79" class="chapter-section">
                                                                    <a href="#ch12lvl1sec79">                    
                                                                        <div class="section-name">Optimizing garbage collection</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="12" data-section-id="ch12lvl1sec80" class="chapter-section">
                                                                    <a href="#ch12lvl1sec80">                    
                                                                        <div class="section-name">Optimizing the level of parallelism</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="12" data-section-id="ch12lvl1sec81" class="chapter-section">
                                                                    <a href="#ch12lvl1sec81">                    
                                                                        <div class="section-name">Understanding the future of optimization project Tungsten</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="backindex">
                                                        <div class="section-name">Appendix A: Index</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="backindex" class="chapter-section">
                                                                    <a href="#backindex">
                                                                        <div class="section-name">Chapter Appendix A: Index</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                            </ul>
                                        </div>
                                        
                                        <div role="tabpanel" class="tab-pane fade" id="author">
                                            <div class="visible-xs">
                                                <h4 class="mobile-title">About the Author</h4>
                                                <hr>
                                            </div>
                                            <p><strong>Rishi Yadav</strong></p>
                                            <div>
                                                <p>Rishi Yadav has 17 years of experience in designing and developing enterprise applications. He is an open source software expert and advises American companies on big data trends. Rishi was honored as one of Silicon Valley's 40 under 40 in 2014. He finished his bachelor's degree at the prestigious Indian Institute of Technology (IIT) Delhi in 1998.</p>
                <p>About 10 years ago, Rishi started InfoObjects, a company that helps data-driven businesses gain new insights into data.</p>
                <p>InfoObjects combines the power of open source and big data to solve business challenges for its clients and has a special focus on Apache Spark. The company has been on the Inc. 5000 list of the fastest growing companies for 4 years in a row. InfoObjects has also been awarded with the #1 best place to work in the Bay Area in 2014 and 2015.</p>
                <p>Rishi is an open source contributor and active blogger.</p>
                                            </div>
                                        </div>
                                        
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    
                    <div class="next-wrapper">
                        <div class="row ns">
                            <hr />
                            <span class="hidden-xs">
                                <h4 class="pull-left">
                                    <strong>Up Next: </strong><span class="section-title"></span>
                                </h4>
                                <a href="#" class="btn btn-primary pull-right btn-lg">
                                    Next Section
                                </a>
                            </span>
                            <span class="visible-xs">
                                <a href="#" class="btn btn-primary btn-block btn-lg">
                                    Next Section
                                </a>
                            </span>
                        </div>
                        <div class="row ns">
                            <hr>
                        </div>
                    </div>
                </div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch01"></a>Chapter 1. Getting Started with Apache Spark</h2></div></div></div><p>In this chapter, we will set up Spark and configure it. This chapter is divided into the following recipes:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Installing Spark from binaries</p></li><li style="list-style-type: disc"><p>Building the Spark source code with Maven</p></li><li style="list-style-type: disc"><p>Launching Spark on Amazon EC2</p></li><li style="list-style-type: disc"><p>Deploying Spark on a cluster in standalone mode</p></li><li style="list-style-type: disc"><p>Deploying Spark on a cluster with Mesos</p></li><li style="list-style-type: disc"><p>Deploying Spark on a cluster with YARN</p></li><li style="list-style-type: disc"><p>Using Tachyon as an off-heap storage layer</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec09"></a>Introduction</h2></div></div><hr /></div><p>Apache Spark<a id="id0" class="indexterm"></a> is a general-purpose cluster computing system to process big data workloads. What sets Spark apart from its predecessors, such as MapReduce, is its speed, ease-of-use, and sophisticated analytics.</p><p>Apache Spark was originally developed at AMPLab, UC Berkeley, in 2009. It was made open source in 2010 under the BSD license and switched to the Apache 2.0 license in 2013. Toward the later part of 2013, the creators of Spark founded Databricks to focus on Spark's development and future releases.</p><p>Talking about speed, Spark can achieve sub-second latency on big data workloads. To achieve such low latency, Spark makes use of the memory for storage. In MapReduce, memory is primarily used for actual computation. Spark uses memory both to compute and store objects.</p><p>Spark also provides a unified runtime connecting to various big data storage sources, such as HDFS, Cassandra, HBase, and S3. It also provides a rich set of higher-level libraries for different <a id="id1" class="indexterm"></a>big data compute tasks, such as machine learning, SQL processing, graph processing, and real-time streaming. These libraries make development faster and can be combined in an arbitrary fashion.</p><p>Though Spark is written in Scala, and this book only focuses on recipes in Scala, Spark also supports Java and Python.</p><p>Spark is an open source community project, and everyone uses the pure open source Apache distributions for deployments, unlike Hadoop, which has multiple distributions available with vendor enhancements.</p><p>The following figure<a id="id2" class="indexterm"></a> shows the Spark ecosystem:</p><div class="mediaobject"><img src="graphics/3056_01_01.jpg" /></div><p>The Spark runtime runs on top of a variety of cluster managers, including YARN (Hadoop's compute framework), Mesos, and Spark's own cluster manager called <span class="strong"><strong>standalone mode</strong></span>. Tachyon<a id="id3" class="indexterm"></a> is a memory-centric distributed file system that<a id="id4" class="indexterm"></a> enables reliable file sharing at memory speed across cluster frameworks. In short, it is an off-heap storage layer in memory, which helps share data across jobs and users. Mesos<a id="id5" class="indexterm"></a> is a cluster manager, which is evolving into a data center operating system. YARN<a id="id6" class="indexterm"></a> is Hadoop's compute framework that has a robust resource management feature that Spark can seamlessly use.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec10"></a>Installing Spark from binaries</h2></div></div><hr /></div><p>Spark can be either built from the source code or precompiled binaries can be downloaded from <a class="ulink" href="http://spark.apache.org" target="_blank">http://spark.apache.org</a>. For a standard use case, binaries are good enough, and <a id="id7" class="indexterm"></a>this recipe will focus on installing Spark using binaries.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec12"></a>Getting ready</h3></div></div></div><p>All the<a id="id8" class="indexterm"></a> recipes in this book are developed using Ubuntu Linux<a id="id9" class="indexterm"></a> but should work fine on any POSIX environment. Spark expects Java to be installed and the <code class="literal">JAVA_HOME</code> environment variable to be set.</p><p>In Linux/Unix systems, there are certain standards for the location of files and directories, which we are going to follow in this book. The following is a quick cheat sheet:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Directory</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Description</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p><code class="literal">/bin</code></p>
</td><td style="" align="left" valign="top">
<p>Essential command binaries</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">/etc</code></p>
</td><td style="" align="left" valign="top">
<p>Host-specific system configuration</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">/opt</code></p>
</td><td style="" align="left" valign="top">
<p>Add-on application software packages</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">/var</code></p>
</td><td style="" align="left" valign="top">
<p>Variable data</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">/tmp</code></p>
</td><td style="" align="left" valign="top">
<p>Temporary files</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">/home</code></p>
</td><td style="" align="left" valign="top">
<p>User home directories</p>
</td></tr></tbody></table></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec13"></a>How to do it...</h3></div></div></div><p>At the time of writing this, Spark's current version is 1.4. Please check the latest version from Spark's download<a id="id10" class="indexterm"></a> page at <a class="ulink" href="http://spark.apache.org/downloads.html" target="_blank">http://spark.apache.org/downloads.html</a>. Binaries are developed with a most recent and stable version of Hadoop. To use a specific version of Hadoop, the recommended approach is to build from sources, which will be covered in the next recipe.</p><p>The following are the installation steps:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Open the terminal and download binaries using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ wget http://d3kbcqa49mib13.cloudfront.net/spark-1.4.0-bin-hadoop2.4.tgz</strong></span>
</pre></div></li><li><p>Unpack binaries:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ tar -zxf spark-1.4.0-bin-hadoop2.4.tgz</strong></span>
</pre></div></li><li><p>Rename the folder containing binaries by stripping the version information:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo mv spark-1.4.0-bin-hadoop2.4 spark</strong></span>
</pre></div></li><li><p>Move<a id="id11" class="indexterm"></a> the configuration folder to the <code class="literal">/etc</code> folder<a id="id12" class="indexterm"></a> so that it can be made a symbolic link later:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo mv spark/conf/* /etc/spark</strong></span>
</pre></div></li><li><p>Create your company-specific installation directory under <code class="literal">/opt</code>. As the recipes in this book are tested on <code class="literal">infoobjects</code> sandbox, we are going to use <code class="literal">infoobjects</code> as directory name. Create the <code class="literal">/opt/infoobjects</code> directory:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo mkdir -p /opt/infoobjects</strong></span>
</pre></div></li><li><p>Move the <code class="literal">spark</code> directory to <code class="literal">/opt/infoobjects</code> as it's an add-on software package:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo mv spark /opt/infoobjects/</strong></span>
</pre></div></li><li><p>Change the ownership of the <code class="literal">spark</code> home directory to <code class="literal">root</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo chown -R root:root /opt/infoobjects/spark</strong></span>
</pre></div></li><li><p>Change permissions of the <code class="literal">spark</code> home directory, <code class="literal">0755 = user:read-write-execute group:read-execute world:read-execute</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo chmod -R 755 /opt/infoobjects/spark</strong></span>
</pre></div></li><li><p>Move to the <code class="literal">spark</code> home directory:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cd /opt/infoobjects/spark</strong></span>
</pre></div></li><li><p>Create the symbolic link:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo ln -s /etc/spark conf</strong></span>
</pre></div></li><li><p>Append to <code class="literal">PATH</code> in <code class="literal">.bashrc</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ echo "export PATH=$PATH:/opt/infoobjects/spark/bin" &gt;&gt; /home/hduser/.bashrc</strong></span>
</pre></div></li><li><p>Open a new terminal.</p></li><li><p>Create the <code class="literal">log</code> directory in <code class="literal">/var</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo mkdir -p /var/log/spark</strong></span>
</pre></div></li><li><p>Make <code class="literal">hduser</code> the owner of the Spark <code class="literal">log</code> directory.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo chown -R hduser:hduser /var/log/spark</strong></span>
</pre></div></li><li><p>Create the Spark <code class="literal">tmp</code> directory:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ mkdir /tmp/spark</strong></span>
</pre></div></li><li><p>Configure <a id="id13" class="indexterm"></a>Spark with the help of the following<a id="id14" class="indexterm"></a> command lines:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cd /etc/spark</strong></span>
<span class="strong"><strong>$ echo "export HADOOP_CONF_DIR=/opt/infoobjects/hadoop/etc/hadoop" &gt;&gt; spark-env.sh</strong></span>
<span class="strong"><strong>$ echo "export YARN_CONF_DIR=/opt/infoobjects/hadoop/etc/Hadoop" &gt;&gt; spark-env.sh</strong></span>
<span class="strong"><strong>$ echo "export SPARK_LOG_DIR=/var/log/spark" &gt;&gt; spark-env.sh</strong></span>
<span class="strong"><strong>$ echo "export SPARK_WORKER_DIR=/tmp/spark" &gt;&gt; spark-env.sh</strong></span>
</pre></div></li></ol></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec11"></a>Building the Spark source code with Maven</h2></div></div><hr /></div><p>Installing <a id="id15" class="indexterm"></a>Spark using binaries works fine in most <a id="id16" class="indexterm"></a>cases. For advanced cases, such as the following (but not limited to), compiling from the source code is a better option:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Compiling for a specific Hadoop version</p></li><li style="list-style-type: disc"><p>Adding the Hive integration</p></li><li style="list-style-type: disc"><p>Adding the YARN integration</p></li></ul></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec14"></a>Getting ready</h3></div></div></div><p>The following are the prerequisites for this recipe to work:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Java 1.6 or a later version</p></li><li style="list-style-type: disc"><p>Maven 3.x</p></li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec15"></a>How to do it...</h3></div></div></div><p>The following are the steps to build the Spark source code with Maven:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Increase <code class="literal">MaxPermSize</code> for heap:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ echo "export _JAVA_OPTIONS=\"-XX:MaxPermSize=1G\""  &gt;&gt; /home/hduser/.bashrc</strong></span>
</pre></div></li><li><p>Open a new terminal window and download the Spark source code from GitHub:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ wget https://github.com/apache/spark/archive/branch-1.4.zip</strong></span>
</pre></div></li><li><p>Unpack the archive:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ gunzip branch-1.4.zip</strong></span>
</pre></div></li><li><p>Move to the <code class="literal">spark</code> directory:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cd spark</strong></span>
</pre></div></li><li><p>Compile<a id="id17" class="indexterm"></a> the sources with these<a id="id18" class="indexterm"></a> flags: Yarn enabled, Hadoop version 2.4, Hive enabled, and skipping tests for faster compilation:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ mvn -Pyarn -Phadoop-2.4 -Dhadoop.version=2.4.0 -Phive -DskipTests clean package</strong></span>
</pre></div></li><li><p>Move the <code class="literal">conf</code> folder to the <code class="literal">etc</code> folder so that it can be made a symbolic link:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo mv spark/conf /etc/</strong></span>
</pre></div></li><li><p>Move the <code class="literal">spark</code> directory to <code class="literal">/opt</code> as it's an add-on software package:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo mv spark /opt/infoobjects/spark</strong></span>
</pre></div></li><li><p>Change the ownership of the <code class="literal">spark</code> home directory to <code class="literal">root</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo chown -R root:root /opt/infoobjects/spark</strong></span>
</pre></div></li><li><p>Change the permissions of the <code class="literal">spark</code> home directory <code class="literal">0755 = user:rwx group:r-x world:r-x</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo chmod -R 755 /opt/infoobjects/spark</strong></span>
</pre></div></li><li><p>Move to the <code class="literal">spark</code> home directory:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cd /opt/infoobjects/spark</strong></span>
</pre></div></li><li><p>Create a symbolic link:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo ln -s /etc/spark conf</strong></span>
</pre></div></li><li><p>Put the Spark executable in the path by editing <code class="literal">.bashrc</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ echo "export PATH=$PATH:/opt/infoobjects/spark/bin" &gt;&gt; /home/hduser/.bashrc</strong></span>
</pre></div></li><li><p>Create the <code class="literal">log</code> directory in <code class="literal">/var</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo mkdir -p /var/log/spark</strong></span>
</pre></div></li><li><p>Make <code class="literal">hduser</code> the owner of the Spark <code class="literal">log</code> directory:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo chown -R hduser:hduser /var/log/spark</strong></span>
</pre></div></li><li><p>Create the Spark <code class="literal">tmp</code> directory:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ mkdir /tmp/spark</strong></span>
</pre></div></li><li><p>Configure Spark with the help of the following command lines:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cd /etc/spark</strong></span>
<span class="strong"><strong>$ echo "export HADOOP_CONF_DIR=/opt/infoobjects/hadoop/etc/hadoop" &gt;&gt; spark-env.sh</strong></span>
<span class="strong"><strong>$ echo "export YARN_CONF_DIR=/opt/infoobjects/hadoop/etc/Hadoop" &gt;&gt; spark-env.sh</strong></span>
<span class="strong"><strong>$ echo "export SPARK_LOG_DIR=/var/log/spark" &gt;&gt; spark-env.sh</strong></span>
<span class="strong"><strong>$ echo "export SPARK_WORKER_DIR=/tmp/spark" &gt;&gt; spark-env.sh</strong></span>
</pre></div></li></ol></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec12"></a>Launching Spark on Amazon EC2</h2></div></div><hr /></div><p><span class="strong"><strong>Amazon Elastic Compute Cloud</strong></span> (<span class="strong"><strong>Amazon EC2</strong></span>) is a web service that provides resizable <a id="id19" class="indexterm"></a>compute instances in the cloud. Amazon EC2 provides the following features:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>On-demand <a id="id20" class="indexterm"></a>delivery of IT resources via the Internet</p></li><li style="list-style-type: disc"><p>The <a id="id21" class="indexterm"></a>provision of as many instances as you like</p></li><li style="list-style-type: disc"><p>Payment<a id="id22" class="indexterm"></a> for the hours you use instances like your utility bill</p></li><li style="list-style-type: disc"><p>No setup cost, no installation, and no overhead at all</p></li><li style="list-style-type: disc"><p>When you no longer need instances, you either shut down or terminate and walk away</p></li><li style="list-style-type: disc"><p>The availability of these instances on all familiar operating systems</p></li></ul></div><p>EC2 provides different types of instances to meet all compute needs, such as general-purpose instances, micro instances, memory-optimized instances, storage-optimized instances, and others. They have a free tier of micro-instances to try.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec16"></a>Getting ready</h3></div></div></div><p>The <code class="literal">spark-ec2</code> script<a id="id23" class="indexterm"></a> comes bundled with Spark and makes it easy to launch, manage, and shut down clusters on Amazon EC2.</p><p>Before you start, you <a id="id24" class="indexterm"></a>need to do the following things:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Log in to the Amazon AWS account (<a class="ulink" href="http://aws.amazon.com" target="_blank">http://aws.amazon.com</a>).</p></li><li><p>Click on <span class="strong"><strong>Security Credentials</strong></span> under your account name in the top-right corner.</p></li><li><p>Click on <span class="strong"><strong>Access Keys</strong></span> and <span class="strong"><strong>Create New Access Key</strong></span>:</p><div class="mediaobject"><img src="graphics/3056_01_02.jpg" /></div></li><li><p>Note down the access key ID and secret access key.</p></li><li><p>Now go to <span class="strong"><strong>Services</strong></span> | <span class="strong"><strong>EC2</strong></span>.</p></li><li><p>Click on <span class="strong"><strong>Key Pairs</strong></span> in left-hand menu under NETWORK &amp; SECURITY.</p></li><li><p>Click on <span class="strong"><strong>Create Key Pair</strong></span> and enter <code class="literal">kp-spark</code> as key-pair name:</p><div class="mediaobject"><img src="graphics/3056_01_15.jpg" /></div></li><li><p>Download the <a id="id25" class="indexterm"></a>private key file and copy it in the <code class="literal">/home/hduser/keypairs folder</code>.</p></li><li><p>Set <a id="id26" class="indexterm"></a>permissions on key file to <code class="literal">600</code>.</p></li><li><p>Set environment variables to reflect access key ID and secret access key (please replace sample values with your own values):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ echo "export AWS_ACCESS_KEY_ID=\"AKIAOD7M2LOWATFXFKQ\"" &gt;&gt; /home/hduser/.bashrc</strong></span>
<span class="strong"><strong>$ echo "export AWS_SECRET_ACCESS_KEY=\"+Xr4UroVYJxiLiY8DLT4DLT4D4sxc3ijZGMx1D3pfZ2q\"" &gt;&gt; /home/hduser/.bashrc</strong></span>
<span class="strong"><strong>$ echo "export PATH=$PATH:/opt/infoobjects/spark/ec2" &gt;&gt; /home/hduser/.bashrc</strong></span>
</pre></div></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec17"></a>How to do it...</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Spark comes bundled with scripts to launch the Spark cluster on Amazon EC2. Let's launch the cluster using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cd /home/hduser</strong></span>
<span class="strong"><strong>$ spark-ec2 -k &lt;key-pair&gt; -i &lt;key-file&gt; -s &lt;num-slaves&gt; launch &lt;cluster-name&gt;</strong></span>
</pre></div></li><li><p>Launch the cluster with the example value:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-ec2 -k kp-spark -i /home/hduser/keypairs/kp-spark.pem --hadoop-major-version 2  -s 3 launch spark-cluster</strong></span>
</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note003"></a>Note</h3><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><code class="literal">&lt;key-pair&gt;</code>: This is the name of EC2 key-pair created in AWS</p></li><li style="list-style-type: disc"><p><code class="literal">&lt;key-file&gt;</code>: This is the private key file you downloaded</p></li><li style="list-style-type: disc"><p><code class="literal">&lt;num-slaves&gt;</code>: This is the number of slave nodes to launch</p></li><li style="list-style-type: disc"><p><code class="literal">&lt;cluster-name&gt;</code>: This is the name of the cluster</p></li></ul></div></div></li><li><p>Sometimes, the default availability zones are not available; in that case, retry sending the request by specifying the specific availability zone you are requesting:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-ec2 -k kp-spark -i /home/hduser/keypairs/kp-spark.pem -z us-east-1b --hadoop-major-version 2  -s 3 launch spark-cluster</strong></span>
</pre></div></li><li><p>If your <a id="id27" class="indexterm"></a>application needs to retain data after<a id="id28" class="indexterm"></a> the instance shuts down, attach EBS volume to it (for example, a 10 GB space):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-ec2 -k kp-spark -i /home/hduser/keypairs/kp-spark.pem --hadoop-major-version 2 -ebs-vol-size 10 -s 3 launch spark-cluster</strong></span>
</pre></div></li><li><p>If you use Amazon spot instances, here's the way to do it:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-ec2 -k kp-spark -i /home/hduser/keypairs/kp-spark.pem -spot-price=0.15 --hadoop-major-version 2  -s 3 launch spark-cluster</strong></span>
</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note03"></a>Note</h3><p>Spot instances allow you to name your own price for Amazon EC2 computing capacity. You simply bid on spare Amazon EC2 instances and run them whenever your bid exceeds the current spot price, which varies in real-time based on supply and demand (source: <a class="ulink" href="http://amazon.com" target="_blank">amazon.com</a>).</p></div></li><li><p>After everything is launched, check the status of the cluster by going to the web UI URL that will be printed at the end.</p><div class="mediaobject"><img src="graphics/3056_01_03.jpg" /></div></li><li><p>Check the status of the cluster:</p><div class="mediaobject"><img src="graphics/3056_01_04.jpg" /></div></li><li><p>Now, to <a id="id29" class="indexterm"></a>access the Spark cluster on EC2, let's <a id="id30" class="indexterm"></a>connect to the master node <a id="id31" class="indexterm"></a>using <span class="strong"><strong>secure shell protocol</strong></span> (<span class="strong"><strong>SSH</strong></span>):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-ec2 -k kp-spark -i /home/hduser/kp/kp-spark.pem  login spark-cluster</strong></span>
</pre></div><p>You should get something like the following:</p><div class="mediaobject"><img src="graphics/3056_01_05.jpg" /></div></li><li><p>Check directories in the master node and see what they do:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Directory</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Description</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p><code class="literal">ephemeral-hdfs</code></p>
</td><td style="" align="left" valign="top">
<p>This<a id="id32" class="indexterm"></a> is the Hadoop instance for which data is ephemeral and gets deleted when you stop or restart the machine.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">persistent-hdfs</code></p>
</td><td style="" align="left" valign="top">
<p>Each <a id="id33" class="indexterm"></a>node has a very small amount of persistent storage (approximately 3 GB). If you use this instance, data will be retained in that space.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">hadoop-native</code></p>
</td><td style="" align="left" valign="top">
<p>These<a id="id34" class="indexterm"></a> are native libraries to support Hadoop, such as snappy compression libraries.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">Scala</code></p>
</td><td style="" align="left" valign="top">
<p>This<a id="id35" class="indexterm"></a> is Scala installation.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">shark</code></p>
</td><td style="" align="left" valign="top">
<p>This<a id="id36" class="indexterm"></a> is Shark installation (Shark is no longer supported and is replaced by Spark SQL).</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">spark</code></p>
</td><td style="" align="left" valign="top">
<p>This<a id="id37" class="indexterm"></a> is Spark installation</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">spark-ec2</code></p>
</td><td style="" align="left" valign="top">
<p>These <a id="id38" class="indexterm"></a>are files to support this cluster deployment.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">tachyon</code></p>
</td><td style="" align="left" valign="top">
<p>This<a id="id39" class="indexterm"></a> is Tachyon installation</p>
</td></tr></tbody></table></div></li><li><p>Check <a id="id40" class="indexterm"></a>the HDFS version in an ephemeral<a id="id41" class="indexterm"></a> instance:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ephemeral-hdfs/bin/hadoop version</strong></span>
<span class="strong"><strong>Hadoop 2.0.0-chd4.2.0</strong></span>
</pre></div></li><li><p>Check the HDFS version in persistent instance with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ persistent-hdfs/bin/hadoop version</strong></span>
<span class="strong"><strong>Hadoop 2.0.0-chd4.2.0</strong></span>
</pre></div></li><li><p>Change the configuration level in logs:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cd spark/conf</strong></span>
</pre></div></li><li><p>The default log level information is too verbose, so let's change it to Error:</p><div class="mediaobject"><img src="graphics/3056_01_06.jpg" /></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Create <a id="id42" class="indexterm"></a>the <code class="literal">log4.properties</code> file by renaming the template:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ mv log4j.properties.template log4j.properties</strong></span>
</pre></div></li><li><p>Open <code class="literal">log4j.properties</code> in vi or your favorite editor:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ vi log4j.properties</strong></span>
</pre></div></li><li><p>Change<a id="id43" class="indexterm"></a> second line from <code class="literal">| log4j.rootCategory=INFO, console</code> to <code class="literal">| log4j.rootCategory=ERROR, console</code>.</p></li></ol></div></li><li><p>Copy the configuration to all slave nodes after the change:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-ec2/copydir spark/conf</strong></span>
</pre></div><p>You should get something like this:</p><div class="mediaobject"><img src="graphics/3056_01_07.jpg" /></div></li><li><p>Destroy the Spark cluster:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-ec2 destroy spark-cluster</strong></span>
</pre></div></li></ol></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl2sec18"></a>See also</h4></div></div></div><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><a class="ulink" href="http://aws.amazon.com/ec2" target="_blank">http://aws.amazon.com/ec2</a></p></li></ul></div></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec13"></a>Deploying on a cluster in standalone mode</h2></div></div><hr /></div><p>Compute resources in a distributed environment need to be managed so that resource utilization is<a id="id44" class="indexterm"></a> efficient and every job gets a fair <a id="id45" class="indexterm"></a>chance to run. Spark comes along with its own cluster manager conveniently called <span class="strong"><strong>standalone mode</strong></span>. Spark also supports working with YARN and Mesos cluster managers.</p><p>The cluster manager that should be chosen is mostly driven by both legacy concerns and whether other frameworks, such as MapReduce, are sharing the same compute resource pool. If your cluster has legacy MapReduce jobs running, and all of them cannot be converted to Spark jobs, it is a good idea to use YARN as the cluster manager. Mesos is emerging as a data center operating system to conveniently manage jobs across frameworks, and is very compatible with Spark.</p><p>If the Spark framework is the only framework in your cluster, then standalone mode is good enough. As Spark evolves as technology, you will see more and more use cases of Spark being used as the standalone framework serving all big data compute needs. For example, some jobs may be using Apache Mahout at present because MLlib does not have a specific machine-learning library, which the job needs. As soon as MLlib gets this library, this particular job can be moved to Spark.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec19"></a>Getting ready</h3></div></div></div><p>Let's consider a cluster of six nodes as an example setup: one master and five slaves (replace them with actual node names in your cluster):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Master</strong></span>
<span class="strong"><strong>m1.zettabytes.com</strong></span>
<span class="strong"><strong>Slaves</strong></span>
<span class="strong"><strong>s1.zettabytes.com</strong></span>
<span class="strong"><strong>s2.zettabytes.com</strong></span>
<span class="strong"><strong>s3.zettabytes.com</strong></span>
<span class="strong"><strong>s4.zettabytes.com</strong></span>
<span class="strong"><strong>s5.zettabytes.com</strong></span>
</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec20"></a>How to do it...</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Since Spark's standalone mode is the default, all you need to do is to have Spark binaries installed on both master and slave machines. Put <code class="literal">/opt/infoobjects/spark/sbin</code> in path on every node:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ echo "export PATH=$PATH:/opt/infoobjects/spark/sbin" &gt;&gt; /home/hduser/.bashrc</strong></span>
</pre></div></li><li><p>Start the<a id="id46" class="indexterm"></a> standalone master <a id="id47" class="indexterm"></a>server (SSH to master first):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>hduser@m1.zettabytes.com~] start-master.sh</strong></span>
</pre></div><p>Master, by default, starts on port 7077, which slaves use to connect to it. It also has a web UI at port 8088.</p></li><li><p>Please SSH to master node and start slaves:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>hduser@s1.zettabytes.com~] spark-class org.apache.spark.deploy.worker.Worker spark://m1.zettabytes.com:7077</strong></span>
</pre></div><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Argument (for fine-grained configuration, the following parameters work with both master and slaves)</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Meaning</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p><code class="literal">-i &lt;ipaddress&gt;,-ip &lt;ipaddress&gt;</code></p>
</td><td style="" align="left" valign="top">
<p>IP address/DNS service listens on</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">-p &lt;port&gt;, --port &lt;port&gt;</code></p>
</td><td style="" align="left" valign="top">
<p>Port service listens on</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">--webui-port &lt;port&gt;</code></p>
</td><td style="" align="left" valign="top">
<p>Port for web UI (by default, 8080 for master and 8081 for worker)</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">-c &lt;cores&gt;,--cores &lt;cores&gt;</code></p>
</td><td style="" align="left" valign="top">
<p>Total CPU cores Spark applications that can be used on a machine (worker only)</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">-m &lt;memory&gt;,--memory &lt;memory&gt;</code></p>
</td><td style="" align="left" valign="top">
<p>Total RAM Spark applications that can be used on a machine (worker only)</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">-d &lt;dir&gt;,--work-dir &lt;dir&gt;</code></p>
</td><td style="" align="left" valign="top">
<p>The directory to use for scratch space and job output logs</p>
</td></tr></tbody></table></div></li><li><p>Rather than manually starting master and slave daemons on each node, it can also be accomplished using cluster launch scripts.</p></li><li><p>First, create the <code class="literal">conf/slaves</code> file on a master node and add one line per slave hostname (using an example of five slaves nodes, replace with the DNS of slave nodes in your cluster):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>hduser@m1.zettabytes.com~] echo "s1.zettabytes.com" &gt;&gt; conf/slaves</strong></span>
<span class="strong"><strong>hduser@m1.zettabytes.com~] echo "s2.zettabytes.com" &gt;&gt; conf/slaves</strong></span>
<span class="strong"><strong>hduser@m1.zettabytes.com~] echo "s3.zettabytes.com" &gt;&gt; conf/slaves</strong></span>
<span class="strong"><strong>hduser@m1.zettabytes.com~] echo "s4.zettabytes.com" &gt;&gt; conf/slaves</strong></span>
<span class="strong"><strong>hduser@m1.zettabytes.com~] echo "s5.zettabytes.com" &gt;&gt; conf/slaves</strong></span>
</pre></div><p>Once<a id="id48" class="indexterm"></a> the slave machine is set up, you can<a id="id49" class="indexterm"></a> call the following scripts to start/stop cluster:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Script name</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Purpose</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p><code class="literal">start-master.sh</code></p>
</td><td style="" align="left" valign="top">
<p>Starts <a id="id50" class="indexterm"></a>a master instance on the host machine</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">start-slaves.sh</code></p>
</td><td style="" align="left" valign="top">
<p>Starts<a id="id51" class="indexterm"></a> a slave instance on each node in the slaves file</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">start-all.sh</code></p>
</td><td style="" align="left" valign="top">
<p>Starts<a id="id52" class="indexterm"></a> both master and slaves</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">stop-master.sh</code></p>
</td><td style="" align="left" valign="top">
<p>Stops <a id="id53" class="indexterm"></a>the master instance on the host machine</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">stop-slaves.sh</code></p>
</td><td style="" align="left" valign="top">
<p>Stops<a id="id54" class="indexterm"></a> the slave instance on all nodes in the slaves file</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">stop-all.sh</code></p>
</td><td style="" align="left" valign="top">
<p>Stops <a id="id55" class="indexterm"></a>both master and slaves</p>
</td></tr></tbody></table></div></li><li><p>Connect an application to the cluster through the Scala code:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>val sparkContext = new SparkContext(new SparkConf().setMaster("spark://m1.zettabytes.com:7077")</strong></span>
</pre></div></li><li><p>Connect to the cluster through Spark shell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell --master spark://master:7077</strong></span>
</pre></div></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec21"></a>How it works...</h3></div></div></div><p>In standalone mode, Spark follows the master slave architecture, very much like Hadoop, MapReduce, and YARN. The <a id="id56" class="indexterm"></a>compute master daemon is called <span class="strong"><strong>Spark master</strong></span> and runs on one master node. Spark master can be made highly available using ZooKeeper. You can also add more standby masters on the fly, if needed.</p><p>The compute slave daemon is <a id="id57" class="indexterm"></a>called <span class="strong"><strong>worker</strong></span> and is on each slave node. The worker daemon does the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Reports the availability of compute resources on a slave node, such as the number of cores, memory, and others, to Spark master</p></li><li style="list-style-type: disc"><p>Spawns the executor when asked to do so by Spark master</p></li><li style="list-style-type: disc"><p>Restarts the executor if it dies</p></li></ul></div><p>There is,<a id="id58" class="indexterm"></a> at most, one executor<a id="id59" class="indexterm"></a> per application per slave machine.</p><p>Both Spark master and worker are very lightweight. Typically, memory allocation between 500 MB to 1 GB is sufficient. This value can be set in <code class="literal">conf/spark-env.sh</code> by setting the <code class="literal">SPARK_DAEMON_MEMORY</code> parameter. For example, the following configuration will set the memory to 1 gigabits for both master and worker daemon. Make sure you have <code class="literal">sudo</code> as the super user before running it:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ echo "export SPARK_DAEMON_MEMORY=1g" &gt;&gt; /opt/infoobjects/spark/conf/spark-env.sh</strong></span>
</pre></div><p>By default, each slave node has one worker instance running on it. Sometimes, you may have a few machines that are more powerful than others. In that case, you can spawn more than one worker on that machine by the following configuration (only on those machines):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ echo "export SPARK_WORKER_INSTANCES=2" &gt;&gt; /opt/infoobjects/spark/conf/spark-env.sh</strong></span>
</pre></div><p>Spark worker, by default, uses all cores on the slave machine for its executors. If you would like to limit the number of cores the worker can use, you can set it to that number (for example, 12) by the following configuration:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ echo "export SPARK_WORKER_CORES=12" &gt;&gt; /opt/infoobjects/spark/conf/spark-env.sh</strong></span>
</pre></div><p>Spark worker, by default, uses all the available RAM (1 GB for executors). Note that you cannot allocate how much memory each specific executor will use (you can control this from the driver configuration). To assign another value for the total memory (for example, 24 GB) to be used by all executors combined, execute the following setting:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ echo "export SPARK_WORKER_MEMORY=24g" &gt;&gt; /opt/infoobjects/spark/conf/spark-env.sh</strong></span>
</pre></div><p>There are some settings you can do at the driver level:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>To specify the maximum number of CPU cores to be used by a given application across the cluster, you can set the <code class="literal">spark.cores.max</code> configuration in Spark submit or Spark shell as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-submit --conf spark.cores.max=12</strong></span>
</pre></div></li><li style="list-style-type: disc"><p>To specify the amount of memory each executor should be allocated (the minimum recommendation is 8 GB), you can set the <code class="literal">spark.executor.memory</code> configuration in Spark submit or Spark shell as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-submit --conf spark.executor.memory=8g</strong></span>
</pre></div></li></ul></div><p>The <a id="id60" class="indexterm"></a>following diagram depicts the <a id="id61" class="indexterm"></a>high-level architecture of a Spark cluster:</p><div class="mediaobject"><img src="graphics/3056_01_08.jpg" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec22"></a>See also</h3></div></div></div><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><a class="ulink" href="http://spark.apache.org/docs/latest/spark-standalone.html" target="_blank">http://spark.apache.org/docs/latest/spark-standalone.html</a> to find more <a id="id62" class="indexterm"></a>configuration options</p></li></ul></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec14"></a>Deploying on a cluster with Mesos</h2></div></div><hr /></div><p>Mesos is <a id="id63" class="indexterm"></a>slowly emerging as a data center <a id="id64" class="indexterm"></a>operating system to manage all compute resources <a id="id65" class="indexterm"></a>across a data center. Mesos runs on any computer running the Linux operating system. Mesos is built using the same principles as Linux kernel. Let's see how we can install Mesos.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec23"></a>How to do it...</h3></div></div></div><p>Mesosphere provides a binary distribution of Mesos. The most recent package for the Mesos distribution can be installed from the Mesosphere repositories by performing the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Execute Mesos on Ubuntu OS with the trusty version:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo apt-key adv --keyserver keyserver.ubuntu.com --recv E56151BF DISTRO=$(lsb_release -is | tr '[:upper:]' '[:lower:]') CODENAME=$(lsb_release -cs)</strong></span>
<span class="strong"><strong>$ sudo vi /etc/apt/sources.list.d/mesosphere.list</strong></span>

<span class="strong"><strong>deb http://repos.mesosphere.io/Ubuntu trusty main</strong></span>
</pre></div></li><li><p>Update the repositories:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo apt-get -y update</strong></span>
</pre></div></li><li><p>Install Mesos:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo apt-get -y install mesos</strong></span>
</pre></div></li><li><p>To connect Spark to Mesos to integrate Spark with Mesos, make Spark binaries available to Mesos and configure the Spark driver to connect to Mesos.</p></li><li><p>Use Spark binaries from the first recipe and upload to HDFS:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ </strong></span>
<span class="strong"><strong>hdfs dfs</strong></span>
<span class="strong"><strong> -put spark-1.4.0-bin-hadoop2.4.tgz spark-1.4.0-bin-hadoop2.4.tgz</strong></span>
</pre></div></li><li><p>The master URL for single master Mesos is <code class="literal">mesos://host:5050</code>, and for the ZooKeeper managed Mesos cluster, it is <code class="literal">mesos://zk://host:2181</code>.</p></li><li><p>Set the following variables in <code class="literal">spark-env.sh</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo vi spark-env.sh</strong></span>
<span class="strong"><strong>export MESOS_NATIVE_LIBRARY=/usr/local/lib/libmesos.so</strong></span>
<span class="strong"><strong>export SPARK_EXECUTOR_URI= hdfs://localhost:9000/user/hduser/spark-1.4.0-bin-hadoop2.4.tgz</strong></span>
</pre></div></li><li><p>Run from the Scala program:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>val conf = new SparkConf().setMaster("mesos://host:5050")</strong></span>
<span class="strong"><strong>val sparkContext = new SparkContext(conf)</strong></span>
</pre></div></li><li><p>Run from <a id="id66" class="indexterm"></a>the Spark shell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell --master mesos://host:5050</strong></span>
</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note04"></a>Note</h3><p>Mesos<a id="id67" class="indexterm"></a> has two run modes:</p><p><span class="strong"><strong>Fine-grained</strong></span>: In<a id="id68" class="indexterm"></a> fine-grained (default) mode, every Spark task runs as a separate Mesos task</p><p><span class="strong"><strong>Coarse-grained</strong></span>: This <a id="id69" class="indexterm"></a>mode will launch only one long-running Spark task on each Mesos machine</p></div></li><li><p>To run in the coarse-grained mode, set the <code class="literal">spark.mesos.coarse</code> property:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>conf.set("spark.mesos.coarse","true")</strong></span>
</pre></div></li></ol></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec15"></a>Deploying on a cluster with YARN</h2></div></div><hr /></div><p><span class="strong"><strong>Yet another resource negotiator</strong></span> (<span class="strong"><strong>YARN</strong></span>) is <a id="id70" class="indexterm"></a>Hadoop's compute framework that runs on top of HDFS, which is Hadoop's storage layer.</p><p>YARN<a id="id71" class="indexterm"></a> follows the master slave architecture. The <a id="id72" class="indexterm"></a>master daemon is called <code class="literal">ResourceManager</code> and the slave daemon is called <code class="literal">NodeManager</code>. Besides this application, life cycle management is done by <code class="literal">ApplicationMaster</code>, which can be spawned on any slave node and is alive for the lifetime of an application.</p><p>When Spark is run on YARN, <code class="literal">ResourceManager</code> performs the role of Spark master and <code class="literal">NodeManagers</code> work as executor nodes.</p><p>While running Spark with YARN, each Spark executor is run as YARN container.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec24"></a>Getting ready</h3></div></div></div><p>Running Spark on YARN requires a binary distribution of Spark that has YARN support. In both Spark installation recipes, we have taken care of it.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec25"></a>How to do it...</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>To run Spark on YARN, the first step is to set the configuration:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>HADOOP_CONF_DIR: to write to HDFS</strong></span>
<span class="strong"><strong>YARN_CONF_DIR: to connect to YARN ResourceManager</strong></span>
<span class="strong"><strong>$ cd /opt/infoobjects/spark/conf (or /etc/spark)</strong></span>
<span class="strong"><strong>$ sudo vi spark-env.sh</strong></span>
<span class="strong"><strong>export HADOOP_CONF_DIR=/opt/infoobjects/hadoop/etc/Hadoop</strong></span>
<span class="strong"><strong>export YARN_CONF_DIR=/opt/infoobjects/hadoop/etc/hadoop</strong></span>
</pre></div><p>You can see this in the following screenshot:</p><div class="mediaobject"><img src="graphics/3056_01_09.jpg" /></div></li><li><p>The <a id="id73" class="indexterm"></a>following command launches <a id="id74" class="indexterm"></a>YARN Spark in the <code class="literal">yarn-client</code> mode:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-submit --class path.to.your.Class --master yarn-client [options] &lt;app jar&gt; [app options]</strong></span>
</pre></div><p>Here's an example:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-submit --class com.infoobjects.TwitterFireHose --master yarn-client --num-executors 3 --driver-memory 4g --executor-memory 2g --executor-cores 1 target/sparkio.jar 10</strong></span>
</pre></div></li><li><p>The following command launches Spark shell in the <code class="literal">yarn-client</code> mode:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell --master yarn-client</strong></span>
</pre></div></li><li><p>The command to launch in the <code class="literal">yarn-cluster</code> mode is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-submit --class path.to.your.Class --master yarn-cluster [options] &lt;app jar&gt; [app options]</strong></span>
</pre></div><p>Here's an example:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-submit --class com.infoobjects.TwitterFireHose --master yarn-cluster --num-executors 3 --driver-memory 4g --executor-memory 2g --executor-cores 1 targe</strong></span>
<span class="strong"><strong>t/sparkio.jar 10</strong></span>
</pre></div></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec26"></a>How it works…</h3></div></div></div><p>Spark <a id="id75" class="indexterm"></a>applications on YARN run in two modes:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><code class="literal">yarn-client</code>: Spark Driver runs in the client process outside of YARN cluster, and<a id="id76" class="indexterm"></a> <code class="literal">ApplicationMaster</code> is only used to negotiate resources from ResourceManager</p></li><li style="list-style-type: disc"><p><code class="literal">yarn-cluster</code>: Spark<a id="id77" class="indexterm"></a> Driver runs in <code class="literal">ApplicationMaster</code> spawned by <code class="literal">NodeManager</code> on a slave node</p></li></ul></div><p>The <code class="literal">yarn-cluster</code> mode is <a id="id78" class="indexterm"></a>recommended for production deployments, while the y<code class="literal">arn-client</code> mode is good for development and debugging when you would like to see immediate output. There is no need to specify Spark master in either mode as it's picked from the Hadoop configuration, and the master parameter is either <code class="literal">yarn-client</code> or <code class="literal">yarn-cluster</code>.</p><p>The following figure shows how Spark is run with YARN in the client mode:</p><div class="mediaobject"><img src="graphics/3056_01_10.jpg" /></div><p>The<a id="id79" class="indexterm"></a> following figure shows how Spark is run <a id="id80" class="indexterm"></a>with YARN in the cluster mode:</p><div class="mediaobject"><img src="graphics/3056_01_11.jpg" /></div><p>In the<a id="id81" class="indexterm"></a> YARN mode, the following configuration <a id="id82" class="indexterm"></a>parameters can be set:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><code class="literal">--num-executors</code>: Configure<a id="id83" class="indexterm"></a> how many executors will be allocated</p></li><li style="list-style-type: disc"><p><code class="literal">--executor-memory</code>: RAM per executor</p></li><li style="list-style-type: disc"><p><code class="literal">--executor-cores</code>: CPU cores per executor</p></li></ul></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec16"></a>Using Tachyon as an off-heap storage layer</h2></div></div><hr /></div><p>Spark RDDs<a id="id84" class="indexterm"></a> are a great way to store datasets in memory while ending up with multiple copies of the same data in different applications. Tachyon solves some of the <a id="id85" class="indexterm"></a>challenges with Spark RDD management. A few of them <a id="id86" class="indexterm"></a>are:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>RDD only exists for the duration of the Spark application</p></li><li style="list-style-type: disc"><p>The same process performs the compute and RDD in-memory storage; so, if a process crashes, in-memory storage also goes away</p></li><li style="list-style-type: disc"><p>Different jobs cannot share an RDD even if they are for the same underlying data, for <a id="id87" class="indexterm"></a>example, an HDFS block that leads to:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Slow writes to disk</p></li><li style="list-style-type: disc"><p>Duplication of data in memory, higher memory footprint</p></li></ul></div></li><li style="list-style-type: disc"><p>If the<a id="id88" class="indexterm"></a> output of one application needs to be shared with the other application, it's slow due to the replication in the disk</p></li></ul></div><p>Tachyon provides an off-heap memory layer to solve these problems. This layer, being off-heap, is immune to process crashes and is also not subject to garbage collection. This also lets RDDs be shared across applications and outlive a specific job or session; in essence, one single copy of data resides in memory, as shown in the following figure:</p><div class="mediaobject"><img src="graphics/3056_01_12.jpg" /></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec27"></a>How to do it...</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Let's download and compile Tachyon (Tachyon, by default, comes configured for Hadoop 1.0.4, so it needs to be compiled from sources for the right Hadoop version). Replace the version with the current version. The current version at the time of writing this book is 0.6.4:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ wget https://github.com/amplab/tachyon/archive/v&lt;version&gt;.zip</strong></span>
</pre></div></li><li><p>Unarchive the source code:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ unzip  v-&lt;version&gt;.zip</strong></span>
</pre></div></li><li><p>Remove the version from the <code class="literal">tachyon</code> source folder name for convenience:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ mv tachyon-&lt;version&gt; tachyon</strong></span>
</pre></div></li><li><p>Change <a id="id89" class="indexterm"></a>the directory to the <code class="literal">tachyon</code> folder:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cd tachyon</strong></span>
<span class="strong"><strong>$ mvn -Dhadoop.version=2.4.0 clean package -DskipTests=true</strong></span>
<span class="strong"><strong>$ cd conf</strong></span>
<span class="strong"><strong>$ sudo mkdir -p /var/tachyon/journal</strong></span>
<span class="strong"><strong>$ sudo chown -R hduser:hduser /var/tachyon/journal</strong></span>
<span class="strong"><strong>$ sudo mkdir -p /var/tachyon/ramdisk</strong></span>
<span class="strong"><strong>$ sudo chown -R hduser:hduser /var/tachyon/ramdisk</strong></span>

<span class="strong"><strong>$ mv tachyon-env.sh.template tachyon-env.sh</strong></span>
<span class="strong"><strong>$ vi tachyon-env.sh</strong></span>
</pre></div></li><li><p>Comment the following line:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>export TACHYON_UNDERFS_ADDRESS=$TACHYON_HOME/underfs</strong></span>
</pre></div></li><li><p>Uncomment the following line:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>export TACHYON_UNDERFS_ADDRESS=hdfs://localhost:9000</strong></span>
</pre></div></li><li><p>Change the following properties:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>-Dtachyon.master.journal.folder=/var/tachyon/journal/</strong></span>

<span class="strong"><strong>export TACHYON_RAM_FOLDER=/var/tachyon/ramdisk</strong></span>

<span class="strong"><strong>$ sudo mkdir -p /var/log/tachyon</strong></span>
<span class="strong"><strong>$ sudo chown -R hduser:hduser /var/log/tachyon</strong></span>
<span class="strong"><strong>$ vi log4j.properties</strong></span>
</pre></div></li><li><p>Replace <code class="literal">${tachyon.home}</code> with <code class="literal">/var/log/tachyon</code>.</p></li><li><p>Create a new <code class="literal">core-site.xml</code> file in the <code class="literal">conf</code> directory:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo vi core-site.xml</strong></span>
<span class="strong"><strong>&lt;configuration&gt;</strong></span>
<span class="strong"><strong>&lt;property&gt;</strong></span>
<span class="strong"><strong>    &lt;name&gt;fs.tachyon.impl&lt;/name&gt;</strong></span>
<span class="strong"><strong>    &lt;value&gt;tachyon.hadoop.TFS&lt;/value&gt;</strong></span>
<span class="strong"><strong>  &lt;/property&gt;</strong></span>
<span class="strong"><strong>&lt;/configuration&gt;</strong></span>
<span class="strong"><strong>$ cd ~</strong></span>
<span class="strong"><strong>$ sudo mv tachyon /opt/infoobjects/</strong></span>
<span class="strong"><strong>$ sudo chown -R root:root /opt/infoobjects/tachyon</strong></span>
<span class="strong"><strong>$ sudo chmod -R 755 /opt/infoobjects/tachyon</strong></span>
</pre></div></li><li><p>Add <code class="literal">&lt;tachyon home&gt;/bin</code> to the path:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ echo "export PATH=$PATH:/opt/infoobjects/tachyon/bin" &gt;&gt; /home/hduser/.bashrc</strong></span>
</pre></div></li><li><p>Restart <a id="id90" class="indexterm"></a>the shell and format Tachyon:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ tachyon format</strong></span>
<span class="strong"><strong>$ tachyon-start.sh local //you need to enter root password as RamFS needs to be formatted</strong></span>
</pre></div><p>Tachyon's web interface is <code class="literal">http://hostname:19999</code>:</p><div class="mediaobject"><img src="graphics/3056_01_13.jpg" /></div></li><li><p>Run the sample program to see whether Tachyon is running fine:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ tachyon runTest Basic CACHE_THROUGH</strong></span>
</pre></div><div class="mediaobject"><img src="graphics/3056_01_14.jpg" /></div></li><li><p>You can <a id="id91" class="indexterm"></a>stop Tachyon any time by running the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ tachyon-stop.sh</strong></span>
</pre></div></li><li><p>Run Spark on Tachyon:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell</strong></span>
<span class="strong"><strong>scala&gt; val words = sc.textFile("tachyon://localhost:19998/words")</strong></span>
<span class="strong"><strong>scala&gt; words.count</strong></span>
<span class="strong"><strong>scala&gt; words.saveAsTextFile("tachyon://localhost:19998/w2")</strong></span>
<span class="strong"><strong>scala&gt; val person = sc.textFile("hdfs://localhost:9000/user/hduser/person")</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.api.java._</strong></span>
<span class="strong"><strong>scala&gt; person.persist(StorageLevels.OFF_HEAP)</strong></span>
</pre></div></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec28"></a>See also</h3></div></div></div><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><a class="ulink" href="http://www.cs.berkeley.edu/~haoyuan/papers/2013_ladis_tachyon.pdf" target="_blank">http://www.cs.berkeley.edu/~haoyuan/papers/2013_ladis_tachyon.pdf</a> to learn<a id="id92" class="indexterm"></a> about the origins of Tachyon</p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://www.tachyonnexus.com" target="_blank">http://www.tachyonnexus.com</a></p></li></ul></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch02"></a>Chapter 2. Developing Applications with Spark</h2></div></div></div><p>In this chapter, we will cover:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Exploring the Spark shell</p></li><li style="list-style-type: disc"><p>Developing a Spark application in Eclipse with Maven</p></li><li style="list-style-type: disc"><p>Developing Spark applications in Eclipse with SBT</p></li><li style="list-style-type: disc"><p>Developing a Spark application in Intellij IDEA with Maven</p></li><li style="list-style-type: disc"><p>Developing a Spark application in Intellij IDEA with SBT</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec17"></a>Introduction</h2></div></div><hr /></div><p>To create production quality Spark jobs/application, it is useful to use various <span class="strong"><strong>integrated development environments</strong></span> (<span class="strong"><strong>IDEs</strong></span>) and build tools. This chapter will cover various IDEs and build tools.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec18"></a>Exploring the Spark shell</h2></div></div><hr /></div><p>Spark comes bundled with a REPL shell, which is a wrapper around the Scala shell. Though the Spark<a id="id93" class="indexterm"></a> shell looks like a command line for simple things, in reality a lot of complex queries can also be executed using it. This chapter explores different development environments in which Spark applications can be developed.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec29"></a>How to do it...</h3></div></div></div><p>Hadoop MapReduce's word count becomes very simple with the Spark shell. In this recipe, we <a id="id94" class="indexterm"></a>are going to create a simple 1-line text file, upload it to the <span class="strong"><strong>Hadoop distributed file system</strong></span> (<span class="strong"><strong>HDFS</strong></span>), and use Spark to count occurrences of words. Let's see how:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Create the <code class="literal">words</code> directory by using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ mkdir words</strong></span>
</pre></div></li><li><p>Get into the <code class="literal">words</code> directory:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cd words</strong></span>
</pre></div></li><li><p>Create a <code class="literal">sh.txt</code> text file and enter <code class="literal">"to be or not to be"</code> in it:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ echo "to be or not to be" &gt; sh.txt</strong></span>
</pre></div></li><li><p>Start the Spark shell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell</strong></span>
</pre></div></li><li><p>Load the <code class="literal">words</code> directory as RDD:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Scala&gt; val words = sc.textFile("hdfs://localhost:9000/user/hduser/words")</strong></span>
</pre></div></li><li><p>Count the number of lines ( result: 1):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Scala&gt; words.count</strong></span>
</pre></div></li><li><p>Divide the line (or lines) into multiple words:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Scala&gt; val wordsFlatMap = words.flatMap(_.split("\\W+"))</strong></span>
</pre></div></li><li><p>Convert <code class="literal">word</code> to (word,1)—that is, output <code class="literal">1</code> as the value for each occurrence of <code class="literal">word</code> as <a id="id95" class="indexterm"></a>a key:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Scala&gt; val wordsMap = wordsFlatMap.map( w =&gt; (w,1))</strong></span>
</pre></div></li><li><p>Use the <code class="literal">reduceByKey</code> method to add the number of occurrences for each word as a key (the function works on two consecutive values at a time represented by <code class="literal">a</code> and <code class="literal">b</code>):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Scala&gt; val wordCount = wordsMap.reduceByKey( (a,b) =&gt; (a+b))</strong></span>
</pre></div></li><li><p>Sort the results:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Scala&gt; val wordCountSorted = wordCount.sortByKey(true)</strong></span>
</pre></div></li><li><p>Print the RDD:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Scala&gt; wordCountSorted.collect.foreach(println)</strong></span>
</pre></div></li><li><p>Doing all of the preceding operations in one step is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Scala&gt; sc.textFile("hdfs://localhost:9000/user/hduser/words"). flatMap(_.split("\\W+")).map( w =&gt; (w,1)). reduceByKey( (a,b) =&gt; (a+b)).sortByKey(true).collect.foreach(println)</strong></span>
</pre></div></li></ol></div><p>This gives us the following output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>(or,1)</strong></span>
<span class="strong"><strong>(to,2)</strong></span>
<span class="strong"><strong>(not,1)</strong></span>
<span class="strong"><strong>(be,2)</strong></span>
</pre></div><p>Now you understand the basics, load HDFS with a large amount of text—for example, stories—and see the magic.</p><p>If you have the files in a compressed format, you can load them as is in HDFS. Both Hadoop and Spark have codecs for unzipping, which they use based on file extensions.</p><p>When <code class="literal">wordsFlatMap</code> was converted to <code class="literal">wordsMap</code> RDD, there was an implicit conversion. This converts RDD into <code class="literal">PairRDD</code>. This is an implicit conversion, which does not require<a id="id96" class="indexterm"></a> anything to be done. If you are doing it in Scala code, please add the following <code class="literal">import</code> statement:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark.SparkContext._</pre></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec19"></a>Developing Spark applications in Eclipse with Maven</h2></div></div><hr /></div><p>Maven as <a id="id97" class="indexterm"></a>a build tool has become<a id="id98" class="indexterm"></a> the de-facto standard over the years. It's not surprising if we look little deeper into the promise Maven brings. Maven <a id="id99" class="indexterm"></a>has two primary features and they<a id="id100" class="indexterm"></a> are:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><span class="strong"><strong>Convention over configuration</strong></span>: Build tools prior to Maven gave developers freedom about where<a id="id101" class="indexterm"></a> to put source files, where to put test files, where to put compiled files, and so on. Maven takes away that freedom. With this freedom, all the confusion about locations also goes. In Maven, there is a specific directory structure for everything. The following table shows a few of the most common locations:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /></colgroup><tbody><tr><td style="" align="left" valign="top">
<p><code class="literal">/src/main/scala</code></p>
</td><td style="" align="left" valign="top">
<p>Source code in Scala</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">/src/main/java</code></p>
</td><td style="" align="left" valign="top">
<p>Source code in Java</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">/src/main/resources</code></p>
</td><td style="" align="left" valign="top">
<p>Resources to be used by source code such as configuration files</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">/src/test/scala</code></p>
</td><td style="" align="left" valign="top">
<p>Test code in Scala</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">/src/test/java</code></p>
</td><td style="" align="left" valign="top">
<p>Test code in Java</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">/src/test/resources</code></p>
</td><td style="" align="left" valign="top">
<p>Resources to be used by test code such as configuration files</p>
</td></tr></tbody></table></div></li><li style="list-style-type: disc"><p><span class="strong"><strong>Declarative dependency management</strong></span>: In Maven, every library is defined by following three coordinates:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /></colgroup><tbody><tr><td style="" align="left" valign="top">
<p><code class="literal">groupId</code></p>
</td><td style="" align="left" valign="top">
<p>A logical way of grouping libraries similar to a package in Java/Scala, which has to be at least the domain name you own—for example, <code class="literal">org.apache.spark</code></p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">artifactId</code></p>
</td><td style="" align="left" valign="top">
<p>The name of the project and JAR</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">version</code></p>
</td><td style="" align="left" valign="top">
<p>Standard version numbers</p>
</td></tr></tbody></table></div></li></ul></div><p>In <code class="literal">pom.xml</code> (the configuration file that tells Maven all the information about a project), dependencies are declared in the form of these three coordinates. There is no need to search over the Internet and download, unpack, and copy libraries. All you need to do is to provide three coordinates of the dependency JAR you need and Maven will do the rest for you. The following is an example of using a JUnit dependency:</p><div class="informalexample"><pre class="programlisting">&lt;dependency&gt;
  &lt;groupId&gt;junit&lt;/groupId&gt;
  &lt;artifactId&gt;junit&lt;/artifactId&gt;
  &lt;version&gt;4.12&lt;/version&gt;
&lt;/dependency&gt;</pre></div><p>This<a id="id102" class="indexterm"></a> makes dependency management<a id="id103" class="indexterm"></a> including transitive<a id="id104" class="indexterm"></a> dependencies very easy. Build tools that came after Maven such as SBT and Gradle also follow these two rules as-is and provide enhancements in other aspects.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec30"></a>Getting ready</h3></div></div></div><p>From this recipe<a id="id105" class="indexterm"></a> onwards, this chapter assumes you have installed Eclipse. Please visit <a class="ulink" href="http://www.eclipse.org" target="_blank">http://www.eclipse.org</a> for details.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec31"></a>How to do it...</h3></div></div></div><p>Let's see how to install the Maven plugin for Eclipse:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Open Eclipse and navigate to <span class="strong"><strong>Help</strong></span> | <span class="strong"><strong>Install New Software</strong></span>.</p></li><li><p>Click on the <span class="strong"><strong>Work with</strong></span> drop-down menu.</p></li><li><p>Select the &lt;eclipse version&gt; update site.</p></li><li><p>Click on <span class="strong"><strong>Collaboration tools</strong></span>.</p></li><li><p>Check Maven's integration with Eclipse, as in the following screenshot:</p><div class="mediaobject"><img src="graphics/B03056_02_01.jpg" /></div></li><li><p>Click<a id="id106" class="indexterm"></a> on <span class="strong"><strong>Next</strong></span> and then <a id="id107" class="indexterm"></a>click on <span class="strong"><strong>Finish</strong></span>.</p><p>There will be a prompt to restart Eclipse and Maven will be installed after the restart.</p></li></ol></div><p>Now let's <a id="id108" class="indexterm"></a>see how we can install the Scala plugin for Eclipse:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Open Eclipse and navigate to <span class="strong"><strong>Help</strong></span> | <span class="strong"><strong>Install New Software</strong></span>.</p></li><li><p>Click on the <span class="strong"><strong>Work with</strong></span> drop-down menu.</p></li><li><p>Select the &lt;eclipse version&gt; update site.</p></li><li><p>Type <code class="literal">http://download.scala-ide.org/sdk/helium/e38/scala210/stable/site</code>.</p></li><li><p>Press<a id="id109" class="indexterm"></a> <span class="emphasis"><em>Enter</em></span>.</p></li><li><p>Select <a id="id110" class="indexterm"></a>
<span class="strong"><strong>Scala IDE for Eclipse</strong></span>:</p><div class="mediaobject"><img src="graphics/B03056_02_02.jpg" /></div></li><li><p>Click<a id="id111" class="indexterm"></a> on <span class="strong"><strong>Next</strong></span> and then click on <span class="strong"><strong>Finish</strong></span>. You will be prompted to restart Eclipse and Scala will be installed after the restart.</p></li><li><p>Navigate<a id="id112" class="indexterm"></a> to <span class="strong"><strong>Window</strong></span> | <span class="strong"><strong>Open Perspective</strong></span> | <span class="strong"><strong>Scala</strong></span>.</p></li></ol></div><p>Eclipse<a id="id113" class="indexterm"></a> is <a id="id114" class="indexterm"></a>now ready for Scala development!</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec20"></a>Developing Spark applications in Eclipse with SBT</h2></div></div><hr /></div><p><span class="strong"><strong>Simple Build Tool</strong></span> (<span class="strong"><strong>SBT</strong></span>) is <a id="id115" class="indexterm"></a>a build tool made especially for Scala-based development. SBT follows Maven-based naming <a id="id116" class="indexterm"></a>conventions and declarative <a id="id117" class="indexterm"></a>dependency management.</p><p>SBT <a id="id118" class="indexterm"></a>provides the following enhancements over Maven:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Dependencies are in the form of key-value pairs in the <code class="literal">build.sbt</code> file as opposed to <code class="literal">pom.xml</code> in Maven</p></li><li style="list-style-type: disc"><p>It provides a shell that makes it very handy to perform build operations</p></li><li style="list-style-type: disc"><p>For simple projects without dependencies, you do not even need the <code class="literal">build.sbt</code> file</p></li></ul></div><p>In <code class="literal">build.sbt</code>, the first line is the project definition:</p><div class="informalexample"><pre class="programlisting">lazy val root = (project in file("."))</pre></div><p>Each project has an immutable map of key-value pairs. This map is changed by settings in SBT like so:</p><div class="informalexample"><pre class="programlisting">lazy val root = (project in file("."))
  settings(
    name := "wordcount"
  )</pre></div><p>Every change in the settings leads to a new map, as it's an immutable map.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec32"></a>How to do it...</h3></div></div></div><p>Here's how we go about adding the <code class="literal">sbteclipse</code> plugin:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Add this to the global plugin file:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ mkdir /home/hduser/.sbt/0.13/plugins</strong></span>
<span class="strong"><strong>$ echo addSbtPlugin("com.typesafe.sbteclipse" % "sbteclipse-plugin" % "2.5.0" )  &gt; /home/hduser/.sbt/0.12/plugins/plugin.sbt</strong></span>
</pre></div><p>Alternatively, you can add the following to your project:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cd &lt;project-home&gt;</strong></span>
<span class="strong"><strong>$ echo addSbtPlugin("com.typesafe.sbteclipse" % "sbteclipse-plugin" % "2.5.0" )  &gt; plugin.sbt</strong></span>
</pre></div></li><li><p>Start <a id="id119" class="indexterm"></a>the <code class="literal">sbt</code> shell <a id="id120" class="indexterm"></a>without any arguments:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$sbt</strong></span>
</pre></div></li><li><p>Type <code class="literal">eclipse</code> and <a id="id121" class="indexterm"></a>it will make an Eclipse-ready project:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ eclipse</strong></span>
</pre></div></li><li><p>Now you can navigate to <span class="strong"><strong>File</strong></span> | <span class="strong"><strong>Import</strong></span> | <span class="strong"><strong>Import existing project into workspace</strong></span> to load the project into Eclipse.</p></li></ol></div><p>Now you can develop the Spark application in Scala using Eclipse and SBT.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec21"></a>Developing a Spark application in IntelliJ IDEA with Maven</h2></div></div><hr /></div><p>IntelliJ IDEA <a id="id122" class="indexterm"></a>comes bundled<a id="id123" class="indexterm"></a> with support for Maven. We <a id="id124" class="indexterm"></a>will see how to create a new Maven project in this recipe.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec33"></a>How to do it...</h3></div></div></div><p>Perform the following steps to develop a Spark application on IntelliJ IDEA with Maven:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Select <span class="strong"><strong>Maven</strong></span> in new project window and click on <span class="strong"><strong>Next</strong></span>:</p><div class="mediaobject"><img src="graphics/B03056_02_03.jpg" /></div></li><li><p>Enter <a id="id125" class="indexterm"></a>three dimensions<a id="id126" class="indexterm"></a> of<a id="id127" class="indexterm"></a> the project:</p><div class="mediaobject"><img src="graphics/B03056_02_04.jpg" /></div></li><li><p>Enter<a id="id128" class="indexterm"></a> the project's <a id="id129" class="indexterm"></a>name <a id="id130" class="indexterm"></a>and location:</p><div class="mediaobject"><img src="graphics/B03056_02_05.jpg" /></div></li><li><p>Click<a id="id131" class="indexterm"></a> on <span class="strong"><strong>Finish</strong></span> and<a id="id132" class="indexterm"></a> the Maven <a id="id133" class="indexterm"></a>project is ready.</p></li></ol></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec22"></a>Developing a Spark application in IntelliJ IDEA with SBT</h2></div></div><hr /></div><p>Before Eclipse<a id="id134" class="indexterm"></a> became famous, IntelliJ IDEA <a id="id135" class="indexterm"></a>was considered<a id="id136" class="indexterm"></a> best of the breed in IDEs. IDEA has not shed its former glory yet and a lot of developers love IDEA. IDEA also has a community edition, which is free. IDEA provides native support for SBT, which makes it ideal for SBT and Scala development.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec34"></a>How to do it...</h3></div></div></div><p>Perform<a id="id137" class="indexterm"></a> the following steps to develop a <a id="id138" class="indexterm"></a>Spark application on IntelliJ IDEA <a id="id139" class="indexterm"></a>with SBT:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Add the <code class="literal">sbt-idea</code> plugin.</p></li><li><p>Add to the global plugin file:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$mkdir /home/hduser/.sbt/0.13/plugins</strong></span>
<span class="strong"><strong>$echo addSbtPlugin("com.github.mpeltone" % "sbt-idea" % "1.6.0" )  &gt; /home/hduser/.sbt/0.12/plugins/plugin.sbt</strong></span>
</pre></div><p>Alternatively, you can add to your project as well:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$cd &lt;project-home&gt;</strong></span>
<span class="strong"><strong>$ echo addSbtPlugin("com.github.mpeltone" % "sbt-idea" % "1.6.0" ) &gt; plugin.sbt</strong></span>
</pre></div></li></ol></div><p>IDEA is ready to use with SBT.</p><p>Now you can develop Spark code using Scala and build using SBT.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch03"></a>Chapter 3. External Data Sources</h2></div></div></div><p>One of the strengths of Spark is that it provides a single runtime that can connect with various underlying data sources.</p><p>In this chapter, we will connect to different data sources. This chapter is divided into the following recipes:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Loading data from the local filesystem</p></li><li style="list-style-type: disc"><p>Loading data from HDFS</p></li><li style="list-style-type: disc"><p>Loading data from HDFS using a custom InputFormat</p></li><li style="list-style-type: disc"><p>Loading data from Amazon S3</p></li><li style="list-style-type: disc"><p>Loading data from Apache Cassandra</p></li><li style="list-style-type: disc"><p>Loading data from relational databases</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec23"></a>Introduction</h2></div></div><hr /></div><p>Spark provides a unified runtime for big data. HDFS, which is Hadoop's filesystem, is the most used storage <a id="id140" class="indexterm"></a>platform for Spark as it provides cost-effective storage for unstructured and semi-structured data on commodity hardware. Spark is not limited to HDFS and can work with any Hadoop-supported storage.</p><p>Hadoop supported storage means a storage format that can work with Hadoop's <code class="literal">InputFormat</code> and <code class="literal">OutputFormat</code> interfaces. <code class="literal">InputFormat</code> is responsible for creating <code class="literal">InputSplits</code> from <a id="id141" class="indexterm"></a>input data and dividing it further into <a id="id142" class="indexterm"></a>records. <code class="literal">OutputFormat</code> is responsible for writing to storage.</p><p>We will start with writing to the local filesystem and then move over to loading data from HDFS. In the <span class="emphasis"><em>Loading data from HDFS</em></span> recipe, we will cover the most common file format: regular text files. In the next recipe, we will cover how to use any <code class="literal">InputFormat</code> interface to load data in Spark. We will also explore loading data stored in Amazon S3, a leading cloud storage platform.</p><p>We will explore loading data from Apache Cassandra, which is a NoSQL database. Finally, we will explore loading data from a relational database.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec24"></a>Loading data from the local filesystem</h2></div></div><hr /></div><p>Though the<a id="id143" class="indexterm"></a> local filesystem is not a good fit to store big <a id="id144" class="indexterm"></a>data due to disk size limitations and lack of distributed nature, technically you can load data in distributed systems using the local filesystem. But then the file/directory you are accessing has to be available on each node.</p><p>Please note that if you are planning to use this feature to load side data, it is not a good idea. To load side data, Spark has a broadcast variable feature, which will be discussed in upcoming chapters.</p><p>In this recipe, we will look at how to load data in Spark from the local filesystem.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec35"></a>How to do it...</h3></div></div></div><p>Let's start with the example of Shakespeare's "to be or not to be":</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Create the <code class="literal">words</code> directory by using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ mkdir words</strong></span>
</pre></div></li><li><p>Get into the <code class="literal">words</code> directory:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cd words</strong></span>
</pre></div></li><li><p>Create the <code class="literal">sh.txt</code> text file and enter <code class="literal">"to be or not to be"</code> in it:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ echo "to be or not to be" &gt; sh.txt</strong></span>
</pre></div></li><li><p>Start the Spark shell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell</strong></span>
</pre></div></li><li><p>Load the <code class="literal">words</code> directory as RDD:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val words = sc.textFile("file:///home/hduser/words")</strong></span>
</pre></div></li><li><p>Count the number of lines:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; words.count</strong></span>
</pre></div></li><li><p>Divide the line (or lines) into multiple words:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val wordsFlatMap = words.flatMap(_.split("\\W+"))</strong></span>
</pre></div></li><li><p>Convert <code class="literal">word</code> to (word,1)—that is, output <code class="literal">1</code> as the value for each occurrence of <code class="literal">word</code> as a key:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val wordsMap = wordsFlatMap.map( w =&gt; (w,1))</strong></span>
</pre></div></li><li><p>Use the <code class="literal">reduceByKey</code> method to add the number of occurrences for each word as a key (this function works on two consecutive values at a time, represented by <code class="literal">a</code> and <code class="literal">b</code>):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val wordCount = wordsMap.reduceByKey( (a,b) =&gt; (a+b))</strong></span>
</pre></div></li><li><p>Print the RDD:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; wordCount.collect.foreach(println)</strong></span>
</pre></div></li><li><p>Doing <a id="id145" class="indexterm"></a>all of the preceding operations<a id="id146" class="indexterm"></a> in one step is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; sc.textFile("file:///home/hduser/ words"). flatMap(_.split("\\W+")).map( w =&gt; (w,1)). reduceByKey( (a,b) =&gt; (a+b)).foreach(println)</strong></span>
</pre></div></li></ol></div><p>This gives the following output:</p><div class="mediaobject"><img src="graphics/B03056_03_01.jpg" /></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec25"></a>Loading data from HDFS</h2></div></div><hr /></div><p>HDFS is the most widely used big data storage system. One of the reasons for the wide adoption of <a id="id147" class="indexterm"></a>HDFS is schema-on-read. What this means is that HDFS does <a id="id148" class="indexterm"></a>not put any restriction on data when data is being written. Any and all kinds of data are welcome and can be stored in a raw format. This feature makes it ideal storage for raw unstructured data and semi-structured data.</p><p>When it comes to reading data, even unstructured data needs to be given some structure to make sense. Hadoop uses <code class="literal">InputFormat</code> to determine how to read the data. Spark provides complete support for Hadoop's <code class="literal">InputFormat</code> so anything that can be read by Hadoop can be read by Spark as well.</p><p>The default <code class="literal">InputFormat</code> is <code class="literal">TextInputFormat</code>. <code class="literal">TextInputFormat</code> takes the byte offset of a line as a key and the content of a line as a value. Spark uses the <code class="literal">sc.textFile</code> method to read using <code class="literal">TextInputFormat</code>. It ignores the byte offset and creates an RDD of strings.</p><p>Sometimes the filename itself contains useful information, for example, time-series data. In that case, you may want to read each file separately. The <code class="literal">sc.wholeTextFiles</code> method allows you to do that. It creates an RDD with the filename and path (for example, <code class="literal">hdfs://localhost:9000/user/hduser/words</code>) as a key and the content of the whole file as the value.</p><p>Spark also supports reading various serialization and compression-friendly formats such as Avro, Parquet, and JSON using DataFrames. These formats will be covered in coming chapters.</p><p>In this recipe, we will look at how to load data in the Spark shell from HDFS.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec36"></a>How to do it...</h3></div></div></div><p>Let's do the <a id="id149" class="indexterm"></a>word count, which counts the number of occurrences of<a id="id150" class="indexterm"></a> each word. In this recipe, we will load data from HDFS:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Create the <code class="literal">words</code> directory by using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ mkdir words</strong></span>
</pre></div></li><li><p>Change the directory to <code class="literal">words</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cd words</strong></span>
</pre></div></li><li><p>Create the <code class="literal">sh.txt text</code> file and enter <code class="literal">"to be or not to be"</code> in it:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ echo "to be or not to be" &gt; sh.txt</strong></span>
</pre></div></li><li><p>Start the Spark shell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell</strong></span>
</pre></div></li><li><p>Load the <code class="literal">words</code> directory as the RDD:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val words = sc.textFile("hdfs://localhost:9000/user/hduser/words")</strong></span>
</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note05"></a>Note</h3><p>The <code class="literal">sc.textFile</code> method also supports passing an additional argument for the number of partitions. By default, Spark creates one partition for each <code class="literal">InputSplit</code> class, which roughly corresponds to one block.</p><p>You can ask for a higher number of partitions. It works really well for compute-intensive jobs such as in machine learning. As one partition cannot contain more than one block, having fewer partitions than blocks is not allowed.</p></div></li><li><p>Count the number of lines (the result will be <code class="literal">1</code>):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; words.count</strong></span>
</pre></div></li><li><p>Divide the line (or lines) into multiple words:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val wordsFlatMap = words.flatMap(_.split("\\W+"))</strong></span>
</pre></div></li><li><p>Convert word to (word,1)—that is, output <code class="literal">1</code> as a value for each occurrence of <code class="literal">word</code> as a key:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val wordsMap = wordsFlatMap.map( w =&gt; (w,1))</strong></span>
</pre></div></li><li><p>Use the <code class="literal">reduceByKey</code> method to add the number of occurrences of each word as a key (this function works on two consecutive values at a time, represented by <code class="literal">a</code> and <code class="literal">b</code>):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val wordCount = wordsMap.reduceByKey( (a,b) =&gt; (a+b))</strong></span>
</pre></div></li><li><p>Print the RDD:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; wordCount.collect.foreach(println)</strong></span>
</pre></div></li><li><p>Doing all of the preceding operations in one step is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; sc.textFile("hdfs://localhost:9000/user/hduser/words"). flatMap(_.split("\\W+")).map( w =&gt; (w,1)). reduceByKey( (a,b) =&gt; (a+b)).foreach(println)</strong></span>
</pre></div></li></ol></div><p>This<a id="id151" class="indexterm"></a> gives<a id="id152" class="indexterm"></a> the following output:</p><div class="mediaobject"><img src="graphics/B03056_03_01.jpg" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec37"></a>There's more…</h3></div></div></div><p>Sometimes we need to access the whole file at once. Sometimes the filename contains useful data like in the case of time-series. Sometimes you need to process more than one line as a record. <code class="literal">sparkContext.wholeTextFiles</code> comes to the rescue here. We will look at weather dataset from <a class="ulink" href="ftp://ftp.ncdc.noaa.gov/pub/data/noaa/" target="_blank">ftp://ftp.ncdc.noaa.gov/pub/data/noaa/</a>.</p><p>Here's what a top-level directory looks like:</p><div class="mediaobject"><img src="graphics/B03056_03_02.jpg" /></div><p>Looking into a particular year directory—for example, 1901 resembles the following screenshot:</p><div class="mediaobject"><img src="graphics/B03056_03_03.jpg" /></div><p>Data here is <a id="id153" class="indexterm"></a>divided in such a way that each filename contains useful <a id="id154" class="indexterm"></a>information, that is, USAF-WBAN-year, where USAF is the US air force station number and WBAN is the weather bureau army navy location number.</p><p>You will also notice that all files are compressed as gzip with a <code class="literal">.gz</code> extension. Compression is handled automatically so all you need to do is to upload data in HDFS. We will come back to this dataset in the coming chapters.</p><p>Since the whole dataset is not large, it can be uploaded in HDFS in the pseudo-distributed mode also:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Download data:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ wget -r ftp://ftp.ncdc.noaa.gov/pub/data/noaa/</strong></span>
</pre></div></li><li><p>Load the weather data in HDFS:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hdfs dfs -put ftp.ncdc.noaa.gov/pub/data/noaa weather/</strong></span>
</pre></div></li><li><p>Start the Spark shell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell</strong></span>
</pre></div></li><li><p>Load weather data for 1901 in the RDD:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val weatherFileRDD = sc.wholeTextFiles("hdfs://localhost:9000/user/hduser/weather/1901")</strong></span>
</pre></div></li><li><p>Cache weather in the RDD so that it is not recomputed every time it's accessed:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val weatherRDD = weatherFileRDD.cache</strong></span>
</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note06"></a>Note</h3><p>In Spark, there are various StorageLevels at which the RDD can be persisted. <code class="literal">rdd.cache</code> is a shorthand for the <code class="literal">rdd.persist(MEMORY_ONLY)</code> StorageLevel.</p></div></li><li><p>Count the<a id="id155" class="indexterm"></a> number of elements:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; weatherRDD.count</strong></span>
</pre></div></li><li><p>Since the <a id="id156" class="indexterm"></a>whole contents of a file are loaded as an element, we need to manually interpret the data, so let's load the first element:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val firstElement = weatherRDD.first</strong></span>
</pre></div></li><li><p>Read the value of the first RDD:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val firstValue = firstElement._2</strong></span>
</pre></div><p>The <code class="literal">firstElement</code> contains tuples in the form (string, string). Tuples can be accessed in two ways:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Using a positional function starting with <code class="literal">_1</code>.</p></li><li style="list-style-type: disc"><p>Using the <code class="literal">productElement</code> method, for example, <code class="literal">tuple.productElement(0)</code>. Indexes here start with <code class="literal">0</code> like most other methods.</p></li></ul></div></li><li><p>Split <code class="literal">firstValue</code> by lines:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val firstVals = firstValue.split("\\n")</strong></span>
</pre></div></li><li><p>Count the number of elements in <code class="literal">firstVals</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; firstVals.size</strong></span>
</pre></div></li><li><p>The schema of weather data is very rich with the position of the text working as a delimiter. You can get more information about schemas at the national weather service website. Let's get wind speed, which is from section 66-69 (in meter/sec):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val windSpeed = firstVals.map(line =&gt; line.substring(65,69)</strong></span>
</pre></div></li></ol></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec26"></a>Loading data from HDFS using a custom InputFormat</h2></div></div><hr /></div><p>Sometimes<a id="id157" class="indexterm"></a> you need to load data in a <a id="id158" class="indexterm"></a>specific format and <code class="literal">TextInputFormat</code> is <a id="id159" class="indexterm"></a>not a good fit for that. Spark provides two methods for this purpose:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><code class="literal">sparkContext.hadoopFile</code>: This supports the old MapReduce API</p></li><li style="list-style-type: disc"><p><code class="literal">sparkContext.newAPIHadoopFile</code>: This supports the new MapReduce API</p></li></ul></div><p>These two<a id="id160" class="indexterm"></a> methods provide support for <a id="id161" class="indexterm"></a>all of Hadoop's built-in InputFormats <a id="id162" class="indexterm"></a>interfaces as well as any custom <code class="literal">InputFormat</code>.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec38"></a>How to do it...</h3></div></div></div><p>We are going to load text data in key-value format and load it in Spark using <code class="literal">KeyValueTextInputFormat</code>:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Create the <code class="literal">currency</code> directory by using the following command:</p><div class="informalexample"><pre class="programlisting">$ mkdir currency</pre></div></li><li><p>Change the current directory to <code class="literal">currency</code>:</p><div class="informalexample"><pre class="programlisting">$ cd currency</pre></div></li><li><p>Create the <code class="literal">na.txt</code> text file and enter currency values in key-value format delimited by tab (key: country, value: currency):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ vi na.txt</strong></span>
<span class="strong"><strong>United States of America        US Dollar</strong></span>
<span class="strong"><strong>Canada  Canadian Dollar</strong></span>
<span class="strong"><strong>Mexico  Peso</strong></span>
</pre></div><p>You can create more files for each continent.</p></li><li><p>Upload the <code class="literal">currency</code> folder to HDFS:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hdfs dfs -put currency /user/hduser/currency</strong></span>
</pre></div></li><li><p>Start the Spark shell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell</strong></span>
</pre></div></li><li><p>Import statements:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.hadoop.io.Text</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat</strong></span>
</pre></div></li><li><p>Load the <code class="literal">currency</code> directory as the RDD:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>val currencyFile = sc.newAPIHadoopFile("hdfs://localhost:9000/user/hduser/currency",classOf[KeyValueTextInputFormat],classOf[Text],classOf[Text])</strong></span>
</pre></div></li><li><p>Convert it from tuple of (Text,Text) to tuple of (String,String):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>val currencyRDD = currencyFile.map( t =&gt; (t._1.toString,t._2.toString))</strong></span>
</pre></div></li><li><p>Count the number of elements in the RDD:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; currencyRDD.count</strong></span>
</pre></div></li><li><p>Print the values:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; currencyRDD.collect.foreach(println)</strong></span>
</pre></div><div class="mediaobject"><img src="graphics/B03056_03_04.jpg" /></div></li></ol></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note07"></a>Note</h3><p>You<a id="id163" class="indexterm"></a> can use this approach <a id="id164" class="indexterm"></a>to load data<a id="id165" class="indexterm"></a> in any Hadoop-supported <code class="literal">InputFormat</code> interface.</p></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec27"></a>Loading data from Amazon S3</h2></div></div><hr /></div><p>Amazon <span class="strong"><strong>Simple Storage Service</strong></span> (<span class="strong"><strong>S3</strong></span>) provides developers and IT teams with a secure, durable, and <a id="id166" class="indexterm"></a>scalable storage platform. The biggest advantage<a id="id167" class="indexterm"></a> of Amazon S3 is that there is no up-front<a id="id168" class="indexterm"></a> IT investment and companies can build capacity (just by clicking a button a button) as they need.</p><p>Though Amazon S3 can be used with any compute platform, it integrates really well with Amazon's cloud services such <a id="id169" class="indexterm"></a>as Amazon <span class="strong"><strong>Elastic Compute Cloud</strong></span> (<span class="strong"><strong>EC2</strong></span>) and Amazon <span class="strong"><strong>Elastic Block Storage</strong></span> (<span class="strong"><strong>EBS</strong></span>). For this <a id="id170" class="indexterm"></a>reason, companies who use <span class="strong"><strong>Amazon Web Services</strong></span> (<span class="strong"><strong>AWS</strong></span>) are likely to have significant data is already stored on <a id="id171" class="indexterm"></a>Amazon S3.</p><p>This makes a good case for loading data in Spark from Amazon S3 and that is exactly what this recipe is about.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec39"></a>How to do it...</h3></div></div></div><p>Let's start with the <a id="id172" class="indexterm"></a>AWS portal:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Go to <a class="ulink" href="http://aws.amazon.com" target="_blank">http://aws.amazon.com</a> and log in with your username and password.</p></li><li><p>Once logged in, navigate to <span class="strong"><strong>Storage &amp; Content Delivery</strong></span> | <span class="strong"><strong>S3</strong></span> | <span class="strong"><strong>Create Bucket</strong></span>:</p><div class="mediaobject"><img src="graphics/B03056_03_05.jpg" /></div></li><li><p>Enter<a id="id173" class="indexterm"></a> the bucket name—for example, <code class="literal">com.infoobjects.wordcount</code>. Please make sure you enter a unique bucket name (no two S3 buckets can have the same name globally).</p></li><li><p>Select <span class="strong"><strong>Region</strong></span>, click on <span class="strong"><strong>Create</strong></span>, and then on the bucket name you created and you <a id="id174" class="indexterm"></a>will see the following screen:</p><div class="mediaobject"><img src="graphics/B03056_03_06.jpg" /></div></li><li><p>Click on <span class="strong"><strong>Create Folder</strong></span> and enter <code class="literal">words</code> as the folder name.</p></li><li><p>Create the <code class="literal">sh.txt</code> text file on the local filesystem:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ echo "to be or not to be" &gt; sh.txt</strong></span>
</pre></div></li><li><p>Navigate to <span class="strong"><strong>Words</strong></span> | <span class="strong"><strong>Upload</strong></span> | <span class="strong"><strong>Add Files</strong></span> and choose <code class="literal">sh.txt</code> from the dialog box, as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/B03056_03_07.jpg" /></div></li><li><p>Click <a id="id175" class="indexterm"></a>on <span class="strong"><strong>Start Upload</strong></span>.</p></li><li><p>Select <span class="strong"><strong>sh.txt</strong></span> and<a id="id176" class="indexterm"></a> click on <span class="strong"><strong>Properties</strong></span> and it will show you details of the file:</p><div class="mediaobject"><img src="graphics/B03056_03_08.jpg" /></div></li><li><p>Set <code class="literal">AWS_ACCESS_KEY</code> and <code class="literal">AWS_SECRET_ACCESS_KEY</code> as environment variables.</p></li><li><p>Open the Spark shell and load the <code class="literal">words</code> directory from <code class="literal">s3</code> in the <code class="literal">words</code> RDD:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt;  val words = sc.textFile("s3n://com.infoobjects.wordcount/words")</strong></span>
</pre></div></li></ol></div><p>Now the RDD is<a id="id177" class="indexterm"></a> loaded and you can continue doing regular<a id="id178" class="indexterm"></a> transformations and actions on the RDD.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note08"></a>Note</h3><p>Sometimes there is <a id="id179" class="indexterm"></a>confusion between <code class="literal">s3://</code> and <code class="literal">s3n://</code>. <code class="literal">s3n://</code> means a<a id="id180" class="indexterm"></a> regular file sitting in the S3 bucket but readable and writable by the outside world. This filesystem puts a 5 GB limit on the file size.</p><p><code class="literal">s3://</code> means an HDFS file sitting in the S3 bucket. It is a block-based filesystem. The filesystem requires you to dedicate a bucket for this filesystem. There is no limit on file size in this system.</p></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec28"></a>Loading data from Apache Cassandra</h2></div></div><hr /></div><p>Apache Cassandra <a id="id181" class="indexterm"></a>is a NoSQL database with a masterless<a id="id182" class="indexterm"></a> ring cluster structure. While HDFS is a good fit for <a id="id183" class="indexterm"></a>streaming data access, it does not work well with random access. For example, HDFS will work well when your average file size is 100 MB and you want to read the whole file. If you frequently access the <span class="emphasis"><em>n</em></span>th line in a file or some other part as a record, HDFS would be too slow.</p><p>Relational databases have traditionally provided a solution to that, providing low latency, random access, but they do not work well with big data. NoSQL databases such as Cassandra fill the gap by providing relational database type access but in a distributed architecture on commodity servers.</p><p>In this recipe, we will load data from Cassandra as a Spark RDD. To make that happen Datastax, the company behind Cassandra, has contributed <code class="literal">spark-cassandra-connector</code>. This connector lets you load Cassandra tables as Spark RDDs, write Spark RDDs back to Cassandra, and execute CQL queries.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec40"></a>How to do it...</h3></div></div></div><p>Perform the following steps to load data from Cassandra:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Create a keyspace named <code class="literal">people</code> in Cassandra using the CQL shell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>cqlsh&gt; CREATE KEYSPACE people WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1 };</strong></span>
</pre></div></li><li><p>Create a column family (from CQL 3.0 onwards, it can also be called a <span class="strong"><strong>table</strong></span>) <code class="literal">person</code> in newer versions of Cassandra:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>cqlsh&gt; create columnfamily person(id int primary key,first_name varchar,last_name varchar);</strong></span>
</pre></div></li><li><p>Insert a few records in the column family:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>cqlsh&gt; insert into person(id,first_name,last_name) values(1,'Barack','Obama');</strong></span>
<span class="strong"><strong>cqlsh&gt; insert into person(id,first_name,last_name) values(2,'Joe','Smith');</strong></span>
</pre></div></li><li><p>Add<a id="id184" class="indexterm"></a> Cassandra connector dependency to SBT:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>"com.datastax.spark" %% "spark-cassandra-connector" % 1.2.0</strong></span>
</pre></div></li><li><p>You can<a id="id185" class="indexterm"></a> also add the Cassandra dependency to Maven:</p><div class="informalexample"><pre class="programlisting">&lt;dependency&gt;
  &lt;groupId&gt;com.datastax.spark&lt;/groupId&gt;
  &lt;artifactId&gt;spark-cassandra-connector_2.10&lt;/artifactId&gt;
  &lt;version&gt;1.2.0&lt;/version&gt;
&lt;/dependency&gt;</pre></div><p>Alternatively, you can also download the <code class="literal">spark-cassandra-connector</code> JAR to use directly with the Spark shell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ wget http://central.maven.org/maven2/com/datastax/spark/spark-cassandra-connector_2.10/1.1.0/spark-cassandra-connector_2.10-1.2.0.jar</strong></span>
</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note09"></a>Note</h3><p>If you would like to build the <code class="literal">uber</code> JAR with all dependencies, refer to the <span class="emphasis"><em>There's more…</em></span> section.</p></div></li><li><p>Now start the Spark shell.</p></li><li><p>Set the <code class="literal">spark.cassandra.connection.host</code> property in the Spark shell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; sc.getConf.set("spark.cassandra.connection.host", "localhost")</strong></span>
</pre></div></li><li><p>Import Cassandra-specific libraries:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import com.datastax.spark.connector._</strong></span>
</pre></div></li><li><p>Load the <code class="literal">person</code> column family as an RDD:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val personRDD = sc.cassandraTable("people","person")</strong></span>
</pre></div></li><li><p>Count the number of records in the RDD:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; personRDD.count</strong></span>
</pre></div></li><li><p>Print data in the RDD:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; personRDD.collect.foreach(println)</strong></span>
</pre></div></li><li><p>Retrieve the first row:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val firstRow = personRDD.first</strong></span>
</pre></div></li><li><p>Get the column names:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; firstRow.columnNames</strong></span>
</pre></div></li><li><p>Cassandra <a id="id186" class="indexterm"></a>can also be accessed through <a id="id187" class="indexterm"></a>Spark SQL. It has a wrapper around <code class="literal">SQLContext</code> called <code class="literal">CassandraSQLContext</code>; let's load it:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val cc = new org.apache.spark.sql.cassandra.CassandraSQLContext(sc)</strong></span>
</pre></div></li><li><p>Load the <code class="literal">person</code> data as <code class="literal">SchemaRDD</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val p = cc.sql("select * from people.person")</strong></span>
</pre></div></li><li><p>Retrieve the <code class="literal">person</code> data:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; p.collect.foreach(println)</strong></span>
</pre></div></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec41"></a>There's more...</h3></div></div></div><p>Spark Cassandra's connector library has a lot of dependencies. The connector itself and several of its<a id="id188" class="indexterm"></a> dependencies are third-party to Spark and are not available as part of the Spark installation.</p><p>These dependencies need to be made available to the driver as well as executors at runtime. One way to do this is to bundle all transitive dependencies, but that is a laborious and error-prone process. The recommended approach is to bundle all the dependencies along with the connector library. This will result in a fat JAR, popularly known as the <code class="literal">uber</code> JAR.</p><p>SBT provides the <code class="literal">sbt-assembly</code> plugin, which makes creating <code class="literal">uber</code> JARs very easy. The following are the steps to create an <code class="literal">uber</code> JAR for <code class="literal">spark-cassandra-connector</code>. These steps are general enough so that you can use them to create any <code class="literal">uber</code> JAR:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Create a folder named <code class="literal">uber</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ mkdir uber</strong></span>
</pre></div></li><li><p>Change the directory to <code class="literal">uber</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cd uber</strong></span>
</pre></div></li><li><p>Open the SBT prompt:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sbt</strong></span>
</pre></div></li><li><p>Give this project a name <code class="literal">sc-uber</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; set name := "sc-uber"</strong></span>
</pre></div></li><li><p>Save the session:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; session save</strong></span>
</pre></div></li><li><p>Exit the session:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; exit</strong></span>
</pre></div><p>This will create <code class="literal">build.sbt</code>, <code class="literal">project</code>, and <code class="literal">target</code> folders in the <code class="literal">uber</code> folder as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/B03056_03_09.jpg" /></div></li><li><p>Add the <code class="literal">spark-cassandra-driver</code> dependency to <code class="literal">build.sbt</code> at the end after<a id="id189" class="indexterm"></a> leaving a blank line as shown in the <a id="id190" class="indexterm"></a>following screenshot:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ vi buid.sbt</strong></span>
</pre></div><div class="mediaobject"><img src="graphics/B03056_03_10.jpg" /></div></li><li><p>We will use <code class="literal">MergeStrategy.first</code> as the default. Besides that, there are some files, such as <code class="literal">manifest.mf</code>, that every JAR bundles for metadata, and we can simply discard them. We are going to use <code class="literal">MergeStrategy.discard</code> for that. The following is the screenshot of <code class="literal">build.sbt</code> with <code class="literal">assemblyMergeStrategy</code> added:</p><div class="mediaobject"><img src="graphics/B03056_03_11.jpg" /></div></li><li><p>Now create <code class="literal">plugins.sbt</code> in the <code class="literal">project</code> folder and type the following for the <code class="literal">sbt-assembly</code> plugin:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.12.0")</strong></span>
</pre></div></li><li><p>We are ready to build (<code class="literal">assembly</code>) a JAR now:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sbt assembly</strong></span>
</pre></div><p>The <code class="literal">uber</code> JAR is now created in <code class="literal">target/scala-2.10/sc-uber-assembly-0.1-SNAPSHOT.jar</code>.</p></li><li><p>Copy it <a id="id191" class="indexterm"></a>to a suitable location where you keep<a id="id192" class="indexterm"></a> all third-party JARs—for example, <code class="literal">/home/hduser/thirdparty</code>—and rename it to an easier name (unless you like longer names):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ mv thirdparty/sc-uber-assembly-0.1-SNAPSHOT.jar  thirdparty/sc-uber.jar</strong></span>
</pre></div></li><li><p>Load the Spark shell with the <code class="literal">uber</code> JAR using <code class="literal">--jars</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell --jars thirdparty/sc-uber.jar</strong></span>
</pre></div></li><li><p>To submit the Scala code to a cluster, you can call <code class="literal">spark-submit</code> with the same JARS option:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-submit --jars thirdparty/sc-uber.jar</strong></span>
</pre></div></li></ol></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec01"></a>Merge strategies in sbt-assembly</h4></div></div></div><p>If multiple JARs have files with the same name and the same relative path, the default merge strategy for the <code class="literal">sbt-assembly</code> plugin is to verify that content is same for all the files and error out otherwise. This strategy is called <code class="literal">MergeStrategy.deduplicate</code>.</p><p>The following <a id="id193" class="indexterm"></a>are the available merge strategies in the <code class="literal">sbt-assembly plugin</code>:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Strategy name</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Description</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p><code class="literal">MergeStrategy.deduplicate</code></p>
</td><td style="" align="left" valign="top">
<p>The default strategy</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">MergeStrategy.first</code></p>
</td><td style="" align="left" valign="top">
<p>Picks first file according to classpath</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">MergeStrategy.last</code></p>
</td><td style="" align="left" valign="top">
<p>Picks last file according to classpath</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">MergeStrategy.singleOrError</code></p>
</td><td style="" align="left" valign="top">
<p>Errors out (merge conflict not expected)</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">MergeStrategy.concat</code></p>
</td><td style="" align="left" valign="top">
<p>Concatenates all matching files together</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">MergeStrategy.filterDistinctLines</code></p>
</td><td style="" align="left" valign="top">
<p>Concatenates leaving out duplicates</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">MergeStrategy.rename</code></p>
</td><td style="" align="left" valign="top">
<p>Renames files</p>
</td></tr></tbody></table></div></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec29"></a>Loading data from relational databases</h2></div></div><hr /></div><p>A lot <a id="id194" class="indexterm"></a>of important data lies in relational databases<a id="id195" class="indexterm"></a> that Spark needs to query. JdbcRDD is a Spark feature that allows relational tables to be loaded as RDDs. This recipe will explain how to<a id="id196" class="indexterm"></a> use JdbcRDD.</p><p>Spark SQL to be introduced in the next chapter includes a data source for JDBC. This should be preferred over the current recipe as results are returned as DataFrames (to be introduced in the next chapter), which can be easily processed by Spark SQL and also joined with other data sources.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec42"></a>Getting ready</h3></div></div></div><p>Please make sure that the JDBC driver JAR is visible on the client node and all slaves nodes on which executor will run.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec43"></a>How to do it...</h3></div></div></div><p>Perform the<a id="id197" class="indexterm"></a> following steps to load data from relational<a id="id198" class="indexterm"></a> databases:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Create a table named <code class="literal">person</code> in MySQL using the following DDL:</p><div class="informalexample"><pre class="programlisting">CREATE TABLE 'person' (
  'person_id' int(11) NOT NULL AUTO_INCREMENT,
  'first_name' varchar(30) DEFAULT NULL,
  'last_name' varchar(30) DEFAULT NULL,
  'gender' char(1) DEFAULT NULL,
  PRIMARY KEY ('person_id');
)</pre></div></li><li><p>Insert some data:</p><div class="informalexample"><pre class="programlisting">Insert into person values('Barack','Obama','M');
Insert into person values('Bill','Clinton','M');
Insert into person values('Hillary','Clinton','F');</pre></div></li><li><p>Download <code class="literal">mysql-connector-java-x.x.xx-bin.jar</code> from <a class="ulink" href="http://dev.mysql.com/downloads/connector/j/" target="_blank">http://dev.mysql.com/downloads/connector/j/</a>.</p></li><li><p>Make the MySQL driver available to the Spark shell and launch it:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell --jars /path-to-mysql-jar/mysql-connector-java-5.1.29-bin.jar</strong></span>
</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note10"></a>Note</h3><p>Please note that <code class="literal">path-to-mysql-jar</code> is not the actual path name. You should use the actual path name.</p></div></li><li><p>Create variables for the username, password, and JDBC URL:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val url="jdbc:mysql://localhost:3306/hadoopdb"</strong></span>
<span class="strong"><strong>scala&gt; val username = "hduser"</strong></span>
<span class="strong"><strong>scala&gt; val password = "******"</strong></span>
</pre></div></li><li><p>Import JdbcRDD:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.rdd.JdbcRDD</strong></span>
</pre></div></li><li><p>Import JDBC-related classes:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import java.sql.{Connection, DriverManager, ResultSet}</strong></span>
</pre></div></li><li><p>Create an instance of the JDBC driver:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; Class.forName("com.mysql.jdbc.Driver").newInstance</strong></span>
</pre></div></li><li><p>Load JdbcRDD:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val myRDD = new JdbcRDD( sc, () =&gt;</strong></span>
<span class="strong"><strong>DriverManager.getConnection(url,username,password) ,</strong></span>
<span class="strong"><strong>"select first_name,last_name,gender from person limit ?, ?",</strong></span>
<span class="strong"><strong>1, 5, 2, r =&gt; r.getString("last_name") + ", " + r.getString("first_name"))</strong></span>
</pre></div></li><li><p>Now query the results:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; myRDD.count</strong></span>
<span class="strong"><strong>scala&gt; myRDD.foreach(println)</strong></span>
</pre></div></li><li><p>Save the RDD to HDFS:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; myRDD.saveAsTextFile("hdfs://localhost:9000/user/hduser/person")</strong></span>
</pre></div></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec44"></a>How it works…</h3></div></div></div><p>JdbcRDD is <a id="id199" class="indexterm"></a>an RDD that executes a SQL query on a <a id="id200" class="indexterm"></a>JDBC connection and retrieves the results. The <a id="id201" class="indexterm"></a>following is a JdbcRDD constructor:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>JdbcRDD( SparkContext, getConnection: () =&gt; Connection,</strong></span>
<span class="strong"><strong>sql: String, lowerBound: Long, upperBound: Long,</strong></span>
<span class="strong"><strong>numPartitions: Int,  mapRow: (ResultSet) =&gt; T =</strong></span>
<span class="strong"><strong> JdbcRDD.resultSetToObjectArray)</strong></span>
</pre></div><p>The two ?'s are bind variables for a prepared statement inside JdbcRDD. The first ? is for the offset (lower bound), that is, which row should we start computing with, the second ? is for the limit (upper bound), that is, how many rows should we read.</p><p>JdbcRDD is a great way to load data in Spark directly from relational databases on an ad-hoc basis. If you would like to load data in bulk from RDBMS, there are other approaches that would work better, for example, Apache Sqoop is a powerful tool that imports and exports data from relational databases to HDFS.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch04"></a>Chapter 4. Spark SQL</h2></div></div></div><p>Spark SQL<a id="id202" class="indexterm"></a> is a Spark module for processing a structured data. This chapter is divided into the following recipes:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Understanding the Catalyst optimizer</p></li><li style="list-style-type: disc"><p>Creating HiveContext</p></li><li style="list-style-type: disc"><p>Inferring schema using case classes</p></li><li style="list-style-type: disc"><p>Programmatically specifying the schema</p></li><li style="list-style-type: disc"><p>Loading and saving data using the Parquet format</p></li><li style="list-style-type: disc"><p>Loading and saving data using the JSON format</p></li><li style="list-style-type: disc"><p>Loading and saving data from relational databases</p></li><li style="list-style-type: disc"><p>Loading and saving data from an arbitrary source</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec30"></a>Introduction</h2></div></div><hr /></div><p>Spark can process data from various data sources such as HDFS, Cassandra, HBase, and relational databases, including HDFS. Big data frameworks (unlike relational database systems) do not enforce schema while writing. HDFS is a perfect example where any arbitrary file is welcome during the write phase. Reading data is a different story, however. You need to give some structure to even completely unstructured data to make sense out of it. With this structured data, SQL comes very handy when it comes to analysis.</p><p>Spark SQL is a relatively new component in Spark ecosystem, introduced in Spark 1.0 for the first time. It incorporates a project named Shark, which was an attempt to make Hive run on Spark.</p><p>Hive is essentially a relational abstraction, which converts SQL queries to MapReduce jobs.</p><div class="mediaobject"><img src="graphics/3056_04_01.jpg" /></div><p>Shark replaced<a id="id203" class="indexterm"></a> the MapReduce part with Spark while retaining most of the code base.</p><div class="mediaobject"><img src="graphics/3056_04_02.jpg" /></div><p>Initially, it worked fine, but very soon, Spark developers hit roadblocks and could not optimize it any further. Finally, they decided to write the SQL Engine from scratch and that gave birth to Spark SQL.</p><div class="mediaobject"><img src="graphics/3056_04_03.jpg" /></div><p>Spark SQL<a id="id204" class="indexterm"></a> took care of all the performance challenges, but it had to provide compatibility with Hive and for that reason, a new wrapper context, <code class="literal">HiveContext</code>, was created on top of <code class="literal">SQLContext</code>.</p><p>Spark SQL supports accessing data using standard SQL queries and HiveQL, a SQL-like query language that Hive uses. In this chapter, we will explore different features of Spark SQL. It supports a subset of HiveQL as well as a subset of SQL 92. It runs SQL/HiveQL queries alongside, or replacing the existing Hive deployments.</p><p>Running SQL is only a part of the reason for the creation of Spark SQL. One big reason is that it helps to create and run Spark programs faster. It lets developers write less code, program read less data, and let the catalyst optimizer do all the heavy lifting.</p><p>Spark SQL uses a programming <a id="id205" class="indexterm"></a>abstraction called <span class="strong"><strong>DataFrame</strong></span>. It is a distributed collection of data organized in named columns. DataFrame is equivalent to a database table, but provides much finer level of optimization. The DataFrame API also ensures that Spark's performance is consistent across different language bindings.</p><p>Let's contrast DataFrames with RDDs. An RDD is an opaque collection of objects with no idea about the format of the underlying data. In contrast, DataFrames have schema associated with them. You can also look at DataFrames as RDDs with schema added to them. In fact, until Spark 1.2, there was an artifact called <span class="strong"><strong>SchemaRDD</strong></span>, which has now evolved into<a id="id206" class="indexterm"></a> DataFrame. They provide much richer functionality than SchemaRDDs.</p><p>This extra information about schema makes possible to do a lot of optimizations, which were not otherwise possible.</p><p>DataFrames also transparently load from various data sources, such as Hive tables, Parquet files, JSON files, and external databases using JDBC. DataFrames can be viewed as RDDs of row objects, allowing users to call the procedural Spark APIs such as map.</p><p>The DataFrame API is available in Scala, Java, Python, and also R starting Spark 1.4.</p><p>Users can perform<a id="id207" class="indexterm"></a> relational operations on DataFrames using a <span class="strong"><strong>domain-specific language</strong></span> (<span class="strong"><strong>DSL</strong></span>). DataFrames support all the common relational operators and they all take expression objects in a limited DSL that lets Spark capture the structure of the expression.</p><p>We will <a id="id208" class="indexterm"></a>start with the entry point into Spark SQL, that is, SQLContext. We will also cover HiveContext that is a wrapper around SQLContext to support Hive functionality. Please note that HiveContext is more battle-tested and provides a richer functionality, so it is strongly recommended to use it even if you do not plan to connect to Hive. Slowly, SQLContext will come to the same level of functionality as HiveContext is.</p><p>There are two ways to associate schema with RDDs to create DataFrames. The easy way is to leverage Scala case classes, which we are going to cover first. Spark uses Java reflection to deduce schema from case classes. There is also a way to programmatically specify schema for advanced needs, which we will cover next.</p><p>Spark SQL provides an easy way to both load and save the Parquet files, which will also be covered. Lastly, we will cover loading from and saving data to JSON.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec31"></a>Understanding the Catalyst optimizer</h2></div></div><hr /></div><p>Most of<a id="id209" class="indexterm"></a> the power of Spark SQL comes due to Catalyst optimizer, so it makes sense to spend some time understanding it.</p><div class="mediaobject"><img src="graphics/3056_04_04.jpg" /></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec45"></a>How it works…</h3></div></div></div><p>Catalyst optimizer primarily leverages functional programming constructs of Scala such as pattern matching. It offers a general framework for transforming trees, which we use to perform analysis, optimization, planning, and runtime code generation.</p><p>Catalyst optimizer<a id="id210" class="indexterm"></a> has two primary goals:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Make adding new optimization techniques easy</p></li><li style="list-style-type: disc"><p>Enable external developers to extend the optimizer</p></li></ul></div><p>Spark SQL uses Catalyst's transformation framework in four phases:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Analyzing a logical plan to resolve references</p></li><li style="list-style-type: disc"><p>Logical plan optimization</p></li><li style="list-style-type: disc"><p>Physical planning</p></li><li style="list-style-type: disc"><p>Code generation to compile the parts of the query to Java bytecode</p></li></ul></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl3sec02"></a>Analysis</h4></div></div></div><p>The analysis phase<a id="id211" class="indexterm"></a> involved looking at a SQL query or a DataFrame, creating a logical plan out of it, which is still unresolved (the columns referred may not exist or may be of wrong datatype) and then resolving this plan using the Catalog object (which connects to the physical data source), and creating a logical plan, as shown in the following diagram:</p><div class="mediaobject"><img src="graphics/3056_04_05.jpg" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl3sec03"></a>Logical plan optimization</h4></div></div></div><p>The <a id="id212" class="indexterm"></a>logical plan optimization phase applies standard rule-based optimization to the logical plan. These include constant folding, predicate pushdown, projection pruning, null propagation, Boolean expression simplification, and other rules.</p><p>I would like to draw special attention to predicate the pushdown rule here. The concept is simple; if you issue a query in one place to run against the massive data, which is another place, it can lead to a lot of unnecessary data moving across the network.</p><p>If we can push down the part of the query to where the data is stored, and thus filter out unnecessary data, it reduces network traffic significantly.</p><div class="mediaobject"><img src="graphics/3056_04_06.jpg" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl3sec04"></a>Physical planning</h4></div></div></div><p>In the <a id="id213" class="indexterm"></a>physical planning phase, Spark SQL takes a logical plan and generates one or more physical plans. It then measures the cost of each physical plan and generates one physical plan based on that.</p><div class="mediaobject"><img src="graphics/3056_04_07.jpg" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl3sec05"></a>Code generation</h4></div></div></div><p>The<a id="id214" class="indexterm"></a> final phase of query optimization involves generating Java bytecode to run on each machine. It uses a special Scala feature <a id="id215" class="indexterm"></a>called <span class="strong"><strong>Quasi quotes</strong></span> to accomplish that.</p></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec32"></a>Creating HiveContext</h2></div></div><hr /></div><p><code class="literal">SQLContext</code> and its descendant <code class="literal">HiveContext</code> are the two entry points into the world of Spark SQL. <code class="literal">HiveContext</code> provides<a id="id216" class="indexterm"></a> a superset of functionality<a id="id217" class="indexterm"></a> provided by SQLContext. The additional features are:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>More complete and battle-tested HiveQL parser</p></li><li style="list-style-type: disc"><p>Access to Hive UDFs</p></li><li style="list-style-type: disc"><p>Ability to read data from Hive tables</p></li></ul></div><p>From <a id="id218" class="indexterm"></a>Spark 1.3 onwards, the Spark shell comes loaded with sqlContext (which is an instance of <code class="literal">HiveContext</code> not <code class="literal">SQLContext</code>). If you are creating <code class="literal">SQLContext</code> in Scala code, it can be created using <code class="literal">SparkContext</code>, as follows:</p><div class="informalexample"><pre class="programlisting">val sc: SparkContext
val sqlContext = new org.apache.spark.sql.SQLContext(sc)</pre></div><p>In this recipe, we will cover how to create instance of <code class="literal">HiveContext</code>, and then access Hive functionality through Spark SQL.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec46"></a>Getting ready</h3></div></div></div><p>To enable Hive functionality, make sure that you have Hive enabled (-Phive) assembly JAR is available on all worker nodes; also, copy <code class="literal">hive-site.xml</code> into the <code class="literal">conf</code> directory of the Spark installation. It is important that Spark has access to <code class="literal">hive-site.xml</code>; otherwise, it <a id="id219" class="indexterm"></a>will create its own Hive metastore and will not connect to your existing Hive warehouse.</p><p>By default, all the tables created by Spark SQL are Hive-managed tables, that is, Hive has complete control on life cycle of a table, including deleting it if table metadata is dropped using the <code class="literal">drop table</code> command. This holds true only for persistent tables. Spark SQL also has mechanism to create temporary tables out of DataFrames for ease of writing queries, and they are not managed by Hive.</p><p>Please note that Spark 1.4 supports Hive versions 0.13.1. You can specify a version of Hive you would like to build against using the <code class="literal">-Phive-&lt;version&gt; build</code> option while building with Maven. For example, to build with 0.12.0, you can use <code class="literal">-Phive-0.12.0</code>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec47"></a>How to do it...</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Start the Spark shell and give it some extra memory:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell --driver-memory 1G</strong></span>
</pre></div></li><li><p>Create an instance of <code class="literal">HiveContext</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val hc = new org.apache.spark.sql.hive.HiveContext(sc)</strong></span>
</pre></div></li><li><p>Create a Hive table <code class="literal">Person</code> with <code class="literal">first_name</code>, <code class="literal">last_name</code>, and <code class="literal">age</code> as columns:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt;  hc.sql("create table if not exists person(first_name string, last_name string, age int) row format delimited fields terminated by ','")</strong></span>
</pre></div></li><li><p>Open another shell and create the <code class="literal">person</code> data in a local file:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ mkdir person</strong></span>
<span class="strong"><strong>$ echo "Barack,Obama,53" &gt;&gt; person/person.txt</strong></span>
<span class="strong"><strong>$ echo "George,Bush,68" &gt;&gt; person/person.txt</strong></span>
<span class="strong"><strong>$ echo "Bill,Clinton,68" &gt;&gt; person/person.txt</strong></span>
</pre></div></li><li><p>Load the data in the <code class="literal">person</code> table:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; hc.sql("load data local inpath \"/home/hduser/person\" into table person")</strong></span>
</pre></div></li><li><p>Alternatively, load that data in the <code class="literal">person</code> table from HDFS:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; hc.sql("load data inpath \"/user/hduser/person\" into table person")</strong></span>
</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note11"></a>Note</h3><p>Please note that using <code class="literal">load data inpath</code> moves the data from another HDFS location to the Hive's <code class="literal">warehouse</code> directory, which is, by default, <code class="literal">/user/hive/warehouse</code>. You can also specify fully qualified path such as <code class="literal">hdfs://localhost:9000/user/hduser/person</code>.</p></div></li><li><p>Select the<a id="id220" class="indexterm"></a> person data using HiveQL:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val persons = hc.sql("from person select first_name,last_name,age")</strong></span>
<span class="strong"><strong>scala&gt; persons.collect.foreach(println)</strong></span>
</pre></div></li><li><p>Create a new table from the output of a <code class="literal">select</code> query:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; hc.sql("create table person2 as select first_name, last_name from person;")</strong></span>
</pre></div></li><li><p>You can also copy directly from one table to another:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; hc.sql("create table person2 like person location '/user/hive/warehouse/person'")</strong></span>
</pre></div></li><li><p>Create two tables <code class="literal">people_by_last_name</code> and <code class="literal">people_by_age</code> to keep counts:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; hc.sql("create table people_by_last_name(last_name string,count int)")</strong></span>
<span class="strong"><strong>scala&gt; hc.sql("create table people_by_age(age int,count int)")</strong></span>
</pre></div></li><li><p>You can also insert records into multiple tables using a HiveQL query:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; hc.sql("""from person</strong></span>
<span class="strong"><strong>  insert overwrite table people_by_last_name</strong></span>
<span class="strong"><strong>    select last_name, count(distinct first_name)</strong></span>
<span class="strong"><strong>    group by last_name</strong></span>
<span class="strong"><strong>insert overwrite table people_by_age</strong></span>
<span class="strong"><strong>    select age, count(distinct first_name)</strong></span>
<span class="strong"><strong>    group by age; """)</strong></span>
</pre></div></li></ol></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec33"></a>Inferring schema using case classes</h2></div></div><hr /></div><p>Case classes <a id="id221" class="indexterm"></a>are special classes in Scala that provide you <a id="id222" class="indexterm"></a>with the boiler plate implementation of the constructor, getters (accessors), equals and hashCode, and implement <code class="literal">Serializable</code>. Case classes work really well to encapsulate data as objects. Readers, familiar <a id="id223" class="indexterm"></a>with Java, can relate it to <span class="strong"><strong>plain old Java objects</strong></span> (<span class="strong"><strong>POJOs</strong></span>) or Java bean.</p><p>The beauty of case classes is that all that grunt work, which is required in Java, can be done with case classes in a single line of code. Spark uses reflection on case classes to infer schema.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec48"></a>How to do it...</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Start <a id="id224" class="indexterm"></a>the Spark shell and give it some extra memory:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell --driver-memory 1G</strong></span>
</pre></div></li><li><p>Import<a id="id225" class="indexterm"></a> for the implicit conversions:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import sqlContext.implicits._</strong></span>
</pre></div></li><li><p>Create a <code class="literal">Person</code> case class:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; case class Person(first_name:String,last_name:String,age:Int)</strong></span>
</pre></div></li><li><p>In another shell, create some sample data to be put in HDFS:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ mkdir person</strong></span>
<span class="strong"><strong>$ echo "Barack,Obama,53" &gt;&gt; person/person.txt</strong></span>
<span class="strong"><strong>$ echo "George,Bush,68" &gt;&gt; person/person.txt</strong></span>
<span class="strong"><strong>$ echo "Bill,Clinton,68" &gt;&gt; person/person.txt</strong></span>
<span class="strong"><strong>$ hdfs dfs -put person person</strong></span>
</pre></div></li><li><p>Load the <code class="literal">person</code> directory as an RDD:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val p = sc.textFile("hdfs://localhost:9000/user/hduser/person")</strong></span>
</pre></div></li><li><p>Split each line into an array of string, based on a comma, as a delimiter:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>val pmap = p.map( line =&gt; line.split(","))</strong></span>
</pre></div></li><li><p>Convert the RDD of Array[String] into the RDD of <code class="literal">Person</code> case objects:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val personRDD = pmap.map( p =&gt; Person(p(0),p(1),p(2).toInt))</strong></span>
</pre></div></li><li><p>Convert the <code class="literal">personRDD</code> into the <code class="literal">personDF</code> DataFrame:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val personDF = personRDD.toDF</strong></span>
</pre></div></li><li><p>Register the <code class="literal">personDF</code> as a table:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; personDF.registerTempTable("person")</strong></span>
</pre></div></li><li><p>Run a SQL query against it:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val people = sql("select * from person")</strong></span>
</pre></div></li><li><p>Get <a id="id226" class="indexterm"></a>the <a id="id227" class="indexterm"></a>output values from <code class="literal">persons</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; people.collect.foreach(println)</strong></span>
</pre></div></li></ol></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec34"></a>Programmatically specifying the schema</h2></div></div><hr /></div><p>There are few <a id="id228" class="indexterm"></a>cases where case classes might not work; one of these cases is that the case classes cannot take more than 22 fields. Another case can be that you do not know about schema beforehand. In this approach, the data is loaded as an RDD of the <code class="literal">Row</code> objects. Schema is created separately using the <code class="literal">StructType</code> and <code class="literal">StructField</code> objects, which represent a table and a field respectively. Schema is applied to the <code class="literal">Row</code> RDD to create a DataFrame.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec49"></a>How to do it...</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Start the Spark shell and give it some extra memory:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell --driver-memory 1G</strong></span>
</pre></div></li><li><p>Import for the implicit conversion:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import sqlContext.implicit._</strong></span>
</pre></div></li><li><p>Import the Spark SQL datatypes and <code class="literal">Row</code> objects:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.sql._</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.sql.types._</strong></span>
</pre></div></li><li><p>In another shell, create some sample data to be put in HDFS:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ mkdir person</strong></span>
<span class="strong"><strong>$ echo "Barack,Obama,53" &gt;&gt; person/person.txt</strong></span>
<span class="strong"><strong>$ echo "George,Bush,68" &gt;&gt; person/person.txt</strong></span>
<span class="strong"><strong>$ echo "Bill,Clinton,68" &gt;&gt; person/person.txt</strong></span>
<span class="strong"><strong>$ hdfs dfs -put person person</strong></span>
</pre></div></li><li><p>Load the <code class="literal">person</code> data in an RDD:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val p = sc.textFile("hdfs://localhost:9000/user/hduser/person")</strong></span>
</pre></div></li><li><p>Split each line into an array of string, based on a comma, as a delimiter:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val pmap = p.map( line =&gt; line.split(","))</strong></span>
</pre></div></li><li><p>Convert the RDD of array[string] to the RDD of the <code class="literal">Row</code> objects:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val personData = pmap.map( p =&gt; Row(p(0),p(1),p(2).toInt))</strong></span>
</pre></div></li><li><p>Create schema using the <code class="literal">StructType</code> and <code class="literal">StructField</code> objects. The <code class="literal">StructField</code> object takes parameters in the form of param name, param type, and nullability:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val schema = StructType(</strong></span>
<span class="strong"><strong>    Array(StructField("first_name",StringType,true),</strong></span>
<span class="strong"><strong>StructField("last_name",StringType,true),</strong></span>
<span class="strong"><strong>StructField("age",IntegerType,true)</strong></span>
<span class="strong"><strong>))</strong></span>
</pre></div></li><li><p>Apply<a id="id229" class="indexterm"></a> schema to create the <code class="literal">personDF</code> DataFrame:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val personDF = sqlContext.createDataFrame(personData,schema)</strong></span>
</pre></div></li><li><p>Register the <code class="literal">personDF</code> as a table:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; personDF.registerTempTable("person")</strong></span>
</pre></div></li><li><p>Run a SQL query against it:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val persons = sql("select * from person")</strong></span>
</pre></div></li><li><p>Get the output values from <code class="literal">persons</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; persons.collect.foreach(println)</strong></span>
</pre></div></li></ol></div><p>In this recipe, we learned how to create a DataFrame by programmatically specifying schema.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec50"></a>How it works…</h3></div></div></div><p>A <code class="literal">StructType</code> object defines the schema. You can consider it equivalent to a table or a row in the relational world. <code class="literal">StructType</code> takes in an array of the <code class="literal">StructField</code> objects, as in the following signature:</p><div class="informalexample"><pre class="programlisting">StructType(fields: Array[StructField])</pre></div><p>A <code class="literal">StructField</code> object has the following signature:</p><div class="informalexample"><pre class="programlisting">StructField(name: String, dataType: DataType, nullable: Boolean = true, metadata: Metadata = Metadata.empty)</pre></div><p>Here is some more information on the parameters used:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><code class="literal">name</code>: This represents the name of the field.</p></li><li style="list-style-type: disc"><p><code class="literal">dataType</code>: This shows the datatype of this field.</p><p>The following datatypes are allowed:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /></colgroup><tbody><tr><td style="" align="left" valign="top">
<p><code class="literal">IntegerType</code></p>
</td><td style="" align="left" valign="top">
<p><code class="literal">FloatType</code></p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">BooleanType</code></p>
</td><td style="" align="left" valign="top">
<p><code class="literal">ShortType</code></p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">LongType</code></p>
</td><td style="" align="left" valign="top">
<p><code class="literal">ByteType</code></p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">DoubleType</code></p>
</td><td style="" align="left" valign="top">
<p><code class="literal">StringType</code></p>
</td></tr></tbody></table></div></li><li style="list-style-type: disc"><p><code class="literal">nullable</code>: This <a id="id230" class="indexterm"></a>shows whether this field can be null.</p></li><li style="list-style-type: disc"><p><code class="literal">metadata</code>: This shows the metadata of this field. Metadata is a wrapper over <code class="literal">Map[String,Any]</code> so that it can contain any arbitrary metadata.</p></li></ul></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec35"></a>Loading and saving data using the Parquet format</h2></div></div><hr /></div><p>Apache Parquet is a columnar data storage format, specifically designed for big data storage and processing. Parquet is based on record shredding and assembly algorithm in the Google Dremel paper. In Parquet, data in a single column is stored contiguously.</p><p>The<a id="id231" class="indexterm"></a> columnar format gives Parquet some unique benefits. For<a id="id232" class="indexterm"></a> example, if you have a table with 100 columns<a id="id233" class="indexterm"></a> and you mostly access 10 columns, in a row-based format <a id="id234" class="indexterm"></a>you will have to load all 100 columns, as granularity level is at row level. But, in Parquet, you will only load 10 columns. Another benefit is that since all the data in a given column is of the same datatype (by definition), compression is much more efficient.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec51"></a>How to do it...</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Open the terminal and create the <code class="literal">person</code> data in a local file:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ mkdir person</strong></span>
<span class="strong"><strong>$ echo "Barack,Obama,53" &gt;&gt; person/person.txt</strong></span>
<span class="strong"><strong>$ echo "George,Bush,68" &gt;&gt; person/person.txt</strong></span>
<span class="strong"><strong>$ echo "Bill,Clinton,68" &gt;&gt; person/person.txt</strong></span>
</pre></div></li><li><p>Upload the <code class="literal">person</code> directory to HDFS:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hdfs dfs -put person /user/hduser/person</strong></span>
</pre></div></li><li><p>Start the Spark shell and give it some extra memory:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell --driver-memory 1G</strong></span>
</pre></div></li><li><p>Import for the implicit conversion:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import sqlContext.implicits._</strong></span>
</pre></div></li><li><p>Create a case class for <code class="literal">Person</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; case class Person(firstName: String, lastName: String, age:Int)</strong></span>
</pre></div></li><li><p>Load the <code class="literal">person</code> directory from HDFS and map it to the <code class="literal">Person</code> case class:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val personRDD = sc.textFile("hdfs://localhost:9000/user/hduser/person").map(_.split("\t")).map(p =&gt; Person(p(0),p(1),p(2).toInt))</strong></span>
</pre></div></li><li><p>Convert the <code class="literal">personRDD</code> into the <code class="literal">person</code> DataFrame:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val person = personRDD.toDF</strong></span>
</pre></div></li><li><p>Register <a id="id235" class="indexterm"></a>the <code class="literal">person</code> DataFrame as a temp <a id="id236" class="indexterm"></a>table so that SQL queries can be run <a id="id237" class="indexterm"></a>against it. Please note that the DataFrame <a id="id238" class="indexterm"></a>name does not have to be the same as the table name.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; person.registerTempTable("person")</strong></span>
</pre></div></li><li><p>Select all the person with age over 60 years:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val sixtyPlus = sql("select * from person where age &gt; 60")</strong></span>
</pre></div></li><li><p>Print values:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; sixtyPlus.collect.foreach(println)</strong></span>
</pre></div></li><li><p>Let's save this <code class="literal">sixtyPlus</code> RDD in the Parquet format:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; sixtyPlus.saveAsParquetFile("hdfs://localhost:9000/user/hduser/sp.parquet")</strong></span>
</pre></div></li><li><p>The previous step created a directory called <code class="literal">sp.parquet</code> in the HDFS root. You can run the <code class="literal">hdfs dfs -ls</code> command in another shell to make sure that it's created:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hdfs dfs -ls sp.parquet</strong></span>
</pre></div></li><li><p>Load contents of the Parquet files in the Spark shell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val parquetDF = sqlContext.load("hdfs://localhost:9000/user/hduser/sp.parquet")</strong></span>
</pre></div></li><li><p>Register the loaded <code class="literal">parquet</code> DF as a <code class="literal">temp</code> table:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; </strong></span>
<span class="strong"><strong>parquetDF</strong></span>
<span class="strong"><strong>.registerTempTable("sixty_plus")</strong></span>
</pre></div></li><li><p>Run a query against the preceding <code class="literal">temp</code> table:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; sql("select * from sixty_plus")</strong></span>
</pre></div></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec52"></a>How it works…</h3></div></div></div><p>Let's spend some time understanding the Parquet format deeper. The following is sample data represented in the table format:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>First_Name</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Last_Name</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Age</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>Barack</p>
</td><td style="" align="left" valign="top">
<p>Obama</p>
</td><td style="" align="left" valign="top">
<p>53</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>George</p>
</td><td style="" align="left" valign="top">
<p>Bush</p>
</td><td style="" align="left" valign="top">
<p>68</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Bill</p>
</td><td style="" align="left" valign="top">
<p>Clinton</p>
</td><td style="" align="left" valign="top">
<p>68</p>
</td></tr></tbody></table></div><p>In the row format, the data will be stored like this:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /><col align="left" /><col align="left" /><col align="left" /><col align="left" /><col align="left" /><col align="left" /></colgroup><tbody><tr><td style="" align="left" valign="top">
<p>Barack</p>
</td><td style="" align="left" valign="top">
<p>Obama</p>
</td><td style="" align="left" valign="top">
<p>53</p>
</td><td style="" align="left" valign="top">
<p>George</p>
</td><td style="" align="left" valign="top">
<p>Bush</p>
</td><td style="" align="left" valign="top">
<p>68</p>
</td><td style="" align="left" valign="top">
<p>Bill</p>
</td><td style="" align="left" valign="top">
<p>Clinton</p>
</td><td style="" align="left" valign="top">
<p>68</p>
</td></tr></tbody></table></div><p>In the columnar layout, the data will be stored like this:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /><col align="left" /><col align="left" /><col align="left" /><col align="left" /><col align="left" /><col align="left" /><col align="left" /></colgroup><tbody><tr><td style="" align="left" valign="top">
<p>Row group =&gt;</p>
</td><td style="" align="left" valign="top">
<p>Barack</p>
</td><td style="" align="left" valign="top">
<p>George</p>
</td><td style="" align="left" valign="top">
<p>Bill</p>
</td><td style="" align="left" valign="top">
<p>Obama</p>
</td><td style="" align="left" valign="top">
<p>Bush</p>
</td><td style="" align="left" valign="top">
<p>Clinton</p>
</td><td style="" align="left" valign="top">
<p>53</p>
</td><td style="" align="left" valign="top">
<p>68</p>
</td><td style="" align="left" valign="top">
<p>68</p>
</td></tr><tr><td style="" align="left" valign="top"> </td><td style="" colspan="3" align="left" valign="top">
<p>Column chunk</p>
</td><td style="" colspan="3" align="left" valign="top">
<p>Column chunk</p>
</td><td style="" colspan="3" align="left" valign="top">
<p>Column chunk</p>
</td></tr></tbody></table></div><p>Here's a<a id="id239" class="indexterm"></a> brief description about the different parts:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><span class="strong"><strong>Row group</strong></span>: This<a id="id240" class="indexterm"></a> shows the horizontal partitioning of data into rows. A row group consists of column chunks.</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Column chunk</strong></span>: A column chunk has data for a given column in a row group. A <a id="id241" class="indexterm"></a>column chunk is always physically <a id="id242" class="indexterm"></a>contiguous. A row group has only one column chunk per column.</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Page</strong></span>: A column chunk is divided into pages. A page is a unit of storage and cannot be further divided. Pages are written back to back in column chunk. The data for a page can be compressed.</p></li></ul></div><p>If there is already data in a Hive table, say, the <code class="literal">person</code> table, you can directly save it in the Parquet format by performing the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Create a table named <code class="literal">person_parquet</code> with schema, the same as <code class="literal">person</code>, but in the Parquet storage format (for Hive 0.13 onwards):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>hive&gt; create table person_parquet like person stored as parquet</strong></span>
</pre></div></li><li><p>Insert data in the <code class="literal">person_parquet</code> table by importing it from the <code class="literal">person</code> table:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>hive&gt; insert overwrite table person_parquet select * from person;</strong></span>
</pre></div></li></ol></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip02"></a>Tip</h3><p>Sometimes, data imported from other sources, such as Impala, saves string as binary. To convert it to string while reading, set the following property in <code class="literal">SparkConf</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; sqlContext.setConf("spark.sql.parquet.binaryAsString","true")</strong></span>
</pre></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec53"></a>There's more…</h3></div></div></div><p>If you <a id="id243" class="indexterm"></a>are using Spark 1.4 or later, there is a new interface <a id="id244" class="indexterm"></a>both to write to and read from Parquet. To write the<a id="id245" class="indexterm"></a> data to Parquet (step 11 rewritten), let's save<a id="id246" class="indexterm"></a> this <code class="literal">sixtyPlus</code> RDD to the Parquet format (RDD implicitly gets converted to DataFrame):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt;sixtyPlus.write.parquet("hdfs://localhost:9000/user/hduser/sp.parquet")</strong></span>
</pre></div><p>To read from Parquet (step 13 rewritten; the result is DataFrame), load the contents of the Parquet files in the Spark shell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt;val parquetDF = sqlContext.read.parquet("hdfs://localhost:9000/user/hduser/sp.parquet")</strong></span>
</pre></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec36"></a>Loading and saving data using the JSON format</h2></div></div><hr /></div><p>JSON is <a id="id247" class="indexterm"></a>a lightweight data-interchange format. It is based on a <a id="id248" class="indexterm"></a>subset of the JavaScript programming language. JSON's popularity is directly related to XML getting unpopular. XML was a great solution <a id="id249" class="indexterm"></a>to provide a structure to the data in a plain text<a id="id250" class="indexterm"></a> format. With time, XML documents became more and more heavy and the overhead was not worth it.</p><p>JSON solved this problem by providing structure with minimal overhead. Some people call JSON <span class="strong"><strong>fat-free XML</strong></span>.</p><p>The JSON <a id="id251" class="indexterm"></a>syntax follows these rules:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Data is in the form of key-value pairs:</p><div class="informalexample"><pre class="programlisting">"firstName" : "Bill"</pre></div></li><li style="list-style-type: disc"><p>There are four datatypes in JSON:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>String ("firstName" : "Barack")</p></li><li style="list-style-type: disc"><p>Number ("age" : 53)</p></li><li style="list-style-type: disc"><p>Boolean ("alive": true)</p></li><li style="list-style-type: disc"><p>null ("manager" : null)</p></li></ul></div></li><li style="list-style-type: disc"><p>Data is delimited by commas</p></li><li style="list-style-type: disc"><p>Curly braces {} represents an object:</p><div class="informalexample"><pre class="programlisting">{ "firstName" : "Bill", "lastName": "Clinton", "age": 68 }</pre></div></li><li style="list-style-type: disc"><p>Square brackets [] represent an array:</p><div class="informalexample"><pre class="programlisting">[{ "firstName" : "Bill", "lastName": "Clinton", "age": 68 },{"firstName": "Barack","lastName": "Obama", "age": 43}]</pre></div></li></ul></div><p>In this recipe, we will explore how to save and load it in the JSON format.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec54"></a>How to do it...</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Open<a id="id252" class="indexterm"></a> the terminal and create the <code class="literal">person</code> data<a id="id253" class="indexterm"></a> in the JSON format:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ mkdir jsondata</strong></span>
<span class="strong"><strong>$ vi jsondata/person.json</strong></span>
<span class="strong"><strong>{"first_name" : "Barack", "last_name" : "Obama", "age" : 53}</strong></span>
<span class="strong"><strong>{"first_name" : "George", "last_name" : "Bush", "age" : 68 }</strong></span>
<span class="strong"><strong>{"first_name" : "Bill", "last_name" : "Clinton", "age" : 68 }</strong></span>
</pre></div></li><li><p>Upload<a id="id254" class="indexterm"></a> the <code class="literal">jsondata</code> directory to HDFS:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hdfs dfs -put jsondata /user/hduser/jsondata</strong></span>
</pre></div></li><li><p>Start the <a id="id255" class="indexterm"></a>Spark shell and give it some extra memory:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell --driver-memory 1G</strong></span>
</pre></div></li><li><p>Create an instance of <code class="literal">SQLContext</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val sqlContext = new org.apache.spark.sql.SQLContext(sc)</strong></span>
</pre></div></li><li><p>Import for the implicit conversion:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import sqlContext.implicits._</strong></span>
</pre></div></li><li><p>Load the <code class="literal">jsondata</code> directory from HDFS:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val person = sqlContext.jsonFile("hdfs://localhost:9000/user/hduser/jsondata")</strong></span>
</pre></div></li><li><p>Register the <code class="literal">person</code> DF as a <code class="literal">temp</code> table so that the SQL queries can be run against it:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; person.registerTempTable("person")</strong></span>
</pre></div></li><li><p>Select all the persons with age over 60 years:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val sixtyPlus = sql("select * from person where age &gt; 60")</strong></span>
</pre></div></li><li><p>Print values:</p><div class="informalexample"><pre class="programlisting">scala&gt; sixtyPlus.collect.foreach(println)</pre></div></li><li><p>Let's save this <code class="literal">sixtyPlus</code> DF in the JSON format</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; sixtyPlus.toJSON.saveAsTextFile("hdfs://localhost:9000/user/hduser/sp")</strong></span>
</pre></div></li><li><p>Last step created a directory called <code class="literal">sp</code> in the HDFS root. You can run the <code class="literal">hdfs dfs -ls</code> command in another shell to make sure it's created:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hdfs dfs -ls sp</strong></span>
</pre></div></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec55"></a>How it works…</h3></div></div></div><p>The <code class="literal">sc.jsonFile</code> internally uses <code class="literal">TextInputFormat</code>, which processes one line at a time. Therefore, one JSON record cannot be on multiple lines. It would be a valid JSON format if <a id="id256" class="indexterm"></a>you use multiple lines, but it will not work with <a id="id257" class="indexterm"></a>Spark and will throw an exception.</p><p>It is allowed<a id="id258" class="indexterm"></a> to have more than one object in a line. For example, you<a id="id259" class="indexterm"></a> can have the information of two persons in one line as an array, as follows:</p><div class="informalexample"><pre class="programlisting">[{"firstName":"Barack", "lastName":"Obama"},{"firstName":"Bill", "lastName":"Clinton"}]</pre></div><p>This recipe concludes saving and loading data in the JSON format using Spark.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec56"></a>There's more…</h3></div></div></div><p>If you are using Spark Version 1.4 or later, <code class="literal">SqlContext</code> provides an easier interface to load the <code class="literal">jsondata</code> directory from HDFS:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val person = sqlContext.read.json ("hdfs://localhost:9000/user/hduser/jsondata")</strong></span>
</pre></div><p>The <code class="literal">sqlContext.jsonFile</code> is deprecated in version 1.4, and <code class="literal">sqlContext.read.json</code> is the recommend approach.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec37"></a>Loading and saving data from relational databases</h2></div></div><hr /></div><p>In the<a id="id260" class="indexterm"></a> previous chapter, we learned how to load data<a id="id261" class="indexterm"></a> from a relational data into an RDD <a id="id262" class="indexterm"></a>using JdbcRDD. Spark 1.4 has support to load data <a id="id263" class="indexterm"></a>directly into Dataframe from a JDBC resource. This recipe will explore how to do it.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec57"></a>Getting ready</h3></div></div></div><p>Please make sure that JDBC driver JAR is visible on the client node and all the slaves nodes on which executor will run.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec58"></a>How to do it...</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Create a<a id="id264" class="indexterm"></a> table named <code class="literal">person</code> in MySQL <a id="id265" class="indexterm"></a>using the following DDL:</p><div class="informalexample"><pre class="programlisting">CREATE TABLE 'person' (
  'person_id' int(11) NOT NULL AUTO_INCREMENT,
  'first_name' varchar(30) DEFAULT NULL,
  'last_name' varchar(30) DEFAULT NULL,
  'gender' char(1) DEFAULT NULL,
  'age' tinyint(4) DEFAULT NULL,
  PRIMARY KEY ('person_id')
)</pre></div></li><li><p>Insert <a id="id266" class="indexterm"></a>some data:</p><div class="informalexample"><pre class="programlisting">Insert into person values('Barack','Obama','M',53);
Insert into person values('Bill','Clinton','M',71);
Insert into person values('Hillary','Clinton','F',68);
Insert into person values('Bill','Gates','M',69);
Insert into person values('Michelle','Obama','F',51);</pre></div></li><li><p>Download <code class="literal">mysql-connector-java-x.x.xx-bin.jar</code> from <a class="ulink" href="http://dev.mysql.com/downloads/connector/j/" target="_blank">http://dev.mysql.com/downloads/connector/j/</a>.</p></li><li><p>Make <a id="id267" class="indexterm"></a>MySQL driver available to the Spark shell and launch it:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell --driver-class-path/path-to-mysql-jar/mysql-connector-java-5.1.34-bin.jar</strong></span>
</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note12"></a>Note</h3><p>Please note that <code class="literal">path-to-mysql-jar</code> is not the actual path name. You need to use your path name.</p></div></li><li><p>Construct <a id="id268" class="indexterm"></a>a JDBC URL:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val url="jdbc:mysql://localhost:3306/hadoopdb"</strong></span>
</pre></div></li><li><p>Create a connection properties object with username and password:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val prop = new java.util.Properties</strong></span>
<span class="strong"><strong>scala&gt; prop.setProperty("user","hduser")</strong></span>
<span class="strong"><strong>scala&gt; prop.setProperty("password","********")</strong></span>
</pre></div></li><li><p>Load DataFrame with JDBC data source (url, table name, properties):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>     scala&gt; val people = sqlContext.read.jdbc(url,"person",prop)</strong></span>
</pre></div></li><li><p>Show the results in a nice tabular format by executing the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; people.show</strong></span>
</pre></div></li><li><p>This has loaded the whole table. What if I only would like to load males (url, table name, predicates, properties)? To do this, run the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val males = sqlContext.read.jdbc(url,"person",Array("gender='M'"),prop)</strong></span>
<span class="strong"><strong>scala&gt; males.show</strong></span>
</pre></div></li><li><p>Show only first names by executing the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val first_names = people.select("first_name")</strong></span>
<span class="strong"><strong>scala&gt; first_names.show</strong></span>
</pre></div></li><li><p>Show only people below age 60 by executing the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val below60 = people.filter(people("age") &lt; 60)</strong></span>
<span class="strong"><strong>scala&gt; below60.show</strong></span>
</pre></div></li><li><p>Group <a id="id269" class="indexterm"></a>people by gender as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val grouped = people.groupBy("gender")</strong></span>
</pre></div></li><li><p>Find the<a id="id270" class="indexterm"></a> number of males and females by<a id="id271" class="indexterm"></a> executing the following <a id="id272" class="indexterm"></a>command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val gender_count = grouped.count</strong></span>
<span class="strong"><strong>scala&gt; gender_count.show</strong></span>
</pre></div></li><li><p>Find the average age of males and females by executing the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val avg_age = grouped.avg("age")</strong></span>
<span class="strong"><strong>scala&gt; avg_age.show</strong></span>
</pre></div></li><li><p>Now if you'd like to save this <code class="literal">avg_age</code> data to a new table, run the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; gender_count.write.jdbc(url,"gender_count",prop)</strong></span>
</pre></div></li><li><p>Save the people DataFrame in the Parquet format:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; people.write.parquet("people.parquet")</strong></span>
</pre></div></li><li><p>Save the people DataFrame in the JSON format:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; people.write.json("people.json")</strong></span>
</pre></div></li></ol></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec38"></a>Loading and saving data from an arbitrary source</h2></div></div><hr /></div><p>So far, we<a id="id273" class="indexterm"></a> have covered three data sources that are inbuilt <a id="id274" class="indexterm"></a>with DataFrames—<code class="literal">parquet</code> (default), <code class="literal">json</code>, and <code class="literal">jdbc</code>. Dataframes<a id="id275" class="indexterm"></a> are not limited to these three<a id="id276" class="indexterm"></a> and can load and save to any arbitrary data source by specifying the format manually.</p><p>In this recipe, we will cover loading and saving data from arbitrary sources.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec59"></a>How to do it...</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Start the Spark shell and give it some extra memory:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell --driver-memory 1G</strong></span>
</pre></div></li><li><p>Load <a id="id277" class="indexterm"></a>the data from Parquet; since <code class="literal">parquet</code> is<a id="id278" class="indexterm"></a> the default data source, you do not have to specify it:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val people = sqlContext.read.load("hdfs://localhost:9000/user/hduser/people.parquet") </strong></span>
</pre></div></li><li><p>Load the <a id="id279" class="indexterm"></a>data from Parquet by manually <a id="id280" class="indexterm"></a>specifying the format:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val people = sqlContext.read.format("org.apache.spark.sql.parquet").load("hdfs://localhost:9000/user/hduser/people.parquet") </strong></span>
</pre></div></li><li><p>For inbuilt datatypes (<code class="literal">parquet</code>,<code class="literal">json</code>, and <code class="literal">jdbc</code>), you do not have to specify the full format name, only specifying <code class="literal">"parquet"</code>, <code class="literal">"json"</code>, or <code class="literal">"jdbc"</code> works:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val people = sqlContext.read.format("parquet").load("hdfs://localhost:9000/user/hduser/people.parquet") </strong></span>
</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note13"></a>Note</h3><p>When writing data, there are four save modes: <code class="literal">append</code>, <code class="literal">overwrite</code>, <code class="literal">errorIfExists</code>, and <code class="literal">ignore</code>. The <code class="literal">append</code> mode adds data to data source, <code class="literal">overwrite</code> overwrites it, <code class="literal">errorIfExists</code> throws an exception that data already exists, and <code class="literal">ignore</code> does nothing when data already exists.</p></div></li><li><p>Save people as JSON in the <code class="literal">append</code> mode:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val people = people.write.format("json").mode("append").save ("hdfs://localhost:9000/user/hduser/people.json") </strong></span>
</pre></div></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec60"></a>There's more…</h3></div></div></div><p>The <a id="id281" class="indexterm"></a>Spark SQL's data source API saves to a variety of data sources. To find more information, visit <a class="ulink" href="http://spark-packages.org/" target="_blank">http://spark-packages.org/</a>.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch05"></a>Chapter 5. Spark Streaming</h2></div></div></div><p>Spark Streaming adds the holy grail of big data processing—that is, real-time analytics—to Apache Spark. It enables Spark to ingest live data streams and provides real-time intelligence at a very low latency of a few seconds.</p><p>In this chapter, we'll cover the following recipes:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Word count using Streaming</p></li><li style="list-style-type: disc"><p>Streaming Twitter data</p></li><li style="list-style-type: disc"><p>Streaming using Kafka</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec39"></a>Introduction</h2></div></div><hr /></div><p>Streaming is the <a id="id282" class="indexterm"></a>process of dividing continuously flowing input data into discreet units so that it can be processed easily. Familiar examples in real life are streaming video and audio content (though a user can download the full movie before he/she can watch it, a faster solution is to stream data in small chunks that start playing for the user while the rest of the data is being downloaded in the background).</p><p>Real-world examples of streaming, besides multimedia, are the processing of market feeds, weather data, electronic stock trading data, and so on. All of these applications produce large volumes of data at very fast rates and require special handling of the data so that insights can be derived from data in real time.</p><p>Streaming has a few basic concepts, which are better to understand before we focus on Spark Streaming. The rate at which a streaming application receives data is called <span class="strong"><strong>data rate</strong></span> and <a id="id283" class="indexterm"></a>is expressed<a id="id284" class="indexterm"></a> in the form of <span class="strong"><strong>kilobytes per second</strong></span> (<span class="strong"><strong>kbps</strong></span>) <a id="id285" class="indexterm"></a>or <span class="strong"><strong>megabytes per second</strong></span> (<span class="strong"><strong>mbps</strong></span>).</p><p>One important <a id="id286" class="indexterm"></a>use case of streaming is <span class="strong"><strong>complex event processing</strong></span> (<span class="strong"><strong>CEP</strong></span>). In CEP, it is important to control the scope of the data being processed. This scope is called window, which can be either based on time or size. An example of a time-based window is to analyze data that has come in last one minute. An example of a size-based window can be the average ask price of the last 100 trades of a given stock.</p><p>Spark Streaming is<a id="id287" class="indexterm"></a> Spark's library that provides support to process live data. This stream can come from any source, such as Twitter, Kafka, or Flume.</p><p>Spark Streaming has a few fundamental building blocks that we need to understand well before diving into the recipes.</p><p>Spark Streaming has a context wrapper called <code class="literal">StreamingContext</code>, which wraps around <code class="literal">SparkContext</code> and is the entry point to the Spark Streaming functionality. Streaming data, by definition, is continuous and needs to be time-sliced to process. This slice of time is called<a id="id288" class="indexterm"></a> the <span class="strong"><strong>batch interval</strong></span>, which is specified when <code class="literal">StreamingContext</code> is created. There is one-to-one mapping of RDD and batch, that is, each batch results in one RDD. As you can see in the following image, Spark Streaming takes continuous data, break it into batches and feed to Spark.</p><div class="mediaobject"><img src="graphics/3056_05_01.jpg" /></div><p>Batch interval is important to optimize your streaming application. Ideally, you want to process data at least as fast as it is getting ingested; otherwise, your application will develop a backlog. Spark Streaming collects data for the duration of a batch interval, say, 2 seconds. The moment this 2 second interval is over, data collected in that interval will be given to Spark for processing and Streaming will focus on collecting data for the next batch interval. Now, this 2 second batch interval is all Spark has to process data, as it should be free to receive data from the next batch. If Spark can process the data faster, you can reduce the batch interval to, say, 1 second. If Spark is not able to keep up with this speed, you have to increase the batch interval.</p><p>The continuous stream of RDDs in Spark Streaming needs to be represented in the form of an abstraction through which it can be processed. This abstraction is called <span class="strong"><strong>Discretized Stream</strong></span> (<span class="strong"><strong>DStream</strong></span>). Any operation applied on DStream results in an operation on <a id="id289" class="indexterm"></a>underlying RDDs.</p><p>Every input DStream is <a id="id290" class="indexterm"></a>associated with a receiver (except for file stream). A receiver receives data from the input source and stores it in Spark's memory. There are two types of streaming sources:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Basic sources, such as file and socket connections</p></li><li style="list-style-type: disc"><p>Advanced sources, such as Kafka and Flume</p></li></ul></div><p>Spark Streaming also provides windowed computations in which you can apply the transformation over a sliding window of data. A sliding window operation is based on two parameters:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><span class="strong"><strong>Window length</strong></span>: This<a id="id291" class="indexterm"></a> is the duration of the window. For example, if you want to get analytics of the last 1 minute of data, the window length will be 1 minute.</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Sliding interval</strong></span>: This depicts how frequently you want to perform an operation. Say you <a id="id292" class="indexterm"></a>want to perform the operation every 10 seconds; this means that every 10 seconds, 1 minute of window will have 50 seconds of data common with the last window and 10 seconds of the new data.</p></li></ul></div><p>Both these parameters work on underlying RDDs that, obviously, cannot be broken apart; therefore, both of these should be a multiple of the batch interval. The window length has to be a multiple of the sliding interval as well.</p><p>DStream also has output operations, which allow data to be pushed to external systems. They are similar to actions on RDDs (one higher level of abstraction of what you do at DStream happens to RDDs).</p><p>Besides print to print content of DStream, standard RDD actions, such as <code class="literal">saveAsTextFile</code>, <code class="literal">saveAsObjectFile</code>, and <code class="literal">saveAsHadoopFile</code>, are supported by similar counterparts, such as <code class="literal">saveAsTextFiles</code>, <code class="literal">saveAsObjectFiles</code>, and <code class="literal">saveAsHadoopFiles</code>, respectively.</p><p>One very useful output operation is <code class="literal">foreachRDD(func)</code>, which applies any arbitrary function to all the RDDs.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec40"></a>Word count using Streaming</h2></div></div><hr /></div><p>Let's start with a simple example of Streaming in which in one terminal, we will type some text <a id="id293" class="indexterm"></a>and the Streaming application will capture it in<a id="id294" class="indexterm"></a> another window.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec61"></a>How to do it...</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Start the Spark shell and give it some extra memory:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell --driver-memory 1G</strong></span>
</pre></div></li><li><p>Stream specific imports:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.SparkConf</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.streaming.{Seconds, StreamingContext}</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.storage.StorageLevel</strong></span>
<span class="strong"><strong>scala&gt; import StorageLevel._</strong></span>
</pre></div></li><li><p>Import for an implicit conversion:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark._</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.streaming._</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.streaming.StreamingContext._</strong></span>
</pre></div></li><li><p>Create <code class="literal">StreamingContext</code> with a 2 second batch interval:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val ssc = new StreamingContext(sc, Seconds(2))</strong></span>
</pre></div></li><li><p>Create a <code class="literal">SocketTextStream</code> Dstream on localhost with port <code class="literal">8585</code> with the <code class="literal">MEMORY_ONLY</code> caching:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val lines = ssc.socketTextStream("localhost",8585,MEMORY_ONLY)</strong></span>
</pre></div></li><li><p>Divide the lines into multiple words:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val wordsFlatMap = lines.flatMap(_.split(" "))</strong></span>
</pre></div></li><li><p>Convert word to (word,1), that is, output <code class="literal">1</code> as the value for each occurrence of a word as the key:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val wordsMap = wordsFlatMap.map( w =&gt; (w,1))</strong></span>
</pre></div></li><li><p>Use the <code class="literal">reduceByKey</code> method to add a number of occurrences for each word as the key (the function works on two consecutive values at a time, represented by <code class="literal">a</code> and <code class="literal">b</code>):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val wordCount = wordsMap.reduceByKey( (a,b) =&gt; (a+b))</strong></span>
</pre></div></li><li><p>Print <code class="literal">wordCount</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; wordCount.print</strong></span>
</pre></div></li><li><p>Start <code class="literal">StreamingContext</code>; remember, nothing happens until <code class="literal">StreamingContext</code> is started:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; ssc.start</strong></span>
</pre></div></li><li><p>Now, in a <a id="id295" class="indexterm"></a>separate window, start the netcat server:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ nc -lk 8585</strong></span>
</pre></div></li><li><p>Enter <a id="id296" class="indexterm"></a>different lines, such as <code class="literal">to be or not to be</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>to be or not to be</strong></span>
</pre></div></li><li><p>Check the Spark shell, and you will see word count results like the following screenshot:</p><div class="mediaobject"><img src="graphics/3056_05_02.jpg" /></div></li></ol></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec41"></a>Streaming Twitter data</h2></div></div><hr /></div><p>Twitter is a famous microblogging platform. It produces a massive amount of data with around 500 million tweets sent each day. Twitter allows its data to be accessed by APIs and that makes it the poster child of testing any big data streaming application.</p><p>In this recipe, we <a id="id297" class="indexterm"></a>will see how we can live stream data in Spark using Twitter streaming libraries. Twitter is just one source of providing the streaming data to Spark and has no special status. Therefore, there are no built-in libraries for Twitter. Spark does provide some APIs to facilitate integration with Twitter libraries, though.</p><p>An example use of live Twitter data feed can be to find trending tweets in the last 5 minutes.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec62"></a>How to do it...</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Create a <a id="id298" class="indexterm"></a>Twitter account if you do not already have one.</p></li><li><p>Go to <a class="ulink" href="http://apps.twitter.com" target="_blank">http://apps.twitter.com</a>.</p></li><li><p>Click on <span class="strong"><strong>Create New App</strong></span>.</p></li><li><p>Enter <span class="strong"><strong>Name</strong></span>, <span class="strong"><strong>Description</strong></span>, <span class="strong"><strong>Website</strong></span>, and <span class="strong"><strong>Callback URL</strong></span>, and then click on <span class="strong"><strong>Create your Twitter Application</strong></span>.</p><div class="mediaobject"><img src="graphics/3056_05_03.jpg" /></div></li><li><p>You will <a id="id299" class="indexterm"></a>reach <span class="strong"><strong>Application Management</strong></span> screen.</p></li><li><p>Navigate to <span class="strong"><strong>Keys and Access Tokens</strong></span> | <span class="strong"><strong>Create my access Token</strong></span>.</p><div class="mediaobject"><img src="graphics/3056_05_04.jpg" /></div></li><li><p>Note down the four values in this screen that we will use in step 14:</p><p><span class="strong"><strong>Consumer Key (API Key)</strong></span></p><p><span class="strong"><strong>Consumer Secret (API Secret)</strong></span></p><p><span class="strong"><strong>Access Token</strong></span></p><p><span class="strong"><strong>Access Token Secret</strong></span></p></li><li><p>We will need to provide the values in this screen in some time, but, for now, let's download the third-party libraries needed from Maven central:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ wget http://central.maven.org/maven2/org/apache/spark/spark-streaming-twitter_2.10/1.2.0/spark-streaming-twitter_2.10-1.2.0.jar</strong></span>
<span class="strong"><strong>$ wget http://central.maven.org/maven2/org/twitter4j/twitter4j-stream/4.0.2/twitter4j-stream-4.0.2.jar</strong></span>
<span class="strong"><strong>$ wget http://central.maven.org/maven2/org/twitter4j/twitter4j-core/4.0.2/twitter4j-core-4.0.2.jar</strong></span>
</pre></div></li><li><p>Open the <a id="id300" class="indexterm"></a>Spark shell, supplying the preceding three JARS as the dependency:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell --jars spark-streaming-twitter_2.10-1.2.0.jar, twitter4j-stream-4.0.2.jar,twitter4j-core-4.0.2.jar</strong></span>
</pre></div></li><li><p>Perform imports that are Twitter-specific:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.streaming.twitter._</strong></span>
<span class="strong"><strong>scala&gt; import twitter4j.auth._</strong></span>
<span class="strong"><strong>scala&gt; import twitter4j.conf._</strong></span>
</pre></div></li><li><p>Stream specific imports:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.streaming.{Seconds, StreamingContext}</strong></span>
</pre></div></li><li><p>Import for an implicit conversion:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark._</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.streaming._</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.streaming.StreamingContext._</strong></span>
</pre></div></li><li><p>Create <code class="literal">StreamingContext</code> with a 10 second batch interval:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val ssc = new StreamingContext(sc, Seconds(10))</strong></span>
</pre></div></li><li><p>Create <code class="literal">StreamingContext</code> with a 2 second batch interval:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val cb = new ConfigurationBuilder</strong></span>
<span class="strong"><strong>scala&gt; cb.setDebugEnabled(true)</strong></span>
<span class="strong"><strong>.setOAuthConsumerKey("FKNryYEKeCrKzGV7zuZW4EKeN")</strong></span>
<span class="strong"><strong>.setOAuthConsumerSecret("x6Y0zcTBOwVxpvekSCnGzbi3NYNrM5b8ZMZRIPI1XRC3pDyOs1")</strong></span>
<span class="strong"><strong>  .setOAuthAccessToken("31548859-DHbESdk6YoghCLcfhMF88QEFDvEjxbM6Q90eoZTGl")</strong></span>
<span class="strong"><strong>.setOAuthAccessTokenSecret("wjcWPvtejZSbp9cgLejUdd6W1MJqFzm5lByUFZl1NYgrV")</strong></span>
<span class="strong"><strong>val auth = new OAuthAuthorization(cb.build)</strong></span>
</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note14"></a>Note</h3><p>These are sample values and you should put your own values.</p></div></li><li><p>Create Twitter DStream:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val tweets = TwitterUtils.createStream(ssc,auth)</strong></span>
</pre></div></li><li><p>Filter out English tweets:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val englishTweets = tweets.filter(_.getLang()=="en")</strong></span>
</pre></div></li><li><p>Get text out of the tweets:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val status = englishTweets.map(status =&gt; status.getText)</strong></span>
</pre></div></li><li><p>Set the <a id="id301" class="indexterm"></a>checkpoint directory:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; ssc.checkpoint("hdfs://localhost:9000/user/hduser/checkpoint")</strong></span>
</pre></div></li><li><p>Start <code class="literal">StreamingContext</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; ssc.start</strong></span>
<span class="strong"><strong>scala&gt; ssc.awaitTermination</strong></span>
</pre></div></li><li><p>You can put all these commands together using <code class="literal">:paste</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; :paste</strong></span>
<span class="strong"><strong>import org.apache.spark.streaming.twitter._</strong></span>
<span class="strong"><strong>import twitter4j.auth._</strong></span>
<span class="strong"><strong>import twitter4j.conf._</strong></span>
<span class="strong"><strong>import org.apache.spark.streaming.{Seconds, StreamingContext}</strong></span>
<span class="strong"><strong>import org.apache.spark._</strong></span>
<span class="strong"><strong>import org.apache.spark.streaming._</strong></span>
<span class="strong"><strong>import org.apache.spark.streaming.StreamingContext._</strong></span>
<span class="strong"><strong>val ssc = new StreamingContext(sc, Seconds(10))</strong></span>
<span class="strong"><strong>val cb = new ConfigurationBuilder</strong></span>
<span class="strong"><strong>cb.setDebugEnabled(true).setOAuthConsumerKey("FKNryYEKeCrKzGV7zuZW4EKeN")</strong></span>
<span class="strong"><strong>  .setOAuthConsumerSecret("x6Y0zcTBOwVxpvekSCnGzbi3NYNrM5b8ZMZRIPI1XRC3pDyOs1")</strong></span>
<span class="strong"><strong>  .setOAuthAccessToken("31548859-DHbESdk6YoghCLcfhMF88QEFDvEjxbM6Q90eoZTGl")</strong></span>
<span class="strong"><strong>  .setOAuthAccessTokenSecret("wjcWPvtejZSbp9cgLejUdd6W1MJqFzm5lByUFZl1NYgrV")</strong></span>
<span class="strong"><strong>val auth = new OAuthAuthorization(cb.build)</strong></span>
<span class="strong"><strong>val tweets = TwitterUtils.createStream(ssc,Some(auth))</strong></span>
<span class="strong"><strong>val englishTweets = tweets.filter(_.getLang()=="en")</strong></span>
<span class="strong"><strong>val status = englishTweets.map(status =&gt; status.getText)</strong></span>
<span class="strong"><strong>status.print</strong></span>
<span class="strong"><strong>ssc.checkpoint("hdfs://localhost:9000/checkpoint")</strong></span>
<span class="strong"><strong>ssc.start</strong></span>
<span class="strong"><strong>ssc.awaitTermination</strong></span>
</pre></div></li></ol></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec42"></a>Streaming using Kafka</h2></div></div><hr /></div><p>Kafka is a distributed, partitioned, and replicated commit log service. In simple words, it is a distributed<a id="id302" class="indexterm"></a> messaging server. Kafka maintains the message feed in categories <a id="id303" class="indexterm"></a>called <span class="strong"><strong>topics</strong></span>. An example of the topic can be a ticker symbol of a <a id="id304" class="indexterm"></a>company you would like to get news about, for example, CSCO for Cisco.</p><p>Processes that produce<a id="id305" class="indexterm"></a> messages are called <span class="strong"><strong>producers</strong></span> and those that consume messages<a id="id306" class="indexterm"></a> are called <span class="strong"><strong>consumers</strong></span>. In traditional messaging, the messaging service has one central messaging server, also called <a id="id307" class="indexterm"></a>
<span class="strong"><strong>broker</strong></span>. Since Kafka is a distributed messaging service, it has a cluster of brokers, which functionally act as one Kafka broker, as shown here:</p><div class="mediaobject"><img src="graphics/B03056_05_06.jpg" /></div><p>For each topic, Kafka<a id="id308" class="indexterm"></a> maintains the partitioned log. This partitioned log consists of one or more partitions spread across the cluster, as shown in the following figure:</p><div class="mediaobject"><img src="graphics/B03056_05_07.jpg" /></div><p>Kafka borrows a lot <a id="id309" class="indexterm"></a>of concepts from Hadoop and other big data frameworks. The concept of partition is very similar to the concept of <code class="literal">InputSplit</code> in Hadoop. In the simplest form, while using <code class="literal">TextInputFormat</code>, an <code class="literal">InputSplit</code> is same as a block. A block is read in the form of a key-value pair in <code class="literal">TextInputFormat</code>, where the key is the byte offset of a line and the value is content of the line itself. In a similar way, in a Kafka partition, records are stored and retrieved in the form of key-value pairs, where the key is a sequential ID number called the offset and the value is the actual<a id="id310" class="indexterm"></a> message.</p><p>In Kafka, message retention does not depend on the consumption by a consumer. Messages are retained for a configurable period of time. Each consumer is free to read messages in any order they like. All it needs to retain is an offset. Another analogy can be reading a book in which the page number is analogous to the offset, while the page content is analogous to the message. The reader is free to read whichever way he/she wants as long as they remember the bookmark (the current offset).</p><p>To provide functionality similar to pub/sub and PTP (queues) in traditional messaging systems, Kafka has the concept of consumer groups. A consumer group is a group of consumers, which the Kafka cluster treats as a single unit. In a consumer group, only one consumer needs to receive a message. If consumer C1, in the following diagram, receives the first message for topic T1, all the following messages on that topic will also be delivered to this consumer. Using this strategy, Kafka guarantees the order of message delivery for a given topic.</p><p>In extreme cases, when all consumers are in one consumer group, the Kafka cluster acts like PTP/queue. In another extreme case, if every consumer belongs to a different group, it acts like pub/sub. In practice, each consumer group has a limited number of consumers.</p><div class="mediaobject"><img src="graphics/B03056_05_08.jpg" /></div><p>This recipe<a id="id311" class="indexterm"></a> will show you how to perform a word count using data <a id="id312" class="indexterm"></a>coming from Kafka.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec63"></a>Getting ready</h3></div></div></div><p>This recipe assumes Kafka is already installed. Kafka comes with ZooKeeper bundled. We are assuming Kafka's home is in <code class="literal">/opt/infoobjects/kafka</code>:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Start ZooKeeper:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ /opt/infoobjects/kafka/bin/zookeeper-server-start.sh /opt/infoobjects/kafka/config/zookeeper.properties</strong></span>
</pre></div></li><li><p>Start the Kafka server:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ /opt/infoobjects/kafka/bin/kafka-server-start.sh /opt/infoobjects/kafka/config/server.properties</strong></span>
</pre></div></li><li><p>Create a <code class="literal">test</code> topic:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ /opt/infoobjects/kafka/bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test</strong></span>
</pre></div></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec64"></a>How to do it...</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Download the <code class="literal">spark-streaming-kafka</code> library and its dependencies:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ wget http://central.maven.org/maven2/org/apache/spark/spark-streaming-kafka_2.10/1.2.0/spark-streaming-kafka_2.10-1.2.0.jar</strong></span>
<span class="strong"><strong>$ wget http://central.maven.org/maven2/org/apache/kafka/kafka_2.10/0.8.1/kafka_2.10-0.8.1.jar</strong></span>
<span class="strong"><strong>$ wget http://central.maven.org/maven2/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar</strong></span>
<span class="strong"><strong>$ wget http://central.maven.org/maven2/com/101tec/zkclient/0.4/zkclient-0.4.jar</strong></span>
</pre></div></li><li><p>Start the Spark shell and provide the <code class="literal">spark-streaming-kafka</code> library:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell --jars spark-streaming-kafka_2.10-1.2.0.jar, kafka_2.10-0.8.1.jar,metrics-core-2.2.0.jar,zkclient-0.4.jar</strong></span>
</pre></div></li><li><p>Stream<a id="id313" class="indexterm"></a> specific imports:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.streaming.{Seconds, StreamingContext}</strong></span>
</pre></div></li><li><p>Import<a id="id314" class="indexterm"></a> for implicit conversion:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark._</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.streaming._</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.streaming.StreamingContext._</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.streaming.kafka.KafkaUtils</strong></span>
</pre></div></li><li><p>Create <code class="literal">StreamingContext</code> with a 2 second batch interval:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val ssc = new StreamingContext(sc, Seconds(2))</strong></span>
</pre></div></li><li><p>Set Kafka-specific variables:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val zkQuorum = "localhost:2181"</strong></span>
<span class="strong"><strong>scala&gt; val group = "test-group"</strong></span>
<span class="strong"><strong>scala&gt; val topics = "test"</strong></span>
<span class="strong"><strong>scala&gt; val numThreads = 1</strong></span>
</pre></div></li><li><p>Create <code class="literal">topicMap</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val topicMap = topics.split(",").map((_,numThreads.toInt)).toMap</strong></span>
</pre></div></li><li><p>Create Kafka DStream:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val lineMap = KafkaUtils.createStream(ssc, zkQuorum, group, topicMap)</strong></span>
</pre></div></li><li><p>Pull the value out of lineMap:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val lines = lineMap.map(_._2)</strong></span>
</pre></div></li><li><p>Create <code class="literal">flatMap</code> of values:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val words = lines.flatMap(_.split(" "))</strong></span>
</pre></div></li><li><p>Create the key-value pair of (word,occurrence):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val pair = words.map( x =&gt; (x,1))</strong></span>
</pre></div></li><li><p>Do the word count for a sliding window:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val wordCounts = pair.reduceByKeyAndWindow(_ + _, _ - _, Minutes(10), Seconds(2), 2)</strong></span>
<span class="strong"><strong>scala&gt; wordCounts.print</strong></span>
</pre></div></li><li><p>Set the <code class="literal">checkpoint</code> directory:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; ssc.checkpoint("hdfs://localhost:9000/user/hduser/checkpoint")</strong></span>
</pre></div></li><li><p>Start <code class="literal">StreamingContext</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; ssc.start</strong></span>
<span class="strong"><strong>scala&gt; ssc.awaitTermination</strong></span>
</pre></div></li><li><p>Publish a <a id="id315" class="indexterm"></a>message on the <code class="literal">test</code> topic in Kafka in another <a id="id316" class="indexterm"></a>window:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ /opt/infoobjects/kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test</strong></span>
</pre></div></li><li><p>Now, publish messages on Kafka by pressing <span class="emphasis"><em>Enter</em></span> at step 15 and after every message.</p></li><li><p>Now, as you publish messages on Kafka, you will see them in the Spark shell:</p><div class="mediaobject"><img src="graphics/3056_05_05.jpg" /></div></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec65"></a>There's more…</h3></div></div></div><p>Let's say you want to maintain a running count of the occurrence of each word. Spark Streaming has a feature for this called <code class="literal">updateStateByKey</code> operation. The <code class="literal">updateStateByKey</code> operation allows you to maintain any arbitrary state while updating it with the new information supplied.</p><p>This arbitrary state can be an aggregation value, or just a change in state (like the mood of a user on twitter). Perform the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Let's call <code class="literal">updateStateByKey</code> on pairs RDD:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val runningCounts = wordCounts.updateStateByKey( (values: Seq[Int], state: Option[Int]) =&gt; Some(state.sum + values.sum))</strong></span>
</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note15"></a>Note</h3><p>The <code class="literal">updateStateByKey</code> operation returns a new "state" DStream where the state for each key is updated by applying the given function on the previous state of the key and the new values for the key. This can be used to maintain arbitrary state data for each key.</p><p>There are two steps involved in making this operation work:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Define the state</p></li><li style="list-style-type: disc"><p>Define the state <code class="literal">update</code> function</p></li></ul></div><p>The <code class="literal">updateStateByKey</code> operation is called once for each key, values represent the sequence of values associated with that key, very much like MapReduce and the state can be any arbitrary state, which we chose to make <code class="literal">Option[Int]</code>. With every call in step 18, the previous state gets updated by adding the sum of current values to it.</p></div></li><li><p>Print the<a id="id317" class="indexterm"></a> results:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; runningCounts.print</strong></span>
</pre></div></li><li><p>The following <a id="id318" class="indexterm"></a>are all the steps combined to maintain the arbitrary state using the <code class="literal">updateStateByKey</code> operation:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Scala&gt; :paste</strong></span>
<span class="strong"><strong>import org.apache.spark.streaming.{Seconds, StreamingContext}</strong></span>
<span class="strong"><strong> import org.apache.spark._</strong></span>
<span class="strong"><strong> import org.apache.spark.streaming._</strong></span>
<span class="strong"><strong> import org.apache.spark.streaming.kafka._</strong></span>
<span class="strong"><strong> import org.apache.spark.streaming.StreamingContext._</strong></span>
<span class="strong"><strong> val ssc = new StreamingContext(sc, Seconds(2))</strong></span>
<span class="strong"><strong> val zkQuorum = "localhost:2181"</strong></span>
<span class="strong"><strong> val group = "test-group"</strong></span>
<span class="strong"><strong> val topics = "test"</strong></span>
<span class="strong"><strong> val numThreads = 1</strong></span>
<span class="strong"><strong> val topicMap = topics.split(",").map((_,numThreads.toInt)).toMap</strong></span>
<span class="strong"><strong> val lineMap = KafkaUtils.createStream(ssc, zkQuorum, group, topicMap)</strong></span>
<span class="strong"><strong> val lines = lineMap.map(_._2)</strong></span>
<span class="strong"><strong> val words = lines.flatMap(_.split(" "))</strong></span>
<span class="strong"><strong> val pairs = words.map(x =&gt; (x,1))</strong></span>
<span class="strong"><strong> val runningCounts = pairs.updateStateByKey( (values: Seq[Int], state: Option[Int]) =&gt; Some(state.sum + values.sum))</strong></span>
<span class="strong"><strong> runningCounts.print</strong></span>
<span class="strong"><strong>ssc.checkpoint("hdfs://localhost:9000/user/hduser/checkpoint")</strong></span>
<span class="strong"><strong> ssc.start</strong></span>
<span class="strong"><strong> ssc.awaitTermination</strong></span>
</pre></div></li><li><p>Run it by pressing <span class="emphasis"><em>Ctrl</em></span> + <span class="emphasis"><em>D</em></span> (which executes the code pasted using <code class="literal">:paste</code>).</p></li></ol></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch06"></a>Chapter 6. Getting Started with Machine Learning Using MLlib</h2></div></div></div><p>This chapter is divided into the following recipes:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Creating vectors</p></li><li style="list-style-type: disc"><p>Creating a labeled point</p></li><li style="list-style-type: disc"><p>Creating matrices</p></li><li style="list-style-type: disc"><p>Calculating summary statistics</p></li><li style="list-style-type: disc"><p>Calculating correlation</p></li><li style="list-style-type: disc"><p>Doing hypothesis testing</p></li><li style="list-style-type: disc"><p>Creating machine learning pipelines using ML</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec43"></a>Introduction</h2></div></div><hr /></div><p>The following<a id="id319" class="indexterm"></a> is Wikipedia's definition of machine learning:</p><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>"Machine learning is a scientific discipline that explores the construction and study of algorithms that can learn from data."</em></span></p></blockquote></div><p>Essentially, machine learning is making use of past data to make predictions about the future. Machine learning heavily depends upon statistical analysis and methodology.</p><p>In statistics, there are four types of measurement scales:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Scale type</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Description</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>Nominal Scale</p>
</td><td style="" align="left" valign="top">
<p>=, ≠</p>
<p>Identifies categories</p>
<p>Can't <a id="id320" class="indexterm"></a>be numeric</p>
<p>Example: male, female</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Ordinal Scale</p>
</td><td style="" align="left" valign="top">
<p>=, ≠, &lt;, &gt;</p>
<p>Nominal scale +</p>
<p>Ranks <a id="id321" class="indexterm"></a>from least important to most important</p>
<p>Example: corporate hierarchy</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Interval Scale</p>
</td><td style="" align="left" valign="top">
<p>=, ≠, &lt;, &gt;, +, -</p>
<p>Ordinal <a id="id322" class="indexterm"></a>scale + distance between observations</p>
<p>Numbers assigned to observations indicate order</p>
<p>Difference between any consecutive values is same as others</p>
<p>60° temperature is not the double of 30°</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Ratio Scale</p>
</td><td style="" align="left" valign="top">
<p>=, ≠, &lt;, &gt;, +, ×, ÷</p>
<p>Interval<a id="id323" class="indexterm"></a> scale +ratios of observations</p>
<p>$20 is twice as costly as $10</p>
</td></tr></tbody></table></div><p>Another distinction that can be made among the data is between the continuous and discrete data. Continuous data can take any value. Most data belonging to the interval and ratio scale is continuous.</p><p>Discrete variables can take on only particular values and there are clear boundaries between the values. For example, a house can have two or three rooms but not 2.75 rooms. Data belonging to nominal and ordinal scale is always discrete.</p><p>MLlib is the Spark's library for machine learning. In this chapter, we will focus on the fundamentals of machine learning.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec44"></a>Creating vectors</h2></div></div><hr /></div><p>Before understanding Vectors, let's focus on what is a point. A point is just a set of numbers. This set<a id="id324" class="indexterm"></a> of numbers or coordinates defines the point's position in space. The numbers of coordinates determine dimensions of the space.</p><p>We can visualize space with up to three dimensions. Space with more than three dimensions is <a id="id325" class="indexterm"></a>called <span class="strong"><strong>hyperspace</strong></span>. Let's put this spatial metaphor to use.</p><p>Let's start with a person. A person has the following dimensions:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Weight</p></li><li style="list-style-type: disc"><p>Height</p></li><li style="list-style-type: disc"><p>Age</p></li></ul></div><p>We are working in three-dimensional space here. Thus, the interpretation of point (160,69,24) would be 160 lb weight, 69 inches height, and 24 years age.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note17"></a>Note</h3><p>Points and vectors are same thing. Dimensions in vectors are called <span class="strong"><strong>features</strong></span>. In another way, we can define a <a id="id326" class="indexterm"></a>feature as an individual measurable property of a phenomenon being observed.</p></div><p>Spark has local vectors and matrices and also distributed matrices. Distributed matrix is backed by one <a id="id327" class="indexterm"></a>or more RDDs. A local vector has numeric indices and double values, and is stored on a single machine.</p><p>There are two types of local vectors in MLlib: dense and sparse. A dense vector is backed by an array of its values, while a sparse vector is backed by two parallel arrays, one for indices and another for values.</p><p>So, person data (160,69,24) will be represented as [160.0,69.0,24.0] using dense vector and as (3,[0,1,2],[160.0,69.0,24.0]) using sparse vector format.</p><p>Whether to make a vector sparse or dense depends upon how many null values or 0s it has. Let's take a case of a vector with 10,000 values with 9,000 of them being 0. If we use dense vector format, it would be a simple structure, but 90 percent of space would be wasted. Sparse vector format would work out better here as it would only keep indices, which are non-zero.</p><p>Sparse data is very common and Spark supports the <code class="literal">libsvm</code> format for it which stores one feature vector per line.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec66"></a>How to do it…</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Start the Spark shell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell</strong></span>
</pre></div></li><li><p>Import the MLlib vector explicitly (not to confuse with other vector classes):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Scala&gt; import org.apache.spark.mllib.linalg.{Vectors,Vector}</strong></span>
</pre></div></li><li><p>Create a dense vector:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val dvPerson = Vectors.dense(160.0,69.0,24.0)</strong></span>
</pre></div></li><li><p>Create a sparse vector:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val svPerson = Vectors.sparse(3,Array(0,1,2),Array(160.0,69.0,24.0))</strong></span>
</pre></div></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec67"></a>How it works...</h3></div></div></div><p>The following<a id="id328" class="indexterm"></a> is the method signature of <code class="literal">vectors.dense</code>:</p><div class="informalexample"><pre class="programlisting">def dense(values: Array[Double]): Vector</pre></div><p>Here, values represent double array of elements in the vector.</p><p>The following is the method signature of <code class="literal">Vectors.sparse</code>:</p><div class="informalexample"><pre class="programlisting">def sparse(size: Int, indices: Array[Int], values: Array[Double]): Vector</pre></div><p>Here, <code class="literal">size</code> represents the size of the vector, <code class="literal">indices</code> is an array of indices, and <code class="literal">values</code> is an array of values as doubles. Do make sure you specify <code class="literal">double</code> as datatype or use decimal in at least one value; otherwise it will throw an exception for the dataset, which has only integer.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec45"></a>Creating a labeled point</h2></div></div><hr /></div><p>Labeled point is <a id="id329" class="indexterm"></a>a local vector (sparse/dense), which has an associated label with it. Labeled data is used in supervised learning to help train algorithms. You<a id="id330" class="indexterm"></a> will get to know more about it in the next chapter.</p><p>Label is stored as a double value in <code class="literal">LabeledPoint</code>. It means that when you have categorical labels, they need to be mapped to double values. What value you assign to a category is immaterial and is only a matter of convenience.</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Type</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Label values</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>Binary classification</p>
</td><td style="" align="left" valign="top">
<p>0 or 1</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Multiclass classification</p>
</td><td style="" align="left" valign="top">
<p>0, 1, 2…</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Regression</p>
</td><td style="" align="left" valign="top">
<p>Decimal values</p>
</td></tr></tbody></table></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec68"></a>How to do it…</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Start the Spark shell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$spark-shell</strong></span>
</pre></div></li><li><p>Import the MLlib vector explicitly:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.linalg.{Vectors,Vector}</strong></span>
</pre></div></li><li><p>Import the <code class="literal">LabeledPoint</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.regression.LabeledPoint</strong></span>
</pre></div></li><li><p>Create a labeled point with a positive label and dense vector:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val willBuySUV = LabeledPoint(1.0,Vectors.dense(300.0,80,40))</strong></span>
</pre></div></li><li><p>Create <a id="id331" class="indexterm"></a>a labeled point with a negative label and dense vector:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val willNotBuySUV = LabeledPoint(0.0,Vectors.dense(150.0,60,25))</strong></span>
</pre></div></li><li><p>Create a labeled point with a positive label and sparse vector:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val willBuySUV = LabeledPoint(1.0,Vectors.sparse(3,Array(0,1,2),Array(300.0,80,40)))</strong></span>
</pre></div></li><li><p>Create a labeled point with a negative label and sparse vector:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val willNotBuySUV = LabeledPoint(0.0,Vectors.sparse(3,Array(0,1,2),Array(150.0,60,25)))</strong></span>
</pre></div></li><li><p>Create a <code class="literal">libsvm</code> file with the same data:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$vi person_libsvm.txt (libsvm indices start with 1)</strong></span>
<span class="strong"><strong>0  1:150 2:60 3:25</strong></span>
<span class="strong"><strong>1  1:300 2:80 3:40</strong></span>
</pre></div></li><li><p>Upload <code class="literal">person_libsvm.txt</code> to <code class="literal">hdfs</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hdfs dfs -put person_libsvm.txt person_libsvm.txt</strong></span>
</pre></div></li><li><p>Do a few more imports:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.util.MLUtils</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.rdd.RDD</strong></span>
</pre></div></li><li><p>Load data from <code class="literal">libsvm</code> file:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val persons = MLUtils.loadLibSVMFile(sc,"person_libsvm.txt")</strong></span>
</pre></div></li></ol></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec46"></a>Creating matrices</h2></div></div><hr /></div><p>Matrix is<a id="id332" class="indexterm"></a> simply a table to represent multiple feature vectors. A matrix that <a id="id333" class="indexterm"></a>can be stored on one machine is called <span class="strong"><strong>local matrix</strong></span> and the <a id="id334" class="indexterm"></a>one<a id="id335" class="indexterm"></a> that can be distributed<a id="id336" class="indexterm"></a> across <a id="id337" class="indexterm"></a>the cluster is called <span class="strong"><strong>distributed matrix</strong></span>.</p><p>Local matrices have integer-based indices, while distributed matrices have long-based indices. Both have values as doubles.</p><p>There are three types of distributed matrices:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><code class="literal">RowMatrix</code>: This has <a id="id338" class="indexterm"></a>each row as a feature vector.</p></li><li style="list-style-type: disc"><p><code class="literal">IndexedRowMatrix</code>: This<a id="id339" class="indexterm"></a> also has row indices.</p></li><li style="list-style-type: disc"><p><code class="literal">CoordinateMatrix</code>: This<a id="id340" class="indexterm"></a> is simply a matrix of <code class="literal">MatrixEntry</code>. A <code class="literal">MatrixEntry</code> represents an entry in the matrix represented by its row and column index.</p></li></ul></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec69"></a>How to do it…</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Start the<a id="id341" class="indexterm"></a> Spark shell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$spark-shell</strong></span>
</pre></div></li><li><p>Import the matrix-related classes:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.linalg.{Vectors,Matrix, Matrices}</strong></span>
</pre></div></li><li><p>Create a dense local matrix:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val people = Matrices.dense(3,2,Array(150d,60d,25d, 300d,80d,40d))</strong></span>
</pre></div></li><li><p>Create a <code class="literal">personRDD</code> as RDD of vectors:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val personRDD = sc.parallelize(List(Vectors.dense(150,60,25), Vectors.dense(300,80,40)))</strong></span>
</pre></div></li><li><p>Import <code class="literal">RowMatrix</code> and related classes:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.linalg.distributed.{IndexedRow, IndexedRowMatrix,RowMatrix, CoordinateMatrix, MatrixEntry}</strong></span>
</pre></div></li><li><p>Create a row matrix of <code class="literal">personRDD</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val personMat = new RowMatrix(personRDD)</strong></span>
</pre></div></li><li><p>Print the number of rows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; print(personMat.numRows)</strong></span>
</pre></div></li><li><p>Print the number of columns:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; print(personMat.numCols)</strong></span>
</pre></div></li><li><p>Create an RDD of indexed rows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val personRDD = sc.parallelize(List(IndexedRow(0L, Vectors.dense(150,60,25)), IndexedRow(1L, Vectors.dense(300,80,40))))</strong></span>
</pre></div></li><li><p>Create an indexed row matrix:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val pirmat = new IndexedRowMatrix(personRDD)</strong></span>
</pre></div></li><li><p>Print the number of rows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; print(pirmat.numRows)</strong></span>
</pre></div></li><li><p>Print the number of columns:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; print(pirmat.numCols)</strong></span>
</pre></div></li><li><p>Convert the indexed row matrix back to row matrix:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val personMat = pirmat.toRowMatrix</strong></span>
</pre></div></li><li><p>Create an RDD of matrix entries:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val meRDD = sc.parallelize(List(</strong></span>
<span class="strong"><strong>  MatrixEntry(0,0,150),</strong></span>
<span class="strong"><strong>  MatrixEntry(1,0,60),</strong></span>
<span class="strong"><strong>MatrixEntry(2,0,25),</strong></span>
<span class="strong"><strong>MatrixEntry(0,1,300),</strong></span>
<span class="strong"><strong>MatrixEntry(1,1,80),</strong></span>
<span class="strong"><strong>MatrixEntry(2,1,40)</strong></span>
<span class="strong"><strong>))</strong></span>
</pre></div></li><li><p>Create a <a id="id342" class="indexterm"></a>coordinate matrix:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val pcmat = new CoordinateMatrix(meRDD)</strong></span>
</pre></div></li><li><p>Print the number of rows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; print(pcmat.numRows)</strong></span>
</pre></div></li><li><p>Print the number of columns:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; print(pcmat.numCols)</strong></span>
</pre></div></li></ol></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec47"></a>Calculating summary statistics</h2></div></div><hr /></div><p>Summary statistics is<a id="id343" class="indexterm"></a> used to summarize observations to get a<a id="id344" class="indexterm"></a> collective sense of the data. The summary includes the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Central tendency of data—mean, mode, median</p></li><li style="list-style-type: disc"><p>Spread of data—variance, standard deviation</p></li><li style="list-style-type: disc"><p>Boundary conditions—min, max</p></li></ul></div><p>This recipe covers how to produce summary statistics.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec70"></a>How to do it…</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Start the Spark shell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell</strong></span>
</pre></div></li><li><p>Import the matrix-related classes:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.linalg.{Vectors,Vector}</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.stat.Statistics</strong></span>
</pre></div></li><li><p>Create a <code class="literal">personRDD</code> as RDD of vectors:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val personRDD = sc.parallelize(List(Vectors.dense(150,60,25), Vectors.dense(300,80,40)))</strong></span>
</pre></div></li><li><p>Compute<a id="id345" class="indexterm"></a> the column summary statistics:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val summary = Statistics.colStats(personRDD)</strong></span>
</pre></div></li><li><p>Print the mean of this summary:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; print(summary.mean)</strong></span>
</pre></div></li><li><p>Print the variance:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; print(summary.variance)</strong></span>
</pre></div></li><li><p>Print the non-zero values in each column:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; print(summary.numNonzeros)</strong></span>
</pre></div></li><li><p>Print the sample size:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; print(summary.count)</strong></span>
</pre></div></li><li><p>Print the max value of each column:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; print(summary.max)</strong></span>
</pre></div></li></ol></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec48"></a>Calculating correlation</h2></div></div><hr /></div><p>Correlation is a<a id="id346" class="indexterm"></a> statistical relationship between two variables such that when <a id="id347" class="indexterm"></a>one variable changes, it leads to a change in the other variable. Correlation analysis measures the extent to which the two variables are correlated.</p><p>If an increase<a id="id348" class="indexterm"></a> in one variable leads to an increase in another, it is <a id="id349" class="indexterm"></a>called a <span class="strong"><strong>positive correlation</strong></span>. If an increase in one variable leads to a decrease<a id="id350" class="indexterm"></a> in the other, it is<a id="id351" class="indexterm"></a> a <span class="strong"><strong>negative correlation</strong></span>.</p><p>Spark supports two correlation algorithms: Pearson and Spearman. Pearson algorithm works with two continuous variables, such as a person's height and weight or house size and house price. Spearman deals with one continuous and one categorical variable, for example, zip code and house price.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec71"></a>Getting ready</h3></div></div></div><p>Let's use some real data so that we can calculate correlation more meaningfully. The following are <a id="id352" class="indexterm"></a>the size and price of houses in the City of Saratoga, California, in early 2014:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>House size (sq ft)</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Price</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>2100</p>
</td><td style="" align="left" valign="top">
<p>$1,620,000</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>2300</p>
</td><td style="" align="left" valign="top">
<p>$1,690,000</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>2046</p>
</td><td style="" align="left" valign="top">
<p>$1,400,000</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>4314</p>
</td><td style="" align="left" valign="top">
<p>$2,000,000</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>1244</p>
</td><td style="" align="left" valign="top">
<p>$1,060,000</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>4608</p>
</td><td style="" align="left" valign="top">
<p>$3,830,000</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>2173</p>
</td><td style="" align="left" valign="top">
<p>$1,230,000</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>2750</p>
</td><td style="" align="left" valign="top">
<p>$2,400,000</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>4010</p>
</td><td style="" align="left" valign="top">
<p>$3,380,000</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>1959</p>
</td><td style="" align="left" valign="top">
<p>$1,480,000</p>
</td></tr></tbody></table></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec72"></a>How to do it…</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Start the Spark shell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell</strong></span>
</pre></div></li><li><p>Import the statistics and related classes:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.linalg._</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.stat.Statistics</strong></span>
</pre></div></li><li><p>Create an RDD of house sizes:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val sizes = sc.parallelize(List(2100, 2300, 2046, 4314, 1244, 4608, 2173, 2750, 4010, 1959.0))</strong></span>
</pre></div></li><li><p>Create an RDD of house prices:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val prices = sc.parallelize(List(1620000 , 1690000, 1400000, 2000000, 1060000, 3830000, 1230000, 2400000, 3380000, 1480000.00))</strong></span>
</pre></div></li><li><p>Compute the correlation:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val correlation = Statistics.corr(sizes,prices)</strong></span>
<span class="strong"><strong>correlation: Double = 0.8577177736252577 </strong></span>
</pre></div><p><code class="literal">0.85</code> means a very strong positive correlation.</p><p>Since we do not have a specific algorithm here, it is, by default, Pearson. The <code class="literal">corr</code> method is overloaded to take the algorithm name as the third parameter.</p></li><li><p>Compute <a id="id353" class="indexterm"></a>the correlation with Pearson:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val correlation = Statistics.corr(sizes,prices)</strong></span>
</pre></div></li><li><p>Compute the correlation with Spearman:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val correlation = Statistics.corr(sizes,prices,"spearman")</strong></span>
</pre></div></li></ol></div><p>Both the variables in the preceding example are continuous, so Spearman assumes the size to be discrete. A better example of Spearman's use would be zip code versus price.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec49"></a>Doing hypothesis testing</h2></div></div><hr /></div><p>Hypothesis testing<a id="id354" class="indexterm"></a> is a way of determining probability that a given hypothesis is true. Let's say a sample data suggests that females tend to vote more for the Democratic Party. This may or may not be true for the larger population. What if this<a id="id355" class="indexterm"></a> pattern is there in the sample data just by chance?</p><p>Another way to look at the goal of hypothesis testing is to answer this question: If a sample has a pattern in it, what are the chances of the pattern being there just by chance?</p><p>How do we do it? There is a saying that the best way to prove something is to try to disprove it.</p><p>The hypothesis to <a id="id356" class="indexterm"></a>disprove is called <span class="strong"><strong>null hypothesis</strong></span>. Hypothesis testing works with categorical data. Let's look at the example of a gallop poll of party affiliations.</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Party</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Male</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Female</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>Democratic Party</p>
</td><td style="" align="left" valign="top">
<p>32</p>
</td><td style="" align="left" valign="top">
<p>41</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Republican Party</p>
</td><td style="" align="left" valign="top">
<p>28</p>
</td><td style="" align="left" valign="top">
<p>25</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Independent</p>
</td><td style="" align="left" valign="top">
<p>34</p>
</td><td style="" align="left" valign="top">
<p>26</p>
</td></tr></tbody></table></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec73"></a>How to do it…</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Start the Spark shell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell</strong></span>
</pre></div></li><li><p>Import the relevant classes:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.stat.Statistics</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.linalg.{Vector,Vectors}</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.linalg.{Matrix, Matrices}</strong></span>
</pre></div></li><li><p>Create a vector for the Democratic Party:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val dems = Vectors.dense(32.0,41.0)</strong></span>
</pre></div></li><li><p>Create a <a id="id357" class="indexterm"></a>vector for the Republican Party:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val reps= Vectors.dense(28.0,25.0)</strong></span>
</pre></div></li><li><p>Create a vector for the Independents:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val indies = Vectors.dense(34.0,26.0)</strong></span>
</pre></div></li><li><p>Do the chi-square goodness of fit test of the observed data against uniform distribution:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val dfit = Statistics.chiSqTest(dems)</strong></span>
<span class="strong"><strong>scala&gt; val rfit = Statistics.chiSqTest(reps)</strong></span>
<span class="strong"><strong>scala&gt; val ifit = Statistics.chiSqTest(indies)</strong></span>
</pre></div></li><li><p>Print the goodness of fit results:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; print(dfit)</strong></span>
<span class="strong"><strong>scala&gt; print(rfit)</strong></span>
<span class="strong"><strong>scala&gt; print(ifit)</strong></span>
</pre></div></li><li><p>Create the input matrix:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val mat = Matrices.dense(2,3,Array(32.0,41.0, 28.0,25.0, 34.0,26.0))</strong></span>
</pre></div></li><li><p>Do the chi-square independence test:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val in = Statistics.chiSqTest(mat)</strong></span>
</pre></div></li><li><p>Print the independence test results:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; print(in)</strong></span>
</pre></div></li></ol></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec50"></a>Creating machine learning pipelines using ML</h2></div></div><hr /></div><p>Spark ML is a <a id="id358" class="indexterm"></a>new library in Spark to<a id="id359" class="indexterm"></a> build machine learning pipelines. This library is being developed along with MLlib. It helps to combine multiple machine learning algorithms into a single pipeline, and uses DataFrame as dataset.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec74"></a>Getting ready</h3></div></div></div><p>Let's first understand some of the basic concepts in Spark ML. It uses transformers to transform one DataFrame into another DataFrame. One example of simple transformations can be to append a column. You can think of it as being equivalent to "alter table" in relational world.</p><p>Estimator, on<a id="id360" class="indexterm"></a> the other hand, represents a machine learning algorithm, which learns from the data. Input to an estimator is a DataFrame and output is a transformer. Every Estimator has a <code class="literal">fit()</code> method, which does the job of training the algorithm.</p><p>A machine<a id="id361" class="indexterm"></a> learning pipeline is <a id="id362" class="indexterm"></a>defined as a sequence of stages; each stage can be either an estimator or a transformer.</p><p>The example we are going to use in this recipe is whether someone is a basketball player or not a basketball player. For this, we are going to have a pipeline of one estimator and one transformer.</p><p>Estimator gets training data to train the algorithms and then transformer makes predictions.</p><p>For now, assume <code class="literal">LogisticRegression</code> to be the machine learning algorithm we are using. We will explain the details about <code class="literal">LogisticRegression</code> along with other algorithms in the subsequent chapters.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec75"></a>How to do it…</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Start the Spark shell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell</strong></span>
</pre></div></li><li><p>Do the imports:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.linalg.{Vector,Vectors}</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.regression.LabeledPoint</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.ml.classification.LogisticRegression</strong></span>
</pre></div></li><li><p>Create a labeled point for Lebron who is a basketball player, is 80 inches tall height and weighs 250 lbs:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val lebron = LabeledPoint(1.0,Vectors.dense(80.0,250.0))</strong></span>
</pre></div></li><li><p>Create a labeled point for Tim who is not a basketball player, is 70 inches tall height and weighs 150 lbs:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val tim = LabeledPoint(0.0,Vectors.dense(70.0,150.0))</strong></span>
</pre></div></li><li><p>Create a labeled point for Brittany who is a basketball player, is 80 inches tall height and weighs 207 lbs:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val brittany = LabeledPoint(1.0,Vectors.dense(80.0,207.0))</strong></span>
</pre></div></li><li><p>Create a labeled point for Stacey who is not a basketball player, is 65 inches tall, and weighs 120 lbs:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val stacey = LabeledPoint(0.0,Vectors.dense(65.0,120.0))</strong></span>
</pre></div></li><li><p>Create a training RDD:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val trainingRDD = sc.parallelize(List(lebron,tim,brittany,stacey))</strong></span>
</pre></div></li><li><p>Create a training DataFrame:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val trainingDF = trainingRDD.toDF</strong></span>
</pre></div></li><li><p>Create a <code class="literal">LogisticRegression</code> estimator:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val estimator = new LogisticRegression</strong></span>
</pre></div></li><li><p>Create <a id="id363" class="indexterm"></a>a transformer<a id="id364" class="indexterm"></a> by fitting the estimator with training DataFrame:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val transformer = estimator.fit(trainingDF)</strong></span>
</pre></div></li><li><p>Now, let's create a test data—John is 90 inches tall and weighs 270 lbs, and is a basketball player:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val john = Vectors.dense(90.0,270.0)</strong></span>
</pre></div></li><li><p>Create another test data—Tom is 62 inches tall and weighs 150 lbs, and is not a basketball player:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val tom = Vectors.dense(62.0,120.0)</strong></span>
</pre></div></li><li><p>Create a training RDD:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val testRDD = sc.parallelize(List(john,tom))</strong></span>
</pre></div></li><li><p>Create a <code class="literal">Features</code> case class:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; case class Feature(v:Vector)</strong></span>
</pre></div></li><li><p>Map the <code class="literal">testRDD</code> to an RDD for <code class="literal">Features</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val featuresRDD = testRDD.map( v =&gt; Feature(v))</strong></span>
</pre></div></li><li><p>Convert <code class="literal">featuresRDD</code> into a DataFrame with column name <code class="literal">"features"</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val featuresDF = featuresRDD.toDF("features")</strong></span>
</pre></div></li><li><p>Transform <code class="literal">featuresDF</code> by adding the <code class="literal">predictions</code> column to it:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val predictionsDF = transformer.transform(featuresDF)</strong></span>
</pre></div></li><li><p>Print the <code class="literal">predictionsDF</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; predictionsDF.foreach(println)</strong></span>
</pre></div></li><li><p><code class="literal">PredictionDF</code>, as you can see, creates three columns—<code class="literal">rawPrediction</code>, <code class="literal">probability</code>, and <code class="literal">prediction</code>—besides keeping features. Let's select only <code class="literal">features</code> and <code class="literal">prediction</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val shorterPredictionsDF = predictionsDF.select("features","prediction")</strong></span>
</pre></div></li><li><p>Rename <a id="id365" class="indexterm"></a>the prediction <a id="id366" class="indexterm"></a>to <code class="literal">isBasketBallPlayer</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val playerDF = shorterPredictionsDF.toDF("features","isBasketBallPlayer")</strong></span>
</pre></div></li><li><p>Print the schema for <code class="literal">playerDF</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; playerDF.printSchema</strong></span>
</pre></div></li></ol></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch07"></a>Chapter 7. Supervised Learning with MLlib – Regression</h2></div></div></div><p>This chapter is divided into the following recipes:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Using linear regression</p></li><li style="list-style-type: disc"><p>Understanding the cost function</p></li><li style="list-style-type: disc"><p>Doing linear regression with lasso</p></li><li style="list-style-type: disc"><p>Doing ridge regression</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec51"></a>Introduction</h2></div></div><hr /></div><p>The following is Wikipedia's definition of supervised learning:</p><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>"Supervised learning is the machine learning task of inferring a function from labeled training data."</em></span></p></blockquote></div><p>Supervised learning <a id="id367" class="indexterm"></a>has two steps:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Train the algorithm with training dataset; it is like giving questions and their answers first</p></li><li style="list-style-type: disc"><p>Use test dataset to ask another set of questions to the trained algorithm</p></li></ul></div><p>There are two types of supervised learning algorithms:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><span class="strong"><strong>Regression</strong></span>: This<a id="id368" class="indexterm"></a> predicts continuous value output, <a id="id369" class="indexterm"></a>such as house price.</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Classification</strong></span>: This <a id="id370" class="indexterm"></a>predicts discreet valued<a id="id371" class="indexterm"></a> output (0 or 1) called label, such as whether an e-mail is a spam or not. Classification is not limited to two values; it can have multiple values such as marking an e-mail important, not important, urgent, and so on (0, 1, 2…).</p></li></ul></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note18"></a>Note</h3><p>We are going to cover regression in this chapter and classification in the next.</p></div><p>As an example dataset for regression, we will use the recently sold house data of the City of Saratoga, CA, as a training set to train the algorithm. Once the algorithm is trained, we will ask it to predict the <a id="id372" class="indexterm"></a>house price by the given size of that house. The following figure illustrates the workflow:</p><div class="mediaobject"><img src="graphics/3056_07_01.jpg" /></div><p>Hypothesis, for what it does, may sound like a misnomer here, and you may think that prediction function may be a better name, but the word hypothesis is used for historic reasons.</p><p>If we use only one <a id="id373" class="indexterm"></a>feature to predict the outcome, it is called <span class="strong"><strong>bivariate analysis</strong></span>. When <a id="id374" class="indexterm"></a>we have multiple features, it is called <span class="strong"><strong>multivariate analysis</strong></span>. In fact, we can have as many features as we like. One such<a id="id375" class="indexterm"></a> algorithm, <span class="strong"><strong>support vector machines</strong></span> (<span class="strong"><strong>SVM</strong></span>), which we will cover in the next chapter, in fact, allows you to have an infinite number of features.</p><p>This chapter will cover how we can do supervised learning using MLlib, Spark's machine learning library.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note19"></a>Note</h3><p>Mathematical explanations have been provided in as simple a way as possible, but you can feel free to skip math and directly go to <span class="emphasis"><em>How to do it...</em></span> section.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec52"></a>Using linear regression</h2></div></div><hr /></div><p>Linear regression is the <a id="id376" class="indexterm"></a>approach to model the value of a response variable <span class="emphasis"><em>y</em></span>, based on one or more predictor variables or feature <span class="emphasis"><em>x</em></span>.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec76"></a>Getting ready</h3></div></div></div><p>Let's use some<a id="id377" class="indexterm"></a> housing data to <a id="id378" class="indexterm"></a>predict the price of a house based on its size. The following are the sizes and prices of houses in the City of Saratoga, CA, in early 2014:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>House size (sq ft)</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Price</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>2100</p>
</td><td style="" align="left" valign="top">
<p>$ 1,620,000</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>2300</p>
</td><td style="" align="left" valign="top">
<p>$ 1,690,000</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>2046</p>
</td><td style="" align="left" valign="top">
<p>$ 1,400,000</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>4314</p>
</td><td style="" align="left" valign="top">
<p>$ 2,000,000</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>1244</p>
</td><td style="" align="left" valign="top">
<p>$ 1,060,000</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>4608</p>
</td><td style="" align="left" valign="top">
<p>$ 3,830,000</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>2173</p>
</td><td style="" align="left" valign="top">
<p>$ 1,230,000</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>2750</p>
</td><td style="" align="left" valign="top">
<p>$ 2,400,000</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>4010</p>
</td><td style="" align="left" valign="top">
<p>$ 3,380,000</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>1959</p>
</td><td style="" align="left" valign="top">
<p>$ 1,480,000</p>
</td></tr></tbody></table></div><p>Here's a <a id="id379" class="indexterm"></a>graphical representation of the same:</p><div class="mediaobject"><img src="graphics/3056_07_04.jpg" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec77"></a>How to do it…</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Start the <a id="id380" class="indexterm"></a>Spark shell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell</strong></span>
</pre></div></li><li><p>Import the statistics and related classes:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.linalg.Vectors</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.regression.LabeledPoint</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.regression.LinearRegressionWithSGD</strong></span>
</pre></div></li><li><p>Create the <code class="literal">LabeledPoint</code> array with the house price as the label:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val points = Array(</strong></span>
<span class="strong"><strong>LabeledPoint(1620000,Vectors.dense(2100)),</strong></span>
<span class="strong"><strong>LabeledPoint(1690000,Vectors.dense(2300)),</strong></span>
<span class="strong"><strong>LabeledPoint(1400000,Vectors.dense(2046)),</strong></span>
<span class="strong"><strong>LabeledPoint(2000000,Vectors.dense(4314)),</strong></span>
<span class="strong"><strong>LabeledPoint(1060000,Vectors.dense(1244)),</strong></span>
<span class="strong"><strong>LabeledPoint(3830000,Vectors.dense(4608)),</strong></span>
<span class="strong"><strong>LabeledPoint(1230000,Vectors.dense(2173)),</strong></span>
<span class="strong"><strong>LabeledPoint(2400000,Vectors.dense(2750)),</strong></span>
<span class="strong"><strong>LabeledPoint(3380000,Vectors.dense(4010)),</strong></span>
<span class="strong"><strong>LabeledPoint(1480000,Vectors.dense(1959))</strong></span>
<span class="strong"><strong>)</strong></span>
</pre></div></li><li><p>Create an RDD of the preceding data:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val pricesRDD = sc.parallelize(points)</strong></span>
</pre></div></li><li><p>Train a model using this data using 100 iterations. Here, step size has been kept small to account for very large values of response variables, that is, the house price. The fourth parameter is a fraction of the dataset to use per iteration, and the last parameter is the initial set of weights to be used (weights of different features):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val model = LinearRegressionWithSGD.train(pricesRDD,100,0.0000006,1.0,Vectors.zeros(1))</strong></span>
</pre></div></li><li><p>Predict the price for a house with 2,500 sq ft:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val prediction = model.predict(Vectors.dense(2500))</strong></span>
</pre></div></li></ol></div><p>House size is just one predictor variable. House price depends upon other variables, such as lot size, age of the house, and so on. The more variables you have, the better your prediction will be.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec53"></a>Understanding cost function</h2></div></div><hr /></div><p>Cost function or loss function is a very important function in machine learning algorithms. Most<a id="id381" class="indexterm"></a> algorithms have some form of cost function and the goal is to minimize that. Parameters, which affect cost function, such as <code class="literal">stepSize</code> in the last recipe, need to be set by hand. Therefore, understanding the whole concept of cost function is very important.</p><p>In this recipe, we <a id="id382" class="indexterm"></a>are going to analyze cost function for linear regression. Linear regression is a simple algorithm to understand and it will help readers<a id="id383" class="indexterm"></a> understand the role of cost functions for<a id="id384" class="indexterm"></a> even complex algorithms.</p><p>Let's go back to linear regression. The goal is to find the best-fitting line so that the mean square of error is minimum. Here, we are referring error as the difference between the value as per the best-fitting line and the actual value of the response variable for the training dataset.</p><p>For a simple case of single predicate variable, the best-fit line can be written as:</p><div class="mediaobject"><img src="graphics/3056_07_02.jpg" /></div><p>This function is<a id="id385" class="indexterm"></a> also called <span class="strong"><strong>hypothesis function</strong></span>, and can be written as:</p><div class="mediaobject"><img src="graphics/3056_07_03.jpg" /></div><p>The goal of the linear regression is to find the best-fit line. On this line, θ<sub>0</sub> represents intercept on the <span class="emphasis"><em>y</em></span> axis and θ<sub>1</sub> represents the slope of the line as obvious from the following equation:</p><div class="mediaobject"><img src="graphics/3056_07_05.jpg" /></div><p>We have to choose θ<sub>0</sub> and θ<sub>1</sub> in a way that <span class="emphasis"><em>h(x)</em></span> is closest to <span class="emphasis"><em>y</em></span> for the training dataset. So, for the <span class="emphasis"><em>i</em></span>
<sup>th</sup> data point, the square of distance between the line and data point is:</p><div class="mediaobject"><img src="graphics/3056_07_06.jpg" /></div><p>To put it in words, this is the square of the difference between the predicted house price and the<a id="id386" class="indexterm"></a> actual price the house got sold for. Now, let's<a id="id387" class="indexterm"></a> take average of this value across the training dataset:</p><div class="mediaobject"><img src="graphics/3056_07_07.jpg" /></div><p>The preceding equation is called the cost function <span class="emphasis"><em>J</em></span> for linear regression. The goal is to minimize this cost function.</p><div class="mediaobject"><img src="graphics/3056_07_08.jpg" /></div><p>This cost function is also called <span class="strong"><strong>squared error function</strong></span>. Both θ<sub>0</sub> and theta θ<sub>1</sub> follow convex curve<a id="id388" class="indexterm"></a> independently if they are plotted against <span class="emphasis"><em>J</em></span>.</p><p>Let's take a very simple example of dataset of three values, (1,1), (2,2), and (3,3), to make the calculation easy:</p><div class="mediaobject"><img src="graphics/3056_07_09.jpg" /></div><p>Let's assume θ<sub>1</sub> is 0, that is, the best-fit line parallel to the <span class="emphasis"><em>x</em></span> axis. In the first case, assume that the best-fit line is the <span class="emphasis"><em>x</em></span> axis, that is, <span class="emphasis"><em>y=0</em></span>. The following will be the value of the cost function:</p><div class="mediaobject"><img src="graphics/3056_07_10.jpg" /></div><p>Now, let's <a id="id389" class="indexterm"></a>move this line slightly up to <span class="emphasis"><em>y=1</em></span>. The<a id="id390" class="indexterm"></a> following will be the value of the cost function:</p><div class="mediaobject"><img src="graphics/3056_07_11.jpg" /></div><p>Now, let's move this line further up to <span class="emphasis"><em>y=2</em></span>. Then, the following will be the value of the cost function:</p><div class="mediaobject"><img src="graphics/3056_07_12.jpg" /></div><p>Now, when we move this line further up to <span class="emphasis"><em>y=3</em></span>, the following will be the value of the cost function:</p><div class="mediaobject"><img src="graphics/3056_07_13.jpg" /></div><p>Now, let's <a id="id391" class="indexterm"></a>move this line further up to <span class="emphasis"><em>y=4</em></span>. The<a id="id392" class="indexterm"></a> following will be the value of the cost function:</p><div class="mediaobject"><img src="graphics/3056_07_14.jpg" /></div><p>So, you saw that the cost function value first decreased, and then increased again like this:</p><div class="mediaobject"><img src="graphics/3056_07_15.jpg" /></div><p>Now, let's<a id="id393" class="indexterm"></a> repeat the exercise by making θ<sub>0</sub> 0 and using different values of θ<sub>1</sub>.</p><p>In the<a id="id394" class="indexterm"></a> first case, assume the best-fit line is the <span class="emphasis"><em>x</em></span> axis, that is, <span class="emphasis"><em>y=0</em></span>. The following will be the value of the cost function:</p><div class="mediaobject"><img src="graphics/3056_07_16.jpg" /></div><p>Now, let's use a slope of 0.5. The following will be the value of the cost function:</p><div class="mediaobject"><img src="graphics/3056_07_17.jpg" /></div><p>Now, let's<a id="id395" class="indexterm"></a> use a slope of 1. The following<a id="id396" class="indexterm"></a> will be the value of the cost function:</p><div class="mediaobject"><img src="graphics/3056_07_18.jpg" /></div><p>Now, when we use a slope of 1.5, the following will be the value of the cost function:</p><div class="mediaobject"><img src="graphics/3056_07_19.jpg" /></div><p>Now, let's use a slope of 2.0. The following will be the value of the cost function:</p><div class="mediaobject"><img src="graphics/3056_07_20.jpg" /></div><p>As you <a id="id397" class="indexterm"></a>can see in both the graphs, the minimum <a id="id398" class="indexterm"></a>value of <span class="emphasis"><em>J</em></span> is when slope or gradient of curve is 0.</p><div class="mediaobject"><img src="graphics/3056_07_21.jpg" /></div><p>When both θ<sub>0</sub> and θ<sub>1</sub> are mapped to a 3D space, it becomes like the shape of a bowl with the minimum value of the cost function being at the bottom of it.</p><p>This approach to arrive<a id="id399" class="indexterm"></a> at this minimum is called <span class="strong"><strong>gradient descent</strong></span>. In Spark, the implementation is stochastic gradient descent.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec54"></a>Doing linear regression with lasso</h2></div></div><hr /></div><p>The lasso<a id="id400" class="indexterm"></a> is a shrinkage and selection method for linear regression. It minimizes<a id="id401" class="indexterm"></a> the usual sum of squared errors, with <a id="id402" class="indexterm"></a>a bound on the sum of the absolute values <a id="id403" class="indexterm"></a>of the coefficients. It is based on the original lasso paper found at <a class="ulink" href="http://statweb.stanford.edu/~tibs/lasso/lasso.pdf" target="_blank">http://statweb.stanford.edu/~tibs/lasso/lasso.pdf</a>.</p><p>The least square <a id="id404" class="indexterm"></a>method we used in the last recipe is also called <span class="strong"><strong>ordinary least squares</strong></span> (<span class="strong"><strong>OLS</strong></span>). OLS has two challenges:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><span class="strong"><strong>Prediction accuracy</strong></span>: Predictions made using OLS usually have low forecast bias and high <a id="id405" class="indexterm"></a>variance. Prediction accuracy can be improved by shrinking some coefficients (or even making them zero). There will be some increase in bias, but overall prediction accuracy will improve.</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Interpretation</strong></span>: With a<a id="id406" class="indexterm"></a> large number of predictors, it is desirable to find a subset of them that exhibits the strongest effect (correlation).</p></li></ul></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note20"></a>Note</h3><p>Bias versus variance</p><p>There are<a id="id407" class="indexterm"></a> two primary reasons behind prediction error: bias and variance. The best way to understand bias and variance is to look at a case <a id="id408" class="indexterm"></a>where we are doing predictions on the same dataset multiple times.</p><p>Bias is an<a id="id409" class="indexterm"></a> estimate of how far the predicted results are from the actual values, <a id="id410" class="indexterm"></a>and variance is an estimate of the difference in predicted values among different predictions.</p><p>Generally, adding more features helps to reduce bias, as can easily be understood. If, in building a prediction model, we have left out some features with significant correlation, it would lead to significant error.</p><p>If your model has high variance, you can remove features to reduce it. A bigger dataset also helps to reduce variance.</p></div><p>Here, we<a id="id411" class="indexterm"></a> are going to use a simple dataset, which<a id="id412" class="indexterm"></a> is ill-posed. An ill-posed dataset is a dataset where the sample data size is smaller than the number of predictors.</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /><col align="left" /><col align="left" /><col align="left" /><col align="left" /><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>y</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>x0</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>x1</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>x2</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>x3</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>x4</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>x5</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>x6</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>x7</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>x8</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>1</p>
</td><td style="" align="left" valign="top">
<p>5</p>
</td><td style="" align="left" valign="top">
<p>3</p>
</td><td style="" align="left" valign="top">
<p>1</p>
</td><td style="" align="left" valign="top">
<p>2</p>
</td><td style="" align="left" valign="top">
<p>1</p>
</td><td style="" align="left" valign="top">
<p>3</p>
</td><td style="" align="left" valign="top">
<p>2</p>
</td><td style="" align="left" valign="top">
<p>2</p>
</td><td style="" align="left" valign="top">
<p>1</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>2</p>
</td><td style="" align="left" valign="top">
<p>9</p>
</td><td style="" align="left" valign="top">
<p>8</p>
</td><td style="" align="left" valign="top">
<p>8</p>
</td><td style="" align="left" valign="top">
<p>9</p>
</td><td style="" align="left" valign="top">
<p>7</p>
</td><td style="" align="left" valign="top">
<p>9</p>
</td><td style="" align="left" valign="top">
<p>8</p>
</td><td style="" align="left" valign="top">
<p>7</p>
</td><td style="" align="left" valign="top">
<p>9</p>
</td></tr></tbody></table></div><p>You can easily guess that, here, out of nine predictors, only two have a strong correlation with <span class="emphasis"><em>y</em></span>, that is, <span class="emphasis"><em>x0</em></span> and <span class="emphasis"><em>x1</em></span>. We will use this dataset with the lasso algorithm to see its validity.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec78"></a>How to do it…</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Start the Spark shell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell</strong></span>
</pre></div></li><li><p>Import the statistics and related classes:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.linalg.Vectors</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.regression.LabeledPoint</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.regression.LassoWithSGD</strong></span>
</pre></div></li><li><p>Create the <code class="literal">LabeledPoint</code> array with the house price as the label:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val points = Array(</strong></span>
<span class="strong"><strong>LabeledPoint(1,Vectors.dense(5,3,1,2,1,3,2,2,1)),</strong></span>
<span class="strong"><strong>LabeledPoint(2,Vectors.dense(9,8,8,9,7,9,8,7,9))</strong></span>
<span class="strong"><strong>)</strong></span>
</pre></div></li><li><p>Create an RDD of the preceding data:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val rdd = sc.parallelize(points)</strong></span>
</pre></div></li><li><p>Train a model using this data using 100 iterations. Here, the step size and regularization parameter have been set by hand:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val model = LassoWithSGD.train(rdd,100,0.02,2.0)</strong></span>
</pre></div></li><li><p>Check how many predictors have their coefficients set to zero:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; model.weights</strong></span>
<span class="strong"><strong>org.apache.spark.mllib.linalg.Vector = [0.13455106581619633,0.02240732644670294,0.0,0.0,0.0,0.01360995990267153,0.0,0.0,0.0]</strong></span>
</pre></div></li></ol></div><p>As you<a id="id413" class="indexterm"></a> can see, six out of nine predictors<a id="id414" class="indexterm"></a> have got their coefficients set to zero. This is the primary feature of lasso: any predictor it thinks is not useful, it literally moves them out of equation by setting their coefficients to zero.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec55"></a>Doing ridge regression</h2></div></div><hr /></div><p>An alternate <a id="id415" class="indexterm"></a>way to lasso to improve prediction quality is ridge regression. While in lasso, a lot of features get their coefficients set to zero and, therefore, eliminated<a id="id416" class="indexterm"></a> from an equation, in ridge, predictors or features are penalized, but are never set to zero.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec79"></a>How to do it…</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Start the Spark shell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell</strong></span>
</pre></div></li><li><p>Import the statistics and related classes:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.linalg.Vectors</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.regression.LabeledPoint</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.regression.RidgeRegressionWithSGD</strong></span>
</pre></div></li><li><p>Create the <code class="literal">LabeledPoint</code> array with the house price as the label:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val points = Array(</strong></span>
<span class="strong"><strong>LabeledPoint(1,Vectors.dense(5,3,1,2,1,3,2,2,1)),</strong></span>
<span class="strong"><strong>LabeledPoint(2,Vectors.dense(9,8,8,9,7,9,8,7,9))</strong></span>
<span class="strong"><strong>)</strong></span>
</pre></div></li><li><p>Create an RDD of the preceding data:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val rdd = sc.parallelize(points)</strong></span>
</pre></div></li><li><p>Train a model using this data using 100 iterations. Here, the step size and regularization parameter have been set by hand :</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val model = RidgeRegressionWithSGD.train(rdd,100,0.02,2.0)</strong></span>
</pre></div></li><li><p>Check how many predictors have their coefficients set to zero:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; model.weights</strong></span>
<span class="strong"><strong>org.apache.spark.mllib.linalg.Vector = [0.049805969577244584,0.029883581746346748,0.009961193915448916,0.019922387830897833,0.009961193915448916,0.029883581746346748,0.019922387830897833,0.019922387830897833,0.009961193915448916]</strong></span>
</pre></div></li></ol></div><p>As you can <a id="id417" class="indexterm"></a>see, unlike lasso, ridge regression does not assign any predictor coefficients zero, but it does make some very close to zero.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch08"></a>Chapter 8. Supervised Learning with MLlib – Classification</h2></div></div></div><p>This chapter is divided into the following recipes:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Doing classification using logistic regression</p></li><li style="list-style-type: disc"><p>Doing binary classification using SVM</p></li><li style="list-style-type: disc"><p>Doing classification using decision trees</p></li><li style="list-style-type: disc"><p>Doing classification using Random Forests</p></li><li style="list-style-type: disc"><p>Doing classification using Gradient Boosted Trees</p></li><li style="list-style-type: disc"><p>Doing classification with Naïve Bayes</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec56"></a>Introduction</h2></div></div><hr /></div><p>The classification problem is like the regression problem discussed in the previous chapter except that the outcome variable <span class="emphasis"><em>y</em></span> takes only a few discrete values. In binary classification, <span class="emphasis"><em>y</em></span> takes only two values: 0 or 1. You can also think of values that the response variable can take in classification as representing categories.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec57"></a>Doing classification using logistic regression</h2></div></div><hr /></div><p>In <a id="id418" class="indexterm"></a>classification, the response variable <span class="emphasis"><em>y</em></span> has <a id="id419" class="indexterm"></a>discreet values as opposed to continuous values. Some examples are e-mail (spam/non-spam), transactions (safe/fraudulent), and so on.</p><p>The <span class="emphasis"><em>y</em></span> variable in the following equation can take on two values, 0 or 1:</p><div class="mediaobject"><img src="graphics/3056_08_01.jpg" /></div><p>Here, 0 is referred to as a negative class and 1 means a positive class. Though we are calling them a positive or negative class, it is only for convenience's sake. Algorithms are neutral about this assignment.</p><p>Linear <a id="id420" class="indexterm"></a>regression, though it works well<a id="id421" class="indexterm"></a> for regression tasks, hits a few limitations for classification tasks. These include:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The fitting process is very susceptible to outliers</p></li><li style="list-style-type: disc"><p>There is no guarantee that the hypothesis function <span class="emphasis"><em>h(x)</em></span> will fit in the range 0 (negative class) to 1 (positive class)</p></li></ul></div><p>Logistic regression guarantees that <span class="emphasis"><em>h(x)</em></span> will fit between 0 and 1. Though logistic regression has the word regression in it, it is more of a misnomer and it is very much a classification algorithm:</p><div class="mediaobject"><img src="graphics/3056_08_02.jpg" /></div><p>In linear regression, the hypothesis function is as follows:</p><div class="mediaobject"><img src="graphics/3056_08_03.jpg" /></div><p>In logistic regression, we slightly modify the hypothesis equation like this:</p><div class="mediaobject"><img src="graphics/3056_08_04.jpg" /></div><p>The <span class="emphasis"><em>g</em></span> function is <a id="id422" class="indexterm"></a>called the <span class="strong"><strong>sigmoid function</strong></span> or <span class="strong"><strong>logistic function</strong></span> and is <a id="id423" class="indexterm"></a>defined as follows for a real number <span class="emphasis"><em>t</em></span>:</p><div class="mediaobject"><img src="graphics/3056_08_05.jpg" /></div><p>This is what the sigmoid function looks like as a graph:</p><div class="mediaobject"><img src="graphics/3056_08_06.jpg" /></div><p>As you can see, as <span class="emphasis"><em>t</em></span> approaches negative infinity, <span class="emphasis"><em>g(t)</em></span> approaches 0 and, as <span class="emphasis"><em>t</em></span> approaches infinity, <span class="emphasis"><em>g(t)</em></span> approaches 1. So, this guarantees that the hypothesis function output will never<a id="id424" class="indexterm"></a> fall out of the 0 to 1 range.</p><p>Now<a id="id425" class="indexterm"></a> the hypothesis function can be rewritten as:</p><div class="mediaobject"><img src="graphics/3056_08_07.jpg" /></div><p><span class="emphasis"><em>h(x)</em></span> is the estimated probability that <span class="emphasis"><em>y = 1</em></span> for a given predictor <span class="emphasis"><em>x</em></span>, so <span class="emphasis"><em>h(x)</em></span> can also be rewritten as:</p><div class="mediaobject"><img src="graphics/3056_08_08.jpg" /></div><p>In other words, the hypothesis function is showing the probability of <span class="emphasis"><em>y</em></span> being 1 given feature matrix <span class="emphasis"><em>x</em></span>, parameterized by <span class="inlinemediaobject"><img src="graphics/3056_08_09.jpg" /></span>. This probability can be any real number between 0 and 1 but our goal of classification does not allow us to have continuous values; we can only have two values 0 or 1 indicating the negative or positive class.</p><p>Let's say that we predict <span class="emphasis"><em>y = 1</em></span> if </p><div class="mediaobject"><img src="graphics/3056_08_10.jpg" /></div><p> and <span class="emphasis"><em>y = 0</em></span> otherwise. If we look at the sigmoid function graph again, we realize that, when the <span class="inlinemediaobject"><img src="graphics/3056_08_11.jpg" /></span> sigmoid function is <span class="inlinemediaobject"><img src="graphics/3056_08_12.jpg" /></span>, that is, for positive values of <span class="emphasis"><em>t</em></span>, it will predict the positive class:</p><p>Since <span class="inlinemediaobject"><img src="graphics/3056_08_13.jpg" /></span>, this means for <span class="inlinemediaobject"><img src="graphics/3056_08_14.jpg" /></span> the positive class will be predicted. To better illustrate this, let's expand it to a non-matrix form for a bivariate case:</p><div class="mediaobject"><img src="graphics/3056_08_15.jpg" /></div><p>The<a id="id426" class="indexterm"></a> plane represented by the equation <span class="inlinemediaobject"><img src="graphics/3056_08_16.jpg" /></span> will decide whether a given vector belongs to the positive class <a id="id427" class="indexterm"></a>or negative class. This line is called the decision boundary.</p><p>This boundary does not have to be linear depending on the training set. If training data does not separate across a linear boundary, higher-level polynomial features can be added to facilitate it. An example can be to add two new features by squaring x1 and x2 as follows:</p><div class="mediaobject"><img src="graphics/3056_08_17.jpg" /></div><p>Please note that, to the learning algorithm, this enhancement is exactly the same as the following equation:</p><div class="mediaobject"><img src="graphics/3056_08_18.jpg" /></div><p>The learning algorithm will treat the introduction of polynomials just as another feature. This gives you great power in the fitting process. It means any complex decision boundary can be created with the right choice of polynomials and parameters.</p><p>Let's spend some time trying to understand how we choose the right value for parameters like we did in the case of linear regression. The cost function <span class="emphasis"><em>J</em></span> in the case of linear regression was:</p><div class="mediaobject"><img src="graphics/3056_08_19.jpg" /></div><p>As <a id="id428" class="indexterm"></a>you know, we are averaging<a id="id429" class="indexterm"></a> the cost in this cost function. Let's represent this in terms of cost term:</p><div class="mediaobject"><img src="graphics/3056_08_20.jpg" /></div><p>In other words, the cost term is the cost the algorithm has to pay if it predicts <span class="emphasis"><em>h(x)</em></span> for the real response variable value <span class="emphasis"><em>y</em></span>:</p><div class="mediaobject"><img src="graphics/3056_08_21.jpg" /></div><p>This cost works fine for linear regression but, for logistic regression, this cost function is non-convex (that is, it leads to multiple local minimums) and we need to find a better convex way to estimate the cost.</p><p>The cost functions that work well for logistic regression are the following:</p><div class="mediaobject"><img src="graphics/3056_08_22.jpg" /></div><p>Let's put these two cost functions into one by combining the two:</p><div class="mediaobject"><img src="graphics/3056_08_23.jpg" /></div><p>Let's<a id="id430" class="indexterm"></a> put back this cost function<a id="id431" class="indexterm"></a> to <span class="emphasis"><em>J</em></span>:</p><div class="mediaobject"><img src="graphics/3056_08_24.jpg" /></div><p>The goal would be to minimize the cost, that is, minimize the value of <span class="inlinemediaobject"><img src="graphics/3056_08_25.jpg" /></span>. This is done using the gradient descent algorithm. Spark has two classes that support logistic regression:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><code class="literal">LogisticRegressionWithSGD</code></p></li><li style="list-style-type: disc"><p><code class="literal">LogisticRegressionWithLBFGS</code></p></li></ul></div><p>The <code class="literal">LogisticRegressionWithLBFGS</code> class is preferred as it eliminates the step of optimizing the step size.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec80"></a>Getting ready</h3></div></div></div><p>In 2006, Suzuki, Tsurusaki, and Kodama did some research on the distribution of an endangered burrowing spider on different beaches in Japan (<a class="ulink" href="https://www.jstage.jst.go.jp/article/asjaa/55/2/55_2_79/_pdf" target="_blank">https://www.jstage.jst.go.jp/article/asjaa/55/2/55_2_79/_pdf</a>).</p><p>Let's see some data about grain size and the presence of spiders:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Grain size (mm)</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Spider present</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>0.245</p>
</td><td style="" align="left" valign="top">
<p>Absent</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>0.247</p>
</td><td style="" align="left" valign="top">
<p>Absent</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>0.285</p>
</td><td style="" align="left" valign="top">
<p>Present</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>0.299</p>
</td><td style="" align="left" valign="top">
<p>Present</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>0.327</p>
</td><td style="" align="left" valign="top">
<p>Present</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>0.347</p>
</td><td style="" align="left" valign="top">
<p>Present</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>0.356</p>
</td><td style="" align="left" valign="top">
<p>Absent</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>0.36</p>
</td><td style="" align="left" valign="top">
<p>Present</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>0.363</p>
</td><td style="" align="left" valign="top">
<p>Absent</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>0.364</p>
</td><td style="" align="left" valign="top">
<p>Present</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>0.398</p>
</td><td style="" align="left" valign="top">
<p>Absent</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>0.4</p>
</td><td style="" align="left" valign="top">
<p>Present</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>0.409</p>
</td><td style="" align="left" valign="top">
<p>Absent</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>0.421</p>
</td><td style="" align="left" valign="top">
<p>Present</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>0.432</p>
</td><td style="" align="left" valign="top">
<p>Absent</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>0.473</p>
</td><td style="" align="left" valign="top">
<p>Present</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>0.509</p>
</td><td style="" align="left" valign="top">
<p>Present</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>0.529</p>
</td><td style="" align="left" valign="top">
<p>Present</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>0.561</p>
</td><td style="" align="left" valign="top">
<p>Absent</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>0.569</p>
</td><td style="" align="left" valign="top">
<p>Absent</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>0.594</p>
</td><td style="" align="left" valign="top">
<p>Present</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>0.638</p>
</td><td style="" align="left" valign="top">
<p>Present</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>0.656</p>
</td><td style="" align="left" valign="top">
<p>Present</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>0.816</p>
</td><td style="" align="left" valign="top">
<p>Present</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>0.853</p>
</td><td style="" align="left" valign="top">
<p>Present</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>0.938</p>
</td><td style="" align="left" valign="top">
<p>Present</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>1.036</p>
</td><td style="" align="left" valign="top">
<p>Present</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>1.045</p>
</td><td style="" align="left" valign="top">
<p>Present</p>
</td></tr></tbody></table></div><p>We <a id="id432" class="indexterm"></a>will use this data to train the<a id="id433" class="indexterm"></a> algorithm. Absent will be denoted as 0 and present will be denoted as 1.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec81"></a>How to do it…</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Start the Spark shell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell</strong></span>
</pre></div></li><li><p>Import statistics and related classes:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.linalg.Vectors</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.regression.LabeledPoint</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS</strong></span>
</pre></div></li><li><p>Create <a id="id434" class="indexterm"></a>a <code class="literal">LabeledPoint</code> array <a id="id435" class="indexterm"></a>with the presence or absence of spiders being the label:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val points = Array(</strong></span>
<span class="strong"><strong>LabeledPoint(0.0,Vectors.dense(0.245)),</strong></span>
<span class="strong"><strong>LabeledPoint(0.0,Vectors.dense(0.247)),</strong></span>
<span class="strong"><strong>LabeledPoint(1.0,Vectors.dense(0.285)),</strong></span>
<span class="strong"><strong>LabeledPoint(1.0,Vectors.dense(0.299)),</strong></span>
<span class="strong"><strong>LabeledPoint(1.0,Vectors.dense(0.327)),</strong></span>
<span class="strong"><strong>LabeledPoint(1.0,Vectors.dense(0.347)),</strong></span>
<span class="strong"><strong>LabeledPoint(0.0,Vectors.dense(0.356)),</strong></span>
<span class="strong"><strong>LabeledPoint(1.0,Vectors.dense(0.36)),</strong></span>
<span class="strong"><strong>LabeledPoint(0.0,Vectors.dense(0.363)),</strong></span>
<span class="strong"><strong>LabeledPoint(1.0,Vectors.dense(0.364)),</strong></span>
<span class="strong"><strong>LabeledPoint(0.0,Vectors.dense(0.398)),</strong></span>
<span class="strong"><strong>LabeledPoint(1.0,Vectors.dense(0.4)),</strong></span>
<span class="strong"><strong>LabeledPoint(0.0,Vectors.dense(0.409)),</strong></span>
<span class="strong"><strong>LabeledPoint(1.0,Vectors.dense(0.421)),</strong></span>
<span class="strong"><strong>LabeledPoint(0.0,Vectors.dense(0.432)),</strong></span>
<span class="strong"><strong>LabeledPoint(1.0,Vectors.dense(0.473)),</strong></span>
<span class="strong"><strong>LabeledPoint(1.0,Vectors.dense(0.509)),</strong></span>
<span class="strong"><strong>LabeledPoint(1.0,Vectors.dense(0.529)),</strong></span>
<span class="strong"><strong>LabeledPoint(0.0,Vectors.dense(0.561)),</strong></span>
<span class="strong"><strong>LabeledPoint(0.0,Vectors.dense(0.569)),</strong></span>
<span class="strong"><strong>LabeledPoint(1.0,Vectors.dense(0.594)),</strong></span>
<span class="strong"><strong>LabeledPoint(1.0,Vectors.dense(0.638)),</strong></span>
<span class="strong"><strong>LabeledPoint(1.0,Vectors.dense(0.656)),</strong></span>
<span class="strong"><strong>LabeledPoint(1.0,Vectors.dense(0.816)),</strong></span>
<span class="strong"><strong>LabeledPoint(1.0,Vectors.dense(0.853)),</strong></span>
<span class="strong"><strong>LabeledPoint(1.0,Vectors.dense(0.938)),</strong></span>
<span class="strong"><strong>LabeledPoint(1.0,Vectors.dense(1.036)),</strong></span>
<span class="strong"><strong>LabeledPoint(1.0,Vectors.dense(1.045)))</strong></span>
</pre></div></li><li><p>Create an RDD of the preceding data:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val spiderRDD = sc.parallelize(points)</strong></span>
</pre></div></li><li><p>Train<a id="id436" class="indexterm"></a> a model using<a id="id437" class="indexterm"></a> this data (intercept is the value when all predictors are zero):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val lr = new LogisticRegressionWithLBFGS().setIntercept(true)</strong></span>
<span class="strong"><strong>scala&gt; val model = lr.run(spiderRDD)</strong></span>
</pre></div></li><li><p>Predict the presence of spiders for grain size <code class="literal">0.938</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val predict = model.predict(Vectors.dense(0.938))</strong></span>
</pre></div></li></ol></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec58"></a>Doing binary classification using SVM</h2></div></div><hr /></div><p>Classification is <a id="id438" class="indexterm"></a>a technique to put data into different <a id="id439" class="indexterm"></a>classes based on its utility. For example, an e-commerce company can apply two labels "will buy" or "will not buy" to potential visitors.</p><p>This classification is done by providing some already labeled data to machine learning algorithms <a id="id440" class="indexterm"></a>called <span class="strong"><strong>training data</strong></span>. The challenge is how to mark the boundary between two classes. Let's take a simple example as shown in the following figure:</p><div class="mediaobject"><img src="graphics/3056_08_26.jpg" /></div><p>In the preceding case, we designated gray and black to the "will not buy" and "will buy" labels. Here, drawing <a id="id441" class="indexterm"></a>a line between the two classes<a id="id442" class="indexterm"></a> is as easy as follows:</p><div class="mediaobject"><img src="graphics/3056_08_27.jpg" /></div><p>Is this the best we can do? Not really, let's try to do a better job. The black classifier is not really equidistant from the "will buy" and "will not buy" carts. Let's make a better attempt like the following:</p><div class="mediaobject"><img src="graphics/3056_08_28.jpg" /></div><p>Now this <a id="id443" class="indexterm"></a>is looking good. This in fact is what the <a id="id444" class="indexterm"></a>SVM algorithm does. You can see in the preceding diagram that in fact there are only three carts that decide the slope of the line: two black carts above the line, and one gray cart below the line. These carts are called <span class="strong"><strong>support vectors</strong></span><a id="id445" class="indexterm"></a> and the rest of the carts, that is, the vectors, are irrelevant.</p><p>Sometimes it's not easy to draw a line and a curve may be needed to separate two classes like the following:</p><div class="mediaobject"><img src="graphics/3056_08_29.jpg" /></div><p>Sometimes <a id="id446" class="indexterm"></a>even that is not enough. In that case, we<a id="id447" class="indexterm"></a> need more than two dimensions to resolve the problem. Rather than a classified line, what we need is a hyperplane. In fact, whenever data is too cluttered, adding extra dimensions help to find a hyperplane to separate classes. The following diagram illustrates this:</p><div class="mediaobject"><img src="graphics/3056_08_30.jpg" /></div><p>This does not mean that adding extra dimensions is always a good idea. Most of the time, our goal is to reduce dimensions and keep only the relevant dimensions/features. A whole <a id="id448" class="indexterm"></a>set of algorithms is dedicated to dimensionality<a id="id449" class="indexterm"></a> reduction; we will cover these in later chapters.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec82"></a>How to do it…</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>The Spark library comes loaded with sample <code class="literal">libsvm</code> data. We will use this and load the data into HDFS:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hdfs dfs -put /opt/infoobjects/spark/data/mllib/sample_libsvm_data.txt /user/hduser/sample_libsvm_data.txt</strong></span>
</pre></div></li><li><p>Start the Spark shell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell</strong></span>
</pre></div></li><li><p>Perform the required imports:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.classification.SVMWithSGD</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.regression.LabeledPoint</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.linalg.Vectors</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.util.MLUtils</strong></span>
</pre></div></li><li><p>Load the data as the RDD:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val svmData = MLUtils.loadLibSVMFile(sc,"sample_libsvm_data.txt")</strong></span>
</pre></div></li><li><p>Count the number of records:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; svmData.count</strong></span>
</pre></div></li><li><p>Now let's divide the dataset into half training data and half testing data:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val trainingAndTest = svmData.randomSplit(Array(0.5,0.5))</strong></span>
</pre></div></li><li><p>Assign the <code class="literal">training</code> and <code class="literal">test</code> data:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val trainingData = trainingAndTest(0)</strong></span>
<span class="strong"><strong>scala&gt; val testData = trainingAndTest(1)</strong></span>
</pre></div></li><li><p>Train the algorithm and build the model for 100 iterations (you can try different iterations but you will see that, at a certain point, the results start to converge and that is a good number to choose):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val model = SVMWithSGD.train(trainingData,100)</strong></span>
</pre></div></li><li><p>Now we can use this model to predict a label for any dataset. Let's predict the label for the first point in the test data:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val label = model.predict(testData.first.features)</strong></span>
</pre></div></li><li><p>Let's <a id="id450" class="indexterm"></a>create a tuple that has the first <a id="id451" class="indexterm"></a>value as a prediction for test data and a second value actual label, which will help us compute the accuracy of our algorithm:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val predictionsAndLabels = testData.map( r =&gt; (model.predict(r.features),r.label))</strong></span>
</pre></div></li><li><p>You can count how many records have prediction and actual label mismatches:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; predictionsAndLabels.filter(p =&gt; p._1 != p._2).count</strong></span>
</pre></div></li></ol></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec59"></a>Doing classification using decision trees</h2></div></div><hr /></div><p>Decision trees are the most intuitive among machine learning algorithms. We use decision trees in daily life all the time.</p><p>Decision tree algorithms have a lot of useful features:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Easy to understand and interpret</p></li><li style="list-style-type: disc"><p>Work with both categorical and continuous features</p></li><li style="list-style-type: disc"><p>Work with missing features</p></li><li style="list-style-type: disc"><p>Do not require feature scaling</p></li></ul></div><p>Decision tree<a id="id452" class="indexterm"></a> algorithms work in an upside-down <a id="id453" class="indexterm"></a>order in which an expression containing a feature is evaluated at every level and that splits the dataset into two categories. We'll help you understand this with the simple example of a dumb charade, which most of us played in college. I guessed an animal and asked my coworker ask me questions to work out my choice. Here's how her questioning went:</p><p>Q1: Is it a big animal?</p><p>A: Yes</p><p>Q2: Does this animal live more than 40 years?</p><p>A: Yes</p><p>Q3: Is this animal an elephant?</p><p>A: Yes</p><p>This is an obviously oversimplified case in which she knew I had postulated an elephant (what else <a id="id454" class="indexterm"></a>would you guess in a Big Data world?). Let's <a id="id455" class="indexterm"></a>expand this example to include some more animals as in the following figure (grayed boxes are classes):</p><div class="mediaobject"><img src="graphics/3056_08_31.jpg" /></div><p>The preceding example is a case of multiclass classification. In this recipe, we are going to focus on binary classification.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec83"></a>Getting ready</h3></div></div></div><p>Whenever our son has to take tennis lessons in the morning, the night before the instructor checks the weather reports and decides whether the next morning would be good to play tennis. This recipe will use this example to build a decision tree.</p><p>Let's decide on the features of weather that affect the decision whether to play tennis in the morning or not:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Rain</p></li><li style="list-style-type: disc"><p>Wind speed</p></li><li style="list-style-type: disc"><p>Temperature</p></li></ul></div><p>Let's build a table of the different combinations:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Rain</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Windy</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Temperature</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Play tennis?</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>Yes</p>
</td><td style="" align="left" valign="top">
<p>Yes</p>
</td><td style="" align="left" valign="top">
<p>Hot</p>
</td><td style="" align="left" valign="top">
<p>No</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Yes</p>
</td><td style="" align="left" valign="top">
<p>Yes</p>
</td><td style="" align="left" valign="top">
<p>Normal</p>
</td><td style="" align="left" valign="top">
<p>No</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Yes</p>
</td><td style="" align="left" valign="top">
<p>Yes</p>
</td><td style="" align="left" valign="top">
<p>Cool</p>
</td><td style="" align="left" valign="top">
<p>No</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>No</p>
</td><td style="" align="left" valign="top">
<p>Yes</p>
</td><td style="" align="left" valign="top">
<p>Hot</p>
</td><td style="" align="left" valign="top">
<p>No</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>No</p>
</td><td style="" align="left" valign="top">
<p>Yes</p>
</td><td style="" align="left" valign="top">
<p>Cool</p>
</td><td style="" align="left" valign="top">
<p>No</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>No</p>
</td><td style="" align="left" valign="top">
<p>No</p>
</td><td style="" align="left" valign="top">
<p>Hot</p>
</td><td style="" align="left" valign="top">
<p>Yes</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>No</p>
</td><td style="" align="left" valign="top">
<p>No</p>
</td><td style="" align="left" valign="top">
<p>Normal</p>
</td><td style="" align="left" valign="top">
<p>Yes</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>No</p>
</td><td style="" align="left" valign="top">
<p>No</p>
</td><td style="" align="left" valign="top">
<p>Cool</p>
</td><td style="" align="left" valign="top">
<p>No</p>
</td></tr></tbody></table></div><p>Now how<a id="id456" class="indexterm"></a> do we build a decision tree? We <a id="id457" class="indexterm"></a>can start with one of three features: rain, windy, or temperature. The rule is to start with a feature so that the maximum information gain is possible.</p><p>On a rainy day, as you can see in the table, other features do not matter and there is no play. The same is true for high wind velocity.</p><p>Decision trees, like most other algorithms, take feature values only as double values. So, let's do the mapping:</p><div class="mediaobject"><img src="graphics/3056_08_32.jpg" /></div><p>The positive class is 1.0 and the negative class is 0.0. Let's load the data using the CSV format using the first value as a label:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$vi tennis.csv</strong></span>
<span class="strong"><strong>0.0,1.0,1.0,2.0</strong></span>
<span class="strong"><strong>0.0,1.0,1.0,1.0</strong></span>
<span class="strong"><strong>0.0,1.0,1.0,0.0</strong></span>
<span class="strong"><strong>0.0,0.0,1.0,2.0</strong></span>
<span class="strong"><strong>0.0,0.0,1.0,0.0</strong></span>
<span class="strong"><strong>1.0,0.0,0.0,2.0</strong></span>
<span class="strong"><strong>1.0,0.0,0.0,1.0</strong></span>
<span class="strong"><strong>0.0,0.0,0.0,0.0</strong></span>
</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec84"></a>How to do it…</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Start the Spark shell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell</strong></span>
</pre></div></li><li><p>Perform <a id="id458" class="indexterm"></a>the required imports:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.tree.DecisionTree</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.regression.LabeledPoint</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.linalg.Vectors</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.tree.configuration.Algo._</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.tree.impurity.Entropy</strong></span>
</pre></div></li><li><p>Load the<a id="id459" class="indexterm"></a> file:</p><div class="informalexample"><pre class="programlisting">scala&gt; val data = sc.textFile("tennis.csv")</pre></div></li><li><p>Parse the data and load it into <code class="literal">LabeledPoint</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val parsedData = data.map {</strong></span>
<span class="strong"><strong>line =&gt;  val parts = line.split(',').map(_.toDouble)</strong></span>
<span class="strong"><strong> LabeledPoint(parts(0), Vectors.dense(parts.tail)) }</strong></span>
</pre></div></li><li><p>Train the algorithm with this data:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val model = DecisionTree.train(parsedData, Classification, Entropy, 3)</strong></span>
</pre></div></li><li><p>Create a vector for no rain, high wind, and a cool temperature:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val v=Vectors.dense(0.0,1.0,0.0)</strong></span>
</pre></div></li><li><p>Predict whether tennis should be played:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; model.predict(v)</strong></span>
</pre></div></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec85"></a>How it works…</h3></div></div></div><p>Let's draw the decision tree for tennis that we created in this recipe:</p><div class="mediaobject"><img src="graphics/3056_08_33.jpg" /></div><p>This model has a depth of three levels. Which attribute to select depends upon how we can maximize<a id="id460" class="indexterm"></a> information gain. The way it is <a id="id461" class="indexterm"></a>measured is by measuring the purity of the split. Purity means that, whether or not certainty is increasing, then that given dataset will be considered as positive or negative. In this example, this equates to whether the chances of play are increasing or the chances of not playing are increasing.</p><p>Purity is measured using entropy. Entropy is a measure of disorder in a system. In this context, it is easier to understand it as a measure of uncertainty:</p><div class="mediaobject"><img src="graphics/3056_08_34.jpg" /></div><p>The highest level of purity is 0 and the lowest is 1. Let's try to determine the purity using the formula.</p><p>When rain is yes, the probability of playing tennis is <span class="emphasis"><em>p+</em></span> is 0/3 = 0. The probability of not playing tennis <span class="emphasis"><em>p_</em></span> is 3/3 = 1:</p><div class="mediaobject"><img src="graphics/3056_08_35.jpg" /></div><p>This is a pure set.</p><p>When rain is <a id="id462" class="indexterm"></a>a no, the probability of playing<a id="id463" class="indexterm"></a> tennis is <span class="emphasis"><em>p+</em></span> is 2/5 = 0.4. The probability of not playing tennis <span class="emphasis"><em>p_</em></span> is 3/5 = 0.6:</p><div class="mediaobject"><img src="graphics/3056_08_36.jpg" /></div><p>This is almost an impure set. The most impure would be the case where the probability is 0.5.</p><p>Spark uses three measures to determine impurity:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Gini impurity (classification)</p></li><li style="list-style-type: disc"><p>Entropy (classification)</p></li><li style="list-style-type: disc"><p>Variance (regression)</p></li></ul></div><p>Information gain is the difference between the parent node impurity and the weighted sum of two child node impurities. Let's look at the first split, which partitions data of size eight to two datasets of size three (left) and five (right). Let's call the first split <span class="emphasis"><em>s1</em></span>, the parent node <span class="emphasis"><em>rain</em></span>, the left child <span class="emphasis"><em>no rain</em></span>, and the right child <span class="emphasis"><em>wind</em></span>. So the information gain would be:</p><div class="mediaobject"><img src="graphics/3056_08_37.jpg" /></div><p>As we <a id="id464" class="indexterm"></a>calculated impurity for <span class="emphasis"><em>no rain</em></span> and <span class="emphasis"><em>wind</em></span> already <a id="id465" class="indexterm"></a>for the entropy, let's calculate the entropy for <span class="emphasis"><em>rain</em></span>:</p><div class="mediaobject"><img src="graphics/3056_08_38.jpg" /></div><p>Let's calculate the information gain now:</p><div class="mediaobject"><img src="graphics/3056_08_39.jpg" /></div><p>So the information gain is 0.2 in the first split. Is this the best we can achieve? Let's see what our algorithm comes up with. First, let's find out the depth of the tree:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; model.depth</strong></span>
<span class="strong"><strong>Int = 2</strong></span>
</pre></div><p>Here, the depth is <code class="literal">2</code> compared to the <code class="literal">3</code> we intuitively built, so this model seems to be better optimized. Let's look at the structure of the tree:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; model.toDebugString</strong></span>
<span class="strong"><strong>String =  "DecisionTreeModel classifier of depth 2 with 5 nodes</strong></span>
<span class="strong"><strong>If (feature 1 &lt;= 0.0)</strong></span>
<span class="strong"><strong>   If (feature 2 &lt;= 0.0)</strong></span>
<span class="strong"><strong>     Predict: 0.0</strong></span>
<span class="strong"><strong>   Else (feature 2 &gt; 0.0)</strong></span>
<span class="strong"><strong>     Predict: 1.0</strong></span>
<span class="strong"><strong>Else (feature 1 &gt; 0.0)</strong></span>
<span class="strong"><strong>    Predict: 0.0</strong></span>
</pre></div><p>Let's <a id="id466" class="indexterm"></a>build it visually to get a better understanding:</p><div class="mediaobject"><img src="graphics/3056_08_40.jpg" /></div><p>We will<a id="id467" class="indexterm"></a> not go into detail here as we already did this with the previous model. We will straightaway calculate the information gain: 0.44</p><p>As you can see in this case, the information gain is 0.44, which is more than double the first model.</p><p>If you look at the second level nodes, the impurity is zero. In this case, it is great as we got it at a depth of 2. Image a situation in which the depth is 50. In that case, the decision tree would work <a id="id468" class="indexterm"></a>well for training data and would do badly for test data. This situation is called <span class="strong"><strong>overfitting</strong></span>.</p><p>One solution<a id="id469" class="indexterm"></a> to avoid overfitting is pruning. You <a id="id470" class="indexterm"></a>divide your training data into two sets: the training set and validation set. You train the model using the training set. Now you test with the model against the validation set by slowly removing the left nodes. If removing the leaf node (which is mostly a singleton—that is, it contains only one data point) improves the performance of the model, this leaf node is pruned from the model.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec60"></a>Doing classification using Random Forests</h2></div></div><hr /></div><p>Sometimes one decision tree is not enough, so a set of decision trees is used to produce more powerful models. These<a id="id471" class="indexterm"></a> are called <span class="strong"><strong>ensemble learning algorithms</strong></span>. Ensemble learning algorithms are not limited to using decision trees as base models.</p><p>The most<a id="id472" class="indexterm"></a> popular among the ensemble<a id="id473" class="indexterm"></a> learning algorithms is Random Forest. In Random Forest, rather than growing one single tree, <span class="emphasis"><em>K</em></span> trees are grown. Every tree is given a random subset <span class="emphasis"><em>S</em></span> of training data. To add a twist to it, every tree only uses a subset of features. When it comes to making predictions, a majority vote is done on the trees and that becomes the prediction.</p><p>Let's explain this with an example. The goal is to make a prediction for a given person about whether he/she has good credit or bad credit.</p><p>To do this, we will provide labeled training data—that is, in this case, a person with features and labels whether he/she has good credit or bad credit. Now we do not want to create feature bias so we will provide a randomly selected set of features. There is another reason to provide a randomly selected subset of features and that is because most real-world data has hundreds if not thousands of features. Text classification algorithms, for example, typically have 50k-100k features.</p><p>In this case, to add flavor to the story we are not going to provide features, but we will ask different people why they think a person has good or bad credit. Now by definition, different people are exposed to different features (sometimes overlapping) of a person, which gives us the same functionality as randomly selected features.</p><p>Our first example is Jack who carries a label "bad credit." We will start with Joey who works at Jack's favorite bar, the Elephant Bar. The only way a person can deduce why a given label was given is by asking yes/no questions. Let's see what Joey says:</p><p>Q1: Does Jack tip well? (Feature: generosity)</p><p>A: No</p><p>Q2: Does Jack spend at least $60 per visit? (Feature: spendthrift)</p><p>A: Yes</p><p>Q3: Does he tend to get into bar fights even at the smallest provocation? (Feature: volatile)</p><p>A: Yes</p><p>That explains why Jack has bad credit.</p><p>We now ask Jack's girlfriend, Stacey:</p><p>Q1: When <a id="id474" class="indexterm"></a>we hangout, does Jack always <a id="id475" class="indexterm"></a>cover the bill? (Feature: generosity)</p><p>A: No</p><p>Q2: Has Jack paid me back the $500 he owes me? (Feature: responsibility)</p><p>A: No</p><p>Q3: Does he overspend sometimes just to show off? (Feature: spendthrift)</p><p>A: Yes</p><p>That explains why Jack has bad credit.</p><p>We now ask Jack's best friend George:</p><p>Q1: When Jack and I hang out at my apartment, does he clean up after himself? (Feature: organized)</p><p>A: No</p><p>Q2: Did Jack arrive empty-handed during my Super Bowl potluck? (Feature: care)</p><p>A: Yes</p><p>Q3: Has he used the "I forgot my wallet at home" excuse for me to cover his tab at restaurants? (Feature: responsibility)</p><p>A: Yes</p><p>That explains why Jack has bad credit.</p><p>Now we talk about Jessica who has good credit. Let's ask Stacey who happens to be Jessica's sister:</p><p>Q1: Whenever I run short of money, does Jessica offer to help? (Feature: generosity)</p><p>A: Yes</p><p>Q2: Does Jessica pay her bills on time? (Feature: responsibility)</p><p>A: Yes</p><p>Q3: Does Jessica offer to babysit my child? (Feature: care)</p><p>A: Yes</p><p>That explains why Jessica has good credit.</p><p>Now we ask George who happens to be her husband:</p><p>Q1: Does <a id="id476" class="indexterm"></a>Jessica keep the house tidy? (Feature: organized)</p><p>A: Yes</p><p>Q2: Does she <a id="id477" class="indexterm"></a>expect expensive gifts? (Feature: spendthrift)</p><p>A: No</p><p>Q3: Does she get upset when you forget to mow the lawn? (Feature: volatile)</p><p>A: No</p><p>That explains why Jessica has good credit.</p><p>Now let's ask Joey, the bartender at the Elephant Bar:</p><p>Q1: Whenever she comes to the bar with friends, is she mostly the designated driver? (Feature: responsible)</p><p>A: Yes</p><p>Q2: Does she always take leftovers home? (Feature: spendthrift)</p><p>A: Yes</p><p>Q3: Does she tip well? (Feature: generosity)</p><p>A: Yes</p><p>The way Random Forest works is that it does random selection on two levels:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>A subset of the data</p></li><li style="list-style-type: disc"><p>A subset of features to split that data</p></li></ul></div><p>Both these subsets can overlap.</p><p>In our example, we have six features and we are going to assign three features to each tree. This way, there is a good chance we will have an overlap.</p><p>Let's add eight more people to our training dataset:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /><col align="left" /><col align="left" /><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Names</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Label</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Generosity</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Responsibility</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Care</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Organization</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Spendthrift</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Volatile</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>Jack</p>
</td><td style="" align="left" valign="top">
<p>0</p>
</td><td style="" align="left" valign="top">
<p>0</p>
</td><td style="" align="left" valign="top">
<p>0</p>
</td><td style="" align="left" valign="top">
<p>0</p>
</td><td style="" align="left" valign="top">
<p>0</p>
</td><td style="" align="left" valign="top">
<p>1</p>
</td><td style="" align="left" valign="top">
<p>1</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Jessica</p>
</td><td style="" align="left" valign="top">
<p>1</p>
</td><td style="" align="left" valign="top">
<p>1</p>
</td><td style="" align="left" valign="top">
<p>1</p>
</td><td style="" align="left" valign="top">
<p>1</p>
</td><td style="" align="left" valign="top">
<p>1</p>
</td><td style="" align="left" valign="top">
<p>0</p>
</td><td style="" align="left" valign="top">
<p>0</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Jenny</p>
</td><td style="" align="left" valign="top">
<p>0</p>
</td><td style="" align="left" valign="top">
<p>0</p>
</td><td style="" align="left" valign="top">
<p>0</p>
</td><td style="" align="left" valign="top">
<p>1</p>
</td><td style="" align="left" valign="top">
<p>0</p>
</td><td style="" align="left" valign="top">
<p>1</p>
</td><td style="" align="left" valign="top">
<p>1</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Rick</p>
</td><td style="" align="left" valign="top">
<p>1</p>
</td><td style="" align="left" valign="top">
<p>1</p>
</td><td style="" align="left" valign="top">
<p>1</p>
</td><td style="" align="left" valign="top">
<p>0</p>
</td><td style="" align="left" valign="top">
<p>1</p>
</td><td style="" align="left" valign="top">
<p>0</p>
</td><td style="" align="left" valign="top">
<p>0</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Pat</p>
</td><td style="" align="left" valign="top">
<p>0</p>
</td><td style="" align="left" valign="top">
<p>0</p>
</td><td style="" align="left" valign="top">
<p>0</p>
</td><td style="" align="left" valign="top">
<p>0</p>
</td><td style="" align="left" valign="top">
<p>0</p>
</td><td style="" align="left" valign="top">
<p>1</p>
</td><td style="" align="left" valign="top">
<p>1</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Jeb</p>
</td><td style="" align="left" valign="top">
<p>1</p>
</td><td style="" align="left" valign="top">
<p>1</p>
</td><td style="" align="left" valign="top">
<p>1</p>
</td><td style="" align="left" valign="top">
<p>1</p>
</td><td style="" align="left" valign="top">
<p>0</p>
</td><td style="" align="left" valign="top">
<p>0</p>
</td><td style="" align="left" valign="top">
<p>0</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Jay</p>
</td><td style="" align="left" valign="top">
<p>1</p>
</td><td style="" align="left" valign="top">
<p>0</p>
</td><td style="" align="left" valign="top">
<p>1</p>
</td><td style="" align="left" valign="top">
<p>1</p>
</td><td style="" align="left" valign="top">
<p>1</p>
</td><td style="" align="left" valign="top">
<p>0</p>
</td><td style="" align="left" valign="top">
<p>0</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Nat</p>
</td><td style="" align="left" valign="top">
<p>0</p>
</td><td style="" align="left" valign="top">
<p>1</p>
</td><td style="" align="left" valign="top">
<p>0</p>
</td><td style="" align="left" valign="top">
<p>0</p>
</td><td style="" align="left" valign="top">
<p>0</p>
</td><td style="" align="left" valign="top">
<p>1</p>
</td><td style="" align="left" valign="top">
<p>1</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Ron</p>
</td><td style="" align="left" valign="top">
<p>1</p>
</td><td style="" align="left" valign="top">
<p>0</p>
</td><td style="" align="left" valign="top">
<p>1</p>
</td><td style="" align="left" valign="top">
<p>1</p>
</td><td style="" align="left" valign="top">
<p>1</p>
</td><td style="" align="left" valign="top">
<p>0</p>
</td><td style="" align="left" valign="top">
<p>0</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Mat</p>
</td><td style="" align="left" valign="top">
<p>0</p>
</td><td style="" align="left" valign="top">
<p>1</p>
</td><td style="" align="left" valign="top">
<p>0</p>
</td><td style="" align="left" valign="top">
<p>0</p>
</td><td style="" align="left" valign="top">
<p>0</p>
</td><td style="" align="left" valign="top">
<p>1</p>
</td><td style="" align="left" valign="top">
<p>1</p>
</td></tr></tbody></table></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec86"></a>Getting ready</h3></div></div></div><p>Let's <a id="id478" class="indexterm"></a>put the data we created into the <code class="literal">libsvm</code> format<a id="id479" class="indexterm"></a> in the following file:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>rf_libsvm_data.txt</strong></span>
<span class="strong"><strong>0 5:1 6:1</strong></span>
<span class="strong"><strong>1 1:1 2:1 3:1 4:1</strong></span>
<span class="strong"><strong>0 3:1 5:1 6:1</strong></span>
<span class="strong"><strong>1 1:1 2:1 4:1</strong></span>
<span class="strong"><strong>0 5:1 6:1</strong></span>
<span class="strong"><strong>1 1:1 2:1 3:1 4:1</strong></span>
<span class="strong"><strong>0 1:1 5:1 6:1</strong></span>
<span class="strong"><strong>1 2:1 3:1 4:1</strong></span>
<span class="strong"><strong>0 1:1 5:1 6:1</strong></span>
</pre></div><p>Now upload it to HDFS:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hdfs dfs -put rf_libsvm_data.txt</strong></span>
</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec87"></a>How to do it…</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Start the Spark shell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell</strong></span>
</pre></div></li><li><p>Perform the required imports:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.tree.RandomForest</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.tree.configuration.Strategy</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.util.MLUtils</strong></span>
</pre></div></li><li><p>Load and parse the data:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val data =</strong></span>
<span class="strong"><strong>  MLUtils.loadLibSVMFile(sc, "rf_libsvm_data.txt")</strong></span>
</pre></div></li><li><p>Split the data into the <code class="literal">training</code> and <code class="literal">test</code> datasets:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val splits = data.randomSplit(Array(0.7, 0.3))</strong></span>
<span class="strong"><strong>scala&gt; val (trainingData, testData) = (splits(0), splits(1))</strong></span>
</pre></div></li><li><p>Create <a id="id480" class="indexterm"></a>a classification as a tree <a id="id481" class="indexterm"></a>strategy (Random Forest also supports regression):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val treeStrategy = Strategy.defaultStrategy("Classification")</strong></span>
</pre></div></li><li><p>Train the model:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val model = RandomForest.trainClassifier(trainingData,</strong></span>
<span class="strong"><strong>  treeStrategy, numTrees=3, featureSubsetStrategy="auto", seed = 12345)</strong></span>
</pre></div></li><li><p>Evaluate the model on test instances and compute the test error:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val testErr = testData.map { point =&gt;</strong></span>
<span class="strong"><strong>  val prediction = model.predict(point.features)</strong></span>
<span class="strong"><strong>  if (point.label == prediction) 1.0 else 0.0</strong></span>
<span class="strong"><strong>}.mean()</strong></span>
<span class="strong"><strong>scala&gt; println("Test Error = " + testErr)</strong></span>
</pre></div></li><li><p>Check the model:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; println("Learned Random Forest:n" + model.toDebugString)</strong></span>
<span class="strong"><strong>Learned Random Forest:nTreeEnsembleModel classifier with 3 trees</strong></span>
<span class="strong"><strong>    Tree 0:</strong></span>
<span class="strong"><strong>  If (feature 5 &lt;= 0.0)</strong></span>
<span class="strong"><strong>    Predict: 1.0</strong></span>
<span class="strong"><strong>  Else (feature 5 &gt; 0.0)</strong></span>
<span class="strong"><strong>    Predict: 0.0</strong></span>
<span class="strong"><strong>    Tree 1:</strong></span>
<span class="strong"><strong>      If (feature 3 &lt;= 0.0)</strong></span>
<span class="strong"><strong>       Predict: 0.0</strong></span>
<span class="strong"><strong>     Else (feature 3 &gt; 0.0)</strong></span>
<span class="strong"><strong>      Predict: 1.0</strong></span>
<span class="strong"><strong>   Tree 2:</strong></span>
<span class="strong"><strong>      If (feature 0 &lt;= 0.0)</strong></span>
<span class="strong"><strong>       Predict: 0.0</strong></span>
<span class="strong"><strong>     Else (feature 0 &gt; 0.0)</strong></span>
<span class="strong"><strong>      Predict: 1.0</strong></span>
</pre></div></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec88"></a>How it works…</h3></div></div></div><p>As you <a id="id482" class="indexterm"></a>can see in such a small example, three<a id="id483" class="indexterm"></a> trees are using different features. In real-world use cases with thousands of features and training data, this would not happen, but most of the trees would differ in how they look at features and the vote of the majority will win. Please remember that, in the case of regression, averaging is done over trees to get a final value.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec61"></a>Doing classification using Gradient Boosted Trees</h2></div></div><hr /></div><p>Another <a id="id484" class="indexterm"></a>ensemble learning algorithm is <span class="strong"><strong>Gradient Boosted Trees</strong></span> (<span class="strong"><strong>GBTs</strong></span>). GBTs train one tree at a time, where each new tree improves<a id="id485" class="indexterm"></a> upon the shortcomings <a id="id486" class="indexterm"></a>of previously trained trees.</p><p>As GBTs train one tree at a time, they can take longer than Random Forest.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec89"></a>Getting ready</h3></div></div></div><p>We are going to use the same data we used in the previous recipe.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec90"></a>How to do it…</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Start the Spark shell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell</strong></span>
</pre></div></li><li><p>Perform the required imports:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.tree.GradientBoostedTrees</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.tree.configuration.BoostingStrategy</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.util.MLUtils</strong></span>
</pre></div></li><li><p>Load and parse the data:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val data =</strong></span>
<span class="strong"><strong>  MLUtils.loadLibSVMFile(sc, "rf_libsvm_data.txt")</strong></span>
</pre></div></li><li><p>Split the data into <code class="literal">training</code> and <code class="literal">test</code> datasets:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val splits = data.randomSplit(Array(0.7, 0.3))</strong></span>
<span class="strong"><strong>scala&gt; val (trainingData, testData) = (splits(0), splits(1))</strong></span>
</pre></div></li><li><p>Create <a id="id487" class="indexterm"></a>a classification<a id="id488" class="indexterm"></a> as a boosting strategy and set the number of iterations to <code class="literal">3</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val boostingStrategy =</strong></span>
<span class="strong"><strong>  BoostingStrategy.defaultParams("Classification")</strong></span>
<span class="strong"><strong>scala&gt; boostingStrategy.numIterations = 3</strong></span>
</pre></div></li><li><p>Train the model:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val model = GradientBoostedTrees.train(trainingData, boostingStrategy)</strong></span>
</pre></div></li><li><p>Evaluate the model on the test instances and compute the test error:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val testErr = testData.map { point =&gt;</strong></span>
<span class="strong"><strong>  val prediction = model.predict(point.features)</strong></span>
<span class="strong"><strong>  if (point.label == prediction) 1.0 else 0.0</strong></span>
<span class="strong"><strong>}.mean()</strong></span>
<span class="strong"><strong>scala&gt; println("Test Error = " + testErr)</strong></span>
</pre></div></li><li><p>Check the model:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; println("Learned Random Forest:n" + model.toDebugString)</strong></span>
</pre></div></li></ol></div><p>In this case, the accuracy of the model is 0.9, which is less than what we got in the case of Random Forest.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec62"></a>Doing classification with Naïve Bayes</h2></div></div><hr /></div><p>Let's consider building an e-mail spam filter using machine learning. Here we are interested in<a id="id489" class="indexterm"></a> two classes: spam for unsolicited <a id="id490" class="indexterm"></a>messages and non-spam for regular emails:</p><div class="mediaobject"><img src="graphics/3056_08_42.jpg" /></div><p>The first challenge is that, when given an e-mail, how do we represent it as feature vector <span class="emphasis"><em>x</em></span>. An e-mail is just bunch of text or a collection of words (therefore, this problem domain falls into a broader category called <span class="strong"><strong>text classification</strong></span>). Let's represent an e-mail with a feature vector<a id="id491" class="indexterm"></a> with the length equal to the size of the dictionary. If a given word in a dictionary appears in an e-mail, the value will be 1; otherwise 0. Let's build a vector representing e-mail with the content <span class="emphasis"><em>online pharmacy sale</em></span>:</p><div class="mediaobject"><img src="graphics/3056_08_43.jpg" /></div><p>The <a id="id492" class="indexterm"></a>dictionary of words in this feature vector is<a id="id493" class="indexterm"></a> called <span class="emphasis"><em>vocabulary</em></span> and the dimensions of the vector are the same as the size of vocabulary. If the vocabulary size is 10,000, the possible values in this feature vector will be 210,000.</p><p>Our goal is to model the probability of <span class="emphasis"><em>x</em></span> given <span class="emphasis"><em>y</em></span>. To model <span class="emphasis"><em>P(x|y)</em></span>, we will make a strong assumption, and that assumption is that <span class="emphasis"><em>x</em></span>'s are conditionally independent. This assumption is <a id="id494" class="indexterm"></a>called the <span class="strong"><strong>Naïve Bayes assumption</strong></span> and the <a id="id495" class="indexterm"></a>algorithm based on this assumption is called the <span class="strong"><strong>Naïve Bayes classifier</strong></span>.</p><p>For example, for <span class="emphasis"><em>y =1</em></span>, which means spam, the probability of "online" appearing and "pharmacy appearing" are independent. This is a strong assumption that has nothing to do with reality but works out really well when it comes to getting good predictions.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec91"></a>Getting ready</h3></div></div></div><p>Spark comes bundled with a sample dataset to use with Naïve Bayes. Let's load this dataset to HDFS:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hdfs dfs -put /opt/infoobjects/spark/data/mllib/sample_naive_bayes_data.txt</strong></span>
<span class="strong"><strong> sample_naive_bayes_data.txt</strong></span>
</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec92"></a>How to do it…</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Start the Spark shell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell</strong></span>
</pre></div></li><li><p>Perform the required imports:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.classification.NaiveBayes</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.linalg.Vectors</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.regression.LabeledPoint</strong></span>
</pre></div></li><li><p>Load the data into RDD:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val data = sc.textFile("sample_naive_bayes_data.txt")</strong></span>
</pre></div></li><li><p>Parse the<a id="id496" class="indexterm"></a> data into <code class="literal">LabeledPoint</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val parsedData = data.map { line =&gt;</strong></span>
<span class="strong"><strong>  val parts = line.split(',')</strong></span>
<span class="strong"><strong>  LabeledPoint(parts(0).toDouble, Vectors.dense(parts(1).split(' ').map(_.toDouble)))</strong></span>
<span class="strong"><strong>}</strong></span>
</pre></div></li><li><p>Split the <a id="id497" class="indexterm"></a>data half and half into the <code class="literal">training</code> and <code class="literal">test</code> datasets:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val splits = parsedData.randomSplit(Array(0.5, 0.5), seed = 11L)</strong></span>
<span class="strong"><strong>scala&gt; val training = splits(0)</strong></span>
<span class="strong"><strong>scala&gt; val test = splits(1)</strong></span>
</pre></div></li><li><p>Train the model with the <code class="literal">training</code> dataset:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>val model = NaiveBayes.train(training, lambda = 1.0)</strong></span>
</pre></div></li><li><p>Predict the label of the <code class="literal">test</code> dataset:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>val predictionAndLabel = test.map(p =&gt; (model.predict(p.features), p.label))</strong></span>
</pre></div></li></ol></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch09"></a>Chapter 9. Unsupervised Learning with MLlib</h2></div></div></div><p>This chapter will cover how we can do unsupervised learning using MLlib, Spark's machine learning library.</p><p>This chapter is divided into the following recipes:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Clustering using k-means</p></li><li style="list-style-type: disc"><p>Dimensionality reduction with principal component analysis</p></li><li style="list-style-type: disc"><p>Dimensionality reduction with singular value decomposition</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec63"></a>Introduction</h2></div></div><hr /></div><p>The following<a id="id498" class="indexterm"></a> is Wikipedia's definition of unsupervised learning:</p><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>"In machine learning, the problem of unsupervised learning is that of trying to find hidden structure in unlabeled data."</em></span></p></blockquote></div><p>In contrast to <a id="id499" class="indexterm"></a>supervised learning where we have labeled data to train an algorithm, in unsupervised learning we ask the algorithm to find a structure on its own. Let's take a look at the following sample dataset:</p><div class="mediaobject"><img src="graphics/3056_09_01.jpg" /></div><p>As you can see from the preceding graph, the data points are forming two clusters as follows:</p><div class="mediaobject"><img src="graphics/3056_09_02.jpg" /></div><p>In fact, clustering<a id="id500" class="indexterm"></a> is the most common type of unsupervised learning algorithm.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec64"></a>Clustering using k-means</h2></div></div><hr /></div><p>Cluster analysis or <a id="id501" class="indexterm"></a>clustering is the process of grouping data into multiple groups <a id="id502" class="indexterm"></a>so that the data in one group is similar to the data in other groups.</p><p>The following are<a id="id503" class="indexterm"></a> a few examples where clustering is used:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><span class="strong"><strong>Market segmentation</strong></span>: Dividing the target market into multiple segments so that the <a id="id504" class="indexterm"></a>needs of each segment can be served better</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Social network analysis</strong></span>: Finding a coherent group of people in the social network <a id="id505" class="indexterm"></a>for ad targeting through a social networking site such as Facebook</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Data center computing clusters</strong></span>: Putting a set of computers together to improve<a id="id506" class="indexterm"></a> performance</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Astronomical data analysis</strong></span>: Understanding astronomical data and events <a id="id507" class="indexterm"></a>such as galaxy formations</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Real estate</strong></span>: Identifying <a id="id508" class="indexterm"></a>neighborhoods based on similar features</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Text analysis</strong></span>: Dividing<a id="id509" class="indexterm"></a> text documents, such as novels or essays, into genres</p></li></ul></div><p>The k-means <a id="id510" class="indexterm"></a>algorithm is best illustrated using imagery, so let's<a id="id511" class="indexterm"></a> look at our sample figure again:</p><div class="mediaobject"><img src="graphics/3056_09_01.jpg" /></div><p>The first step in k-means<a id="id512" class="indexterm"></a> is to randomly select two points called <span class="strong"><strong>cluster centroids</strong></span>:</p><div class="mediaobject"><img src="graphics/3056_09_03.jpg" /></div><p>The k-means algorithm<a id="id513" class="indexterm"></a> is an iterative algorithm and<a id="id514" class="indexterm"></a> works in two steps:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><span class="strong"><strong>Cluster assignment step</strong></span>: This algorithm will go through each data point and, depending <a id="id515" class="indexterm"></a>upon which centroid it is nearer to, it will be assigned that centroid and, in turn, the cluster it represents</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Move centroid step</strong></span>: This algorithm will take each centroid and move it to the<a id="id516" class="indexterm"></a> mean of the data points in the cluster</p></li></ul></div><p>Let's see how our data looks after the cluster assignment:</p><div class="mediaobject"><img src="graphics/3056_09_04.jpg" /></div><p>Now let's<a id="id517" class="indexterm"></a> move the cluster centroids to the mean value of the<a id="id518" class="indexterm"></a> data points in a cluster, as follows:</p><div class="mediaobject"><img src="graphics/3056_09_05.jpg" /></div><p>In this <a id="id519" class="indexterm"></a>case, one iteration is enough and further iterations <a id="id520" class="indexterm"></a>will not move the cluster centroids. For most real data, multiple iterations are required to move the centroid to the final position.</p><p>The k-means algorithm takes a number of clusters as input.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec93"></a>Getting ready</h3></div></div></div><p>Let's use some different housing data from the City of Saratoga, CA. This time, we are going to take lot size and house price:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Lot size</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>House price (in $1,000)</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>12839</p>
</td><td style="" align="left" valign="top">
<p>2405</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>10000</p>
</td><td style="" align="left" valign="top">
<p>2200</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>8040</p>
</td><td style="" align="left" valign="top">
<p>1400</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>13104</p>
</td><td style="" align="left" valign="top">
<p>1800</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>10000</p>
</td><td style="" align="left" valign="top">
<p>2351</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>3049</p>
</td><td style="" align="left" valign="top">
<p>795</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>38768</p>
</td><td style="" align="left" valign="top">
<p>2725</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>16250</p>
</td><td style="" align="left" valign="top">
<p>2150</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>43026</p>
</td><td style="" align="left" valign="top">
<p>2724</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>44431</p>
</td><td style="" align="left" valign="top">
<p>2675</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>40000</p>
</td><td style="" align="left" valign="top">
<p>2930</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>1260</p>
</td><td style="" align="left" valign="top">
<p>870</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>15000</p>
</td><td style="" align="left" valign="top">
<p>2210</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>10032</p>
</td><td style="" align="left" valign="top">
<p>1145</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>12420</p>
</td><td style="" align="left" valign="top">
<p>2419</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>69696</p>
</td><td style="" align="left" valign="top">
<p>2750</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>12600</p>
</td><td style="" align="left" valign="top">
<p>2035</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>10240</p>
</td><td style="" align="left" valign="top">
<p>1150</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>876</p>
</td><td style="" align="left" valign="top">
<p>665</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>8125</p>
</td><td style="" align="left" valign="top">
<p>1430</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>11792</p>
</td><td style="" align="left" valign="top">
<p>1920</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>1512</p>
</td><td style="" align="left" valign="top">
<p>1230</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>1276</p>
</td><td style="" align="left" valign="top">
<p>975</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>67518</p>
</td><td style="" align="left" valign="top">
<p>2400</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>9810</p>
</td><td style="" align="left" valign="top">
<p>1725</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>6324</p>
</td><td style="" align="left" valign="top">
<p>2300</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>12510</p>
</td><td style="" align="left" valign="top">
<p>1700</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>15616</p>
</td><td style="" align="left" valign="top">
<p>1915</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>15476</p>
</td><td style="" align="left" valign="top">
<p>2278</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>13390</p>
</td><td style="" align="left" valign="top">
<p>2497.5</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>1158</p>
</td><td style="" align="left" valign="top">
<p>725</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>2000</p>
</td><td style="" align="left" valign="top">
<p>870</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>2614</p>
</td><td style="" align="left" valign="top">
<p>730</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>13433</p>
</td><td style="" align="left" valign="top">
<p>2050</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>12500</p>
</td><td style="" align="left" valign="top">
<p>3330</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>15750</p>
</td><td style="" align="left" valign="top">
<p>1120</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>13996</p>
</td><td style="" align="left" valign="top">
<p>4100</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>10450</p>
</td><td style="" align="left" valign="top">
<p>1655</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>7500</p>
</td><td style="" align="left" valign="top">
<p>1550</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>12125</p>
</td><td style="" align="left" valign="top">
<p>2100</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>14500</p>
</td><td style="" align="left" valign="top">
<p>2100</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>10000</p>
</td><td style="" align="left" valign="top">
<p>1175</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>10019</p>
</td><td style="" align="left" valign="top">
<p>2047.5</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>48787</p>
</td><td style="" align="left" valign="top">
<p>3998</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>53579</p>
</td><td style="" align="left" valign="top">
<p>2688</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>10788</p>
</td><td style="" align="left" valign="top">
<p>2251</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>11865</p>
</td><td style="" align="left" valign="top">
<p>1906</p>
</td></tr></tbody></table></div><p>Let's<a id="id521" class="indexterm"></a> convert<a id="id522" class="indexterm"></a> this data into a <span class="strong"><strong>comma-separated value</strong></span> (<span class="strong"><strong>CSV</strong></span>) file called <code class="literal">saratoga.c</code>
<code class="literal">sv</code> and<a id="id523" class="indexterm"></a> draw it as a scatter plot:</p><div class="mediaobject"><img src="graphics/3056_09_06.jpg" /></div><p>Finding a number of clusters is a tricky task. Here, we have the advantage of visual inspection, which is not available for data on hyperplanes (more than three dimensions). Let's roughly divide the data into four clusters as follows:</p><div class="mediaobject"><img src="graphics/3056_09_07.jpg" /></div><p>We will<a id="id524" class="indexterm"></a> run the k-means algorithm to do the same and see <a id="id525" class="indexterm"></a>how close our results come.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec94"></a>How to do it…</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Load <code class="literal">sarataga.csv</code> to HDFS:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hdfs dfs -put saratoga.csv saratoga.csv</strong></span>
</pre></div></li><li><p>Start the Spark shell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell</strong></span>
</pre></div></li><li><p>Import statistics and related classes:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.linalg.Vectors</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.clustering.KMeans</strong></span>
</pre></div></li><li><p>Load <code class="literal">saratoga.csv</code> as an RDD:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val data = sc.textFile("saratoga.csv")</strong></span>
</pre></div></li><li><p>Transform the data into an RDD of dense vectors:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val parsedData = data.map( line =&gt; Vectors.dense(line.split(',').map(_.toDouble)))</strong></span>
</pre></div></li><li><p>Train the model for four clusters and five iterations:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val kmmodel= KMeans.train(parsedData,4,5)</strong></span>
</pre></div></li><li><p>Collect <code class="literal">parsedData</code> as a local scala collection:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val houses = parsedData.collect</strong></span>
</pre></div></li><li><p>Predict the cluster for the 0th element:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val prediction = kmmodel.predict(houses(0))</strong></span>
</pre></div></li><li><p>Now let's compare the cluster assignments by k-means versus the ones we have done individually. The k-means algorithm gives the cluster IDs starting from 0. Once you inspect the data, you find out the following mapping between the A to D cluster IDs we gave versus k-means: A=&gt;3, B=&gt;1, C=&gt;0, D=&gt;2.</p></li><li><p>Now, let's pick some of the data from different parts of the chart and predict which <a id="id526" class="indexterm"></a>cluster it belongs to.</p></li><li><p>Let's look<a id="id527" class="indexterm"></a> at the house (18) data, which has a lot size of 876 sq ft and is priced at $665K:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val prediction = kmmodel.predict(houses(18))</strong></span>
<span class="strong"><strong>resxx: Int = 3</strong></span>
</pre></div></li><li><p>Now, look at the data for house (35) with a lot size of 15,750 sq ft and a price of $1.12 million:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val prediction = kmmodel.predict(houses(35))</strong></span>
<span class="strong"><strong>resxx: Int = 1</strong></span>
</pre></div></li><li><p>Now look at the house (6) data, which has a lot size of 38,768 sq ft and is priced at $2.725 million:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val prediction = kmmodel.predict(houses(6))</strong></span>
<span class="strong"><strong>resxx: Int = 0</strong></span>
</pre></div></li><li><p>Now look at the house (15) data, which has a lot size of 69,696 sq ft and is priced at $2.75 million:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt;  val prediction = kmmodel.predict(houses(15))</strong></span>
<span class="strong"><strong>resxx: Int = 2</strong></span>
</pre></div></li></ol></div><p>You can test the prediction capability with more data. Let's do some neighborhood analysis to see what meaning these clusters carry. Most of the houses in cluster 3 are near downtown. The cluster 2 houses are on hilly terrain.</p><p>In this example, we dealt with a very small set of features; common sense and visual inspection would also lead us to the same conclusions. The beauty of the k-means algorithm is that it does the clustering on the data with an unlimited number of features. It is a great tool to use when you have a raw data and would like to know the patterns in that data.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec65"></a>Dimensionality reduction with principal component analysis</h2></div></div><hr /></div><p>Dimensionality reduction<a id="id528" class="indexterm"></a> is the process of reducing the number of dimensions or features. A lot of real data contains a very high number of features. It is not uncommon to have thousands of features. Now, we need to drill down to features that matter.</p><p>Dimensionality reduction<a id="id529" class="indexterm"></a> serves several purposes such as:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Data compression</p></li><li style="list-style-type: disc"><p>Visualization</p></li></ul></div><p>When the number of dimensions is reduced, it reduces the disk footprint and memory footprint. Last but not least; it helps algorithms to run much faster. It also helps reduce highly<a id="id530" class="indexterm"></a> correlated dimensions to one.</p><p>Humans can only visualize three dimensions, but data can have a much higher number of dimensions. Visualization can help find hidden patterns in the data. Dimensionality reduction helps visualization by compacting multiple features into one.</p><p>The most <a id="id531" class="indexterm"></a>popular algorithm for dimensionality reduction is <span class="strong"><strong>principal component analysis</strong></span> (<span class="strong"><strong>PCA</strong></span>).</p><p>Let's look at the following dataset:</p><div class="mediaobject"><img src="graphics/3056_09_08.jpg" /></div><p>Let's say the <a id="id532" class="indexterm"></a>goal is to<a id="id533" class="indexterm"></a> divide this two-dimensional data into one dimension. The way to do that would be to find a line on which we can project this data. Let's find a line that is good for projecting this data on:</p><div class="mediaobject"><img src="graphics/3056_09_09.jpg" /></div><p>This is <a id="id534" class="indexterm"></a>the line that has<a id="id535" class="indexterm"></a> the shortest projected distance from the data points. Let's explain it further by dropping the shortest lines from each data point to this projected line:</p><div class="mediaobject"><img src="graphics/3056_09_10.jpg" /></div><p>Another <a id="id536" class="indexterm"></a>way to look at it is<a id="id537" class="indexterm"></a> that we have to find a line to project the data on so that the sum of the square distances of the data points from this line is minimized. These <a id="id538" class="indexterm"></a>gray line segments are also called <span class="strong"><strong>projection errors</strong></span>.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec95"></a>Getting ready</h3></div></div></div><p>Let's look at the three features of the housing data of the City of Saratoga, CA—that is, house size, lot size, and price. Using PCA, we will merge the house size and lot size features into one feature— <span class="emphasis"><em>z</em></span>. Let's <a id="id539" class="indexterm"></a>call this feature <span class="strong"><strong>z density of a house</strong></span>.</p><p>It is worth noting that it is not always possible to give meaning to the new feature created. In this case, it is easy as we have only two features to combine and we can use our common sense to combine the effect of the two. In a more practical case, you may have 1,000 features that you are trying to project to 100 features. It may not be possible to give real-life meaning to each of those 100 features.</p><p>In this exercise, we will derive the housing density using PCA and then we will do linear regression to see how this density affects the house price.</p><p>There is a preprocessing stage before we <a id="id540" class="indexterm"></a>delve into PCA: <span class="strong"><strong>feature scaling</strong></span>. Feature scaling comes into the picture when two features have ranges that are at very different scales. Here, house size varies in the range of 800 sq ft to 7,000 sq ft, while the lot size varies between 800 sq ft to a few acres.</p><p>Why <a id="id541" class="indexterm"></a>did we not have to do<a id="id542" class="indexterm"></a> feature scaling before? The answer is that we really did not have to put features on a level playing field. Gradient descent is another area where feature scaling is very useful.</p><p>There are <a id="id543" class="indexterm"></a>different ways of doing feature scaling:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc">
Dividing a feature value with a maximum value that will put every feature in the <span class="inlinemediaobject"><img src="graphics/3056_09_22.jpg" /></span> range
</li><li style="list-style-type: disc"><p>Dividing a feature value with the range, that is, maximum value - minimum value</p></li><li style="list-style-type: disc"><p>Subtracting a feature value by its mean and then dividing by the range</p></li><li style="list-style-type: disc"><p>Subtracting a feature value by its mean and then dividing by the standard deviation</p></li></ul></div><p>We are going to use the fourth choice to scale in the best way possible. The following is the data we are going to use for this recipe:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>House size</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Lot size</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Scaled house size</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Scaled lot size</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>House price (in $1,000)</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>2524</p>
</td><td style="" align="left" valign="top">
<p>12839</p>
</td><td style="" align="left" valign="top">
<p>-0.025</p>
</td><td style="" align="left" valign="top">
<p>-0.231</p>
</td><td style="" align="left" valign="top">
<p>2405</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>2937</p>
</td><td style="" align="left" valign="top">
<p>10000</p>
</td><td style="" align="left" valign="top">
<p>0.323</p>
</td><td style="" align="left" valign="top">
<p>-0.4</p>
</td><td style="" align="left" valign="top">
<p>2200</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>1778</p>
</td><td style="" align="left" valign="top">
<p>8040</p>
</td><td style="" align="left" valign="top">
<p>-0.654</p>
</td><td style="" align="left" valign="top">
<p>-0.517</p>
</td><td style="" align="left" valign="top">
<p>1400</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>1242</p>
</td><td style="" align="left" valign="top">
<p>13104</p>
</td><td style="" align="left" valign="top">
<p>-1.105</p>
</td><td style="" align="left" valign="top">
<p>-0.215</p>
</td><td style="" align="left" valign="top">
<p>1800</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>2900</p>
</td><td style="" align="left" valign="top">
<p>10000</p>
</td><td style="" align="left" valign="top">
<p>0.291</p>
</td><td style="" align="left" valign="top">
<p>-0.4</p>
</td><td style="" align="left" valign="top">
<p>2351</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>1218</p>
</td><td style="" align="left" valign="top">
<p>3049</p>
</td><td style="" align="left" valign="top">
<p>-1.126</p>
</td><td style="" align="left" valign="top">
<p>-0.814</p>
</td><td style="" align="left" valign="top">
<p>795</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>2722</p>
</td><td style="" align="left" valign="top">
<p>38768</p>
</td><td style="" align="left" valign="top">
<p>0.142</p>
</td><td style="" align="left" valign="top">
<p>1.312</p>
</td><td style="" align="left" valign="top">
<p>2725</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>2553</p>
</td><td style="" align="left" valign="top">
<p>16250</p>
</td><td style="" align="left" valign="top">
<p>-0.001</p>
</td><td style="" align="left" valign="top">
<p>-0.028</p>
</td><td style="" align="left" valign="top">
<p>2150</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>3681</p>
</td><td style="" align="left" valign="top">
<p>43026</p>
</td><td style="" align="left" valign="top">
<p>0.949</p>
</td><td style="" align="left" valign="top">
<p>1.566</p>
</td><td style="" align="left" valign="top">
<p>2724</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>3032</p>
</td><td style="" align="left" valign="top">
<p>44431</p>
</td><td style="" align="left" valign="top">
<p>0.403</p>
</td><td style="" align="left" valign="top">
<p>1.649</p>
</td><td style="" align="left" valign="top">
<p>2675</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>3437</p>
</td><td style="" align="left" valign="top">
<p>40000</p>
</td><td style="" align="left" valign="top">
<p>0.744</p>
</td><td style="" align="left" valign="top">
<p>1.385</p>
</td><td style="" align="left" valign="top">
<p>2930</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>1680</p>
</td><td style="" align="left" valign="top">
<p>1260</p>
</td><td style="" align="left" valign="top">
<p>-0.736</p>
</td><td style="" align="left" valign="top">
<p>-0.92</p>
</td><td style="" align="left" valign="top">
<p>870</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>2260</p>
</td><td style="" align="left" valign="top">
<p>15000</p>
</td><td style="" align="left" valign="top">
<p>-0.248</p>
</td><td style="" align="left" valign="top">
<p>-0.103</p>
</td><td style="" align="left" valign="top">
<p>2210</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>1660</p>
</td><td style="" align="left" valign="top">
<p>10032</p>
</td><td style="" align="left" valign="top">
<p>-0.753</p>
</td><td style="" align="left" valign="top">
<p>-0.398</p>
</td><td style="" align="left" valign="top">
<p>1145</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>3251</p>
</td><td style="" align="left" valign="top">
<p>12420</p>
</td><td style="" align="left" valign="top">
<p>0.587</p>
</td><td style="" align="left" valign="top">
<p>-0.256</p>
</td><td style="" align="left" valign="top">
<p>2419</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>3039</p>
</td><td style="" align="left" valign="top">
<p>69696</p>
</td><td style="" align="left" valign="top">
<p>0.409</p>
</td><td style="" align="left" valign="top">
<p>3.153</p>
</td><td style="" align="left" valign="top">
<p>2750</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>3401</p>
</td><td style="" align="left" valign="top">
<p>12600</p>
</td><td style="" align="left" valign="top">
<p>0.714</p>
</td><td style="" align="left" valign="top">
<p>-0.245</p>
</td><td style="" align="left" valign="top">
<p>2035</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>1620</p>
</td><td style="" align="left" valign="top">
<p>10240</p>
</td><td style="" align="left" valign="top">
<p>-0.787</p>
</td><td style="" align="left" valign="top">
<p>-0.386</p>
</td><td style="" align="left" valign="top">
<p>1150</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>876</p>
</td><td style="" align="left" valign="top">
<p>876</p>
</td><td style="" align="left" valign="top">
<p>-1.414</p>
</td><td style="" align="left" valign="top">
<p>-0.943</p>
</td><td style="" align="left" valign="top">
<p>665</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>1889</p>
</td><td style="" align="left" valign="top">
<p>8125</p>
</td><td style="" align="left" valign="top">
<p>-0.56</p>
</td><td style="" align="left" valign="top">
<p>-0.512</p>
</td><td style="" align="left" valign="top">
<p>1430</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>4406</p>
</td><td style="" align="left" valign="top">
<p>11792</p>
</td><td style="" align="left" valign="top">
<p>1.56</p>
</td><td style="" align="left" valign="top">
<p>-0.294</p>
</td><td style="" align="left" valign="top">
<p>1920</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>1885</p>
</td><td style="" align="left" valign="top">
<p>1512</p>
</td><td style="" align="left" valign="top">
<p>-0.564</p>
</td><td style="" align="left" valign="top">
<p>-0.905</p>
</td><td style="" align="left" valign="top">
<p>1230</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>1276</p>
</td><td style="" align="left" valign="top">
<p>1276</p>
</td><td style="" align="left" valign="top">
<p>-1.077</p>
</td><td style="" align="left" valign="top">
<p>-0.92</p>
</td><td style="" align="left" valign="top">
<p>975</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>3053</p>
</td><td style="" align="left" valign="top">
<p>67518</p>
</td><td style="" align="left" valign="top">
<p>0.42</p>
</td><td style="" align="left" valign="top">
<p>3.023</p>
</td><td style="" align="left" valign="top">
<p>2400</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>2323</p>
</td><td style="" align="left" valign="top">
<p>9810</p>
</td><td style="" align="left" valign="top">
<p>-0.195</p>
</td><td style="" align="left" valign="top">
<p>-0.412</p>
</td><td style="" align="left" valign="top">
<p>1725</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>3139</p>
</td><td style="" align="left" valign="top">
<p>6324</p>
</td><td style="" align="left" valign="top">
<p>0.493</p>
</td><td style="" align="left" valign="top">
<p>-0.619</p>
</td><td style="" align="left" valign="top">
<p>2300</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>2293</p>
</td><td style="" align="left" valign="top">
<p>12510</p>
</td><td style="" align="left" valign="top">
<p>-0.22</p>
</td><td style="" align="left" valign="top">
<p>-0.251</p>
</td><td style="" align="left" valign="top">
<p>1700</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>2635</p>
</td><td style="" align="left" valign="top">
<p>15616</p>
</td><td style="" align="left" valign="top">
<p>0.068</p>
</td><td style="" align="left" valign="top">
<p>-0.066</p>
</td><td style="" align="left" valign="top">
<p>1915</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>2298</p>
</td><td style="" align="left" valign="top">
<p>15476</p>
</td><td style="" align="left" valign="top">
<p>-0.216</p>
</td><td style="" align="left" valign="top">
<p>-0.074</p>
</td><td style="" align="left" valign="top">
<p>2278</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>2656</p>
</td><td style="" align="left" valign="top">
<p>13390</p>
</td><td style="" align="left" valign="top">
<p>0.086</p>
</td><td style="" align="left" valign="top">
<p>-0.198</p>
</td><td style="" align="left" valign="top">
<p>2497.5</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>1158</p>
</td><td style="" align="left" valign="top">
<p>1158</p>
</td><td style="" align="left" valign="top">
<p>-1.176</p>
</td><td style="" align="left" valign="top">
<p>-0.927</p>
</td><td style="" align="left" valign="top">
<p>725</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>1511</p>
</td><td style="" align="left" valign="top">
<p>2000</p>
</td><td style="" align="left" valign="top">
<p>-0.879</p>
</td><td style="" align="left" valign="top">
<p>-0.876</p>
</td><td style="" align="left" valign="top">
<p>870</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>1252</p>
</td><td style="" align="left" valign="top">
<p>2614</p>
</td><td style="" align="left" valign="top">
<p>-1.097</p>
</td><td style="" align="left" valign="top">
<p>-0.84</p>
</td><td style="" align="left" valign="top">
<p>730</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>2141</p>
</td><td style="" align="left" valign="top">
<p>13433</p>
</td><td style="" align="left" valign="top">
<p>-0.348</p>
</td><td style="" align="left" valign="top">
<p>-0.196</p>
</td><td style="" align="left" valign="top">
<p>2050</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>3565</p>
</td><td style="" align="left" valign="top">
<p>12500</p>
</td><td style="" align="left" valign="top">
<p>0.852</p>
</td><td style="" align="left" valign="top">
<p>-0.251</p>
</td><td style="" align="left" valign="top">
<p>3330</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>1368</p>
</td><td style="" align="left" valign="top">
<p>15750</p>
</td><td style="" align="left" valign="top">
<p>-0.999</p>
</td><td style="" align="left" valign="top">
<p>-0.058</p>
</td><td style="" align="left" valign="top">
<p>1120</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>5726</p>
</td><td style="" align="left" valign="top">
<p>13996</p>
</td><td style="" align="left" valign="top">
<p>2.672</p>
</td><td style="" align="left" valign="top">
<p>-0.162</p>
</td><td style="" align="left" valign="top">
<p>4100</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>2563</p>
</td><td style="" align="left" valign="top">
<p>10450</p>
</td><td style="" align="left" valign="top">
<p>0.008</p>
</td><td style="" align="left" valign="top">
<p>-0.373</p>
</td><td style="" align="left" valign="top">
<p>1655</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>1551</p>
</td><td style="" align="left" valign="top">
<p>7500</p>
</td><td style="" align="left" valign="top">
<p>-0.845</p>
</td><td style="" align="left" valign="top">
<p>-0.549</p>
</td><td style="" align="left" valign="top">
<p>1550</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>1993</p>
</td><td style="" align="left" valign="top">
<p>12125</p>
</td><td style="" align="left" valign="top">
<p>-0.473</p>
</td><td style="" align="left" valign="top">
<p>-0.274</p>
</td><td style="" align="left" valign="top">
<p>2100</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>2555</p>
</td><td style="" align="left" valign="top">
<p>14500</p>
</td><td style="" align="left" valign="top">
<p>0.001</p>
</td><td style="" align="left" valign="top">
<p>-0.132</p>
</td><td style="" align="left" valign="top">
<p>2100</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>1572</p>
</td><td style="" align="left" valign="top">
<p>10000</p>
</td><td style="" align="left" valign="top">
<p>-0.827</p>
</td><td style="" align="left" valign="top">
<p>-0.4</p>
</td><td style="" align="left" valign="top">
<p>1175</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>2764</p>
</td><td style="" align="left" valign="top">
<p>10019</p>
</td><td style="" align="left" valign="top">
<p>0.177</p>
</td><td style="" align="left" valign="top">
<p>-0.399</p>
</td><td style="" align="left" valign="top">
<p>2047.5</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>7168</p>
</td><td style="" align="left" valign="top">
<p>48787</p>
</td><td style="" align="left" valign="top">
<p>3.887</p>
</td><td style="" align="left" valign="top">
<p>1.909</p>
</td><td style="" align="left" valign="top">
<p>3998</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>4392</p>
</td><td style="" align="left" valign="top">
<p>53579</p>
</td><td style="" align="left" valign="top">
<p>1.548</p>
</td><td style="" align="left" valign="top">
<p>2.194</p>
</td><td style="" align="left" valign="top">
<p>2688</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>3096</p>
</td><td style="" align="left" valign="top">
<p>10788</p>
</td><td style="" align="left" valign="top">
<p>0.457</p>
</td><td style="" align="left" valign="top">
<p>-0.353</p>
</td><td style="" align="left" valign="top">
<p>2251</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>2003</p>
</td><td style="" align="left" valign="top">
<p>11865</p>
</td><td style="" align="left" valign="top">
<p>-0.464</p>
</td><td style="" align="left" valign="top">
<p>-0.289</p>
</td><td style="" align="left" valign="top">
<p>1906</p>
</td></tr></tbody></table></div><p>Let's take the scaled house size and scaled house price data and save it as <code class="literal">scaledhousedata.csv</code>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec96"></a>How to do it…</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Load <code class="literal">scaledhousedata.csv</code> to HDFS:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hdfs dfs -put scaledhousedata.csv scaledhousedata.csv</strong></span>
</pre></div></li><li><p>Start <a id="id544" class="indexterm"></a>the <a id="id545" class="indexterm"></a>Spark shell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell</strong></span>
</pre></div></li><li><p>Import statistics and related classes:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.linalg.Vectors</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.linalg.distributed.RowMatrix</strong></span>
</pre></div></li><li><p>Load <code class="literal">saratoga.csv</code> as an RDD:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val data = sc.textFile("scaledhousedata.csv")</strong></span>
</pre></div></li><li><p>Transform the data into an RDD of dense vectors:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val parsedData = data.map( line =&gt; Vectors.dense(line.split(',').map(_.toDouble)))</strong></span>
</pre></div></li><li><p>Create a <code class="literal">RowMatrix</code> from <code class="literal">parsedData</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val mat = new RowMatrix(parsedData)</strong></span>
</pre></div></li><li><p>Compute one principal component:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val pc= mat.computePrincipalComponents(1)</strong></span>
</pre></div></li><li><p>Project the rows to the linear space spanned by the principal component:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val projected = mat.multiply(pc)</strong></span>
</pre></div></li><li><p>Convert the projected <code class="literal">RowMatrix</code> back to the RDD:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val projectedRDD = projected.rows</strong></span>
</pre></div></li><li><p>Save <code class="literal">projectedRDD</code> back to HDFS:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; projectedRDD.saveAsTextFile("phdata")</strong></span>
</pre></div></li></ol></div><p>Now we will use this projected feature, which we decided to call housing density, plot it against the house price, and see whether any new pattern emerges:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Download the HDFS directory <code class="literal">phdata</code> to the local directory <code class="literal">phdata</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; hdfs dfs -get phdata phdata</strong></span>
</pre></div></li><li><p>Trim start and end brackets in the data and load the data into MS Excel, next to the house price.</p></li></ol></div><p>The following is the plot of the house price versus the housing density:</p><div class="mediaobject"><img src="graphics/3056_09_11.jpg" /></div><p>Let's <a id="id546" class="indexterm"></a>draw some <a id="id547" class="indexterm"></a>patterns in this data as follows:</p><div class="mediaobject"><img src="graphics/3056_09_12.jpg" /></div><p>What <a id="id548" class="indexterm"></a>patterns do<a id="id549" class="indexterm"></a> we see here? For moving from a very high-density to low-density housing, people are ready to pay a heavy premium. As the housing density reduces, this premium flattens out. For example, people will pay a heavy premium to move from condominiums and town-homes to a single-family home, but the premium on a single- family home with a 3-acre lot size is not going to be much different from a single-family house with a 2-acre lot size in a comparable built-up area.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec66"></a>Dimensionality reduction with singular value decomposition</h2></div></div><hr /></div><p>Often, the original dimensions do not represent data in the best way possible. As we saw <a id="id550" class="indexterm"></a>in PCA, you can, sometimes, project the data to fewer dimensions and still retain most of the useful<a id="id551" class="indexterm"></a> information.</p><p>Sometimes, the best approach is to align dimensions along the features that exhibit most of the variations. This approach helps to eliminate dimensions that are not representative of the data.</p><p>Let's look at the following figure again, which shows the best-fit line on two dimensions:</p><div class="mediaobject"><img src="graphics/3056_09_10.jpg" /></div><p>The<a id="id552" class="indexterm"></a> projection<a id="id553" class="indexterm"></a> line shows the best approximation of the original data with one dimension. If we take the points where the gray line is intersecting with the black line and isolates the black line, we will have a reduced representation of the original data with as much variation retained as possible, as shown in the following figure:</p><div class="mediaobject"><img src="graphics/3056_09_13.jpg" /></div><p>Let's draw a line perpendicular to the first projection line, as shown in the following figure:</p><div class="mediaobject"><img src="graphics/3056_09_14.jpg" /></div><p>This line<a id="id554" class="indexterm"></a> captures<a id="id555" class="indexterm"></a> as much variation as possible along the second dimension of the original dataset. It does a bad job at approximating the original data as this dimension exhibits less variation to start with. It is possible to use these projection lines to generate a set of uncorrelated data points that will show subgroupings in the original data, not visible at first glance.</p><p>This is the basic idea behind SVD. Take a high dimension, a highly variable set of data points, and reduce it to a lower dimensional space that exposes the structure of the original data more clearly and orders it from the most variation to the least. What makes SVD very useful, especially for NLP application, is that you can simply ignore variation below a certain threshold to massively reduce the original data, making sure that the original relationship interests are retained.</p><p>Let's get slightly into the theory now. SVD is based on a theorem from linear algebra that a rectangular matrix A can be broken down into a product of three matrices—an orthogonal matrix U, a diagonal matrix S, and the transpose of an orthogonal matrix V. We can show it as follows:</p><div class="mediaobject"><img src="graphics/3056_09_15.jpg" /></div><p><span class="emphasis"><em>U</em></span> and <span class="emphasis"><em>V</em></span> are orthogonal matrices:</p><div class="mediaobject"><img src="graphics/3056_09_16.jpg" /></div><div class="mediaobject"><img src="graphics/3056_09_17.jpg" /></div><p>The <a id="id556" class="indexterm"></a>columns of <span class="emphasis"><em>U</em></span> are <a id="id557" class="indexterm"></a>orthonormal eigenvectors of <span class="inlinemediaobject"><img src="graphics/3056_09_18.jpg" /></span> and the columns of <span class="emphasis"><em>V</em></span> are orthonormal eigenvectors of <span class="inlinemediaobject"><img src="graphics/3056_09_19.jpg" /></span>. <span class="emphasis"><em>S</em></span> is a diagonal matrix containing the square roots of eigenvalues from <span class="emphasis"><em>U</em></span> or <span class="emphasis"><em>V</em></span> in descending order.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec97"></a>Getting ready</h3></div></div></div><p>Let's look at an example of a term-document matrix. We are going to look at two new items about the US presidential elections. The following are the links to the two documents:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><span class="strong"><strong>Fox</strong></span>: <a class="ulink" href="http://www.foxnews.com/politics/2015/03/08/top-2016-gop-presidential-hopefuls-return-to-iowa-to-hone-message-including/" target="_blank">http://www.foxnews.com/politics/2015/03/08/top-2016-gop-presidential-hopefuls-return-to-iowa-to-hone-message-including/</a></p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Npr</strong></span>: <a class="ulink" href="http://www.npr.org/blogs/itsallpolitics/2015/03/09/391704815/in-iowa-2016-has-begun-at-least-for-the-republican-party" target="_blank">http://www.npr.org/blogs/itsallpolitics/2015/03/09/391704815/in-iowa-2016-has-begun-at-least-for-the-republican-party</a></p></li></ul></div><p>Let's build the presidential candidate matrix out of these two news items:</p><div class="mediaobject"><img src="graphics/3056_09_20.jpg" /></div><div class="mediaobject"><img src="graphics/3056_09_21.jpg" /></div><p>Let's put<a id="id558" class="indexterm"></a> this matrix in a<a id="id559" class="indexterm"></a> CSV file and then put it in HDFS. We will apply SVD to this matrix and analyze the results.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec98"></a>How to do it…</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Load <code class="literal">scaledhousedata.csv</code> to HDFS:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hdfs dfs -put pres.csv scaledhousedata.csv</strong></span>
</pre></div></li><li><p>Start the Spark shell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell</strong></span>
</pre></div></li><li><p>Import statistics and related classes:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.linalg.Vectors</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.linalg.distributed.RowMatrix</strong></span>
</pre></div></li><li><p>Load <code class="literal">pres.csv</code> as an RDD:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val data = sc.textFile("pres.csv")</strong></span>
</pre></div></li><li><p>Transform data into an RDD of dense vectors:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val parsedData = data.map( line =&gt; Vectors.dense(line.split(',').map(_.toDouble)))</strong></span>
</pre></div></li><li><p>Create a <code class="literal">RowMatrix</code> from <code class="literal">parsedData</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val mat = new RowMatrix(parsedData)</strong></span>
</pre></div></li><li><p>Compute <code class="literal">svd</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val svd = mat.computeSVD(2,true)</strong></span>
</pre></div></li><li><p>Calculate the <code class="literal">U</code> factor (eigenvector):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val U = svd.U</strong></span>
</pre></div></li><li><p>Calculate the matrix of singular values (eigenvalues):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val s = svd.s</strong></span>
</pre></div></li><li><p>Calculate the <code class="literal">V</code> factor (eigenvector):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val s = svd.s</strong></span>
</pre></div></li></ol></div><p>If you<a id="id560" class="indexterm"></a> look at <code class="literal">s</code>, you <a id="id561" class="indexterm"></a>will realize that it gave a much higher score to the Npr article than to the Fox article.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch10"></a>Chapter 10. Recommender Systems</h2></div></div></div><p>In this chapter, we will cover the following recipes:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Collaborative filtering using explicit feedback</p></li><li style="list-style-type: disc"><p>Collaborative filtering using implicit feedback</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec67"></a>Introduction</h2></div></div><hr /></div><p>The following is Wikipedia's definition of recommender systems:</p><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>"Recommender systems are a subclass of information filtering system that seek to predict the 'rating' or 'preference' that user would give to an item."</em></span></p></blockquote></div><p>Recommender systems<a id="id562" class="indexterm"></a> have gained immense popularity in recent years. Amazon uses them to recommend books, Netflix for movies, and Google News to recommend news stories. As the proof is in the pudding, here are some examples of the impact recommendations can have (source: Celma, Lamere, 2008):</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Two-thirds of the movies watched on Netflix are recommended</p></li><li style="list-style-type: disc"><p>38 percent of the news clicks on Google News are recommended</p></li><li style="list-style-type: disc"><p>35 percent of the sales at Amazon sales are the result of recommendations</p></li></ul></div><p>As we have seen in the previous chapters, features and feature selection play a major role in the efficacy of machine learning algorithms. Recommender engine algorithms discover these features, called <span class="strong"><strong>latent features</strong></span>, automatically. In short, there are latent features responsible for a user<a id="id563" class="indexterm"></a> to like one movie and dislike another. If another user has corresponding latent features, there is a good chance that this person will also have a similar taste for movies.</p><p>To understand this better, let's look at some sample movie ratings:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Movie</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Rich</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Bob</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Peter</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Chris</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p><span class="emphasis"><em>Titanic</em></span></p>
</td><td style="" align="left" valign="top">
<p>5</p>
</td><td style="" align="left" valign="top">
<p>3</p>
</td><td style="" align="left" valign="top">
<p>5</p>
</td><td style="" align="left" valign="top">
<p>?</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><span class="emphasis"><em>GoldenEye</em></span></p>
</td><td style="" align="left" valign="top">
<p>3</p>
</td><td style="" align="left" valign="top">
<p>2</p>
</td><td style="" align="left" valign="top">
<p>1</p>
</td><td style="" align="left" valign="top">
<p>5</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><span class="emphasis"><em>Toy Story</em></span></p>
</td><td style="" align="left" valign="top">
<p>1</p>
</td><td style="" align="left" valign="top">
<p>?</p>
</td><td style="" align="left" valign="top">
<p>2</p>
</td><td style="" align="left" valign="top">
<p>2</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><span class="emphasis"><em>Disclosure</em></span></p>
</td><td style="" align="left" valign="top">
<p>4</p>
</td><td style="" align="left" valign="top">
<p>4</p>
</td><td style="" align="left" valign="top">
<p>?</p>
</td><td style="" align="left" valign="top">
<p>4</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><span class="emphasis"><em>Ace Ventura</em></span></p>
</td><td style="" align="left" valign="top">
<p>4</p>
</td><td style="" align="left" valign="top">
<p>?</p>
</td><td style="" align="left" valign="top">
<p>4</p>
</td><td style="" align="left" valign="top">
<p>?</p>
</td></tr></tbody></table></div><p>Our goal is to predict the missing entries shown with the ? symbol. Let's see if we can find some features associated with movies. At first, you will look at the genres, as shown here:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Movie</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Genre</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p><span class="emphasis"><em>Titanic</em></span></p>
</td><td style="" align="left" valign="top">
<p>Action, Romance</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><span class="emphasis"><em>GoldenEye</em></span></p>
</td><td style="" align="left" valign="top">
<p>Action, Adventure, Thriller</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><span class="emphasis"><em>Toy Story</em></span></p>
</td><td style="" align="left" valign="top">
<p>Animation, Children's, Comedy</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><span class="emphasis"><em>Disclosure</em></span></p>
</td><td style="" align="left" valign="top">
<p>Drama, Thriller</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><span class="emphasis"><em>Ace Ventura</em></span></p>
</td><td style="" align="left" valign="top">
<p>Comedy</p>
</td></tr></tbody></table></div><p>Now each movie can be rated for each genre from 0 to 1. For example, <span class="emphasis"><em>GoldenEye</em></span> is not primarily a romance, so it may have 0.1 rating for romance, but 0.98 rating for action. Therefore, each movie can be represented as a feature vector.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note21"></a>Note</h3><p>In this chapter, we <a id="id564" class="indexterm"></a>are going to use the MovieLens dataset from <a class="ulink" href="http://grouplens.org/datasets/movielens/" target="_blank">grouplens.org/datasets/movielens/</a>.</p></div><p>The InfoObjects big data sandbox comes loaded with 100k movie ratings. From GroupLens you can also download 1 million-or even up to 10 million-ratings if you would like to analyze bigger dataset for better predictions.</p><p>We are going to use two files from this dataset:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><code class="literal">u.data</code>: This has a tab-separated list of movie ratings in the following format:</p><div class="informalexample"><pre class="programlisting">user id | item id | rating | epoch time</pre></div><p>Since we are not going to need the time stamp, we are going to filter it out from the data in our recipe</p></li><li style="list-style-type: disc"><p><code class="literal">u.item</code>: This has a tab-separated list of movies in the following format:</p><div class="informalexample"><pre class="programlisting">movie id | movie title | release date | video release date |               IMDb URL | unknown | Action | Adventure | Animation |               Children's | Comedy | Crime | Documentary | Drama | Fantasy |               Film-Noir | Horror | Musical | Mystery | Romance | Sci-Fi |               Thriller | War | Western |</pre></div></li></ul></div><p>This chapter will cover how we can make recommendations using MLlib, the Spark's machine learning library.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec68"></a>Collaborative filtering using explicit feedback</h2></div></div><hr /></div><p>Collaborative filtering is the most commonly used technique for recommender systems. It has an <a id="id565" class="indexterm"></a>interesting property—it learns the features on its own. So, in<a id="id566" class="indexterm"></a> the case of movie ratings, we do <a id="id567" class="indexterm"></a>not need to provide actual human feedback on whether the movie is romantic or action.</p><p>As we saw in the <span class="emphasis"><em>Introduction</em></span> section that movies have some latent features, such as genre, in the same way users have some latent features, such as age, gender, and more. Collaborative filtering does not need them, and figures out latent features on its own.</p><p>We are going to <a id="id568" class="indexterm"></a>use an algorithm called <span class="strong"><strong>Alternating Least Squares</strong></span> (<span class="strong"><strong>ALS</strong></span>) in this example. This algorithm explains the association between a movie and a user based on a small number of latent features. It uses three training parameters: rank, number of iterations, and lambda (explained later in the chapter). The best way to figure out the optimum values of these three parameters is to try<a id="id569" class="indexterm"></a> different values and see which value has the smallest amount of <span class="strong"><strong>Root Mean Square Error</strong></span> (<span class="strong"><strong>RMSE</strong></span>). This error is like a standard deviation, but it is based on model results rather than actual data.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec99"></a>Getting ready</h3></div></div></div><p>Upload the <code class="literal">moviedata</code> downloaded from GroupLens to the <code class="literal">moviedata</code> folder in <code class="literal">hdfs</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hdfs dfs -put moviedata moviedata</strong></span>
</pre></div><p>We are going to add some personalized ratings to this database so that we can test the accuracy of the recommendations.</p><p>You can look at <code class="literal">u.item</code> to pick some movies and rate them. The following are some movies I chose, alongside my ratings. Feel free to choose the movies you would like to rate and provide your own ratings.</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Movie ID</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Movie name</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Rating (1-5)</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>313</p>
</td><td style="" align="left" valign="top">
<p><span class="emphasis"><em>Titanic</em></span></p>
</td><td style="" align="left" valign="top">
<p>5</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>2</p>
</td><td style="" align="left" valign="top">
<p><span class="emphasis"><em>GoldenEye</em></span></p>
</td><td style="" align="left" valign="top">
<p>3</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>1</p>
</td><td style="" align="left" valign="top">
<p><span class="emphasis"><em>Toy Story</em></span></p>
</td><td style="" align="left" valign="top">
<p>1</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>43</p>
</td><td style="" align="left" valign="top">
<p><span class="emphasis"><em>Disclosure</em></span></p>
</td><td style="" align="left" valign="top">
<p>4</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>67</p>
</td><td style="" align="left" valign="top">
<p><span class="emphasis"><em>Ace Ventura</em></span></p>
</td><td style="" align="left" valign="top">
<p>4</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>82</p>
</td><td style="" align="left" valign="top">
<p><span class="emphasis"><em>Jurassic Park</em></span></p>
</td><td style="" align="left" valign="top">
<p>5</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>96</p>
</td><td style="" align="left" valign="top">
<p><span class="emphasis"><em>Terminator 2</em></span></p>
</td><td style="" align="left" valign="top">
<p>5</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>121</p>
</td><td style="" align="left" valign="top">
<p><span class="emphasis"><em>Independence Day</em></span></p>
</td><td style="" align="left" valign="top">
<p>4</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>148</p>
</td><td style="" align="left" valign="top">
<p><span class="emphasis"><em>The Ghost and the Darkness</em></span></p>
</td><td style="" align="left" valign="top">
<p>4</p>
</td></tr></tbody></table></div><p>The<a id="id570" class="indexterm"></a> highest user ID is 943, so we are going<a id="id571" class="indexterm"></a> to add the new user as 944. Let's create a new comma-separated file <code class="literal">p.data</code> with the following data:</p><div class="informalexample"><pre class="programlisting">944,313,5
944,2,3
944,1,1
944,43,4
944,67,4
944,82,5
944,96,5
944,121,4
944,148,4</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec100"></a>How to do it…</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Upload the personalized movie data to <code class="literal">hdfs</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hdfs dfs -put p.data p.data</strong></span>
</pre></div></li><li><p>Import the ALS and rating classes:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.recommendation.ALS</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.recommendation.Rating</strong></span>
</pre></div></li><li><p>Load the rating data into an RDD:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val data = sc.textFile("moviedata/u.data")</strong></span>
</pre></div></li><li><p>Transform the <code class="literal">val data</code> into the RDD of rating:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val ratings = data.map { line =&gt; </strong></span>
<span class="strong"><strong>  val Array(userId, itemId, rating, _) = line.split("\t") </strong></span>
<span class="strong"><strong>  Rating(userId.toInt, itemId.toInt, rating.toDouble) </strong></span>
<span class="strong"><strong>}</strong></span>
</pre></div></li><li><p>Load the personalized rating data into the RDD:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val pdata = sc.textFile("p.data")</strong></span>
</pre></div></li><li><p>Transform the data into the RDD of personalized rating:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val pratings = pdata.map { line =&gt; </strong></span>
<span class="strong"><strong>  val Array(userId, itemId, rating) = line.split(",")</strong></span>
<span class="strong"><strong>  Rating(userId.toInt, itemId.toInt, rating.toDouble) </strong></span>
<span class="strong"><strong>}</strong></span>
</pre></div></li><li><p>Combine ratings with personalized ratings:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val movieratings = ratings.union(pratings)</strong></span>
</pre></div></li><li><p>Build the model using ALS with rank 5 and 10 iterations and 0.01 as lambda:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val model = ALS.train(movieratings, 10, 10, 0.01)</strong></span>
</pre></div></li><li><p>Let's <a id="id572" class="indexterm"></a>predict what my rating <a id="id573" class="indexterm"></a>would be for a given movie based on this model.</p></li><li><p>Let's start with original <span class="emphasis"><em>Terminator</em></span> with movie ID 195:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; model.predict(sc.parallelize(Array((944,195)))).collect.foreach(println)</strong></span>
<span class="strong"><strong>Rating(944,195,4.198642954004738)</strong></span>
</pre></div><p>Since I rated <span class="emphasis"><em>Terminator</em></span> <span class="emphasis"><em>2</em></span> 5, this is a reasonable prediction.</p></li><li><p>Let's try <span class="emphasis"><em>Ghost</em></span> with movie ID 402:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; model.predict(sc.parallelize(Array((944,402)))).collect.foreach(println)</strong></span>
<span class="strong"><strong>Rating(944,402,2.982213836456829)</strong></span>
</pre></div><p>It's a reasonable guess.</p></li><li><p>Let's try <span class="emphasis"><em>The Ghost and the Darkness</em></span>, the movie I already rated, with the ID 148:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; model.predict(sc.parallelize(Array((944,402)))).collect.foreach(println)</strong></span>
<span class="strong"><strong>Rating(944,148,3.8629938805450035)</strong></span>
</pre></div><p>Very close prediction, knowing that I rated the movie 4.</p></li></ol></div><p>You can use more movies to the <code class="literal">train</code> dataset. There are also 1 million and 10 million rating datasets available that will refine the algorithm even more.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec69"></a>Collaborative filtering using implicit feedback</h2></div></div><hr /></div><p>Sometimes<a id="id574" class="indexterm"></a> the feedback available is <a id="id575" class="indexterm"></a>not in the form of ratings but in the form of audio tracks played, movies watched, and so on. This data, at first glance, may not look as good as explicit ratings by users, but this is much more exhaustive.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec101"></a>Getting ready</h3></div></div></div><p>We are going to use million song data from <a class="ulink" href="http://www.kaggle.com/c/msdchallenge/data" target="_blank">http://www.kaggle.com/c/msdchallenge/data</a>. You need to download three files:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><code class="literal">kaggle_visible_evaluation_triplets</code></p></li><li style="list-style-type: disc"><p><code class="literal">kaggle_users.txt</code></p></li><li style="list-style-type: disc"><p><code class="literal">kaggle_songs.txt</code></p></li></ul></div><p>Now perform<a id="id576" class="indexterm"></a> the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Create <a id="id577" class="indexterm"></a>a <code class="literal">songdata</code> folder in <code class="literal">hdfs</code> and put all the three files here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hdfs dfs -mkdir songdata</strong></span>
</pre></div></li><li><p>Upload the song data to <code class="literal">hdfs</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hdfs dfs -put kaggle_visible_evaluation_triplets.txt songdata/</strong></span>
<span class="strong"><strong>$ hdfs dfs -put kaggle_users.txt songdata/</strong></span>
<span class="strong"><strong>$ hdfs dfs -put kaggle_songs.txt songdata/</strong></span>
</pre></div></li></ol></div><p>We still need to do some more preprocessing. ALS in MLlib takes both user and product IDs as integer. The <code class="literal">Kaggle_songs.txt</code> file has song IDs and sequence number next to it, The <code class="literal">Kaggle_users.txt</code> file does not have it. Our goal is to replace the <code class="literal">userid</code> and <code class="literal">songid</code> in <code class="literal">triplets</code> data with the corresponding integer sequence numbers. To do this, follow these steps:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Load the <code class="literal">kaggle_songs</code> data as an RDD:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val songs = sc.textFile("songdata/kaggle_songs.txt")</strong></span>
</pre></div></li><li><p>Load the user data as an RDD:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val users = sc.textFile("songdata/kaggle_users.txt")</strong></span>
</pre></div></li><li><p>Load the triplets (user, song, plays) data as an RDD:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val triplets = sc.textFile("songdata/kaggle_visible_evaluation_triplets.txt")</strong></span>
</pre></div></li><li><p>Convert the song data into the <code class="literal">PairRDD</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val songIndex = songs.map(_.split("\\W+")).map(v =&gt; (v(0),v(1).toInt))</strong></span>
</pre></div></li><li><p>Collect the <code class="literal">songIndex</code> as Map:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val songMap = songIndex.collectAsMap</strong></span>
</pre></div></li><li><p>Convert the user data into the <code class="literal">PairRDD</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val userIndex = users.zipWithIndex.map( t =&gt; (t._1,t._2.toInt))</strong></span>
</pre></div></li><li><p>Collect the <code class="literal">userIndex</code> as Map:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val userMap = userIndex.collectAsMap</strong></span>
</pre></div></li></ol></div><p>We will need both <code class="literal">songMap</code> and <code class="literal">userMap</code> to replace <code class="literal">userId</code> and <code class="literal">songId</code> in triplets. Spark will <a id="id578" class="indexterm"></a>automatically make both these <a id="id579" class="indexterm"></a>maps available on the cluster as needed. This works fine but is expensive to send across the cluster every time it is needed.</p><p>A better approach is to use a Spark feature called <code class="literal">broadcast</code> variables. The <code class="literal">broadcast</code> variables allow the Spark job to keep a read-only copy of a variable cached on each machine, rather than shipping a copy with each task. Spark distributes broadcast variables using efficient broadcast algorithms, so communication cost over the network is negligible.</p><p>As you can guess, both <code class="literal">songMap</code> and <code class="literal">userMap</code> are good candidates to be wrapped around the <code class="literal">broadcast</code> variables. Perform the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Broadcast the <code class="literal">userMap</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val broadcastUserMap = sc.broadcast(userMap)</strong></span>
</pre></div></li><li><p>Broadcast the <code class="literal">songMap</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val broadcastSongMap = sc.broadcast(songMap)</strong></span>
</pre></div></li><li><p>Convert the <code class="literal">triplet</code> into an array:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val tripArray = triplets.map(_.split("\\W+"))</strong></span>
</pre></div></li><li><p>Import the rating:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.recommendation.Rating</strong></span>
</pre></div></li><li><p>Convert the <code class="literal">triplet</code> array into an RDD of rating objects:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val ratings = tripArray.map { case Array(user, song, plays) =&gt;</strong></span>
<span class="strong"><strong>  val userId = broadcastUserMap.value.getOrElse(user, 0)</strong></span>
<span class="strong"><strong>  val songId = broadcastUserMap.value.getOrElse(song, 0)</strong></span>
<span class="strong"><strong>  Rating(userId, songId, plays.toDouble)</strong></span>
<span class="strong"><strong>}</strong></span>
</pre></div></li></ol></div><p>Now, our data is ready to do the modeling and prediction.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec102"></a>How to do it…</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Import ALS:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.recommendation.ALS</strong></span>
</pre></div></li><li><p>Build a model using the ALS with rank 10 and 10 iterations:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val model = ALS.trainImplicit(ratings, 10, 10)</strong></span>
</pre></div></li><li><p>Extract the user and song tuples from the triplet:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val usersSongs = ratings.map( r =&gt; (r.user, r.product) )</strong></span>
</pre></div></li><li><p>Make predictions for the user and song tuples:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val predictions = model.predict(usersSongs)</strong></span>
</pre></div></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec103"></a>How it works…</h3></div></div></div><p>Our <a id="id580" class="indexterm"></a>model takes four parameters<a id="id581" class="indexterm"></a> to work, as shown here:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Parameter name</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Description</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>Rank</p>
</td><td style="" align="left" valign="top">
<p>Number of latent features in the model</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Iterations</p>
</td><td style="" align="left" valign="top">
<p>Number of iterations for this factorization to run</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Lambda</p>
</td><td style="" align="left" valign="top">
<p>Over fitting parameter</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Alpha</p>
</td><td style="" align="left" valign="top">
<p>Relative weight of observed interactions</p>
</td></tr></tbody></table></div><p>As you saw in the case of gradient descent, these parameters need to be set by hand. We can try different values, but the value that works best is rank=50, iterations=30, lambda=0.00001, and alpha= 40.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec104"></a>There's more…</h3></div></div></div><p>One way to test different parameters quickly is to spawn a spark cluster on Amazon EC2. This gives you flexibility to go with a powerful instance to test these parameters fast. I have created a public s3 bucket <code class="literal">com.infoobjects.songdata</code> to pull data to Spark.</p><p>Here are the steps you need to follow to load the data from S3 and run the ALS:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sc.hadoopConfiguration.set("fs.s3n.awsAccessKeyId", "&lt;your access key&gt;")</strong></span>
<span class="strong"><strong>sc.hadoopConfiguration.set("fs.s3n.awsSecretAccessKey","&lt;your secret key&gt;")</strong></span>
<span class="strong"><strong>val songs = sc.textFile("s3n://com.infoobjects.songdata/kaggle_songs.txt")</strong></span>
<span class="strong"><strong>val users = sc.textFile("s3n://com.infoobjects.songdata/kaggle_users.txt")</strong></span>
<span class="strong"><strong>val triplets = sc.textFile("s3n://com.infoobjects.songdata/kaggle_visible_evaluation_triplets.txt")</strong></span>
<span class="strong"><strong>val songIndex = songs.map(_.split("\\W+")).map(v =&gt; (v(0),v(1).toInt))</strong></span>
<span class="strong"><strong>val songMap = songIndex.collectAsMap</strong></span>
<span class="strong"><strong>val userIndex = users.zipWithIndex.map( t =&gt; (t._1,t._2.toInt))</strong></span>
<span class="strong"><strong>val userMap = userIndex.collectAsMap</strong></span>
<span class="strong"><strong>val broadcastUserMap = sc.broadcast(userMap)</strong></span>
<span class="strong"><strong>val broadcastSongMap = sc.broadcast(songMap)</strong></span>
<span class="strong"><strong>val tripArray = triplets.map(_.split("\\W+"))</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.recommendation.Rating</strong></span>
<span class="strong"><strong>val ratings = tripArray.map{ v =&gt;</strong></span>
<span class="strong"><strong>  val userId: Int = broadcastUserMap.value.get(v(0)).fold(0)(num =&gt; num)</strong></span>
<span class="strong"><strong>  val songId: Int = broadcastSongMap.value.get(v(1)).fold(0)(num =&gt; num)</strong></span>
<span class="strong"><strong>  Rating(userId,songId,v(2).toDouble)</strong></span>
<span class="strong"><strong>  }</strong></span>
<span class="strong"><strong>import org.apache.spark.mllib.recommendation.ALS</strong></span>
<span class="strong"><strong>val model = ALS.trainImplicit(ratings, 50, 30, 0.000001, 40)</strong></span>
<span class="strong"><strong>val usersSongs = ratings.map( r =&gt; (r.user, r.product) )</strong></span>
<span class="strong"><strong>val predictions =model.predict(usersSongs)</strong></span>
</pre></div><p>These<a id="id582" class="indexterm"></a> are the predictions made on <a id="id583" class="indexterm"></a>the <code class="literal">usersSongs</code> matrix.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch11"></a>Chapter 11. Graph Processing Using GraphX</h2></div></div></div><p>This chapter will cover how we can do graph processing using GraphX, Spark's graph processing library.</p><p>The chapter is divided into the following recipes:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Fundamental operations on graphs</p></li><li style="list-style-type: disc"><p>Using PageRank</p></li><li style="list-style-type: disc"><p>Finding connected components</p></li><li style="list-style-type: disc"><p>Performing neighborhood aggregation</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch11lvl1sec70"></a>Introduction</h2></div></div><hr /></div><p>Graph analysis is much more commonplace in our life than we think. To take the most common example, when we ask a GPS to find the shortest route to a destination, it uses a graph-processing algorithm.</p><p>Let's start by understanding graphs. A graph is a representation of a set of vertices where some pairs of vertices are connected by edges. When these edges move from one direction to another, it's called a <span class="strong"><strong>directed graph</strong></span> or <a id="id584" class="indexterm"></a>
<span class="strong"><strong>digraph</strong></span>.</p><p>GraphX is the Spark API for graph processing. It provides a wrapper around an RDD called <span class="strong"><strong>resilient distributed property graph</strong></span>. The property graph is a directed multigraph with properties <a id="id585" class="indexterm"></a>attached to each vertex and edge.</p><p>There are two types<a id="id586" class="indexterm"></a> of graphs—directed graphs (digraphs) and <a id="id587" class="indexterm"></a>regular graphs. Directed graphs have edges that run in one direction, for example, from vertex A to vertex B. Twitter follower is a good example of a digraph. If John is David's Twitter follower, it does not mean that David is John's follower. On the other hand, Facebook is a good example of a regular graph. If John is David's Facebook friend, David is also John's Facebook friend.</p><p>A multigraph is<a id="id588" class="indexterm"></a> a graph which is allowed to have multiple edges (also called <span class="strong"><strong>parallel edges</strong></span>). Since <a id="id589" class="indexterm"></a>every edge in GraphX has properties, each edge has its own identity.</p><p>Traditionally, for <a id="id590" class="indexterm"></a>distributed graph processing, there<a id="id591" class="indexterm"></a> have been two types of systems:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Data parallel</p></li><li style="list-style-type: disc"><p>Graph parallel</p></li></ul></div><p>GraphX aims to combine the two together in one system. GraphX API enables users to view the data both as graphs and as collections (RDDs) without data movement.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch11lvl1sec71"></a>Fundamental operations on graphs</h2></div></div><hr /></div><p>In this recipe, we <a id="id592" class="indexterm"></a>will learn how to create graphs and do basic operations on them.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec105"></a>Getting ready</h3></div></div></div><p>As a starting example, we will have three vertices, each representing the city center of three cities in California—Santa Clara, Fremont, and San Francisco. The following is the distance between these cities:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Source</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Destination</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Distance (miles)</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>Santa Clara, CA</p>
</td><td style="" align="left" valign="top">
<p>Fremont, CA</p>
</td><td style="" align="left" valign="top">
<p>20</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Fremont, CA</p>
</td><td style="" align="left" valign="top">
<p>San Francisco, CA</p>
</td><td style="" align="left" valign="top">
<p>44</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>San Francisco, CA</p>
</td><td style="" align="left" valign="top">
<p>Santa Clara, CA</p>
</td><td style="" align="left" valign="top">
<p>53</p>
</td></tr></tbody></table></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec106"></a>How to do it…</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Import the GraphX-related classes:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.graphx._</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.rdd.RDD</strong></span>
</pre></div></li><li><p>Load the vertex data in an array:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val vertices = Array((1L, ("Santa Clara","CA")),(2L, ("Fremont","CA")),(3L, ("San Francisco","CA")))</strong></span>
</pre></div></li><li><p>Load the array of vertices into the RDD of vertices:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val vrdd = sc.parallelize(vertices)</strong></span>
</pre></div></li><li><p>Load the edge data in an array:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val edges = Array(Edge(1L,2L,20),Edge(2L,3L,44),Edge(3L,1L,53))</strong></span>
</pre></div></li><li><p>Load the<a id="id593" class="indexterm"></a> data into the RDD of edges:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val erdd = sc.parallelize(edges)</strong></span>
</pre></div></li><li><p>Create the graph:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val graph = Graph(vrdd,erdd)</strong></span>
</pre></div></li><li><p>Print all the vertices of the graph:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; graph.vertices.collect.foreach(println)</strong></span>
</pre></div></li><li><p>Print all the edges of the graph:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; graph.edges.collect.foreach(println)</strong></span>
</pre></div></li><li><p>Print the edge triplets; a triplet is created by adding source and destination attributes to an edge:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; graph.triplets.collect.foreach(println)</strong></span>
</pre></div></li><li><p>In-degree of a graph is the number of inward-directed edges it has. Print the in-degree of each vertex (as <code class="literal">VertexRDD[Int]</code>):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; graph.inDegrees</strong></span>
</pre></div></li></ol></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch11lvl1sec72"></a>Using PageRank</h2></div></div><hr /></div><p>PageRank measures <a id="id594" class="indexterm"></a>the importance of each vertex in a graph. PageRank was started <a id="id595" class="indexterm"></a>by Google's founders, who used the theory that the most important pages on the Internet are the pages with the most links leading to them. PageRank also looks at the importance of a page leading to the target page. So, if a given web page has incoming links from higher rank pages, it will be ranked higher.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec107"></a>Getting ready</h3></div></div></div><p>We are going to use Wikipedia page link data to calculate page rank. Wikipedia publishes its data <a id="id596" class="indexterm"></a>in the form of a database dump. We are going to use link data from <a class="ulink" href="http://haselgrove.id.au/wikipedia.htm" target="_blank">http://haselgrove.id.au/wikipedia.htm</a>, which has the data in two files:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><code class="literal">links-simple-sorted.txt</code></p></li><li style="list-style-type: disc"><p><code class="literal">titles-sorted.txt</code></p></li></ul></div><p>I have put both of them on <a id="id597" class="indexterm"></a>Amazon S3 at <code class="literal">s3n://com.infoobjects.wiki/links</code> and <code class="literal">s3n://com.infoobjects.wiki/nodes</code>. Since the data size is larger, it is recommended that you run it on either Amazon EC2 or your local cluster. Sandbox may be very slow.</p><p>You can<a id="id598" class="indexterm"></a> load the files to <code class="literal">hdfs</code> using the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hdfs dfs -mkdir wiki</strong></span>
<span class="strong"><strong>$ hdfs dfs -put links-simple-sorted.txt wiki/links.txt</strong></span>
<span class="strong"><strong>$ hdfs dfs -put titles-sorted.txt wiki/nodes.txt</strong></span>
</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec108"></a>How to do it…</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Import the GraphX related classes:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.graphx._</strong></span>
</pre></div></li><li><p>Load the edges from <code class="literal">hdfs</code> with 20 partitions:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val edgesFile = sc.textFile("wiki/links.txt",20)</strong></span>
</pre></div><p>Or, load the edges from Amazon S3:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val edgesFile = sc.textFile("s3n:// com.infoobjects.wiki/links",20)</strong></span>
</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note22"></a>Note</h3><p>The <code class="literal">links</code> file has links in the "sourcelink: link1 link2 …" format.</p></div></li><li><p>Flatten and convert it into an RDD of "link1,link2" format and then convert it into an RDD of <code class="literal">Edge</code> objects:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val edges = edgesFile.flatMap { line =&gt;</strong></span>
<span class="strong"><strong>   val links = line.split("\\W+")</strong></span>
<span class="strong"><strong>   val from = links(0)</strong></span>
<span class="strong"><strong>     val to = links.tail</strong></span>
<span class="strong"><strong>   for ( link &lt;- to) yield (from,link)</strong></span>
<span class="strong"><strong>    }.map( e =&gt; Edge(e._1.toLong,e._2.toLong,1))</strong></span>
</pre></div></li><li><p>Load the vertices from <code class="literal">hdfs</code> with 20 partitions:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val verticesFile = sc.textFile("wiki/nodes.txt",20)</strong></span>
</pre></div></li><li><p>Or, load the edges from Amazon S3:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val verticesFile = sc.textFile("s3n:// com.infoobjects.wiki/nodes",20)</strong></span>
</pre></div></li><li><p>Provide an index to the vertices and then swap it to make it in the (index, title) format:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val vertices = verticesFile.zipWithIndex.map(_.swap)</strong></span>
</pre></div></li><li><p>Create the <code class="literal">graph</code> object:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val graph = Graph(vertices,edges)</strong></span>
</pre></div></li><li><p>Run PageRank and get the vertices:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val ranks = graph.pageRank(0.001).vertices</strong></span>
</pre></div></li><li><p>As ranks is in the (vertex ID, pagerank) format, swap it to make it in the (pagerank, vertex ID) format:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val swappedRanks = ranks.map(_.swap)</strong></span>
</pre></div></li><li><p>Sort to get the highest ranked pages first:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val sortedRanks = swappedRanks.sortByKey(false)</strong></span>
</pre></div></li><li><p>Get the<a id="id599" class="indexterm"></a> highest ranked page:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val highest = sortedRanks.first</strong></span>
</pre></div></li><li><p>The preceding command gives the vertex id, which you still have to look up to see the actual title with rank. Let's do a join:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val join = sortedRanks.join(vertices)</strong></span>
</pre></div></li><li><p>Sort the joined RDD again after converting from the (vertex ID, (page rank, title)) format to the (page rank, (vertex ID, title)) format:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val final = join.map ( v =&gt; (v._2._1, (v._1,v._2._2))).sortByKey(false)</strong></span>
</pre></div></li><li><p>Print the top five ranked pages</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; final.take(5).collect.foreach(println)</strong></span>
</pre></div></li></ol></div><p>Here's what the output should be:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>(12406.054646736622,(5302153,United_States'_Country_Reports_on_Human_Rights_Practices))</strong></span>
<span class="strong"><strong>(7925.094429748747,(84707,2007,_Canada_budget)) (7635.6564216408515,(88822,2008,_Madrid_plane_crash)) (7041.479913258444,(1921890,Geographic_coordinates)) (5675.169862343964,(5300058,United_Kingdom's))</strong></span>
</pre></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch11lvl1sec73"></a>Finding connected components</h2></div></div><hr /></div><p>A connected component is a subgraph (a graph whose vertices are a subset of the vertex set of the <a id="id600" class="indexterm"></a>original graph and whose edges are a subset of the edge set of the original graph) in which any two vertices are connected to each other by an<a id="id601" class="indexterm"></a> edge or a series of edges.</p><p>An easy way to understand it would be by taking a look at the road network graph of Hawaii. This state has numerous islands, which are not connected by roads. Within each island, most roads will be connected to each other. The goal of finding the connected components is to find these clusters.</p><p>The connected components algorithm labels each connected component of the graph with the ID of its lowest-numbered vertex.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec109"></a>Getting ready</h3></div></div></div><p>We will build<a id="id602" class="indexterm"></a> a small graph here for the clusters we know and use connected components to segregate them. Let's look at the following data:</p><div class="mediaobject"><img src="graphics/3056_11_01.jpg" /></div><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Follower</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Followee</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>John</p>
</td><td style="" align="left" valign="top">
<p>Pat</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Pat</p>
</td><td style="" align="left" valign="top">
<p>Dave</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Gary</p>
</td><td style="" align="left" valign="top">
<p>Chris</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Chris</p>
</td><td style="" align="left" valign="top">
<p>Bill</p>
</td></tr></tbody></table></div><p>The preceding data is a simple one with six vertices and two clusters. Let's put this data in the form of two files: <code class="literal">nodes.csv</code> and <code class="literal">edges.csv</code>.</p><p>The following is the content of <code class="literal">nodes.csv</code>:</p><div class="informalexample"><pre class="programlisting">1,John
2,Pat
3,Dave
4,Gary
5,Chris
6,Bill</pre></div><p>The following is the content of <code class="literal">edges.csv</code>:</p><div class="informalexample"><pre class="programlisting">1,2,follows
2,3,follows
4,5,follows
5,6,follows</pre></div><p>We should expect a connected component algorithm to identify two clusters, the first one identified by (1,John) and the second by (4,Gary).</p><p>You can load the files to <code class="literal">hdfs</code> using the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hdfs dfs -mkdir data/cc</strong></span>
<span class="strong"><strong>$ hdfs dfs -put nodes.csv data/cc/nodes.csv</strong></span>
<span class="strong"><strong>$ hdfs dfs -put edges.csv data/cc/edges.csv</strong></span>
</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec110"></a>How to do it…</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Load the <a id="id603" class="indexterm"></a>Spark shell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell</strong></span>
</pre></div></li><li><p>Import the GraphX-related classes:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.graphx._</strong></span>
</pre></div></li><li><p>Load the edges from <code class="literal">hdfs</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val edgesFile = sc.textFile("hdfs://localhost:9000/user/hduser/data/cc/edges.csv")</strong></span>
</pre></div></li><li><p>Convert the <code class="literal">edgesFile</code> RDD into the RDD of edges:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val edges = edgesFile.map(_.split(",")).map(e =&gt; Edge(e(0).toLong,e(1).toLong,e(2)))</strong></span>
</pre></div></li><li><p>Load the vertices from <code class="literal">hdfs</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val verticesFile = sc.textFile("hdfs://localhost:9000/user/hduser/data/cc/nodes.csv")</strong></span>
</pre></div></li><li><p>Map the vertices:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val vertices = verticesFile.map(_.split(",")).map( e =&gt; (e(0).toLong,e(1)))</strong></span>
</pre></div></li><li><p>Create the <code class="literal">graph</code> object:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val graph = Graph(vertices,edges)</strong></span>
</pre></div></li><li><p>Calculate the connected components:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val cc = graph.connectedComponents</strong></span>
</pre></div></li><li><p>Find the vertices for the connected components (which is a subgraph):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val ccVertices = cc.vertices</strong></span>
</pre></div></li><li><p>Print the <code class="literal">ccVertices</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; ccVertices.collect.foreach(println)</strong></span>
</pre></div></li></ol></div><p>As you can see in the output, vertices 1,2,3 are pointing to 1, while 4,5,6 are pointing to 4. Both of these are the lowest-indexed vertices in their respective clusters.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch11lvl1sec74"></a>Performing neighborhood aggregation</h2></div></div><hr /></div><p>GraphX does most of the computation by isolating each vertex and its neighbors. It makes it easier to process the massive graph data on distributed systems. This makes the neighborhood operations very important. GraphX has a mechanism to do it at each neighborhood level<a id="id604" class="indexterm"></a> in the form of the <code class="literal">aggregateMessages</code> method. It does it in two steps:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>In the first step (first function of the method), messages are send to the destination vertex or source vertex (similar to the Map function in MapReduce).</p></li><li><p>In the second step (second function of the method), aggregation is done on these messages (similar to the Reduce function in MapReduce).</p></li></ol></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec111"></a>Getting ready</h3></div></div></div><p>Let's build a small dataset of the followers:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Follower</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Followee</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>John</p>
</td><td style="" align="left" valign="top">
<p>Barack</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Pat</p>
</td><td style="" align="left" valign="top">
<p>Barack</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Gary</p>
</td><td style="" align="left" valign="top">
<p>Barack</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Chris</p>
</td><td style="" align="left" valign="top">
<p>Mitt</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Rob</p>
</td><td style="" align="left" valign="top">
<p>Mitt</p>
</td></tr></tbody></table></div><p>Our goal is to find out how many followers each node has. Let's load this data in the form of two files: <code class="literal">nodes.csv</code> and <code class="literal">edges.csv</code>.</p><p>The following is the content of <code class="literal">nodes.csv</code>:</p><div class="informalexample"><pre class="programlisting">1,Barack
2,John
3,Pat
4,Gary
5,Mitt
6,Chris
7,Rob</pre></div><p>The following is the content of <code class="literal">edges.csv</code>:</p><div class="informalexample"><pre class="programlisting">2,1,follows
3,1,follows
4,1,follows
6,5,follows
7,5,follows</pre></div><p>You can load the files to <code class="literal">hdfs</code> using the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hdfs dfs -mkdir data/na</strong></span>
<span class="strong"><strong>$ hdfs dfs -put nodes.csv data/na/nodes.csv</strong></span>
<span class="strong"><strong>$ hdfs dfs -put edges.csv data/na/edges.csv</strong></span>
</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec112"></a>How to do it…</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Load the <a id="id605" class="indexterm"></a>Spark shell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell</strong></span>
</pre></div></li><li><p>Import the GraphX related classes:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.graphx._</strong></span>
</pre></div></li><li><p>Load the edges from <code class="literal">hdfs</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val edgesFile = sc.textFile("hdfs://localhost:9000/user/hduser/data/na/edges.csv")</strong></span>
</pre></div></li><li><p>Convert the edges into the RDD of edges:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val edges = edgesFile.map(_.split(",")).map(e =&gt; Edge(e(0).toLong,e(1).toLong,e(2)))</strong></span>
</pre></div></li><li><p>Load the vertices from <code class="literal">hdfs</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val verticesFile = sc.textFile("hdfs://localhost:9000/user/hduser/data/cc/nodes.csv")</strong></span>
</pre></div></li><li><p>Map the vertices:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val vertices = verticesFile.map(_.split(",")).map( e =&gt; (e(0).toLong,e(1)))</strong></span>
</pre></div></li><li><p>Create the <code class="literal">graph</code> object:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val graph = Graph(vertices,edges)</strong></span>
</pre></div></li><li><p>Do the neighborhood aggregation by sending messages to the followees with the number of followers from each follower, that is, 1 and then adding the number of followers:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val followerCount = graph.aggregateMessages[(Int)]( t =&gt; t.sendToDst(1), (a, b) =&gt; (a+b))</strong></span>
</pre></div></li><li><p>Print <code class="literal">followerCount</code> in the form of (followee, number of followers):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; followerCount.collect.foreach(println)</strong></span>
</pre></div></li></ol></div><p>You should get an output similar to the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>(1,3)</strong></span>
<span class="strong"><strong>(5,2)</strong></span>
</pre></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch12"></a>Chapter 12. Optimizations and Performance Tuning</h2></div></div></div><p>This chapter covers various optimizations and performance-tuning best practices when working with Spark.</p><p>The chapter is divided into the following recipes:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Optimizing memory</p></li><li style="list-style-type: disc"><p>Using compression to improve performance</p></li><li style="list-style-type: disc"><p>Using serialization to improve performance</p></li><li style="list-style-type: disc"><p>Optimizing garbage collection</p></li><li style="list-style-type: disc"><p>Optimizing the level of parallelism</p></li><li style="list-style-type: disc"><p>Understanding the future of optimization – project Tungsten</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch12lvl1sec75"></a>Introduction</h2></div></div><hr /></div><p>Before looking into various ways to optimize Spark, it is a good idea to look at the Spark internals. So far, we have looked at Spark at higher level, where focus was the functionality provided by the various libraries.</p><p>Let's start with <a id="id606" class="indexterm"></a>redefining an RDD. Externally, an RDD is a distributed immutable collection of objects. Internally, it consists of the following five parts:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Set of partitions (<code class="literal">rdd.getPartitions</code>)</p></li><li style="list-style-type: disc"><p>List of dependencies on parent RDDs (<code class="literal">rdd.dependencies</code>)</p></li><li style="list-style-type: disc"><p>Function to compute a partition, given its parents</p></li><li style="list-style-type: disc"><p>Partitioner (optional) (<code class="literal">rdd.partitioner</code>)</p></li><li style="list-style-type: disc"><p>Preferred location of each partition (optional) (<code class="literal">rdd.preferredLocations</code>)</p></li></ul></div><p>The first three are needed for an RDD to be recomputed, in case the data is lost. When combined, it is <a id="id607" class="indexterm"></a>called <span class="strong"><strong>lineage</strong></span>. The last two parts are optimizations.</p><p>A set of partitions is<a id="id608" class="indexterm"></a> how data is divided into nodes. In case of HDFS, it means <code class="literal">InputSplits</code>, which are mostly the same as block (except when a record crosses block boundaries; in that case, it will be slightly bigger than a block).</p><p>Let's revisit<a id="id609" class="indexterm"></a> our <code class="literal">wordCount</code> example to understand these five parts. This is how the RDD graph looks for <code class="literal">wordCount</code> at dataset level view:</p><div class="mediaobject"><img src="graphics/3056_12_01.jpg" /></div><p>Basically, this is how the flow goes:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Load the <code class="literal">words</code> folder as an RDD:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val words = sc.textFile("hdfs://localhost:9000/user/hduser/words")</strong></span>
</pre></div><p>The following are the five parts of <code class="literal">words</code> RDD:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /></colgroup><tbody><tr><td style="" align="left" valign="top">
<p><span class="strong"><strong>Partitions</strong></span></p>
</td><td style="" align="left" valign="top">
<p>One partition per hdfs inputsplit/block (<code class="literal">org.apache.spark.rdd.HadoopPartition</code>)</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><span class="strong"><strong>Dependencies</strong></span></p>
</td><td style="" align="left" valign="top">
<p>None</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><span class="strong"><strong>Compute function</strong></span></p>
</td><td style="" align="left" valign="top">
<p>Read the block</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><span class="strong"><strong>Preferred location</strong></span></p>
</td><td style="" align="left" valign="top">
<p>The hdfs block location</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><span class="strong"><strong>Partitioner</strong></span></p>
</td><td style="" align="left" valign="top">
<p>None</p>
</td></tr></tbody></table></div></li><li><p>Tokenize the <a id="id610" class="indexterm"></a>words from <code class="literal">words</code> RDD with each word on a separate line:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val wordsFlatMap = words.flatMap(_.split("\\W+"))</strong></span>
</pre></div><p>The following are the five parts of <code class="literal">wordsFlatMap</code> RDD:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /></colgroup><tbody><tr><td style="" align="left" valign="top">
<p><span class="strong"><strong>Partitions</strong></span></p>
</td><td style="" align="left" valign="top">
<p>Same as parent RDD, that is, <code class="literal">words</code> (<code class="literal">org.apache.spark.rdd.HadoopPartition</code>)</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><span class="strong"><strong>Dependencies</strong></span></p>
</td><td style="" align="left" valign="top">
<p>Same as parent RDD, that is, <code class="literal">words</code> (<code class="literal">org.apache.spark.OneToOneDependency</code>)</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><span class="strong"><strong>Compute function</strong></span></p>
</td><td style="" align="left" valign="top">
<p>Compute parent and split each element and flattens the results</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><span class="strong"><strong>Preferred location</strong></span></p>
</td><td style="" align="left" valign="top">
<p>Ask parent</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><span class="strong"><strong>Partitioner</strong></span></p>
</td><td style="" align="left" valign="top">
<p>None</p>
</td></tr></tbody></table></div></li><li><p>Transform each word in <code class="literal">wordsFlatMap</code> RDD to (word,1) tuple:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val wordsMap = wordsFlatMap.map( w =&gt; (w,1))</strong></span>
</pre></div><p>The following are the five parts of <code class="literal">wordsMap</code> RDD:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /></colgroup><tbody><tr><td style="" align="left" valign="top">
<p><span class="strong"><strong>Partitions</strong></span></p>
</td><td style="" align="left" valign="top">
<p>Same as parent RDD, that is, wordsFlatMap (org.apache.spark.rdd.HadoopPartition)</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><span class="strong"><strong>Dependencies</strong></span></p>
</td><td style="" align="left" valign="top">
<p>Same as parent RDD, that is, wordsFlatMap (org.apache.spark.OneToOneDependency)</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><span class="strong"><strong>Compute function</strong></span></p>
</td><td style="" align="left" valign="top">
<p>Compute parent and map it to PairRDD</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><span class="strong"><strong>Preferred Location</strong></span></p>
</td><td style="" align="left" valign="top">
<p>Ask parent</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><span class="strong"><strong>Partitioner</strong></span></p>
</td><td style="" align="left" valign="top">
<p>None</p>
</td></tr></tbody></table></div></li><li><p>Reduce all the values for a given key and sum them up:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val wordCount = wordsMap.reduceByKey(_+_)</strong></span>
</pre></div><p>The following are the five parts of <code class="literal">wordCount</code> RDD:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /></colgroup><tbody><tr><td style="" align="left" valign="top">
<p><span class="strong"><strong>Partitions</strong></span></p>
</td><td style="" align="left" valign="top">
<p>One per reduce task (<code class="literal">org.apache.spark.rdd.ShuffledRDDPartition</code>)</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><span class="strong"><strong>Dependencies</strong></span></p>
</td><td style="" align="left" valign="top">
<p>Shuffle dependency on each parent (<code class="literal">org.apache.spark.ShuffleDependency</code>)</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><span class="strong"><strong>Compute function</strong></span></p>
</td><td style="" align="left" valign="top">
<p>Do addition on shuffled data</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><span class="strong"><strong>Preferred location</strong></span></p>
</td><td style="" align="left" valign="top">
<p>None</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><span class="strong"><strong>Partitioner</strong></span></p>
</td><td style="" align="left" valign="top">
<p>HashPartitioner (<code class="literal">org.apache.spark.HashPartitioner</code>)</p>
</td></tr></tbody></table></div></li></ol></div><p>This is how <a id="id611" class="indexterm"></a>an RDD graph for <code class="literal">wordCount</code> looks at the partition level view:</p><div class="mediaobject"><img src="graphics/3056_12_02.jpg" /></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch12lvl1sec76"></a>Optimizing memory</h2></div></div><hr /></div><p>Spark is a complex distributed computing framework, and has many moving parts. Various<a id="id612" class="indexterm"></a> cluster resources, such as memory, CPU, and network bandwidth, can become bottlenecks at various points. As Spark is an in-memory compute framework, the impact of the memory is the biggest.</p><p>Another issue is that it is common for Spark applications to use a huge amount of memory, sometimes more than 100 GB. This amount of memory usage is not common in traditional Java applications.</p><p>In Spark, there are two places where memory optimization is needed, and that is at the driver and at the executor level.</p><p>You can use the following commands to set the driver memory:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Spark shell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell --drive-memory 4g</strong></span>
</pre></div></li><li style="list-style-type: disc"><p>Spark submit:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-submit --drive-memory 4g</strong></span>
</pre></div></li></ul></div><p>You can use the following commands to set the executor memory:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Spark shell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell --executor-memory 4g</strong></span>
</pre></div></li><li style="list-style-type: disc"><p>Spark submit:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-submit --executor-memory 4g</strong></span>
</pre></div></li></ul></div><p>To understand memory optimization, it is a good idea to understand how memory management <a id="id613" class="indexterm"></a>works in Java. Objects reside in Heap in Java. Heap is created when JVM starts, and it can resize itself when needed (based on minimum and maximum size, that is, <code class="literal">-Xms</code> and <code class="literal">-Xmx</code>, respectively assigned in configuration).</p><p>Heap is divided into two spaces or generations: young space and old space. The young space is reserved for the allocation of new objects. Young space consists of an area called <span class="strong"><strong>Eden</strong></span> and<a id="id614" class="indexterm"></a> two smaller survivor spaces. When the nursery becomes full, garbage is collected by running<a id="id615" class="indexterm"></a> a special process called <span class="strong"><strong>young collection</strong></span>, where all the objects, which have lived long enough, are promoted to old space. When the old space becomes full, the garbage is collected there by running a <a id="id616" class="indexterm"></a>process called <span class="strong"><strong>old collection</strong></span>.</p><div class="mediaobject"><img src="graphics/3056_12_03.jpg" /></div><p>The logic behind nursery is that most objects have a very short life span. A young collection is designed to be fast at finding newly allocated objects and moving them to the old space.</p><p>The JVM uses mark and sweep algorithm for garbage collection. Mark and sweep collection consists of two phases.</p><p>During the mark phase, all the objects, which have live references, are marked alive, the rest are presumed candidates for garbage collection. During the sweep phase, the space occupied by garbage collectable candidates is added to the free list, that is, they are available to be allocated to new objects.</p><p>There are<a id="id617" class="indexterm"></a> two improvements to mark and sweep. One is <span class="strong"><strong>concurrent mark and sweep</strong></span> (<span class="strong"><strong>CMS</strong></span>) and the other is parallel mark and sweep. CMS<a id="id618" class="indexterm"></a> focuses on lower latency, while the latter focuses on higher throughput. Both strategies have performance trade-offs. CMS does not do <a id="id619" class="indexterm"></a>compaction, while parallel <span class="strong"><strong>garbage collector</strong></span> (<span class="strong"><strong>GC</strong></span>) performs whole-heap only compaction, which results in pause times. As a thumb rule, for real-time streaming, CMS should be used, and parallel GC otherwise.</p><p>If you would like to have both low latency and high throughput, Java 1.7 update 4 onwards has another option called <span class="strong"><strong>garbage-first GC</strong></span> (<span class="strong"><strong>G1</strong></span>). G1 is a server-style garbage collector, primarily <a id="id620" class="indexterm"></a>meant for multicore machines with large memories. It is planned as a long-term replacement for CMS. So, to modify our thumb rule, if you are using Java 7 onwards, simply use G1.</p><p>G1 partitions the heap into a set of equal-sized regions, where each set is a contiguous range of virtual memory. Each region is assigned a role like Eden, Survivor, and Old. G1 performs a concurrent global marking phase to determine the live references of objects throughout the heap. After the mark phase is over, G1 knows which regions are mostly empty. It collects in these regions first and this frees the larger amount of memory.</p><div class="mediaobject"><img src="graphics/3056_12_04.jpg" /></div><p>The regions selected by G1 as candidates for garbage collection are garbage collected using evacuation. G1 copies objects from one or more regions of the heap to a single region on the heap, and it both compacts and frees up memory. This evacuation is performed in parallel on multiple cores to reduce pause times and increase throughput. So, each garbage collection round reduces fragmentation while working within user-defined pause times.</p><p>There are three <a id="id621" class="indexterm"></a>aspects in memory optimization in Java:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Memory footprint</p></li><li style="list-style-type: disc"><p>Cost of accessing objects in memory</p></li><li style="list-style-type: disc"><p>Cost of garbage collection</p></li></ul></div><p>Java objects, in general, are fast to access but consume much more space than the actual data inside them.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch12lvl1sec77"></a>Using compression to improve performance</h2></div></div><hr /></div><p>Data compression involves encoding information using fewer bits than the original representation. Compression<a id="id622" class="indexterm"></a> has an important role to play in big data technologies. It makes both storage and transport of data more efficient.</p><p>When data is <a id="id623" class="indexterm"></a>compressed, it becomes <a id="id624" class="indexterm"></a>smaller, so both disk I/O and network I/O become faster. It also saves storage space. Every optimization has a cost, and the cost of compression comes in the form of added CPU cycles to compress and decompress data.</p><p>Hadoop needs to split data to put them into blocks, irrespective of whether the data is compressed or not. Only few compression formats are splittable.</p><p>Two most popular compression<a id="id625" class="indexterm"></a> formats for big data loads are LZO and<a id="id626" class="indexterm"></a> Snappy. Snappy is not splittable, while LZO is. Snappy, on the other hand, is a much faster format.</p><p>If compression format is splittable like LZO, input file is first split into blocks and then compressed. Since compression happened at block level, decompression can happen at block level as well as node level.</p><p>If compression format is not splittable, compression happens at file level and then it is split into blocks. In this case, blocks have to be merged back to file before they can be decompressed, so decompression cannot happen at node level.</p><p>For supported compression formats, Spark will deploy codecs automatically to decompress, and no action is required from the user's side.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch12lvl1sec78"></a>Using serialization to improve performance</h2></div></div><hr /></div><p>Serialization <a id="id627" class="indexterm"></a>plays an important part in <a id="id628" class="indexterm"></a>distributed computing. There are two persistence (storage) levels, which support serializing RDDs:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><code class="literal">MEMORY_ONLY_SER</code>: This stores RDDs as serialized objects. It will create one byte array per partition</p></li><li style="list-style-type: disc"><p><code class="literal">MEMORY_AND_DISK_SER</code>: This is similar to the <code class="literal">MEMORY_ONLY_SER</code>, but it spills partitions that do not fit in the memory to disk</p></li></ul></div><p>The following are the steps to add appropriate persistence levels:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Start the Spark shell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell</strong></span>
</pre></div></li><li><p>Import the <code class="literal">StorageLevel</code> and implicits associated with it:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.storage.StorageLevel._</strong></span>
</pre></div></li><li><p>Create an RDD:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val words = sc.textFile("words")</strong></span>
</pre></div></li><li><p>Persist the RDD:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; words.persist(MEMORY_ONLY_SER)</strong></span>
</pre></div></li></ol></div><p>Though <a id="id629" class="indexterm"></a>serialization reduces the <a id="id630" class="indexterm"></a>memory footprint substantially, it adds extra CPU cycles due to deserialization.</p><p>By default, Spark uses Java's serialization. Since the Java serialization is slow, the better approach is to use <code class="literal">Kryo</code> library. <code class="literal">Kryo</code> is much faster and sometimes even 10 times more compact than the<a id="id631" class="indexterm"></a> default.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lvl2sec113"></a>How to do it…</h3></div></div></div><p>You can use <code class="literal">Kryo</code> by doing the following settings in your <code class="literal">SparkConf</code>:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Start the Spark shell by setting <code class="literal">Kryo</code> as serializer:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell --conf spark.serializer=org.apache.spark.serializer.KryoSerializer</strong></span>
</pre></div></li><li><p><code class="literal">Kryo</code> automatically registers most of the core Scala classes, but if you would like to register your own classes, you can use the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; sc.getConf.registerKryoClasses(Array(classOf[com.infoobjects.CustomClass1],classOf[com.infoobjects.CustomClass2])</strong></span>
</pre></div></li></ol></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch12lvl1sec79"></a>Optimizing garbage collection</h2></div></div><hr /></div><p>JVM garbage collection can be a challenge if you have a lot of short lived RDDs. JVM needs to <a id="id632" class="indexterm"></a>go over all the objects to find the ones it needs to garbage collect. The cost of the garbage collection is proportional to the number of objects the GC needs to go through. Therefore, using fewer objects and the data structures that use fewer objects (simpler data structures, such as arrays) helps.</p><p>Serialization also shines here as a byte array needs only one object to be garbage collected.</p><p>By default, Spark uses 60 percent of the executor memory to cache RDDs and the rest 40 percent for regular objects. Sometimes, you may not need 60 percent for RDDs and can reduce this limit so that more space is available for object creation (less need for GC).</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lvl2sec114"></a>How to do it…</h3></div></div></div><p>You can <a id="id633" class="indexterm"></a>set the memory allocated for RDD cache to 40 percent by starting the Spark shell and setting the memory fraction:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell --conf spark.storage.memoryFraction=0.4</strong></span>
</pre></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch12lvl1sec80"></a>Optimizing the level of parallelism</h2></div></div><hr /></div><p>Optimizing the level <a id="id634" class="indexterm"></a>of parallelism is very important to fully utilize the cluster capacity. In the case of HDFS, it means that the number of partitions is the same as the number of <code class="literal">InputSplits</code>, which is mostly the same as the number of blocks.</p><p>In this recipe, we will cover different ways to optimize the number of partitions.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lvl2sec115"></a>How to do it…</h3></div></div></div><p>Specify the number of partitions when loading a file into RDD with the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Start the Spark shell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell</strong></span>
</pre></div></li><li><p>Load the RDD with a custom number of partitions as a second parameter:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; sc.textFile("hdfs://localhost:9000/user/hduser/words",10)</strong></span>
</pre></div></li></ol></div><p>Another approach is to change the default parallelism by performing the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Start the Spark shell with the new value of default parallelism:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell --conf spark.default.parallelism=10</strong></span>
</pre></div></li><li><p>Check the default value of parallelism:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; sc.defaultParallelism</strong></span>
</pre></div></li></ol></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note23"></a>Note</h3><p>You can also reduce the number of partitions using an RDD method called <code class="literal">coalesce(numPartitions)</code> where <code class="literal">numPartitions</code> is the final number of partitions you would like. If you would like the data to be reshuffled over the network, you can call the RDD method called <code class="literal">repartition(numPartitions)</code> where <code class="literal">numPartitions</code> is the final number of partitions you would like.</p></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch12lvl1sec81"></a>Understanding the future of optimization – project Tungsten</h2></div></div><hr /></div><p>Project Tungsten, starting with Spark Version 1.4, is the initiative to bring Spark closer to bare metal. The goal of this project is to substantially improve the memory and CPU efficiency of the<a id="id635" class="indexterm"></a> Spark applications and push the limits of underlying hardware.</p><p>In distributed systems, conventional wisdom has been to always optimize network I/O as that has been the most scarce and bottlenecked resource. This trend has changed in the last few years. Network bandwidth, in the last 5 years, has changed from 1 gigabit per second to 10 gigabit per second.</p><p>On similar lines, the disk bandwidth has increased from 50 MB/s to 500 MB/s and SSDs are being deployed more and more. CPU clock speed, on the other hand, was ~3 GHz 5 years back and is still the same. This has unseated the network and made CPU the new bottleneck in distributed processing.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note24"></a>Note</h3><p>Another trend that has put more load on CPU performance is the new compressed data formats such as Parquet. Both compression and serialization, as we have seen in the previous recipes in this chapter, lead to more CPU cycles. This trend has also pushed the need for CPU optimization to reduce the CPU cycle cost.</p></div><p>On the similar lines, let's look at the memory footprint. In Java, GC does memory management. GC has done an amazing job at taking away the memory management from the programmer and making it transparent. To do this, Java has to put a lot of overhead, and that substantially increases the memory footprint. As an example, a simple String "abcd", which should ideally take 4 bytes, takes 48 bytes in Java.</p><p>What if we do away with GC and manage memory manually like in lower-level programming languages such as C? Java does provide a way to do that since 1.7 version and it is called <code class="literal">sun.misc.Unsafe</code>. Unsafe essentially means that you can build long regions of memory without any safety checks. This is the first feature of project Tungsten.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lvl2sec116"></a>Manual memory management by leverage application semantics</h3></div></div></div><p>Manual memory management by leverage application semantics, which can be very risky if you do<a id="id636" class="indexterm"></a> not know what you are doing, is<a id="id637" class="indexterm"></a> a blessing <a id="id638" class="indexterm"></a>with Spark. We used knowledge of data schema (DataFrames) to directly layout the memory ourselves. It not only gets rid of GC overheads, but lets you minimize the memory footprint.</p><p>The second point is storing data in CPU cache versus memory. Everyone knows CPU cache is <a id="id639" class="indexterm"></a>great as it takes three cycles to <a id="id640" class="indexterm"></a>get data from <a id="id641" class="indexterm"></a>the main memory versus one cycle in cache. This is the second feature of project Tungsten.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lvl2sec117"></a>Using algorithms and data structures</h3></div></div></div><p>Algorithms <a id="id642" class="indexterm"></a>and data structures are used to exploit <a id="id643" class="indexterm"></a>memory hierarchy and enable more cache-aware computation.</p><p>CPU caches are small pools of memory that store the data the CPU is going to need next. CPUs have two types of caches: instruction cache and data cache. Data caches are arranged in hierarchy of L1, L2, and L3:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>L1 cache is the fastest and most expensive cache in a computer. It stores the most critical data and is the first place the CPU looks for information.</p></li><li style="list-style-type: disc"><p>L2 cache is slightly slower than L1, but still located on the same processor chip. It is the second place the CPU looks for information.</p></li><li style="list-style-type: disc"><p>L3 cache is still slower, but is shared by all cores, such as DRAM (memory).</p></li></ul></div><p>These can be seen in the following diagram:</p><div class="mediaobject"><img src="graphics/3056_12_05.jpg" /></div><p>The third point is that Java is not very good at bytecode generation for things like expression evaluation. If this code generation is done manually, it is much more efficient. Code <a id="id644" class="indexterm"></a>generation is the third feature of project <a id="id645" class="indexterm"></a>Tungsten.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch12lvl3sec06"></a>Code generation</h4></div></div></div><p>This involves<a id="id646" class="indexterm"></a> exploiting modern compliers and CPUs to allow efficient operations directly on binary data. Project Tungsten is in its infancy at present and will have much wider support in version 1.5.</p></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">

    <div id="backindex">
      <h1 class="title">Index</h1>
      <h2>A</h2>
      <ul>
        <li>Alternating Least Squares (ALS)<ul><li>about / <a href="#ch10lvl1sec68" title="Collaborative filtering using explicit feedback" class="link">Collaborative filtering using explicit feedback</a></li></ul></li>
        <li>Amazon EC2<ul><li>about / <a href="#ch01lvl1sec12" title="Launching Spark on Amazon EC2" class="link">Launching Spark on Amazon EC2</a></li><li>features / <a href="#ch01lvl1sec12" title="Launching Spark on Amazon EC2" class="link">Launching Spark on Amazon EC2</a></li><li>Spark, launching / <a href="#ch01lvl1sec12" title="Launching Spark on Amazon EC2" class="link">Launching Spark on Amazon EC2</a>, <a href="#ch01lvl1sec12" title="Getting ready" class="link">Getting ready</a>, <a href="#ch01lvl1sec12" title="How to do it..." class="link">How to do it...</a></li><li>URL / <a href="#ch01lvl1sec12" title="Getting ready" class="link">Getting ready</a></li></ul></li>
        <li>Amazon Elastic Block Storage (EBS)<ul><li>about / <a href="#ch03lvl1sec27" title="Loading data from Amazon S3" class="link">Loading data from Amazon S3</a></li></ul></li>
        <li>Amazon Elastic Compute Cloud (EC2)<ul><li>about / <a href="#ch03lvl1sec27" title="Loading data from Amazon S3" class="link">Loading data from Amazon S3</a></li></ul></li>
        <li>Amazon S3<ul><li>data, loading / <a href="#ch03lvl1sec27" title="Loading data from Amazon S3" class="link">Loading data from Amazon S3</a>, <a href="#ch03lvl1sec27" title="How to do it..." class="link">How to do it...</a></li><li>about / <a href="#ch03lvl1sec27" title="Loading data from Amazon S3" class="link">Loading data from Amazon S3</a></li><li>URL / <a href="#ch11lvl1sec72" title="Getting ready" class="link">Getting ready</a></li></ul></li>
        <li>Amazon Web Services (AWS)<ul><li>about / <a href="#ch03lvl1sec27" title="Loading data from Amazon S3" class="link">Loading data from Amazon S3</a></li><li>URL / <a href="#ch03lvl1sec27" title="How to do it..." class="link">How to do it...</a></li></ul></li>
        <li>Apache Cassandra<ul><li>about / <a href="#ch03lvl1sec28" title="Loading data from Apache Cassandra" class="link">Loading data from Apache Cassandra</a></li><li>data, loading / <a href="#ch03lvl1sec28" title="Loading data from Apache Cassandra" class="link">Loading data from Apache Cassandra</a>, <a href="#ch03lvl1sec28" title="How to do it..." class="link">How to do it...</a>, <a href="#ch03lvl1sec28" title="There's more..." class="link">There's more...</a></li></ul></li>
        <li>arbitrary source<ul><li>data, saving / <a href="#ch04lvl1sec38" title="Loading and saving data from an arbitrary source" class="link">Loading and saving data from an arbitrary source</a>, <a href="#ch04lvl1sec38" title="How to do it..." class="link">How to do it...</a></li><li>data, loading / <a href="#ch04lvl1sec38" title="Loading and saving data from an arbitrary source" class="link">Loading and saving data from an arbitrary source</a>, <a href="#ch04lvl1sec38" title="How to do it..." class="link">How to do it...</a></li></ul></li>
      </ul>
      <h2>B</h2>
      <ul>
        <li>batch interval<ul><li>about / <a href="#ch05lvl1sec39" title="Introduction" class="link">Introduction</a></li></ul></li>
        <li>bias<ul><li>versus variance / <a href="#ch07lvl1sec54" title="Doing linear regression with lasso" class="link">Doing linear regression with lasso</a></li><li>about / <a href="#ch07lvl1sec54" title="Doing linear regression with lasso" class="link">Doing linear regression with lasso</a></li></ul></li>
        <li>binaries<ul><li>Spark, installing / <a href="#ch01lvl1sec10" title="Getting ready" class="link">Getting ready</a>, <a href="#ch01lvl1sec10" title="How to do it..." class="link">How to do it...</a></li></ul></li>
        <li>binary classification<ul><li>performing, with SVM / <a href="#ch08lvl1sec58" title="Doing binary classification using SVM" class="link">Doing binary classification using SVM</a>, <a href="#ch08lvl1sec58" title="How to do it…" class="link">How to do it…</a></li></ul></li>
        <li>bivariate analysis<ul><li>about / <a href="#ch07lvl1sec51" title="Introduction" class="link">Introduction</a></li></ul></li>
        <li>broker<ul><li>about / <a href="#ch05lvl1sec42" title="Streaming using Kafka" class="link">Streaming using Kafka</a></li></ul></li>
      </ul>
      <h2>C</h2>
      <ul>
        <li>case classes<ul><li>used, for inferring schema / <a href="#ch04lvl1sec33" title="Inferring schema using case classes" class="link">Inferring schema using case classes</a>, <a href="#ch04lvl1sec33" title="How to do it..." class="link">How to do it...</a></li></ul></li>
        <li>Catalyst optimizer<ul><li>about / <a href="#ch04lvl1sec31" title="Understanding the Catalyst optimizer" class="link">Understanding the Catalyst optimizer</a></li><li>goals / <a href="#ch04lvl1sec31" title="How it works…" class="link">How it works…</a></li><li>using, in analysis phase / <a href="#ch04lvl1sec31" title="Analysis" class="link">Analysis</a></li><li>using, in logical plan optimization phase / <a href="#ch04lvl1sec31" title="Logical plan optimization" class="link">Logical plan optimization</a></li><li>using, in physical planning phase / <a href="#ch04lvl1sec31" title="Physical planning" class="link">Physical planning</a></li><li>using, in code generation phase / <a href="#ch04lvl1sec31" title="Code generation" class="link">Code generation</a></li></ul></li>
        <li>classification<ul><li>about / <a href="#ch07lvl1sec51" title="Introduction" class="link">Introduction</a></li><li>performing, with logistic regression / <a href="#ch08lvl1sec57" title="Doing classification using logistic regression" class="link">Doing classification using logistic regression</a>, <a href="#ch08lvl1sec57" title="Getting ready" class="link">Getting ready</a>, <a href="#ch08lvl1sec57" title="How to do it…" class="link">How to do it…</a></li><li>performing, with decision trees / <a href="#ch08lvl1sec59" title="Doing classification using decision trees" class="link">Doing classification using decision trees</a>, <a href="#ch08lvl1sec59" title="Getting ready" class="link">Getting ready</a>, <a href="#ch08lvl1sec59" title="How to do it…" class="link">How to do it…</a>, <a href="#ch08lvl1sec59" title="How it works…" class="link">How it works…</a></li><li>performing, with Random Forests / <a href="#ch08lvl1sec60" title="Doing classification using Random Forests" class="link">Doing classification using Random Forests</a>, <a href="#ch08lvl1sec60" title="Getting ready" class="link">Getting ready</a>, <a href="#ch08lvl1sec60" title="How to do it…" class="link">How to do it…</a>, <a href="#ch08lvl1sec60" title="How it works…" class="link">How it works…</a></li><li>performing, with Gradient Boosted Trees / <a href="#ch08lvl1sec61" title="Doing classification using Gradient Boosted Trees" class="link">Doing classification using Gradient Boosted Trees</a>, <a href="#ch08lvl1sec61" title="How to do it…" class="link">How to do it…</a></li><li>performing, with Naïve Bayes / <a href="#ch08lvl1sec62" title="Doing classification with Naïve Bayes" class="link">Doing classification with Naïve Bayes</a>, <a href="#ch08lvl1sec62" title="How to do it…" class="link">How to do it…</a></li></ul></li>
        <li>cluster centroids<ul><li>about / <a href="#ch09lvl1sec64" title="Clustering using k-means" class="link">Clustering using k-means</a></li></ul></li>
        <li>clustering<ul><li>about / <a href="#ch09lvl1sec63" title="Introduction" class="link">Introduction</a>, <a href="#ch09lvl1sec64" title="Clustering using k-means" class="link">Clustering using k-means</a></li><li>k-means algorithm, using / <a href="#ch09lvl1sec64" title="Clustering using k-means" class="link">Clustering using k-means</a>, <a href="#ch09lvl1sec64" title="Getting ready" class="link">Getting ready</a>, <a href="#ch09lvl1sec64" title="How to do it…" class="link">How to do it…</a></li></ul></li>
        <li>collaborative filtering<ul><li>about / <a href="#ch10lvl1sec68" title="Collaborative filtering using explicit feedback" class="link">Collaborative filtering using explicit feedback</a></li><li>explicit feedback, using / <a href="#ch10lvl1sec68" title="Collaborative filtering using explicit feedback" class="link">Collaborative filtering using explicit feedback</a>, <a href="#ch10lvl1sec68" title="Getting ready" class="link">Getting ready</a>, <a href="#ch10lvl1sec68" title="How to do it…" class="link">How to do it…</a></li><li>implicit feedback, using / <a href="#ch10lvl1sec69" title="Collaborative filtering using implicit feedback" class="link">Collaborative filtering using implicit feedback</a>, <a href="#ch10lvl1sec69" title="Getting ready" class="link">Getting ready</a>, <a href="#ch10lvl1sec69" title="How it works…" class="link">How it works…</a>, <a href="#ch10lvl1sec69" title="There's more…" class="link">There's more…</a></li></ul></li>
        <li>comma-separate value (CSV) file<ul><li>about / <a href="#ch09lvl1sec64" title="Getting ready" class="link">Getting ready</a></li></ul></li>
        <li>complex event processing (CEP)<ul><li>about / <a href="#ch05lvl1sec39" title="Introduction" class="link">Introduction</a></li></ul></li>
        <li>compression<ul><li>about / <a href="#ch12lvl1sec77" title="Using compression to improve performance" class="link">Using compression to improve performance</a></li><li>used, for performance improvement / <a href="#ch12lvl1sec77" title="Using compression to improve performance" class="link">Using compression to improve performance</a></li></ul></li>
        <li>concurrent mark and sweep (CMS)<ul><li>about / <a href="#ch12lvl1sec76" title="Optimizing memory" class="link">Optimizing memory</a></li></ul></li>
        <li>connected component<ul><li>searching / <a href="#ch11lvl1sec73" title="Finding connected components" class="link">Finding connected components</a>, <a href="#ch11lvl1sec73" title="Getting ready" class="link">Getting ready</a>, <a href="#ch11lvl1sec73" title="How to do it…" class="link">How to do it…</a></li></ul></li>
        <li>Connector/J<ul><li>URL / <a href="#ch04lvl1sec37" title="How to do it..." class="link">How to do it...</a></li></ul></li>
        <li>connector library<ul><li>about / <a href="#ch03lvl1sec28" title="There's more..." class="link">There's more...</a></li></ul></li>
        <li>consumers<ul><li>about / <a href="#ch05lvl1sec42" title="Streaming using Kafka" class="link">Streaming using Kafka</a></li></ul></li>
        <li>correlation<ul><li>about / <a href="#ch06lvl1sec48" title="Calculating correlation" class="link">Calculating correlation</a></li><li>calculating / <a href="#ch06lvl1sec48" title="Calculating correlation" class="link">Calculating correlation</a>, <a href="#ch06lvl1sec48" title="Getting ready" class="link">Getting ready</a>, <a href="#ch06lvl1sec48" title="How to do it…" class="link">How to do it…</a></li><li>positive correlation / <a href="#ch06lvl1sec48" title="Calculating correlation" class="link">Calculating correlation</a></li><li>negative correlation / <a href="#ch06lvl1sec48" title="Calculating correlation" class="link">Calculating correlation</a></li></ul></li>
        <li>cost function<ul><li>about / <a href="#ch07lvl1sec53" title="Understanding cost function" class="link">Understanding cost function</a></li><li>analyzing, for linear regression / <a href="#ch07lvl1sec53" title="Understanding cost function" class="link">Understanding cost function</a></li></ul></li>
        <li>custom InputFormat<ul><li>used, for loading data from HDFS / <a href="#ch03lvl1sec26" title="Loading data from HDFS using a custom InputFormat" class="link">Loading data from HDFS using a custom InputFormat</a>, <a href="#ch03lvl1sec26" title="How to do it..." class="link">How to do it...</a></li></ul></li>
      </ul>
      <h2>D</h2>
      <ul>
        <li>data<ul><li>loading, from local filesystem / <a href="#ch03lvl1sec24" title="Loading data from the local filesystem" class="link">Loading data from the local filesystem</a>, <a href="#ch03lvl1sec24" title="How to do it..." class="link">How to do it...</a></li><li>loading, from HDFS / <a href="#ch03lvl1sec25" title="Loading data from HDFS" class="link">Loading data from HDFS</a>, <a href="#ch03lvl1sec25" title="How to do it..." class="link">How to do it...</a>, <a href="#ch03lvl1sec25" title="There's more…" class="link">There's more…</a></li><li>loading from HDFS, custom InputFormat used / <a href="#ch03lvl1sec26" title="Loading data from HDFS using a custom InputFormat" class="link">Loading data from HDFS using a custom InputFormat</a>, <a href="#ch03lvl1sec26" title="How to do it..." class="link">How to do it...</a></li><li>loading, from Amazon S3 / <a href="#ch03lvl1sec27" title="Loading data from Amazon S3" class="link">Loading data from Amazon S3</a>, <a href="#ch03lvl1sec27" title="How to do it..." class="link">How to do it...</a></li><li>loading, from Apache Cassandra / <a href="#ch03lvl1sec28" title="Loading data from Apache Cassandra" class="link">Loading data from Apache Cassandra</a>, <a href="#ch03lvl1sec28" title="How to do it..." class="link">How to do it...</a>, <a href="#ch03lvl1sec28" title="There's more..." class="link">There's more...</a></li><li>loading, from relational databases / <a href="#ch03lvl1sec29" title="Loading data from relational databases" class="link">Loading data from relational databases</a>, <a href="#ch03lvl1sec29" title="How to do it..." class="link">How to do it...</a>, <a href="#ch03lvl1sec29" title="How it works…" class="link">How it works…</a>, <a href="#ch04lvl1sec37" title="Loading and saving data from relational databases" class="link">Loading and saving data from relational databases</a>, <a href="#ch04lvl1sec37" title="How to do it..." class="link">How to do it...</a></li><li>loading, in Parquet format / <a href="#ch04lvl1sec35" title="Loading and saving data using the Parquet format" class="link">Loading and saving data using the Parquet format</a>, <a href="#ch04lvl1sec35" title="How to do it..." class="link">How to do it...</a>, <a href="#ch04lvl1sec35" title="How it works…" class="link">How it works…</a>, <a href="#ch04lvl1sec35" title="There's more…" class="link">There's more…</a></li><li>saving, in Parquet format / <a href="#ch04lvl1sec35" title="Loading and saving data using the Parquet format" class="link">Loading and saving data using the Parquet format</a>, <a href="#ch04lvl1sec35" title="How to do it..." class="link">How to do it...</a>, <a href="#ch04lvl1sec35" title="How it works…" class="link">How it works…</a>, <a href="#ch04lvl1sec35" title="There's more…" class="link">There's more…</a></li><li>loading, in JSON format / <a href="#ch04lvl1sec36" title="Loading and saving data using the JSON format" class="link">Loading and saving data using the JSON format</a>, <a href="#ch04lvl1sec36" title="How to do it..." class="link">How to do it...</a>, <a href="#ch04lvl1sec36" title="How it works…" class="link">How it works…</a></li><li>saving, in JSON format / <a href="#ch04lvl1sec36" title="Loading and saving data using the JSON format" class="link">Loading and saving data using the JSON format</a>, <a href="#ch04lvl1sec36" title="How to do it..." class="link">How to do it...</a>, <a href="#ch04lvl1sec36" title="How it works…" class="link">How it works…</a></li><li>saving, from relational databases / <a href="#ch04lvl1sec37" title="Loading and saving data from relational databases" class="link">Loading and saving data from relational databases</a>, <a href="#ch04lvl1sec37" title="How to do it..." class="link">How to do it...</a></li><li>loading, from arbitrary source / <a href="#ch04lvl1sec38" title="Loading and saving data from an arbitrary source" class="link">Loading and saving data from an arbitrary source</a>, <a href="#ch04lvl1sec38" title="How to do it..." class="link">How to do it...</a></li><li>saving, from arbitrary source / <a href="#ch04lvl1sec38" title="Loading and saving data from an arbitrary source" class="link">Loading and saving data from an arbitrary source</a>, <a href="#ch04lvl1sec38" title="How to do it..." class="link">How to do it...</a></li></ul></li>
        <li>DataFrame<ul><li>about / <a href="#ch04lvl1sec30" title="Introduction" class="link">Introduction</a></li></ul></li>
        <li>data rate<ul><li>about / <a href="#ch05lvl1sec39" title="Introduction" class="link">Introduction</a></li></ul></li>
        <li>data source API<ul><li>URL / <a href="#ch04lvl1sec38" title="There's more…" class="link">There's more…</a></li></ul></li>
        <li>decision trees<ul><li>classification, performing / <a href="#ch08lvl1sec59" title="Doing classification using decision trees" class="link">Doing classification using decision trees</a>, <a href="#ch08lvl1sec59" title="Getting ready" class="link">Getting ready</a>, <a href="#ch08lvl1sec59" title="How to do it…" class="link">How to do it…</a>, <a href="#ch08lvl1sec59" title="How it works…" class="link">How it works…</a></li></ul></li>
        <li>dimensionality reduction<ul><li>about / <a href="#ch09lvl1sec65" title="Dimensionality reduction with principal component analysis" class="link">Dimensionality reduction with principal component analysis</a></li><li>purposes / <a href="#ch09lvl1sec65" title="Dimensionality reduction with principal component analysis" class="link">Dimensionality reduction with principal component analysis</a></li><li>with principal component analysis (PCA) / <a href="#ch09lvl1sec65" title="Dimensionality reduction with principal component analysis" class="link">Dimensionality reduction with principal component analysis</a>, <a href="#ch09lvl1sec65" title="Getting ready" class="link">Getting ready</a>, <a href="#ch09lvl1sec65" title="How to do it…" class="link">How to do it…</a></li><li>with singular value decomposition (SVD) / <a href="#ch09lvl1sec66" title="Dimensionality reduction with singular value decomposition" class="link">Dimensionality reduction with singular value decomposition</a>, <a href="#ch09lvl1sec66" title="Getting ready" class="link">Getting ready</a>, <a href="#ch09lvl1sec66" title="How to do it…" class="link">How to do it…</a></li></ul></li>
        <li>directed graph<ul><li>about / <a href="#ch11lvl1sec70" title="Introduction" class="link">Introduction</a></li></ul></li>
        <li>directories<ul><li>ephemeral-hdfs / <a href="#ch01lvl1sec12" title="How to do it..." class="link">How to do it...</a></li><li>persistent-hdfs / <a href="#ch01lvl1sec12" title="How to do it..." class="link">How to do it...</a></li><li>hadoop-native / <a href="#ch01lvl1sec12" title="How to do it..." class="link">How to do it...</a></li><li>Scala / <a href="#ch01lvl1sec12" title="How to do it..." class="link">How to do it...</a></li><li>Shark / <a href="#ch01lvl1sec12" title="How to do it..." class="link">How to do it...</a></li><li>Spark / <a href="#ch01lvl1sec12" title="How to do it..." class="link">How to do it...</a></li><li>spark-ec2 / <a href="#ch01lvl1sec12" title="How to do it..." class="link">How to do it...</a></li><li>Tachyon / <a href="#ch01lvl1sec12" title="How to do it..." class="link">How to do it...</a></li></ul></li>
        <li>Discretized Stream (DStream)<ul><li>about / <a href="#ch05lvl1sec39" title="Introduction" class="link">Introduction</a></li></ul></li>
        <li>distributed graph processing<ul><li>data parallel / <a href="#ch11lvl1sec70" title="Introduction" class="link">Introduction</a></li><li>graph parallel / <a href="#ch11lvl1sec70" title="Introduction" class="link">Introduction</a></li></ul></li>
        <li>distributed matrix<ul><li>about / <a href="#ch06lvl1sec46" title="Creating matrices" class="link">Creating matrices</a></li><li>RowMatrix / <a href="#ch06lvl1sec46" title="Creating matrices" class="link">Creating matrices</a></li><li>IndexedRowMatrix / <a href="#ch06lvl1sec46" title="Creating matrices" class="link">Creating matrices</a></li><li>CoordinateMatrix / <a href="#ch06lvl1sec46" title="Creating matrices" class="link">Creating matrices</a></li></ul></li>
        <li>domain-specific language (DSL)<ul><li>about / <a href="#ch04lvl1sec30" title="Introduction" class="link">Introduction</a></li></ul></li>
      </ul>
      <h2>E</h2>
      <ul>
        <li>Eclipse<ul><li>Spark application, developing with Maven / <a href="#ch02lvl1sec19" title="Developing Spark applications in Eclipse with Maven" class="link">Developing Spark applications in Eclipse with Maven</a>, <a href="#ch02lvl1sec19" title="How to do it..." class="link">How to do it...</a></li><li>URL / <a href="#ch02lvl1sec19" title="Getting ready" class="link">Getting ready</a></li><li>Spark application, developing with SBT / <a href="#ch02lvl1sec20" title="Developing Spark applications in Eclipse with SBT" class="link">Developing Spark applications in Eclipse with SBT</a>, <a href="#ch02lvl1sec20" title="How to do it..." class="link">How to do it...</a></li></ul></li>
        <li>Eden<ul><li>about / <a href="#ch12lvl1sec76" title="Optimizing memory" class="link">Optimizing memory</a></li></ul></li>
        <li>ensemble learning algorithms<ul><li>about / <a href="#ch08lvl1sec60" title="Doing classification using Random Forests" class="link">Doing classification using Random Forests</a></li></ul></li>
        <li>Estimator<ul><li>about / <a href="#ch06lvl1sec50" title="Getting ready" class="link">Getting ready</a></li></ul></li>
        <li>explicit feedback<ul><li>used, for collaborative filtering / <a href="#ch10lvl1sec68" title="Collaborative filtering using explicit feedback" class="link">Collaborative filtering using explicit feedback</a>, <a href="#ch10lvl1sec68" title="Getting ready" class="link">Getting ready</a>, <a href="#ch10lvl1sec68" title="How to do it…" class="link">How to do it…</a></li></ul></li>
      </ul>
      <h2>F</h2>
      <ul>
        <li>fat-free XML<ul><li>about / <a href="#ch04lvl1sec36" title="Loading and saving data using the JSON format" class="link">Loading and saving data using the JSON format</a></li></ul></li>
        <li>features, vectors<ul><li>about / <a href="#ch06lvl1sec44" title="Creating vectors" class="link">Creating vectors</a></li></ul></li>
        <li>feature scaling<ul><li>about / <a href="#ch09lvl1sec65" title="Getting ready" class="link">Getting ready</a></li><li>performing / <a href="#ch09lvl1sec65" title="Getting ready" class="link">Getting ready</a></li></ul></li>
      </ul>
      <h2>G</h2>
      <ul>
        <li>garbage-first GC (G1)<ul><li>about / <a href="#ch12lvl1sec76" title="Optimizing memory" class="link">Optimizing memory</a></li></ul></li>
        <li>garbage collection<ul><li>optimizing / <a href="#ch12lvl1sec79" title="Optimizing garbage collection" class="link">Optimizing garbage collection</a>, <a href="#ch12lvl1sec79" title="How to do it…" class="link">How to do it…</a></li></ul></li>
        <li>garbage collector (GC)<ul><li>about / <a href="#ch12lvl1sec76" title="Optimizing memory" class="link">Optimizing memory</a></li></ul></li>
        <li>Gradient Boosted Trees (GBTs)<ul><li>about / <a href="#ch08lvl1sec61" title="Doing classification using Gradient Boosted Trees" class="link">Doing classification using Gradient Boosted Trees</a></li><li>classification, performing / <a href="#ch08lvl1sec61" title="Doing classification using Gradient Boosted Trees" class="link">Doing classification using Gradient Boosted Trees</a>, <a href="#ch08lvl1sec61" title="How to do it…" class="link">How to do it…</a></li></ul></li>
        <li>gradient descent<ul><li>about / <a href="#ch07lvl1sec53" title="Understanding cost function" class="link">Understanding cost function</a></li></ul></li>
        <li>graphs<ul><li>directed graph / <a href="#ch11lvl1sec70" title="Introduction" class="link">Introduction</a></li><li>regular graph / <a href="#ch11lvl1sec70" title="Introduction" class="link">Introduction</a></li><li>fundamental operations / <a href="#ch11lvl1sec71" title="Fundamental operations on graphs" class="link">Fundamental operations on graphs</a>, <a href="#ch11lvl1sec71" title="How to do it…" class="link">How to do it…</a></li></ul></li>
      </ul>
      <h2>H</h2>
      <ul>
        <li>Hadoop distributed file system (HDFS)<ul><li>about / <a href="#ch02lvl1sec18" title="How to do it..." class="link">How to do it...</a></li></ul></li>
        <li>HDFS<ul><li>about / <a href="#ch03lvl1sec23" title="Introduction" class="link">Introduction</a></li><li>data, loading / <a href="#ch03lvl1sec25" title="Loading data from HDFS" class="link">Loading data from HDFS</a>, <a href="#ch03lvl1sec25" title="How to do it..." class="link">How to do it...</a>, <a href="#ch03lvl1sec25" title="There's more…" class="link">There's more…</a></li><li>data loading, custom InputFormat used / <a href="#ch03lvl1sec26" title="Loading data from HDFS using a custom InputFormat" class="link">Loading data from HDFS using a custom InputFormat</a>, <a href="#ch03lvl1sec26" title="How to do it..." class="link">How to do it...</a></li></ul></li>
        <li>HiveContext<ul><li>about / <a href="#ch04lvl1sec32" title="Creating HiveContext" class="link">Creating HiveContext</a></li><li>features / <a href="#ch04lvl1sec32" title="Creating HiveContext" class="link">Creating HiveContext</a></li><li>creating / <a href="#ch04lvl1sec32" title="Creating HiveContext" class="link">Creating HiveContext</a>, <a href="#ch04lvl1sec32" title="Getting ready" class="link">Getting ready</a>, <a href="#ch04lvl1sec32" title="How to do it..." class="link">How to do it...</a></li></ul></li>
        <li>hyperspace<ul><li>about / <a href="#ch06lvl1sec44" title="Creating vectors" class="link">Creating vectors</a></li></ul></li>
        <li>hypothesis function<ul><li>about / <a href="#ch07lvl1sec52" title="Getting ready" class="link">Getting ready</a>, <a href="#ch07lvl1sec53" title="Understanding cost function" class="link">Understanding cost function</a></li></ul></li>
        <li>hypothesis testing<ul><li>about / <a href="#ch06lvl1sec49" title="Doing hypothesis testing" class="link">Doing hypothesis testing</a></li><li>performing / <a href="#ch06lvl1sec49" title="Doing hypothesis testing" class="link">Doing hypothesis testing</a>, <a href="#ch06lvl1sec49" title="How to do it…" class="link">How to do it…</a></li></ul></li>
      </ul>
      <h2>I</h2>
      <ul>
        <li>implicit feedback<ul><li>used, for collaborative filtering / <a href="#ch10lvl1sec69" title="Collaborative filtering using implicit feedback" class="link">Collaborative filtering using implicit feedback</a>, <a href="#ch10lvl1sec69" title="Getting ready" class="link">Getting ready</a>, <a href="#ch10lvl1sec69" title="How it works…" class="link">How it works…</a>, <a href="#ch10lvl1sec69" title="There's more…" class="link">There's more…</a></li></ul></li>
        <li>InputFormat storage format<ul><li>about / <a href="#ch03lvl1sec23" title="Introduction" class="link">Introduction</a></li></ul></li>
        <li>IntelliJ idea<ul><li>Spark application, developing with Maven / <a href="#ch02lvl1sec21" title="Developing a Spark application in IntelliJ IDEA with Maven" class="link">Developing a Spark application in IntelliJ IDEA with Maven</a>, <a href="#ch02lvl1sec21" title="How to do it..." class="link">How to do it...</a></li><li>Spark application, developing with SBT / <a href="#ch02lvl1sec22" title="Developing a Spark application in IntelliJ IDEA with SBT" class="link">Developing a Spark application in IntelliJ IDEA with SBT</a>, <a href="#ch02lvl1sec22" title="How to do it..." class="link">How to do it...</a></li></ul></li>
      </ul>
      <h2>J</h2>
      <ul>
        <li>JdbcRDD<ul><li>about / <a href="#ch03lvl1sec29" title="Loading data from relational databases" class="link">Loading data from relational databases</a>, <a href="#ch03lvl1sec29" title="How it works…" class="link">How it works…</a></li></ul></li>
        <li>JSON format<ul><li>data, loading / <a href="#ch04lvl1sec36" title="Loading and saving data using the JSON format" class="link">Loading and saving data using the JSON format</a>, <a href="#ch04lvl1sec36" title="How to do it..." class="link">How to do it...</a>, <a href="#ch04lvl1sec36" title="How it works…" class="link">How it works…</a></li><li>data, saving / <a href="#ch04lvl1sec36" title="Loading and saving data using the JSON format" class="link">Loading and saving data using the JSON format</a>, <a href="#ch04lvl1sec36" title="How to do it..." class="link">How to do it...</a>, <a href="#ch04lvl1sec36" title="How it works…" class="link">How it works…</a></li></ul></li>
      </ul>
      <h2>K</h2>
      <ul>
        <li>k-means algorithm<ul><li>using / <a href="#ch09lvl1sec64" title="Clustering using k-means" class="link">Clustering using k-means</a>, <a href="#ch09lvl1sec64" title="Getting ready" class="link">Getting ready</a>, <a href="#ch09lvl1sec64" title="How to do it…" class="link">How to do it…</a></li><li>cluster assignment step / <a href="#ch09lvl1sec64" title="Clustering using k-means" class="link">Clustering using k-means</a></li><li>move centroid step / <a href="#ch09lvl1sec64" title="Clustering using k-means" class="link">Clustering using k-means</a></li></ul></li>
        <li>Kafka<ul><li>about / <a href="#ch05lvl1sec42" title="Streaming using Kafka" class="link">Streaming using Kafka</a></li><li>using / <a href="#ch05lvl1sec42" title="Streaming using Kafka" class="link">Streaming using Kafka</a>, <a href="#ch05lvl1sec42" title="How to do it..." class="link">How to do it...</a>, <a href="#ch05lvl1sec42" title="There's more…" class="link">There's more…</a></li></ul></li>
        <li>kilobytes per second (kbps)<ul><li>about / <a href="#ch05lvl1sec39" title="Introduction" class="link">Introduction</a></li></ul></li>
        <li>Kryo library<ul><li>about / <a href="#ch12lvl1sec78" title="Using serialization to improve performance" class="link">Using serialization to improve performance</a></li></ul></li>
      </ul>
      <h2>L</h2>
      <ul>
        <li>labeled point<ul><li>about / <a href="#ch06lvl1sec45" title="Creating a labeled point" class="link">Creating a labeled point</a></li><li>creating / <a href="#ch06lvl1sec45" title="Creating a labeled point" class="link">Creating a labeled point</a>, <a href="#ch06lvl1sec45" title="How to do it…" class="link">How to do it…</a></li></ul></li>
        <li>lasso<ul><li>about / <a href="#ch07lvl1sec54" title="Doing linear regression with lasso" class="link">Doing linear regression with lasso</a></li><li>linear regression, performing / <a href="#ch07lvl1sec54" title="Doing linear regression with lasso" class="link">Doing linear regression with lasso</a>, <a href="#ch07lvl1sec54" title="How to do it…" class="link">How to do it…</a></li><li>URL / <a href="#ch07lvl1sec54" title="Doing linear regression with lasso" class="link">Doing linear regression with lasso</a></li></ul></li>
        <li>latent features<ul><li>about / <a href="#ch10lvl1sec67" title="Introduction" class="link">Introduction</a></li></ul></li>
        <li>level of parallelism<ul><li>optimizing / <a href="#ch12lvl1sec80" title="Optimizing the level of parallelism" class="link">Optimizing the level of parallelism</a></li></ul></li>
        <li>leverage application semantics<ul><li>used, for manual memory management / <a href="#ch12lvl1sec81" title="Manual memory management by leverage application semantics" class="link">Manual memory management by leverage application semantics</a></li></ul></li>
        <li>lineage<ul><li>about / <a href="#ch12lvl1sec75" title="Introduction" class="link">Introduction</a></li></ul></li>
        <li>linear regression<ul><li>about / <a href="#ch07lvl1sec52" title="Using linear regression" class="link">Using linear regression</a>, <a href="#ch07lvl1sec53" title="Understanding cost function" class="link">Understanding cost function</a></li><li>using / <a href="#ch07lvl1sec52" title="Getting ready" class="link">Getting ready</a>, <a href="#ch07lvl1sec52" title="How to do it…" class="link">How to do it…</a></li><li>analyzing, for cost function / <a href="#ch07lvl1sec53" title="Understanding cost function" class="link">Understanding cost function</a></li><li>performing, with lasso / <a href="#ch07lvl1sec54" title="Doing linear regression with lasso" class="link">Doing linear regression with lasso</a>, <a href="#ch07lvl1sec54" title="How to do it…" class="link">How to do it…</a></li></ul></li>
        <li>local filesystem<ul><li>data, loading / <a href="#ch03lvl1sec24" title="Loading data from the local filesystem" class="link">Loading data from the local filesystem</a>, <a href="#ch03lvl1sec24" title="How to do it..." class="link">How to do it...</a></li></ul></li>
        <li>local matrix<ul><li>about / <a href="#ch06lvl1sec46" title="Creating matrices" class="link">Creating matrices</a></li></ul></li>
        <li>logistic function<ul><li>about / <a href="#ch08lvl1sec57" title="Doing classification using logistic regression" class="link">Doing classification using logistic regression</a></li></ul></li>
        <li>logistic regression<ul><li>classification, performing / <a href="#ch08lvl1sec57" title="Doing classification using logistic regression" class="link">Doing classification using logistic regression</a>, <a href="#ch08lvl1sec57" title="Getting ready" class="link">Getting ready</a>, <a href="#ch08lvl1sec57" title="How to do it…" class="link">How to do it…</a></li></ul></li>
        <li>LZO<ul><li>about / <a href="#ch12lvl1sec77" title="Using compression to improve performance" class="link">Using compression to improve performance</a></li></ul></li>
      </ul>
      <h2>M</h2>
      <ul>
        <li>machine learning<ul><li>about / <a href="#ch06lvl1sec43" title="Introduction" class="link">Introduction</a></li></ul></li>
        <li>machine learning pipelines<ul><li>creating, ML library used / <a href="#ch06lvl1sec50" title="Creating machine learning pipelines using ML" class="link">Creating machine learning pipelines using ML</a>, <a href="#ch06lvl1sec50" title="Getting ready" class="link">Getting ready</a>, <a href="#ch06lvl1sec50" title="How to do it…" class="link">How to do it…</a></li></ul></li>
        <li>manual memory management<ul><li>by leverage application semantics / <a href="#ch12lvl1sec81" title="Manual memory management by leverage application semantics" class="link">Manual memory management by leverage application semantics</a></li></ul></li>
        <li>matrices<ul><li>about / <a href="#ch06lvl1sec46" title="Creating matrices" class="link">Creating matrices</a></li><li>creating / <a href="#ch06lvl1sec46" title="Creating matrices" class="link">Creating matrices</a>, <a href="#ch06lvl1sec46" title="How to do it…" class="link">How to do it…</a></li><li>local matrix / <a href="#ch06lvl1sec46" title="Creating matrices" class="link">Creating matrices</a></li><li>distributed matrix / <a href="#ch06lvl1sec46" title="Creating matrices" class="link">Creating matrices</a></li></ul></li>
        <li>Maven<ul><li>Spark source code, building / <a href="#ch01lvl1sec11" title="Building the Spark source code with Maven" class="link">Building the Spark source code with Maven</a>, <a href="#ch01lvl1sec11" title="How to do it..." class="link">How to do it...</a></li><li>Spark application, developing in Eclipse / <a href="#ch02lvl1sec19" title="Developing Spark applications in Eclipse with Maven" class="link">Developing Spark applications in Eclipse with Maven</a>, <a href="#ch02lvl1sec19" title="How to do it..." class="link">How to do it...</a></li><li>about / <a href="#ch02lvl1sec19" title="Developing Spark applications in Eclipse with Maven" class="link">Developing Spark applications in Eclipse with Maven</a></li><li>features / <a href="#ch02lvl1sec19" title="Developing Spark applications in Eclipse with Maven" class="link">Developing Spark applications in Eclipse with Maven</a></li><li>Spark application, developing in IntelliJ idea / <a href="#ch02lvl1sec21" title="Developing a Spark application in IntelliJ IDEA with Maven" class="link">Developing a Spark application in IntelliJ IDEA with Maven</a>, <a href="#ch02lvl1sec21" title="How to do it..." class="link">How to do it...</a></li></ul></li>
        <li>measurement scales<ul><li>Nominal Scale / <a href="#ch06lvl1sec43" title="Introduction" class="link">Introduction</a></li><li>Ordinal Scale / <a href="#ch06lvl1sec43" title="Introduction" class="link">Introduction</a></li><li>Interval Scale / <a href="#ch06lvl1sec43" title="Introduction" class="link">Introduction</a></li><li>Ratio Scale / <a href="#ch06lvl1sec43" title="Introduction" class="link">Introduction</a></li></ul></li>
        <li>megabytes per second (mbps)<ul><li>about / <a href="#ch05lvl1sec39" title="Introduction" class="link">Introduction</a></li></ul></li>
        <li>memory optimization<ul><li>about / <a href="#ch12lvl1sec76" title="Optimizing memory" class="link">Optimizing memory</a></li><li>improvements / <a href="#ch12lvl1sec76" title="Optimizing memory" class="link">Optimizing memory</a></li><li>aspects / <a href="#ch12lvl1sec76" title="Optimizing memory" class="link">Optimizing memory</a></li></ul></li>
        <li>Mesos<ul><li>about / <a href="#ch01lvl1sec09" title="Introduction" class="link">Introduction</a>, <a href="#ch01lvl1sec14" title="Deploying on a cluster with Mesos" class="link">Deploying on a cluster with Mesos</a></li><li>Spark, deploying / <a href="#ch01lvl1sec14" title="Deploying on a cluster with Mesos" class="link">Deploying on a cluster with Mesos</a>, <a href="#ch01lvl1sec14" title="How to do it..." class="link">How to do it...</a></li><li>fine-grained mode / <a href="#ch01lvl1sec14" title="How to do it..." class="link">How to do it...</a></li><li>coarse-grained mode / <a href="#ch01lvl1sec14" title="How to do it..." class="link">How to do it...</a></li></ul></li>
        <li>ML library<ul><li>used, for creating machine learning pipelines / <a href="#ch06lvl1sec50" title="Creating machine learning pipelines using ML" class="link">Creating machine learning pipelines using ML</a>, <a href="#ch06lvl1sec50" title="Getting ready" class="link">Getting ready</a>, <a href="#ch06lvl1sec50" title="How to do it…" class="link">How to do it…</a></li></ul></li>
        <li>MovieLens dataset<ul><li>URL / <a href="#ch10lvl1sec67" title="Introduction" class="link">Introduction</a></li></ul></li>
        <li>multigraph<ul><li>about / <a href="#ch11lvl1sec70" title="Introduction" class="link">Introduction</a></li></ul></li>
        <li>multivariate analysis<ul><li>about / <a href="#ch07lvl1sec51" title="Introduction" class="link">Introduction</a></li></ul></li>
      </ul>
      <h2>N</h2>
      <ul>
        <li>Naïve Bayes<ul><li>classification, performing / <a href="#ch08lvl1sec62" title="Doing classification with Naïve Bayes" class="link">Doing classification with Naïve Bayes</a>, <a href="#ch08lvl1sec62" title="How to do it…" class="link">How to do it…</a></li></ul></li>
        <li>Naïve Bayes assumption<ul><li>about / <a href="#ch08lvl1sec62" title="Doing classification with Naïve Bayes" class="link">Doing classification with Naïve Bayes</a></li></ul></li>
        <li>Naïve Bayes classifier<ul><li>about / <a href="#ch08lvl1sec62" title="Doing classification with Naïve Bayes" class="link">Doing classification with Naïve Bayes</a></li></ul></li>
        <li>negative correlation<ul><li>about / <a href="#ch06lvl1sec48" title="Calculating correlation" class="link">Calculating correlation</a></li></ul></li>
        <li>neighborhood aggregation<ul><li>performing / <a href="#ch11lvl1sec74" title="Performing neighborhood aggregation" class="link">Performing neighborhood aggregation</a>, <a href="#ch11lvl1sec74" title="How to do it…" class="link">How to do it…</a></li></ul></li>
        <li>null hypothesis<ul><li>about / <a href="#ch06lvl1sec49" title="Doing hypothesis testing" class="link">Doing hypothesis testing</a></li></ul></li>
      </ul>
      <h2>O</h2>
      <ul>
        <li>old collection<ul><li>about / <a href="#ch12lvl1sec76" title="Optimizing memory" class="link">Optimizing memory</a></li></ul></li>
        <li>ordinary least squares (OLS)<ul><li>about / <a href="#ch07lvl1sec54" title="Doing linear regression with lasso" class="link">Doing linear regression with lasso</a></li><li>prediction accuracy / <a href="#ch07lvl1sec54" title="Doing linear regression with lasso" class="link">Doing linear regression with lasso</a></li><li>interpretation / <a href="#ch07lvl1sec54" title="Doing linear regression with lasso" class="link">Doing linear regression with lasso</a></li></ul></li>
        <li>OutputFormat storage format<ul><li>about / <a href="#ch03lvl1sec23" title="Introduction" class="link">Introduction</a></li></ul></li>
        <li>overfitting<ul><li>about / <a href="#ch08lvl1sec59" title="How it works…" class="link">How it works…</a></li></ul></li>
      </ul>
      <h2>P</h2>
      <ul>
        <li>PageRank<ul><li>about / <a href="#ch11lvl1sec72" title="Using PageRank" class="link">Using PageRank</a></li><li>using / <a href="#ch11lvl1sec72" title="Using PageRank" class="link">Using PageRank</a>, <a href="#ch11lvl1sec72" title="Getting ready" class="link">Getting ready</a>, <a href="#ch11lvl1sec72" title="How to do it…" class="link">How to do it…</a></li></ul></li>
        <li>parallel edges<ul><li>about / <a href="#ch11lvl1sec70" title="Introduction" class="link">Introduction</a></li></ul></li>
        <li>Parquet format<ul><li>data, saving / <a href="#ch04lvl1sec35" title="Loading and saving data using the Parquet format" class="link">Loading and saving data using the Parquet format</a>, <a href="#ch04lvl1sec35" title="How to do it..." class="link">How to do it...</a>, <a href="#ch04lvl1sec35" title="How it works…" class="link">How it works…</a>, <a href="#ch04lvl1sec35" title="There's more…" class="link">There's more…</a></li><li>data, loading / <a href="#ch04lvl1sec35" title="Loading and saving data using the Parquet format" class="link">Loading and saving data using the Parquet format</a>, <a href="#ch04lvl1sec35" title="How to do it..." class="link">How to do it...</a>, <a href="#ch04lvl1sec35" title="How it works…" class="link">How it works…</a>, <a href="#ch04lvl1sec35" title="There's more…" class="link">There's more…</a></li></ul></li>
        <li>partitioned log<ul><li>about / <a href="#ch05lvl1sec42" title="Streaming using Kafka" class="link">Streaming using Kafka</a></li></ul></li>
        <li>performance improvement<ul><li>with compression / <a href="#ch12lvl1sec77" title="Using compression to improve performance" class="link">Using compression to improve performance</a></li><li>with serialization / <a href="#ch12lvl1sec78" title="Using serialization to improve performance" class="link">Using serialization to improve performance</a></li></ul></li>
        <li>plain old Java objects (POJOs)<ul><li>about / <a href="#ch04lvl1sec33" title="Inferring schema using case classes" class="link">Inferring schema using case classes</a></li></ul></li>
        <li>positive correlation<ul><li>about / <a href="#ch06lvl1sec48" title="Calculating correlation" class="link">Calculating correlation</a></li></ul></li>
        <li>principal component analysis (PCA)<ul><li>about / <a href="#ch09lvl1sec65" title="Dimensionality reduction with principal component analysis" class="link">Dimensionality reduction with principal component analysis</a></li><li>using / <a href="#ch09lvl1sec65" title="Dimensionality reduction with principal component analysis" class="link">Dimensionality reduction with principal component analysis</a>, <a href="#ch09lvl1sec65" title="Getting ready" class="link">Getting ready</a>, <a href="#ch09lvl1sec65" title="How to do it…" class="link">How to do it…</a></li></ul></li>
        <li>producers<ul><li>about / <a href="#ch05lvl1sec42" title="Streaming using Kafka" class="link">Streaming using Kafka</a></li></ul></li>
        <li>projection error<ul><li>about / <a href="#ch09lvl1sec65" title="Dimensionality reduction with principal component analysis" class="link">Dimensionality reduction with principal component analysis</a></li></ul></li>
        <li>project Tungsten<ul><li>about / <a href="#ch12lvl1sec81" title="Understanding the future of optimization – project Tungsten" class="link">Understanding the future of optimization – project Tungsten</a></li><li>manual memory management / <a href="#ch12lvl1sec81" title="Manual memory management by leverage application semantics" class="link">Manual memory management by leverage application semantics</a></li><li>algorithms, using / <a href="#ch12lvl1sec81" title="Using algorithms and data structures" class="link">Using algorithms and data structures</a></li><li>data structures, using / <a href="#ch12lvl1sec81" title="Using algorithms and data structures" class="link">Using algorithms and data structures</a></li><li>code generation / <a href="#ch12lvl1sec81" title="Code generation" class="link">Code generation</a></li></ul></li>
      </ul>
      <h2>Q</h2>
      <ul>
        <li>Quasi quotes<ul><li>about / <a href="#ch04lvl1sec31" title="Code generation" class="link">Code generation</a></li></ul></li>
      </ul>
      <h2>R</h2>
      <ul>
        <li>Random Forests<ul><li>classification, performing / <a href="#ch08lvl1sec60" title="Doing classification using Random Forests" class="link">Doing classification using Random Forests</a>, <a href="#ch08lvl1sec60" title="Getting ready" class="link">Getting ready</a>, <a href="#ch08lvl1sec60" title="How to do it…" class="link">How to do it…</a>, <a href="#ch08lvl1sec60" title="How it works…" class="link">How it works…</a></li></ul></li>
        <li>RDD<ul><li>about / <a href="#ch12lvl1sec75" title="Introduction" class="link">Introduction</a></li><li>wordcount example / <a href="#ch12lvl1sec75" title="Introduction" class="link">Introduction</a></li></ul></li>
        <li>recommender systems<ul><li>about / <a href="#ch10lvl1sec67" title="Introduction" class="link">Introduction</a></li></ul></li>
        <li>regression<ul><li>about / <a href="#ch07lvl1sec51" title="Introduction" class="link">Introduction</a></li></ul></li>
        <li>relational databases<ul><li>data, loading / <a href="#ch03lvl1sec29" title="Loading data from relational databases" class="link">Loading data from relational databases</a>, <a href="#ch03lvl1sec29" title="How to do it..." class="link">How to do it...</a>, <a href="#ch03lvl1sec29" title="How it works…" class="link">How it works…</a>, <a href="#ch04lvl1sec37" title="Loading and saving data from relational databases" class="link">Loading and saving data from relational databases</a>, <a href="#ch04lvl1sec37" title="How to do it..." class="link">How to do it...</a></li><li>data, saving / <a href="#ch04lvl1sec37" title="Loading and saving data from relational databases" class="link">Loading and saving data from relational databases</a>, <a href="#ch04lvl1sec37" title="How to do it..." class="link">How to do it...</a></li></ul></li>
        <li>resilient distributed property graph<ul><li>about / <a href="#ch11lvl1sec70" title="Introduction" class="link">Introduction</a></li></ul></li>
        <li>ridge regression<ul><li>about / <a href="#ch07lvl1sec55" title="Doing ridge regression" class="link">Doing ridge regression</a></li><li>performing / <a href="#ch07lvl1sec55" title="Doing ridge regression" class="link">Doing ridge regression</a>, <a href="#ch07lvl1sec55" title="How to do it…" class="link">How to do it…</a></li></ul></li>
        <li>Root Mean Square Error (RMSE)<ul><li>about / <a href="#ch10lvl1sec68" title="Collaborative filtering using explicit feedback" class="link">Collaborative filtering using explicit feedback</a></li></ul></li>
      </ul>
      <h2>S</h2>
      <ul>
        <li>s3*//<ul><li>about / <a href="#ch03lvl1sec27" title="How to do it..." class="link">How to do it...</a></li></ul></li>
        <li>s3n*//<ul><li>about / <a href="#ch03lvl1sec27" title="How to do it..." class="link">How to do it...</a></li></ul></li>
        <li>SBT<ul><li>about / <a href="#ch02lvl1sec20" title="Developing Spark applications in Eclipse with SBT" class="link">Developing Spark applications in Eclipse with SBT</a></li><li>Spark application, developing in Eclipse / <a href="#ch02lvl1sec20" title="Developing Spark applications in Eclipse with SBT" class="link">Developing Spark applications in Eclipse with SBT</a>, <a href="#ch02lvl1sec20" title="How to do it..." class="link">How to do it...</a></li><li>Spark application, developing in IntelliJ idea / <a href="#ch02lvl1sec22" title="Developing a Spark application in IntelliJ IDEA with SBT" class="link">Developing a Spark application in IntelliJ IDEA with SBT</a>, <a href="#ch02lvl1sec22" title="How to do it..." class="link">How to do it...</a></li></ul></li>
        <li>sbt-assembly plugin<ul><li>merge strategies / <a href="#ch03lvl1sec28" title="Merge strategies in sbt-assembly" class="link">Merge strategies in sbt-assembly</a></li></ul></li>
        <li>schema<ul><li>inferring, case classes used / <a href="#ch04lvl1sec33" title="Inferring schema using case classes" class="link">Inferring schema using case classes</a>, <a href="#ch04lvl1sec33" title="How to do it..." class="link">How to do it...</a></li><li>programmatically specifying / <a href="#ch04lvl1sec34" title="Programmatically specifying the schema" class="link">Programmatically specifying the schema</a>, <a href="#ch04lvl1sec34" title="How to do it..." class="link">How to do it...</a>, <a href="#ch04lvl1sec34" title="How it works…" class="link">How it works…</a></li></ul></li>
        <li>SchemaRDD<ul><li>about / <a href="#ch04lvl1sec30" title="Introduction" class="link">Introduction</a></li></ul></li>
        <li>secure shell protocol (SSH)<ul><li>about / <a href="#ch01lvl1sec12" title="How to do it..." class="link">How to do it...</a></li></ul></li>
        <li>serialization<ul><li>used, for performance improvement / <a href="#ch12lvl1sec78" title="Using serialization to improve performance" class="link">Using serialization to improve performance</a></li></ul></li>
        <li>sigmoid function<ul><li>about / <a href="#ch08lvl1sec57" title="Doing classification using logistic regression" class="link">Doing classification using logistic regression</a></li></ul></li>
        <li>singular value decomposition (SVD)<ul><li>using / <a href="#ch09lvl1sec66" title="Dimensionality reduction with singular value decomposition" class="link">Dimensionality reduction with singular value decomposition</a>, <a href="#ch09lvl1sec66" title="Getting ready" class="link">Getting ready</a>, <a href="#ch09lvl1sec66" title="How to do it…" class="link">How to do it…</a></li></ul></li>
        <li>sliding window, parameters<ul><li>window length / <a href="#ch05lvl1sec39" title="Introduction" class="link">Introduction</a></li><li>sliding interval / <a href="#ch05lvl1sec39" title="Introduction" class="link">Introduction</a></li></ul></li>
        <li>Snappy<ul><li>about / <a href="#ch12lvl1sec77" title="Using compression to improve performance" class="link">Using compression to improve performance</a></li></ul></li>
        <li>Spark<ul><li>about / <a href="#ch01lvl1sec09" title="Introduction" class="link">Introduction</a></li><li>ecosystem / <a href="#ch01lvl1sec09" title="Introduction" class="link">Introduction</a></li><li>URL / <a href="#ch01lvl1sec10" title="Installing Spark from binaries" class="link">Installing Spark from binaries</a></li><li>installing, from binaries / <a href="#ch01lvl1sec10" title="Getting ready" class="link">Getting ready</a>, <a href="#ch01lvl1sec10" title="How to do it..." class="link">How to do it...</a></li><li>source code, building with Maven / <a href="#ch01lvl1sec11" title="Building the Spark source code with Maven" class="link">Building the Spark source code with Maven</a>, <a href="#ch01lvl1sec11" title="How to do it..." class="link">How to do it...</a></li><li>launching, on Amazon EC2 / <a href="#ch01lvl1sec12" title="Launching Spark on Amazon EC2" class="link">Launching Spark on Amazon EC2</a>, <a href="#ch01lvl1sec12" title="Getting ready" class="link">Getting ready</a>, <a href="#ch01lvl1sec12" title="How to do it..." class="link">How to do it...</a></li><li>deploying, on cluster in standalone mode / <a href="#ch01lvl1sec13" title="Deploying on a cluster in standalone mode" class="link">Deploying on a cluster in standalone mode</a>, <a href="#ch01lvl1sec13" title="How to do it..." class="link">How to do it...</a>, <a href="#ch01lvl1sec13" title="How it works..." class="link">How it works...</a></li><li>deploying, on cluster with Mesos / <a href="#ch01lvl1sec14" title="Deploying on a cluster with Mesos" class="link">Deploying on a cluster with Mesos</a>, <a href="#ch01lvl1sec14" title="How to do it..." class="link">How to do it...</a></li><li>deploying, on cluster with YARN / <a href="#ch01lvl1sec15" title="Deploying on a cluster with YARN" class="link">Deploying on a cluster with YARN</a>, <a href="#ch01lvl1sec15" title="How to do it..." class="link">How to do it...</a>, <a href="#ch01lvl1sec15" title="How it works…" class="link">How it works…</a></li></ul></li>
        <li>spark-ec2 script<ul><li>about / <a href="#ch01lvl1sec12" title="Getting ready" class="link">Getting ready</a></li></ul></li>
        <li>Spark 1.3 version<ul><li>URL / <a href="#ch01lvl1sec10" title="How to do it..." class="link">How to do it...</a></li></ul></li>
        <li>Spark application<ul><li>developing, in Eclipse with Maven / <a href="#ch02lvl1sec19" title="Developing Spark applications in Eclipse with Maven" class="link">Developing Spark applications in Eclipse with Maven</a>, <a href="#ch02lvl1sec19" title="How to do it..." class="link">How to do it...</a></li><li>developing, in Eclipse with SBT / <a href="#ch02lvl1sec20" title="Developing Spark applications in Eclipse with SBT" class="link">Developing Spark applications in Eclipse with SBT</a>, <a href="#ch02lvl1sec20" title="How to do it..." class="link">How to do it...</a></li><li>developing, in IntelliJ idea with Maven / <a href="#ch02lvl1sec21" title="Developing a Spark application in IntelliJ IDEA with Maven" class="link">Developing a Spark application in IntelliJ IDEA with Maven</a>, <a href="#ch02lvl1sec21" title="How to do it..." class="link">How to do it...</a></li><li>developing, in IntelliJ idea / <a href="#ch02lvl1sec22" title="Developing a Spark application in IntelliJ IDEA with SBT" class="link">Developing a Spark application in IntelliJ IDEA with SBT</a>, <a href="#ch02lvl1sec22" title="How to do it..." class="link">How to do it...</a></li></ul></li>
        <li>Spark master<ul><li>about / <a href="#ch01lvl1sec13" title="How it works..." class="link">How it works...</a></li></ul></li>
        <li>Spark RDD<ul><li>about / <a href="#ch01lvl1sec16" title="Using Tachyon as an off-heap storage layer" class="link">Using Tachyon as an off-heap storage layer</a></li><li>challenges / <a href="#ch01lvl1sec16" title="Using Tachyon as an off-heap storage layer" class="link">Using Tachyon as an off-heap storage layer</a></li></ul></li>
        <li>Spark shell<ul><li>exploring / <a href="#ch02lvl1sec18" title="Exploring the Spark shell" class="link">Exploring the Spark shell</a>, <a href="#ch02lvl1sec18" title="How to do it..." class="link">How to do it...</a></li></ul></li>
        <li>Spark SQL<ul><li>about / <a href="#ch04lvl1sec30" title="Introduction" class="link">Introduction</a></li></ul></li>
        <li>squared error function<ul><li>about / <a href="#ch07lvl1sec53" title="Understanding cost function" class="link">Understanding cost function</a></li></ul></li>
        <li>Standalone mode<ul><li>about / <a href="#ch01lvl1sec09" title="Introduction" class="link">Introduction</a></li><li>reference link / <a href="#ch01lvl1sec13" title="See also" class="link">See also</a></li></ul></li>
        <li>standalone mode<ul><li>Spark, deploying / <a href="#ch01lvl1sec13" title="Deploying on a cluster in standalone mode" class="link">Deploying on a cluster in standalone mode</a>, <a href="#ch01lvl1sec13" title="How to do it..." class="link">How to do it...</a>, <a href="#ch01lvl1sec13" title="How it works..." class="link">How it works...</a></li></ul></li>
        <li>start-all.sh script<ul><li>about / <a href="#ch01lvl1sec13" title="How to do it..." class="link">How to do it...</a></li></ul></li>
        <li>start-master.sh script<ul><li>about / <a href="#ch01lvl1sec13" title="How to do it..." class="link">How to do it...</a></li></ul></li>
        <li>start-slaves.sh script<ul><li>about / <a href="#ch01lvl1sec13" title="How to do it..." class="link">How to do it...</a></li></ul></li>
        <li>stop-all.sh script<ul><li>about / <a href="#ch01lvl1sec13" title="How to do it..." class="link">How to do it...</a></li></ul></li>
        <li>stop-master.sh script<ul><li>about / <a href="#ch01lvl1sec13" title="How to do it..." class="link">How to do it...</a></li></ul></li>
        <li>stop-slaves.sh script<ul><li>about / <a href="#ch01lvl1sec13" title="How to do it..." class="link">How to do it...</a></li></ul></li>
        <li>Streaming<ul><li>about / <a href="#ch05lvl1sec39" title="Introduction" class="link">Introduction</a></li><li>used, for word count / <a href="#ch05lvl1sec40" title="Word count using Streaming" class="link">Word count using Streaming</a>, <a href="#ch05lvl1sec40" title="How to do it..." class="link">How to do it...</a></li><li>with Kafka / <a href="#ch05lvl1sec42" title="Streaming using Kafka" class="link">Streaming using Kafka</a>, <a href="#ch05lvl1sec42" title="How to do it..." class="link">How to do it...</a>, <a href="#ch05lvl1sec42" title="There's more…" class="link">There's more…</a></li></ul></li>
        <li>subgraph<ul><li>about / <a href="#ch11lvl1sec73" title="Finding connected components" class="link">Finding connected components</a></li></ul></li>
        <li>summary statistics<ul><li>about / <a href="#ch06lvl1sec47" title="Calculating summary statistics" class="link">Calculating summary statistics</a></li><li>calculating / <a href="#ch06lvl1sec47" title="Calculating summary statistics" class="link">Calculating summary statistics</a>, <a href="#ch06lvl1sec47" title="How to do it…" class="link">How to do it…</a></li></ul></li>
        <li>supervised learning<ul><li>about / <a href="#ch07lvl1sec51" title="Introduction" class="link">Introduction</a>, <a href="#ch09lvl1sec63" title="Introduction" class="link">Introduction</a></li><li>regression / <a href="#ch07lvl1sec51" title="Introduction" class="link">Introduction</a></li><li>classification / <a href="#ch07lvl1sec51" title="Introduction" class="link">Introduction</a></li><li>example / <a href="#ch07lvl1sec51" title="Introduction" class="link">Introduction</a></li></ul></li>
        <li>support vector machines (SVM)<ul><li>about / <a href="#ch07lvl1sec51" title="Introduction" class="link">Introduction</a></li></ul></li>
        <li>support vectors<ul><li>about / <a href="#ch08lvl1sec58" title="Doing binary classification using SVM" class="link">Doing binary classification using SVM</a></li></ul></li>
        <li>SVM<ul><li>binary classification, performing / <a href="#ch08lvl1sec58" title="Doing binary classification using SVM" class="link">Doing binary classification using SVM</a>, <a href="#ch08lvl1sec58" title="How to do it…" class="link">How to do it…</a></li></ul></li>
      </ul>
      <h2>T</h2>
      <ul>
        <li>Tachyon<ul><li>about / <a href="#ch01lvl1sec09" title="Introduction" class="link">Introduction</a></li><li>using, as off-heap storage layer / <a href="#ch01lvl1sec16" title="Using Tachyon as an off-heap storage layer" class="link">Using Tachyon as an off-heap storage layer</a>, <a href="#ch01lvl1sec16" title="How to do it..." class="link">How to do it...</a></li><li>reference link / <a href="#ch01lvl1sec16" title="See also" class="link">See also</a></li></ul></li>
        <li>text classification<ul><li>about / <a href="#ch08lvl1sec62" title="Doing classification with Naïve Bayes" class="link">Doing classification with Naïve Bayes</a></li></ul></li>
        <li>topics<ul><li>about / <a href="#ch05lvl1sec42" title="Streaming using Kafka" class="link">Streaming using Kafka</a></li></ul></li>
        <li>training data<ul><li>about / <a href="#ch08lvl1sec58" title="Doing binary classification using SVM" class="link">Doing binary classification using SVM</a></li></ul></li>
        <li>Twitter data<ul><li>live streaming / <a href="#ch05lvl1sec41" title="Streaming Twitter data" class="link">Streaming Twitter data</a>, <a href="#ch05lvl1sec41" title="How to do it..." class="link">How to do it...</a></li></ul></li>
      </ul>
      <h2>U</h2>
      <ul>
        <li>unsupervised learning<ul><li>about / <a href="#ch09lvl1sec63" title="Introduction" class="link">Introduction</a></li></ul></li>
        <li>use case, clustering<ul><li>market segmentation / <a href="#ch09lvl1sec64" title="Clustering using k-means" class="link">Clustering using k-means</a></li><li>social network analysis / <a href="#ch09lvl1sec64" title="Clustering using k-means" class="link">Clustering using k-means</a></li><li>data center computing clusters / <a href="#ch09lvl1sec64" title="Clustering using k-means" class="link">Clustering using k-means</a></li><li>astronomical data analysis / <a href="#ch09lvl1sec64" title="Clustering using k-means" class="link">Clustering using k-means</a></li><li>real estate / <a href="#ch09lvl1sec64" title="Clustering using k-means" class="link">Clustering using k-means</a></li><li>text analysis / <a href="#ch09lvl1sec64" title="Clustering using k-means" class="link">Clustering using k-means</a></li></ul></li>
      </ul>
      <h2>V</h2>
      <ul>
        <li>variance<ul><li>versus bias / <a href="#ch07lvl1sec54" title="Doing linear regression with lasso" class="link">Doing linear regression with lasso</a></li><li>about / <a href="#ch07lvl1sec54" title="Doing linear regression with lasso" class="link">Doing linear regression with lasso</a></li></ul></li>
        <li>vectors<ul><li>creating / <a href="#ch06lvl1sec44" title="Creating vectors" class="link">Creating vectors</a>, <a href="#ch06lvl1sec44" title="How it works..." class="link">How it works...</a></li></ul></li>
      </ul>
      <h2>W</h2>
      <ul>
        <li>Wikipedia page link data<ul><li>URL / <a href="#ch11lvl1sec72" title="Getting ready" class="link">Getting ready</a></li></ul></li>
        <li>word count<ul><li>with Streaming / <a href="#ch05lvl1sec40" title="Word count using Streaming" class="link">Word count using Streaming</a>, <a href="#ch05lvl1sec40" title="How to do it..." class="link">How to do it...</a></li></ul></li>
        <li>worker<ul><li>about / <a href="#ch01lvl1sec13" title="How it works..." class="link">How it works...</a></li></ul></li>
      </ul>
      <h2>Y</h2>
      <ul>
        <li>YARN<ul><li>about / <a href="#ch01lvl1sec09" title="Introduction" class="link">Introduction</a>, <a href="#ch01lvl1sec15" title="Deploying on a cluster with YARN" class="link">Deploying on a cluster with YARN</a></li><li>Spark, deploying on cluster / <a href="#ch01lvl1sec15" title="Deploying on a cluster with YARN" class="link">Deploying on a cluster with YARN</a>, <a href="#ch01lvl1sec15" title="How to do it..." class="link">How to do it...</a>, <a href="#ch01lvl1sec15" title="How it works…" class="link">How it works…</a></li><li>yarn-client mode / <a href="#ch01lvl1sec15" title="How it works…" class="link">How it works…</a></li><li>yarn-cluster mode / <a href="#ch01lvl1sec15" title="How it works…" class="link">How it works…</a></li><li>configuration parameters / <a href="#ch01lvl1sec15" title="How it works…" class="link">How it works…</a></li></ul></li>
        <li>young collection<ul><li>about / <a href="#ch12lvl1sec76" title="Optimizing memory" class="link">Optimizing memory</a></li></ul></li>
      </ul>
      <h2>Z</h2>
      <ul>
        <li>z density of house<ul><li>about / <a href="#ch09lvl1sec65" title="Getting ready" class="link">Getting ready</a></li></ul></li>
      </ul>
    </div>
  </div></div></div>
</div></div></div></body></html>
