<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="">

    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0"> <!--320-->
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no, user-scalable=no">

    <link rel="icon" href="../../mapt/images/favicon.ico">

    <link rel="stylesheet" href="../../mapt/css/font-awesome.css">
    <link rel="stylesheet" href="../../mapt/css/google-fonts.css">
    <link rel="stylesheet" href="../../mapt/css/devicon.css">

    <link rel="stylesheet" href="../../mapt/css/bootstrap.css">
    <link rel="stylesheet" href="../../mapt/css/bootstrap-xl.css">
    <link rel="stylesheet" href="../../mapt/css/magnific-popup.css">
    <link rel="stylesheet" href="../../mapt/css/prism.css">
    <link rel="stylesheet" href="../../mapt/css/hljs-github.css">

    <link rel="stylesheet" href="../../mapt/css/mapt.css">
    <link rel="stylesheet" href="../../mapt/css/custom.css">

    <script src="../../mapt/js/jquery.js"></script>
    <script src="../../mapt/js/bootstrap.js"></script>
    <script src="../../mapt/js/jquery.magnific-popup.js"></script>
    <script src="../../mapt/js/highlight.min.js"></script>

    <script src="../../mapt/js/custom.js"></script>
    
    <title>Apache Spark for Data Science Cookbook</title>
</head>

<body class="home-body">
    <div id="wrapper">
        <div id="sidebar-wrapper">    
            <ul class="sidebar-nav">
                <div class="list-group" id="sidebar-nav" role="tablist">
                    <li>
                        <a href="../../index.html" class="sidenav-menu-holder back-btn" id="back-link">
                            <span class="sidenav-menu">Book List</span>
                            <span class="pull-left mr5"><i class="fa fa-chevron-left"></i></span>
                        </a>
                    </li>
                    
                    <li class="book-info copyright">
                        <span class="info text-nowrap"><span class="copyleft">&copy;</span><span><strong>RuTracker</strong>.org</span></span>
                    </li>          
                    <li class="book-info copyright">
                        <span class="info text-nowrap">Pub date: <strong>22 Dec 2016</strong></span>
                    </li>         
                    <li class="book-info">
                        <span class="info text-nowrap">Price: â‚¬<strong>31.99</strong></span>
                        <span class="info text-nowrap">ISBN: <strong>9781785880100</strong></span>
                    </li>     
            
                    <li>
                        <a href="graphics/cover.jpg" class="sidenav-menu-holder cover-img">
                            <img src="graphics/cover.jpg" class="cover-image">
                        </a>
                    </li>        
            
                    <div class="book_navigation">
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse1">
                                <div class="section-name">1: Big Data Analytics with Spark</div>
                            </a>
                        </li>
                        <div id="collapse1" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="1" class="sub-nav">
                                <a href="#ch01">
                                    <div class="section-name">Chapter 1: Big Data Analytics with Spark</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec8" class="sub-nav">
                                <a href="#ch01lvl1sec8">                    
                                    <div class="section-name">Introduction</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec9" class="sub-nav">
                                <a href="#ch01lvl1sec9">                    
                                    <div class="section-name">Initializing SparkContext</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec10" class="sub-nav">
                                <a href="#ch01lvl1sec10">                    
                                    <div class="section-name">Working with Spark&#x27;s Python and Scala shells</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec11" class="sub-nav">
                                <a href="#ch01lvl1sec11">                    
                                    <div class="section-name">Building standalone applications</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec12" class="sub-nav">
                                <a href="#ch01lvl1sec12">                    
                                    <div class="section-name">Working with the Spark programming model</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec13" class="sub-nav">
                                <a href="#ch01lvl1sec13">                    
                                    <div class="section-name">Working with pair RDDs</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec14" class="sub-nav">
                                <a href="#ch01lvl1sec14">                    
                                    <div class="section-name">Persisting RDDs</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec15" class="sub-nav">
                                <a href="#ch01lvl1sec15">                    
                                    <div class="section-name">Loading and saving data</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec16" class="sub-nav">
                                <a href="#ch01lvl1sec16">                    
                                    <div class="section-name">Creating broadcast variables and accumulators</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec17" class="sub-nav">
                                <a href="#ch01lvl1sec17">                    
                                    <div class="section-name">Submitting applications to a cluster</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec18" class="sub-nav">
                                <a href="#ch01lvl1sec18">                    
                                    <div class="section-name">Working with DataFrames</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec19" class="sub-nav">
                                <a href="#ch01lvl1sec19">                    
                                    <div class="section-name">Working with Spark Streaming</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse2">
                                <div class="section-name">2: Tricky Statistics with Spark</div>
                            </a>
                        </li>
                        <div id="collapse2" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="2" class="sub-nav">
                                <a href="#ch02">
                                    <div class="section-name">Chapter 2: Tricky Statistics with Spark</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec20" class="sub-nav">
                                <a href="#ch02lvl1sec20">                    
                                    <div class="section-name">Introduction</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec21" class="sub-nav">
                                <a href="#ch02lvl1sec21">                    
                                    <div class="section-name">Variable identification</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec22" class="sub-nav">
                                <a href="#ch02lvl1sec22">                    
                                    <div class="section-name">Sampling data</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec23" class="sub-nav">
                                <a href="#ch02lvl1sec23">                    
                                    <div class="section-name">Summary and descriptive statistics</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec24" class="sub-nav">
                                <a href="#ch02lvl1sec24">                    
                                    <div class="section-name">Generating frequency tables</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec25" class="sub-nav">
                                <a href="#ch02lvl1sec25">                    
                                    <div class="section-name">Installing Pandas on Linux</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec26" class="sub-nav">
                                <a href="#ch02lvl1sec26">                    
                                    <div class="section-name">Installing Pandas from source</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec27" class="sub-nav">
                                <a href="#ch02lvl1sec27">                    
                                    <div class="section-name">Using IPython with PySpark</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec28" class="sub-nav">
                                <a href="#ch02lvl1sec28">                    
                                    <div class="section-name">Creating Pandas DataFrames over Spark</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec29" class="sub-nav">
                                <a href="#ch02lvl1sec29">                    
                                    <div class="section-name">Splitting, slicing, sorting, filtering, and grouping DataFrames over Spark</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec30" class="sub-nav">
                                <a href="#ch02lvl1sec30">                    
                                    <div class="section-name">Implementing co-variance and correlation using Pandas</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec31" class="sub-nav">
                                <a href="#ch02lvl1sec31">                    
                                    <div class="section-name">Concatenating and merging operations over DataFrames</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec32" class="sub-nav">
                                <a href="#ch02lvl1sec32">                    
                                    <div class="section-name">Complex operations over DataFrames</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec33" class="sub-nav">
                                <a href="#ch02lvl1sec33">                    
                                    <div class="section-name">Sparkling Pandas</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse3">
                                <div class="section-name">3: Data Analysis with Spark</div>
                            </a>
                        </li>
                        <div id="collapse3" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="3" class="sub-nav">
                                <a href="#ch03">
                                    <div class="section-name">Chapter 3: Data Analysis with Spark</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec34" class="sub-nav">
                                <a href="#ch03lvl1sec34">                    
                                    <div class="section-name">Introduction</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec35" class="sub-nav">
                                <a href="#ch03lvl1sec35">                    
                                    <div class="section-name">Univariate analysis</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec36" class="sub-nav">
                                <a href="#ch03lvl1sec36">                    
                                    <div class="section-name">Bivariate analysis</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec37" class="sub-nav">
                                <a href="#ch03lvl1sec37">                    
                                    <div class="section-name">Missing value treatment</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec38" class="sub-nav">
                                <a href="#ch03lvl1sec38">                    
                                    <div class="section-name">Outlier detection</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec39" class="sub-nav">
                                <a href="#ch03lvl1sec39">                    
                                    <div class="section-name">Use case - analyzing the MovieLens dataset</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec40" class="sub-nav">
                                <a href="#ch03lvl1sec40">                    
                                    <div class="section-name">Use case - analyzing the Uber dataset</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse4">
                                <div class="section-name">4: Clustering, Classification, and Regression</div>
                            </a>
                        </li>
                        <div id="collapse4" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="4" class="sub-nav">
                                <a href="#ch04">
                                    <div class="section-name">Chapter 4: Clustering, Classification, and Regression</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec41" class="sub-nav">
                                <a href="#ch04lvl1sec41">                    
                                    <div class="section-name">Introduction</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec42" class="sub-nav">
                                <a href="#ch04lvl1sec42">                    
                                    <div class="section-name">Supervised learning</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec43" class="sub-nav">
                                <a href="#ch04lvl1sec43">                    
                                    <div class="section-name">Unsupervised learning</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec44" class="sub-nav">
                                <a href="#ch04lvl1sec44">                    
                                    <div class="section-name">Applying regression analysis for sales data</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec45" class="sub-nav">
                                <a href="#ch04lvl1sec45">                    
                                    <div class="section-name">Variable identification</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec46" class="sub-nav">
                                <a href="#ch04lvl1sec46">                    
                                    <div class="section-name">Data exploration</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec47" class="sub-nav">
                                <a href="#ch04lvl1sec47">                    
                                    <div class="section-name">Feature engineering</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec48" class="sub-nav">
                                <a href="#ch04lvl1sec48">                    
                                    <div class="section-name">Applying linear regression</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec49" class="sub-nav">
                                <a href="#ch04lvl1sec49">                    
                                    <div class="section-name">Applying logistic regression on bank marketing data</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec50" class="sub-nav">
                                <a href="#ch04lvl1sec50">                    
                                    <div class="section-name">Variable identification</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec51" class="sub-nav">
                                <a href="#ch04lvl1sec51">                    
                                    <div class="section-name">Data exploration</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec52" class="sub-nav">
                                <a href="#ch04lvl1sec52">                    
                                    <div class="section-name">Feature engineering</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec53" class="sub-nav">
                                <a href="#ch04lvl1sec53">                    
                                    <div class="section-name">Applying logistic regression</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec54" class="sub-nav">
                                <a href="#ch04lvl1sec54">                    
                                    <div class="section-name">Real-time intrusion detection using streaming k-means</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec55" class="sub-nav">
                                <a href="#ch04lvl1sec55">                    
                                    <div class="section-name">Variable identification</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec56" class="sub-nav">
                                <a href="#ch04lvl1sec56">                    
                                    <div class="section-name">Simulating real-time data</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec57" class="sub-nav">
                                <a href="#ch04lvl1sec57">                    
                                    <div class="section-name">Applying streaming k-means</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse5">
                                <div class="section-name">5: Working with Spark MLlib</div>
                            </a>
                        </li>
                        <div id="collapse5" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="5" class="sub-nav">
                                <a href="#ch05">
                                    <div class="section-name">Chapter 5: Working with Spark MLlib</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec58" class="sub-nav">
                                <a href="#ch05lvl1sec58">                    
                                    <div class="section-name">Introduction</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec59" class="sub-nav">
                                <a href="#ch05lvl1sec59">                    
                                    <div class="section-name">Working with Spark ML pipelines</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec60" class="sub-nav">
                                <a href="#ch05lvl1sec60">                    
                                    <div class="section-name">Implementing Naive Bayes&#x27; classification</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec61" class="sub-nav">
                                <a href="#ch05lvl1sec61">                    
                                    <div class="section-name">Implementing decision trees</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec62" class="sub-nav">
                                <a href="#ch05lvl1sec62">                    
                                    <div class="section-name">Building a recommendation system</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec63" class="sub-nav">
                                <a href="#ch05lvl1sec63">                    
                                    <div class="section-name">Implementing logistic regression using Spark ML pipelines</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse6">
                                <div class="section-name">6: NLP with Spark</div>
                            </a>
                        </li>
                        <div id="collapse6" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="6" class="sub-nav">
                                <a href="#ch06">
                                    <div class="section-name">Chapter 6: NLP with Spark</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec64" class="sub-nav">
                                <a href="#ch06lvl1sec64">                    
                                    <div class="section-name">Introduction</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec65" class="sub-nav">
                                <a href="#ch06lvl1sec65">                    
                                    <div class="section-name">Installing NLTK on Linux</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec66" class="sub-nav">
                                <a href="#ch06lvl1sec66">                    
                                    <div class="section-name">Installing Anaconda on Linux</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec67" class="sub-nav">
                                <a href="#ch06lvl1sec67">                    
                                    <div class="section-name">Anaconda for cluster management</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec68" class="sub-nav">
                                <a href="#ch06lvl1sec68">                    
                                    <div class="section-name">POS tagging with PySpark on an Anaconda cluster</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec69" class="sub-nav">
                                <a href="#ch06lvl1sec69">                    
                                    <div class="section-name">NER with IPython over Spark</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec70" class="sub-nav">
                                <a href="#ch06lvl1sec70">                    
                                    <div class="section-name">Implementing openNLP - chunker over Spark</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec71" class="sub-nav">
                                <a href="#ch06lvl1sec71">                    
                                    <div class="section-name">Implementing openNLP - sentence detector over Spark</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec72" class="sub-nav">
                                <a href="#ch06lvl1sec72">                    
                                    <div class="section-name">Implementing stanford NLP - lemmatization over Spark</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec73" class="sub-nav">
                                <a href="#ch06lvl1sec73">                    
                                    <div class="section-name">Implementing sentiment analysis using stanford NLP over Spark</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse7">
                                <div class="section-name">7: Working with Sparkling Water - H2O</div>
                            </a>
                        </li>
                        <div id="collapse7" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="7" class="sub-nav">
                                <a href="#ch07">
                                    <div class="section-name">Chapter 7: Working with Sparkling Water - H2O</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec74" class="sub-nav">
                                <a href="#ch07lvl1sec74">                    
                                    <div class="section-name">Introduction</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec75" class="sub-nav">
                                <a href="#ch07lvl1sec75">                    
                                    <div class="section-name">Features</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec76" class="sub-nav">
                                <a href="#ch07lvl1sec76">                    
                                    <div class="section-name">Working with H2O on Spark</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec77" class="sub-nav">
                                <a href="#ch07lvl1sec77">                    
                                    <div class="section-name">Implementing k-means using H2O over Spark</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec78" class="sub-nav">
                                <a href="#ch07lvl1sec78">                    
                                    <div class="section-name">Implementing spam detection with Sparkling Water</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec79" class="sub-nav">
                                <a href="#ch07lvl1sec79">                    
                                    <div class="section-name">Deep learning with airlines and weather data</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec80" class="sub-nav">
                                <a href="#ch07lvl1sec80">                    
                                    <div class="section-name">Implementing a crime detection application</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec81" class="sub-nav">
                                <a href="#ch07lvl1sec81">                    
                                    <div class="section-name">Running SVM with H2O over Spark</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse8">
                                <div class="section-name">8: Data Visualization with Spark</div>
                            </a>
                        </li>
                        <div id="collapse8" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="8" class="sub-nav">
                                <a href="#ch08">
                                    <div class="section-name">Chapter 8: Data Visualization with Spark</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec82" class="sub-nav">
                                <a href="#ch08lvl1sec82">                    
                                    <div class="section-name">Introduction</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec83" class="sub-nav">
                                <a href="#ch08lvl1sec83">                    
                                    <div class="section-name">Visualization using Zeppelin</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec84" class="sub-nav">
                                <a href="#ch08lvl1sec84">                    
                                    <div class="section-name">Installing Zeppelin</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec85" class="sub-nav">
                                <a href="#ch08lvl1sec85">                    
                                    <div class="section-name">Customizing Zeppelin&#x27;s server and websocket port</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec86" class="sub-nav">
                                <a href="#ch08lvl1sec86">                    
                                    <div class="section-name">Visualizing data on HDFS - parameterizing inputs</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec87" class="sub-nav">
                                <a href="#ch08lvl1sec87">                    
                                    <div class="section-name">Running custom functions</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec88" class="sub-nav">
                                <a href="#ch08lvl1sec88">                    
                                    <div class="section-name">Adding external dependencies to Zeppelin</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec89" class="sub-nav">
                                <a href="#ch08lvl1sec89">                    
                                    <div class="section-name">Pointing to an external Spark Cluster</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec90" class="sub-nav">
                                <a href="#ch08lvl1sec90">                    
                                    <div class="section-name">Creating scatter plots with Bokeh-Scala</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec91" class="sub-nav">
                                <a href="#ch08lvl1sec91">                    
                                    <div class="section-name">Creating a time series MultiPlot with Bokeh-Scala</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec92" class="sub-nav">
                                <a href="#ch08lvl1sec92">                    
                                    <div class="section-name">Creating plots with the lightning visualization server</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec93" class="sub-nav">
                                <a href="#ch08lvl1sec93">                    
                                    <div class="section-name">Visualize machine learning models with Databricks notebook</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse9">
                                <div class="section-name">9: Deep Learning on Spark</div>
                            </a>
                        </li>
                        <div id="collapse9" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="9" class="sub-nav">
                                <a href="#ch09">
                                    <div class="section-name">Chapter 9: Deep Learning on Spark</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec94" class="sub-nav">
                                <a href="#ch09lvl1sec94">                    
                                    <div class="section-name">Introduction</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec95" class="sub-nav">
                                <a href="#ch09lvl1sec95">                    
                                    <div class="section-name">Installing CaffeOnSpark</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec96" class="sub-nav">
                                <a href="#ch09lvl1sec96">                    
                                    <div class="section-name">Working with CaffeOnSpark</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec97" class="sub-nav">
                                <a href="#ch09lvl1sec97">                    
                                    <div class="section-name">Running a feed-forward neural network with DeepLearning 4j over Spark</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec98" class="sub-nav">
                                <a href="#ch09lvl1sec98">                    
                                    <div class="section-name">Running an RBM with DeepLearning4j over Spark</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec99" class="sub-nav">
                                <a href="#ch09lvl1sec99">                    
                                    <div class="section-name">Running a CNN for learning MNIST with DeepLearning4j over Spark</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec100" class="sub-nav">
                                <a href="#ch09lvl1sec100">                    
                                    <div class="section-name">Installing TensorFlow</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec101" class="sub-nav">
                                <a href="#ch09lvl1sec101">                    
                                    <div class="section-name">Working with Spark TensorFlow</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse10">
                                <div class="section-name">10: Working with SparkR</div>
                            </a>
                        </li>
                        <div id="collapse10" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="10" class="sub-nav">
                                <a href="#ch10">
                                    <div class="section-name">Chapter 10: Working with SparkR</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec102" class="sub-nav">
                                <a href="#ch10lvl1sec102">                    
                                    <div class="section-name">Introduction</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec103" class="sub-nav">
                                <a href="#ch10lvl1sec103">                    
                                    <div class="section-name">Installing R</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec104" class="sub-nav">
                                <a href="#ch10lvl1sec104">                    
                                    <div class="section-name">Interactive analysis with the SparkR shell</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec105" class="sub-nav">
                                <a href="#ch10lvl1sec105">                    
                                    <div class="section-name">Creating a SparkR standalone application from RStudio</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec106" class="sub-nav">
                                <a href="#ch10lvl1sec106">                    
                                    <div class="section-name">Creating SparkR DataFrames</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec107" class="sub-nav">
                                <a href="#ch10lvl1sec107">                    
                                    <div class="section-name">SparkR DataFrame operations</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec108" class="sub-nav">
                                <a href="#ch10lvl1sec108">                    
                                    <div class="section-name">Applying user-defined functions in SparkR</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec109" class="sub-nav">
                                <a href="#ch10lvl1sec109">                    
                                    <div class="section-name">Running SQL queries from SparkR and caching DataFrames</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec110" class="sub-nav">
                                <a href="#ch10lvl1sec110">                    
                                    <div class="section-name">Machine learning with SparkR</div>
                                </a>
                            </li>
                        </div>
                    </div>
                </div>
            </ul>
        </div>
        
        <div id="page-content-wrapper" class="book-page">
            <a href="#" id="menu-toggle" class="toggle-nav"><i class="fa fa-bars fa-2x mr5"></i></a>
            
            <a href="#" id="back_to_top" class="back-to-top"><img src="../../mapt/images/kopimi.svg"></a>
            
            <div class="col-sm-12 col-xl-offset-2 col-xl-8 col-lg-offset-1 col-lg-10">
                <div class="btn-group pull-right mt15 mb30" role="group">
                    <a href="#home" class="btn btn-default">
                        <i class="fa fa-share fa-lg no-text-padding"></i>
                        <span class="hidden-xs ml5">Book Home</span>
                    </a>
                    <button class="btn btn-default" data-nid="25029" id="code-download">
                        <i class="fa fa-file fa-lg"></i>
                        <span class="hidden-xs ml5">Download Code Files</span>
                    </button>
                </div>
            </div>
            <div class="clearfix"></div>
            
            <div id="book-wrapper" class="container-fluid">
                <div class="col-sm-12 col-xl-offset-2 col-xl-8 col-lg-offset-1 col-lg-10" id="home">
                    <h2 class="product-title">Apache Spark for Data Science Cookbook</h2>
                    <hr>
                    <div class="row">
                        <div class="col-sm-12">
                            <h5 class="mt10">By Padma Priya Chitturi</h5>
                            <div>
                                <p class="mb20"><b>Over insightful 90 recipes to get lightning-fast analytics with Apache Spark</b></p>
                                <a href="#ch01" class="btn btn-info btn-lg pull-right hidden-xs">
                                    Start Reading <i class="fa fa-chevron-right ml5"></i>
                                </a>
                                <a href="#ch01" class="btn btn-info btn-lg btn-block mt20 mb20 visible-xs">
                                    Start Reading <i class="fa fa-chevron-right ml5"></i>
                                </a>
                                <div class="clearfix"></div>
                                <div class="col-sm-12">
                                    <ul id="myTabs" class="nav nav-tabs nav-justified hidden-xs mt20" role="tablist">
                                        <li class="active">
                                            <a href="#info" role="tab" id="info-tab" data-toggle="tab">
                                                <h5>Info</h5>
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#content" role="tab" id="content-tab" data-toggle="tab">
                                                <h5>Contents</h5>
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#author" role="tab" id="author-tab" data-toggle="tab">
                                                <h5>Author</h5>
                                            </a>
                                        </li>
                                    </ul>
                
                                    <ul id="myTabsMobile" class="nav nav-pills text-center nav-stacked visible-xs mb60" role="tablist">
                                        <li class="active">
                                            <a href="#info" role="tab" id="info-tab-responsive" data-toggle="tab">
                                                Info
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#content" role="tab" id="content-tab-responsive" data-toggle="tab">
                                                Contents
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#author" role="tab" id="author-tab-responsive" data-toggle="tab">
                                                Author
                                            </a>
                                        </li>
                                    </ul>
                
                                    <div id="myTabContent" class="tab-content pt30">
                                    
                                        <div role="tabpanel" class="tab-pane active fade in" id="info">
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">Features</h5>
                                            </div>
                                            <div class="hidden-xs">
                                                <h5>Features</h5>
                                            </div>
                                            <hr>
                                            <div>
                                                <ul>
                <li>Use Apache Spark for data processing with these hands-on recipes</li>
                <li>Implement end-to-end, large-scale data analysis better than ever before</li>
                <li>Work with powerful libraries such as MLLib, SciPy, NumPy, and Pandas to gain insights from your data</li>
                </ul>
                                            </div>
                                            <br>
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">Learning</h5>
                                            </div>
                                            <div class="hidden-xs">
                                                <h5>Learning</h5>
                                            </div>
                                            <hr>
                                            <div>
                                                <ul>
                <li>Explore the topics of data mining, text mining, Natural Language Processing, information retrieval, and machine learning.</li>
                <li>Solve real-world analytical problems with large data sets.</li>
                <li>Address data science challenges with analytical tools on a distributed system like Spark (apt for iterative algorithms), which offers in-memory processing and more flexibility for data analysis at scale.</li>
                <li>Get hands-on experience with algorithms like Classification, regression, and recommendation on real datasets using Spark MLLib package.</li>
                <li>Learn about numerical and scientific computing using NumPy and SciPy on Spark.</li>
                <li>Use Predictive Model Markup Language (PMML) in Spark for statistical data mining models.</li>
                </ul>
                                            </div>
                                            <br>
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">About</h5>
                                            </div>
                                            <div class="hidden-xs">
                                                <h5>About</h5>
                                            </div>
                                            <hr>
                                            <div>
                                                <p>Spark has emerged as the most promising big data analytics engine for data science professionals. The true power and value of Apache Spark lies in its ability to execute data science tasks with speed and accuracy. Sparkâ€™s selling point is that it combines ETL, batch analytics, real-time stream analysis, machine learning, graph processing, and visualizations. It lets you tackle the complexities that come with raw unstructured data sets with ease.</p>
                <p>This guide will get you comfortable and confident performing data science tasks with Spark. You will learn about implementations including distributed deep learning, numerical computing, and scalable machine learning. You will be shown effective solutions to problematic concepts in data science using Sparkâ€™s data science libraries such as MLLib, Pandas, NumPy, SciPy, and more. These simple and efficient recipes will show you how to implement algorithms and optimize your work.</p>
                                            </div>
                                        </div>
                                        
                                        <div role="tabpanel" class="tab-pane fade in" id="content">
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">Contents</h5>
                                                <hr>
                                            </div>
                                            <ul>
                                                <div>
                                                    <li data-chapter="1">
                                                        <div class="section-name">1: Big Data Analytics with Spark</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="1" class="chapter-section">
                                                                    <a href="#ch01">        
                                                                        <div class="section-name">Chapter 1: Big Data Analytics with Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec8" class="chapter-section">
                                                                    <a href="#ch01lvl1sec8">                    
                                                                        <div class="section-name">Introduction</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec9" class="chapter-section">
                                                                    <a href="#ch01lvl1sec9">                    
                                                                        <div class="section-name">Initializing SparkContext</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec10" class="chapter-section">
                                                                    <a href="#ch01lvl1sec10">                    
                                                                        <div class="section-name">Working with Spark&#x27;s Python and Scala shells</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec11" class="chapter-section">
                                                                    <a href="#ch01lvl1sec11">                    
                                                                        <div class="section-name">Building standalone applications</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec12" class="chapter-section">
                                                                    <a href="#ch01lvl1sec12">                    
                                                                        <div class="section-name">Working with the Spark programming model</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec13" class="chapter-section">
                                                                    <a href="#ch01lvl1sec13">                    
                                                                        <div class="section-name">Working with pair RDDs</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec14" class="chapter-section">
                                                                    <a href="#ch01lvl1sec14">                    
                                                                        <div class="section-name">Persisting RDDs</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec15" class="chapter-section">
                                                                    <a href="#ch01lvl1sec15">                    
                                                                        <div class="section-name">Loading and saving data</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec16" class="chapter-section">
                                                                    <a href="#ch01lvl1sec16">                    
                                                                        <div class="section-name">Creating broadcast variables and accumulators</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec17" class="chapter-section">
                                                                    <a href="#ch01lvl1sec17">                    
                                                                        <div class="section-name">Submitting applications to a cluster</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec18" class="chapter-section">
                                                                    <a href="#ch01lvl1sec18">                    
                                                                        <div class="section-name">Working with DataFrames</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec19" class="chapter-section">
                                                                    <a href="#ch01lvl1sec19">                    
                                                                        <div class="section-name">Working with Spark Streaming</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="2">
                                                        <div class="section-name">2: Tricky Statistics with Spark</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="2" class="chapter-section">
                                                                    <a href="#ch02">        
                                                                        <div class="section-name">Chapter 2: Tricky Statistics with Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec20" class="chapter-section">
                                                                    <a href="#ch02lvl1sec20">                    
                                                                        <div class="section-name">Introduction</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec21" class="chapter-section">
                                                                    <a href="#ch02lvl1sec21">                    
                                                                        <div class="section-name">Variable identification</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec22" class="chapter-section">
                                                                    <a href="#ch02lvl1sec22">                    
                                                                        <div class="section-name">Sampling data</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec23" class="chapter-section">
                                                                    <a href="#ch02lvl1sec23">                    
                                                                        <div class="section-name">Summary and descriptive statistics</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec24" class="chapter-section">
                                                                    <a href="#ch02lvl1sec24">                    
                                                                        <div class="section-name">Generating frequency tables</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec25" class="chapter-section">
                                                                    <a href="#ch02lvl1sec25">                    
                                                                        <div class="section-name">Installing Pandas on Linux</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec26" class="chapter-section">
                                                                    <a href="#ch02lvl1sec26">                    
                                                                        <div class="section-name">Installing Pandas from source</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec27" class="chapter-section">
                                                                    <a href="#ch02lvl1sec27">                    
                                                                        <div class="section-name">Using IPython with PySpark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec28" class="chapter-section">
                                                                    <a href="#ch02lvl1sec28">                    
                                                                        <div class="section-name">Creating Pandas DataFrames over Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec29" class="chapter-section">
                                                                    <a href="#ch02lvl1sec29">                    
                                                                        <div class="section-name">Splitting, slicing, sorting, filtering, and grouping DataFrames over Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec30" class="chapter-section">
                                                                    <a href="#ch02lvl1sec30">                    
                                                                        <div class="section-name">Implementing co-variance and correlation using Pandas</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec31" class="chapter-section">
                                                                    <a href="#ch02lvl1sec31">                    
                                                                        <div class="section-name">Concatenating and merging operations over DataFrames</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec32" class="chapter-section">
                                                                    <a href="#ch02lvl1sec32">                    
                                                                        <div class="section-name">Complex operations over DataFrames</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec33" class="chapter-section">
                                                                    <a href="#ch02lvl1sec33">                    
                                                                        <div class="section-name">Sparkling Pandas</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="3">
                                                        <div class="section-name">3: Data Analysis with Spark</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="3" class="chapter-section">
                                                                    <a href="#ch03">        
                                                                        <div class="section-name">Chapter 3: Data Analysis with Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec34" class="chapter-section">
                                                                    <a href="#ch03lvl1sec34">                    
                                                                        <div class="section-name">Introduction</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec35" class="chapter-section">
                                                                    <a href="#ch03lvl1sec35">                    
                                                                        <div class="section-name">Univariate analysis</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec36" class="chapter-section">
                                                                    <a href="#ch03lvl1sec36">                    
                                                                        <div class="section-name">Bivariate analysis</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec37" class="chapter-section">
                                                                    <a href="#ch03lvl1sec37">                    
                                                                        <div class="section-name">Missing value treatment</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec38" class="chapter-section">
                                                                    <a href="#ch03lvl1sec38">                    
                                                                        <div class="section-name">Outlier detection</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec39" class="chapter-section">
                                                                    <a href="#ch03lvl1sec39">                    
                                                                        <div class="section-name">Use case - analyzing the MovieLens dataset</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec40" class="chapter-section">
                                                                    <a href="#ch03lvl1sec40">                    
                                                                        <div class="section-name">Use case - analyzing the Uber dataset</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="4">
                                                        <div class="section-name">4: Clustering, Classification, and Regression</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="4" class="chapter-section">
                                                                    <a href="#ch04">        
                                                                        <div class="section-name">Chapter 4: Clustering, Classification, and Regression</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec41" class="chapter-section">
                                                                    <a href="#ch04lvl1sec41">                    
                                                                        <div class="section-name">Introduction</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec42" class="chapter-section">
                                                                    <a href="#ch04lvl1sec42">                    
                                                                        <div class="section-name">Supervised learning</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec43" class="chapter-section">
                                                                    <a href="#ch04lvl1sec43">                    
                                                                        <div class="section-name">Unsupervised learning</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec44" class="chapter-section">
                                                                    <a href="#ch04lvl1sec44">                    
                                                                        <div class="section-name">Applying regression analysis for sales data</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec45" class="chapter-section">
                                                                    <a href="#ch04lvl1sec45">                    
                                                                        <div class="section-name">Variable identification</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec46" class="chapter-section">
                                                                    <a href="#ch04lvl1sec46">                    
                                                                        <div class="section-name">Data exploration</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec47" class="chapter-section">
                                                                    <a href="#ch04lvl1sec47">                    
                                                                        <div class="section-name">Feature engineering</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec48" class="chapter-section">
                                                                    <a href="#ch04lvl1sec48">                    
                                                                        <div class="section-name">Applying linear regression</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec49" class="chapter-section">
                                                                    <a href="#ch04lvl1sec49">                    
                                                                        <div class="section-name">Applying logistic regression on bank marketing data</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec50" class="chapter-section">
                                                                    <a href="#ch04lvl1sec50">                    
                                                                        <div class="section-name">Variable identification</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec51" class="chapter-section">
                                                                    <a href="#ch04lvl1sec51">                    
                                                                        <div class="section-name">Data exploration</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec52" class="chapter-section">
                                                                    <a href="#ch04lvl1sec52">                    
                                                                        <div class="section-name">Feature engineering</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec53" class="chapter-section">
                                                                    <a href="#ch04lvl1sec53">                    
                                                                        <div class="section-name">Applying logistic regression</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec54" class="chapter-section">
                                                                    <a href="#ch04lvl1sec54">                    
                                                                        <div class="section-name">Real-time intrusion detection using streaming k-means</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec55" class="chapter-section">
                                                                    <a href="#ch04lvl1sec55">                    
                                                                        <div class="section-name">Variable identification</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec56" class="chapter-section">
                                                                    <a href="#ch04lvl1sec56">                    
                                                                        <div class="section-name">Simulating real-time data</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec57" class="chapter-section">
                                                                    <a href="#ch04lvl1sec57">                    
                                                                        <div class="section-name">Applying streaming k-means</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="5">
                                                        <div class="section-name">5: Working with Spark MLlib</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="5" class="chapter-section">
                                                                    <a href="#ch05">        
                                                                        <div class="section-name">Chapter 5: Working with Spark MLlib</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec58" class="chapter-section">
                                                                    <a href="#ch05lvl1sec58">                    
                                                                        <div class="section-name">Introduction</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec59" class="chapter-section">
                                                                    <a href="#ch05lvl1sec59">                    
                                                                        <div class="section-name">Working with Spark ML pipelines</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec60" class="chapter-section">
                                                                    <a href="#ch05lvl1sec60">                    
                                                                        <div class="section-name">Implementing Naive Bayes&#x27; classification</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec61" class="chapter-section">
                                                                    <a href="#ch05lvl1sec61">                    
                                                                        <div class="section-name">Implementing decision trees</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec62" class="chapter-section">
                                                                    <a href="#ch05lvl1sec62">                    
                                                                        <div class="section-name">Building a recommendation system</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec63" class="chapter-section">
                                                                    <a href="#ch05lvl1sec63">                    
                                                                        <div class="section-name">Implementing logistic regression using Spark ML pipelines</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="6">
                                                        <div class="section-name">6: NLP with Spark</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="6" class="chapter-section">
                                                                    <a href="#ch06">        
                                                                        <div class="section-name">Chapter 6: NLP with Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec64" class="chapter-section">
                                                                    <a href="#ch06lvl1sec64">                    
                                                                        <div class="section-name">Introduction</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec65" class="chapter-section">
                                                                    <a href="#ch06lvl1sec65">                    
                                                                        <div class="section-name">Installing NLTK on Linux</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec66" class="chapter-section">
                                                                    <a href="#ch06lvl1sec66">                    
                                                                        <div class="section-name">Installing Anaconda on Linux</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec67" class="chapter-section">
                                                                    <a href="#ch06lvl1sec67">                    
                                                                        <div class="section-name">Anaconda for cluster management</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec68" class="chapter-section">
                                                                    <a href="#ch06lvl1sec68">                    
                                                                        <div class="section-name">POS tagging with PySpark on an Anaconda cluster</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec69" class="chapter-section">
                                                                    <a href="#ch06lvl1sec69">                    
                                                                        <div class="section-name">NER with IPython over Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec70" class="chapter-section">
                                                                    <a href="#ch06lvl1sec70">                    
                                                                        <div class="section-name">Implementing openNLP - chunker over Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec71" class="chapter-section">
                                                                    <a href="#ch06lvl1sec71">                    
                                                                        <div class="section-name">Implementing openNLP - sentence detector over Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec72" class="chapter-section">
                                                                    <a href="#ch06lvl1sec72">                    
                                                                        <div class="section-name">Implementing stanford NLP - lemmatization over Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec73" class="chapter-section">
                                                                    <a href="#ch06lvl1sec73">                    
                                                                        <div class="section-name">Implementing sentiment analysis using stanford NLP over Spark</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="7">
                                                        <div class="section-name">7: Working with Sparkling Water - H2O</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="7" class="chapter-section">
                                                                    <a href="#ch07">        
                                                                        <div class="section-name">Chapter 7: Working with Sparkling Water - H2O</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec74" class="chapter-section">
                                                                    <a href="#ch07lvl1sec74">                    
                                                                        <div class="section-name">Introduction</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec75" class="chapter-section">
                                                                    <a href="#ch07lvl1sec75">                    
                                                                        <div class="section-name">Features</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec76" class="chapter-section">
                                                                    <a href="#ch07lvl1sec76">                    
                                                                        <div class="section-name">Working with H2O on Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec77" class="chapter-section">
                                                                    <a href="#ch07lvl1sec77">                    
                                                                        <div class="section-name">Implementing k-means using H2O over Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec78" class="chapter-section">
                                                                    <a href="#ch07lvl1sec78">                    
                                                                        <div class="section-name">Implementing spam detection with Sparkling Water</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec79" class="chapter-section">
                                                                    <a href="#ch07lvl1sec79">                    
                                                                        <div class="section-name">Deep learning with airlines and weather data</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec80" class="chapter-section">
                                                                    <a href="#ch07lvl1sec80">                    
                                                                        <div class="section-name">Implementing a crime detection application</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec81" class="chapter-section">
                                                                    <a href="#ch07lvl1sec81">                    
                                                                        <div class="section-name">Running SVM with H2O over Spark</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="8">
                                                        <div class="section-name">8: Data Visualization with Spark</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="8" class="chapter-section">
                                                                    <a href="#ch08">        
                                                                        <div class="section-name">Chapter 8: Data Visualization with Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec82" class="chapter-section">
                                                                    <a href="#ch08lvl1sec82">                    
                                                                        <div class="section-name">Introduction</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec83" class="chapter-section">
                                                                    <a href="#ch08lvl1sec83">                    
                                                                        <div class="section-name">Visualization using Zeppelin</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec84" class="chapter-section">
                                                                    <a href="#ch08lvl1sec84">                    
                                                                        <div class="section-name">Installing Zeppelin</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec85" class="chapter-section">
                                                                    <a href="#ch08lvl1sec85">                    
                                                                        <div class="section-name">Customizing Zeppelin&#x27;s server and websocket port</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec86" class="chapter-section">
                                                                    <a href="#ch08lvl1sec86">                    
                                                                        <div class="section-name">Visualizing data on HDFS - parameterizing inputs</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec87" class="chapter-section">
                                                                    <a href="#ch08lvl1sec87">                    
                                                                        <div class="section-name">Running custom functions</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec88" class="chapter-section">
                                                                    <a href="#ch08lvl1sec88">                    
                                                                        <div class="section-name">Adding external dependencies to Zeppelin</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec89" class="chapter-section">
                                                                    <a href="#ch08lvl1sec89">                    
                                                                        <div class="section-name">Pointing to an external Spark Cluster</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec90" class="chapter-section">
                                                                    <a href="#ch08lvl1sec90">                    
                                                                        <div class="section-name">Creating scatter plots with Bokeh-Scala</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec91" class="chapter-section">
                                                                    <a href="#ch08lvl1sec91">                    
                                                                        <div class="section-name">Creating a time series MultiPlot with Bokeh-Scala</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec92" class="chapter-section">
                                                                    <a href="#ch08lvl1sec92">                    
                                                                        <div class="section-name">Creating plots with the lightning visualization server</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec93" class="chapter-section">
                                                                    <a href="#ch08lvl1sec93">                    
                                                                        <div class="section-name">Visualize machine learning models with Databricks notebook</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="9">
                                                        <div class="section-name">9: Deep Learning on Spark</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="9" class="chapter-section">
                                                                    <a href="#ch09">        
                                                                        <div class="section-name">Chapter 9: Deep Learning on Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec94" class="chapter-section">
                                                                    <a href="#ch09lvl1sec94">                    
                                                                        <div class="section-name">Introduction</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec95" class="chapter-section">
                                                                    <a href="#ch09lvl1sec95">                    
                                                                        <div class="section-name">Installing CaffeOnSpark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec96" class="chapter-section">
                                                                    <a href="#ch09lvl1sec96">                    
                                                                        <div class="section-name">Working with CaffeOnSpark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec97" class="chapter-section">
                                                                    <a href="#ch09lvl1sec97">                    
                                                                        <div class="section-name">Running a feed-forward neural network with DeepLearning 4j over Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec98" class="chapter-section">
                                                                    <a href="#ch09lvl1sec98">                    
                                                                        <div class="section-name">Running an RBM with DeepLearning4j over Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec99" class="chapter-section">
                                                                    <a href="#ch09lvl1sec99">                    
                                                                        <div class="section-name">Running a CNN for learning MNIST with DeepLearning4j over Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec100" class="chapter-section">
                                                                    <a href="#ch09lvl1sec100">                    
                                                                        <div class="section-name">Installing TensorFlow</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec101" class="chapter-section">
                                                                    <a href="#ch09lvl1sec101">                    
                                                                        <div class="section-name">Working with Spark TensorFlow</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="10">
                                                        <div class="section-name">10: Working with SparkR</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="10" class="chapter-section">
                                                                    <a href="#ch10">        
                                                                        <div class="section-name">Chapter 10: Working with SparkR</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec102" class="chapter-section">
                                                                    <a href="#ch10lvl1sec102">                    
                                                                        <div class="section-name">Introduction</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec103" class="chapter-section">
                                                                    <a href="#ch10lvl1sec103">                    
                                                                        <div class="section-name">Installing R</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec104" class="chapter-section">
                                                                    <a href="#ch10lvl1sec104">                    
                                                                        <div class="section-name">Interactive analysis with the SparkR shell</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec105" class="chapter-section">
                                                                    <a href="#ch10lvl1sec105">                    
                                                                        <div class="section-name">Creating a SparkR standalone application from RStudio</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec106" class="chapter-section">
                                                                    <a href="#ch10lvl1sec106">                    
                                                                        <div class="section-name">Creating SparkR DataFrames</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec107" class="chapter-section">
                                                                    <a href="#ch10lvl1sec107">                    
                                                                        <div class="section-name">SparkR DataFrame operations</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec108" class="chapter-section">
                                                                    <a href="#ch10lvl1sec108">                    
                                                                        <div class="section-name">Applying user-defined functions in SparkR</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec109" class="chapter-section">
                                                                    <a href="#ch10lvl1sec109">                    
                                                                        <div class="section-name">Running SQL queries from SparkR and caching DataFrames</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec110" class="chapter-section">
                                                                    <a href="#ch10lvl1sec110">                    
                                                                        <div class="section-name">Machine learning with SparkR</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                            </ul>
                                        </div>
                                        
                                        <div role="tabpanel" class="tab-pane fade" id="author">
                                            <div class="visible-xs">
                                                <h4 class="mobile-title">About the Author</h4>
                                                <hr>
                                            </div>
                                            <p><strong>Padma Priya Chitturi</strong></p>
                                            <div>
                                                <p>Padma Priya Chitturi is Analytics Lead at Fractal Analytics Pvt Ltd and has over five years of experience in Big Data processing. Currently, she is part of capability development at Fractal and responsible for solution development for analytical problems across multiple business domains at large scale. Prior to this, she worked for an Airlines product on a real-time processing platform serving one million user requests/sec at Amadeus Software Labs. She has worked on realizing large-scale deep networks (Jeffrey deanâ€™s work in Google brain) for image classification on the big data platform Spark. She works closely with Big Data technologies such as Spark, Storm, Cassandra and Hadoop. She was an open source contributor to Apache Storm.</p>
                                            </div>
                                        </div>
                                        
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    
                    <div class="next-wrapper">
                        <div class="row ns">
                            <hr />
                            <span class="hidden-xs">
                                <h4 class="pull-left">
                                    <strong>Up Next: </strong><span class="section-title"></span>
                                </h4>
                                <a href="#" class="btn btn-primary pull-right btn-lg">
                                    Next Section
                                </a>
                            </span>
                            <span class="visible-xs">
                                <a href="#" class="btn btn-primary btn-block btn-lg">
                                    Next Section
                                </a>
                            </span>
                        </div>
                        <div class="row ns">
                            <hr>
                        </div>
                    </div>
                </div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch01"></a>ChapterÂ 1.Â Big Data Analytics with Spark</h2></div></div></div><p>In this chapter, we will cover the components of Spark. You will learn them through the following recipes:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Initializing SparkContext</p></li><li style="list-style-type: disc"><p>Working with Spark's Python and Scala shells</p></li><li style="list-style-type: disc"><p>Building standalone applications</p></li><li style="list-style-type: disc"><p>Working with the Spark programming model</p></li><li style="list-style-type: disc"><p>Working with pair RDDs</p></li><li style="list-style-type: disc"><p>Persisting RDDs</p></li><li style="list-style-type: disc"><p>Loading and saving data</p></li><li style="list-style-type: disc"><p>Creating broadcast variables and accumulators</p></li><li style="list-style-type: disc"><p>Submitting applications to a cluster</p></li><li style="list-style-type: disc"><p>Working with DataFrames</p></li><li style="list-style-type: disc"><p>Working with Spark Streaming</p></li></ul></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec8"></a>Introduction</h2></div></div><hr /></div><p>Apache Spark is a general-purpose distributed computing engine for large-scale data processing. It is an open source initiative from AMPLab and donated to the Apache Software Foundation. It is one of the top-level projects under the Apache Software Foundation. Apache Spark offers a data abstraction calledÂ <span class="strong"><strong>Resilient Distributed Datasets</strong></span> (<span class="strong"><strong>RDDs</strong></span>) to analyze the data in parallel on top of a cluster of resources. The Apache Spark framework is an alternative to Hadoop MapReduce. It is up to 100X faster than MapReduce and offers the best APIs for iterative and expressive data processing. This project is written in Scala and it offers client APIs in Scala, Java, Python, and R.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec9"></a>Initializing SparkContext</h2></div></div><hr /></div><p>This recipe shows how to initialize the <code class="literal">SparkContext</code> object as a part of many Spark applications. <code class="literal">SparkContext</code> is an object which allows us to create the base RDDs. Every Spark application must contain this object to interact with Spark. It is also used to initialize <code class="literal">StreamingContext</code>, <code class="literal">SQLContext</code>Â and <code class="literal">HiveContext</code>.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec10"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark Cluster in any one of the modes that is, local, standalone, YARN, or Mesos. For installing Spark on a standalone cluster, please refer to <a class="ulink" href="http://spark.apache.org/docs/latest/spark-standalone.html" target="_blank">http://spark.apache.org/docs/latest/spark-standalone.html</a>. Install Hadoop (optional), Scala, and Java. Please download the data from the following location:</p><p><a class="ulink" href="https://github.com/ChitturiPadma/datasets/blob/master/stocks.txt" target="_blank">https://github.com/ChitturiPadma/datasets/blob/master/stocks.txt</a></p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec11"></a>How to do itâ€¦</h3></div></div></div><p>Let's see how to initialize SparkContext:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Invoke spark-shell:</p><pre class="programlisting">
<span class="strong"><strong>     $SPARK_HOME/bin/spark-shell --master &lt;master type&gt;
     Spark context available as sc.</strong></span>
</pre></li><li><p>Invoke PySpark:</p><pre class="programlisting">
<span class="strong"><strong>     $SPARK_HOME/bin/pyspark --master &lt;master type&gt;
     SparkContext available as sc</strong></span>
</pre></li><li><p>Invoke SparkR:</p><pre class="programlisting">
<span class="strong"><strong>     $SPARK_HOME/bin/sparkR --master &lt;master type&gt;
     Spark context is available as sc</strong></span>
</pre></li><li><p>Now, let's initiate <code class="literal">SparkContext</code> in different standalone applications, such as Scala, Java, and Python:</p></li></ol></div><p><span class="strong"><strong>Scala</strong></span>:</p><pre class="programlisting">import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf

 object SparkContextExample {
   def main(args: Array[String]) {
    val stocksPath = "hdfs://namenode:9000/stocks.txt"
    val conf = new SparkConf().setAppName("Counting
     Lines").setMaster("spark://master:7077")
     val sc = new SparkContext(conf)
     val data = sc.textFile(stocksPath, 2)
     val totalLines = data.count()
     println("Total number of Lines: %s".format(totalLines))
   }
 }
</pre><p><span class="strong"><strong>Java</strong></span>:</p><pre class="programlisting">import org.apache.spark.api.java.*;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.function.Function;

public class SparkContextExample {
   public static void main(String[] args) {
     String stocks = "hdfs://namenode:9000/stocks.txt"
     SparkConf conf = new SparkConf().setAppName("Counting
     Lines").setMaster("spark://master:7077");
     JavaSparkContext sc = new JavaSparkContext(conf);
     JavaRDD&lt;String&gt; logData = sc.textFile(stocks);

     long totalLines = stocks.count();
     System.out.println("Total number of Lines " + totalLines);
 }
}
</pre><p><span class="strong"><strong>Python</strong></span>:</p><pre class="programlisting">from pyspark
import SparkContext

stocks = "hdfs://namenode:9000/stocks.txt"

sc = SparkContext("&lt;master URI&gt;", "ApplicationName")
data = sc.textFile(stocks)

totalLines = data.count()
print("Total Lines are: %i" % (totalLines))
</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec12"></a>How it worksâ€¦</h3></div></div></div><p>In the preceding code snippets, <code class="literal">new SparkContext(conf)</code>, <code class="literal">new JavaSparkContext(conf)</code>, and <code class="literal">SparkContext("&lt;master URI&gt;", "ApplicationName")</code> initialize SparkContext in three different languages: Scala, Java, and Python. SparkContext is the starting point for Spark functionality. It represents the connection to a Spark Cluster, and can be used to create RDDs, accumulators, and broadcast variables on that cluster.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec13"></a>There's moreâ€¦</h3></div></div></div><p>SparkContext is created on the driver. It connects with the cluster. Initially, RDDs are created using SparkContext. It is not serialized. Hence it cannot be shipped to workers. Also, only one SparkContext is available per application. In the case of Streaming applications and Spark SQL modules, StreamingContext and SQLContext are created on top of SparkContext.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec14"></a>See also</h3></div></div></div><p>To understand more about the SparkContext object and its methods, please refer to this documentation page: <a class="ulink" href="https://spark.apache.org/docs/1.6.0/api/scala/index.html#org.apache.spark.SparkContext" target="_blank">https://spark.apache.org/docs/1.6.0/api/scala/index.html#org.apache.spark.SparkContext</a>.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec10"></a>Working with Spark's Python and Scala shells</h2></div></div><hr /></div><p>This recipe explainsÂ the spark-shell and PySpark command-line interface tools from the Apache Spark project. Spark-shell is the Scala-based command line interface tool and PySpark is the Python-based command-line tool used to develop Spark interactive applications. They are already initialized with SparkContext, SQLContext, and HiveContext.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec15"></a>How to do itâ€¦</h3></div></div></div><p>Both spark-shell and PySpark are available in the <code class="literal">bin</code> directory of <code class="literal">SPARK_HOME</code>, that is, <code class="literal">SPARK_HOME/bin</code>:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Invoke spark-shell as follows:</p><pre class="programlisting">
<span class="strong"><strong>    $SPARK_HOME/bin/spark-shell [Options]

    $SPARK_HOME/bin/spark-shell --master &lt;master type&gt; i.e., local,
    spark, yarn, mesos.
    $SPARK_HOME/bin/spark-shell --master
    spark://&lt;sparkmasterHostName&gt;:7077

    Welcome to
         ____              __
        / __/__  ___ _____/ /__
       _\ \/ _ \/ _ `/ __/  '_/
      /__ / .__/\_,_/_/ /_/\_\   version 1.6.0
         /_/

    Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java
    1.7.0_79)
    Type in expressions to have them evaluated.
    Type :help for more information.
    16/01/17 20:05:38 WARN Utils: Your hostname, localhost resolves to
    a loopback address: 127.0.0.1; using 192.168.1.6 instead (on
    interface en0)</strong></span>
<span class="strong"><strong>  SQL context available as sqlContext.

    scala&gt; val data = sc.textFile("hdfs://namenode:9000/stocks.txt");
    data: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[1]
    textFile at &lt;console&gt;:27

    scala&gt; data.count()
    res0: Long = 57391

    scala&gt; data.first()
    res1: String = NYSE  CLI   2009-12-31  35.39 35.70 34.50 34.5
                   890100         34.12

    scala&gt; data.top(2)
    res5: Array[String] = Array(NYSE CZZ   2009-12-31  8.77  8.77  8.67
         8.70  694200    8.70, NYSE  CZZ   2009- 12-30  8.71  8.80
         8.46    8.68  1588200     8.68)

    scala&gt; val mydata = data.map(line =&gt; line.toLowerCase())
    mydata: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[3] at
    map at &lt;console&gt;:29

    scala&gt; mydata.collect()
    res6: Array[String] = Array(nyse cli 2009-12-31 35.39 35.70
    34.50 34.57 890100 34.12, nyse cli 2009-12-30 35.22 35.46
    34.96 35.40 516900 34.94, nyse cli 2009-12-29 35.69 35.95
    35.21 35.34 556500 34.88, nyse cli 2009-12-28 35.67 36.23
    35.49 35.69 565000 35.23, nyse cli 2009-12-24 35.38 35.60
    35.19 35.47 230200 35.01, nyse cli 2009-12-23 35.13 35.51
    35.07 35.21 520200 34.75, nyse cli 2009-12-22 34.76 35.04
    34.71 35.04 564600 34.58, nyse cli 2009-12-21 34.65 34.74</strong></span>
<span class="strong"><strong> 34.41 34.73 428400 34.28, nyse cli 2009-12-18 34.11 34.38
    33.73 34.22 1152600 33.77, nyse cli 2009-12-17 34.18 34.53
    33.84 34.21 1082600 33.76, nyse cli 2009-12-16 34.79 35.10
    34.48 34.66 1007900 34.21, nyse cli 2009-12-15 34.60 34.91
    34.39 34.84 813200 34.39, nyse cli 2009-12-14 34.21 34.90
    33.86 34.82 987700 34.37, nyse cli 200...)</strong></span>
</pre></li><li><p>Invoke PySpark as follows:</p><pre class="programlisting">
<span class="strong"><strong>    $SPARK_HOME/bin/pyspark [options]
    $SPARK_HOME/bin/pyspark --master &lt;master type&gt; i.e., local,
    spark, yarn, mesos
    $SPARK_HOME/bin/pyspark --master spark://
    sparkmasterHostName:7077

    Python 2.7.6 (default, Sep  9 2014, 15:04:36)
    [GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.39)] on darwin
    Type "help", "copyright", "credits" or "license" for more
    information.
    Using Spark's default log4j profile: org/apache/spark/log4j-
    defaults.properties
    16/01/17 20:25:48 INFO SparkContext: Running Spark version 1.6.0
    ...

    Welcome to
         ____              __
        / __/__  ___ _____/ /__
       _\ \/ _ \/ _ `/ __/  '_/
      /__ / .__/\_,_/_/ /_/\_\   version 1.6.0
         /_/</strong></span>
<span class="strong"><strong>Using Python version 2.7.6 (default, Sep  9 2014 15:04:36)
    SparkContext available as sc, HiveContext available as sqlContext.

    &gt;&gt;&gt; data = sc.textFile"hdfs://namenode:9000/stocks.txt");

    &gt;&gt;&gt; data.count()
    57391
    &gt;&gt;&gt; data.first()
    NYSE     CLI   2009-12-31  35.39 35.70 34.50 34.57 890100
    34.12
    &gt;&gt;&gt; data.top(2)
    ['NYSE   CZZ   2009-12-31  8.77  8.77  8.67  8.70  694200   8.70',
     'NYSE   CZZ   2009-12-30  8.71  8.80  8.46  8.68  1588200  8.68' ]

    &gt;&gt;&gt; data.collect()</strong></span>
<span class="strong"><strong> ['NYSE CLI 2009-12-31 35.39 35.70 34.50 34.57 890100 34.12,
     'NYSE CLI 2009-12-30 35.22 35.46 34.96 35.40 516900 34.94,
     'NYSE CLI 2009-12-29 35.69 35.95 35.21 35.34 556500 34.88',
     'NYSE CLI 2009-12-28 35.67 36.23 35.49 35.69 565000 35.23',
     'NYSE CLI 2009-12-24 35.38 35.60 35.19 35.47 230200 35.01',
     'NYSE CLI 2009-12-23 35.13 35.51 35.07 35.21 520200 34.75',
     'NYSE CLI 2009-12-22 34.76 35.04 34.71 35.04 564600 34.58',
     'NYSE CLI 2009-12-21 34.65 34.74 34.41 34.73 428400 34.28',
     'NYSE CLI 2009-12-18 34.11 34.38 33.73 34.22 1152600 33.77',
     'NYSE CLI 2009-12-17 34.18 34.53 33.84 34.21 1082600 33.76',
     'NYSE CLI 2009-12-16 34.79 35.10 34.48 34.66 1007900 34.21',
     'NYSE CLI 2009-12-15 34.60 34.91 34.39 34.84 813200 34.39',
     'NYSE CLI 2009-12-14 34.21 34.90 33.86 34.82 987700 34.37',
     'NYSE CLI 200...</strong></span>
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec16"></a>How it worksâ€¦</h3></div></div></div><p>In the preceding code snippets, Spark RDD transformations and actions are executed interactively in both Spark-shell and PySpark. They work in <span class="strong"><strong>Read Eval Print Loop</strong></span> (<span class="strong"><strong>REPL</strong></span>) style and represent a computer environment such as a Window console or Unix/Linux shell where a command is entered and the system responds with an output in interactive mode.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec17"></a>There's moreâ€¦</h3></div></div></div><p>Both Spark-shell and PySpark are better command-line interfaces for developing Spark applications interactively. They have advanced features for application prototyping and quicker development. Also, they have numerous options for customizing them.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec18"></a>See also</h3></div></div></div><p>The Apache Spark documentation offers plenty of examples using these two command-line interfaces; please refer to this documentation page: <a class="ulink" href="http://spark.apache.org/docs/latest/quick-start.html#interactive-analysis-with-the-spark-shell" target="_blank">http://spark.apache.org/docs/latest/quick-start.html#interactive-analysis-with-the-spark-shell</a>.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec11"></a>Building standalone applications</h2></div></div><hr /></div><p>This recipe explains how to develop and build Spark standalone applications using programming languages such as Scala, Java, Python, and R. The sample application under this recipe is written in Scala.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec19"></a>Getting ready</h3></div></div></div><p>Install any IDE tool for application development (the preferred one is Eclipse). Install the SBT build tool to build the project. Create the Scala project and add all the necessary libraries to the <code class="literal">build.sbt</code> file. Add this project to Eclipse. SBT is a build tool like Maven for Scala projects.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec20"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Develop a Spark standalone application using the Eclipse IDE as follows:</p><pre class="programlisting">       import org.apache.spark.SparkContext
       import org.apache.spark.SparkContext._
       import org.apache.spark.SparkConf

        object SparkContextExample {
        def main(args: Array[String]) {
        val file="hdfs://namenode:9000/stocks.txt"
        val conf = new SparkConf().setAppName("Counting
                   Lines").setMaster("spark://master:7077")
        val sc = new SparkContext(conf)
        val data = sc.textFile(file, 2)
        val totalLines = data.count()

        println("Total number of Lines: %s".format(totalLines))}}
</pre></li><li><p>Now go to the project directory and build the project using <code class="literal">sbt assembly</code> and <code class="literal">sbt package</code> manually or build it using eclipse:</p><pre class="programlisting">        ~/SparkProject/ SparkContextExample/sbt assembly
        ~/SparkProject/ SparkContextExample/sbt package
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec21"></a>How it worksâ€¦</h3></div></div></div><p><code class="literal">sbt assembly</code> compiles the program and generates the JAR as <code class="literal">SparkContextExample-assembly-&lt;version&gt;.jar</code>. The <code class="literal">sbt package</code> generates the jar as <code class="literal">SparkContextExample_2.10-1.0.jar</code>. Both the jars are generated in the path <code class="literal">~/SparkProject/SparkContextExample/target/scala-2.10</code>. Submit <code class="literal">SparkContextExample-assembly-&lt;version&gt;.jar</code> to the Spark cluster using the <code class="literal">spark-submit</code> shell script under the <code class="literal">bin</code> directory of <code class="literal">SPARK_HOME</code>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec22"></a>There's moreâ€¦</h3></div></div></div><p>We can develop a variety of complex Spark standalone applications to analyze the data in various ways. When working with any third-party libraries, include the corresponding dependency jars in the <code class="literal">build.sbt</code> file. Invoking <code class="literal">sbt update</code> will download the respective dependencies and will include them in the project classpath.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec23"></a>See also</h3></div></div></div><p>The Apache Spark documentation covers how to build standalone Spark applications. Please refer to this documentation page: <a class="ulink" href="https://spark.apache.org/docs/latest/quick-start.html#self-contained-applications" target="_blank">https://spark.apache.org/docs/latest/quick-start.html#self-contained-applications</a>.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec12"></a>Working with the Spark programming model</h2></div></div><hr /></div><p>This recipe explains the fundamentals of the Spark programming model. It covers the RDD basics that is, Spark provides a <span class="strong"><strong>Resilient Distributed Dataset</strong></span> (<span class="strong"><strong>RDD</strong></span>), which is a collection of elements partitioned across the nodes of the cluster that can be operated in parallel. It also covers how to create and perform transformations and actions on RDDs.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec24"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Let's create RDDs and apply a few transformations such as <code class="literal">map</code> and <code class="literal">filter</code>, and a <code class="literal">few</code> actions such as <code class="literal">count</code>, <code class="literal">take</code>, <code class="literal">top</code>, and so on, in Spark-shell:</p><pre class="programlisting">
<span class="strong"><strong>        scala&gt; val data = Array(1, 2, 3, 4, 5)
        scala&gt; val rddData = sc.parallelize(data)
        scala&gt; val mydata = data.filter(ele =&gt; ele%2==0)
        mydata: org.apache.spark.rdd.RDD[String] =
        MapPartitionsRDD[3]   at
        filter at &lt;console&gt;:29
        scala&gt; val mydata = data.map(ele =&gt; ele+2)
        mydata: org.apache.spark.rdd.RDD[String] =
        MapPartitionsRDD[3]  at
        filter at &lt;console&gt;:30
        scala&gt; mydata.count()
        res1: Long = 5
        scala&gt; mydata.take(2)
        res2:Array[Int] = Array(1,2)
        scala&gt; mydata.top(1)
        res2:Array[Int] = Array(5)</strong></span>
</pre></li><li><p>Now let's work with the transformations and actions in a Spark standalone application:</p><pre class="programlisting">          object SparkTransformations {
          def main(args:Array[String]){
          val conf = new SparkConf
                     conf.setMaster("spark://master:7077")
          val sc = new SparkContext(conf)
          val baseRdd1 =
          sc.parallelize(Array("hello","hi","priya","big","data","hub",
          "hub","hi"),1)
          val baseRdd2 =
          sc.parallelize(Array("hey","ram","krishna","priya"),1)
          val baseRdd3 =  sc.parallelize(Array(1,2,3,4),2)
          val sampledRdd = baseRdd1.sample(false,0.5)
          val unionRdd = baseRdd1.union(baseRdd2).repartition(1)
          val intersectionRdd = baseRdd1.intersection(baseRdd2)
          val distinctRdd = baseRdd1.distinct.repartition(1)
          val subtractRdd = baseRdd1.subtract(baseRdd2)
          val cartesianRdd = sampledRdd.cartesian(baseRdd2)
          val reducedValue = baseRdd3.reduce((a,b) =&gt; a+b)

          val collectedRdd = distinctRdd.collect
          collectedRdd.foreach(println)
          val count = distinctRdd.count
          val first = distinctRdd.first
          println("Count is..."+count); println("First Element
          is..."+first)
          val takeValues = distinctRdd.take(3)
          val takeSample = distinctRdd.takeSample(false, 2)
          val takeOrdered = distinctRdd.takeOrdered(2)
          takeValues.foreach(println)
          println("Take Sample Values..")
          takeSample.foreach(println)
          val foldResult = distinctRdd.fold("&lt;&gt;")((a,b) =&gt; a+b)
          println(foldResult) }}
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec25"></a>How it worksâ€¦</h3></div></div></div><p>Spark offers an abstraction called an RDD as part of its programming model. The preceding code snippets show RDD creation, transformations, and actions. Transformations such as <code class="literal">union</code>, <code class="literal">subtract</code>, <code class="literal">intersection</code>, <code class="literal">sample</code>, <code class="literal">cartesian</code>, <code class="literal">map</code>, <code class="literal">filter</code>, and <code class="literal">flatMap</code> when applied on a RDD result in a new RDD, whereas actions such as <code class="literal">count</code>, <code class="literal">first</code>, <code class="literal">take(3)</code>, <code class="literal">takeSample(false, 2)</code> and <code class="literal">takeOrdered(2)</code> compute the result on the RDD and return it to the driver program or save it to external storage. Although we can define RDDs at any point, Spark computes them in lazy fashion, that is, the first time it is used in any action.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec26"></a>There's moreâ€¦</h3></div></div></div><p>There are a few transformations, such as <code class="literal">reduceByKey</code>, <code class="literal">groupByKey</code>, <code class="literal">repartition</code>, <code class="literal">distinct</code>, <code class="literal">intersection</code>, <code class="literal">subtract</code>, and so on, which result in shuffle operation. This shuffle is very expensive as it involves disk I/O, data serialization, and network I/O. Using certain configuration parameters, shuffle can be optimized.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec27"></a>See also</h3></div></div></div><p>The Apache Spark documentation offers a detailed explanation about the Spark programming model. Please refer to this documentation page: <a class="ulink" href="http://spark.apache.org/docs/latest/programming-guide.html" target="_blank">http://spark.apache.org/docs/latest/programming-guide.html</a>.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec13"></a>Working with pair RDDs</h2></div></div><hr /></div><p>This recipe shows how to work with RDDs of key/value pairs. Key/value RDDs are often widely used to perform aggregations. These key/value RDDs are called pair RDDs. We'll do some initial ETL to get the data into a key/value format and see how to apply transformations on single-pair RDDs and two-pair RDDs.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec28"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark cluster either in pseudo distributed mode or in one of the other distributed modes, that is, standalone, YARN, or Mesos. It could be run in local mode as well.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec29"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>We can create a pair RDD from a collection of strings in the following way:</p><pre class="programlisting">        val baseRdd =
        sc.parallelize(Array("this,is,a,ball","it,is,a,cat","john,is,
        in,town,hall"))
        val inputRdd = sc.makeRDD(List(("is",2), ("it",2), ("cat",8
        ("this",6),("john",5),("a",1)))
        val wordsRdd = baseRdd.flatMap(record =&gt; record.split(","))
        val wordPairs =  wordsRdd.map(word =&gt; (word, word.length))
        val filteredWordPairs = wordPairs.filter{case(word, length) =&gt;
        length &gt;=2}
</pre></li><li><p>Also, pair RDDs can be created from the <code class="literal">hdfs</code> input files. Let's take a text file which contains stocks data as follows:</p><pre class="programlisting">         IBM,20160113,133.5,134.279999,131.100006,131.169998,4672300
         GOOG,20160113,730.849976,734.73999,698.609985,700.559998,
         2468300
         MSFT,20160113,53.799999,54.07,51.299999,51.639999,66119000
         MSFT,20160112,52.759998,53.099998,52.060001,52.779999,35650700
         YHOO,20160113,30.889999,31.17,29.33,29.440001,16593700
</pre></li><li><p>Now, creating pair RDDs for the preceding data looks like this:</p><pre class="programlisting">        val textFile = sc.textFile("hdfs://namenodeHostName:8020
        /data/stocks.txt")
        val stocksPairRdd = textFile.map{record =&gt; val colData =
        record.split(",")
        (colData(0),colData(6))}
</pre></li><li><p>Let's apply transformations on pair RDDs as follows:</p><pre class="programlisting">      val stocksGroupedRdd = stocksPairRdd.groupByKey
      val stocksReducedRdd = stocksPairRdd.reduceByKey((x,y)=&gt;x+y)
      val subtractedRdd = wordPairs.subtractByKey(inputRdd)
      val cogroupedRdd = wordPairs.cogroup(inputRdd)
      val joinedRdd = filteredWordPairs.join(inputRdd)
      val sortedRdd = wordPairs.sortByKey
      val leftOuterJoinRdd = inputRdd.leftOuterJoin(filteredWordPairs)
      val rightOuterJoinRdd = wordPairs.rightOuterJoin(inputRdd)
      val flatMapValuesRdd = filteredWordPairs.flatMapValues(length =&gt;
      1 to 5)
      val mapValuesRdd = wordPairs.mapValues(length =&gt; length*2)
      val keys = wordPairs.keys
      val values = filteredWordPairs.values
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec30"></a>How it worksâ€¦</h3></div></div></div><p>The usage of various pair RDD transformations is given as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><code class="literal">groupByKey</code> groups the values of the RDD by key.</p></li><li style="list-style-type: disc"><p><code class="literal">reduceByKey</code> performs aggregation on the grouped values corresponding to a key.</p></li><li style="list-style-type: disc"><p><code class="literal">subtractByKey</code> removes tuples in the first RDD whose key matches with the other RDD.</p></li><li style="list-style-type: disc"><p><code class="literal">join</code> groups all the values pertaining to a particular key in both the RDDs.</p></li><li style="list-style-type: disc"><p><code class="literal">cogroup</code> does the same job as <code class="literal">join</code> but in addition it first groups the values in the first RDD and then in the other RDD.</p></li><li style="list-style-type: disc"><p><code class="literal">leftOuterJoin</code> and <code class="literal">rightOuterJoin</code> work similarly to join with a slight variation that is, <code class="literal">leftOuterJoin</code> includes all the records from left RDD and if there is no matching record found in the right RDD, the corresponding values are represented as none and vice versa for <code class="literal">rightOuterJoin</code>.</p></li><li style="list-style-type: disc"><p><code class="literal">mapValues</code> transformation applies a function to each of the values of the pair RDD without changing the key.</p></li><li style="list-style-type: disc"><p>The functioning of <code class="literal">flatMapValues</code> is typical. It applies the function which returns an iterator to each value of a pair RDD, and for each element returned, a key/value entry is produced with the old key.</p></li><li style="list-style-type: disc"><p><code class="literal">keys</code> and <code class="literal">values</code> transformations return respectively all keys and all values of a pair RDD.</p></li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec31"></a>There's moreâ€¦</h3></div></div></div><p>There are other pair RDD transformations, such as, <code class="literal">foldByKey</code>, <code class="literal">combineByKey</code>, and <code class="literal">aggregateByKey</code>, and actions such as <code class="literal">countByKey</code> and <code class="literal">countByValue</code> along with the available regular actions such as <code class="literal">count</code>, <code class="literal">first</code>, <code class="literal">take</code>, and so on. Any pair RDD transformation would involve a shuffle operation which shuffles the data across the partitions. To know more about the working of the shuffle operation and its performance impact, please refer to <a class="ulink" href="http://spark.apache.org/docs/latest/programming-guide.html#working-with-key-value-pairs" target="_blank">http://spark.apache.org/docs/latest/programming-guide.html#working-with-key-value-pairs</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec32"></a>See also</h3></div></div></div><p>The <span class="emphasis"><em>
Working with the Spark programming mode</em></span>l and <span class="emphasis"><em>
Working with Spark's Python and Scala shells
</em></span> recipes explain how to work with RDDs and how to make use of Spark shell for testing the application logic.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec14"></a>Persisting RDDs</h2></div></div><hr /></div><p>This recipe shows how to persist an RDD. As a known fact, RDDs are lazily evaluated and sometimes it is necessary to reuse the RDD multiple times. In such cases, Spark will re-compute the RDD and all of its dependencies, each time we call an action on the RDD. This is expensive for iterative algorithms which need the computed dataset multiple times. To avoid computing an RDD multiple times, Spark provides a mechanism for persisting the data in an RDD.</p><p>After the first time an action computes the RDD's contents, they can be stored in memory or disk across the cluster. The next time an action depends on the RDD, it need not be recomputed from its dependencies.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec33"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark cluster either in pseudo distributed mode or in one of the distributed modes, that is, standalone, YARN, or Mesos.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec34"></a>How to do itâ€¦</h3></div></div></div><p>Let's see how to persist RDDs using the following code:</p><pre class="programlisting">val inputRdd = sc.parallelize(Array("this,is,a,ball","it,is,a,cat","julie,is,in,the,church"))
val wordsRdd = inputRdd.flatMap(record =&gt; record.split(","))
val wordLengthPairs = wordsRdd.map(word before code=&gt; (word, word.length))
val wordPairs = wordsRdd.map(word =&gt; (word,1))
val reducedWordCountRdd = wordPairs.reduceByKey((x,y) =&gt; x+y)
val filteredWordLengthPairs = wordLengthPairs.filter{case(word,length) =&gt; length &gt;=3}
reducedWordCountRdd.cache()
val joinedRdd = reducedWordCountRdd.join(filteredWordLengthPairs)
joinedRdd.persist(StorageLevel.MEMORY_AND_DISK)
val wordPairsCount =  reducedWordCountRdd.count
val wordPairsCollection = reducedWordCountRdd.take(10)
val joinedRddCount = joinedRdd.count
val joinedPairs = joinedRdd.collect()
reducedWordCountRdd.unpersist()
joinedRdd.unpersist()
</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec35"></a>How it worksâ€¦</h3></div></div></div><p>The call to <code class="literal">cache()</code> on <code class="literal">reducedWordCountRdd</code> indicates that the RDD should be stored in memory for the next time it's computed. The <code class="literal">count</code> action computes it initially. When the <code class="literal">take</code> action is invoked, it accesses the cached elements of the RDD instead of re-computing them from the dependencies.</p><p>Spark defines levels of persistence or <code class="literal">StorageLevel</code> values for persisting RDDs. <code class="literal">rdd.cache()</code> is shorthand for <code class="literal">rdd.persist(StorageLevel.MEMORY)</code>. In the preceding example, <code class="literal">joinedRdd</code> is persisted with storage level as <code class="literal">MEMORY_AND_DISK</code> which indicates persisting the RDD in memory as well as in disk. It is good practice to un-persist the RDD at the end, which lets us manually remove it from the cache.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec36"></a>There's moreâ€¦</h3></div></div></div><p>Spark defines various levels of persistence, such as <code class="literal">MEMORY_ONLY</code>, <code class="literal">MEMORY_AND_DISK</code>, <code class="literal">MEMORY_AND_DISK2</code>, and so on. Deciding when to cache/persist the data can be an art. The decision typically involves trade-offs between space and speed. If you attempt to cache too much data to fit in memory, Spark will use the LRU cache policy to evict old partitions. In general, RDDs should be persisted when they are likely to be referenced by multiple actions and are expensive to regenerate.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec37"></a>See also</h3></div></div></div><p>Please refer toÂ <a class="ulink" href="http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence" target="_blank">http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence</a> to gain a detailed understanding of persistence in Spark.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec15"></a>Loading and saving data</h2></div></div><hr /></div><p>This recipe shows how Spark supports a wide range of input and output sources. Spark makes it very simple to load and save data in a large number of file formats. Formats range from unstructured, such as <code class="literal">text</code>, to semi-structured, such as <code class="literal">JSON</code>, to structured, such as <code class="literal">SequenceFiles</code>.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec38"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark cluster either in pseudo distributed mode or in one of the distributed modes, that is, standalone, YARN, or Mesos. Also, the reader is expected to have an understanding of text files, JSON, CSV, SequenceFiles, and object files.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec39"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Load and save a text file as follows:</p><pre class="programlisting">      val input =
      sc.textFile("hdfs://namenodeHostName:8020/repos/spark/README.md")
      val wholeInput =
      sc.wholeTextFiles("file://home/padma/salesFiles")
      val result = wholeInput.mapValues{value =&gt; val nums = value.split
      (" ").map(x =&gt; x.toDouble)
      nums.sum/nums.size.toDouble}
      result.saveAsTextFile("/home/Padma/outputFile.txt")
</pre></li><li><p>For loading a JSON file, the <code class="literal">people.json</code> input fileÂ is taken from the <code class="literal">SPARK_HOME</code> folder whose location is <code class="literal">/spark-1.6.0/examples/src/main/resource/people.json</code>. Now, loading and saving a JSON file looks like this:</p><pre class="programlisting">        // Loading JSON file
        import com.fasterxml.jackson.module.scala.DefaultScalaModule
        import com.fasterxml.jackson.module.scala.
        experimental.ScalaObjectMapper
        import com.fasterxml.jackson.databind.ObjectMapper
        import com.fasterxml.jackson.module.databind.
            DeserializatiuonFeature
        ...
        case class Person(name:String, age:Int)
        ...
        val jsonInput =
        sc.textFile(""hdfs://namenode:9000/data/people.json")
        val result = jsonInput.flatMap(record =&gt; {
        try{Some(mapper.readValue(record, classOf[Person]))
        }
        catch{
        case e:Exception =&gt; None
        }} )
        result.filter(person =&gt;
        person.age&gt;15).map(mapper.writeValueAsString(_)).
        saveAsTextFile(output File)
</pre></li><li><p>To load and save a CSV file, let's take the stocks data:</p><pre class="programlisting">       IBM,20160113,133.5,134.279999,131.100006,131.169998,4672300
       GOOG,20160113,730.849976,734.73999,698.609985,700.559998,2468300
       MSFT,20160113,53.799999,54.07,51.299999,51.639999,66119000
       MSFT,20160112,52.759998,53.099998,52.060001,52.779999,35650700
       YHOO,20160113,30.889999,31.17,29.33,29.440001,16593700
       .
       .
       import java.io.StringReader
       import au.com.bytecode.opencsv.CSVReader
       ...
       case class Stocks(name:String, totalPrice:Long)
       ...
       val input = sc.textFile("hdfs://namenodeHostName:8020
       /data/stocks.txt")
       val result = input.map{line =&gt; val reader = new CSVReader(new
       StringReader(line))
       reader.readAll().map(x =&gt; Stocks(x(0), x(6)))
       }
       result.map(stock =&gt; Array(stock.name, stock.
       totalPrice)).mapPartitions {stock =&gt;
       val stringWriter = new StringWriter
       val csvWriter = new CSVWriter(stringWriter)
       csvWriter.writeAll(people.toList)
       Iterator(stringWriter.toString)
       }.saveAsTextFilehdfs://namenode:9000/CSVOutputFile")
</pre></li><li><p>Now, let's see the way <code class="literal">sequenceFile</code> is loaded and saved:</p><pre class="programlisting">       val data = sc.sequenceFile(inputFile, classOf[Text],
       classOf[IntWritable]).map{case(x,y) =&gt; (x.toString, y.get())}
       val input = sc.parallelize(List(("Panda",3),("Kay",6),
       ("Snail",2)))
       input.saveAsSequenceFilehdfs://namenode:9000/
       sequenceOutputFile")
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec40"></a>How it worksâ€¦</h3></div></div></div><p>The call to <code class="literal">textFile()</code> on the SparkContext with the path to the file loads the text file as RDD. If there exists multiple input parts in the form of a directory then we can use <code class="literal">SparkContext.wholeTextFiles()</code>, which returns a pair RDD with the key as the name of the input file. Well, for handling JSON files, the data is loaded as a text file and then it is parsed using a JSON parser. There are a number of JSON libraries available, but in the example we used the Jackson (<a class="ulink" href="http://bit.ly/17k6vli" target="_blank">http://bit.ly/17k6vli</a>) library as it is relatively simple to implement.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip3"></a>Tip</h3><p>Please refer to other JSON libraries, such as this one: <a class="ulink" href="http://bit.ly/1xP8JFK" target="_blank">http://bit.ly/1xP8JFK</a></p></div><p>Loading CSV/TSV data is similar to JSON data, that is, first the data is loaded as text and then processed. Similar to JSON, there are various CSV libraries, but for Scala, we used <code class="literal">opencsv</code>Â (<a class="ulink" href="http://opencsv.sourceforge.net" target="_blank">h</a><a class="ulink" href="http://opencsv.sourceforge.net" target="_blank">ttp://opencsv.sourceforge.net</a>). Using <code class="literal">CSVReader</code>, the records are parsed and mapped to case class structure. While saving the file, <code class="literal">CSVWriter</code> is used to output the file.</p><p>When coming to <code class="literal">SequenceFile</code>, it is a popular Hadoop format composed of a flat file with key/value pairs. This sequence file implements Hadoop's writable interface. <code class="literal">SparkContext.sequenceFile()</code> is the API to load the sequence file in which the parameters <code class="literal">classOf[Text]</code> and <code class="literal">classOf[IntWritable]</code> indicate the <code class="literal">keyClass</code> and <code class="literal">valueClass</code>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec41"></a>There's moreâ€¦</h3></div></div></div><p>As Spark is built on the ecosystem of Hadoop, it can access data through the <code class="literal">InputFormat</code> and <code class="literal">OutputFormat</code> interfaces used by Hadoop MapReduce, which are available for many common file formats and storage systems (for example, S3, HDFS, Cassandra, HBase, and so on).</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip4"></a>Tip</h3><p>For more information, please refer Hadoop InputFormat (<a class="ulink" href="http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/InputFormat.html" target="_blank">http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/InputFormat.html</a>) and SequenceFilesÂ (<a class="ulink" href="http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/SequenceFileInputFormat.html" target="_blank">http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/SequenceFileInputFormat.html</a>).</p></div><p>Spark can also interact with any Hadoop supported formats (for both old and new Hadoop file APIs) using <code class="literal">newAPIHadoopFile</code>, which takes a path and three classes. The first class represents the input format. The next class is for our key and the final class is the class of our value. The Spark SQL module provides a more efficient API for structured data sources, which includes JSON and Hive.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec42"></a>See also</h3></div></div></div><p>For more details on Hadoop input and output formats and <code class="literal">SequenceFiles</code> input format, please refer to the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><a class="ulink" href="http://spark.apache.org/docs/latest/programming-guide.html#external-datasets" target="_blank">http://spark.apache.org/docs/latest/programming-guide.html#external-datasets</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://commons.apache.org/proper/commons-csv" target="_blank">http://commons.apache.org/proper/commons-csv</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/SequenceFileInputFormat.html" target="_blank">http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/SequenceFileInputFormat.html</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/InputFormat.html" target="_blank">http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/InputFormat.html</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://wiki.apache.org/hadoop/SequenceFile" target="_blank">http://wiki.apache.org/hadoop/SequenceFile</a></p></li></ul></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec16"></a>Creating broadcast variables and accumulators</h2></div></div><hr /></div><p>This recipe shows how to use accumulators and broadcast variables. <span class="strong"><strong>Accumulators</strong></span> are used to aggregate values from worker nodes back to the driver program. One of the most common uses of accumulators is to count events that occur during job execution for debugging purposes. The other type of shared variable is the broadcast variable, which allows the program to efficiently send a large, read-only value to all the worker nodes for use in one or more Spark operations. Such variables are used in cases where the application needs to send a large, read-only lookup table to all the nodes.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec43"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark cluster either in pseudo distributed mode or in one of the distributed modes, that is, standalone, YARN, or Mesos.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec44"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>The log file of workers from Spark log <code class="literal">$SPARK_HOME/logs</code> is taken, whose filename looks like this: <code class="literal">spark-padma-org.apache.spark.deploy.worker.Worker-1-blrrndtipdl19</code>. Place this file in HDFS. This log file contains Spark log information with different trace levels, such as <code class="literal">DEBUG</code>, <code class="literal">INFO</code>, <code class="literal">WARN</code>, and <code class="literal">ERROR</code>. The sample data looks as follows:</p><div class="mediaobject"><img src="graphics/image_01_001.jpg" /></div></li><li><p>Let's work with an accumulator now:</p><pre class="programlisting">        val sc = new SparkContext
        val logFile =
        sc.textFile("hdfs://namenodeHostName:8020/data/spark-
        worker-Worker1.out")
        val errorLines = sc.accumulator(0)
        val debugLines = logFile.map{line =&gt;
        if(line.contains("ERROR"))
        errorLines +=1
        if(line.contains("DEBUG"))line
        }
       debugLines.saveAsTextFile("hdfs://namenodeHostName:8020/data
       /out/
       debugLines.txt")
       println("ERROR Lines: "+ errorLines.value)
</pre></li><li><p>Now create a broadcast variable and use it in the workers as follows:</p><pre class="programlisting">      val sc = new SparkContext
      val broadCastedTemperatures = sc.broadcast(Map("KOCHI" -&gt;
      22,"BNGLR" -&gt; 22, "HYD" -&gt; 24, "MUMBAI" -&gt; 21, "DELHI" -&gt; 17,
      "NOIDA" -&gt; 19, "SIMLA" -&gt; 9))
      val inputRdd = sc.parallelize(Array("BNGLR",20), ("BNGLR",16),
      ("KOCHI",-999), ("SIMLA",-999), ("DELHI",19, ("DELHI",-999),
      ("MUMBAI",27), ("MUMBAI",-999), ("HYD",19), ("HYD",25),
      ("NOIDA",-999) )
      val replacedRdd = inputRdd.map{case(location, temperature) =&gt;
      val standardTemperatures = broadCastedTemperatures.value
      if(temperature == -999 &amp;&amp; standardTemperatures.get(location) !=
      None) (location, standardTemperatures.get(location).get) else
      if(temperature != -999) (location, temperature )
      }
      val locationsWithMaxTemperatures =
      replacedRdd.reduceByKey{(temp1,
      temp2) =&gt; if (temp1 &gt; temp2) temp1 else temp2}
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec45"></a>How it worksâ€¦</h3></div></div></div><p>Initially, when working with accumulators, we created <code class="literal">Accumulator[Int]</code>, called <code class="literal">errorLines</code>, and added <code class="literal">1</code> to it whenever we saw a line that contained <code class="literal">ERROR</code>. We will see the correct count for <code class="literal">errorLines</code> only after the <code class="literal">saveAsTextFile()</code> action runs because the transformation <code class="literal">map()</code> is lazy, so the side-effect, incrementing the accumulator happens only when the <code class="literal">map()</code> is forced to occur by <code class="literal">saveAsTextFile()</code>. The return type of the accumulator would be the <code class="literal">org.apache.spark.Accumulator[T]</code> object where <code class="literal">T</code> is the type of the value.</p><p>Well, coming to broadcast variables, <code class="literal">SparkContext.broadcast</code> creates a broadcast variable of type <code class="literal">Broadcast[T]</code>. <code class="literal">T</code> is of any type and it should be serializable. The value of the broadcast variable is accessed using the <code class="literal">value</code> property. The variable is sent to each node only once and is read-only.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec46"></a>There's moreâ€¦</h3></div></div></div><p>Spark has support for custom accumulator types. They need to extend <code class="literal">AccumulatorParam</code>.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip5"></a>Tip</h3><p>For additional information on this, please visit: <a class="ulink" href="http://spark.apache.org/docs/latest/programming-guide.html#accumulators-a-nameaccumlinka" target="_blank">http://spark.apache.org/docs/latest/programming-guide.html#accumulators-a-nameaccumlinka</a>.</p></div><p>Also, when working with broadcast variables, it is essential to choose a serialization format which is fast and compact.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip6"></a>Tip</h3><p>For more information on broadcast variables, please refer: <a class="ulink" href="http://spark.apache.org/docs/latest/programming-guide.html#broadcast-variables" target="_blank">http://spark.apache.org/docs/latest/programming-guide.html#broadcast-variables</a>.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec47"></a>See also</h3></div></div></div><p>Please visit the earlier <span class="emphasis"><em>
Working with the Spark programming model
</em></span>, <span class="emphasis"><em>
Working with Spark's Python and Scala shells
</em></span>, and <span class="emphasis"><em>
Working with pair RDDs
</em></span> recipes to get familiar with Spark.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec17"></a>Submitting applications to a cluster</h2></div></div><hr /></div><p>This recipe shows how to run an application on distributed clusters. An application is launched on a set of machines using an external service called a <span class="strong"><strong>cluster manager</strong></span>. There is a wide variety of cluster managers such as Hadoop YARN, Apache Mesos, and Spark's own built-in standalone cluster manager. Spark provides a single tool for submitting jobs across all cluster managers, called <span class="strong"><strong>spark-submit</strong></span>. Through various options, spark-submit can connect to different cluster managers and control how many resources your application gets.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec48"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark cluster either in pseudo distributed mode or in one of the distributed modes, that is, standalone, YARN, or Mesos.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec49"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Let's create a word count application:</p><pre class="programlisting">        package org.apache.spark.programs
        object WordCount{
        def main(args:Array[String]) {
        val conf = new SparkConf
        conf.setAppName("WordCount")
        val sc = new SparkContext(conf)
        val input =
        sc.parallelize(Array("this,is,a,ball","it,is,a,cat","john,is,
        in,town,hall"))
        val words = input.flatMap{record =&gt; record.split(",")}
        val wordPairs = words.map(word =&gt; (word,1))
        val wordCounts = wordPairs.reduceByKey{(a,b) =&gt; a+b}
        val result = wordCounts.collect
        println("Displaying the WordCounts:")
        result.foreach(println)
</pre></li><li><p>Submit the application to Spark's standalone cluster manager:</p><pre class="programlisting">
<span class="strong"><strong>      spark-submit --class org.apache.spark.programs.WordCount --master
      spark://master:7077 WordCount.jar
</strong></span>
</pre></li><li><p>Submit the application to YARN:</p><pre class="programlisting">
<span class="strong"><strong>      spark-submit --class org.apache.spark.programs.WordCount --master
      yarn WordCount.jar</strong></span>
</pre></li><li><p>Submit the application to Mesos:</p><pre class="programlisting">
<span class="strong"><strong>      spark-submit --class org.apache.spark.programs.WordCount --master
      mesos://mesos-master:5050 WordCount.jar</strong></span>
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec50"></a>How it worksâ€¦</h3></div></div></div><p>When <code class="literal">spark-submit</code> is called with the <code class="literal">--master</code> flag as <code class="literal">spark://master:7077</code> submits the application to Spark's standalone cluster. Invoking with the <code class="literal">--master</code> flag as <code class="literal">yarn</code> runs the application in the YARN cluster, whereas specifying the <code class="literal">--master</code> flag as <code class="literal">mesos://mesos-master:5050</code> runs the application on <code class="literal">Mesos</code> cluster.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec51"></a>There's moreâ€¦</h3></div></div></div><p>Whenever <code class="literal">spark-submit</code> is invoked, it launches the driver program. This driver program contacts the cluster manager and requests resources to launch executors. Once the executors are launched by the cluster manager, the driver runs through the user application. It delegates the work to executors in the form of tasks. When the driver's <code class="literal">main()</code> method exits, it will terminate the executors and releases resources from the cluster manager. <code class="literal">spark-submit</code> provides various options as well to control specific details.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec52"></a>See also</h3></div></div></div><p>For more information on submitting applications to a cluster and the various options provided by Spark-submit, please visit: <a class="ulink" href="http://spark.apache.org/docs/latest/submitting-applications.html" target="_blank">http://spark.apache.org/docs/latest/submitting-applications.html</a>. Also, for detailed information about the different cluster managers, please refer to the following:</p><p>Also, to learn in details about the different cluster managers, please refer:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><a class="ulink" href="http://spark.apache.org/docs/latest/spark-standalone.html" target="_blank">http://spark.apache.org/docs/latest/spark-standalone.html</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://spark.apache.org/docs/latest/running-on-mesos.html" target="_blank">http://spark.apache.org/docs/latest/running-on-mesos.html</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://spark.apache.org/docs/latest/running-on-yarn.html" target="_blank">http://spark.apache.org/docs/latest/running-on-yarn.html</a></p></li></ul></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec18"></a>Working with DataFrames</h2></div></div><hr /></div><p>Spark SQL is a Spark module for structured data processing. It provides the programming abstraction called <span class="strong"><strong>DataFrame</strong></span> (in earlier versions of Spark, it is called <span class="strong"><strong>SchemaRDD</strong></span>) and also acts as distributed SQL query engine. The capabilities it provides are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>It loads data from a variety of structured sources (for example, JSON, Hive, and Parquet)</p></li><li style="list-style-type: disc"><p>It lets you query data using SQL, both inside a Spark program and from external tools that connect to Spark SQL through standard database connectors (JDBC/ODBC), such as BI tools like Tableau.</p></li><li style="list-style-type: disc"><p>Spark SQL provides rich integration between SQL and regular Python/Java/Scala code, including the ability to join RDDs and SQL tables, expose custom functions in SQL, and more.</p></li></ul></div><p>A DataFrame is an RDD of row objects, each representing a record. It is also known as a <span class="strong"><strong>schema of records</strong></span>. These can be created from external data sources, from results of queries, or from regular RDDs. The created DataFrame can be registered as a temporary table and apply <code class="literal">SQLContext.sql</code> or <code class="literal">HiveContext.sql</code> to query the table. This recipe shows how to work with DataFrames.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec53"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark cluster either in pseudo distributed mode or in one of the distributed modes, that is, standalone, YARN, or Mesos.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec54"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Let's see how to create a DataFrame from a JSON file:</p><pre class="programlisting">         import org.apache.spark.sql._
         import org.apache.spark.sql.SQLContext
         import org.apache.spark.SparkConf
         import org.apache.spark.SparkContext

         object JSONDataFrame {

         def main(args:Array[String])
         {
         val conf=new SparkConf
         conf.setMaster("spark://master:7077")
         conf.setAppName("sql_Sample")
         val sc=new SparkContext(conf)
         val sqlcontxt=new SQLContext(sc)
         val df = sqlContext.read.json("/home/padma/Sparkdev/spark-
         1.6.0/examples/src/main/resources/people.json")
         df.show
         df.printSchema
         df.select("name").show
         df.select("name","age").show
         df.select(df("name"),df("age")+4).show
         df.groupBy("age").count.show
         df.describe("name,age") } }
</pre></li><li><p>Now create a DataFrame from a text file and query on the DataFrame:</p><pre class="programlisting">        object DataFrames {
        case class Person(name:String, age:Int)
        def main(args:Array[String])
        {
        val conf = new SparkConf
        conf.setMaster("spark://master:7077")
        conf.setAppName("DataFramesApp")
        val sc = new SparkContext(conf)
        val sqlContext = new SQLContext(sc)

        import sqlContext.implicits._
        val peopleDf = sc.textFile("/home/padma/Sparkdev/spark-
        1.6.0/examples/src/main/resources/people.txt").
        map(line =&gt; line.split(",")).map(p =&gt;
        Person(p(0),p(1).trim.toInt)).toDF
        peopleDf.registerTempTable("people")
        val teenagers = sqlContext.sql("select name, age from people
        where age &gt;=13 AND name in(select name from people where age=
        30)")
        teenagers.map(t =&gt; "Name: " + t(0)).collect().foreach(println)
          }
        }
</pre></li><li><p>Here is the code snippet to show how to create a DataFrame from a parquet file:</p><pre class="programlisting">        val sc = new SparkContext(conf)
        val sqlContext = new SQLContext(sc)
        import sqlContext.implicits._

        val df1 = sc.makeRDD(1 to 5).map(i =&gt;
        (i,i*2)).toDF("single","double")
        df1.write.parquet("/home/padma/Sparkdev/SparkApp/
        test_table/key=1")

        val df2 = sc.makeRDD(6 to 10).map(i =&gt;
        (i,i*4)).toDF("single","triple")
        df2.write.parquet("/home/padma/Sparkdev/SparkApp/
        test_table/key=2")

        val df3 = sqlContext.read.parquet("/home/padma/Sparkdev/
        SparkApp/test_table")
        df3.show
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec55"></a>How it worksâ€¦</h3></div></div></div><p>Initially, the JSON file is read, which is the DataFrame, and the API such as <code class="literal">show()</code>, <code class="literal">printSchema()</code>, <code class="literal">select()</code>, or <code class="literal">groupBy()</code> can be invoked on the data frame. In the second code snippet, an <code class="literal">RDD</code> is created from the text file and the fields are mapped to the case class structure <code class="literal">Person</code> and the <code class="literal">RDD</code> is converted to a data frame using <code class="literal">toDF</code>. This data frame <code class="literal">peopleDF</code> is converted to a table using <code class="literal">registerTempTable()</code> whose table name is <code class="literal">people</code>. Now this table <code class="literal">people </code>can be queried using <code class="literal">SQLContext.sql</code>.</p><p>The final code snippet shows how to write a data frame as a parquet file using <code class="literal">df1.write.parquet()</code> and the parquet file is read using <code class="literal">sqlContext.read.parquet()</code>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec56"></a>There's moreâ€¦</h3></div></div></div><p>Spark SQL in addition provides HiveContext, using which we can access Hive tables, UDFS, SerDes, and also HiveQL. There are ways to create DataFrames by converting an RDD to a DataFrame or creating them programmatically. The different data sources, such as JSON, Parquet, and Avro, can be handled and there is provision to directly run <code class="literal">sql</code> queries on the files. Also, data from other databases can be read using JDBC. In Spark 1.6.0, a new feature known as <span class="strong"><strong>Dataset</strong></span> is introduced, which provides the benefits of Spark SQL's optimized execution engine over RDDs.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec57"></a>See also</h3></div></div></div><p>For more information on Spark SQL, please visit: <a class="ulink" href="http://spark.apache.org/docs/latest/sql-programming-guide.html" target="_blank">http://spark.apache.org/docs/latest/sql-programming-guide.html</a>. The earlier 
<span class="emphasis"><em>Working with the Spark programming model</em></span>, <span class="emphasis"><em>Working with Spark's Python and Scala shells</em></span>, and<span class="emphasis"><em>
 Working with pair RDDs
</em></span> recipes covered the initial steps in Spark and the basics of RDDs.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec19"></a>Working with Spark Streaming</h2></div></div><hr /></div><p>Spark Streaming is a library in the Spark ecosystem which addresses real-time processing needs. Spark's batch processing executes the job over large datasets at once, where as Streaming aims for low latency (in hundreds of milliseconds), as data becomes available, it is transformed and processing is done in near real time.</p><p>Spark Streaming functions by running jobs over the small batches of data that accumulate in small intervals. It is used for rapid alerting, for supplying dashboards with up-to-date information, as well as for scenarios that need more complex analytics. For example, a common use case in anomaly detection to run K-means clustering on small batches of data and trigger an alert if the cluster center deviates from what is normal.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip7"></a>Tip</h3><p>For more information, please visit: <a class="ulink" href="http://spark.apache.org/docs/latest/streaming-programming-guide.html" target="_blank">http://spark.apache.org/docs/latest/streaming-programming-guide.html</a>.</p></div><p>This recipe shows how to work with Spark Streaming and apply stateless transformations and Windowed transformations.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec58"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark cluster either in pseudo distributed mode or in one of the distributed modes, that is, standalone, YARN, or Mesos.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec59"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>There are two types of transformations supported in Spark Streaming: stateless and stateful (windowed). Let's apply the stateless transformations:</p><pre class="programlisting">       import org.apache.spark.SparkConf
       import org.apache.spark.SparkContext._
       import org.apache.spark.streaming._
       import org.apache.spark.streaming.StreamingContext._
       import org.apache.spark.streaming.dstream._
       import org.apache.spark.SparkConf

       object StatelessTransformations {
       def main(args:Array[String]) {
       val conf= new SparkConf
       conf.setMaster("spark://master:7077").setAppName("StreamingApp")
       val sc = new SparkContext(conf)
       val ssc = new StreamingContext(sc, Seconds(5))
       val spamInfoRDD = ssc.sparkContext.textFile("/path/fileName", 2)
       val lines = ssc.socketTextStream("172.22.225.174", 7777)
       val mapLines = lines.map(ele =&gt; ele+"&lt;&lt;&lt;&gt;&gt;&gt;")
       val mapLines2 = lines.map(ele =&gt; (ele,1))
       val errorLines = lines.filter(line =&gt;line.contains("Padma"))
       val flatMapLines = lines.flatMap(ele =&gt; ele.split(","))
       val reduceByKeyLines = mapLines2.reduceByKey((a,b) =&gt; a+b)
       val groupByKeyLines = mapLines2.groupByKey().mapValues(names =&gt;
       names.toSet.size)
       val unionedDstreams = mapLines.union(flatMapLines)
       val joinedDstream = reduceByKeyLines.join(groupByKeyLines)
       val cogroupedDStream = reduceByKeyLines.cogroup(groupByKeyLines)
       val transformLines = lines.transform(rdd =&gt;
       {rdd.union(spamInfoRDD).filter(_.contains("Padma"))})
       errorLines.print
       mapLines.print
       flatMapLines.print
       reduceByKeyLines.print
       groupByKeyLines.print
       joinedDstream.print
       cogroupedDStream.print
       unionedDstreams.print
       ssc.start
       ssc.awaitTermination
       ssc.stop
        }
       }
</pre></li><li><p>Now let's apply windowed/stateful transformations:</p><pre class="programlisting">       object StatefulTransformations {
       def updateRunningSum(values:Seq[Long], state:Option[Long])
       Some(state.getOrElse(0L) + values.size)

       def main(args:Array[String])
       {
       val conf = new SparkConf
       conf.setMaster("spark://master:7077").setAppName("
       Stateful_transformations")
       val sc = new SparkContext(conf)
       val ssc = new StreamingContext(sc, Seconds(1))
       val lines = ssc.socketTextStream("172.25.41.66", 7777)
       val windowedLines = lines.window(Seconds(4),Seconds(2))
       val mapLines = lines.map(ele =&gt; (ele,1L))
       val windowCounts = windowedLines.count
       val countByWindowLines = lines.countByWindow(Seconds(4),
       Seconds(2))
       val reducedLines = lines.reduce(_+_)
       val updateDStream = mapLines.updateStateByKey
       (updateRunningSum _)
       val mapLines = lines.map(ele =&gt; (ele,1))
       val reducedKeyWindow  = mapLines.reduceByKeyAndWindow({(x,y)=&gt;
       x+y}, {(x,y) =&gt; x-y}, Seconds(4), Seconds(2))

        windowedLines.print
        windowCounts.print
        reducedKeyWindow.print
        countByWindowLines.print
        updateDStream.print
        reducedLines.print
        ssc.checkpoint("/home/padma/StreamingCheckPoint/")
        ssc.start
        ssc.awaitTermination
          }
        }
</pre></li><li><p>It's also possible to apply DataFrames, that is, SQL operations on streaming data. The following code snippet shows SQL operations over streams of data:</p><pre class="programlisting">        import org.apache.spark.streaming.StreamingContext._
        import org.apache.spark.streaming.kafka._
        import org.apache.spark.sql._
        import org.apache.spark.sql.SQLContext
        ...
        object StreamingSQL {
          case class Words(wordName:String, count:Int)

         def main(args:Array[String])
         {
         val conf = new SparkConf
         conf.setAppName("StreamingSQL").setMaster
         ("spark://master:7077")
            val sc = new SparkContext(conf)
            val ssc = new StreamingContext(sc,Seconds(4))
            val kafkaParams = Map("test-consumer-group" -&gt; 1)
            val topicMap = Map("test" -&gt; 1)
            val kafkaLines =
            KafkaUtils.createStream(ssc,"blrovh:2181",
            "test-consumer-group",topicMap)
            val words = kafkaLines.map{tuple =&gt; tuple._2}
            val wordPairs = words.map(word =&gt; (word,1))
            val reduceWords = wordPairs.reduceByKey((a,b) =&gt; a+b)
            reduceWords.foreachRDD{
            rdd =&gt;
            {
              val sqlContext = new SQLContext(rdd.sparkContext)
              import sqlContext.implicits._
              val df = rdd.map(record =&gt; Words(record._1, record._2))
              val dfNew = sqlContext.createDataFrame(df)
              dfNew.registerTempTable("Words")
              val data = sqlContext.sql("select wordName from Words")
              data.foreach(row =&gt; println(row.toString))
              }
            }
        ssc.start
        ssc.awaitTermination  } }
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec60"></a>How it worksâ€¦</h3></div></div></div><p>TheÂ <code class="literal">new StreamingContext(SparkContext, Seconds(1))</code>Â line instantiates the <code class="literal">StreamingContext</code>, which takes <code class="literal">SparkContext</code> and batch interval as parameters. <code class="literal">StreamingContext.socketTextStream(&lt;ip&gt;,&lt;port-number&gt;)</code> initializes a socket stream, which listens on the specified port for messages. This creates a DStream (discretized stream) which is a sequence of RDDs, being generated for each batch.</p><p>When working with windowed/stateful transformations, <code class="literal">lines.window(Seconds(4),Seconds(2))</code> creates a window of 4 seconds and a sliding duration of 2 seconds on the incoming DStream lines. The window-based transformations such as <code class="literal">reduceByKeyAndWindow</code> and <code class="literal">countByWindow</code> use data or intermediate results from previous batches to compute the results of the current batch. The <code class="literal">updateStateByKey</code> transformation constructs DStream (key, state) pairs by the specified function, where this function indicates how to update the state for each key given new values.</p><p>In the case of applying SQL operations on streaming data, <code class="literal">KafkaUtils.createStream</code> initializes a DStream from Kafka. It takes <code class="literal">StreamingContext</code>, <code class="literal">zookeeperhostname</code> (<code class="literal">blrovh</code>), <code class="literal">port-number</code> of zookeeper (<code class="literal">2181</code>), <code class="literal">consumer-group name</code> (test-consumer-group) and <code class="literal">topic map</code> (<code class="literal">Map("test" -&gt; 1)</code>) as parameters. The <code class="literal">new SQLContext</code>Â line creates SQLContext and <code class="literal">SQLContext.createDataFrame</code> creates a data frame for each RDD of the DStream. Using the DataFrame, the SQL queries are executed.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec61"></a>There's moreâ€¦</h3></div></div></div><p>Spark Streaming uses a <span class="strong"><strong>micro-batch</strong></span> architecture, where the streaming computation is a continuous series of batch computations on small batches of data. For each input source, Spark Streaming launches receivers, which are long-running tasks within an application executor, collects the data, replicates it to another executor for fault tolerance, and saves it as RDDs. Using the Kafka Direct Streaming approach, the Dstream is created as <code class="literal">KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kakfaParams, topicSet)</code>. To address the fault tolerance, check-pointing is done, which periodically saves the state to a reliable filesystem.</p><p>Also, when running SQL queries on streaming data, there is a method for retaining the data for a specific duration before the query can complete. As in Spark batch processing, streams of data can be persisted either in memory or in disk. In the case of stateful operations, the data is, by default, persistent in memory.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec62"></a>See also</h3></div></div></div><p>For more details on Spark Streaming, internal architecture, check-pointing, performance tuning, and receiver parallelism, please refer to the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><a class="ulink" href="http://spark.apache.org/docs/latest/streaming-programming-guide.html" target="_blank">http://spark.apache.org/docs/latest/streaming-programming-guide.html</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://www.michael-noll.com/blog/2014/10/01/kafka-spark-streaming-integration-example-tutorial/" target="_blank">http://www.michael-noll.com/blog/2014/10/01/kafka-spark-streaming-integration-example-tutorial/</a></p></li></ul></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch02"></a>ChapterÂ 2.Â Tricky Statistics with Spark</h2></div></div></div><p>In this chapter, you will learn the following recipes:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Working with Pandas</p></li><li style="list-style-type: disc"><p>Variable identification</p></li><li style="list-style-type: disc"><p>Sampling data</p></li><li style="list-style-type: disc"><p>Summary and descriptive statistics</p></li><li style="list-style-type: disc"><p>Generating frequency tables</p></li><li style="list-style-type: disc"><p>Installing Pandas on Linux</p></li><li style="list-style-type: disc"><p>Installing Pandas from source</p></li><li style="list-style-type: disc"><p>Using IPython with PySpark</p></li><li style="list-style-type: disc"><p>Creating Pandas DataFrames over Spark</p></li><li style="list-style-type: disc"><p>Splitting, slicing, sorting, filtering and grouping DataFrames over Spark.</p></li><li style="list-style-type: disc"><p>Implementing co-variance and correlation using DataFrames over Spark.</p></li><li style="list-style-type: disc"><p>Concatenating and merging operations over DataFrames</p></li><li style="list-style-type: disc"><p>Complex operations over DataFrames.</p></li><li style="list-style-type: disc"><p>Sparkling Pandas</p></li></ul></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec20"></a>Introduction</h2></div></div><hr /></div><p>Statistics refers to the mathematics and techniques with which we understand data. It is a vast field which plays a key role in the areas of data mining and artificial intelligence, intersecting with the areas of engineering and other disciplines. Statistics helps in describing data, that is, descriptive statistics reveals the distribution of the data for each variable. Also, statistics is widely used for the purpose of prediction.</p><p>In this chapter, we'll see how to apply various statistical measures and functions on large datasets using Spark.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec63"></a>Working with Pandas</h3></div></div></div><p><span class="strong"><strong>Pandas</strong></span>Â is an open source Python library for highly specialized data analysis. It is the reference point that all professionals using the Python language need to study and analyze data sets for statistical purposes of analysis and decision-making. Pandas arises from the need to have a specific library for the analysis of the data which provides tools for data processing , data extraction and data manipulation.</p><p>It is designed on the NumPy library, hence this increased its rapid spread of Pandas. This makes the library compatible with the other modules and it also takes advantage of the high quality performance in the calculations of the NumPy module. Some of the key features of Pandas include the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>It processes a variety of data sets in different formats such as time series, tabular, heterogeneous, and matrix data</p></li><li style="list-style-type: disc"><p>Facilitates the loading/importing of data from varied sources such as CSV and DB/SQL</p></li><li style="list-style-type: disc"><p>It handles a myriad of operations on data sets such as sub-setting, slicing, filtering, merging, groupBy, re-ordering and re-shaping</p></li><li style="list-style-type: disc"><p>It deals with missing data as per the rules defined by the user/developer</p></li><li style="list-style-type: disc"><p>It is used for parsing and munging (conversion) of data as well as modeling and statistical analysis</p></li><li style="list-style-type: disc"><p>It integrates with other Python modules such as statsmodels, SciPy, and scikit-learn</p></li><li style="list-style-type: disc"><p>It provides fast performance and the speed can be improved by making use of Cython (C extensions to Python)</p></li></ul></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec21"></a>Variable identification</h2></div></div><hr /></div><p>In this recipe, we will see how to identify predictor (input) and target (output) variables for data at scale in Spark. Then the next step is to identify the category of the variables.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec64"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need Ubuntu 14.04 (Linux flavor) installed on the machine. Also, you need to have Apache Hadoop 2.6 and Apache Spark 1.6.0 installed.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec65"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Let's take an example of student's data, using which we want to predict whether a student will play cricket or not. Here is what the sample data looks like:</p><div class="mediaobject"><img src="graphics/Capture.jpg" /></div></li><li><p>The preceding data resides in HDFS and load the data into Spark as follows:</p><pre class="programlisting">      import org.apache.spark._
      import org.apache.spark.sql._
        object tricky_Stats {
         def main(args:Array[String]): Unit = {
            val conf = new SparkConf()
                .setMaster("spark://master:7077")
                .setAppName("Variable_Identification")
            val sc = new SparkContext(conf)
            val sqlContext = new SQLContext(sc)
      import sqlContext.implicits._
            val students_data = sqlContext.read.format
            ("com.databricks.spark.csv")
            .option("header","true")
            .option("inferSchema", "true")
            .load("hdfs://namenode:9000/students.csv")
            students_data.show(5)   }
          }
</pre><p>The following is the output:</p><pre class="programlisting">  +---------+------+---------------+------+------+------------+
  |Sudent_ID|Gender|Prev_Exam_Marks|Height|Weight|Play_Cricket|
  +---------+------+---------------+------+------+------------+
  |     S001|     M|             65|   178|    61|           1|
  |     S002|     F|             75|   174|    56|           0|
  |     S003|     M|             45|   163|    62|           1|
  |     S004|     M|             57|   175|    70|           0|
  |     S005|     F|             59|   162|    67|           0|
  +---------+------+---------------+------+------+------------+
</pre></li><li><p>From the preceding result, the variables have been defined in different categories as follows:</p><div class="mediaobject"><img src="graphics/Capture-2.jpg" /></div></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec66"></a>How it worksâ€¦</h3></div></div></div><p>In the preceding code snippet, since the file is of CSV format, we imported the data initially into Spark using the CSVÂ package. Once the DataFrame is loaded, it internally has the schema, and the line <code class="literal">students_data.show(5)</code> displays the first five records. From this sample data, the independent and dependent variables are identified. Also, the data type of the variables and category of the variables is known.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec67"></a>There's moreâ€¦</h3></div></div></div><p>The data can be in multiple formats, such as JSON, Parquet, ORC, Avro, Hive tables, and so on. For loading the semi-structured files, the related packages are available. In the case of unstructured files, the data can be loaded in RDD format. After bringing the data into structured format, it can be converted into a DataFrame and then it can proceed with the analysis on the variables.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec68"></a>See also</h3></div></div></div><p>Please refer toÂ <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Big Data Analytics with Spark</em></span> to get familiar with Spark and refresh your knowledge of basic statistics and distributions.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec22"></a>Sampling data</h2></div></div><hr /></div><p>In this recipe, we will see how to generate sample data from the entire population.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec69"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you need Ubuntu 14.04 (Linux flavor) installed on the machine. Also, have Apache Hadoop 2.6 and Apache Spark 1.6.0 installed. Readers are expected to have knowledge of sampling techniques.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec70"></a>How to do itâ€¦</h3></div></div></div><p>Let's take an example of load prediction data. Here is what the sample data looks like:</p><div class="mediaobject"><img src="graphics/Capture-3.jpg" /></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note8"></a>Note</h3><p>Download the data from the following locationÂ <a class="ulink" href="https://github.com/ChitturiPadma/datasets/blob/master/Loan_Prediction_Data.csv" target="_blank">https://github.com/ChitturiPadma/datasets/blob/master/Loan_Prediction_Data.csv</a>.</p></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Here is the code for sampling data from a DataFrame:</p><pre class="programlisting">      import org.apache.spark._
      import org.apache.spark.sql.SQLContext
      import org.apache.spark.sql.types.{StructType,
      StringType,DoubleType, StructField}

      object Sampling_Demo {
        def main(args:Array[String]): Unit = {
          val conf = new SparkConf()
            .setMaster("spark://master:7077")
               .setAppName("Sampling")
         val sc = new SparkContext(conf)
         val sqlContext = new SQLContext(sc)
           import sqlContext.implicits._
           val schemaString =
            "Loan_ID,Gender,Married,Dependents,Education,
             Self_Employed,ApplicantIncome,
         CoapplicantIncome,LoanAmount,Loan_Amount_Term,
         Credit_History,Property_Area, Loan_Status"
          val schema = schemaString.split(",").map {
            field =&gt;
              if (field == "ApplicantIncome" || field ==
       "CoapplicantIncome" || field ==   "LoanAmount" || field ==
       "Loan_Amount_Term" || field == "Credit_History")
                StructField(field, DoubleType)
              else
                StructField(field, StringType)
          }
          val schema_Applied = StructType(schema)
          val loan_Data =
      sqlContext.read.format("com.databricks.spark.csv")
            .option("header", "true")
           .schema(schema_Applied).load("hdfs://namenode:9000/
          loan_prediction_data.csv")
      // Generating Sample data (10%) without replacement
          val sampled_Data1 = loan_Data.sample(false, 0.1)
          println("Sample Data Row Count (without Replacement):
          "+sampled_Data1.count)
      // Generating Sample data (10%) with replacement
          val sampled_Data2 = loan_Data.sample(true, 0.2)
          println("Sample Data Row Count (with Replacement):
          "+sampled_Data2.count)
        }
      }
</pre><p>The following is the output:</p><pre class="programlisting">          Sample Data Row Count (without Replacement): 60
          Sample Data Row Count (with Replacement): 135</pre></li><li><p>Spark MLlib provides other sampling methods (stratified sampling) that can be performed on RDDs of key-value pairs:</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note9"></a>Note</h3><p>Please download the KDD99 dataset, which contains network traffic details, from <a class="ulink" href="http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html" target="_blank">http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html</a>.</p></div></li><li><p>Here is what the data looks as follows:</p><pre class="programlisting">     0,tcp,http,SF,181,5450,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,
     0.00,0.00,0.00,1.00,0.00,0.00,9,9,1.00,0.00,0.11,0.00,0.00,0.00,
     0.00,0.00,normal.
     0,tcp,http,SF,239,486,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,
     0.00,0.00,0.00,1.00,0.00,0.00,19,19,1.00,0.00,0.05,0.00,0.00,0.00,
     0.00,0.00,normal.
     0,icmp,ecr_i,SF,1032,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,511,511,
     0.00,0.00,0.00,0.00,1.00,0.00,0.00,255,255,1.00,0.00,1.00,0.00,
     0.00,0.00,0.00,0.00,smurf.
     0,tcp,telnet,S0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,16,15,0.94,
     1.00,0.00,0.00,0.94,0.12,0.00,15,16,1.00,0.00,0.07,0.12,1.00,0.94,
     0.00,0.00,neptune.
     1,tcp,private,RSTR,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,8,2,0.00,
     0.00,1.00,1.00,0.25,0.50,0.00,178,2,0.01,0.04,0.04,0.00,0.01,0.00,
     0.32,1.00,portsweep.
</pre></li><li><p>Each observation contains various features such as <code class="literal">duration</code>, <code class="literal">protocol</code>, <code class="literal">service</code>, <code class="literal">src_bytes</code>, <code class="literal">dst_bytes</code>, and so on. The last feature is the label which indicates the type of attack. From the entire dataset, the percentage of attacks in the sample should represent the same percentage in the population as well. Here is the way to generate sample data with the same ratio as in the population:</p><pre class="programlisting">      import org.apache.spark._
      import org.apache.spark.mllib.linalg.{Vector, Vectors}
      import org.apache.spark.mllib._
      object StratifiedSampling {

        def main(args:Array[String]): Unit =
        {
           val conf = new SparkConf
           conf.setMaster("spark://master:7077").setAppName
           ("StratifiedSampling")
           val sc = new SparkContext(conf)
           val fractions: Map[String, Double] =
           Map("loadmodule." -&gt; 0.000018, "nmap." -&gt; 0.004728,
           "guess_passwd." -&gt; 0.000108, "rootkit.-&gt; 0.000020,
           "satan." -&gt;   0.032443, "perl." -&gt; 0.000006,
           "imap." -&gt; 0.000024, "multihop." -&gt; 0.000014,
           "neptune." -&gt; 2.188490, "normal." -&gt; 1.985903,
           "pod." -&gt; 0.000539, "warezmaster." -&gt; 0.000040,
           "back." -&gt; 0.004497, "ipsweep." -&gt; 0.025480,
           "buffer_overflow." -&gt; 0.000061,   "warezclient." -&gt;
            0.002082,  "phf." -&gt; 0.000008, "spy." -&gt; 0.000004,
           "ftp_write." -&gt; 0.000016,"portsweep." -&gt; 0.021258,
           "land." -&gt; 0.000042, "teardrop." -&gt; 0.001999,
           "smurf." -&gt; 5.732214)
           val fractionsBroadcasted = sc.broadcast(fractions)
           val inputRdd =
           sc.textFile("hdfs://namenode:9000//data/kddcup.data.
           corrected.csv")
           val labelsAndVectors = inputRdd.map{
           record =&gt; val parts = record.split(",").toBuffer
           parts.remove(1,3)
           val label = parts.remove(parts.length-1)
           val vector = Vectors.dense(parts.map(_.toDouble).toArray)
           (label, vector)
         }
       val approxSample = labelsAndVectors.sampleByKey
       (withReplacement = false, fractions)
       val exactSample = labelsAndVectors.sampleByKeyExact(false,
       fractions)
       approxSample.saveAsTextFile("hdfs://namenode:9000/data/
       Approx_Sample/")
       exactSample.saveAsTextFile("hdfs://namenode:9000/data/
       Exact_Sample/")
        }
       }
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec71"></a>How it worksâ€¦</h3></div></div></div><p>Initially, we imported the data into Spark using the csv package. Once the DataFrame was loaded, we generated sample data (10% of the population) without replacement using the line <code class="literal">loan_Data.sample(false, 0.1)</code>. The next line <code class="literal">loan_Data.sample(true, 0.2)</code> generates 20% of the population as sample data with replacement. (When <code class="literal">withReplacement = true</code>, there could exist the same observation multiple times in the sample.)</p><p>In the preceding code of stratified sampling, initially a map of key-value pair fractions is constructed, where <code class="literal">key</code> represents the key in the data and value represents the fraction of observations that need to be extracted from each key. The line <code class="literal">labelsAndVectors.sampleByKey(withReplacement = false, fractions)</code> generates an approximate sample and the line <code class="literal">labelsAndVectors.sampleByKeyExact(false, fractions)</code> generates an exact sample.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec72"></a>There's moreâ€¦</h3></div></div></div><p>In the stratified sampling technique, the sample should represent the exact ratio of the observations as in the population pertaining to a particular key. Spark supports applying stratified sampling on pair RDDs. There are two ways of applying this sampling technique: <code class="literal">sampleByKey</code> requires one pass over the data and generates the expected sample size, whereas <code class="literal">sampleByKeyExact</code> generates the exact sample size.</p><p>In many situations, we are presented only with a sample and then must decide if this sample represents the entire population so that we can use statistical inference to draw conclusions about the entire population by studying the elements of the sample.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec73"></a>See also</h3></div></div></div><p>Please refer <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Big Data Analytics with Spark</em></span> to get familiar with Spark and revise your knowledge of basic statistics and distributions.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec23"></a>Summary and descriptive statistics</h2></div></div><hr /></div><p>In this recipe, we will see how to get the summary statistics for data at scale in Spark. The descriptive summary statistics helps in understanding the distribution of data.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec74"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you need Ubuntu 14.04 (Linux flavor) installed on the machine. Also, have Apache Hadoop 2.6 and Apache Spark 1.6.0 installed.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec75"></a>How to do itâ€¦</h3></div></div></div><p>Let's take an example of load prediction data. Here is what the sample data looks like:</p><div class="mediaobject"><img src="graphics/Capture-4.jpg" /></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note10"></a>Note</h3><p>Download the data from the following location: <a class="ulink" href="https://github.com/ChitturiPadma/datasets/blob/master/Loan_Prediction_Data.csv" target="_blank">https://github.com/ChitturiPadma/datasets/blob/master/Loan_Prediction_Data.csv</a>.</p></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>The preceding data contains numerical as well as categorical fields. We can get the summary of numerical fields as follows:</p><pre class="programlisting">      import org.apache.spark._
      import org.apache.spark.sql._
      object Summary_Statistics {
         def main(args:Array[String]): Unit = {
             val conf = new SparkConf()
              .setMaster("spark://master:7077")
              .setAppName("Summary_Statistics")
            val sc = new SparkContext(conf)
            val sqlContext = new SQLContext(sc)
            import sqlContext.implicits._
              val loan_Data =
        sqlContext.read.format("com.databricks.spark.csv")
            .option("header","true")
            .option("inferSchema", "true")
            .load("hdfs://namenode:9000/loan_prediction_data.csv")
          // Gets summary of all numeric fields
          val summary = loan_Data.describe()
           summary.show()
          // Get Summary on subset of columns
          val summary_subsetColumns =
          loan_Data.describe("ApplicantIncome", "Loan_Amount_Term")
          summary_subsetColumns.show()
      // Get subset of statistics
      val subset_summary = loan_Data.select(mean("ApplicantIncome"),
      min("ApplicantIncome"), max("ApplicantIncome"))
      subset_summary.show() } }
</pre><p>The summary for all columns is as follows:</p><pre class="programlisting">+-------+-----------------+------------------+------------------+
|summary|  ApplicantIncome| CoapplicantIncome|        LoanAmount|
+-------+-----------------+------------------+------------------+
|  count|              614|               614|               592|
|   mean|5403.459283387622|1621.2457980271008|146.41216216216216|
| stddev|6109.041673387179|2926.2483692241885| 85.58732523570544|
|    min|              150|               0.0|                 9|
|    max|            81000|           41667.0|               700|
+-------+-----------------+------------------+------------------+

--------------------+-------------------+
| Loan_Amount_Term  | Credit_History    |
+-------------------+-------------------+
|  600              | 564               |
|342.0              | 0.8421985815602837|
|65.12040985461256  | 0.3648783192364048|
|12                 | 0                 |
|480                | 1                 |
+-------------------+-------------------+
</pre><p>The summary for the subset of columns is as follows:</p><pre class="programlisting">+-------+-----------------+-----------------+
|summary|  ApplicantIncome| Loan_Amount_Term|
+-------+-----------------+-----------------+
|  count|              614|              600|
|   mean|5403.459283387622|            342.0|
| stddev|6109.041673387179|65.12040985461256|
|    min|              150|               12|
|    max|            81000|              480|
+-------+-----------------+-----------------+
</pre><p>The subset of descriptive stats on a column is as follows:</p><pre class="programlisting">+--------------------+--------------------+--------------------+
|avg(ApplicantIncome)|min(ApplicantIncome)|max(ApplicantIncome)|
+--------------------+--------------------+--------------------+
| 5403.459283387622| 150| 81000|
+--------------------+--------------------+--------------------+</pre></li><li><p>There is also an API-based on RDDs in the <code class="literal">org.apache.spark.mllib.stat</code> package which generates summary statistics on a few columns as follows:</p><pre class="programlisting">      val selected_Df =
      loan_Data.select("ApplicantIncome","CoapplicantIncome",
      "LoanAmount")
      val observations = selected_Df.rdd.map{
      row =&gt;
      val applicantIncome = row.getDouble(0)
      val co_applicantIncome = row.getDouble(1)
      val loan_Amount = if(row.isNullAt(2)) 0.0 else row.getDouble(2)
      Vect  ors.dense(applicantIncome, co_applicantIncome,loan_Amount)}
      val summary = Statistics.colStats(observations)
      println("Mean: "+summary.mean)
      println("Variance: "+summary.variance)
      println("Num of Non-zeros: "+summary.numNonzeros)
</pre><p>The following is the output:</p><pre class="programlisting">
<span class="strong"><strong> Mean: [5403.459283387623,1621.2457980271017,141.16612377850163]
 Variance: [3.732039016718122E7,8562929.518387223,7804.066974509938]
 Num of Non-zeros: [614.0,341.0,592.0]</strong></span>
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec76"></a>How it worksâ€¦</h3></div></div></div><p>Initially, we imported the data into Spark using the CSV package. Once the DataFrame is loaded, the line <code class="literal">loan_Data.describe()</code> gives the summary statistics of all the numeric fields. Next, the line <code class="literal">loan_Data.describe("ApplicantIncome", "Loan_Amount_Term")</code> gives summary statistics for the subset of columns. Also, a subset of stats could be obtained for one or more columns as <code class="literal">loan_Data.select(mean("ApplicantIncome"), min("ApplicantIncome"), max("ApplicantIncome"))</code>.</p><p>Later, we used an API based on RDDs which is available in <code class="literal">org.apache.spark.mllib.stat</code>. From the DataFrames, the numeric columns <code class="literal">ApplicantIncome</code>, <code class="literal">CoapplicantIncome</code>, and <code class="literal">LoanAmount</code> are selected and <code class="literal">RDD[Vector]</code> is generated out of these fields. The line <code class="literal">Statistics.colStats(observations)</code> generates the <code class="literal">MultiVariateStatisticalSummary</code> object, using which the stats such as <code class="literal">mean</code>, <code class="literal">variance</code>Â and <code class="literal">numNonZeros</code> are displayed.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec77"></a>There's moreâ€¦</h3></div></div></div><p>Apart from the summary and descriptive stats that can be obtained, there are also other statistical measures available, such as correlation, covariance, generating the frequency distribution of a set of variables, identifying the items that are frequent in each column can be very useful to understand a dataset. Mathematical functions such as <code class="literal">cos</code>, <code class="literal">sin</code>, <code class="literal">floor</code>, <code class="literal">ceil</code>, <code class="literal">exp</code>, <code class="literal">round</code>, <code class="literal">sqrt</code>, and so on are also available with DataFrames.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec78"></a>See also</h3></div></div></div><p>Please refer <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Big Data Analytics with Spark</em></span> to get familiar with Spark and revise your knowledge of basic statistics and distributions. Also visit the previous <span class="emphasis"><em>Variable identification</em></span> recipe, which is the primary step in data preparation.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec24"></a>Generating frequency tables</h2></div></div><hr /></div><p>In this recipe, we will see how to analyze the distribution of various variables in the data. Generally, we can take a histogram/boxplot of the variables to understand the distribution and also identify the outliers. But currently, Spark has no support for plotting the data. Let's see how we can perform analysis by generating frequency tables.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec79"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you need Ubuntu 14.04 (Linux flavor) installed on the machine. Also, have Apache Hadoop 2.6 and Apache Spark 1.6.0 installed.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec80"></a>How to do itâ€¦</h3></div></div></div><p>Let's take an example of load prediction data. Here is what the sample data looks like:</p><div class="mediaobject"><img src="graphics/B05317_02_04.jpg" /></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note11"></a>Note</h3><p>Download the data from the following location: <a class="ulink" href="https://github.com/ChitturiPadma/datasets/blob/master/Loan_Prediction_Data.csv" target="_blank">https://github.com/ChitturiPadma/datasets/blob/master/Loan_Prediction_Data.csv</a>.</p></div><p>The total record count is <code class="literal">614</code>.</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Let us look at the chances of getting a loan-based on <code class="literal">Credit_History</code>. Here is the code to generate the frequency distribution of set of variables such as <code class="literal">Loan_Status</code> and <code class="literal">Credit_History</code> :</p><pre class="programlisting">      import org.apache.spark._
      import org.apache.spark.sql.SQLContext
      import org.apache.spark.sql.types.{StructType, StringType,
           DoubleType, StructField}
      object Frequency_Tables {
        def main(args:Array[String]): Unit = {
          val conf = new SparkConf()
            .setMaster("spark://master:7077")
            .setAppName("Frequency_Tables")
          val sc = new SparkContext(conf)
          val sqlContext = new SQLContext(sc)
          import sqlContext.implicits._
          val schemaString =
          "Loan_ID,Gender,Married,Dependents,Education,Self_Employed,
           ApplicantIncome, CoapplicantIncome,LoanAmount,
           Loan_Amount_Term,Credit_History,Property_Area,Loan_Status"
          val schema = schemaString.split(",").map {
            field =&gt;
              if (field == "ApplicantIncome" || field ==
          "CoapplicantIncome" || field == "LoanAmount" || field ==
          "Loan_Amount_Term" || field == "Credit_History")
              StructField(field, DoubleType)
            else
              StructField(field, StringType)
          }
          val schema_Applied = StructType(schema)
          val loan_Data =
          sqlContext.read.format("com.databricks.spark.csv")
            .option("header", "true")
            .schema(schema_Applied).load("hdfs://namenode:9000/
             loan_prediction_data.csv")
          val crossTab_Df= loan_Data.stat.crosstab("Credit_History",
          "Loan_Status")
          crossTab_Df.show()
           }
         }
</pre><p>The following is the output:</p><pre class="programlisting">     +--------------------------+---+---+
     |Credit_History_Loan_Status|  N|  Y|
     +--------------------------+---+---+
     |                       1.0| 97|378|
     |                      null| 13| 37|
     |                       0.0| 82|  7|
     +--------------------------+---+---+
</pre></li><li><p>Suppose we want to understand the average <code class="literal">ApplicantIncome</code> for a particular <code class="literal">Loan_Staus</code> and <code class="literal">Credit_History</code>. We can generate a pivot table which helps in understanding the distribution as follows:</p><pre class="programlisting">  val pivot_Df =
         loan_Data.groupBy("Loan_Status").pivot("Credit_History").avg
          ("ApplicantIncome")
  pivot_Df.show()

</pre><p>The following is the output:</p><pre class="programlisting">  +-----------+----+-----------------+-----------------+
  |Loan_Status|null|              0.0|              1.0|
  +-----------+----+-----------------+-----------------+
  |          N|null|5382.841463414634|5613.927835051546|
  |          Y|null|9153.857142857143|5378.436507936508|
  +-----------+----+-----------------+-----------------+
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec81"></a>How it worksâ€¦</h3></div></div></div><p>In the preceding code snippet, we have seen how we could analyze the distribution of a set of variables using cross tables and pivot tables. The <code class="literal">crosstab</code> and <code class="literal">pivot</code> functions are available in the <code class="literal">stat</code> library of DataFrame. The line <code class="literal">loan_Data.stat.crosstab("Credit_History", "Loan_Status")</code> generates a table which contains the frequency distribution of <code class="literal">Credit_History</code> and <code class="literal">Loan_Status</code>. For <code class="literal">Credit_History =1</code>, the number of observations with <code class="literal">Loan_Status = "N"</code> is <code class="literal">97</code> and with <code class="literal">Loan_Status = "Y"</code> is <code class="literal">378</code>. Similarly for <code class="literal">Credit_History =0</code>, the number of observations with <code class="literal">Loan_Status = "N"</code> is <code class="literal">82</code> and with <code class="literal">Loan_Status = "Y"</code> is <code class="literal">7</code>.</p><p>Next, we generated a pivot table. Pivot is an aggregation where one of the grouping columns having distinct values are transposed into columns. These tables are essential for data analysis and reporting. Most of the popular data manipulation tools, such as Excel, MS SQL, Oracle, Pandas, and so on, have the ability to pivot data. The line <code class="literal">loan_Data.groupBy("Loan_Status").pivot("Credit_History").avg("ApplicantIncome")</code> groups the observations based on <code class="literal">Loan_Status</code>, and calculates the mean of the <code class="literal">ApplicantIncome </code>for distinct <code class="literal">fordistinct</code>, and <code class="literal">Credit_History</code> values.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec82"></a>There's moreâ€¦</h3></div></div></div><p>Apart from the cross table and Pivot tables used for data analysis, there are other statistical functions available which generate frequent items in each column in a dataset. Data analysis would also involves identifying outliers and extracting relationships between the variables, which requires plotting the data. As Spark is a distributed computing framework, it doesn't support plots. Although this seems to be a limitation when compared with R and Pandas, plotting data at scale is not a feasible methodology with any framework.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec83"></a>See also</h3></div></div></div><p>Please refer <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Big Data Analytics with Spark</em></span> to get familiar with Spark and refresh your knowledge of basic statistics and distributions. Also visit the previous <span class="emphasis"><em>Variable identification</em></span> recipe, which is the primary step in data preparation.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec25"></a>Installing Pandas on Linux</h2></div></div><hr /></div><p>In this recipe, we will see how to install Pandas on Linux. Before proceeding with the installation, let's consider the version of Python we're going to use. There are two versions or flavors of Python, namely Python 2.7.x and Python 3.x. Although the latest version, Python 3.x, appears to be the better choice, for scientific, numeric, or data analysis work, Python 2.7 is recommended.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec84"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you need Ubuntu 14.04 (Linux flavor) installed on the machine. Python comes pre-installed. The <code class="literal">python --version</code> command gives the version of Python installed. If the version seems to be 2.6.x, upgrade it to Python 2.7 as follows:</p><pre class="programlisting">
<span class="strong"><strong>sudo apt-get install python2.7
</strong></span>
</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec85"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Once Python version is available, make sure that the Python <code class="literal">.dev</code> files are installed. If not, install them as follows:</p><pre class="programlisting">
<span class="strong"><strong>      sudo apt-get install python-dev
</strong></span>
</pre></li><li><p>Installing through <code class="literal">pip</code>:</p><pre class="programlisting">
<span class="strong"><strong>      sudo apt-get install python-pip
      sudo pip install numpy
      sudo pip install pandas
</strong></span>
</pre></li><li><p>Sometimes, pip-installed NumPy might be slower and the installation might require dependencies such as <code class="literal">python-setuptools</code>. Hence, instead of pip, <code class="literal">easy-install</code> can be used as follows:</p><pre class="programlisting">
<span class="strong"><strong>      sudo apt-get install python-setuptools
      sudo easy_install pandas</strong></span>
</pre></li><li><p>Instead of <code class="literal">easy_install</code>, <code class="literal">sudo apt-getinstall python-pandas</code> also installs the Pandas library.</p></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec86"></a>How it worksâ€¦</h3></div></div></div><p>The preceding commands install NumPy, the Pandas library, and the related dependencies. If the installation fails with missing dependencies, please install the required ones and continue with the installation.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec87"></a>There's moreâ€¦</h3></div></div></div><p>The easiest and most general way to install the Pandas library is to use a pre-packaged solution, that is, installing it through the distribution Anaconda or Enthought distributions. Also, when installing other Linux flavors such as Centos and OpenSuse, we use the following code:</p><pre class="programlisting">
<span class="strong"><strong>sudo yum install &lt;package&gt; and sudo zypper install &lt;package&gt;
</strong></span>
</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec88"></a>See also</h3></div></div></div><p>For installing Pandas from the Anaconda distribution, please refer to <a class="ulink" href="http://docs.continuum.io/anaconda/install" target="_blank">http://docs.continuum.io/anaconda/install</a> and <a class="ulink" href="http://pandas.pydata.org/pandas-docs/stable/install.html" target="_blank">http://pandas.pydata.org/pandas-docs/stable/install.html</a>.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec26"></a>Installing Pandas from source</h2></div></div><hr /></div><p>In this recipe, we will see how to install Pandas from Source on Linux. Before proceeding with the installation, let's consider the version of Python we're going to use. There are two versions or flavors of Python, namely Python 2.7.x and Python 3.x. Although the latest version, Python 3.x, appears to be the better choice, for scientific, numeric, or data analysis work, Python 2.7 is recommended.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec89"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you need Ubuntu 14.04 (Linux flavor) installed on the machine. Python comes pre-installed. The <code class="literal">python --version</code> command gives the version of Python installed. If the version seems to be 2.6.x, upgrade it to Python 2.7 as follows:</p><pre class="programlisting">
<span class="strong"><strong>    sudo apt-get install python2.7
</strong></span>
</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec90"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Install the <code class="literal">easy_install</code> program:</p><pre class="programlisting">
<span class="strong"><strong>       wget http://python-distribute.org/distribute_setup.pysudo python
       distribute_setup.py
</strong></span>
</pre></li><li><p>Install Cython:</p><pre class="programlisting">
<span class="strong"><strong>       sudo easy_install -U Cython
</strong></span>
</pre></li><li><p>Install from the source code as follows:</p><pre class="programlisting">
<span class="strong"><strong>       git clone git://github.com/pydata/pandas.git
       cd pandas
       sudo python setup.py install
</strong></span>
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec91"></a>How it worksâ€¦</h3></div></div></div><p>The preceding commands install the Pandas library and the related dependencies. The source code could be found on <a class="ulink" href="http://github.com/pydata/pandas" target="_blank">http://github.com/pydata/pandas</a>. It has to be ensured that Cython is installed at compile time.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec92"></a>There's moreâ€¦</h3></div></div></div><p>The easiest and most general way to install the Pandas library is to use a pre-packaged solution, that is, installing it through the distribution Anaconda or Enthought. Also, when installing other Linux flavors such as Centos and OpenSuse, we use:</p><pre class="programlisting">
<span class="strong"><strong>    sudo yum install &lt;package&gt; and sudo zypper install &lt;package&gt;
</strong></span>
</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec93"></a>See also</h3></div></div></div><p>For installing Pandas from the Anaconda distribution, please refer to <a class="ulink" href="http://docs.continuum.io/anaconda/install" target="_blank">http://docs.continuum.io/anaconda/install</a>Â andÂ <a class="ulink" href="http://pandas.pydata.org/pandas-docs/stable/install.html" target="_blank">http://pandas.pydata.org/pandas-docs/stable/install.html</a>. Visit the earlier <span class="emphasis"><em>Installation on Linux</em></span> recipe for details on installing Pandas.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec27"></a>Using IPython with PySpark</h2></div></div><hr /></div><p>As Python is the most preferred choice for data scientists due to its high-level syntax and extensive library of packages, Spark developers have considered it for data analysis. The PySpark API has been developed for working with RDDs in Python. IPython Notebook is an essential tool for data scientists to present the scientific and theoretical work in an interactive fashion, integrating both text and Python code.</p><p>This recipe shows how to configure IPython with PySpark and also focuses on connecting the IPython shell to PySpark.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec94"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you need Ubuntu 14.04 (Linux flavor) installed on the machine. Python comes pre-installed. The <code class="literal">python --version</code> command gives the version of the Python installed. If the version seems to be 2.6.x, upgrade it to Python 2.7 as follows:</p><pre class="programlisting">
<span class="strong"><strong>    sudo apt-get install python2.7
</strong></span>
</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec95"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Install IPython as follows:</p><pre class="programlisting">
<span class="strong"><strong>       sudo pip install ipython
</strong></span>
</pre></li><li><p>Create an IPython profile for use with PySpark as follows:</p><pre class="programlisting">
<span class="strong"><strong>       ipython profile create pyspark</strong></span>
</pre></li><li><p>Create a profile startup script as follows:</p><pre class="programlisting">
<span class="strong"><strong>      vim ~/.ipython/profile_pyspark/startup/00-pyspark-setup.py
</strong></span>
</pre></li><li><p>Put the following code inside the file:</p><pre class="programlisting">      import os
      import sys
      spark_home = os.environ.get('SPARK_HOME', None)
      if not spark_home:
      raise ValueError('SPARK_HOME environment variable is not set')
      sys.path.insert(0, os.path.join(spark_home, 'python'))
      sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.9-
       src.zip'))
      execfile(os.path.join(spark_home, 'python/pyspark/shell.py'))
</pre></li><li><p>Edit the <code class="literal">~/.ipython/profile_pyspark/ipython_config.py</code> file and put the following inside the file:</p><pre class="programlisting">        c = get_config()
        c.NotebookApp.ip = '*'
        c.NotebookApp.open_browser = False
        c.NotebookApp.port = 8880
</pre></li><li><p>Install <code class="literal">py4j</code> as follows:</p><pre class="programlisting">       sudo pip install py4j</pre></li><li><p>Place the following environment variables inside <code class="literal">~/.bashrc</code>:</p><pre class="programlisting">        export PYTHONPATH=
        $SPARK_HOME/python/:$SPARK_HOME/python/lib/py4j-0.9-src.zip
        export PYTHONSTARTUP=$SPARK_HOME/python/pyspark/shell.py
        export PYSPARK_SUBMIT_ARGS="--master
        spark://master: pyspark-shell"
</pre></li><li><p>If you are using Jupyter, create a kernel that uses this profile as touch <code class="literal">~/.ipython/kernels/pyspark/kernel.json</code> and place the following content in the kernel file:</p><pre class="programlisting">        {
          "display_name": "pySpark (Spark 1.6.0)",
          "language": "python",
          "argv": [
          "/usr/bin/python2",
          "-m",
          "IPython.kernel",
          "-f",
          "{connection_file}"
         ]
        }
</pre></li><li><p>Invoke the IPython console as follows:</p><pre class="programlisting">
<span class="strong"><strong>        ipython console -profile=pyspark
</strong></span>
</pre></li><li><p>Invoke IPython Notebook as follows:</p><pre class="programlisting">
<span class="strong"><strong>        ipython notebook --profile=pyspark
</strong></span>
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec96"></a>How it workâ€¦</h3></div></div></div><p>The preceding commands install the IPython console and IPython notebook, which are used to develop code by importing the PySpark API.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec97"></a>There's moreâ€¦</h3></div></div></div><p>From Spark 1.4,'the Pandas DataFrame computation can be moved to the Apache Spark parallel computation framework using Spark SQL's DataFrame. A few operations in Pandas don't translate to Spark as DataFrames are immutable data sets. Spark is trying to bridge the gap that data analysis performed using Pandas, that is, from Spark 1.4, port any Pandas computation in a distributed environment using the very similar DataFrame API.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec98"></a>See also</h3></div></div></div><p>Please refer the earlier <span class="emphasis"><em>Installing Pandas on Linux</em></span> and <span class="emphasis"><em>Installing PandasÂ </em></span>
<span class="emphasis"><em>from Source</em></span> recipes to understand how to proceed with Pandas installation.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec28"></a>Creating Pandas DataFrames over Spark</h2></div></div><hr /></div><p>A DataFrame is a distributed collection of data organized into named columns. It is equivalent to a table in a relational database or a DataFrame in R/Python Python with rich optimizations. These can be constructed from a wide variety of sources, such as structured data files (JSON and parquet files), Hive tables, external databases, or from existing RDDs.</p><p>PySpark is the Python API for Apache Spark which is designed to scale to huge amounts of data. This recipe shows how to make use of Pandas over Spark.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec99"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark cluster either in pseudo distributed mode or in one of the distributed modes, that is, standalone, YARN, or Mesos. Also, have Python and IPython installed on the Linux machine, that is, Ubuntu 14.04.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec100"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Invoke <code class="literal">ipython console -profile=pyspark </code>Â as follows:</p><pre class="programlisting">      In [4]: from pyspark import SparkConf, SparkContext, SQLContext
      In [5]: import pandas as pd
</pre></li><li><p>Creating a Pandas DataFrame as follows:</p><pre class="programlisting">     In [6]: pdf = pd.DataFrame({'Name':['Padma','Major','Priya'],
     'Age':  [23,45,30]})
</pre></li><li><p>Creating a Spark DataFrame from a Pandas DataFrame as follows:</p><pre class="programlisting">      In [7]: sqlc=SQLContext(sc)
      In [8]: spark_df = sqlc.createDataFrame(pdf)
</pre></li><li><p>Referring a column in Pandas as follows:</p><pre class="programlisting">      In [9]: pdf.Name
      Out[9]:
      0    Padma
      1    Major
      2    Priya
      Name: Name, dtype: object
      In [10]: pdf['Name']
      Out[10]:
      0    Padma
      1    Major
      2    Priya
      Name: Name, dtype: object
</pre></li><li><p>Referring a column in a Spark DataFrame as follows:</p><pre class="programlisting">      In [11]: spark_df.Name
      Out[11]: Column&lt;Name&gt;
      In [12]: spark_df['Name']
      Out[12]: Column&lt;Name&gt;
</pre></li><li><p>Adding a column to a Pandas DataFrame as follows:</p><pre class="programlisting">      In [13]: pdf['C']=0
      In [14]: pdf
      Out[14]:
         Age   Name  C
      0   23  Padma  0
      1   45  Major  0
      2   30  Priya  0
</pre></li><li><p>Adding a column to a Spark DataFrame as follows:</p><pre class="programlisting">      In [15]: from pyspark.sql import functions
      In [16]: spark_df.withColumn('C',functions.lit(0))
      Out[16]: DataFrame[Age: bigint, Name: string, C: int]
      In [17]: spark_df.show()
      +---+-----+
      |Age| Name|
      +---+-----+
      | 23|Padma|
      | 45|Major|
      | 30|Priya|
      +---+-----+
      In [18]: spark_df.withColumn('C',functions.lit(0)).show()
      +---+-----+---+
      |Age| Name|  C|
      +---+-----+---+
      | 23|Padma|  0|
      | 45|Major|  0|
      | 30|Priya|  0|
      +---+-----+---+
</pre></li><li><p>Manipulating column C to take values twice the value of column <code class="literal">Age</code>:</p><pre class="programlisting">      In [19]:spark_df.withColumn('C',spark_df.Age  *2).show()
      +---+-----+---+
      |Age| Name|  C|
      +---+-----+---+
      | 23|Padma| 46|
      | 45|Major| 90|
      | 30|Priya| 60|
      +---+-----+---+
      In [20]: pdf['C']=pdf.Age*2
      In [21]: pdf
      Out[21]:
         Age   Name   C
      0   23  Padma  46
      1   45  Major  90
      2   30  Priya  60
</pre></li><li><p>Selecting a column in a Spark DataFrame:</p><pre class="programlisting">    In [22]: spark_df.select(spark_df.Age &gt; 0).show()

    +---------+
    |(Age &gt; 0)|
    +---------+
    |     true|
    |     true|
    |     true|
    +---------+
</pre></li><li><p>Selecting a column in a Pandas DataFrame:</p><pre class="programlisting">   In [23]: pdf['Age']&gt;0
   Out[23]:
   0    True
   1    True
   2    True
   Name: Age, dtype: bool
</pre></li><li><p>Converting a Spark DataFrame back to Pandas:</p><pre class="programlisting">   In [47]: spark_df.toPandas
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec101"></a>How it worksâ€¦</h3></div></div></div><p>In the preceding code snippets, we have seen that <code class="literal">pdf = pd.DataFrame</code> creates a Pandas DataFrame and <code class="literal">sqlcontext.createDataFrame(pdf)</code> converts a Pandas DataFrame to a Spark DataFrame. We have seen various ways to add a column in Pandas and the same which could be done in Spark. The only difference is that in Pandas, it is a mutable data structure whereas in Spark, it is immutable. Also, the columns can be manipulated using an expression such as <code class="literal">spark_df.withColumn('C',spark_df.Age *2)</code>. The Spark DataFrame can be converted to a Pandas DataFrame as <code class="literal">spark_df.toPandas</code>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec102"></a>There's moreâ€¦</h3></div></div></div><p>Spark and Pandas DataFrames are very similar. Although the Pandas API remains more convenient and powerful, the gap is shrinking quickly. Spark is well known for its features such as immutability, distributed computation, lazy evaluation, and so on. Spark wants to mimic Pandas. With <code class="literal">Spark.ml</code> available, it becomes the perfect one-stop-shop for industrialized data science.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec103"></a>See also</h3></div></div></div><p>Please refer to the earlier <span class="emphasis"><em>Installing Pandas on Linux</em></span>, <span class="emphasis"><em>Installing Pandas from Source</em></span> and <span class="emphasis"><em>Using IPython with PySpark</em></span> recipes to understand how to proceed with Pandas and IPython installation.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec29"></a>Splitting, slicing, sorting, filtering, and grouping DataFrames over Spark</h2></div></div><hr /></div><p>This recipe shows how to filter, slice, sort, index, and group Pandas DataFrames as well as Spark DataFrames.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec104"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark cluster either in pseudo distributed mode or in one of the distributed modes, that is, standalone, YARN, or Mesos. Also, have Python and IPython installed on the Linux machine, that is, Ubuntu 14.04.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec105"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Invoke <code class="literal">ipython console -profile=pyspark</code> as follows:</p><pre class="programlisting">        In [4]: from pyspark import SparkConf, SparkContext, SQLContext
        In [5]: import pandas as pd
</pre></li><li><p>Creating a Pandas DataFrame as follows:</p><pre class="programlisting">       In [6]: pdf = pd.DataFrame({'Name':['Padma','Major','Priya'],
                                   'Age':  [23,45,30]})
</pre></li><li><p>Creating a Spark DataFrame from a Pandas DataFrame as follows:</p><pre class="programlisting">     In [7]: sqlc=SQLContext(sc)
     In [8]: spark_df = sqlc.createDataFrame(pdf)
</pre></li><li><p>Splitting a Pandas DataFrame into two based on column <code class="literal">Age</code>:</p><pre class="programlisting">     In [9]: pdf1 = pdf[pdf['Age'] &gt;=30]
     In [10]: pdf1
     Out[10]:
        Age   Name   C
     1   45  Major  90
     2   30  Priya  60
     In [11]: pdf2 = pdf[pdf['Age'] &lt;30]
     In [12]: pdf2
     Out[12]:
        Age   Name   C
     0   23  Padma  46
</pre></li><li><p>Splitting a Spark DataFrame into two based on column <code class="literal">Age</code>:</p><pre class="programlisting">     In [13]: spark_df2 = spark_df[spark_df['Age'] &gt;=30]
     In [14]: spark_df2.show()
     +---+-----+
     |Age| Name|
     +---+-----+
     | 45|Major|
     | 30|Priya|
     +---+-----+
</pre></li><li><p>Slicing a Pandas DataFrame as follows:</p><pre class="programlisting">     In [15]: pdf[0:1]
     Out[15]:
         Age   Name   C
      0   23  Padma  46
     In [16]: pdf[1:2]
     Out[16]:
        Age   Name   C
     1   45  Major  90
</pre></li><li><p>Slicing a Spark DataFrame as follows:</p><pre class="programlisting">     In [17]: spark_df['Age','Name'].show()
     +---+-----+
     |Age| Name|
     +---+-----+
     | 23|Padma|
     | 45|Major|
     | 30|Priya|
     +---+-----+
</pre></li><li><p>Sorting a Pandas DataFrame as follows:</p><pre class="programlisting">     In [18]: pdf.sort('Name')
     Out[18]:
     Age   Name   C
     1   45  Major  90
     0   23  Padma  46
     2   30  Priya  60
     In [19]: pdf.sort(['Name','Age'], ascending=False)
     Out[19]:
        Age   Name   C
    2   30  Priya  60
    0   23  Padma  46
    1   45  Major  90
</pre></li><li><p>Sorting a Spark DataFrame as follows:</p><pre class="programlisting">     In [20]: spark_df.sort(spark_df['Name']).show()
     +---+-----+
     |Age| Name|
     +---+-----+
     | 45|Major|
     | 23|Padma|
     | 30|Priya|
     +---+-----+
</pre></li><li><p>Filtering pandas and Spark DataFrames as follows:</p><pre class="programlisting">     pdf = pd.DataFrame.from_items([('A', [1, 2, 3]), ('B', [4, 5,
                                   6])])
     df = sqlcontext.createDataFrame([(1, 4), (2, 5), (3, 6)], ["A",
                                    "B"])
     In [21]: pdf[(pdf.B &gt; 0) &amp; (pdf.A &lt; 2)]
     Out[21]:
        A  B
     0  1  4
     In [22]:  df.filter((df.B &gt; 0) &amp; (df.A &lt; 2)).show()

     +---+---+
     |  A|  B|
     +---+---+
     |  1|  4|
     +---+---+
</pre></li><li><p>The <code class="literal">GroupBy</code> operation on Pandas and Spark DataFrames as follows:</p><pre class="programlisting">     In [23]: pdf.groupby(['A']).mean()
     Out[23]:
     B
     A
     1  4
     2  5
     3  6
     In [117]: df.groupBy("A").avg("B").show()
     +---+------+
     |  A|avg(B)|
     +---+------+
     |  1|   4.0|
     |  2|   5.0|
     |  3|   6.0|
     +---+------+
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec106"></a>How it worksâ€¦</h3></div></div></div><p>In the preceding code snippets, we have seen splitting Pandas as well as Spark DataFrames as <code class="literal">pdf[pdf['Age'] &lt;30]</code> and <code class="literal">spark_df[spark_df['Age'] &gt;=30]</code>. Also, we have seen how to slice, sort, and group the data in Pandas and Spark DataFrames. The syntax of the operations seem to be almost the same in both.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec107"></a>There's moreâ€¦</h3></div></div></div><p>There are numerous other operations available in Pandas, such as indexing, merging, joining, concatenating, reshaping, dealing with time-series data and so on. Spark and Pandas DataFrames are very similar. Although the Pandas API remains more convenient and powerful, the gap is shrinking quickly. Spark is well known for its features, such as immutability, distributed computation, lazy evaluation, and so on. Spark wants to mimic Pandas. With <code class="literal">Spark.ml</code> available, it becomes the perfect one-stop-shop for industrialized data science.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec108"></a>See also</h3></div></div></div><p>Please refer to earlier <span class="emphasis"><em>Creating Pandas DataFrames over Spark</em></span> recipe to get familiar with working on Pandas with Spark.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec30"></a>Implementing co-variance and correlation using Pandas</h2></div></div><hr /></div><p>This recipe shows how to implement co-variance using Pandas DataFrames over Spark.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec109"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark cluster either in pseudo distributed mode or in one of the distributed modes, that is, standalone, YARN, or Mesos. Also, have Python and IPython installed on the Linux machine, that is, Ubuntu 14.04.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec110"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Invoke <code class="literal">ipython console -profile=pyspark</code>.</p></li><li><p>Computing correlation and co-variance using Pandas in PySpark:</p><pre class="programlisting">      In [1]: from pyspark import SparkConf, SparkContext, SQLContext
      In [2]: import pandas as pd
      In [3]: seq=pd.Series([1,2,3,4,4,3,2,1],
      ['2006','2007','2008','2009','2010','2011','2012','2013'])
      In [4]: seq2 = pd.Series([3,4,3,4,5,4,3,2],
      ['2006','2007','2008','2009','2010','2011','2012','2013'])
      In [5]: seq.corr(seq2)
      Out[5]: 0.77459666924148329
      In [6]: seq.cov(seq2)
      Out[6]: 0.8571428571428571
</pre></li><li><p>Computing correlation and co-variance using Spark DataFrames:</p><pre class="programlisting">      In [186]: seq=pd.DataFrame.from_items([('A', [1,2,3,4,4,3,2,1]),
       ('B',[1,4,2,3,4,2,3,2])  ])
      In [187]: df = sqlc.createDataFrame(seq)
      In [189]: df.stat.cov('A','B')
      Out[189]: 0.6428571428571429
      In [190]: df.stat.corr('A','B')
      Out[190]: 0.50709255283711
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec111"></a>How it worksâ€¦</h3></div></div></div><p>In the preceding code snippets, we have seen how covariance and correlation can be calculated using Pandas over PySpark as well as using Spark. DataFrames.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note12"></a>Note</h3><p>For more detail on correlation and co-variance, please refer to <a class="ulink" href="https://en.wikipedia.org/wiki/Covariance" target="_blank">https://en.wikipedia.org/wiki/Covariance</a>Â and <a class="ulink" href="https://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient" target="_blank">https://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient</a>.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec112"></a>There's moreâ€¦</h3></div></div></div><p>Apart from correlation and co-variance, there are other statistical functions such as random data generation, cross tabulation, frequent item set generation, mathematical functions, and so on. For details, please refer to <a class="ulink" href="https://databricks.com/blog/2015/06/02/statistical-and-mathematical-functions-with-dataframes-in-spark.html" target="_blank">https://databricks.com/blog/2015/06/02/statistical-and-mathematical-functions-with-dataframes-in-spark.html</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec113"></a>See also</h3></div></div></div><p>Please refer to the earlier <span class="emphasis"><em>Creating Pandas DataFrames over Spark</em></span> recipe to get familiar with working on Pandas with Spark and splitting, slicing, sorting, filtering, and grouping DataFrames over Spark to get hands-on with Pandas on different functionalities.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec31"></a>Concatenating and merging operations over DataFrames</h2></div></div><hr /></div><p>This recipe shows how to concatenate, merge/join, and perform complex operations over Pandas DataFrames as well as Spark DataFrames.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec114"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark cluster either in pseudo distributed mode or in one of the distributed modes, that is, standalone, YARN, or Mesos. Also, have Python and IPython installed on the Linux machine, that is, Ubuntu 14.04.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec115"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Invoke <code class="literal">ipython console -profile=pyspark</code>:</p><pre class="programlisting">      In [1]: from pyspark import SparkConf, SparkContext, SQLContext
      In [2]: import pandas as pd
      In [3]: sqlcontext = SQLContext(sc)
      In [4]: pdf1 = pd.DataFrame({'A':['A0','A1','A2','A3'], 'B':
      ['B0','B1','B2','B3'], 'C':['C0','C1','C2','C3'],'D':
      ['D0','D1','D2','D3']},index=[0,1,2,3])
      In [5]: pdf2 = pd.DataFrame({'A':['A4','A5','A6','A7'], 'B':
      ['B4','B5','B6','B7'], 'C':['C4','C5','C6','C7'],'D':
      ['D4','D5','D6','D7']},index=[4,5,6,7])
      In [6]: frames = [pdf1,pdf2]
</pre></li><li><p>Concatenating Pandas DataFrames:</p><pre class="programlisting">        In [7]: result = pd.concat(frames)
        In [8]: df1 = sqlcontext.createDataFrame(pdf1)
        In [9]: df2 = sqlcontext.createDataFrame(pdf2)
        In [10]: df1.unionAll(df2).show()
        +---+---+---+---+
        |  A|  B|  C|  D|
        +---+---+---+---+
        | A0| B0| C0| D0|
        | A1| B1| C1| D1|
        | A2| B2| C2| D2|
        | A3| B3| C3| D3|
        | A4| B4| C4| D4|
        | A5| B5| C5| D5|
        | A6| B6| C6| D6|
        | A7| B7| C7| D7|
        +---+---+---+---+
</pre></li><li><p>Displaying concatenated Pandas DataFrame result:</p><pre class="programlisting">      In [11]: result
      Out[11]:
          A   B   C   D
      0  A0  B0  C0  D0
      1  A1  B1  C1  D1
      2  A2  B2  C2  D2
      3  A3  B3  C3  D3
      4  A4  B4  C4  D4
      5  A5  B5  C5  D5
      6  A6  B6  C6  D6
      7  A7  B7  C7  D7
</pre></li><li><p>Here is an example of a many-to-many join case in Pandas:</p><pre class="programlisting">      In [12]: left = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3'],
      'A': ['A0', 'A1', 'A2', 'A3'],
      'B': ['B0', 'B1', 'B2', 'B3']})
      In [13]: right = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3'],
      'C': ['C0', 'C1', 'C2', 'C3'],
      'D': ['D0', 'D1', 'D2', 'D3']})
      In [14]: result = pd.merge(left, right, on='key')
      In [15]: result
      Out[25]:
          A   B key   C   D
      0  A0
      B0  K0  C0  D0
       1  A1  B1  K1  C1  D1
       2  A2  B2  K2  C2  D2
       3  A3  B3  K3  C3  D3
</pre></li><li><p>Merging the preceding DataFrames in Spark:</p><pre class="programlisting">      In [26]: left_df = sqlContext.createDataFrame(left)
      In [27]: right_df = sqlContext.createDataFrame(right)
      In [28]: result_df = left_df1.join(right_df, left_df1.key ==
      right_df.key, "leftOuter")
      In [29]: result_df.show()
      +---+---+---+---+---+---+
      |  A|  B|key|  C|  D|key|
      +---+---+---+---+---+---+
      | A0| B0| K0| C0| D0| K0|
      | A1| B1| K1| C1| D1| K1|
      | A2| B2| K2| C2| D2| K2|
      | A3| B3| K3| C3| D3| K3|
      +---+---+---+---+---+---+
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec116"></a>How it worksâ€¦</h3></div></div></div><p>In the preceding code snippets, the Pandas DataFrames are concatenated using <code class="literal">pd.concat</code>. There are also joins similar to SQL. We have shown an example of a many-to-many join case which generates the Cartesian product of the associated data on a key that appears in both the tables. <code class="literal">pd.merge(left, right, on='key')</code> joins DataFrames on the column <code class="literal">key</code>. The Spark DataFrames are joined using <code class="literal">left_df1.join(right_df, left_df1.key == right_df.key, "leftOuter")</code>. In both the <code class="literal">left_df1</code> and <code class="literal">right_df</code>, the common column is <code class="literal">key</code> on which join is imposed.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec117"></a>There's moreâ€¦</h3></div></div></div><p>In Pandas, there are other advanced functionalities, such as multi-indexing, concatenating objects (using append, with mixed ndims, with group keys), a variety of joins (joining on index, joining key columns on an index, joining single index to a multi-index), merging ordered data, time series data, reshaping using pivot tables, visualization, a variety of data structures, a number of I/O tools (text, CSV, HDFS, and so on), and so on. The Spark community is slowly incorporating all the operations doable with Pandas into Spark. However, it takes little time to have DataFrame API in Spark fully evolved and this will ultimately be a one-stop solution for data scientists and data engineers.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec118"></a>See also</h3></div></div></div><p>Please refer to the earlier <span class="emphasis"><em>Creating Pandas DataFrames over Spark</em></span> recipe to get familiar with working on Pandas with Spark and splitting, slicing, sorting, filtering, and grouping DataFrames over Spark to get hands-on with Pandas on different functionalities.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note13"></a>Note</h3><p>For more on the advanced functionalities of Pandas, please refer to <a class="ulink" href="http://pandas.pydata.org/pandas-docs/stable/" target="_blank">http://pandas.pydata.org/pandas-docs/stable/</a>.</p></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec32"></a>Complex operations over DataFrames</h2></div></div><hr /></div><p>This recipe shows how to perform complex operations such as computing difference on a column in Pandas DataFrames as well as Spark DataFrames.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec119"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark cluster either in pseudo distributed mode or in one of the distributed modes, that is, standalone, YARN, or Mesos. Also, have Python and IPython installed on the Linux machine, that is, Ubuntu 14.04.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec120"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Invoke <code class="literal">ipython console -profile=pyspark</code>:</p><pre class="programlisting">      In [1]: from pyspark import SparkConf, SparkContext, SQLContext
        In [2]: import pandas as pd
        In [3]: sqlcontext = SQLContext(sc)</pre></li><li><p>Computing <code class="literal">diff</code> on a column in Pandas:</p><pre class="programlisting">    In [4]: df = sqlCtx.createDataFrame([(1, 4), (1, 5), (2, 6),
    (2,   6), (3, 0)], ["A", "B"])
    In [5]: pdf = df.toPandas()
    In [6]: pdf
    Out[6]:

    A  B
    0  1  4
    1  1  5
    2  2  6
    3  2  6
    4  3  0
    In [7]: pdf['diff'] = pdf.B.diff()
    In [8]: pdf
    Out[8]:
       A  B  diff
    0  1  4   NaN
    1  1  5     1
    2  2  6     1
    3  2  6     0
    4  3  0    -6
</pre></li><li><p>Computing <code class="literal">diff</code> on a column given a specific key using the Window operation:</p><pre class="programlisting">      In [9]: from pyspark.sql.window import Window
      In [10]: window_over_A = Window.partitionBy("A").orderBy("B")
      In [11]: df.withColumn("diff", F.lead("B").over(window_over_A) -
          df.B).show()
    +---+---+-----+
    |  A|  B|diff |
    +---+---+-----+
    | 1 | 4 | 1   |
    | 1 | 5 | null|
    | 2 | 6 | 0   |
    | 2 | 6 | null|
    | 3 | 0 | null|
    +---+---+-----+
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec121"></a>How it worksâ€¦</h3></div></div></div><p>The above code snippets show how to perform complex operations. In Pandas, we can compute diff on a column by comparing the values of one line to the last one and computing the difference between them. <code class="literal">pdf['diff'] = pdf.B.diff()</code> computes the difference. But the same feature is tough to implement in a distributed environment because each line is supposed to be treated independently. In Spark 1.4, there is support for window operations that is, Window is defined over a column on which Spark will execute some aggregation functions but relative to a specific line. The <code class="literal">df.withColumn("diff", F.lead("B").over(window_over_A) - df.B)</code> line computes the difference line-by-line ordered or not given a specific key.</p><p>With Window operations, the results are copied as additional columns without any explicit join.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec122"></a>There's moreâ€¦</h3></div></div></div><p>The Spark community is slowly incorporating all the operations doable with Pandas in to. However, it takes a little time to have DataFrame API fully evolved in Spark and this will ultimately be one-stop solution for data scientists and data engineers.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec123"></a>See also</h3></div></div></div><p>Please refer to the earlier <span class="emphasis"><em>Creating Pandas DataFrames over Spark</em></span> recipe to get familiar with working on Pandas with Spark and splitting, slicing, sorting, filtering, and grouping DataFrames over Spark to get hands-on with Pandas on different functionalities.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note14"></a>Note</h3><p>For more on the advanced functionalities of Pandas, please refer to <a class="ulink" href="http://pandas.pydata.org/pandas-docs/stable/" target="_blank">http://pandas.pydata.org/pandas-docs/stable/</a>.</p></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec33"></a>Sparkling Pandas</h2></div></div><hr /></div><p>Sparkling Pandas aims to make use of the distributed computing power of PySpark to scale the data analysis with Pandas. This is built on Spark's DataFrame and provides API similar to Pandas. This recipe shows how to use Sparkling Pandas.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec124"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark Cluster either in pseudo distributed mode or in one of the distributed modes, that is, standalone, YARN, or Mesos. Also, have Python and IPython installed on the Linux machine, that is, Ubuntu 14.04.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec125"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Install <code class="literal">blas</code>, <code class="literal">scipy</code>, and <code class="literal">sparklingpandas</code> as follows:</p><pre class="programlisting">
<span class="strong"><strong>     apt-get install libblas-dev liblapack-dev libatlas-base-dev
     gfortran
     pip install scipy
     pip install sparklingpandas
</strong></span>
</pre></li><li><p>Now import the package as follows:</p><pre class="programlisting">     import sparklingpandas
     ipython console -profile=pyspark
     In [1]: from pyspark import SparkConf, SparkContext, SQLContext
     In [2]: import pandas as pd
     In [3]: sqlcontext = SQLContext(sc)
     In [4]: import sparklingpandas</pre></li><li><p>Creating a distributed collection of DataFrames in Spark:</p><pre class="programlisting">     In [5]: input = [1,2,3,4]
     In [6]: rdd = sc.parallelize(zip(range(len(input)), input))
     In [7]: frames_rdd = rdd.map(lambda x:pd.DataFrame(data=
             [x[1]],index=[x[0]]))</pre></li><li><p>Distributed operations:</p><pre class="programlisting">    In [8]: sqF = (frames_rdd.map(lambda f:f.applymap(lambda x:x*x)))
    In [9]: sqDFs = sqF.collect()
    In [10]: sqDF = reduce(lambda x,y:x.append(y),sqDFs<span class="strong"><strong>)</strong></span>
</pre></li><li><p>Creating distributed DataFrames using <code class="literal">sparklingpandas</code>:</p><pre class="programlisting">     In [11]: from sparklingpandas.pcontext import PsparkContext
     In [12]: ddf = psc.read_csv()
     In [13]: schema_rdd = sqlContext.sql("SELECT panda_id, num_babies,
              city FROM panda_info")
              ddf = psc.from_schema_rdd(schema_rdd) //creating from
              schema rdd of spark SQL
</pre></li><li><p>Simple analysis with the data:</p><pre class="programlisting">     pandas_with_kids = ddf.dropna(subset=['num_babies'])
     sqDDF = ddf.applymap(lambda x:x*x)
     ddfFour = ddf.query("b==4")
     sqDF = sqDDF.collect()
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec126"></a>How it worksâ€¦</h3></div></div></div><p>In the preceding code snippets, we have seen how to use distributed DataFrame operations that could be done using SparklingÂ Pandas. The normal DataFrames on Spark as are created in such a way that each partition contains many DataFrames instead of one DataFrame containing many rows in each partition which is slightly less efficient.</p><p>This works in a way that we started with the data in-memory on single node, distributed across the operations, and finally retrieved the results back to the single node. If the data doesn't fit into the memory of a single node initially, the approach doesn't work. Hence <code class="literal">sparklingpandas</code> distributes the data on multiple nodes in the initial stage itself. <code class="literal">ddf</code> is the distributed DataFrame which can be created either directly or from the schema <code class="literal">rdd</code> of Spark SQL.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec127"></a>There's moreâ€¦</h3></div></div></div><p>Sparkling Pandas is a new library that brings together the best features of Pandas and PySpark: expressiveness, speed, and scalability. This is comparatively faster than PySpark. However, with the latest improvements in Spark's DataFrame API, the gap between Spark DataFrames and Pandas is slowly being minimized and the project <code class="literal">sparklingpandas</code> is no longer active.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec128"></a>See also</h3></div></div></div><p>Please refer to the earlierÂ <span class="emphasis"><em>Creating Pandas DataFrames over Spark</em></span> recipe to get familiar with working on Pandas with Spark and splitting, slicing, sorting, filtering, and grouping DataFrames over Spark to get hands-on with Pandas on different functionalities.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note15"></a>Note</h3><p>For details on <code class="literal">sparklingpandas</code>, please visit <a class="ulink" href="https://github.com/sparklingpandas/sparklingpandas" target="_blank">https://github.com/sparklingpandas/sparklingpandas</a> and <a class="ulink" href="http://sparklingpandas.com/" target="_blank">http://sparklingpandas.com/</a>.</p></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch03"></a>ChapterÂ 3.Â Data Analysis with Spark</h2></div></div></div><p>In this chapter, we will cover the following recipes on performing data analysis with Spark:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Univariate analysis</p></li><li style="list-style-type: disc"><p>Bivariate analysis</p></li><li style="list-style-type: disc"><p>Missing value treatment</p></li><li style="list-style-type: disc"><p>Outlier detection</p></li><li style="list-style-type: disc"><p>Use case - analyzing the MovieLens dataset</p></li><li style="list-style-type: disc"><p>Use case - analyzing the Uber dataset</p></li></ul></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec34"></a>Introduction</h2></div></div><hr /></div><p>The techniques for data exploration and preparation are typically applied before applying models on the data and this also helps in developing complex statistical models. These techniques are also important for eliminating or sharpening a potential hypothesis which can be addressed by the data. The amount of time spent in preprocessing and data exploration provides the quality input which decides the quality of the output. Once the business hypothesis is ready, a series of steps in data exploration and preparation decides the accuracy of the model and reliable results.</p><p>In this chapter, we are going to look at the following common data analysis techniques such as univariate analysis, bivariate analysis, missing values treatment, identifying the outliers, and techniques for variable transformation.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec35"></a>Univariate analysis</h2></div></div><hr /></div><p>Once the data is available, we have to spend lot of time and effort in data exploration, cleaning and preparation because the quality of the high input data decides the quality of calculating the output. Hence, once we identify the business questions, the first step of data exploration/analysis is univariate analysis, which explores the variables one by one. The methods of univariate analysis depend on whether the variable type is categorical or continuous.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec129"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark cluster in any one of the modes, that is, local, standalone, YARN, or Mesos. For installing Spark on a standalone cluster, please refer to <a class="ulink" href="http://spark.apache.org/docs/latest/spark-standalone.html" target="_blank">http://spark.apache.org/docs/latest/spark-standalone.html</a>. Also, include the Spark MLlib package in the <code class="literal">build.sbt</code> file so that it downloads the related libraries and the API can be used. Install Hadoop (optionally), Scala, and Java.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec130"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Let's take the example ofÂ <code class="literal">Titanic</code> dataset. On April 15, 1912, the titanic sank after colliding with an iceberg, killing 1,502 out of 2,224 passengers. Some groups of people were more likely to survive than others, such as women, children, and the upper class. Let's try to do some data exploration on each variable and come up with inferences about the data.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note16"></a>Note</h3><p>Please download the data from <a class="ulink" href="https://github.com/ChitturiPadma/datasets/blob/master/titanic_data.csv" target="_blank">https://github.com/ChitturiPadma/datasets/blob/master/titanic_data.csv</a> and the description about the variables from <a class="ulink" href="https://github.com/ChitturiPadma/datasets/blob/master/titanic_data_description" target="_blank">https://github.com/ChitturiPadma/datasets/blob/master/titanic_data_description</a>.</p></div></li><li><p>The data resides in HDFS. Here is the code which loads the data. Let's have a look at a few records of the data:</p><pre class="programlisting">       import org.apache.spark._
       import org.apache.spark.sql._
       import org.apache.spark.sql.functions._
       import org.apache.spark.sql.DataFrameNaFunctions
       import org.apache.spark.sql.types._
       object Univariate_Analysis {
         def main(args:Array[String]): Unit = {
           val conf = new SparkConf()
             .setMaster("spark://master:7077")
             .setAppName("Univariate_Analysis")
           val sc = new SparkContext(conf)
           val sqlContext = new SQLContext(sc)
           import sqlContext.implicits._
           //Loading data
           val titanic_data =
       sqlContext.read.format("com.databricks.spark.csv")
             .option("header", "true")
             .option("inferSchema","true")
           .load("hdfs://namenode:9000/titanic_data.csv")
           titanic_data.show(10) } }
</pre><p>The following is the output:</p><pre class="programlisting">+-----------+--------+------+--------------------+------+
|PassengerId|Survived|Pclass|                Name|   Sex|
+-----------+--------+------+--------------------+------+
|          1|       0|     3|Braund, Mr. Owen ...|  male|
|          2|       1|     1|Cumings, Mrs. Joh...|female|
|          3|       1|     3|Heikkinen, Miss. ...|female|
|          4|       1|     1|Futrelle, Mrs. Ja...|female|
|          5|       0|     3|Allen, Mr. Willia...|  male|
|          6|       0|     3|    Moran, Mr. James|  male|
|          7|       0|     1|McCarthy, Mr. Tim...|  male|
|          8|       0|     3|Palsson, Master. ...|  male|
|          9|       1|     3|Johnson, Mrs. Osc...|female|
|         10|       1|     2|Nasser, Mrs. Nich...|female|
+-----------+--------+------+--------------------+------+

+-----+------+-----+-----------------+-------+-----+---------+
|Age  |SibSp |Parch|     Ticket      | Fare  |Cabin|Embarked |
+-----+------+-----+-----------------+-------+-----+---------+
|22.0 |1     |0    | A/5 21171       |7.25   |     |S        |
|38.0 |1     |0    | PC 17599        |71.2833|C85  |C        |
|26.0 |0     |0    | STON/O2. 3101282|7.925  |     |S        |
|35.0 |1     |0    |113803           |53.1   |C123 |S        |
|35.0 |0     |0    |373450           |8.05   |     |S        |
|null |0     |0    |330877           |8.4583 |     |Q        |
|54.0 |0     |0    |17463            |51.8625|E46  |S        |
|2.0  |3     |1    |349909           |21.075 |     |S        |
|27.0 |0     |2    |347742           |11.1333|     |S        |
|14.0 |1     |0    |237736           |30.0708|     |C        |
+-----+------+-----+-----------------+-------+-----+---------+
</pre></li><li><p>For the continuous variables, let's try to understand the central tendency and spread of the variables:</p><pre class="programlisting">      /* Mean value of Fare charged to board Titanic Ship using
      MultiVariateStatisticalSummary */
       val fare_Details_Df = titanic_data.select("Fare")
       val fareObservations = fare_Details_Df.map{row =&gt;
         Vectors.dense(row.getDouble(0))}
      val summary_Fare:MultivariateStatisticalSummary =
          Statistics.colStats(fareObservations)
     println("Mean of Fare: "+summary_Fare.mean)

     // Other way of finding the mean
      val fare_DetailsRdd = fare_Details_Df.map{row =&gt;
                            row.getDouble(0)}
      val meanValue = fare_DetailsRdd.mean()
      println("Mean Value of Fare From RDD: "+meanValue)

    // Median of the variable Fare
     val countOfFare = fare_DetailsRdd.count()
     val sortedFare_Rdd = fare_DetailsRdd.sortBy(fareVal =&gt; fareVal )
     val sortedFareRdd_WithIndex = sortedFare_Rdd.zipWithIndex()
     val median_Fare = if(countOfFare%2 ==1)
     sortedFareRdd_WithIndex.filter{case(fareVal:Double, index:Long) =&gt;
    index == (countOfFare-1)/2}.first._1
       else{
       val elementAtFirstIndex =
           sortedFareRdd_WithIndex.filter{case(fareVal:Double,
           index:Long) =&gt; index == (countOfFare/2)-1}.first._1
       val elementAtSecondIndex =
       sortedFareRdd_WithIndex.filter{case(fareVal:Double,index:Long)
       =&gt; index == (countOfFare/2)}.first._1
       (elementAtFirstIndex+elementAtSecondIndex)/2.0
     }
       println("Median of Fare variable is: "+median_Fare)

     // Mode of the variable Fare
     val fareDetails_WithCount =
     fare_Details_Df.groupBy("Fare").count()
     val maxOccurrence_CountsDf =
     fareDetails_WithCount.select(max("count")).alias("MaxCount")
     val maxOccurrence = maxOccurrence_CountsDf.first().getLong(0)
     val fares_AtMaxOccurrence =
     fareDetails_WithCount.filter(fareDetails_WithCount("count") ===
     maxOccurrence)
      if(fares_AtMaxOccurrence.count() == 1)
      println ("Mode of Fare variable is:
      "+fares_AtMaxOccurrence.first().getDouble(0))
      else {
       val modeValues = fares_AtMaxOccurrence.collect().map{row =&gt;
                        row.getDouble(0)}
      println("Fare variable has more 1 mode: ")
      modeValues.foreach(println)
     }

    //Spread of the variable
    println("Variance is: "+summary_Fare.variance)

</pre><p>The following is the output:</p><pre class="programlisting">   Mean of Fare: [32.20420796857466]
   Mean Value of Fare From RDD: 32.20420796857464
   Median of Fare variable is: 14.4542
   Mode of Fare variable is: 8.05
   Variance is: [2469.436845743116]

</pre></li><li><p>For the nominal variables, let's try to understand the frequency distribution of each category. Here is the code for the same:</p><pre class="programlisting">   // Univariate analysis for Categorical data
   val class_Details_Df = titanic_data.select("Pclass")
   val count_OfRows = class_Details_Df.count()
   println("Count of Pclass rows: "+count_OfRows)
   val classDetails_GroupedCount =
   class_Details_Df.groupBy("Pclass").count()
   val classDetails_PercentageDist =
   classDetails_GroupedCount.withColumn("PercentDistribution",
   classDetails_GroupedCount("count")/count_OfRows)
   classDetails_PercentageDist.show()

</pre><p>The following is the output:</p><pre class="programlisting">   Count of Pclass rows: 891
   +------+-----+-------------------+
   |Pclass|count|PercentDistribution|
   +------+-----+-------------------+
   |     1|  216|0.24242424242424243|
   |     2|  184|0.20650953984287318|
   |     3|  491| 0.5510662177328844|
   +------+-----+-------------------+

</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec131"></a>How it worksâ€¦</h3></div></div></div><p>Initially, the dataset is loaded as a CSV file with theÂ <code class="literal">sqlContext.read.format("com.databricks.spark.csv").option("header", "true").option("inferSchema","true").load("hdfs://namenode:9000/titanic_data.csv")</code>Â statement. The schema is inferred and the statement <code class="literal">titanic_data.show(10)</code> displays the first 10 rows of all the available fields. We identified that the variable <code class="literal">Fare</code> is continuous and performed analysis on it.</p><p>Next, as part of the data analysis, the <code class="literal">Fare</code> details are selected using the <code class="literal">titanic_data.select("Fare") </code>statement. The <code class="literal">fare_Details_Df</code>Â statement is converted to an RDD of vectors, that is, <code class="literal">fareObservations</code>. The <code class="literal">Statistics.colStats(fareObservations)</code> statement generates a statistical summary of the <code class="literal">Fare</code> variable. Now, the mean is obtained using <code class="literal">summary_Fare.mean</code>. The mean is also calculated using an alternative way by converting <code class="literal">fare_Details_Df</code> to <code class="literal">rdd</code> and invoking the <code class="literal">mean</code> function on the <code class="literal">rdd</code> as <code class="literal">fare_DetailsRdd.mean()</code>.</p><p>Now, to calculate the median, there is no built-in function available with DataFrames or RDDs. The <code class="literal">fare_DetailsRdd</code>Â variable is sorted using the <code class="literal">fare_DetailsRdd.sortBy(fareVal =&gt; fareVal )</code> statement. Next, for the sorted <code class="literal">rdd</code>, indices are generated as <code class="literal">sortedFare_Rdd.zipWithIndex()</code>. If the number of rows, <code class="literal">countOfFare</code>, is <code class="literal">odd</code>, then the value at the index <code class="literal">(countOfFare-1)/2</code> is the median. If theÂ <code class="literal">countOfFare</code>Â value is <code class="literal">even</code>, then the mean of the values at indices <code class="literal">(countOfFare/2)-1</code> and <code class="literal">countOfFare/2</code> becomes the median.</p><p>For calculating the mode, first the <code class="literal">Fare</code> values are grouped and <code class="literal">count</code> is calculated using the <code class="literal">fare_Details_Df.groupBy("Fare").count()</code> statement. Next, from theÂ <code class="literal">fareDetails_WithCount</code>Â DataFrame, the maximum count is obtained as <code class="literal">fareDetails_WithCount.select(max("count"))</code>. Once the maximum count is known, the fare values with maximum count are obtained using the <code class="literal">fareDetails_WithCount.filter(fareDetails_WithCount("count") === maxOccurrence)</code> statement. The <code class="literal">fares_AtMaxOccurrence</code> DataFrame contains mode values for the variable <code class="literal">Fare</code>, which can be 1 or &gt; 1. The calculated <code class="literal">summary_Fare</code>, which is of type <code class="literal">MultivariateStatisticalSummary</code>, also contains the measure <code class="literal">variance</code>, which is directly displayed as <code class="literal">summary_Fare.variance</code>.</p><p>Finally, for the <code class="literal">nominal</code> variable <code class="literal">Pclass</code>, the frequency distribution is calculated by aggregating the <code class="literal">Pclass</code> categories and obtaining the <code class="literal">Count</code> and <code class="literal">Count%</code> against each category.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec132"></a>There's moreâ€¦</h3></div></div></div><p>In the preceding recipe, we saw how to perform univariate analysis on the continuous variables. Statistics such as <code class="literal">min</code>, <code class="literal">max</code>, <code class="literal">count</code>, <code class="literal">man</code>, and <code class="literal">stddev</code> can also be obtained using <code class="literal">describe</code>. Apart from variance and standard deviation, there are also other measures, such as range and interquartile range which explain the spread of the variable. The distribution of the variable and outliers can be identified using box plot and histograms. Since Spark doesn't have support for visualization, sample data from RDDs can be collected on the driver and the plots can be visualized in PySpark.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec133"></a>See also</h3></div></div></div><p>Please refer to <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Big Data Analytics with Spark</em></span> to get familiar with Spark and refresh your knowledge of basic statistics and distributions. Also refer to <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Tricky Statistics with Spark</em></span> to understand how to apply statistical functions of DataFrames/RDDs in Spark.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec36"></a>Bivariate analysis</h2></div></div><hr /></div><p>Bivariate analysis finds out the relationship between two variables. In this, we always look for association and disassociation between variables at a predefined significance level. This analysis could be performed for any combination of categorical and continuous variables. The various combinations can be: both the variables categorical, categorical and continuous, and continuous and continuous.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec134"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark cluster in any one of the modes, that is, local, standalone, YARN, or Mesos. For installing Spark on a standalone cluster, please refer to <a class="ulink" href="http://spark.apache.org/docs/latest/spark-standalone.html" target="_blank">http://spark.apache.org/docs/latest/spark-standalone.html</a>.Â Also, include the Spark MLlib package in the <code class="literal">build.sbt</code> file so that it downloads the related libraries and the API can be used. Install Hadoop (optionally), Scala, and Java.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec135"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>After univariate analysis, let's try to perform bivariate analysis on various combinations of continuous and categorical variables. Here is the code:</p><pre class="programlisting">       import org.apache.spark._
       import org.apache.spark.sql._
       import org.apache.spark.sql.functions._
       import org.apache.spark.sql.DataFrameNaFunctions
       import org.apache.spark.sql.types._
       import org.apache.spark.mllib.linalg.Vectors
       import org.apache.spark.mllib.stat.
       {MultivariateStatisticalSummary, Statistics}

       object Bivariate_Analysis {
         def main(args:Array[String]): Unit = {
           val conf = new SparkConf()
             .setMaster("spark://master:7077")
             .setAppName("Bivariate_Analysis")
           val sc = new SparkContext(conf)
           val sqlContext = new SQLContext(sc)
           import sqlContext.implicits._

       //Loading data
           val titanic_data =
       sqlContext.read.format("com.databricks.spark.csv")
             .option("header", "true")
             .option("inferSchema", "true")
       .load("hdfs://namenode:9000/titanic_data.csv")

       //Correlation and Covariance between Age and Fare
       val correlated_value_AgeFare = titanic_data.stat.corr("Age",
       "Fare")
       val covariance_AgeFare = titanic_data.stat.cov("Age","Fare")
       println("Correlation Value For Age and Fare:
       "+correlated_value_AgeFare)
       println("Covariance For Age and Fare: "+ covariance_AgeFare)

       //Correlation and Covariance between Pclass and Fare
       val correlated_value_PclassFare =
       titanic_data.stat.corr("Pclass", "Fare")
       val covariance_PclassFare =
       titanic_data.stat.cov("Pclass","Fare")
       println("Correlation Value For Pclass and Fare:
      "+correlated_value_PclassFare)
       println("Covariance For Pclass and Fare: "+
       covariance_PclassFare)
        }
      }

</pre><p>The following is the output:</p><pre class="programlisting">   Correlation Value For Age and Fare: 0.135515853527051
   Covariance For Age and Fare: 118.49631587080917

   Correlation Value For Pclass and Fare: -0.5494996199439078
   Covariance For Pclass and Fare: -22.8301961700652
</pre></li><li><p>The relationship between two categorical variables can be determined using a two-way table. This table is created by calculating <code class="literal">count</code> and <code class="literal">count%</code>. The row represents the category of one variable and the columns represent the categories of the other variable. Here is the code:</p><pre class="programlisting">    // Creating two-way table between Pclass and Sex variables
    println("Frequency distribution of Pclass against variable Sex:")
    val twoWayTable_PclassSex = titanic_data.stat.crosstab("Pclass", "Sex")
    twoWayTable_PclassSex.show()

    // Creating two-way table between Sex and Embarked variables
    println("Frequency distribution of Sex variable against Embarked:")
    titanic_data.stat.crosstab("Sex","Embarked").show()

</pre><p>The following is the output:</p><pre class="programlisting">   Frequency distribution of Pclass against variable Sex:
   +----------+------+----+
   |Pclass_Sex|female|male|
   +----------+------+----+
   |         2|    76| 108|
   |         1|    94| 122|
   |         3|   144| 347|
   +----------+------+----+

   Frequency distribution of Sex variable against Embarked:
   +------------+---+---+---+---+
   |Sex_Embarked|  C|  Q|  S|   |
   +------------+---+---+---+---+
   |        male| 95| 41|441|  0|
   |      female| 73| 36|203|  2|
   +------------+---+---+---+---+

</pre></li><li><p>Let's try to test the statistical significance of the relationship between the variables in the above two-way table generated for <code class="literal">Pclass</code> and <code class="literal">Sex</code> as follows:</p><pre class="programlisting">       val PclassSex_Array = twoWayTable_PclassSex
        .drop("Pclass_Sex")
        .collect()
        .map{row =&gt; val female = row.getLong(0).toDouble
           val male = row.getLong(1).toDouble
          (female,male)}
       val femaleValues = PclassSex_Array.map{case(female, male) =&gt;
                          female}
       val maleValues = PclassSex_Array.map{case(female, male) =&gt; male}
       val goodnessOfFitTestResult =
       Statistics.chiSqTest(Matrices.dense(
       twoWayTable_PclassSex.count().toInt,
       twoWayTable_PclassSex.columns.length-1,
       femaleValues ++ maleValues ))
       println("Chi Square Test Value: "+goodnessOfFitTestResult)

</pre><p>The following is the output:</p><pre class="programlisting">   Chi Square Test Value: Chi squared test summary:
   method: pearson
   degrees of freedom = 2
   statistic = 16.971499095517114
   pValue = 2.0638864348232477E-4
   Very strong presumption against null hypothesis: the occurrence of
   the outcomes is statistically independent..

</pre></li><li><p>Now, let's try to perform analysis between the <code class="literal">Pclass</code> and <code class="literal">Fare</code> variables where <code class="literal">Pclass</code> is nominal and <code class="literal">Fare</code> is continuous:</p><pre class="programlisting">       // Analysis between categorical and continuous variables
       titanic_data.groupBy("Pclass").agg(sum("Fare"), count("Fare"),
       max("Fare"), min("Fare"), stddev("Fare") ).show()
</pre><p>The following is the output:</p><pre class="programlisting">Summary Of Fare value per each Pclass category:
+------+------------------+-----------+---------+---------+
|Pclass| sum(Fare)        |count(Fare)|max(Fare)|min(Fare)|
+------+------------------+-----------+---------+---------
|     1|18177.412500000006|        216| 512.3292|      0.0|
|     2|         3801.8417|        184|     73.5|      0.0|
|     3| 6714.695100000002|        491|    69.55|      0.0|
+------+------------------+-----------+---------+---------+




+------------------+
|stddev_samp       |
+------------------+
|78.38037264672882 |
|13.41739875614934 |
|11.778141704387306|
+------------------+
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec136"></a>How it worksâ€¦</h3></div></div></div><p>Initially, the dataset is loaded as aÂ CSVÂ file with the <code class="literal">sqlContext.read.format("com.databricks.spark.csv").option("header", "true").option("inferSchema","true").load("hdfs://namenode:9000/titanic_data.csv")</code> statement and the schema is also inferred. Next, for continuousÂ <code class="literal">Age</code> and <code class="literal">Fare</code>Â variables, correlation is obtained using the <code class="literal">val correlated_value_AgeFare = titanic_data.stat.corr("Age", "Fare")</code> statement. Also, the co-variance is obtained as <code class="literal">val covariance_AgeFare = titanic_data.stat.cov("Age","Fare")</code>. Correlation takes values between <code class="literal">-1</code> and <code class="literal">+1</code>. <code class="literal">-1</code> indicates perfect negative linear correlation, <code class="literal">+1</code> indicates perfect positive linear correlation and 0 indicates no correlation. The correlation value of <code class="literal">0.136</code> specifies weak positive correlation between the <code class="literal">Age</code> and <code class="literal">Fare</code> variables.</p><p>Next, the correlation value of <code class="literal">-0.55</code> between <code class="literal">Pclass</code> and <code class="literal">Fare</code> variables indicates that they are negatively correlated. However, from the description of the dataset, <code class="literal">Pclass</code> of 1 indicates upper class, 2 indicates middle class, and 3 indicates lower class, which means when <code class="literal">Pclass</code> value is higher, that is, <code class="literal">3</code>, <code class="literal">fare</code> is low and this satisfies the hypothesis that for lower class, the ticket fare should be low. Covariance also indicates how two variables change or vary with respect to each other, that is, it explains the dependency between two variables. In fact, it is difficult to compare co-variances among datasets that have different scales. Correlation addresses this by normalizing the co-variance to the product of the standard deviations of the variables.</p><p>Next, the relationship between the categorical variables <code class="literal">Pclass</code> and <code class="literal">Sex</code> is analyzed by creating a two-way table which represents the count of males and females for each <code class="literal">Pclass</code> category using the <code class="literal">titanic_data.stat.crosstab("Pclass", "Sex").show()</code> statement. The first row indicates that there are 76 females and 108 males under <code class="literal">Pclass</code><code class="literal">2</code>. Similarly, for the variables <code class="literal">Sex</code> and <code class="literal">Embarked</code> the <code class="literal">titanic_data.stat.crosstab("Sex","Embarked").show()</code> table is created. The first row indicates that under males, there are 95 embarked as <code class="literal">C</code>, 41 embarked as <code class="literal">Q</code>, and 441 embarked as <code class="literal">S</code>.</p><p>A chi-square test assesses whether the evidence in the sample is strong enough to generalize a relationship for a large population. From the frequency distribution table created for <code class="literal">Pclass</code> and <code class="literal">Sex</code>, the column <code class="literal">Pclass_Sex</code> is dropped and the DataFrame is converted to <code class="literal">Array[(Double,Double)]</code> represented by <code class="literal">PclassSex_Array</code>. The female values and male values are generated into two separate arrays as <code class="literal">val femaleValues = PclassSex_Array.map{case(female, male) =&gt; female}</code> and <code class="literal">val maleValues = PclassSex_Array.map{case(female, male) =&gt; male}</code>. The chi-square test is invoked by constructing <code class="literal">DenseMatrix</code> from the <code class="literal">femaleValues</code> and <code class="literal">maleValues</code> lists with the <code class="literal">val goodnessOfFitTestResult = Statistics.chiSqTest(Matrices.dense( twoWayTable_PclassSex.count().toInt, twoWayTable_PclassSex.columns.length-1, femaleValues ++ maleValues))</code> statement.</p><p>Finally, between the <code class="literal">Pclass</code> and <code class="literal">Fare</code> variables which are categorical and continuous, statistical measures such as <code class="literal">min</code>, <code class="literal">max</code>, <code class="literal">sum</code>, <code class="literal">count</code>, and <code class="literal">stddev</code> are determined for each <code class="literal">Pclass</code> category using the <code class="literal">titanic_data.groupBy("Pclass").agg(sum("Fare"), count("Fare"), max("Fare"), min("Fare"), stddev("Fare")).show()</code> statement.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec137"></a>There's moreâ€¦</h3></div></div></div><p>In the preceding recipe, we saw how to perform bivariate analysis on different combination of variables such as continuous and continuous, categorical and categorical, and, finally, categorical and continuous. The relationship between continuous variables can be determined using a scatter plot. Since Spark doesn't have support for visualization, sample data from RDDs can be collected on the driver and the plots can be visualized in PySpark.</p><p>Between the categorical variables, the statistical significance is derived by a chi-square test, which is based on the difference between the expected and observed frequencies in one or more categories in the two-way table. There are also other statistical measures which analyze the power of relationships, such as Cramer's V for nominal categorical variables and Mantel-Haenszel Chi-Square for ordinal categorical variables.</p><p>Also, for categorical and continuous variables, the statistical significance is known by performing a Z-test, T-test, or ANOVA. A Z-test/T-test determines whether the means of two groups are statistically different from each other or not. ANOVA indicates whether the averages of more than two groups are statistically different. As of now, Spark doesn't have support for performing the above-mentioned tests natively; either one can write code for the same or sample data from rdds can be taken and the measures can be applied at the Spark driver using Python.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec138"></a>See also</h3></div></div></div><p>Please refer to <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Tricky Statistics with Spark</em></span> to understand how to apply statistical functions of DataFrames/rdds in Spark. Also refer to the earlier <span class="emphasis"><em>Univariate analysis</em></span> recipe to understand the types of analysis on single variables.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec37"></a>Missing value treatment</h2></div></div><hr /></div><p>Missing data in the training dataset can reduce the fitness of a model or can lead to a biased model because we have not analyzed the behavior and relationship with other variables correctly. This could also lead to wrong predictions or classifications. The reasons for the occurrence of the missing values could be that while extracting data from multiple sources, there is a possible chance to have missing data. Hence, using some hashing procedure ensures that the data extraction is correct. The errors that occur at the time of data collection are tougher to correct as the values might miss at random and the missing values might also depend on the unobserved predictors.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec139"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark cluster in any one of the modes, that is, local, standalone, YARN, or Mesos. For installing Spark on a standalone cluster, please refer <a class="ulink" href="http://spark.apache.org/docs/latest/spark-standalone.html" target="_blank">http://spark.apache.org/docs/latest/spark-standalone.html</a>. Also, include the Spark MLlib package in the <code class="literal">build.sbt</code> file so that it downloads the related libraries and the API can be used. Install Hadoop (optionally), Scala, and Java.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec140"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Let's take an example of load prediction data. Here is what the sample data looks like:</p><div class="mediaobject"><img src="graphics/Capture-1.jpg" /></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note17"></a>Note</h3><p>Please download the data from the following location: <a class="ulink" href="https://github.com/ChitturiPadma/datasets/blob/master/Loan_Prediction_Data.csv" target="_blank">https://github.com/ChitturiPadma/datasets/blob/master/Loan_Prediction_Data.csv</a>.</p></div><p>The total record count is <code class="literal">614</code>.</p></li><li><p>The preceding data contains numeric as well as categorical data. The data resides in HDFS. Here is the code which loads the data and obtains a summary on the numeric fields:</p><pre class="programlisting">       import org.apache.spark._
       import org.apache.spark.sql._
       import org.apache.spark.sql.functions._
       import org.apache.spark.sql.DataFrameNaFunctions
       import org.apache.spark.sql.types._
       import org.apache.spark.mllib.linalg.Vectors
       import org.apache.spark.mllib.stat.
       {MultivariateStatisticalSummary, Statistics}

       object MissingValue_Treatment {
       def main(args:Array[String]): Unit = {
           val conf = new SparkConf()
             .setMaster("spark://master:7077")
             .setAppName("MissingValue_Treatment")
           val sc = new SparkContext(conf)
           val sqlContext = new SQLContext(sc)
           import sqlContext.implicits._
           //Loading data
           val loan_Data = sqlContext.read.format
    ("com.databricks.spark.csv")
             .option("header", "true")
             .option("inferSchema", "true")
    .load("hdfs://namenode:9000/Loan_Prediction_Data.csv")
       val summary = loan_Data.describe()
       summary.show() }}
</pre><p>The following is the output:</p><pre class="programlisting">+-------+-----------------+------------------+------------------+
|summary|  ApplicantIncome| CoapplicantIncome|        LoanAmount|
+-------+-----------------+------------------+------------------+
|  count|              614|               614|               592|
|   mean|5403.459283387622|1621.2457980271008|146.41216216216216|
| stddev|6109.041673387179|2926.2483692241885| 85.58732523570544|
|    min|              150|               0.0|                 9|
|    max|            81000|           41667.0|               700|
+-------+-----------------+------------------+------------------+-

+-----------------+-------------------+
| Loan_Amount_Term| Credit_History    |
+-----------------+-------------------+
|    600          |   564             |
|342.0            |0.8421985815602837 |
|65.12040985461256|0.3648783192364048 |
|12               |0                  |
|480              |1                  |
+-----------------+-------------------+

</pre></li><li><p>From the preceding results, here are the few inferences:</p><pre class="programlisting">    LoanAmount has (614-592) = 22 missing values
    Loan_Amount_Term has (614-600) = 14 missing values
    Credit_History has (614 - 564) 50 missing values.
</pre><p>The <code class="literal">ApplicantIncome</code> and <code class="literal">CoapplicantIncome</code> distribution seems to be as per the expectation.</p></li><li><p>The rows containing missing values in the specified columns can be deleted as follows:</p><pre class="programlisting">        val newDf_afterDroppedRows =
        loan_Data.na.drop(Seq("LoanAmount",
        "Loan_Amount_Term", "Credit_History"))
        println("Total Rows Count after Deleting null value records:
        "+newDf_afterDroppedRows.count())

</pre><p>The following is the output:</p><pre class="programlisting">    Total Rows Count after Deleting null value records: 529
</pre></li><li><p>The missing values in columns can be replaced with a specified value as follows:</p><pre class="programlisting">        val schemaString =
          "Loan_ID,Gender,Married,Dependents,Education,
           Self_Employed,ApplicantIncome,
       CoapplicantIncome,LoanAmount,Loan_Amount_Term,
          Credit_History,Property_Area,Loan_Status"
       val schema = schemaString.split(",").map{
         field =&gt;
       if(field == "ApplicantIncome" || field == "CoapplicantIncome" ||
       field == "LoanAmount" || field == "Loan_Amount_Term" || field ==
       "Credit_History")
          StructField(field, DoubleType)
           else
          StructField(field, StringType)
         }
      val schema_Applied = StructType(schema)
      val  loan_Data =
      sqlContext.read.format("com.databricks.spark.csv")
            .option("header","true")

         .schema(schema_Applied).load("hdfs://namenode:9000/
          loan_prediction_data.csv")

       /* Fill missing values (null or NaN) with a
          specific value for  all columns */
         val filledWith_half = loan_Data.na.fill(0.5)

      /* Fill missing values (null or NaN) with a specific
       value for certain columns */
       val filledWith_halfFewColumns = loan_Data.na.fill(0.5,
       Seq("Credit_History"))

      /* Fill missing values of each column with specified value */
         val fill_FewColumns = loan_Data.na.fill(
           Map(
             "ApplicantIncome" -&gt; 1000.0,
             "LoanAmount" -&gt; 500.0,
                     "Credit_History" -&gt; 0.5
           ) ) } }
</pre></li><li><p>The missing values can also be imputed with the measures of central tendency. Here is the code which inputs the mean into the missing values:</p><pre class="programlisting">
        val df_CreditHistoryNull =
             loan_Data.filter(loan_Data("Credit_History").isNull)
        println("Missing rows for Credit_History:
          "+df_CreditHistoryNull.count)
      val mean_CreditHist = loan_Data.select(mean("Credit_History"))
         .first()(0).asInstanceOf[Double]
      val fill_MissingValues_CrediHist =
          loan_Data.na.fill(mean_CreditHist,Seq("Credit_History"))


      println("After replacing the mean for Credit History..."+
      fill_MissingValues_CrediHist.filter(fill_MissingValues_CrediHist
      ("Credit_History").isNull).count)

</pre><p>The following is the output:</p><pre class="programlisting">       Missing rows for Credit_History: 50
      After replacing the mean for Credit History...0
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec141"></a>How it worksâ€¦</h3></div></div></div><p>Initially, we imported the data into Spark and the summary on the data reveals that the field <code class="literal">LoanAmount</code> has 22 missing values, <code class="literal">Loan_Amount_Term</code> has 14 missing values, and <code class="literal">Credit_History</code> has 50 missing values. The <code class="literal">loan_Data.na.fill(0.5)</code> line replaces the <code class="literal">null</code> or <code class="literal">NaN</code> values in all columns with <code class="literal">0.5</code>. The next line <code class="literal">loan_Data.na.fill(0.5, Seq("Credit_History"))</code> replaces <code class="literal">null</code> or <code class="literal">NaN</code> values in the <code class="literal">Credit_History</code> column with <code class="literal">0.5</code>. Similarly, the <code class="literal">loan_Data.na.fill(</code><code class="literal">Map("ApplicantIncome" -&gt; 1000.0,"LoanAmount" -&gt; 500.0,"Credit_History" -&gt; 0.5)</code><code class="literal">)</code> line takes a map of key-value pairs where the key is the column name and the value contains either <code class="literal">int</code>, <code class="literal">float</code>, <code class="literal">string</code>, or <code class="literal">double</code> type data that needs to replace the <code class="literal">null</code> or <code class="literal">NaN</code> entries.</p><p>Next, the missing values for <code class="literal">Credit_History</code> are replaced with the mean of the remaining values. The <code class="literal">loan_Data.select(mean("Credit_History")).first()(0).asInstanceOf[Double]</code> line generates the mean of the <code class="literal">Credit_History</code> column. The <code class="literal">loan_Data.na.fill(mean_CreditHist,Seq("Credit_History"))</code> line replaces the missing values with the calculated mean.</p><p>The missing values can be treated in several ways. The first approach is to delete observations where any of the variables is missing. This we term it as listwise deletion. The second approach is pairwise deletion, where we perform analysis with all cases in which the variables of interest are present. This approach keeps as many cases available for analysis but uses a different sample size for different variables.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec142"></a>There's moreâ€¦</h3></div></div></div><p>In the preceding discussion of treating missing values, if we remove the observations where any of the variables is missing with the likewise deletion approach, it reduces the power of the model because it reduces the sample size. Similarly, the pairwise deletion approach uses a different sample size for different variables, which also affects the model output. The observations are deleted when the nature of the missing data is <span class="emphasis"><em>missing completely at random</em></span>.</p><p>The other way of dealing with missing values is imputation, which is a method to fill in the missing values with estimated ones. Mean/mode/median imputation is one of the most frequently used methods, which replaces the missing data for a given attribute by the mean or median (quantitative attribute) or mode (qualitative attribute) of all known values of that variable.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec143"></a>See also</h3></div></div></div><p>Please refer <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Big Data Analytics with Spark</em></span> to get familiar with Spark. Also visit the earlier <span class="emphasis"><em>Univariate analysis</em></span> and <span class="emphasis"><em>Bivariate analysis</em></span> recipes to understand the types of analysis on a single variable and two variables.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec38"></a>Outlier detection</h2></div></div><hr /></div><p>Outliers are infrequent observations, that is, the data points that do not appear to follow the characteristic distribution of the rest of the data. They appear far away and diverge from the overall pattern of the data. These might occur due to measurement errors or other anomalies which result in wrong estimations. Outliers can be univariate and multivariate. Univariate outliers can be determined by looking at the distribution of a single variable whereas multivariate outliers are present in an <span class="emphasis"><em>n</em></span>-dimensional space which can be found by looking at the distributions in multi-dimensions.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec144"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark cluster in any one of the modes, that is, local, standalone, YARN, or Mesos. For installing Spark on a standalone cluster, please refer <a class="ulink" href="http://spark.apache.org/docs/latest/spark-standalone.html" target="_blank">http://spark.apache.org/docs/latest/spark-standalone.html</a>. Also, include the Spark MLlib package in the <code class="literal">build.sbt</code> file so that it downloads the related libraries and the API can be used. Install Hadoop (optionally), Scala, and Java.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec145"></a>How to do itâ€¦</h3></div></div></div><p>We'll see how to detect outliers using the following code snippets:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Let's take the <code class="literal">titanic</code> dataset. Please download the data from the following location: <a class="ulink" href="https://github.com/ChitturiPadma/datasets/blob/master/titanic_data.csv" target="_blank">https://github.com/ChitturiPadma/datasets/blob/master/titanic_data.csv</a>.</p><p>The data contains numeric and categorical fields. Let's try to load the dataset and see the summary statistics on the dataset. Here is the code for the same:</p><pre class="programlisting">       import org.apache.spark._
       import org.apache.spark.sql._
       import org.apache.spark.sql.functions._
       import org.apache.spark.sql.DataFrameNaFunctions
       import org.apache.spark.sql.types._
       object Outlier_Detection {
         def main(args:Array[String]): Unit = {
           val conf = new SparkConf()
             .setMaster("spark://master:7077")
             .setAppName("Outlier_Detection")
            val sc = new SparkContext(conf)
            val sqlContext = new SQLContext(sc)
           import sqlContext.implicits._
       //Loading data
      val titanic_data =
      sqlContext.read.format("com.databricks.spark.csv")
          .option("header", "true")
          .option("inferSchema","true")
          .load("hdfs://namenode:9000/titanic_data.csv")
        val summary = titanic_data.describe()
        summary.show() } }

</pre><p>The following is the output:</p><pre class="programlisting">+-------+-----------------+-------------------+------------------
|summary|      PassengerId|           Survived|            Pclass|
+-------+-----------------+-------------------+------------------
|  count|              891|                891|               891|
|   mean|            446.0| 0.3838383838383838| 2.308641975308642|
| stddev|257.3538420152301|0.48659245426485737|0.8360712409770491|
|    min|                1|                  0|                 1|
|    max|              891|                  1|                 3|
+-------+-----------------+-------------------+------------------+
+------------------+------------------+------------------+
|SibSp             |      Parch        |           Fare   |
+------------------+-------------------+------------------+
|891               |891                |891               |
|0.5230078563411896|0.38159371492704824|32.204207968574615|
|1.102743432293432 |0.8060572211299486 |49.6934285971809  |
|0                 |0                  |0.0               |
|8                 |6                  |512.3292          |
+------------------+-------------------+------------------+
+------------------+
|Age               |
+------------------+
|714               |
|29.69911764705882 |
|14.526497332334039|
|0.42              |
|80.0              |
+------------------+

</pre></li><li><p>From the preceding values of the variable <code class="literal">Fare</code>, we observe that the mean of <code class="literal">891</code> values is <code class="literal">32.204</code>, whereas the maximum value is <code class="literal">512.3292</code>, which has huge variation from the mean and it potentially looks like an outlier. The minimum value is 0 and maximum value is <code class="literal">512.292</code>, which indicates that the range is <code class="literal">512.292</code>. Now, let's try to analyze the distribution of <code class="literal">Fare</code> values across multiple intervals:</p><pre class="programlisting">
       val fareVaues_AtDiffr_Itervals =
       scala.collection.mutable.ListBuffer[Long]()
       val  minValue = 0.0
       val maxValue = 513
       val bins = 5
       val range = (maxValue - minValue)/5.0
       var minCounter = minValue
       var maxCounter = range
       while(minCounter &lt; maxValue)
         {
           val valuesBetweenRange =
              titanic_data.filter(titanic_data("Fare").between
              (minCounter,maxCounter))
            fareVaues_AtDiffr_Itervals.+=(valuesBetweenRange.count())
           minCounter = maxCounter
           maxCounter = maxCounter+range
         }
       println("Fare Values at Different Ranges:")
       fareVaues_AtDiffr_Itervals.foreach(println)
</pre><p>The following is the output:</p><pre class="programlisting">   Fare Values at Different Ranges:
   838
   33
   17
   0
   3
</pre></li><li><p>The preceding output shows that out of <code class="literal">891</code>, <code class="literal">838</code> values, that is, 94% of the <code class="literal">Fare</code> values, are in the range of <code class="literal">[0 - 102.6]</code> and only three values, that is, 0.3% of them, are in the range of <code class="literal">[410.4 - 513]</code>. By assuming that the observations obey normal distribution, the data points which fall above <code class="literal">mean+2(stddev)</code> and below <code class="literal">mean - 2(stddev)</code> are considered as outliers. The constant can be either 2 or 3 depending on the extent to which values are distributed. Here is the code to detect outliers:</p><pre class="programlisting">        val meanFare = titanic_data.select(mean("Fare"))
            .first()(0).asInstanceOf[Double]
        val stddevFare = titanic_data.select(stddev("Fare"))
            .first()(0).asInstanceOf[Double]
        val upper_threshold = meanFare + 2*stddevFare
        val lower_threshold = meanFare - 2*stddevFare
        val fareValues_MoreThanUpperthrshold =
            titanic_data.select("Fare").filter(titanic_data("Fare")
                  &gt; upper_threshold)
         val fareValues_LessThanLowerthrshold =
            titanic_data.select("Fare").filter(titanic_data("Fare") &lt;
               lower_threshold)
         val summary_FareValuesMoreThanUppr =
            fareValues_MoreThanUpperthrshold.describe()
         println("Summary Of Fare Values Greater Than Upper
                  Threshold:")
         summary_FareValuesMoreThanUppr.show()

         val summary_FareValuesLessThanLowr =
           fareValues_LessThanLowerthrshold.describe()
         println("Summary Of Fare Values Less Than Lower Threshold:")
         summary_FareValuesLessThanLowr.show()
</pre><p>The following is the output:</p><pre class="programlisting">  Summary Of Fare Values Greater Than Upper Threshold:
  +-------+------------------+
  |summary|              Fare|
  +-------+------------------+
  |  count|                38|
  |   mean|216.28245526315789|
  | stddev| 99.81881725000063|
  |    min|            133.65|
  |    max|          512.3292|
  +-------+------------------+

  Summary Of Fare Values Less Than Lower Threshold:
  +-------+----+
  |summary|Fare|
  +-------+----+
  |  count|   0|
  |   mean|null|
  | stddev|null|
  |    min|null|
  |    max|null|
  +-------+----+
</pre></li><li><p>Let's take the upper threshold and lower threshold as <code class="literal">mean+3 (stddev)</code> and <code class="literal">mean-3(stddev)</code> and see the how the outliers are detected:</p><pre class="programlisting">        val upper_threshold1 = meanFare + 3*stddevFare
        val lower_threshold1 = meanFare - 3*stddevFare
        val fareValues_MoreThanUpperthrshold1 =
           titanic_data.select("Fare").filter(titanic_data("Fare") &gt;
           upper_threshold1)
        val fareValues_LessThanLowerthrshold1 =
           titanic_data.select("Fare").filter(titanic_data("Fare") &lt;
                      lower_threshold1)
        val summary_FareValuesMoreThanUppr1 =
            fareValues_MoreThanUpperthrshold1.describe()
         println("Summary Of Fare Values Greater Than Upper
                 Threshold:")
        summary_FareValuesMoreThanUppr1.show()

         val summary_FareValuesLessThanLowr1 =
         fareValues_LessThanLowerthrshold1.describe()
         println("Summary Of Fare Values Less Than Lower Threshold:")
         summary_FareValuesLessThanLowr1.show()
</pre><p>The following is the output:</p><pre class="programlisting">  Summary Of Fare Values Greater Than Upper Threshold:
  +-------+------------------+
  |summary|              Fare|
  +-------+------------------+
  |  count|                20|
  |   mean|        279.308545|
  | stddev|102.35339148065432|
  |    min|          211.3375|
  |    max|          512.3292|
  +-------+------------------+

  Summary Of Fare Values Less Than Lower Threshold:
  +-------+----+
  |summary|Fare|
  +-------+----+
  |  count|   0|
  |   mean|null|
  | stddev|null|
  |    min|null|
  |    max|null|
  +-------+----+
</pre></li><li><p>We can also standardize the values by calculating z-scores as <code class="literal">(x-mean)/stddev</code> which force fits standard normal distribution and can apply the same technique. Here is the code:</p><pre class="programlisting">       val titanic_Data_StdFareValues =
         titanic_data.withColumn("StdFareValue", (titanic_data("Fare")-
         meanFare)/stddevFare)
       val mean_FareStdvalue =
         titanic_Data_StdFareValues.select(mean("StdFareValue"))
        .first()(0).asInstanceOf[Double]
       val stddev_FareStdvalue =
         titanic_Data_StdFareValues.select(stddev("StdFareValue")
         ).first()(0).asInstanceOf[Double]
       val upper_threshold_std = mean_FareStdvalue +
         3*stddev_FareStdvalue
       val lower_threshold_std = mean_FareStdvalue -
         3*stddev_FareStdvalue
       val fareValues_MoreThanUpperthrshold_std =
         titanic_Data_StdFareValues.select("StdFareValue")
        .filter(titanic_Data_StdFareValues("StdFareValue") &gt;
         upper_threshold_std)
       val fareValues_LessThanLowerthrshold_std =
         titanic_Data_StdFareValues.select("StdFareValue")
         .filter(titanic_Data_StdFareValues("StdFareValue") &lt;
          lower_threshold_std)

       val summary_FareValuesMoreThanUppr_Std =
          fareValues_MoreThanUpperthrshold_std.describe()
          println("Summary Of Standardized Fare Values Greater Than
          Upper Threshold")
       summary_FareValuesMoreThanUppr_Std.show()

       val summary_FareValuesLessThanLowr_Std =
           fareValues_LessThanLowerthrshold_std.describe()
       println("Summary Of Standardized Fare Values Less Than Lower
           Threshold")
       summary_FareValuesLessThanLowr_Std.show()
</pre><p>The following is the output:</p><pre class="programlisting">
  Summary Of Fare Values Greater Than Upper Threshold:
  +-------+------------------+
  |summary|      StdFareValue|
  +-------+------------------+
  |  count|                20|
  |   mean| 4.972575730977911|
  | stddev|2.0596967118195746|
  |    min| 3.604768217614745|
  |    max| 9.661740104981664|
  +-------+------------------+

  Summary Of Fare Values Less Than Lower Threshold:
  +-------+------------+
  |summary|StdFareValue|
  +-------+------------+
  |  count|           0|
  |   mean|        null|
  | stddev|        null|
  |    min|        null|
  |    max|        null|
  +-------+------------+
</pre></li><li><p>Here is another statistical technique, <span class="strong"><strong>median absolute deviation</strong></span> (<span class="strong"><strong>MAD</strong></span>), which is a robust measure of variability. We use the median calculated for the variable <code class="literal">Fare</code> in the <span class="emphasis"><em>Univariate analysis</em></span> recipe, which is <code class="literal">14.4542</code>. Here is the code for MAD:</p><pre class="programlisting">
       def main(args:Array[String]): Unit = {

       val sqlFunc = udf(coder)
       val fare_Details_WithAbsDeviations =
         fare_Details_Df.withColumn("AbsDev_FromMedian",
         sqlFunc(col("Fare"), lit(median_Fare)))
       val fare_AbsDevs_Rdd = fare_Details_WithAbsDeviations.map{row =&gt;
          row.getDouble(1)}
       val count = fare_AbsDevs_Rdd.count()
       val sortedFareAbsDev_Rdd = fare_AbsDevs_Rdd.sortBy(fareAbsVal =&gt;
          fareAbsVal )
       val sortedFare_AbsDevRdd_WithIndex =
          sortedFareAbsDev_Rdd.zipWithIndex()
       val median_AbsFareDevs = if(count%2 ==1)
         sortedFare_AbsDevRdd_WithIndex.filter{case(fareAbsVal:Double,
         index:Long) =&gt;    index == (count-1)/2}.first._1
       else{
       val elementAtFirstIndex =
          sortedFare_AbsDevRdd_WithIndex.filter{case(fareAbsVal:Double,
          index:Long) =&gt;    index == (count/2)-1}.first._1
       val elementAtSecondIndex =
          sortedFare_AbsDevRdd_WithIndex.filter{case(fareAbsVal:Double,
          index:Long) =&gt;    index == (count/2)}.first._1
          (elementAtFirstIndex+elementAtSecondIndex)/2.0
       }
       val mad = 1.4826*median_AbsFareDevs
       println("Median Absolute Deviation is:"+mad)}

        //UDF Code
       val coder= (fareValue:Double, medianValue:Double) =&gt;
       if((fareValue-medianValue)    &lt; 0) -1*(fareValue-medianValue)
       else (fareValue-medianValue)

</pre><p>The following is the output:</p><pre class="programlisting">   Median Absolute Deviation is:10.23616692
</pre></li><li><p>As seen in the earlier case, the outliers lie preceding <code class="literal">median+3(mad)</code> and followingÂ <code class="literal">median-3(mad)</code>. Here is the code for the same:</p><pre class="programlisting">       val upper_mad = median_Fare + 3 * mad
       val lower_mad = median_Fare - 3 * mad
       val fareValues_MoreThanUpperthrshold_mad=
         titanic_data.select("Fare").filter(titanic_data("Fare") &gt;
         upper_mad)
       val fareValues_LessThanLowerthrshold_mad =
         titanic_data.select("Fare").filter(titanic_data("Fare") &lt;
         lower_mad)

       val summary_FareValuesMoreThanUppr_MAD =
         fareValues_MoreThanUpperthrshold_mad.describe()
         println("Summary Of Fare Values Greater Than Upper Threshold
         In MAD Approach:")
       summary_FareValuesMoreThanUppr_MAD.show()
       val summary_FareValuesLessThanLowr_MAD =
       fareValues_LessThanLowerthrshold_mad.describe()
       println("Summary Of Fare Values Less Than Lower Threshold In MAD
         Approach:")
       summary_FareValuesLessThanLowr_MAD.show()

</pre><p>The following is the output:</p><pre class="programlisting">  Summary Of Fare Values Greater Than Upper Threshold In MAD Approach:
  +-------+------------------+
  |summary|              Fare|
  +-------+------------------+
  |  count|               171|
  |   mean|104.41825029239766|
  | stddev| 77.85677558392068|
  |    min|              46.9|
  |    max|          512.3292|
  +-------+------------------+

  Summary Of Fare Values Less Than Lower Threshold In MAD Approach:
  +-------+----+
  |summary|Fare|
  +-------+----+
  |  count|   0|
  |   mean|null|
  | stddev|null|
  |    min|null|
  |    max|null|
  +-------+----+
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec146"></a>How it worksâ€¦</h3></div></div></div><p>Initially, we imported the <code class="literal">titanic</code> dataset into Spark and the summary of the data for the variable <code class="literal">Fare</code> reveals that there are no missing values. Also, we notice that the mean of the values is <code class="literal">32.204</code>, whereas the maximum value is <code class="literal">512.3292</code>, indicating the existence of potential outliers. Next, from the minimum value of <code class="literal">0.0</code> to the maximum value of <code class="literal">512.3292</code>, we divided the data into five bins and analyzed the frequency of <code class="literal">Fare</code> values across five intervals, that is, <code class="literal">[0-102.6]</code>, <code class="literal">[102.6-205.2]</code>, <code class="literal">[205.2-307.8]</code>, <code class="literal">[307.8-410.4]</code>, and <code class="literal">[410.4-513]</code>.</p><p>This is done by running the loop as long as the condition <code class="literal">minCounter &lt; maxValue</code> is satisfied and values between <code class="literal">minCounter</code> and <code class="literal">maxCounter</code> are picked with the statement <code class="literal">titanic_data.filter(titanic_data("Fare").between(minCounter,maxCounter))</code>. The output indicates that 94% of the values are in the range <code class="literal">[0-102.6]</code> and 3% of the values in the interval <code class="literal">[410.4-513]</code>, which increases the confidence for the existence of outliers in the extreme interval, that is, <code class="literal">[410.4 - 513]</code>.</p><p>If a random variable <code class="literal">X</code> obeys normal distribution then 68% of the values lie within mean plus or minus <code class="literal">stddev</code>, 95% of the values lie within mean plus or minus two times <code class="literal">stddev</code>, and 99% lie within mean plus or minus three times <code class="literal">stddev</code>. Hence the approach mean plus or minus three or two times <code class="literal">stddev</code> will detect the outliers. The <code class="literal">titanic_data.select(mean("Fare")).first()(0).asInstanceOf[Double]</code> statement calculates the mean of the <code class="literal">Fare</code> variable. The <code class="literal">titanic_data.select(stddev("Fare")).first()(0).asInstanceOf[Double]</code> line calculates the standard deviation of <code class="literal">Fare</code>. The upper threshold is calculated as <code class="literal">meanFare + 2*stddevFare</code> and the lower threshold as <code class="literal">meanFare - 2*stddevFare</code>. The values above the upper threshold and below the lower threshold are identified with the statements <code class="literal">titanic_data.select("Fare").filter(titanic_data("Fare") &gt; upper_threshold)</code> and <code class="literal">titanic_data.select("Fare").filter(titanic_data("Fare") &lt; lower_threshold)</code>. The output of values above the upper threshold specifies that there are a total of 38 values above the 95th percentile and the maximum value among them is <code class="literal">512.3292</code>. With the same approach, values above <code class="literal">meanFare + 3*stddevFare</code> and values below <code class="literal">meanFare-3*stddevFare</code> are identified. The other variation of the approach is to standardize the values, which is done with the <code class="literal">titanic_data.withColumn("StdFareValue", (titanic_data("Fare")-meanFare)/stddevFare)</code> statement and then the mean plus or minus three times <code class="literal">stddev</code> approach is applied on the standardized values.</p><p>Finally, the MAD technique is applied, which is based on median central tendency. The median of <code class="literal">Fare</code> values is <code class="literal">14.4542</code> (please refer the <span class="emphasis"><em>Univariate analysis</em></span> recipe for the calculation). The <code class="literal">fare_Details_Df.withColumn("AbsDev_FromMedian", sqlFunc(col("Fare"),lit(median_Fare)))</code> statement calculates absolute values for the observations deviating from the median. Next, the median of the absolute deviations is calculated with similar approach of calculating the median as seen in the <span class="emphasis"><em>Univariate analysis</em></span> recipe. Next, the MAD is calculated as <code class="literal">1.4836*(median of absolute deviations)</code>. The thresholds, median plus or minus three times MAD, are calculated to identify the outliers.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec147"></a>There's moreâ€¦</h3></div></div></div><p>In the preceding recipe, we saw statistical techniques for detecting outliers. Outliers can be well identified by visualizing the data, that is, looking at the scatter plot for a single variable or from the box plots. As Spark doesn't have native support for visualization, sample data can be taken and it can be visualized using Python or R.</p><p>Although both the statistical approaches, mean plus or minus three times <code class="literal">stddev</code> and median plus or minus three times MAD, determine the outliers, the MAD is robust and is not affected by the outliers. The central tendency mean and the measure of spread, that is, standard deviation, are strongly impacted by the outliers. In the case of the mean, if one of the observations is infinite, the mean of all values becomes infinite, whereas the median value remains unchanged. The median becomes absurd only when more than 50% of the observations are infinite. Also, MAD is totally immune to the sample size. MAD is multiplied by the content <code class="literal">1.4826</code>, if the distribution of the data is assumed to be normal. If another distribution is measured then the content is calculated as <code class="literal">1/Q(0.75)</code> where <code class="literal">Q(0.75)</code> is the <code class="literal">0.75</code> quantile of that underlying distribution.</p><p>The ways to deal with outliers are similar to the methods of missing values, such as deleting observations, transforming them, binning them, treating them as a separate group, imputing values, and other statistical methods. We can delete outlier values if they occur due to data entry error or data processing error, or outlier observations are very small in number. Transforming variables can also eliminate outliers. The natural log of a value reduces the variation caused by extreme values. Binning is also a form of variable transformation. Similar to the missing value treatment, we can also impute outliers. The measures of central tendency, mean, median, and mode, can be applied. Before imputing values, we should analyze whether it is a natural or artificial outlier. If it is artificial, we can go with imputing values. We can also use a statistical model to predict the values of outlier observation and after that we can impute it with predicted values.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec148"></a>See also</h3></div></div></div><p>Please refer the <span class="emphasis"><em>Univariate analysis</em></span>, <span class="emphasis"><em>Bivariate analysis</em></span>Â and <span class="emphasis"><em>Missing value treatment</em></span> recipes to understand the types of analysis that can be performed on a single variable and two variables and also various ways of imputing missing values.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec39"></a>Use case - analyzing the MovieLens dataset</h2></div></div><hr /></div><p>In the previous recipes, we saw various steps of performing data analysis. In this recipe, let's download the commonly used dataset for movie recommendations. The dataset is known as the <code class="literal">MovieLens</code> dataset. The dataset is quite applicable for recommender systems as well as potentially for other machine learning tasks.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec149"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark cluster in any one of the modes, that is, local, standalone, YARN, or Mesos. For installing Spark on a standalone cluster, please refer toÂ <a class="ulink" href="http://spark.apache.org/docs/latest/spark-standalone.html" target="_blank">http://spark.apache.org/docs/latest/spark-standalone.html</a>. Also, include the SparkÂ MLlib package in the <code class="literal">build.sbt</code> file so that it downloads the related libraries and the API can be used. Install Hadoop (optionally), Scala, and Java.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec150"></a>How to do itâ€¦</h3></div></div></div><p>Let's see how to analyse the MovieLens dataset.</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Let's download the <code class="literal">MovieLens</code> dataset from the following location:Â <a class="ulink" href="https://drive.google.com/file/d/0Bxr27gVaXO5sRUZnMjBQR0lqNDA/view?usp=sharing" target="_blank">https://drive.google.com/file/d/0Bxr27gVaXO5sRUZnMjBQR0lqNDA/view?usp=sharing</a>.</p><p>The dataset contains 100,000 data points related to ratings given by a set of users to a set of movies. It also contains movie metadata and user profiles.</p></li><li><p>Once you have downloaded the data, unzip it using your terminal as follows:</p><pre class="programlisting">       &gt;unzip ml-100k.zip
       inflating: ml-100k/allbut.pl
       inflating: ml-100k/mku.sh
       inflating: ml-100k/README
       ...
       inflating: ml-100k/ub.base
       inflating: ml-100k/ub.test
</pre></li><li><p>This will create a directory called <code class="literal">ml-100k</code>. Change into this directory and examine the contents. The important files are <code class="literal">u.user</code> (user profiles), <code class="literal">u.item</code> (movie metadata), and <code class="literal">u.data</code> (the ratings given by users to movies).</p></li><li><p>The <code class="literal">README</code> file contains more information on the dataset, including the variables present in each data file. We can use the <code class="literal">head</code> command to examine the contents of the various files. For example, we can see that the <code class="literal">u.user</code> file contains the user ID, age , gender , occupation , and ZIP code fields, separated by a pipe ( <code class="literal">|</code> character) as follows:</p><pre class="programlisting">    &gt;head -5 u.user
       1|24|M|technician|85711
       2|53|F|other|94043
       3|23|M|writer|32067
       4|24|M|technician|43537
       5|33|F|other|15213
</pre></li><li><p>The <code class="literal">u.item</code> file contains the movie ID, title, release data, and IMDB link fields and a set of fields related to movie category data. It is also separated by a character as follows:</p><pre class="programlisting">      &gt;head -5 u.item
      1|Toy Story (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?
      Toy%20
      Story%20(1995)|0|0|0|1|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0
      2|GoldenEye (1995)|01-Jan-1995||http://us.imdb.com/M/title-
      exact?GoldenEye%20(1995)|0|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0|1|0|0
      3|Four Rooms (1995)|01-Jan-1995||http://us.imdb.com/M/title-
      exact?Four%20Rooms%20(1995)|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|1|0|0
      4|Get Shorty (1995)|01-Jan-1995||http://us.imdb.com/M/title-
      exact?Get%20Shorty%20(1995)|0|1|0|0|0|1|0|0|1|0|0|0|0|0|0|0|0|0|0
      5|Copycat (1995)|01-Jan-1995||http://us.imdb.com/M/title-
      exact?Copycat%20(1995)|0|0|0|0|0|0|1|0|1|0|0|0|0|0|0|0|1|0|0

</pre></li><li><p>Finally, the <code class="literal">u.data</code> file contains the user ID , movie ID , rating (1-5 scale) , and timestamp fields is separated by a tab (the <code class="literal">\t</code> character) as follows:</p><pre class="programlisting">      &gt;head -5 u.data
          196   242   3    881250949
          186   302   3    891717742
          22    377   1    878887116
          244   51    2    880606923
          166   346   1    886397596
</pre></li><li><p>Let's try to load the <code class="literal">u.data</code> file. Here is the code for the same:</p><pre class="programlisting">      import org.apache.spark._
      import org.apache.spark.sql._
      import org.apache.spark.sql.functions._
      import org.apache.spark.sql.DataFrameNaFunctions
      import org.apache.spark.sql.types._

      object MovieLens_DataAnalysis {
      case class MovieLens(userId:String, movieId:String, rating:Int,
      timestamp:Long)
      def main(args:Array[String]): Unit = {
        val conf = new SparkConf()
          .setMaster("spark://master:7077")
          .setAppName("MovieLens_DataAnalysis")
        val sc = new SparkContext(conf)
        val sqlContext = new SQLContext(sc)
        import sqlContext.implicits._
       //Loading data
        val movieLens_data = sc.textFile("hdfs://namenode:9000/ml-
        100k/u.data")
        val movieLens_DataStructured = movieLens_data.map{record =&gt; val
      fields = record.split("\t")
            val userId = fields(0)
            val movieId = fields(1)
            val rating = fields(2).toInt
            val timestamp = fields(3).toLong
            MovieLens(userId, movieId, rating, timestamp)
          }
        val movieLens_Df=
       sqlContext.createDataFrame(movieLens_DataStructured)
       pspspps
       println("Movie Lens data:")
        movieLens_Df.show(10)
        }
       }
</pre><p>The following is the output:</p><pre class="programlisting">
  Movie Lens data:
  +------+-------+------+---------+
  |userId|movieId|rating|timestamp|
  +------+-------+------+---------+
  |   196|    242|     3|881250949|
  |   186|    302|     3|891717742|
  |    22|    377|     1|878887116|
  |   244|     51|     2|880606923|
  |   166|    346|     1|886397596|
  |   298|    474|     4|884182806|
  |   115|    265|     2|881171488|
  |   253|    465|     5|891628467|
  |   305|    451|     3|886324817|
  |     6|     86|     3|883603013|
  +------+-------+------+---------+
</pre></li><li><p>Now let's try to calculate the number of movies that are rated on a scale of 1 to 5. Here is the code:</p><pre class="programlisting">       val ratings_Count = movieLens_Df.groupBy("rating").count()
         println("Ratings Count:")
       ratings_Count.show()
</pre><p>The following is the output:</p><pre class="programlisting">
  Ratings Count:
  +------+-----+
  |rating|count|
  +------+-----+
  |     1| 6110|
  |     2|11370|
  |     3|27145|
  |     4|34174|
  |     5|21201|
  +------+-----+

</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec151"></a>How it worksâ€¦</h3></div></div></div><p>Initially, the dataset is downloaded and all the files are extracted. The files such as <code class="literal">u.user</code>, <code class="literal">u.item</code>Â and <code class="literal">u.data</code> are identified in the dataset. The <code class="literal">u.user</code> file contains user details, <code class="literal">u.item</code> contains movie details, and <code class="literal">u.data</code> contains ratings information for movies from different users. Since the data is not in structured format, it is loaded into the RDD using the <code class="literal">sc.textFile("hdfs://namenode:9000/ml-100k/u.data")</code> statement. After loading the data, each record is split on the tab space <code class="literal">\t</code> as <code class="literal">val fields = record.split("\t")</code>. The individual fields are extracted as <code class="literal">val userId = fields(0)</code>, <code class="literal">val movieId = fields(1)</code> and so on, and <code class="literal">movieLens_DataStructured</code>, which is of type <code class="literal">RDD[MovieLens]</code>, is created. Now, the <code class="literal">movieLens_Df</code> DataFrame is created from the RDD with the <code class="literal">sqlContext.createDataFrame (movieLens_DataStructured)</code> statement. The DataFrame is aggregated on the field <code class="literal">rating</code> and <code class="literal">count</code> is extracted as <code class="literal">val ratings_Count = movieLens_Df.groupBy("rating").count()</code>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec152"></a>There's moreâ€¦</h3></div></div></div><p>Similar to the above problem, in the subsequent recipes, we'll address some more problems on various datasets.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec153"></a>See also</h3></div></div></div><p>Please refer to the <span class="emphasis"><em>Univariate analysis</em></span>, <span class="emphasis"><em>Bivariate analysis</em></span>, <span class="emphasis"><em>Missing value treatment</em></span>, and <span class="emphasis"><em>Outlier detection</em></span> recipes to understand the types of analysis that can be performed on a single variable and two variables, various ways of imputing missing values and statistical techniques in detecting outliers.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec40"></a>Use case - analyzing the Uber dataset</h2></div></div><hr /></div><p>In the previous recipes, we saw various steps of performing data analysis. In this recipe, let's download the Uber dataset and try to solve some of the analytical questions that arise on such data.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec154"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark cluster in any one of the modes, that is, local, standalone, YARN, or Mesos. For installing Spark on a standalone cluster, please refer <a class="ulink" href="http://spark.apache.org/docs/latest/spark-standalone.html" target="_blank">http://spark.apache.org/docs/latest/spark-standalone.html</a>. Also, include the SparkÂ MLlib package in the <code class="literal">build.sbt</code> file so that it downloads the related libraries and the API can be used. Install Hadoop (optionally), Scala, and Java.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec155"></a>How to do itâ€¦</h3></div></div></div><p>In this section, let's see how to analyse the Uber dataset.</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Let's download the <code class="literal">Uber</code> dataset from the following location: <a class="ulink" href="https://github.com/ChitturiPadma/datasets/blob/master/uber.csv" target="_blank">https://github.com/ChitturiPadma/datasets/blob/master/uber.csv</a>.</p></li><li><p>The dataset contains four columns:Â <code class="literal">dispatching_base_number</code>, <code class="literal">date</code>, <code class="literal">active_vehicles</code>, and <code class="literal">trips</code>. Let's load the data and see what the records look like with the following code:</p><pre class="programlisting">    import org.apache.spark._
    import org.apache.spark.sql._
    import org.apache.spark.sql.functions._
    import org.apache.spark.sql.DataFrameNaFunctions
    import org.apache.spark.sql.types._
    import org.apache.spark.mllib.linalg.Vectors
    import org.apache.spark.mllib.stat.
    {MultivariateStatisticalSummary, Statistics}

    object Ubser_Dataset_Analysis {
      def main(args:Array[String]): Unit = {
        val conf = new SparkConf()
          .setMaster("spark://master:7077")
          .setAppName("Uber_Dataset_Analysis")
        val sc = new SparkContext(conf)
        val sqlContext = new SQLContext(sc)
        import sqlContext.implicits._
    val uber_data = sqlContext.read.format("com.databricks.spark.csv")
          .option("header", "true")
          .option("inferSchema","true").load
              ("hdfs://namenode:9000/uber.csv")
        println("Uber Data:")
        uber_data.show(5) } }
</pre><p>The following is the output:</p><pre class="programlisting">
   Uber Data:
  +-----------------------+--------+---------------+-----+
  |dispatching_base_number|    date|active_vehicles|trips|
  +-----------------------+--------+---------------+-----+
  |                 B02512|1/1/2015|            190| 1132|
  |                 B02765|1/1/2015|            225| 1765|
  |                 B02764|1/1/2015|           3427|29421|
  |                 B02682|1/1/2015|            945| 7679|
  |                 B02617|1/1/2015|           1228| 9537|
  +-----------------------+--------+---------------+-----+

</pre></li><li><p>Let's try to find the days on which each base has more trips. Here is the code:</p><pre class="programlisting">      val uberData_new = uber_data.withColumn("BaseNo_Date",
      concat($"dispatching_base_number", lit(":"), $"date"))

      val maxTrips_PerBaseAndDate =
      uberData_new.groupBy("BaseNo_Date").max("trips")
      maxTrips_PerBaseAndDate.show(10)

</pre><p>The following is the output:</p><pre class="programlisting">  +----------------+----------+
  |     BaseNo_Date|max(trips)|
  +----------------+----------+
  | B02617:1/9/2015|     13165|
  |B02512:1/15/2015|      1636|
  | B02512:1/5/2015|       984|
  |B02598:2/23/2015|      8943|
  |B02617:2/23/2015|     11720|
  |B02764:1/10/2015|     38864|
  |B02512:1/19/2015|      1025|
  | B02512:1/9/2015|      1560|
  |B02682:1/23/2015|     11767|
  |B02682:2/12/2015|     13786|
  +----------------+----------+

</pre></li><li><p>Now let's try to find the month on which each basement has more trips. Here is the code:</p><pre class="programlisting">        def main(args:Array[String]): Unit = {
       // Loading data
        .
        .
       // Find the month on which basement has more trips
       val sqlFunc1 = udf(coder1)
       val sqlFunc2 = udf(coder2)
       val uberdata_newMonthCol = uber_data.withColumn("month",
       sqlFunc1(col("date")))
       val uberData_ConcatBaseNo_Month =
       uberdata_newMonthCol.withColumn("BaseNo_Month",
       concat($"dispatching_base_number", lit(":"), $"month"))

       val sumTrips_PerBaseAndMonth =
       uberData_ConcatBaseNo_Month.groupBy("BaseNo_Month").sum("trips")
       val sumTrips_PerBaseMonth_new =
       sumTrips_PerBaseAndMonth.withColumn("BaseNo",
       sqlFunc2(col("BaseNo_Month")))
       val maxTrips_PerBaseMonth =
       sumTrips_PerBaseMonth_new.groupBy("BaseNo").max("sum(trips)")
        .withColumnRenamed("max(sum(trips))","MaxTrips_PerMonth")
       val maxTrips_Final =
       maxTrips_PerBaseMonth.join(sumTrips_PerBaseMonth_new,
       sumTrips_PerBaseMonth_new("BaseNo") ===
       maxTrips_PerBaseMonth("BaseNo") &amp;&amp;
       sumTrips_PerBaseMonth_new("sum(trips)") ===
       maxTrips_PerBaseMonth("MaxTrips_PerMonth")).select
       ("BaseNo_Month","MaxTrips_PerMonth")

       println("Maximum Trips per basement per month:")
       maxTrips_Final.show() }
     val coder1 = (dateValue:String) =&gt; {
     val format =   new java.text.SimpleDateFormat("MM/dd/yyyy")
     val formated_Date = format.parse(dateValue)
     formated_Date.getMonth()+1
   }
   val coder2 =(baseMonthConcat:String) =&gt; baseMonthConcat.split(":")
   (0)

</pre><p>The following is the output:</p><pre class="programlisting">Maximum Trips per basement per month

+----------------+----------+
|     baseNo_Date|max(trips)|
+----------------+----------+
| B02617:1/9/2015|     13165|
|B02512:1/15/2015|      1636|
| B02512:1/5/2015|       984|
|B02598:2/23/2015|      8943|
|B02617:2/23/2015|     11720|
|B02764:1/10/2015|     38864|
|B02512:1/19/2015|      1025|
| B02512:1/9/2015|      1560|
|B02682:1/23/2015|     11767|
|B02682:2/12/2015|     13786|
+----------------+----------+

</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec156"></a>How it worksâ€¦</h3></div></div></div><p>Initially, the dataset is loaded in CSV format and the schema is inferred using the <code class="literal">sqlContext.read.format("com.databricks.spark.csv").option("header", "true").option("inferSchema","true").load("hdfs://namenode:9000/uber.csv")</code> statement. A new column, <code class="literal">BaseNo_Date</code>, is created by concatenating the columns <code class="literal">dispatching_base_number</code> and <code class="literal">date</code>. The <code class="literal">uberData_new</code> DataFrame is aggregated on the field <code class="literal">BaseNo_Date</code> and the maximum of trips is determined using the <code class="literal">uberData_new.groupBy("BaseNo_Date").max("trips")</code> statement.</p><p>Next, to find the month on which each basement has more trips, a new column month is created with the <code class="literal">uber_data.withColumn("month", sqlFunc1(col("date")))</code> statement. The <code class="literal">uberdata_newMonthCol.withColumn("BaseNo_Month", concat($"dispatching_base_number", lit(":"), $"month"))</code> statement creates a new column <code class="literal">BaseNo_Month</code> by concatenating <code class="literal">dispatching_base_number</code> and <code class="literal">month</code>. Next, the <code class="literal">uberData_ConcatBaseNo_Month</code> DataFrame is aggregated on <code class="literal">BaseNo_Month</code> and sum of trips is calculated. The <code class="literal">sumTrips_PerBaseAndMonth</code> DataFrame contains aggregated trips across <code class="literal">dispatching_base_number</code> and <code class="literal">month</code>. The maximum trips for a particular <code class="literal">dispatching_base_number</code> and <code class="literal">month</code> is obtained with the <code class="literal">sumTrips_PerBaseMonth_new.groupBy("BaseNo").max("sum(trips)").withColumnRenamed("max(sum(trips))","MaxTrips_PerMonth")</code> statement. The Â <code class="literal">maxTrips_PerBaseMonth.join(sumTrips_PerBaseMonth_new, sumTrips_PerBaseMonth_new("BaseNo") === maxTrips_PerBaseMonth("BaseNo") &amp;&amp; sumTrips_PerBaseMonth_new("sum(trips)") === maxTrips_PerBaseMonth("MaxTrips_PerMonth")).select("BaseNo_Month", "MaxTrips_PerMonth")</code> final statement displays the <code class="literal">dispatching_base_number</code>, <code class="literal">month</code> and maximum trips.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec157"></a>There's moreâ€¦</h3></div></div></div><p>In the preceding recipe, we addressed two problem statements using the Spark DataFrame API. When we deal with real-world problems, there is a need for building model on the analyzed data, which is seen in the next chapter.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec158"></a>See also</h3></div></div></div><p>Please refer the <span class="emphasis"><em>Univariate analysis</em></span>, <span class="emphasis"><em>Bivariate analysis</em></span>, <span class="emphasis"><em>Missing value treatment</em></span>, and <span class="emphasis"><em>Outlier detection</em></span> recipes to understand the types of analysis that can be performed on a single variable and two variables, various ways of imputing missing values and statistical techniques in detecting outliers.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch04"></a>ChapterÂ 4.Â Clustering, Classification, and Regression</h2></div></div></div><p>In this chapter, we will cover the following recipes:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Introduction</p></li><li style="list-style-type: disc"><p>Applying regression analysis for sales data
</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Variable identification</p></li><li style="list-style-type: disc"><p>Data exploration</p></li><li style="list-style-type: disc"><p>Feature engineering</p></li><li style="list-style-type: disc"><p>Applying linear regression</p></li></ul></div></li><li style="list-style-type: disc"><p>Applying logistic regression on bank marketing data
</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Variable identification</p></li><li style="list-style-type: disc"><p>Data exploration</p></li><li style="list-style-type: disc"><p>Feature engineering</p></li><li style="list-style-type: disc"><p>Applying logistic regression</p></li></ul></div></li><li style="list-style-type: disc"><p>Real-time intrusion detection using streaming k-means
</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Variable identification</p></li><li style="list-style-type: disc"><p>Producer code generating real-time data</p></li><li style="list-style-type: disc"><p>Applying streaming k-means</p></li></ul></div></li></ul></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec41"></a>Introduction</h2></div></div><hr /></div><p>Machine learning is a field of study that gives computers the ability to learn without being explicitly programmed. Many successful applications of machine learning exist already, including systems that analyse past sales data to predict customer behavior, optimizing robot behavior so that a task can be completed using minimum resources and extracting knowledge from bio-informatics data. With the advent of big data, maintaining large collections of data is one thing, but extracting useful information from these collections is even more challenging. The ML system should be able to scale on high volumes of data, the accuracy of the models built would also have to be quite high as the training takes place on large data.</p><p>Big data and machine learning take place in three steps-collecting, analyzing, and predicting. For this purpose, the Spark ecosystem supports a wide range of workloads including batch applications, iterative algorithms, interactive queries, and stream processing. The Spark MLlib component offers a variety of scalable ML algorithms.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec42"></a>Supervised learning</h2></div></div><hr /></div><p>Supervised learning deals with training algorithms with labeled data, inputs for which the outcome or target variables are known. It then predicts the outcome/target with the trained model for unseen future data. For example, historical e-mail data will have individual e-mails marked as ham or spam; this data is then used for training a model that can predict future e-mails as ham or spam. Supervised learning problems can be broadly divided into two major areas; classification and regression. Classification deals with predicting categorical variables or classes; for example, whether an e-mail is ham or spam or whether a customer is going to renew a subscription or not in a post-paid telecom subscription. This target variable is discrete and has a predefined set of values.</p><p>Regression deals with a target variable, which is continuous. For example, when we need to predict house prices, the target variable price is continuous and doesn't have a predefined set of values. In order to solve a given problem of supervised learning, one has to perform the following steps:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Determine the objective</p></li><li style="list-style-type: disc"><p>Decide the training data</p></li><li style="list-style-type: disc"><p>Cleaning the training dataset</p></li><li style="list-style-type: disc"><p>Feature extraction</p></li><li style="list-style-type: disc"><p>Training the models</p></li><li style="list-style-type: disc"><p>Validation</p></li><li style="list-style-type: disc"><p>Evaluation of trained model</p></li></ul></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec43"></a>Unsupervised learning</h2></div></div><hr /></div><p>Unsupervised learning deals with unlabeled data. The objective is to observe the structure of data and find patterns. Tasks like cluster analysis, association rule mining, outlier detection, dimensionality reduction and so on can be modeled as unsupervised learning problems. As the tasks involved in unsupervised learning vary vastly, there is no single process outline that we can follow.</p><p>Cluster analysis is a subset of unsupervised learning that aims to create groups of similar items from a set of items. This analysis helps us identify interesting groups of objects that we are interested in. It could be items we encounter in day-to-day life such as movies or songs according to taste, or interests of users in terms of their demography or purchasing patterns.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec44"></a>Applying regression analysis for sales data</h2></div></div><hr /></div><p>Regression analysis is a type of predictive modeling technique which investigates the relationship between variables. This is widely used for forecasting, time series modeling and finding the effect of relationships between the variables. In this, we try to fit a curve/line for the data points such that the differences between the distances of data points from the curve or line is minimized. Linear regression is the mostly frequently used technique. In this technique, the dependent variable is continuous, independent variables can be continuous or discrete, and the nature of the regression line is linear. The equation which is used to predict the value of the target variable is based on the following predictor variables:</p><pre class="programlisting">y = aX +E
y = target variable
X = input variable
a = regression coefficient
E = the error term
</pre><p>Regression techniques are driven by three metrics-the number of independent variables, the type of dependent variables, and the shape of the regression line. We can also evaluate the model performance using the metric R-square. In this recipe, we'll see how to apply regression analysis on sales data at scale.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec45"></a>Variable identification</h2></div></div><hr /></div><p>In this recipe, we'll see how to identify the required variables for analysis and understand their description.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec159"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark cluster in any one of the modes, that is, local, standalone, YARN, or Mesos. For installing Spark on a standalone cluster, please refer to <a class="ulink" href="http://spark.apache.org/docs/latest/spark-standalone.html" target="_blank">http://spark.apache.org/docs/latest/spark-standalone.html</a>. Also, include the Spark MLlib package in the <code class="literal">build.sbt</code> file so that it downloads the related libraries and the API can be used. Install Hadoop (optionally), Scala, and Java.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec160"></a>How to do itâ€¦</h3></div></div></div><p>Let's look at an example of sales data. It contains 2013 sales data for nearly 1600 products across 10 stores in different cities. The data contains product and store attributes. We'll look at the properties of the different variables.</p><p>Please download the data from the following locations: <a class="ulink" href="https://github.com/ChitturiPadma/datasets/blob/master/Sales_Train.csv" target="_blank">https://github.com/ChitturiPadma/datasets/blob/master/Sales_Train.csv</a><a class="ulink" href="https://github.com/ChitturiPadma/datasets/blob/master/Sales_Test.csv" target="_blank">https://github.com/ChitturiPadma/datasets/blob/master/Sales_Test.csv</a></p><p>We have train and test datasets available. The data contains the following variables:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Variable</strong></span></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Description</strong></span></p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">Item_Identifier</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Unique product ID</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">Item_Weight</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Weight of the product</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">Item_Fat_Content</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Whether the product has low fat or not</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">Item_Visibility</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Percentage of total display area of all products in a store allocated to the particular product</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">Item_Type</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Category to which the product belongs</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">Item_MRP</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Maximum Retail Price of the product</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">Outlet_Identifier</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Unique store ID</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">Outlet_Establishement_Year</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Year in which store was established</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">Outlet_Size</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Size of the store in terms of ground area covered</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">Outlet_Location_Type</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Type of the city in which store is located.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">Outlet_Type</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>If the outlet is grocery or pharmacy</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; ">
<p><code class="literal">Item_Outlet_Sales</code></p>
</td><td style="">
<p>Sales of the product in a particular store. This is the outcome to be predicted.</p>
</td></tr></tbody></table></div><p>The data resides in HDFS. Let's load the data into Spark and see the values corresponding to different variables:</p><pre class="programlisting">  import org.apache.spark._
  import org.apache.spark.mllib.linalg.Vectors
  import org.apache.spark.sql._
  import org.apache.spark.sql.functions._
  import org.apache.spark.sql.DataFrameNaFunctions
  import org.apache.spark.sql.types._
  importorg.apache.spark.mllib.stat
  .{MultivariateStatisticalSummary, Statistics}

   object Sales_Prediction {
    def main(args:Array[String]): Unit ={
      val conf = new SparkConf()
        .setMaster("spark://master:7077")
        .setAppName("Sales_Prediction")
      val sc = new SparkContext(conf)
      val sqlContext = new SQLContext(sc)
      import sqlContext.implicits._

              //Loading data
      val sales_data_train =
      sqlContext.read.format("com.databricks.spark.csv")
      .option("header", "true")
      .option("inferSchema","true")
      .load("hdfs://namenode:9000/Sales_Train.csv")
      val sales_data_test =
      sqlContext.read.format("com.databricks.spark.csv")
      .option("header", "true")
      .option("inferSchema","true")
      .load("hdfs://namenode:9000/Sales_Test.csv")
      val sales_data_union = sales_data_train.unionAll(sales_data_test)
      val sales_data = sales_data_union.withColumn("Item_Outlet_Sales",
      sales_data_union.col("Item_Outlet_Sales").cast(DoubleType))
      sales_data.show(5) } }
</pre><p>The following is the output:</p><pre class="programlisting">+---------------+-----------+----------------------+---------------+
|Item_Identifier|Item_Weight|Item_Fat_Content|Item|Item_Visibility |
+---------------+-----------+----------------------+---------------+
| FDA15         | 9.3       | Low Fat              |0.016047301    |
| DRC01         | 5.92      | Regular              |0.019278216    |
| FDN15         | 17.5      | Low Fat              |0.016760075    |
| FDX07         | 19.2      | Regular              | 0.0           |
| NCD19         | 8.93      | Low Fat              | 0.0           |
+---------------+-----------+----------------------+---------------+
+------------+-----------------+------------------+
| Item_Type  |Item_Outlet_Sales| Outlet_Identifier|
+------------+-----------------+------------------+
| Dairy      | 249.8092        | OUT049           |
| Soft Drinks| 48.2692         | OUT018           |
| Meat       | 141.618         | OUT049           |
| Fruits and | 182.095         | OUT010           |
|Household   | 53.8614         | OUT013           |
+------------+-----------------+------------------+

+-------------------------+------------+----------------------+
|Outlet_Establishment_Year|Outlet_Size | Outlet_Location_Type |
+-------------------------+------------+----------------------+
| 1999                    | Medium     | Tier 1               |
| 2009                    | Medium     | Tier 3               |
| 1999                    | Medium     | Tier 1               |
| 1998                    |            | Tier 3               |
| 1987                    | High       | Tier 3               |
+-------------------------+------------+----------------------+
+-------------------+-----------------+
| Outlet_Type       |Item_Outlet_Sales|
+-------------------+-----------------+
| Supermarket Type1 | 3735.138        |
| Supermarket Type2 | 443.4228        |
| Supermarket Type1 | 2097.27         |
| Grocery Store     | 732.38          |
|Supermarket Type1  | 994.7052        |
+-------------------+-----------------+</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec161"></a>How it worksâ€¦</h3></div></div></div><p>In the previous code snippet, since the file is of CSV format, we imported the data initially into Spark using theÂ CSVÂ package. The schema is inferred and variables are identified. Next, the <code class="literal">sales_data_train.unionAll(sales_data_test)</code> statement merges the training and testing datasets. The <code class="literal">val sales_data = sales_data_union.withColumn("Item_Outlet_Sales",sales_data_union.col("Item_Outlet_Sales").cast(DoubleType))</code> statement converts the <code class="literal">Item_Outlet_Sales </code>to double attribute. The <code class="literal">sales_data.show(5)</code> line displays the first five records. From this sample data, the independent and dependent variables are identified.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec162"></a>There's moreâ€¦</h3></div></div></div><p>In the subsequent recipes, let's see how to explore data and apply feature engineering.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec163"></a>See also</h3></div></div></div><p>Please refer to the <span class="emphasis"><em>Data exploration</em></span> and <span class="emphasis"><em>Feature engineering</em></span> recipes for the subsequent steps.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec46"></a>Data exploration</h2></div></div><hr /></div><p>In this recipe, we'll see how to explore data.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec164"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark cluster in any one of the modes, that is, local, standalone, YARN, or Mesos. For installing Spark on a standalone cluster, please refer to <a class="ulink" href="http://spark.apache.org/docs/latest/spark-standalone.html" target="_blank">http://spark.apache.org/docs/latest/spark-standalone.html</a>. Also, include the Spark MLlib package in the <code class="literal">build.sbt</code> file so that it downloads the related libraries and the API can be used. Install Hadoop (optionally), Scala, and Java.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec165"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>After variable identification, let's try to do some data exploration and come up with inferences about the data. Here is the code which does data exploration:</p><pre class="programlisting">            // Basic Statistics for numerical variables
            val summary_stats = sales_data.describe()
            println("Summary Statistics")
            summary_stats.show()

            // Unique values in each field
            val columnNames = sales_data.columns
            val uniqueValues_PerField = columnNames.map{
            fieldName =&gt; fieldName + ":" +
            sales_data.select(fieldName).distinct().count.toString
             }
            uniqueValues_PerField.foreach(println)

             //Frequency of Categories for categorical variables
             val frequency_Variables  = columnNames.map{
             fieldName =&gt;
             if(fieldName == "Item_Fat_Content" || fieldName ==
             "Item_Type" || fieldName ==
             "Outlet_Size" || fieldName == "Outlet_Location_Type" ||
             fieldName == "Outlet_Type")
             Option(fieldName,sales_data.groupBy(fieldName).count())
             else  None
              }
             val seq_Df_WithFrequencyCount =
             frequency_Variables.filter(optionalDf =&gt;
             optionalDf!=None).map{optionalDf =&gt; optionalDf.get}
             seq_Df_WithFrequencyCount.foreach{case(fieldName, df) =&gt;
             println("Frequency Count of "+fieldName)
             df.show()
</pre><p>The following is the output:</p><pre class="programlisting"> Summary statistics:
+-------+-------------------+-------------------+------------------+
|summary| Item_Weight       | Item_Visibility   |Item_MRP          |
+-------+-------------------+-------------------+------------------+
|count  |11765              |14204              |14204             |
|mean   |12.792854228644092 |0.0659527800739932 |141.00497725992673|
|stddev |4.65250228641284   |0.05145859524842308|62.086938014764094|
|min    |4.555              |0.0                |31.29             |
|max    |21.35              |0.328390948        |266.8884          |
+-------+-----------------------------+---------+------------------+
+------------------+-------------------+
| Outlet_Est_Year  |Item_Outlet_Sales  |
+------------------+-------------------+
|14204             |8523               |
|1997.8306814981695|2181.2889135750343 |
|8.37166387089612  |1706.4996157338342 |
|1985              |33.29              |
|2009              |13086.9648         |
+------------------+-------------------+

Unique values of each field
Item_Identifier:1559
Item_Weight:416
Item_Fat_Content:5
Item_Visibility:13006
Item_Type:16
Item_MRP:8052
Outlet_Identifier:10
Outlet_Establishment_Year:9
Outlet_Size:4
Outlet_Location_Type:3
Outlet_Type:4
Item_Outlet_Sales:3494

Frequency Count of Item_Fat_Content:
+----------------+-----+
|Item_Fat_Content|count|
+----------------+-----+
| low fat        | 178 |
| Low Fat        | 8485|
| reg            | 195 |
| LF             | 522 |
| Regular        | 4824|
+----------------+-----+
Frequency Count of Item_Type:
+--------------------+-----+
| Item_Type          |count|
+--------------------+-----+
| Frozen Foods       | 1426|
| Breakfast          | 186 |
| Dairy              | 1136|
|Fruits and Vegeta...| 2013|
| Breads             | 416 |
| Starchy Foods      | 269 |
| Others             | 280 |
| Soft Drinks        | 726 |
| Household          | 1548|
| Health and Hygiene | 858 |
| Baking Goods       | 1086|
| Canned             | 1084|
| Snack Foods        | 1989|
| Meat               | 736 |
| Seafood            | 89  |
| Hard Drinks        | 362 |
+--------------------+-----+

Frequency Count of Outlet_Size:
+-----------+-----+
|Outlet_Size|count|
+-----------+-----+
| High      | 1553|
| Small     | 3980|
| Medium    | 4655|
|           | 4016|
+-----------+-----+

Frequency Count of Outlet_Location_Type:
+--------------------+-----+
|Outlet_Location_Type|count|
+--------------------+-----+
| Tier 1             | 3980|
| Tier 2             | 4641|
| Tier 3             | 5583|
+--------------------+-----+

Frequency Count of Outlet_Type:
+-----------------+-----+
| Outlet_Type     |count|
+-----------------+-----+
| Grocery Store   | 1805|
|Supermarket Type1| 9294|
|Supermarket Type2| 1546|
|Supermarket Type3| 1559|
+-----------------+-----+
</pre></li><li><p>The next step in data exploration is to deal with missing values. Here is the code which involves imputing missing values:</p><pre class="programlisting">    // Replace Missing values for Item_Weight
    val df_Item_WeightNull =
      sales_data.filter(sales_data("Item_Weight").isNull)
    println("Missing rows for Item_Weight: "+df_Item_WeightNull.count)
    val mean_ItemWeight = sales_data.select(mean("Item_Weight"))
    .first()(0).asInstanceOf[Double]
    val fill_MissingValues_ItemWeight =
       sales_data.na.fill(mean_ItemWeight,Seq("Item_Weight"))
       println("After replacing the mean for Item_Weight..."+
    fill_MissingValues_ItemWeight.filter(fill_MissingValues_ItemWeight
    ("Item_Weight").isNull).count)

    //Replace missing values for Outlet_Size with the mode of
    Outlet_Size
    val df_OutletSizeNull =
    fill_MissingValues_ItemWeight.filter(fill_MissingValues_ItemWeight
    ("Outlet_Size").like("") )
    println("Missing Outlet Size Rows: "+df_OutletSizeNull .count)
    val new_Df =
    fill_MissingValues_ItemWeight.withColumn("Outlet_Type_Size",
    concat($"Outlet_Type", lit(":"), $"Outlet_Size"))
    val aggregated_Df  = new_Df.groupBy("Outlet_Type_Size").count()
    val modified_Df = new_Df.na.replace("Outlet_Size", Map("" -&gt; "NA"))
    modified_Df.show()
    val df_SuperMarketType1 =
    modified_Df.filter(new_Df("Outlet_Type").contains
    ("SupermarketType1"))
    val df_SuperMarketType2 =
    modified_Df.filter(new_Df("Outlet_Type").contains
    ("Supermarket Type2"))
    val df_SuperMarketType3 =
    modified_Df.filter(new_Df("Outlet_Type").contains
    ("SupermarketType3"))
    val df_GroceryStore =
    modified_Df.filter(new_Df("Outlet_Type").contains
    ("Grocery Store"))
    val replacedMissingValues_ForOutletSize_With_SuperMarketType1 =
    df_SuperMarketType1.na.replace("Outlet_Size", Map("NA" -&gt; "Small"))
    val replacedMissingValues_ForOutletSize_With_SuperMarketType2 =
    df_SuperMarketType2.na.replace("Outlet_Size", Map("NA" -&gt;
    "Medium"))
    val replacedMissingValues_ForOutletSize_With_SuperMarketType3 =
    df_SuperMarketType3.na.replace("Outlet_Size", Map("NA" -&gt;
    "Medium"))
    val replacedMissingValues_ForOutletSize_With_GroceryStore =
    df_GroceryStore.na.replace("Outlet_Size", Map("NA" -&gt; "Small"))

   val replaced_MissingValues_ForOutletSize =
   replacedMissingValues_ForOutletSize_With_SuperMarketType1
   .unionAll(replacedMissingValues_ForOutletSize_With_SuperMarketType2)
   .unionAll(replacedMissingValues_ForOutletSize_With_SuperMarketType3)
   .unionAll(replacedMissingValues_ForOutletSize_With_GroceryStore)

   val missing_Rows =  replaced_MissingValues_ForOutletSize.filter
   (replaced_MissingValues_ForOutletSize("Outlet_Size").like(""))
   println("After replacing the mode for missing values of Outlet Size:
   "+missing_Rows.count)
</pre><p>The following is the output:</p><pre class="programlisting">      Missing rows for Item_Weight: 2439
      After replacing the mean for Item_Weight ...0
      Missing Outlet Size Rows: 4016
      After replacing the mode for missing values of Outlet Size: 0
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec166"></a>How it worksâ€¦</h3></div></div></div><p>As a part of data exploration, the <code class="literal">sales_data.describe() </code>line gives the summary statistics of all the numeric fields. A few observations that we could figure out are:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><code class="literal">Item_Visibility</code> has a minimum value of zero and this doesn't make practical sense since when a product is sold in a store, visibility cannot be zero.</p></li><li style="list-style-type: disc"><p><code class="literal">Outlet_Establishment_Years</code> varies from 1985 to 2009. If we convert the values to how old a particular store is, it will have better impact on the sales.</p></li></ul></div><p>Next, for each field, we had a number of unique values. The line <code class="literal">val columnNames = sales_data.columns</code> generates all column names. Next we iterate through all the columns and generate unique values as <code class="literal">sales_data.select(fieldName).distinct().count.toString</code></p><p>From this, we can understand that there are 1559 products and 10 outlet/stores. Also, <code class="literal">Item_Type</code> has 16 unique values. We also explored the frequency of different categories for each nominal attribute as <code class="literal">sales_data.groupBy(fieldName).count()</code>. The output provides the following observations:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>For <code class="literal">Item_Fat_Content</code>Â field, some of the low-fat values are miscoded as <code class="literal">low fat</code> and <code class="literal">LF.</code> Also, regular is mentioned as <code class="literal">regular</code> and <code class="literal">reg</code>.</p></li><li style="list-style-type: disc"><p>All categories for the attribute <code class="literal">Item_Type</code> doesn't have significant numbers. It looks like combining them would give better results.</p></li></ul></div><p>Next, in the data cleaning stage, missing values for <code class="literal">Item_Weight</code> are replaced with the mean of the remaining values. The <code class="literal">sales_data.select(mean("Item_Weight")).first()(0).asInstanceOf[Double]</code>Â line, generates the mean of the <code class="literal">Item_Weight</code> column. The line, <code class="literal">sales_data.na.fill(mean_ItemWeight,Seq("Item_Weight"))</code> replaces the missing values with the calculated mean. Also, for treating missing values of <code class="literal">Outlet_Size,</code> mode is the statistic to be used. Since spark doesn't have support for mode, for each <code class="literal">Outlet_Type</code>, whichever <code class="literal">Outlet_Size</code> combination gives the maximum number of rows, the corresponding <code class="literal">Outlet_Size</code> is replaced with the missing value for the respective <code class="literal">Outlet_Type</code>.</p><p>TheÂ <code class="literal">new_Df.na.replace("Outlet_Size", Map("" -&gt; "NA"))</code>Â line, replaces the blank values of <code class="literal">Outlet_Size</code> with <code class="literal">"NA"</code>. Next, the entire data frame is split into four parts, each containing records with a unique <code class="literal">Outlet_Type</code> using the statement <code class="literal">modified_Df.filter(new_Df("Outlet_Type").contains("Supermarket Type1"))</code>.</p><p>Now, the <code class="literal">SupermarketType1</code> outlet type has the maximum number of rows with the outlet size 'Small'. Hence the line, <code class="literal">df_SuperMarketType1.na.replace("Outlet_Size", Map("NA" -&gt; "Small"))</code> replaces all the missing values (represented as <code class="literal">NA)</code> with <code class="literal">Small.</code> This is done for other data frames containing records of other outlet types.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec167"></a>There's moreâ€¦</h3></div></div></div><p>In the preceding recipe, we saw how to explore data. The missing value treatment for the variables could be done in various ways as replacing it with the mean value, removing the rows if the majority of the fields also have a value missing for a particular row, and so on. In the next recipe, let's see how to apply feature engineering.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec168"></a>See also</h3></div></div></div><p>Please refer to the <span class="emphasis"><em>Variable identification</em></span> recipe to understand the initial steps.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec47"></a>Feature engineering</h2></div></div><hr /></div><p>In this recipe, we'll see how to apply feature engineering on the explored data.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec169"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark cluster in any one of the modes, that is, local, standalone, YARN, or Mesos. For installing Spark on a standalone cluster, please refer to <a class="ulink" href="http://spark.apache.org/docs/latest/spark-standalone.html" target="_blank">http://spark.apache.org/docs/latest/spark-standalone.html</a>. Also, include the Spark MLlib package in the <code class="literal">build.sbt</code> file so that it downloads the related libraries and the API can be used. Install Hadoop (optionally), Scala, and Java.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec170"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>After data exploration, the next step is to perform feature engineering. Let's try to apply feature engineering and make the data ready for analysis.</p></li><li><p>From the available attributes, we can see that the minimum value of <code class="literal">Item_Visibility</code> is 0. We can consider this missing information and replace with mean values as follows:</p><pre class="programlisting">       // Replace Missing values for Item_Visibility
       val df_Item_VisibilityNull =
       replaced_MissingValues_ForOutletSize
       .filter(sales_Data_Refined("Item_Visibility") === 0)
       println("Missing rows for Item_Visibility:
       "+df_Item_VisibilityNull.count)
       val mean_ItemVisibility = replaced_MissingValues_ForOutletSize
       .select(mean("Item_Visibility")).first()(0).asInstanceOf[Double]
       val df_ForFeatureEngg  =
       replaced_MissingValues_ForOutletSize
       .na.replace("Item_Visibility", Map(0.0 -&gt; mean_ItemVisibility))
       println("After replacing the mean for Item_Visibility ..."
       +df_ForFeatureEngg
       .filter(df_ForFeatureEngg("Item_Visibility") === 0).count)
</pre><p>The following is the output:</p><pre class="programlisting">    Missing rows for Item_Visibility: 879
    After replacing the mean for Item_Visibility ...0
</pre></li><li><p>From the data exploration step, theÂ <code class="literal">Frequency Count for Outlet_Type</code>Â output reflects that for the variables supermarket type2 and type3, the count of variables is the same. Let's try to analyze the mean sales per <code class="literal">Outlet_Type</code> and see if we can combine the supermarket type2 and type3 variables:</p><pre class="programlisting">     val df_MeanSales_PerOutletType =
             df_ForFeatureEngg.groupBy("Outlet_Type").mean
            ("Item_Outlet_Sales")
     println("Mean Sales Per Outlet_Type")
     df_MeanSales_PerOutletType.show()
</pre><p>The following is the output:</p><pre class="programlisting">     +-----------------+-----------------------+
     | Outlet_Type     |avg(Item_Outlet_Sales) |
     +------------------------+----------------+
     | Grocery Store   | 339.82850046168045    |
     |Supermarket Type1| 2316.1811481082987    |
     |Supermarket Type2| 1995.498739224137     |
     |Supermarket Type3| 3694.038557647059     |
     +-----------------+-----------------------+</pre></li><li><p>We also see that the <code class="literal">Item_Type</code> variable has 16 categories. The categories seem to be <code class="literal">Food</code>, <code class="literal">Drinks</code>, <code class="literal">Non-Consumables</code>, and the <code class="literal">Item_Identifier</code> starts with <code class="literal">FD</code>, <code class="literal">DR</code>Â or <code class="literal">NC</code>. Using the <code class="literal">Item_Identifier</code>, we can create a broader item type column as:</p><pre class="programlisting">      val sqlFunc = udf(coder)
      val new_Df_WithItemTypeCombined = df_ForFeatureEngg
      .withColumn("Item_Type_Combined",sqlFunc(col("Item_Identifier")
       ))
       new_Df_WithItemTypeCombined.groupBy
       ("Item_Type_Combined").count().
      show()
      val coder = (id:String) =&gt; id.substring(0,2) match{
      case "FD" =&gt; "Food"
      case "NC" =&gt; "Non-Consumable"
      case "DR" =&gt; "Drinks"
       }

</pre><p>The following is the output:</p><pre class="programlisting">  +----------------------------+-------+
  |Item_Type_Combined          |count  |
  +----------------------------+-------+
  | Drinks                     | 1317  |
  | Food                       |10201  |
  | Non-Consumable             | 2686  |
  +----------------------------+-------+</pre></li><li><p>We can also make a new column showing the years of operation of a store. This could be done in the following way:</p><pre class="programlisting">       val sqlFunc2 = udf(coder2)
       val revised_Df
       =new_Df_WithItemTypeCombined.withColumn("Outlet_Years",
       sqlFunc2(col("Outlet_Establishment_Year")) )
       revised_Df.select("Outlet_Years").describe().show()
       val coder2 = (value:Int) =&gt; 2013 - value
</pre><p>The following is the output:</p><pre class="programlisting">      +-----------+------------------+
      |summary    | Outlet_Years     |
      +-----------+------------------+
      | count     | 14204            |
      | mean      |15.169318501830471|
      | stddev    | 8.371663870896116|
      | min       | 4                |
      | max       | 28               |
      +----------+-------------------+</pre></li><li><p>Also, create a broader category for <code class="literal">Item_Fat_Content</code> as follows:</p><pre class="programlisting">       val sqlFunc3 = udf(coder3)
       val new_Df_WithItem_Fat_Content_Combined =  revised_Df
      .withColumn("Item_Fat_Content_Combined",sqlFunc(col
       ("Item_Fat_Content")))
       new_Df_WithItem_Fat_Content_Combined
      .groupBy("Item_Fat_Content_Combined").count().show()

      val coder3 = (id:String) =&gt; id match{
      case "LF" | "Low Fat"  =&gt; "Low Fat"
      case "reg" | "Regular" =&gt; "Regular"
      case "low fat" =&gt; "Low Fat" }</pre><p>The following is the output:</p><pre class="programlisting">      +-------------------------+-----+
      |Item_Fat_Content_Combined|count|
      +-------------------------+-----+
      |                  Low Fat| 9185|
      |                  Regular| 5019|
      +-------------------------+-----+
</pre></li><li><p>Remove the ID variable and unused column as follows:</p><pre class="programlisting">        // Exclude ID variable
        val sales_Data_new = new_Df_WithItem_Fat_Content_Combined
       .drop("Item_Identifier").drop("Outlet_Identifier")

        // Removing unused Columns
        val sales_Data_Final = sales_Data_new
       .drop("Item_Fat_Content").drop("Item_Type").drop
       ("Outlet_Type_Size")
       .drop("Outlet_Establishment_Year")
</pre></li><li><p>Finally, as linear regression accepts only numerical variables, we need to convert categories of nominal variables into numeric types as follows:</p><pre class="programlisting">       /* Applying One Hot encoding of Categorical Variables */
       val sqlFunc_CreateDummyVariables = udf(udf_returnDummyValues)
       // One Hot Encoding for Outlet_Type
       val new_Df_WithDummy_OutletType =
       create_DummyVariables(sales_Data_Final,
       sqlFunc_CreateDummyVariables, "Outlet_Type", 0)
       // One Hot Encoding for  Outlet_Size
       val new_Df_WithDummy_OutletSize =
       create_DummyVariables(new_Df_WithDummy_OutletType,
       sqlFunc_CreateDummyVariables, "Outlet_Size", 0)
       // One Hot Encoding for  Outlet_Location_Type
       val new_Df_WithDummy_OutletLocationType =
       create_DummyVariables(new_Df_WithDummy_OutletSize,
       sqlFunc_CreateDummyVariables, "Outlet_Location_Type", 0)
       // One Hot Encoding for  Item_Type_Combined
       val new_Df_WithDummy_ItemTypeCombined =
       create_DummyVariables(new_Df_WithDummy_OutletLocationType,
       sqlFunc_CreateDummyVariables, "Item_Type_Combined", 0)
       // One Hot Encoding for  Item_Fat_Content_Combined
       val new_Df_WithDummy_ItemFatContentCombined =
       create_DummyVariables(new_Df_WithDummy_ItemTypeCombined,
       sqlFunc_CreateDummyVariables, "Item_Fat_Content_Combined", 0)

</pre></li><li><p>Here is the function which creates <span class="emphasis"><em>n</em></span> columns for all the categorical variables having <span class="emphasis"><em>n</em></span> distinct categories:</p><pre class="programlisting">       def create_DummyVariables(df:DataFrame,
       udf_Func:UserDefinedFunction,
       variableType:String, i:Int ): DataFrame = {
       variableType match {
       case "Outlet_Type" =&gt; if (i == 4) df
       else {
         val df_new = df.withColumn (variableType + "_" + i.toString,
         udf_Func(lit(variableType),col ("Outlet_Type"), lit (i) ) )
         create_DummyVariables (df_new, udf_Func, variableType, i + 1)
       }
       case "Outlet_Size" =&gt; if(i == 3)  df
       else {
        val df_new = df.withColumn (variableType + "_" + i.toString,
        udf_Func(lit(variableType), col ("Outlet_Size"), lit (i) ) )
        create_DummyVariables (df_new, udf_Func, variableType, i + 1)
       }
       case "Outlet_Location_Type" =&gt;  if(i == 3)  df
       else {
        val df_new = df.withColumn (variableType + "_" + i.toString,
        udf_Func(lit(variableType), col ("Outlet_Location_Type"),
        lit (i) ) )
        create_DummyVariables (df_new, udf_Func, variableType, i + 1)
       }
       case "Item_Type_Combined" =&gt;  if(i == 3)  df
       else {
        val df_new = df.withColumn (variableType + "_" + i.toString,
        udf_Func(lit(variableType), col ("Item_Type_Combined"),
        lit (i) ) )
        create_DummyVariables (df_new, udf_Func, variableType, i + 1)
       }
       case "Item_Fat_Content_Combined" =&gt;  if(i == 2)  df
       else {
        val df_new = df.withColumn (variableType + "_" + i.toString,
        udf_Func(lit(variableType), col("Item_Fat_Content_Combined"),
        lit (i) ) )
        create_DummyVariables (df_new, udf_Func, variableType, i + 1)
       } } }
</pre></li><li><p>Here is the definition for UDF <code class="literal">udf_returnDummyValues</code> which returns values 0/1 for every distinct category:</p><pre class="programlisting">       val udf_returnDummyValues  = (variableType:String,
       columnValue:String,   jobNo:Int) =&gt; variableType match{
       case "Outlet_Type" =&gt; columnValue match {
       case "Grocery Store" =&gt; if (jobNo == 0) 1.0 else 0.0
       case "Supermarket Type1" =&gt; if (jobNo == 1) 1.0   else 0.0
       case "Supermarket Type2" =&gt; if (jobNo == 2) 1.0
       else 0.0
       case "Supermarket Type3" =&gt; if (jobNo == 3) 1.0
       else 0.0
       }
       case "Outlet_Size" =&gt; columnValue match {
       case "High" =&gt; if(jobNo == 0) 1.0 else 0.0
       case "Small" =&gt; if(jobNo == 1) 1.0 else 0.0
       case "Medium" =&gt; if(jobNo == 2) 1.0 else 0.0
       }
       case "Outlet_Location_Type" =&gt; columnValue match {
       case "Tier 1" =&gt; if(jobNo == 0) 1.0 else 0.0
       case "Tier 2" =&gt; if(jobNo == 1) 1.0 else 0.0
       case "Tier 3" =&gt; if(jobNo == 2) 1.0 else 0.0
       }
       case "Item_Type_Combined" =&gt; columnValue match {
       case "Drinks" =&gt; if(jobNo == 0) 1.0 else 0.0
       case "Food" =&gt; if(jobNo == 1) 1.0 else 0.0
       case "Non-Consumable" =&gt; if(jobNo == 2) 1.0 else 0.0
       }
       case "Item_Fat_Content_Combined" =&gt; columnValue match {
       case "Low Fat" =&gt; if(jobNo == 0) 1.0 else 0.0
       case "Regular" =&gt; if(jobNo == 1) 1.0 else 0.0
        } }
</pre></li><li><p>Now, as the dummy variables are created, let's remove the original categorical variables from the final data frame to fit into a linear regression model:</p><pre class="programlisting">        //Remove categorical columns
        val final_Df = new_Df_WithDummy_ItemFatContentCombined
               .drop("Outlet_Size")
               .drop("Outlet_Location_Type")
               .drop("Outlet_Type")
               .drop("Item_Type_Combined")
               .drop("Item_Fat_Content_Combined")
         final_Df.show(5)
</pre><p>The following is the output:</p><pre class="programlisting">+-----------+-------------------+---------+-----------------+
|Item_Weight|Item_Visibility    |Item_MRP |Item_Outlet_Sales|
+-----------+-------------------+---------+-----------------+
| 9.3       |0.016047301        |249.8092 |3735.138         |
| 17.5      |0.016760075        |141.618 |2097.27           |
| 8.93      |0.06595278007399324|53.8614 |994.7052          |
| 13.65     |0.012741089        |57.6588 |343.5528          |
| 16.2      |0.016687114        |96.9726 |1076.5986         |
+----------------+------------------------+------------------+
+-------------+---------------+---------------+--------------+
|Outlet_Years | Outlet_Type_0 | Outlet_Type_1 |Outlet_Type_2 |
+-------------+---------------+---------------+--------------+
|14           |0.0            |1.0            |0.0           |
|14           |0.0            |1.0            |0.0           |
|26           |0.0            |1.0            |0.0           |
|26           |0.0            |1.0            |0.0           |
|11           |0.0            |1.0            |0.0           |
+-------------+---------------+---------------+--------------+

+---------------+---------------+
| Outlet_Type_3 | Outlet_Size_0 |
+---------------+---------------+
|0.0            |0.0            |
|0.0            |0.0            |
|0.0            |1.0            |
|0.0            |1.0            |
|0.0            |0.0            |
+--------------+----------------+</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec171"></a>How it worksâ€¦</h3></div></div></div><p>The final step is performing feature engineering and as a part of this, the missing values for <code class="literal">Item_Visibility</code> are replaced with its mean as <code class="literal">val df_ForFeatureEngg replaced_MissingValues_ForOutletSize.na.replace("Item_Visibility", Map(0.0 -&gt; mean_ItemVisibility))</code>. We also noticed from the data exploration step, the frequency count of <code class="literal">Outlet_Type</code> shows that for the variables supermarket type2 and type3, the count of variables is almost the same. When we took mean sales by <code class="literal">Outlet_Type</code> as <code class="literal">df_ForFeatureEngg.groupBy("Outlet_Type").mean("Item_Outlet_Sales")</code>, it clearly shows significant differences, hence we leave it as it is.</p><p>Also, <code class="literal">Item_Type</code> has 16 unique categories. The first two characters of <code class="literal">Item_Identifier</code> are either <code class="literal">FD</code>, <code class="literal">DR</code>Â or <code class="literal">NC</code>. We created a broader category as <code class="literal">Item_Type_Combined from the column Item_Identifier using the lines </code><code class="literal">val sqlFunc = udf(coder)</code> and <code class="literal">val new_Df_WithItemTypeCombined = df_ForFeatureEngg.withColumn("Item_Type_Combined sqlFunc(col("Item_Identifier")))</code>. The variable coder takes the initial two characters of <code class="literal">Item_Identifier</code> and a broader category is generated. Similarly, for <code class="literal">Item_Fat_Content</code>, a broader category is generated as <code class="literal">Item_Fat_Content_Combined</code>. A new <code class="literal">Outlet_Years</code> column depicting the years of operation of a store is created as <code class="literal">new_Df_WithItemTypeCombined.withColumn("Outlet_Years",sqlFunc2(col("Outlet_Establishment_Year")))</code>. The ID variables and unused columns are dropped as <code class="literal">val sales_Data_new = new_Df_WithItem_Fat_Content_Combined.drop("Item_Identifier")</code>.</p><pre class="programlisting">.drop("Outlet_Identifier") and val sales_Data_Final = sales_Data_new.drop("Item_Fat_Content").drop("Item_Type").drop("Outlet_Type_Size")
.drop("Outlet_Establishment_Year").
</pre><p>Next, all the categorical variables need to be converted to numeric types. For this, a <code class="literal">create_DummyVariables(df:DataFrame, udf_Func:UserDefinedFunction, variableType:String, i:Int ): DataFrame</code> function is created which takes the data frame, user defined function, categorical variable and iteration count as input. For each categorical variable; <code class="literal">Outlet_Type</code>, <code class="literal">Outlet_Size</code>, <code class="literal">Outlet_Location_Type</code>, <code class="literal">Item_Type_Combined</code>, <code class="literal">Item_Fat_Content_Combined</code>, the dummy variables such as <code class="literal">Outlet_Type_0</code>, <code class="literal">Outlet_Type_1</code>, <code class="literal">Outlet_Type_2</code>, <code class="literal">Outlet_Type_3</code> are created in an iterative fashion as <code class="literal">val df_new = df.withColumn (variableType + "_" + i.toString</code>, <code class="literal">udf_Func (lit(variableType)</code>, <code class="literal">col ("Outlet_Type"), lit (i)))</code> and <code class="literal">create_DummyVariables (df_new, udf_Func, variableType, i + 1)</code>. This is done for all five categorical variables. The user defined <code class="literal">udf_returnDummyValues</code> function generates values for the dummy variables using a match clause. For example, the <code class="literal">Outlet_Size</code> variable has three unique categories; <code class="literal">High</code>, <code class="literal">Small</code>, and <code class="literal">Medium</code>. These three variables, <code class="literal">Outlet_Size_0</code> (represents <code class="literal">High</code>), <code class="literal">Outlet_Size_1</code> (represents <code class="literal">Small</code>), and <code class="literal">Outlet_Size_2</code> (represents <code class="literal">Medium</code>) are created, where <code class="literal">Outlet_Size_0</code> represents <code class="literal">High</code> created as part of the recursive <code class="literal">create_DummyVariables</code> function.</p><p>After this, the categorical variables are dropped using theÂ <code class="literal">val final_Df = new_Df_WithDummy_ItemFatContentCombined.drop("Outlet_Size").drop("Outlet_Location_Type").drop("Outlet_Type").drop("Item_Type_Combined").drop("Item_Fat_Content_Combined")</code>Â statement.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec172"></a>There's moreâ€¦</h3></div></div></div><p>In the preceding recipe, we saw how to apply feature engineering. In the next recipe, let's see how to apply the algorithm on the final dataset.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec173"></a>See also</h3></div></div></div><p>Please refer to the earlier <span class="emphasis"><em>Variable identification</em></span> and <span class="emphasis"><em>Data exploration</em></span>Â recipes.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec48"></a>Applying linear regression</h2></div></div><hr /></div><p>In this recipe, we'll see how to apply linear regression.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec174"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark cluster in any one of the modes, for instance, local, standalone, YARN, or Mesos. For installing Spark on a standalone cluster, please refer to <a class="ulink" href="http://spark.apache.org/docs/latest/spark-standalone.html" target="_blank">http://spark.apache.org/docs/latest/spark-standalone.html</a>. Also, include the Spark MLlib package in the <code class="literal">build.sbt</code> file so that it downloads the related libraries and the API can be used. Install Hadoop (optionally), Scala, and Java.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec175"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>The final step is to convert data into train and test datasets and apply the regression model on the training data:</p><pre class="programlisting">        //Split the data frame into train and test sets
        val train_Df =
        final_Df.filter(final_Df("Item_Outlet_Sales").isNotNull)
        val test_Df =
        final_Df.filter(final_Df("Item_Outlet_Sales").isNull)
        val train_Rdd =  train_Df.rdd.map {
        row =&gt; val item_weight = row.getAs[Double]("Item_Weight")
        val item_Visibility = row.getAs[Double]("Item_Visibility")
        val item_mrp = row.getAs[Double]("Item_MRP")
        val item_outlet_sales = row.getAs[Double]("Item_Outlet_Sales")
        val otlet_years = row.getAs[Int]("Outlet_Years").toDouble
        val outlet_type_0 = row.getAs[Double]("Outlet_Type_0")
        val outlet_type_1 = row.getAs[Double]("Outlet_Type_1")
        val outlet_type_2 = row.getAs[Double]("Outlet_Type_2")
        val outlet_type_3 = row.getAs[Double]("Outlet_Type_3")
        val outlet_size_0 = row.getAs[Double]("Outlet_Size_0")
        val outlet_size_1 = row.getAs[Double]("Outlet_Size_1")
        val outlet_size_2 = row.getAs[Double]("Outlet_Size_2")
        val outlet_Location_Type_0 = row.getAs[Double]
        ("Outlet_Location_Type_0")
        val outlet_Location_Type_1 = row.getAs[Double]
        ("Outlet_Location_Type_1")
        val outlet_Location_Type_2 = row.getAs[Double]
        ("Outlet_Location_Type_2")
        val item_type_0 =  row.getAs[Double]("Item_Type_Combined_0")
        val item_type_1 =  row.getAs[Double]("Item_Type_Combined_1")
        val item_type_2 =  row.getAs[Double]("Item_Type_Combined_2")
        val item_fat_content_0 = row.getAs[Double]
        ("Item_Fat_Content_Combined_0")
        val item_fat_content_1 = row.getAs[Double]
        ("Item_Fat_Content_Combined_1")
        val featurecVec = Vectors.dense(Array(item_weight,
        item_Visibility,item_mrp,otlet_years,outlet_type_0,
        outlet_type_1,outlet_type_2,outlet_type_3,outlet_size_0,
        outlet_size_1,outlet_size_2,outlet_Location_Type_0,
        outlet_Location_Type_1,outlet_Location_Type_2,
        item_type_0,item_type_1,item_type_2,item_fat_content_0,
        item_fat_content_1))
        LabeledPoint(item_outlet_sales, featurecVec)
        }.cache()
        val numIters = 500
        val stepSize =0.0001

        // Applying the linear Regression Model
        val lm = new LinearRegressionWithSGD().setIntercept(true)
        lm.optimizer.setStepSize(stepSize)
        lm.optimizer.setNumIterations(numIters)
        lm.optimizer.setMiniBatchFraction(0.2)
        lm.optimizer.setConvergenceTol(0.0001)
        lm.optimizer.setRegParam(0.1)
        val model = lm.run(train_Rdd)
        val predictedData = train_Rdd.map{
        labeledPoint =&gt;
        val featureVec = labeledPoint.features
        val originalValue = labeledPoint.label
        val predictedValue = model.predict(featureVec)
        (originalValue, predictedValue)
         }
        val mse = predictedData.map{case(original, predicted) =&gt;
        (original-predicted)*(original-predicted)
         }.mean()
        val metricsObject = new RegressionMetrics(predictedData )
        println("R2 Value: "+metricsObject.r2)
        println("Mean Squared Error: "+mse)
</pre><p>The following is the output:</p><pre class="programlisting">      R2 Value: 0.8346890
      Mean Squared Error:7573.1746617937
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec176"></a>How it worksâ€¦</h3></div></div></div><p>The final steps of applying the algorithm are the DataFrame is split into train and test sets as <code class="literal">final_Df.filter(final_Df("Item_Outlet_Sales").isNotNull)</code> and <code class="literal">final_Df.filter(final_Df("Item_Outlet_Sales").isNull)</code>. Now, the <code class="literal">train_Df</code>Â varaible is converted to RDD. The output variable <code class="literal">Item_Outlet_Sales</code> is the label and from the remaining variables, a dense vector is created. Once the <code class="literal">RDD[LabeledPoint]</code> is generated, the <code class="literal">val lm = new LinearRegressionWithSGD().setIntercept(true)</code> line creates the object <code class="literal">LinearRegressionWithSGD</code>. The necessary parameters such as step-size are set as <code class="literal">lm.optimizer.setStepSize(stepSize)</code>. The other parameters such as <code class="literal">NumIterations</code>, <code class="literal">MiniBatchFraction</code>, <code class="literal">ConvergenceTol</code>, <code class="literal">RegParam</code> are also set. Now the algorithm runs on the training dataset with the <code class="literal">val model = lm.run(train_Rdd)</code> statement. Once the model is ready, mean squared error and <code class="literal">r2</code> value are seen. The algorithm is made to fit on the data by changing the parameters such as step-size, number of iterations, regularization parameter, convergence tolerance, setIntercept and so on.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec177"></a>There's moreâ€¦</h3></div></div></div><p>Once the model is built, the initially-created hypothesis would be tested and the data needs to be sufficient to test all the scenarios. In the above case study, we applied the linear regression model from MLlib, Spark has support for <code class="literal">ml</code> pipelines as well which means we can apply algorithms directly on the data frame. There are other flavors of regression such as ridge regression, stepwise regression, <span class="strong"><strong>Ordinary Least Squares Regression</strong></span> (<span class="strong"><strong>OLSR</strong></span>), multivariate regression, and so on. Spark MLlib offers several regression algorithms with different estimation techniques.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec178"></a>See also</h3></div></div></div><p>Please refer to this textbook for better understanding of regression algorithms and implementation of the different techniques using Spark MLlib: <a class="ulink" href="http://www.statsoft.com/Textbook/Multiple-Regression" target="_blank">http://www.statsoft.com/Textbook/Multiple-Regression</a>.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec49"></a>Applying logistic regression on bank marketing data</h2></div></div><hr /></div><p>Logistic regression is a classification algorithm. It is used to predict a binary outcome (<code class="literal">0/1</code>, <code class="literal">Yes/No</code>, <code class="literal">True/False</code>) from the set of independent variables. It is a special case of linear regression where the outcome variable is categorical. The log of odds is the dependent variables, that is, it predicts the probability of occurrence of an event by fitting data to a logit function. Logistic regression is also termed as <span class="strong"><strong>linear classification model</strong></span>. The link function used in the logistic regression is the logic link <span class="emphasis"><em>1/(1+exp(-wTx))</em></span>. The related loss function for logistic regression is the logistic loss, that is, <span class="emphasis"><em>log(1+exp(-ywTx))</em></span>. Here <span class="emphasis"><em>y</em></span> is the actual target variable (either 1 for the positive class or -1 for the negative class).</p><p>This recipe shows how to apply the logistic regression algorithm available in the Spark MLlib package on Bank Marketing Data. The code is written in Scala.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec50"></a>Variable identification</h2></div></div><hr /></div><p>In this recipe, we'll see how to identify the required variables for analysis and understand their description.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec179"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark cluster in any one of the modes, that is, local, standalone, YARN, or Mesos. For installing Spark on a standalone cluster, please refer to <a class="ulink" href="http://spark.apache.org/docs/latest/spark-standalone.html" target="_blank">http://spark.apache.org/docs/latest/spark-standalone.html</a>. Also, include the Spark MLlib package in the <code class="literal">build.sbt</code> file so that it downloads the related libraries and the API can be used. Install Hadoop (optionally), Scala, and Java.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec180"></a>How to do itâ€¦</h3></div></div></div><p>Let's take <span class="emphasis"><em>Bank Marketing</em></span> data which contains information related to a direct marketing campaign of a Portuguese bank institute and its attempts to make their clients subscribe for a term deposit. The data originally contains 41,188 rows and 21 columns. For our analysis, we'll use 10 variables. Let's see the properties of the variables.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip18"></a>Tip</h3><p>Please download the dataset from the following location: <a class="ulink" href="https://github.com/ChitturiPadma/datasets/blob/master/bank_marketing_data.csv" target="_blank">https://github.com/ChitturiPadma/datasets/blob/master/bank_marketing_data.csv</a>.</p></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Here is the description of the variables that we are going to use in our analysis:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Variable</strong></span></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Description</strong></span></p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">age</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Age of the customer</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">job</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Customer's occupation</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">marital</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Marital status</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">default</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Indicates whether the customer has credit in default.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">housing</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Indicates if the customer has a housing loan</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">loan</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Indicates if the customer has a personal loan</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">duration</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Duration of last contact in seconds</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">previous</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Number of contacts performed before the campaign for the customer</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">poutcome</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Outcome of the previous marketing campaign</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">empvarrate</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Employment variation rate</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; ">
<p><code class="literal">y</code></p>
</td><td style="">
<p>Response variable which indicates if the customer has subscribed for deposit</p>
</td></tr></tbody></table></div></li><li><p>The data resides in HDFS. Let's load the data into Spark and see the values corresponding to different variables:</p><pre class="programlisting">         import org.apache.spark._
         import org.apache.spark.ml.feature.StringIndexer
         import org.apache.spark.mllib.evaluation.RegressionMetrics
         import org.apache.spark.mllib.linalg.Vectors
         import org.apache.spark.mllib.regression.
         {LinearRegressionWithSGD, LabeledPoint}
         import org.apache.spark.sql._
         import org.apache.spark.sql.functions._
         import org.apache.spark.sql.DataFrameNaFunctions
         import org.apache.spark.sql.types._
         import org.apache.spark.mllib.stat.
         {MultivariateStatisticalSummary, Statistics}
         import org.apache.spark.ml.regression.LinearRegression

         object Logistic_Regression_Demo {
         def main(args: Array[String]): Unit = {
         val conf = new SparkConf().setMaster("spark://master:7077")
         .setAppName("Logistic_Prediction")
          val sc = new SparkContext(conf)
          val sqlContext = new SQLContext(sc)
          import sqlContext.implicits._

          //Loading data
          val bank_Marketing_Data =
          sqlContext.read.format("com.databricks.spark.csv")
                .option("header", "true")
                .option("inferSchema", "true")
                .load("hdfs://namenode:9000/data/bank-data.csv")
          val selected_Data =  bank_Marketing_Data.select("age", "job",
          "marital",   "default", "housing", "loan", "duration",
          "previous", "poutcome", "empvarrate", "y").withColumn("age",
          bank_Marketing_Data("age").cast(DoubleType))
          .withColumn("duration",
          bank_Marketing_Data("duration").cast(DoubleType))
          .withColumn("previous",
          bank_Marketing_Data("previous").cast(DoubleType))
          selected_Data.show(5) } }
</pre><p>The following is the output:</p><pre class="programlisting">  +----+----------+-----------+---------------+------+---------+
  |age |     job  |    marital|default|housing| loan | duration|
  +----+----------+-----------+---------------+------+---------+
  |56.0| housemaid|    married|     no|     no|    no| 261.0   |
  |57.0|  services|    married|unknown|     no|    no| 149.0   |
  |37.0|  services|    married|     no|    yes|    no| 226.0   |
  |40.0|     admin|    married|     no|     no|    no| 151.0   |
  |56.0|  services|    married|     no|     no|   yes| 307.0   |
  +---------------+-----------+----------------+---------------+

  +---------+-----------+----------+----+
  | previous|poutcome   |empvarrate| y  |
  +---------+-----------+----------+----+
  |     0.0 |nonexistent|      1.1 | no |
  |     0.0 |nonexistent|      1.1 | no |
  |     0.0 |nonexistent|      1.1 | no |
  |     0.0 |nonexistent|      1.1 | no |
  |     0.0 |nonexistent|      1.1 | no |
  +---------+-----------+----------+----+
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec181"></a>How it worksâ€¦</h3></div></div></div><p>The Bank Marketing data is loaded as a csv file, schema is inferred and variables are identified. From the 21 columns available, 10 columns as mentioned in the beginning are considered for running the logistic regression. Once the dataset is loaded, the <code class="literal">age</code>,Â <code class="literal">duration</code>Â and <code class="literal">previous</code> columns are converted to double with the statements <code class="literal">.withColumn("age", bank_Marketing_Data("age").cast(DoubleType))</code>. The <code class="literal">selected_Data.show(5)</code> line displays the first five records. From this sample data, the independent and dependent variables are identified.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec182"></a>There's moreâ€¦</h3></div></div></div><p>In the subsequent recipes, let's see how to explore data and apply feature engineering.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec183"></a>See also</h3></div></div></div><p>Please refer to the <span class="emphasis"><em>Data Exploration</em></span> and <span class="emphasis"><em>Feature Engineering</em></span> recipes for the following steps.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec51"></a>Data exploration</h2></div></div><hr /></div><p>In this recipe, we'll see how to explore data.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec184"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark cluster in any one of the modes, that is, local, standalone, YARN, and Mesos. For installing Spark on a standalone cluster, please refer to <a class="ulink" href="http://spark.apache.org/docs/latest/spark-standalone.html" target="_blank">http://spark.apache.org/docs/latest/spark-standalone.html</a>. Also, include the Spark MLlib package in the <code class="literal">build.sbt</code> file so that it downloads the related libraries and the API can be used. Install Hadoop (optionally), Scala, and Java.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec185"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>After variable identification, let's try do some data exploration and come up with inferences about the data. Here is the code which does data exploration:</p><pre class="programlisting">         /*Summary statistics*/
         val summary = selected_Data.describe()
         println("Summary Statistics")
         summary.show()

         /* Unique values for each Field */
         val columnNames = selected_Data.columns
        val uniqueValues_PerField = columnNames.map{field =&gt; field +":"
        +selected_Data.select(field).distinct().count()}
        println("Unique Values for each Field: ")
        uniqueValues_PerField.foreach(println)

          // Frequency of Categories for categorical variables
         val frequency_Variables  = columnNames.map{
         fieldName =&gt;
         if(fieldName == "job" || fieldName == "marital" || fieldName
         == "default" ||   fieldName == "housing" || fieldName ==
         "poutcome")
         Option(fieldName,selected_Data.groupBy(fieldName).count())
         else  None }
         val seq_Df_WithFrequencyCount =
         frequency_Variables.filter(optionalDf =&gt;
         optionalDf!=None).map{optionalDf =&gt; optionalDf.get}
         seq_Df_WithFrequencyCount.foreach{case(fieldName, df) =&gt;
         println("Frequency Count of "+fieldName)
         df.show()
</pre><p>The following is the output:</p><pre class="programlisting">    Summary Statistics:
    +-------+------------------+------------------+
    |summary|              age |          duration|
    +-------+------------------+------------------+
    |  count|            41188 |             41188|
    |   mean|40.02406040594348 |258.2850101971448 |
    |stddev |10.421249980934034|259.27924883646506|
    |    min|                17|                 0|
    |    max|                98|              4918|
    +-------+------------------+------------------+


    +-------+-------------------+--------------------+
    |summary|           previous|      empvarrate    |
    +-------+-------------------+--------------------+
    |  count|            41188  |             41188  |
    |   mean|0.17296299893172767|0.08188550063146532 |
    |stddev |0.49490107983928977|1.5709597405170272  |
    |    min|                0  |               -3.4 |
    |    max|                7  |                 1.4|
    +-------+------------------+---------------------+

    Unique Values for each Field:
    age:78
    job:12
    marital:4
    default:3
    housing:3
    loan:3
    duration:1544
    previous:8
    poutcome:3
    empvarrate:10
    y:2

    Frequency Count of job:
    +-------------+-----+
    |          job|count|
    +-------------+-----+
    |   unemployed| 1014|
    |     services| 3969|
    |  blue-collar| 9254|
    |      unknown|  330|
    |    housemaid| 1060|
    | entrepreneur| 1456|
    |self-employed| 1421|
    |      retired| 1720|
    |       admin.|10422|
    |   management| 2924|
    |   technician| 6743|
    |      student|  875|
    +-------------+-----+


    Frequency Count of marital:
    +--------+-----+
    | marital|count|
    +--------+-----+
    | unknown|   80|
    |divorced| 4612|
    |  single|11568|
    | married|24928|
    +--------+-----+

    Frequency Count of default:
    +-------+-----+
    |default|count|
    +-------+-----+
    |unknown| 8597|
    |     no|32588|
    |    yes|    3|
    +-------+-----+

    Frequency Count of housing:
    +-------+-----+
    |housing|count|
    +-------+-----+
    |unknown|  990|
    |     no|18622|
    |    yes|21576|
    +-------+-----+

    Frequency Count of loan:
    +-------+-----+
    |   loan|count|
    +-------+-----+
    |unknown|  990|
    |     no|33950|
    |    yes| 6248|
    +-------+-----+

    Frequency Count of poutcome:
    +-----------+-----+
    |   poutcome|count|
    +-----------+-----+
    |nonexistent|35563|
    |    failure| 4252|
    |    success| 1373|
    +-----------+-----+
</pre></li><li><p>From the previous data exploration, we see that the numeric columns <code class="literal">age, duration</code>, <code class="literal">previous</code> and <code class="literal">empvarrate</code> do not have any missing values. Also, the frequency count of the categorical variables reveal that there are no missing values in the nominal variables too.</p></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec186"></a>How it worksâ€¦</h3></div></div></div><p>As a part of data exploration, the <code class="literal">selected_Data.describe()</code> line gives the summary statistics of all the numeric fields. A few observations that we can figure out are:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The numeric <code class="literal">age</code>, <code class="literal">duration</code>, <code class="literal">previous</code>Â and <code class="literal">empvarrate</code> fields do not have any missing values. Next, for each field, we had a number of unique values. The <code class="literal">v</code><code class="literal">al columnNames = selected_Data.columns</code> line generates all column names. Next we iterate through all the columns and generate unique values such as <code class="literal">selected_Data.select(fieldName).distinct().count.toString</code>. From this, we can understand that the <code class="literal">job</code>, <code class="literal">marital</code>, <code class="literal">default</code>, <code class="literal">housing</code>, <code class="literal">poutcome</code>Â and <code class="literal">loan</code> variables have a reasonable number of categories. We also explored the frequency of different categories for each nominal attribute as <code class="literal">selected_Data.groupBy(fieldName).count()</code>. The output provides the following observations:</p></li><li style="list-style-type: disc"><p>The sum of counts of different categories for each nominal variable is <code class="literal">41188</code> which means the nominal variables do not have any missing values.</p></li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec187"></a>There's moreâ€¦</h3></div></div></div><p>In the preceding recipe, we saw how to explore data. In the next recipe, let's see how to apply feature engineering.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec188"></a>See also</h3></div></div></div><p>Please refer to the <span class="emphasis"><em>Variable identification</em></span> recipe to understand the initial steps.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec52"></a>Feature engineering</h2></div></div><hr /></div><p>In this recipe, we'll see how to apply feature engineering on the explored data.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec189"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark cluster in any one of the modes, that is, local, standalone, YARN, and Mesos. For installing Spark on a standalone cluster, please refer to <a class="ulink" href="http://spark.apache.org/docs/latest/spark-standalone.html" target="_blank">http://spark.apache.org/docs/latest/spark-standalone.html</a>. Also, include the Spark MLlib package in the <code class="literal">build.sbt</code> file so that it downloads the related libraries and the API can be used. Install Hadoop (optionally), Scala, and Java.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec190"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>After data exploration, the next step is to perform feature engineering. Let's try to convert nominal variables into numeric types. Here is the code which does encoding for the nominal variables:</p><pre class="programlisting">        /* Applying One Hot encoding of Categorical Variables */
        // One Hot Encoding for Job
        val sqlFunc = udf(coder)
        val new_Df_WithDummyJob =
        create_DummyVariables(selected_Data, sqlFunc,"job",0)
        val new_Df_WithDummyMarital =
        create_DummyVariables(new_Df_WithDummyJob, sqlFunc,
        "marital", 0)
        val new_Df_WithDummyDefault =
        create_DummyVariables(new_Df_WithDummyMarital, sqlFunc,
        "default", 0)
        val new_Df_WithDummyHousing =
        create_DummyVariables(new_Df_WithDummyDefault, sqlFunc,
        "housing", 0)
        val new_Df_WithDummyPoutcome =
        create_DummyVariables(new_Df_WithDummyHousing, sqlFunc,
        "poutcome", 0)
        val new_Df_WithDummyLoan =
        create_DummyVariables(new_Df_WithDummyPoutcome, sqlFunc,
        "loan", 0)


</pre></li><li><p>Here is the function which creates <span class="emphasis"><em>n</em></span> columns for all the categorical variables having <span class="emphasis"><em>n</em></span> distinct categories:</p><pre class="programlisting">        def create_DummyVariables(df:DataFrame,
        udf_Func:UserDefinedFunction, variableType:String, i:Int ):
        DataFrame = {

        variableType match {
            case "job" =&gt; if (i == 12) df
                     else {
                     val df_new = df.withColumn (variableType + "_" +
                     i.toString, udf_Func(lit(variableType),col
                     ("job"), lit (i) ) )
                     create_DummyVariables (df_new, udf_Func,
                     variableType, i + 1)
                      }
            case "marital" =&gt; if(i == 4)  df
                     else {
                     val df_new = df.withColumn (variableType +
                     "_" + i.toString, udf_Func(lit(variableType),
                     col ("marital"), lit (i) ) )
                     create_DummyVariables (df_new, udf_Func,
                     variableType, i + 1)
                      }
           case "default" =&gt;  if(i == 3)  df
                     else {
                     val df_new = df.withColumn (variableType + "_"
                     + i.toString, udf_Func(lit(variableType),
                     col ("default"), lit (i) ) )
                     create_DummyVariables (df_new, udf_Func,
                     variableType, i + 1)
                      }
          case "housing" =&gt;  if(i == 3)  df
                     else {
                     val df_new = df.withColumn (variableType +
                     "_" + i.toString, udf_Func(lit(variableType),
                     col ("housing"), lit (i) ) )
                     create_DummyVariables (df_new, udf_Func,
                     variableType, i + 1)
                       }
          case "poutcome" =&gt;  if(i == 3)  df
                      else {
                      val df_new = df.withColumn (variableType +
                      "_" + i.toString, udf_Func(lit(variableType),
                       col ("poutcome"), lit (i) ) )
                       create_DummyVariables (df_new, udf_Func,
                       variableType, i + 1)
                        }
          case "loan" =&gt;  if(i == 3)  df
                     else {
                     val df_new = df.withColumn (variableType + "_" +
                     i.toString, udf_Func(lit(variableType),
                     col ("loan"), lit (i) ) )
                     create_DummyVariables (df_new, udf_Func,
                     variableType, i + 1)
                     } } }

</pre></li><li><p>Here is the definition for udf Â <code class="literal">coder</code> which returns values 0/1 for every distinct category:</p><pre class="programlisting">        val coder  = (variableType:String, columnValue:String,
        jobNo:Int) =&gt;
        variableType match{
        case "job" =&gt; columnValue match {
        case "unemployed" =&gt; if (jobNo == 0) 1.0 else 0.0
        case "services" =&gt; if (jobNo == 1) 1.0 else 0.0
        case "blue-collar" =&gt; if (jobNo == 2) 1.0 else 0.0
        case "unknown" =&gt; if (jobNo == 3) 1.0 else 0.0
        case "housemaid" =&gt; if (jobNo == 4) 1.0 else 0.0
        case "entrepreneur" =&gt; if (jobNo == 5) 1.0 else 0.0
        case "self-employed" =&gt; if (jobNo == 6) 1.0 else 0.0
        case "retired" =&gt; if (jobNo == 7) 1.0 else 0.0
        case "admin." =&gt; if (jobNo == 8) 1.0 else 0.0
        case "management" =&gt; if (jobNo == 9) 1.0 else 0.0
        case "technician" =&gt; if (jobNo == 10) 1.0 else 0.0
        case "student" =&gt; if (jobNo == 11) 1.0 else 0.0
        }
        case "marital" =&gt; columnValue match {
        case "unknown" =&gt; if(jobNo == 0) 1.0 else 0.0
        case "divorced" =&gt; if(jobNo == 1) 1.0 else 0.0
        case "single" =&gt; if(jobNo == 2) 1.0 else 0.0
        case "married" =&gt; if(jobNo == 3) 1.0 else 0.0
        }
        case "default" =&gt; columnValue match {
        case "unknown" =&gt; if(jobNo == 0) 1.0 else 0.0
        case "no" =&gt; if(jobNo == 1) 1.0 else 0.0
        case "yes" =&gt; if(jobNo == 2) 1.0 else 0.0
        }
        case "housing" =&gt; columnValue match {
        case "unknown" =&gt; if(jobNo == 0) 1.0 else 0.0
        case "no" =&gt; if(jobNo == 1) 1.0 else 0.0
        case "yes" =&gt; if(jobNo == 2) 1.0 else 0.0
        }
        case "poutcome" =&gt; columnValue match {
        case "nonexistent" =&gt; if(jobNo == 0) 1.0 else 0.0
        case "failure" =&gt; if(jobNo == 1) 1.0 else 0.0
        case "success" =&gt; if(jobNo == 2) 1.0 else 0.0
        }
        case "loan" =&gt; columnValue match {
        case "unknown" =&gt; if(jobNo == 0) 1.0 else 0.0
        case "no" =&gt; if(jobNo == 1) 1.0 else 0.0
        case "yes" =&gt; if(jobNo == 2) 1.0 else 0.0
        } }
</pre></li><li><p>Now, as the dummy variables are created, let's remove the original categorical variables from the final data frame to fit into the logistic regression model:</p><pre class="programlisting">        val final_Df = new_Df_WithDummyLoan.drop("job")
              .drop("marital")
              .drop("default")
              .drop("housing").drop("loan").drop("poutcome")
        val indexerModel = new StringIndexer()
          .setInputCol("y")
          .setOutputCol("y_Index")
          .fit(final_Df)
        val indexedDf = indexerModel.transform(final_Df).drop("y")
        indexedDf.show(5)
</pre><p>The following is the output:</p><pre class="programlisting">      +----+----------+--------+------------+------+--------+
      |age |  duration|previous| empvarrate |job_0 |   job_1|
      +----+----------+--------+------------+------+--------+
      |56.0|   261.0  |     0.0|        1.1 |   0.0|     0.0|
      |57.0|   149.0  |     0.0|        1.1 |   0.0|     1.0|
      |37.0|   226.0  |     0.0|        1.1 |   0.0|     1.0|
      |40.0|   151.0  |     0.0|        1.1 |   0.0|     0.0|
      |56.0|   307.0  |     0.0|        1.1 |   0.0|     1.0|
      +---------------+-----------+----------------+--------+
      +------+-----+
      |job_2 |job_3|
      +------+-----+
      |  0.0|   0.0|
      |0.0  |  0.0 |
      |0.0  |  0.0 |
      |0.0  |  0.0 |
      |0.0  |  0.0 |
      +-----+------+
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec191"></a>How it worksâ€¦</h3></div></div></div><p>Since there is no missing value treatment, we directly applied feature engineering. As a part of this, since the categorical variables have reasonable categories, there is no need to create broader category fields out of any nominal field. Next, all the categorical variables need to be converted to numeric types. For this, a <code class="literal">create_DummyVariables(df:DataFrame, udf_Func:UserDefinedFunction, variableType:String, i:Int ): DataFrame</code> function is created which takes the DataFrame, user defined function, categorical variable and iteration count as input.</p><p>For each categorical variable job, marital, default, housing, poutcome, and loan, the dummy variables such as <code class="literal">marital_0</code>, <code class="literal">marital_1</code>, <code class="literal">marital_2</code>, <code class="literal">marital_3</code> are created in an iterative fashion as <code class="literal">val df_new = df.withColumn (variableType + "_" + i.toString</code>, <code class="literal">udf_Func (lit(variableType)</code>, <code class="literal">col ("marital"), lit (i) ) )</code> and <code class="literal">create_DummyVariables (df_new, udf_Func, variableType, i + 1)</code>. This is done for all six categorical variables. The user defined function generates values for the dummy variables using a match clause. For example, the housing variable has three unique categories; <code class="literal">unknown</code>, <code class="literal">yes</code>Â and <code class="literal">no</code>Â corresponding to which three variables- <code class="literal">housing_0 (represents 'unknown')</code>, <code class="literal">housing_1 (represents 'no')</code> and <code class="literal">housing_2 (represents 'yes')</code> are created as part of recursive function <code class="literal">create_DummyVariables</code>.</p><p>After this, the categorical variables are dropped using the statement <code class="literal">val final_Df = new_Df_WithDummyLoan.drop("job").drop("marital").drop("default").drop("housing").drop("loan").drop("poutcome")</code>. Next, for the target variable <code class="literal">y</code> representing <code class="literal">yes</code> or <code class="literal">no</code> we used <code class="literal">StringIndexer</code> in Spark. The statement Â <code class="literal">val indexerModel = new StringIndexer().setInputCol("y").setOutputCol("y_Index").fit(final_Df)</code>Â creates a <code class="literal">StringIndexerModel</code> which takes the input column (<code class="literal">nominal variable</code>) and output column which contains the unique representation for each category.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec192"></a>There's moreâ€¦</h3></div></div></div><p>In the above recipe, we saw how to apply feature engineering. In the next recipe, let's see how to apply the algorithm on the final dataset.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec193"></a>See also</h3></div></div></div><p>Please refer to the <span class="emphasis"><em>Variable identification</em></span> and <span class="emphasis"><em>Data exploration</em></span> recipes to understand the initial steps.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec53"></a>Applying logistic regression</h2></div></div><hr /></div><p>In this recipe, we'll see how to apply Logistic regression.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec194"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark cluster in any one of the modes, that is, local, standalone, YARN, and Mesos. For installing Spark on a standalone cluster, please refer to <a class="ulink" href="http://spark.apache.org/docs/latest/spark-standalone.html" target="_blank">http://spark.apache.org/docs/latest/spark-standalone.html</a>. Also, include the Spark MLlib package in the <code class="literal">build.sbt</code> file so that it downloads the related libraries and the API can be used. Install Hadoop (optionally), Scala, and Java.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec195"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>The final step is splitting the DataFrame/RDD into train and test sets and applying logistic regression on the training set:</p><pre class="programlisting">
        val final_Rdd  =  indexedDf.rdd.map {
        row =&gt;
        val age = row.getAs[Double]("age")
        val duration = row.getAs[Double]("duration")
        val previous = row.getAs[Double]("previous")
        val empvarrate = row.getAs[Double]("empvarrate")
        val job_0 = row.getAs[Double]("job_0")
        val job_1 = row.getAs[Double]("job_1")
        val job_2 = row.getAs[Double]("job_2")
        val job_3 = row.getAs[Double]("job_3")
        val job_4 = row.getAs[Double]("job_4")
        val job_5 = row.getAs[Double]("job_5")
        val job_6 = row.getAs[Double]("job_6")
        val job_7 = row.getAs[Double]("job_7")
        val job_8 = row.getAs[Double]("job_8")
        val job_9 = row.getAs[Double]("job_9")
        val job_10 = row.getAs[Double]("job_10")
        val job_11 = row.getAs[Double]("job_11")
        val marital_0 = row.getAs[Double]("marital_0")
        val marital_1 = row.getAs[Double]("marital_1")
        val marital_2 = row.getAs[Double]("marital_2")
        val marital_3 = row.getAs[Double]("marital_3")
        val default_0 = row.getAs[Double]("default_0")
        val default_1 = row.getAs[Double]("default_1")
        val default_2 = row.getAs[Double]("default_2")
        val housing_0 = row.getAs[Double]("housing_0")
        val housing_1 = row.getAs[Double]("housing_1")
        val housing_2 = row.getAs[Double]("housing_2")
        val poutcome_0 = row.getAs[Double]("poutcome_0")
        val poutcome_1 = row.getAs[Double]("poutcome_1")
        val poutcome_2 = row.getAs[Double]("poutcome_2")
        val loan_0 = row.getAs[Double]("loan_0")
        val loan_1 = row.getAs[Double]("loan_1")
        val loan_2 = row.getAs[Double]("loan_2")
        val label = row.getAs[Double]("y_Index")
        val featurecVec = Vectors.dense(Array(age,duration,previous,
        empvarrate,job_0,job_1,job_2,job_3,job_4,job_5,job_6,job_7,
        job_8,job_9,job_10,job_11,marital_0
        marital_1,marital_2,marital_3,
        default_0,default_1,default_2,housing_0,housing_1,housing_2,
        poutcome_0,poutcome_1,poutcome_2,loan_0,loan_1,loan_2))
        LabeledPoint(label, featurecVec) }
        val splits = final_Rdd.randomSplit(Array(0.8,0.2))
        val training = splits(0).cache()
        val test = splits(1)
        val model = new LogisticRegressionWithLBFGS()
        .setNumClasses(2)
        .run(training)
        // Compute raw scores on the test set.
        val predictionAndLabels = test.map { case LabeledPoint(label,
        features) =&gt;
        val prediction = model.predict(features)
        (prediction, label) }
        // Get evaluation metrics.
        val metrics = new MulticlassMetrics(predictionAndLabels)
        val precision = metrics.precision
        println("Precision = "+precision)
</pre><p>The following is the output:</p><pre class="programlisting">       Precision = 0.908132347594231
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec196"></a>How it worksâ€¦</h3></div></div></div><p>As a final step of applying the algorithm, the DataFrame is converted to RDD. The output <code class="literal">y_Index</code> variable is the label and from the remaining variables, dense vector is created. Once the <code class="literal">RDD[LabeledPoint]</code> is generated, the line <code class="literal">final_Rdd.randomSplit(Array(0.8,0.2))</code> splits the RDD into training and testing containing 80% and 20% of the records, respectively. The line <code class="literal">val model = new LogisticRegressionWithLBFGS().setNumClasses(2).run(training)</code> runs the algorithm on the training dataset. From the test set, the model predicts the label from the features as <code class="literal">model.predict(features)</code>. After this, using <code class="literal">MultiClassMetrics</code>, the precision of the model is displayed.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec197"></a>There's moreâ€¦</h3></div></div></div><p>Once the model is built, the initially created hypothesis will be tested and data needs to be sufficient to test all the scenarios. In the preceding case study, we applied the logistic regression model from MLlib, Spark has support for <code class="literal">ml</code> pipelines as well which we can use to apply algorithms directly on the DataFrame. Also, there is support for evaluating different models built in the <code class="literal">org.apache.spark.mllib.evaluation</code> package. There are other flavors of regression such as ridge regression, stepwise regression, <span class="strong"><strong>Ordinary Least Squares Regression</strong></span> (<span class="strong"><strong>OLSR</strong></span>), multivariate regression and so on. Spark MLlib offers several regression algorithms with different estimation techniques.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec198"></a>See also</h3></div></div></div><p>Please visit the recipes <span class="emphasis"><em>Working with Spark programming model</em></span>, <span class="emphasis"><em>Working with Spark's Python and Scala shells</em></span>, <span class="emphasis"><em>Working with pair RDDs </em></span>in <a class="link" href="#" linkend="ch03">Chapter 1</a>, <span class="emphasis"><em>Big Data Analytics with Spark</em></span>. To understand more about statistics, go through the elementary concepts of statistics at <a class="ulink" href="http://www.statsoft.com/Textbook/Elementary-Statistics-Concepts" target="_blank">http://www.statsoft.com/Textbook/Elementary-Statistics-Concepts</a>.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec54"></a>Real-time intrusion detection using streaming k-means</h2></div></div><hr /></div><p>Clustering analysis is the task of grouping a set of objects in such a way that objects in the same group (cluster) are more similar than those in other clusters. It is one of the subjective modeling techniques widely used in the industry. One example of its usage is segmenting customer portfolios based on demographics, transaction behavior, or other behavioral attributes. Clustering generates natural clusters and is not dependent on any of the driving objective functions. Once the clustering does initial profiling of the portfolio, the objective modeling technique can be used to build a specific strategy.</p><p>There are a number of clustering algorithms such as hierarchical clustering, k-means clustering, spectral clustering, DBSCAN and so on. This recipe shows how to detect an anomaly from the network data based on the clustering technique.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec55"></a>Variable identification</h2></div></div><hr /></div><p>In this recipe, we'll see how to identify the required variables for analysis and understand their description.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec199"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark cluster in any one of the modes, that is, local, standalone, YARN, or Mesos. For installing Spark on a standalone cluster, please refer to <a class="ulink" href="http://spark.apache.org/docs/latest/spark-standalone.html" target="_blank">http://spark.apache.org/docs/latest/spark-standalone.html</a>. Also, include the Spark MLlib package in the <code class="literal">build.sbt</code> file so that it downloads the related libraries and the API can be used. Install Hadoop (optionally), Scala, and Java.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec200"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Let's take an example of network data which contains information on network access and a variety of network intrusions. This is the NSL-KDD dataset, which is a refined version of the KDD'99 dataset. Although the dataset has intrusions being represented as labels, we use the k-means clustering algorithm which is an unsupervised learning approach to cluster the dataset into normal and four major attack categories, that is, DoS, Probe, R2L and U2R.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note19"></a>Note</h3><p>Please download the dataset from the following location: <a class="ulink" href="https://github.com/ChitturiPadma/datasets/blob/master/KDD_Data.csv" target="_blank">https://github.com/ChitturiPadma/datasets/blob/master/KDD_Data.csv</a></p></div></li><li><p>Here is a description of some of the variables that we are going to use in our analysis:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Variable</strong></span></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Description</strong></span></p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">duration</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Length (number of seconds) of the connection</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">protocol_type</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Type of the protocol for example, TCP, UDP and so on.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">service</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Network service at the destination for example, HTTP, telnet and so on.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">src_bytes</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Number of data bytes from source to destination</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">dst_bytes</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Number of data bytes from destination to source</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">flag</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Normal or error status of the connection</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">land</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>1 if connection is from/to same host/port and 0 otherwise</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">wrong_fragment</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Number of wrong fragments</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">urgent</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Number of urgent packets</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">hot</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Number of hot indicators</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">num_failed_logins</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Number of failed login attempts</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; ">
<p><code class="literal">logged_in</code></p>
</td><td style="">
<p>1 if successfully logged in; 0 otherwise</p>
</td></tr></tbody></table></div><p>There are a total of 43 variables and 148517 rows. For the description of other variables, please visit: <a class="ulink" href="https://github.com/ChitturiPadma/datasets/blob/master/FieldNames" target="_blank">https://github.com/ChitturiPadma/datasets/blob/master/FieldNames</a>.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note20"></a>Note</h3><p>To learn about the attack types, please refer to:Â 
<a class="ulink" href="https://github.com/ChitturiPadma/datasets/blob/master/Attack_Types" target="_blank">https://github.com/ChitturiPadma/datasets/blob/master/Attack_Types</a></p></div></li><li><p>The data resides in HDFS. Let's load the data into Spark and see the values corresponding to different variables:</p><pre class="programlisting">      import org.apache.spark._
      import org.apache.spark.mllib.linalg.Vectors
      import org.apache.spark.sql._
      import org.apache.spark.sql.functions._
      import org.apache.spark.sql.DataFrameNaFunctions
      import org.apache.spark.sql.types._
      import org.apache.spark.mllib.stat.
      {MultivariateStatisticalSummary, Statistics}

      object Variable_Identification {
      def main(args:Array[String]): Unit = {
      val conf = new SparkConf()
        .setMaster("spark://master:7077")
        .setAppName("Variable_Identification")
      val sc = new SparkContext(conf)
      val sqlContext = new SQLContext(sc)
      import sqlContext.implicits._
      //Loading data
      val network_data =
      sqlContext.read.format("com.databricks.spark.csv")
        .option("header", "true")
        .option("inferSchema","true")
        .load("hdfs://namenode:9000/KDD_Data.csv")
      network_data.show(5)
      } }
</pre><p>The following is the output:</p><pre class="programlisting">    +--------+-------------+--------+------+----------+---------+
    |duration|protocol_type|service |  flag|src_bytes |dst_bytes|
    +--------+-------------+--------+------+----------+---------+
    |       0|          tcp|ftp_data|    SF|      491 |        0|
    |       0|          udp|other   |    SF|       146|        0|
    |       0|          tcp|private |    S0|         0|        0|
    |       0|          tcp|http    |    SF|       232|     8153|
    |       0|          tcp|http    |    SF|       199|      420|
    +---------------+-----------+----------------+---------------+



    +----+--------------+
    |land|wrong_fragment|
    +----+--------------+
    |  0 |            0 |
    |  0 |            0 |
    |  0 |            0 |
    |  0 |            0 |
    |  0 |            0 |
    +----+--------------+
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec201"></a>How it worksâ€¦</h3></div></div></div><p>In the preceding code snippet, since the file is ofÂ CSVÂ format, we imported the data initially into Spark using theÂ CSVÂ package. The schema is inferred and variables are identified. The <code class="literal">network_data.show(5)</code> line displays the first five records. From this sample data, the independent and dependent variables are identified. There are in total 41 attributes and a few sample attributes <code class="literal">duration</code>, <code class="literal">protocol_type</code>, <code class="literal">service</code>, <code class="literal">flag</code>, <code class="literal">src_bytes</code>, <code class="literal">dst_bytes</code>, <code class="literal">land</code> and <code class="literal">wrong_fragment</code> are displayed in the output.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec202"></a>There's moreâ€¦</h3></div></div></div><p>In subsequent recipes, let's look at how to simulate file data as real-time streaming data, explore, and apply online k-means.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec203"></a>See also</h3></div></div></div><p>Please refer to the <span class="emphasis"><em>Producer code generating real-time data</em></span> and <span class="emphasis"><em>Apply streaming k-means</em></span> recipes for the subsequent steps.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec56"></a>Simulating real-time data</h2></div></div><hr /></div><p>In this recipe, we'll see how to simulate real-time data.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec204"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need Kafka and Zookeeper running on the cluster. Install Scala and Java.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec205"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Since the data is available in files, let's simulate the data in real time using a producer which writes the data into Kafka. Here is the code:</p><pre class="programlisting">      import java.util.{Date, Properties}
      import kafka.javaapi.producer.Producer
      import kafka.producer.KeyedMessage
      import kafka.producer.ProducerConfig
      import org.apache.spark.mllib.linalg.Vectors
      import scala.io.{BufferedSource, Source}
      import scala.util.Random

      object KafkaProducer {
         def main(args:Array[String]): Unit ={
           val random:Random = new Random
           val props = new Properties
       props.put("metadata.broker.list","172.22.128.16:9092")
       props.put("serializer.class","kafka.serializer.StringEncoder")
       props.put("request.required.acks","1")
       val config:ProducerConfig =  new ProducerConfig(props)
       val producer:Producer[String,String] = new
       Producer[String,String](config)
       val (fileObject1, fileObject2) =
          getFileObject(hdfs://namenode:9000/KDD_Data.csv")
       val lines = readFile(fileObject1,fileObject2)
          while(true)
               {
           val record = lines(random.nextInt(148517))
            if(!record.contains("duration")) {
           val parts = record.split(",").toBuffer
           parts.remove(42)
           parts.remove(1,3)
           val msg = parts.toArray.mkString(",")
           val data:KeyedMessage[String,String] =  new
           KeyedMessage[String,String]    ("test",msg)
           producer.send(data) } }
           producer.close
            }
       def readFile(buffer1:BufferedSource):List[String]  =
       buffer1.getLines().toList
       def getFileObject(path1:String)
      :BufferedSource=Source.fromFile(path1)
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec206"></a>How it worksâ€¦</h3></div></div></div><p>The <code class="literal">KafkaProducer</code> code simulates the network intrusion data in real time. For this, kafka and zookeeper must be running on the cluster. The line, <code class="literal">val props = new Properties</code> creates a properties object. The kafka broker and serializer information is set in the properties object as <code class="literal">props.put("metadata.broker.list","172.22.128.16:9092")</code>. The file from HDFS is read and a loop is run continuously which reads a random record from the file as <code class="literal">val record = lines(random.nextInt(148517))</code>. If the record is not the header, the last column representing the unique value for attack is removed as <code class="literal">parts.remove(42)</code>. The categorical <code class="literal">protocol_type</code>, <code class="literal">service</code>Â and <code class="literal">flag</code> variables are also removed using the line <code class="literal">parts.remove(1,3)</code> as k-means clustering needs only numeric fields. Now the record is transformed to a <code class="literal">KeyedMessage</code> type using the line, <code class="literal">val data:KeyedMessage[String,String] = new KeyedMessage[String,String]("test",msg)</code>. The message is sent to kafka using the line, <code class="literal">producer.send(data)</code>. This happens iteratively and sends a random record to Kafka.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec207"></a>There's moreâ€¦</h3></div></div></div><p>In the preceding recipe, we saw how to write producer code that simulates the streaming data. In practical scenarios, the data would be coming either from a port, website, or even from a database. The change in records can trigger real-time events. In the next recipe, let's see how to process real-time streaming data and apply a streaming k-means algorithm.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec208"></a>See also</h3></div></div></div><p>Please refer to the <span class="emphasis"><em>Applying streaming k-means</em></span> recipe for the subsequent steps.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec57"></a>Applying streaming k-means</h2></div></div><hr /></div><p>In this recipe, we'll see how to applyÂ online k-means on streaming data.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec209"></a>Getting ready</h3></div></div></div><p>To work through this recipe, you will need a running Spark cluster in any one of the modes, that is, local, standalone, YARN, or Mesos. For installing Spark on a standalone cluster, please refer to <a class="ulink" href="http://spark.apache.org/docs/latest/spark-standalone.html" target="_blank">http://spark.apache.org/docs/latest/spark-standalone.html</a>. Also, include the Spark MLlib package in the <code class="literal">build.sbt</code> file so that it downloads the related libraries and the API can be used. Install Hadoop (optionally), Scala, and Java. This recipe also requires Kafka and Zookeeper running on the cluster. We are going to run the algorithm on real data.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec210"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Let's try to build a real-time network detection system using Spark streaming and MLlib. Here is the code which processes the real time data, performs pre-processing and applies k-means algorithm on live data:</p><pre class="programlisting">      import java.net.InetAddress
      import _root_.kafka.serializer.StringDecoder
      import kafka.server.KafkaApis
      import org.apache.spark._
      import org.apache.spark.streaming._
      import org.apache.spark.streaming.StreamingContext._
      import org.apache.spark.streaming.kafka._
      import org.apache.spark.broadcast._
      import org.apache.spark.mllib.linalg._
      import org.apache.spark.mllib.clustering._
      import org.apache.spark.rdd.RDD
      import org.apache.spark.mllib.regression.LabeledPoint
      import org.joda.time.DateTime

      object AnomalyDetection_StreamingKMeans {

      def distance(featureVector:Vector, centroid:Vector)=
      math.sqrt(featureVector.toArray.zip(centroid.toArray).map{case
      (vec1,vec2) =&gt; vec1-vec2}.map(diff =&gt; diff*diff).sum)

      def distToCentroid(featureVector:Vector,
      broadCastModel:Broadcast[StreamingKMeansModel]):Double={
      val model = broadCastModel.value
      val cluster = model.predict(featureVector)
      val centroid = model.clusterCenters(cluster)
      distance(featureVector, centroid)
       }

      def getCurrentDateTime:Long =  DateTime.now.getMillis
      def getIpAddress = InetAddress.getLocalHost.getHostAddress

      def normalize(dataum:Vector, means:Array[Double],
      stdevs:Array[Double]):Vector = {
      val normalizedArray = (dataum.toArray, means, stdevs).zipped.map{
      (value, mean, stdev ) =&gt; if(stdev &lt;=0) (value-mean) else (value-
       mean)/stdev
       }
      Vectors.dense(normalizedArray)
       }
      def main(args:Array[String]): Unit ={
      println("Entering Streaming K-Means Application")
      val conf = new SparkConf().setMaster("spark://master:7077")
      .setAppName("Anomaly-Detection_System")
      val sc = new SparkContext(conf)
      val ssc = new StreamingContext(sc, Seconds(6))
      val topicName = "test"
      val topicSet = topicName.split(",").toSet
      val brokerName = getIpAddress+":"+"9092"
      val kafkaParams = Map[String,String]("metadata.broker.list" -&gt;
      brokerName)
      val inputDstream = KafkaUtils.createDirectStream[String, String,
      StringDecoder, StringDecoder](ssc, kafkaParams,
      topicSet).map(_._2)

      val labelsAndData = inputDstream.map{dataPoint =&gt; val parts =
      dataPoint.split(",").toBuffer
      val label = parts.remove(parts.length-1)
      val labelModified = label match {

      case str:String=&gt; if(str == "back" || str == "neptune" ||
          str  == "land" || str == "smurf" || str == "pod" || str ==
          "teardrop" || str == "apache2" || str == "udpstorm" || str ==
          "processtable" || str == "mailbomb" || str == "worm")
          "DoS"

        else if(str == "nmap" || str == "ipsweep" || str == "satan" ||
        str == "portsweep" || str == "mscan" || str == "saint")
        "Probe"
        else if(str == "multihop" || str == "ftp_write" || str ==
        "guess_passwd" || str == "phf" || str == "spy" || str ==
        "warezclient" || str == "imap" || str == "warezmaster" || str
        == "snmpgetattack" || str == "snmpguess"  || str ==
        "httptunnel" || str == "sendmail" || str == "xlock" || str ==
        "named" || str == "xsnoop")
         "R2L"

        else if (str == "loadmodule" || str == "rootkit" || str ==
        "buffer_overflow" || str == "perl" || str == "xterm" || str ==
        "sqlattack" || str == "ps" )
          "U2R"
        else if(str=="normal") "normal"
        else "unknown"
       }

      val vector = Vectors.dense(parts.map(_.toDouble).toArray)
      (labelModified, (dataPoint,vector))
      }
      val normalizedDStream =
      labelsAndData.transform{labelsAndVectorsRdd
      =&gt;
      val rddVector = labelsAndVectorsRdd.values.values
      val dataAsArray =  rddVector.map(_.toArray)
      val n = dataAsArray.count
      val sums = dataAsArray.reduce((a,b) =&gt; a.zip(b).map
      (t =&gt; t._1 +t._2))
      val means = sums.map(_/n)
      val meanBroadcasted = sc.broadcast(means)
      val dataWithMeanDiffSquared =  dataAsArray.map{features =&gt; val
      meanValue =  meanBroadcasted.value
      val length = features.length
      val featuresMapped = (0 to length-1).map{i =&gt; val diff =
      features(i)-meanValue(i)
      diff*diff}
      featuresMapped.toArray}
      val squaredSumReduced = dataWithMeanDiffSquared.reduce((a,b) =&gt;
      a.zip(b).map(t =&gt;
      t._1+t._2) )
      val variance = squaredSumReduced.map(_/n)
      val stdevs = variance.map(ele =&gt; math.sqrt(ele))
      val stdevBroadcasted = sc.broadcast(stdevs)
      val normalizedData = labelsAndVectorsRdd.map{case(label,
      (original,vector)) =&gt;
      val meanValue = meanBroadcasted.value
      val stdevValue = stdevBroadcasted.value
      (label,(original,normalize(vector, meanValue, stdevValue)))
        }
      normalizedData
         }.cache()
       val streaming_k_means = new StreamingKMeans()
       .setK(5)
       .setDecayFactor(1.0)
       .setRandomCenters(38,0.0)
       streaming_k_means.trainOn(normalizedDStream.map(_._2).map(_._2))
       val model = streaming_k_means.latestModel()
       val broadCastedKMeans = sc.broadcast(model)
       normalizedDStream.foreachRDD{
       labelsAndVectorsRdd =&gt;
       val distances =  labelsAndVectorsRdd.map{
       case(label, (original, dataVector)) =&gt;
       distToCentroid(dataVector,broadCastedKMeans)}
       val threshold = distances.top(100).last
       val detectedAnomalies = labelsAndVectorsRdd.filter{
       case(label, (original, dataVector)) =&gt;
       distToCentroid(dataVector,broadCastedKMeans) &gt; threshold
       }
       val mappedDetectedAnomalies = detectedAnomalies.map{case(label,
       (original, dataVector)) =&gt; (label, "anomaly")}
       val detectedAnomalyCount =
       mappedDetectedAnomalies.count.toDouble
       val realAnomalies =
       mappedDetectedAnomalies.filter{case(originalLabel,
       predictedLabel) =&gt; originalLabel!="normal"}
       val correctAnomalyCount = realAnomalies.count.toDouble
       println("Accuracy is:"+correctAnomalyCount/detectedAnomalyCount)
       }
        ssc.start()
        ssc.awaitTermination()
        ssc.stop()
       }
      }
</pre><p>The following is the output:</p><p>Entering streamingÂ k-means application</p><div class="mediaobject"><img src="graphics/image_04_001.jpg" /></div></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec211"></a>How it worksâ€¦</h3></div></div></div><p>The <code class="literal">AnomalyDetection_StreamingKMeans</code> code initializes the <code class="literal">Dstream</code> to read data from <code class="literal">Kafka</code> using the line, <code class="literal">KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, topicSet).map(_._2)</code>. Next, the <code class="literal">Dstream</code> is transformed such that the line, <code class="literal">val label = parts.remove(parts.length-1)</code> removes the label and the label is mapped to four major attack categories; <code class="literal">DoS</code>, <code class="literal">Probe</code>, <code class="literal">U2R</code>, <code class="literal">R2L</code>Â and <code class="literal">normal</code>. The features are mapped to a dense vector as <code class="literal">val vector = Vectors.dense(parts.map(_.toDouble).toArray)</code> and the <code class="literal">Dstream </code><code class="literal">labelsAndData</code> contains a modified label, original record and vector. The Dstream <code class="literal">labelsAndData</code> is transformed to perform the following operation-the line <code class="literal">val sums = dataAsArray.reduce((a,b) =&gt; a.zip(b).map(t =&gt; t._1 +t._2))</code> adds all the feature vectors to a single vector (represented as array) and mean is calculated as <code class="literal">val</code><code class="literal">means = sums.map(_/n)</code>. For each vector in the <code class="literal">rdddataAsArray</code>, the square of difference from the mean is calculated as <code class="literal">val featuresMapped = (0 to length-1).map{i =&gt; val diff = features(i)-meanValue(i).diff*diff</code>.</p><p>From the calculated difference, variance is calculated using the lines, <code class="literal">val squaredSumReduced = dataWithMeanDiffSquared.reduce((a,b) =&gt; a.zip(b).map(t =&gt; t._1+t._2) )</code> and <code class="literal">val variance = squaredSumReduced.map(_/n)</code>. Also, standard deviation is calculated using the line, <code class="literal">val stdevs = variance.map(ele =&gt; math.sqrt(ele))</code>. Using the calculated standard deviation and mean, the feature vectors are normalized with the <code class="literal">formula (x-mu)/sigma</code>. This is performed in the function, <code class="literal">normalize(dataum:Vector, means:Array[Double], stdevs:Array[Double]):Vector</code>. The streaming k-means object is initialized as <code class="literal">val streaming_k_means = new StreamingKMeans().setK(3).setDecayFactor(1.0).setRandomCenters(38,0.0)</code>.</p><p>Now the algorithm is applied on the <code class="literal">normalizedDStream</code> as <code class="literal">streaming_k_means.trainOn(normalizedDStream.map(_._2).map(_._2))</code>. Using the built model, the distance of each feature vector to its centroid as <code class="literal">distToCentroid(dataVector,broadCastedKMeans)</code>. The threshold distance is defined as <code class="literal">val threshold = distances.top(100).last</code>. Any feature vector (data point) whose distance to centroid greater than the threshold is the anomaly and this is done using the following line: <code class="literal">distToCentroid(dataVector,broadCastedKMeans) &gt; threshold</code>. Finally, to calculate the accuracy, the count of predicted labels which are the same as the original label is found out (which are not normal) and the <code class="literal">correctAnomalyCount/detectedAnomalyCount</code> ratio gives the percentage of correct anomalies detected.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec212"></a>There's moreâ€¦</h3></div></div></div><p>For the preceding dataset, although there is no missing value treatment, there is feature engineering applied, which created a broader category-four attacks instead of having the actual labels, which are 21 in number. The interesting point is that we applied streaming k-means which is mini batch k-means available in Python Scikit learn. MLlib includes a variation called streaming k-means, which can update a clustering incrementally as new data arrives in a <code class="literal">StreamingKMeansModel</code>. This could be used to continuously learn, approximately, how new data affects the clustering and not just assess new data against existing clusters. It integrates with Spark streaming.</p><p>Here we applied the simplistic model. For example, the Euclidean distance is used in this example because it is the only distance function supported by Spark MLlib at this time. In future, it may be possible to use distance functions that can better account for the distributions of, and correlations between, features, such as the Mahalanobis distance. Coming to the model evaluation part, there are more sophisticated cluster quality evaluation metrics that could be applied, even without labels, to pick k, such as the Silhouette coefficient. These tend to evaluate not just closeness of points within one cluster, but closeness of points to other clusters.</p><p>Finally, different models could be applied too, instead of simple k-means clustering; for example, a Gaussian mixture model or DBSCAN could capture more subtle relationships between data points and the cluster centers.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec213"></a>See also</h3></div></div></div><p>Please visit the earlier <span class="emphasis"><em>Working with Spark programming model</em></span>, <span class="emphasis"><em>Work with Spark's Python and Scala shells</em></span>, <span class="emphasis"><em>Working with pair RDDs</em></span> recipes from <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Big Data Analytics with Spark</em></span> to become more familiar with Spark. Also, to learn how to work with linear and logistic regression, please refer to the earlier <span class="emphasis"><em>Apply regression analysis for sales data</em></span> and <span class="emphasis"><em>Apply logistic regression on bank marketing data</em></span> recipes. To understand more about statistics, go through the elementary concepts in statistics from <a class="ulink" href="http://www.statsoft.com/Textbook/Elementary-Statistics-Concepts" target="_blank">http://www.statsoft.com/Textbook/Elementary-Statistics-Concepts</a>.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch05"></a>ChapterÂ 5.Â Working with Spark MLlib</h2></div></div></div><p>In this chapter, you will learn about the MLlib component of Spark. We will cover the following recipes:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Implementing Naive Bayes classification</p></li><li style="list-style-type: disc"><p>Implementing decision trees</p></li><li style="list-style-type: disc"><p>Building a recommendation system</p></li><li style="list-style-type: disc"><p>Implementing logistic regression using Spark ML pipelines</p></li></ul></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec58"></a>Introduction</h2></div></div><hr /></div><p>MLlib is the <span class="strong"><strong>machine learning</strong></span> (<span class="strong"><strong>ML</strong></span>) library that is provided with Apache Spark, the in-memory, cluster-based, open source data processing system. In this chapter, I will examine the functionality of algorithms provided within the MLlib library in terms of areas of machine learning tasks such as classification, recommendation, and neural processing. For each algorithm, we'll provide working examples that tackle real problems. We will take a step-by-step approach in describing how the following algorithms can be used, and what they are capable of doing.</p><p>Big data and machine learning takes place in three steps-collect, analyze and predict. For this purpose, the Spark ecosystem supports a wide range of workloads, including batch applications, iterative algorithms, interactive queries, and stream processing. The Spark MLlib component offers a variety of ML algorithms which are scalable.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec59"></a>Working with Spark ML pipelines</h2></div></div><hr /></div><p>Spark MLlib's goal is to make practical ML scalable and easy. Similar to Spark Core, MLlib provides APIs in three languages that is, Python, Scala, and Java-with example code which will ease the learning curve for users coming from different backgrounds. The pipeline API in MLlib provides a uniform set of high-level APIs built on top of DataFrames that helps users create and tune practicalÂ ML pipelines. This API is under a new package with nameÂ <code class="literal">spark.ml</code>.</p><p>MLlib standardizes APIs for machine learning algorithms to make it easier to combine multiple algorithms into a single pipeline or workflow. Let's see the key terms introduced by the pipeline API:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><span class="strong"><strong>DataFrame</strong></span>: The ML API uses DataFrame from Spark SQL as an ML dataset, which can hold a variety of data types. For example, a DataFrame could have different columns storing text, feature vectors, true labels and predictions.</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Transformer</strong></span>: A transformer is an algorithm which can transform one DataFrame into another DataFrame. For example, an ML model is a transformer which transforms a DataFrame with features into a DataFrame with predictions.</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Estimator</strong></span>: An estimator is an algorithm which can be fit on a DataFrame to produce a Transformer. For example, a learning algorithm is an Estimator which trains on a DataFrame and produces a model.</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Pipeline</strong></span>: A pipeline chains multiple transformers and estimators together to specify an ML workflow.</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>P</strong></span>
<span class="strong"><strong>arameter</strong></span>: All transformers and estimators share a common API for specifying parameters.</p></li></ul></div><p>The following are the pipeline components:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><span class="strong"><strong>Transformer</strong></span>: It is an abstraction which includes feature transformers and learned models. A transformer internally implements a Â <code class="literal">transform()</code> method which converts one DataFrame into another by appending one or more columns. For example:
</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>A feature transformer might take a DataFrame, read a column, map it into a new column (feature vectors) and output a new DataFrame with the mapped column appended.</p></li><li style="list-style-type: disc"><p>A learning model might take a DataFrame, read the column containing feature vectors, predict the label for the each feature vector and results in a DataFrame with predicted labels as a field.</p></li></ul></div></li><li style="list-style-type: disc"><p><span class="strong"><strong>Estimator</strong></span>: It abstracts the concept of a learning algorithm or any algorithm that fits or trains on data. An estimator implements the <code class="literal">fit()</code> methodÂ which accepts a DataFrame and produces the model (which is a transformer).</p></li></ul></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec60"></a>Implementing Naive Bayes' classification</h2></div></div><hr /></div><p>Naive Bayes is a simple probabilistic classifier based on the Bayes theorem. This classifier is capable of calculating the most probable output depending on the input. It is possible to add new raw data at runtime and have a better probabilistic classifier. The Naive Bayes model is typically used for classification. There will be a bunch of features <span class="emphasis"><em>X1, X2,....Xn</em></span> observed for an instance. The goal is to infer to which class among the limited set of classes the particular instance belongs. This model makes the assumption that every pair of features <span class="emphasis"><em>Xi</em></span> and <span class="emphasis"><em>Xj</em></span> is conditionally independent given the class. This classifier is a sub-class of Bayesian networks. For more information about the classifier, please refer to <a class="ulink" href="http://www.statsoft.com/textbook/naive-bayes-classifier" target="_blank">http://www.statsoft.com/textbook/naive-bayes-classifier</a>.</p><p>This recipe shows how to run the Naive Bayes classifier on the <code class="literal">weather</code> dataset using the Naive Bayes classifier algorithm available in the Spark MLlib package. The code is written in Scala.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec214"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark cluster in any one of the modes, that is, local, standalone, YARN, or Mesos. For installing Spark on a standalone cluster, please refer <a class="ulink" href="http://spark.apache.org/docs/latest/spark-standalone.html" target="_blank">http://spark.apache.org/docs/latest/spark-standalone.html</a>. Also, include the Spark MLlib package in the <code class="literal">build.sbt</code> file so that it downloads the related libraries and the API can be used. Install Hadoop (optionally), Scala, and Java. Please download the dataset from the following location: <a class="ulink" href="https://github.com/ChitturiPadma/datasets/blob/master/weather.csv" target="_blank">https://github.com/ChitturiPadma/datasets/blob/master/weather.csv</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec215"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Let us see how to implement Naive Baye's classification:</p><pre class="programlisting">      import org.apache.spark.{SparkContext, SparkConf}
      import org.apache.spark.mllib.classification.{NaiveBayes,
      NaiveBayesModel}
      import org.apache.spark.mllib.linalg.Vectors
      import org.apache.spark.mllib.regression.LabeledPoint
      object NaiveBayesSample {
      def main (args:Array[String]): Unit =
      {
      val conf = new SparkConf
      conf.setMaster("spark://master:7077").setAppName
      ("NaiveBayesSample")
      val sc = new SparkContext(conf)
      val data =
      sc.textFile("hdfs://namenode:9000/datasets/weather.csv")
      val parsedData = data.map{line =&gt; val parts = line.split(",")
      val label = if(parts(0)=="overcast") 0.0 else
      if(parts(0)=="rainy") 1.0 else 2.0
      val feature1 = parts(1).toDouble
      val feature2 = parts(2).toDouble
      val feature3 = if(parts(3)=="FALSE") 0.0 else 1.0
      LabeledPoint(label,
      Vectors.dense(Array(feature1,feature2,feature3)))}
      // Split data into training (60%) and test (40%)
      val splits = parsedData.randomSplit(Array(0.6,0.4), seed = 11L)
      val training = splits(0)
      val test = splits(1)
      val model = NaiveBayes.train(training, lambda = 1.0, modelType =
      "multinomial")
      val predictionAndLabel = test.map(p =&gt;
      (model.predict(p.features), p.label))
      val accuracy = 1.0*predictionAndLabel.filter(x =&gt; x._1 ==
      x._2).count()/test.count()
      println("Accuracy is..."+accuracy)
      //Save and Load model
      model.save(sc, "hdfs://namenode:9000/models/myNaiveBayesModel")
      val sameModel = NaiveBayesModel.load(sc,
      "hdfs://namenode:9000/models/myNaiveBayesModel")
        }
      }
</pre></li><li><p>The weather dataset <code class="literal">weather.txt</code>Â looks like this:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Season</strong></span></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Temperature</strong></span></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Humidity</strong></span></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Windy</strong></span></p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>overcast</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>83</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>86</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>FALSE</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>overcast</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>64</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>65</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>TRUE</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>overcast</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>72</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>90</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>TRUE</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>overcast</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>81</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>75</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>FALSE</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>rainy</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>70</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>96</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>FALSE</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>rainy</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>68</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>80</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>FALSE</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>rainy</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>65</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>70</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>TRUE</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>rainy</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>75</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>80</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>FALSE</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>rainy</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>71</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>91</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>TRUE</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>sunny</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>85</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>85</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>FALSE</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>sunny</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>80</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>90</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>TRUE</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>sunny</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>72</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>95</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>FALSE</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>unny</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>69</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>70</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>FALSE</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; ">
<p>sunny</p>
</td><td style="border-right: 0.5pt solid ; ">
<p>75</p>
</td><td style="border-right: 0.5pt solid ; ">
<p>70</p>
</td><td style="">
<p>TRUE</p>
</td></tr></tbody></table></div></li><li><p>Convert the data into numerics as follows:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Season</strong></span></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Temperature</strong></span></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Humidity</strong></span></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Windy</strong></span></p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>0</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>83</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>86</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>0</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>0</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>64</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>65</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>1</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>0</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>72</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>90</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>1</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>0</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>81</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>75</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>0</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>1</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>70</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>96</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>0</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>1</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>68</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>80</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>0</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>1</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>65</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>70</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>1</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>1</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>75</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>80</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>0</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>1</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>71</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>91</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>1</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>2</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>85</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>85</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>0</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>2</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>80</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>90</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>1</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>2</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>72</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>95</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>0</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>2</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>69</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>70</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>0</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; ">
<p>2</p>
</td><td style="border-right: 0.5pt solid ; ">
<p>75</p>
</td><td style="border-right: 0.5pt solid ; ">
<p>70</p>
</td><td style="">
<p>1</p>
</td></tr></tbody></table></div></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec216"></a>How it worksâ€¦</h3></div></div></div><p>The preceding code snippet <code class="literal">NaiveBayesSample</code> implements multinomial Naive Bayes. It takes an RDD of <code class="literal">LabeledPoint</code> and an optional smoothing parameter <code class="literal">lambda</code> as input and also an optional model type parameter <code class="literal">multinomial</code>. It outputs <code class="literal">NaiveBayesModel</code>, which is used for evaluation and prediction. <code class="literal">LabeledPoint</code> is a class that represents the features and labels of a data point. These labeled points are used in supervised leaning algorithms such as regression and classification. For multiclass classification, labels should be of class indices starting from 0, 1, 2, ..... Hence, the weather data is converted to numeric and is given as input for the algorithm.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec217"></a>There's moreâ€¦</h3></div></div></div><p>The labeled point takes a label and local vector either dense or sparse as input. To get familiar with local vectors, labeled points and so on, please refer toÂ <a class="ulink" href="http://spark.apache.org/docs/latest/mllib-data-types.html" target="_blank">http://spark.apache.org/docs/latest/mllib-data-types.html</a> in Spark MLlib package. Also, refer <a class="ulink" href="http://spark.apache.org/docs/latest/mllib-statistics.html" target="_blank">http://spark.apache.org/docs/latest/mllib-statistics.html</a> for information on basic statistics.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec218"></a>See also</h3></div></div></div><p>Please visit the <span class="emphasis"><em>Working with the Spark programming model</em></span>, <span class="emphasis"><em>Working with Spark's Python and Scala shells</em></span>Â and <span class="emphasis"><em>Working with pair RDDs</em></span> recipes in <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Big Data Analytics with Spark</em></span> to get familiar with Spark. To understand more about statistics, go through the elementary concepts in statistics from <a class="ulink" href="http://www.statsoft.com/Textbook/Elementary-Statistics-Concepts" target="_blank">http://www.statsoft.com/Textbook/Elementary-Statistics-Concepts</a>.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec61"></a>Implementing decision trees</h2></div></div><hr /></div><p>Decision trees are the most widely used data mining machine learning algorithm in practice for classification and regression. They are easy to interpret, handle categorical features and extend to the multiclass classification. This decision tree model, which is a powerful, non-probabilistic technique, captures more complex nonlinear patterns and feature interactions. Their outcome is quite understandable. They are not hard to use since it's not required to tweak a lot of parameters.</p><p>This recipe shows how to run the decision tree on web content which evaluates a large set of URLs and classifies them as <span class="emphasis"><em>ephemeral</em></span> (that is, short-lived and will cease being popular soon) or <span class="emphasis"><em>evergreen</em></span> (that last for longer time). It is available in the Spark MLlib package. The code is written in Scala.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec219"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark cluster in any one of the modes, that is, local, standalone, YARN, or Mesos. For installing Spark on a standalone cluster, please refer toÂ <a class="ulink" href="http://spark.apache.org/docs/latest/spark-standalone.html" target="_blank">http://spark.apache.org/docs/latest/spark-standalone.html</a>. Also, include the Spark MLlib package in the <code class="literal">build.sbt</code> file so that it downloads the related libraries and the API can be used. Install Hadoop (optionally), Scala, and Java. Please download the dataset from <a class="ulink" href="http://www.kaggle.com/c/stumbleupon/data" target="_blank">http://www.kaggle.com/c/stumbleupon/data</a>. Download the training data (<code class="literal">train.tsv</code>); you will need to accept the terms and conditions before downloading the dataset.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec220"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Before we begin, remove the column name header from the first line of the file. Change to the directory in which you downloaded the data and run the following to remove the first line and pipe the result to a new file called <code class="literal">train_noheader.tsv</code>:</p><pre class="programlisting">        sed 1d train.tsv &gt; train_noheader.tsv
</pre></li><li><p>Here is the code that implements Decision trees:</p><pre class="programlisting">
        import org.apache.spark.{SparkContext, SparkConf}
        import org.apache.spark.mllib.tree.DecisionTree
        import org.apache.spark.mllib.tree.model.DecisionTreeModel
        import org.apache.spark.mllib.tree.impurity._
        import org.apache.spark.mllib.linalg.Vectors
        import org.apache.spark.mllib.regression.LabeledPoint
        import org.apache.spark.mllib.tree.configuration.Algo
        object DecisionTreeSample {
        def main(args:Array[String])
        {
        val conf = new SparkConf
        conf.setMaster("spark://master:7077")
          .setAppName("DecisionTreeSample")
        val sc = new SparkContext(conf)
        val rawData = sc.textFile("hdfs://namenode:9000/datsets/
                      train_noheader.tsv")
        val records = rawData.map(line =&gt; line.split("\t"))
        val data = records.map{record =&gt;
        val trimmed = record.map(_.replaceAll(""", ""))
        val label = trimmed(record.size-1).toInt
        val features = trimmed.slice(4,record.size-1).map(d =&gt;
            if(d=="?")  0.0 else   d.toDouble)
        LabeledPoint(label, Vectors.dense(features))}
        val maxTreeDepth = 5
        val dtModel = DecisionTree.train(data, Algo.Classification,
        Entropy, maxTreeDepth)
        val predictions = dtModel.predict(data.map(lp =&gt; lp.features))
        predictions.take(5).foreach(println)
          }
       }
</pre><p>The following is the output:</p><pre class="programlisting">    0.0
    0.0
     .0
    0.0
    1.0
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec221"></a>How it worksâ€¦</h3></div></div></div><p>The dataset contains the URL and ID of the page in the first two columns. The other columns contain textual content and the final column contains the target - 1 is evergreen, while 0 is non-evergreen. All the available numeric features are used directly. Initially, data cleaning is done by trimming out the extra quotation <code class="literal">"</code>. For the missing values denoted by <code class="literal">"?"</code>, zero is assigned to these missing values.</p><p>In the preceding code snippet, the label variable is extracted from the last column and an array of features for columns 5 to 25 (after cleaning and dealing with missing values). The Label is converted to a <code class="literal">Int</code> value and the features to an <code class="literal">Array[Double]</code>. Now, the label and the features are wrapped in a <code class="literal">LabeledPoint</code>, converting the features into a MLlib vector. The data <code class="literal">(RDD[LabeledPoint])</code> is trained using <code class="literal">DecisionTree</code>. Finally, <code class="literal">predictions</code> are made using the <code class="literal">DecisiconTreeModel</code>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec222"></a>There's moreâ€¦</h3></div></div></div><p>The tree complexity has a crucial effect on its accuracy. The decision tree is a top-down approach that begins at a root node (or feature) and then selects at each step the feature that gives the best split of the dataset, as measured by the information gain of this split. The information gain is computed from the node impurity (which is the extent to which the labels at the node are similar, or homogenous) minus the weighted sum of the impurities for the two child nodes that would be created by the split. For classification tasks, there are two measures to select the best split - Gini impurity and entropy.</p><p>When we make predictions using the model, we should be able to evaluate how well the model performs. Hence, prediction error and accuracy could be calculated. The performance of the model is determined by calculating the average classification accuracy as the total number of correctly classified instances divided by the total number of data points.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec223"></a>See also</h3></div></div></div><p>For more about decision trees, please refer the Spark documentation at <a class="ulink" href="http://spark.apache.org/docs/latest/mllib-decision-tree.html" target="_blank">http://spark.apache.org/docs/latest/mllib-decision-tree.html</a>. The earlier <span class="emphasis"><em>Implementing Naive Bayes classification</em></span> recipe also talks about classification based on Bayes' theorem.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec62"></a>Building a recommendation system</h2></div></div><hr /></div><p>Recommendation engines are one of the types of machine learning algorithms. Often, people might have experienced them using the popular websites such as Amazon, Netflix, YouTube, Twitter, LinkedIn and Facebook. The idea behind recommendation engines is to predict what people might like and to uncover relationships between the items to aid in the discovery process.</p><p>Recommender systems are widely studied and there are many approaches such as - content-based filtering and collaborative filtering. Other approaches, such as ranking models, have also gained popularity. Since Spark's recommendation models only include an implementation of matrix factorization, this recipe shows how to run <span class="strong"><strong>matrix factorization</strong></span> on rating datasets from the MovieLens website.</p><p>The algorithm is available in the Spark MLLib package. The code is written in Scala.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec224"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark cluster in any one of the modes, that is, local, standalone, YARN, or Mesos. For installing Spark on a standalone cluster, please refer toÂ <a class="ulink" href="http://spark.apache.org/docs/latest/spark-standalone.html" target="_blank">http://spark.apache.org/docs/latest/spark-standalone.html</a>. Also, include the Spark MLlib package in the <code class="literal">build.sbt</code> file so that it downloads the related libraries and the API can be used. Install Hadoop (optionally), Scala, and Java. Please download the dataset from <a class="ulink" href="http://files.grouplens.org/datasets/movielens/ml-100k.zip" target="_blank">http://files.grouplens.org/datasets/movielens/ml-100k.zip</a>. This is a 100k dataset, which is a set of 100,000 data points related to ratings given by a set of users to a set of movies. It also contains movie metadata and user profiles.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec225"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>In the following code snippet, <span class="strong"><strong>alternating least squares</strong></span> (<span class="strong"><strong>ALS</strong></span>), which solves matrix factorization problems, is used to train the model. This ALS achieves good performance and it's also easy to implement in a parallel fashion. Develop a Spark standalone application using the Eclipse IDE as follows:</p><pre class="programlisting">      import org.apache.spark.{SparkContext, SparkConf}
      import org.apache.spark.mllib.recommendation.ALS
      import org.apache.spark.mllib.recommendation.Rating
      object RecommendationSample {
      def main(args:Array[String]): Unit =
      {
      //Initalize SparkConf and SparkContext
      val conf = new SparkConf
      conf.setMaster("spark://master:7077")
      .setAppName("Alternating_Least_Squares")
      val sc = new SparkContext(conf)
      //Load and parse the data
      val rawData = sc.textFile("hdfs://namenode:9000/datsets/u.data")
      val ratings = rawData.map(line =&gt; line.split("\t").take(3) match
      {
      case Array(userid, movieid, rating) =&gt; Rating(userid.toInt,
      movieid.toInt, rating.toDouble)
      })
      val rank = 10
      val numIterations = 10
      val model = ALS.train(ratings, rank, numIterations, 0.01)
      val userId = 789
      val movieId = 123
      val predictedRating = model.predict(userId, movieId)
      val K=10
      val topKRecs = model.recommendProducts(userId, K)
      println("Predicted Rating: " +predictedRating+" for User:
      "+userId)
      println("top-K recommended Items:")
      println(topKRecs.mkString("\n"))
      //Evaluate the model on training data
     val userProducts = ratings.map{case Rating(user, product, rate) =&gt;
      (user,product)}
      val predictions = model.predict(userProducts).map{case
      Rating(user,product,rate) =&gt;   ((user,product), rate)}
      val ratesAndPreds = ratings.map{case Rating(user,product,rate) =&gt;
      ((user,product),   rate)}.join(predictions)
      val meanSquaredError = ratesAndPreds.map{case ((user,product),
      (r1,r2)) =&gt;
      val err = r1 -r2
          err*err}.mean
      println("Mean Squared Error: "+meanSquaredError)
        }
      }
</pre><p>The following is the output:</p><pre class="programlisting">    Predicted Rating: 3.7145698398131506 for User: 789
    top-K recommended Items:
    Rating(789,634,10.417101374782789)
    Rating(789,390,9.698498529021368)
    Rating(789,793,9.075930316942125)
    Rating(789,1093,8.939263466151157)
    Rating(789,1184,8.758673390571044)
    Rating(789,1316,8.247478149972812)
    Rating(789,1496,8.020321930691964)
    Rating(789,962,7.638944782465387)
    Rating(789,1283,7.638183734722482)
    Rating(789,624,7.629865917211151)
    Mean Squared Error: 0.48259870392275334
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec226"></a>How it worksâ€¦</h3></div></div></div><p>The dataset consists of user ID, movie ID, rating, and timestamp fields separated by tabs (<code class="literal">\t</code>). The <code class="literal">val ratings = rawData.map(line =&gt; line.split("\t").take(3) match {case Array(userid, movieid, rating) =&gt; Rating(userid.toInt, movieid.toInt, rating.toDouble)})</code> line extracts the first three fields using <code class="literal">split</code> and transform the array of IDs and ratings into a <code class="literal">Rating</code> object. The <code class="literal">Rating</code> class is a wrapper around user ID, movie ID and the actual rating arguments.</p><p>Now, the <code class="literal">ALS.train</code> method (trains the ALS model) requires parameters such as <code class="literal">rank</code> (number of factors in ALS model), <code class="literal">iterations</code> (number of iterations to run) and <code class="literal">lambda</code> (controls the regularization of the model).</p><p>The <code class="literal">model.predict</code> computes the predicted score for a given <code class="literal">user</code> and <code class="literal">item</code>. The <code class="literal">model.recommendProducts</code> generates top-K recommended items for a user. Finally mean-squared error is calculated, which gives the performance of the ALS model.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec227"></a>There's moreâ€¦</h3></div></div></div><p>As said in the beginning, the most prevalent recommendation model is content-based filtering, which uses the content or attributes of an item, finds similarity between two pieces of content and generates items similar to a given item. The other one is collaborative filtering, in which there is a user-based approach and an item-based approach. The matrix factorization model takes a 2-D matrix with users as rows and items as columns. This 2-D matrix is represented as the product of two smaller matrices (factor matrices) and the prediction happens by computing the dot product between row of a user-factor matrix and the relevant row of an item-factor matrix. There is also the implicit matrix factorization method, which takes implicit feedback and computes the recommendation.</p><p>Once the model is generated, it is essential to evaluate its predictive capability or accuracy. There are some direct measures that determine how well a model predicts such as mean squared error. The Mean average precision determines how best the model performs at predicting things, which is not directly optimized in the model.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec228"></a>See also</h3></div></div></div><p>For more about collaborative filtering, please refer to the Spark documentation <a class="ulink" href="http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html" target="_blank">http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html</a>. The documentation also contains references to the papers that underlie the ALS algorithm implemented on each component of explicit and implicit data. The earlier <span class="emphasis"><em>Implementing decision trees</em></span> recipe also talks about classification models.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec63"></a>Implementing logistic regression using Spark ML pipelines</h2></div></div><hr /></div><p>In this recipe, let's see how to run logistic regression algorithms using Spark ML pipelines.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec229"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark cluster in any one of the modes, that is, local, standalone, YARN, or Mesos. For installing Spark on a standalone cluster, please refer <a class="ulink" href="http://spark.apache.org/docs/latest/spark-standalone.html" target="_blank">http://spark.apache.org/docs/latest/spark-standalone.html</a>. Also, include the Spark MLlib package in the <code class="literal">build.sbt</code> file so that it downloads the related libraries and the API can be used. Install Hadoop (optionally), Scala, and Java.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note21"></a>Note</h3><p>Please download the dataset from the following location: <a class="ulink" href="https://github.com/ChitturiPadma/datasets/blob/master/Community_Dataset.csv" target="_blank">https://github.com/ChitturiPadma/datasets/blob/master/Community_Dataset.csv</a>.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec230"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Let's have a look at some of the records in the dataset. The first column corresponds to the label and the other three columns are the features:</p><div class="mediaobject"><img src="graphics/B05317_05_01-1.jpg" /></div></li><li><p>Here is the code which creates a DataFrame out of the previous data and creates an instance for <code class="literal">LogisticRegression</code>:</p><pre class="programlisting">      import org.apache.spark._
      import org.apache.spark.rdd._
      import org.apache.spark.sql._
      import org.apache.spark.sql.functions._
      import org.apache.spark.mllib.linalg._
      import org.apache.spark.ml.classification.LogisticRegression
      import org.apache.spark.ml.param.ParamMap
      import org.apache.spark.sql.Row
      object LogisticRegression_MLPipeline {
      def main(args:Array[String]): Unit = {
      val conf = new SparkConf()
        .setMaster("spark://master:7077")
        .setAppName("Logistic_MLPipeline")
      val sc = new SparkContext(conf)
      val sqlContext = new SQLContext(sc)
      import sqlContext.implicits._

      //Loading data
      val community_data =
      sqlContext.read.format("com.databricks.spark.csv")
        .option("inferSchema", "true")
      .load("hdfs://namenode:9000/Community_Dataset.csv")

      /* function that returns 0.0 is string is "PA" and 1.0 if string
        is "MP" */
      val func = udf((s:String) =&gt; if(s== "PA") 0.0 else 1.0)
      val final_data = community_data.withColumn("label",func($"C0")
      .as("label")).drop("C0")
      final_data.show(5) } }
</pre><p>The following is the output:</p><pre class="programlisting">     +---+---+---+-----+
     | C1| C2| C3|label|
     +---+---+---+-----+
    |640|  5|6.0|  0.0|
    |140|  2|5.5|  0.0|
    |405|  2|7.5|  0.0|
    |389|  3|7.0|  0.0|
    |359|  3|5.7|  0.0|
    +---+---+---+-----+
</pre></li><li><p>Let's now create DataFrame from the <code class="literal">final_data</code> RDD and also an instance for <code class="literal">LogisticRegression</code> as follows:</p><pre class="programlisting">       val training = final_data.rdd.map{
       row =&gt; val feature1 = row.getInt(0).toDouble
       val feature2 = row.getInt(1).toDouble
       val feature3 = row.getDouble(2)
       val label = row.getAs[Double]("label")
       (label,Vectors.dense(feature1,feature2,feature3))
        }.toDF("label","features")
       //Create instance for the LogisticRegression
       val lr = new LogisticRegression()

       //Display the parameters and any any default values
       println("LogisticRegression parameters:\n" + lr.explainParams()
       + "\n")
</pre><p>The following is the output:</p><pre class="programlisting">    LogisticRegression parameters:
    elasticNetParam: the ElasticNet mixing parameter, in range [0, 1].
    For alpha = 0, the penalty is an L2 penalty. For alpha = 1,
    it is an
    L1 penalty (default:   0.0)
    featuresCol: features column name (default: features)
    fitIntercept: whether to fit an intercept term (default: true)
    labelCol: label column name (default: label)
    maxIter: maximum number of iterations (&gt;= 0) (default: 100)
</pre></li><li><p>Let's set the parameters and try to fit the model for the DataFrame training as follows:</p><pre class="programlisting">    //Set the parameters
    lr.setMaxIter(10)
    .setRegParam(0.01)
    //Fit the model
    val model1 = lr.fit(training)
    println("Model 1 was fit using parameters: " +
    model1.parent.extractParamMap)
</pre><p>The following is the output:</p><pre class="programlisting">    Model 1 was fit using parameters: {
      logreg_584f918c00a8-elasticNetParam: 0.0,
      logreg_584f918c00a8-featuresCol: features,
      logreg_584f918c00a8-fitIntercept: true,
      logreg_584f918c00a8-labelCol: label,
      logreg_584f918c00a8-maxIter: 10,
      logreg_584f918c00a8-predictionCol: prediction,
      logreg_584f918c00a8-probabilityCol: probability,
      logreg_584f918c00a8-rawPredictionCol: rawPrediction,
      logreg_584f918c00a8-regParam: 0.01,
      logreg_584f918c00a8-standardization: true,
      logreg_584f918c00a8-threshold: 0.5,
      logreg_584f918c00a8-tol: 1.0E-6,
      logreg_584f918c00a8-weightCol: }
</pre></li><li><p>There is also an alternative way of specifying the parameters using a <code class="literal">ParamMap</code> and learning a new model as follows:</p><pre class="programlisting">      //Alternative way of specifying the parameters using a ParamMap
      val paramMap = ParamMap(lr.maxIter -&gt; 20)
      .put(lr.maxIter, 30)
      // Specify 1 Param. This overwrites the  original maxIter.
      .put(lr.regParam -&gt; 0.1, lr.threshold -&gt; 0.55)
      // Specify multiple Params.
      // Also can combine ParamMaps.
      val paramMap2 = ParamMap(lr.probabilityCol -&gt; "myProbability")
      // Change output column name.
      val paramMapCombined = paramMap ++ paramMap2
      //Learn a new model using the paramMapCombined parameters.
      /* paramMapCombined overrides all parameters set earlier via
      lr.set* methods */
      val model2 = lr.fit(training, paramMapCombined)
      println("Model 2 was fit using parameters: " +
      model2.parent.extractParamMap)
</pre><p>The following is the output:</p><pre class="programlisting">    Model 2 was fit using parameters: {
      logreg_5d7f5ebdfbc2-elasticNetParam: 0.0,
      logreg_5d7f5ebdfbc2-featuresCol: features,
      logreg_5d7f5ebdfbc2-fitIntercept: true,
      logreg_5d7f5ebdfbc2-labelCol: label,
      logreg_5d7f5ebdfbc2-maxIter: 30,
      logreg_5d7f5ebdfbc2-predictionCol: prediction,
      logreg_5d7f5ebdfbc2-probabilityCol: myProbability,
      logreg_5d7f5ebdfbc2-rawPredictionCol: rawPrediction,
      logreg_5d7f5ebdfbc2-regParam: 0.1,
      logreg_5d7f5ebdfbc2-standardization: true,
      logreg_5d7f5ebdfbc2-threshold: 0.55,
      logreg_5d7f5ebdfbc2-tol: 1.0E-6,
      logreg_5d7f5ebdfbc2-weightCol: }
</pre></li><li><p>Now let's prepare the test data and make predictions using the <code class="literal">Transformer.transform()</code> method. Here is the code for the same:</p><pre class="programlisting">    // Prepare test data.
    val test = sqlContext.createDataFrame(Seq(
      (1.0, Vectors.dense(400, 5, 8.7)),
      (0.0, Vectors.dense(500, 5, 11.9)),
      (1.0, Vectors.dense(650, 6, 7.8))
    )).toDF("label", "features")
    model2.transform(test)
      .select("features", "label", "myProbability", "prediction")
      .collect()
      .foreach { case Row(features: Vector, label: Double, prob:
   Vector, prediction:Double) =&gt;
      println(s"($features, $label) -&gt; prob=$prob,
   prediction=$prediction")
      }
</pre><p>The following is the output:</p><pre class="programlisting">      ([400.0,5.0,8.7], 1.0) -&gt; prob=
      [0.2179020321435895,0.7820979678564105], prediction=1.0
      ([500.0,5.0,11.9], 0.0) -&gt; prob=
      [0.29104102896280887,0.7089589710371911], prediction=1.0
      ([650.0,6.0,7.8], 1.0) -&gt; prob=
      [0.1620630814622409,0.8379369185377591],   prediction=1.0
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec231"></a>How it worksâ€¦</h3></div></div></div><p>In the preceding code snippet, we saw how to run <code class="literal">LogisticRegression</code> using an ML pipeline. Initially, the data is loaded using the Spark CSV package and a column <code class="literal">label</code> is created which contains <code class="literal">0.0</code> when the string in the column <code class="literal">C0</code> is <code class="literal">PA</code> and <code class="literal">1.0</code> when the string is <code class="literal">MP</code>. From the DataFrame <code class="literal">final_data</code>, the DataFrame <code class="literal">training</code> is obtained, which contains the variable <code class="literal">label</code> and feature <code class="literal">vector</code> (using <code class="literal">Vectors.dense)</code>.</p><p>Next, an instance of <code class="literal">LogisticRegression</code> is created as <code class="literal">val lr = new LogisticRegression()</code>. The default parameters of the <code class="literal">lr</code> are displayed as <code class="literal">lr.explainParams()</code>. The parameters, such as the maximum number of iterations and regularization parameter are set as <code class="literal">lr.setMaxIter(10).setRegParam(0.01)</code>. The model is fit on the <code class="literal">training</code> data as <code class="literal">val model1 = lr.fit(training)</code>. The parameters of the model are displayed as <code class="literal">model1.parent.extractParamMap</code>. We also saw alternative ways of specifying the parameters using <code class="literal">ParamMap</code>. The test data is prepared and the <code class="literal">transform</code> method (which uses the <code class="literal">features</code> column) outputs the probability as <code class="literal">myProbability</code> and the respective prediction.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec232"></a>There's moreâ€¦</h3></div></div></div><p>A pipeline is a sequence of stages where each stage is either a transformer or an estimator. The stages run in order, and the input DataFrame is transformed as it passes through each stage. For transformer stages, the <code class="literal">transform()</code> method is called on the DataFrame. For estimator stages, the <code class="literal">fit()</code> method is called to produce a transformer (which becomes part of the <code class="literal">PipelineModel</code>, or fitted <code class="literal">Pipeline</code>), and that transformer's <code class="literal">transform()</code> method is called on the DataFrame.</p><p>A pipeline's stages are specified as an ordered array. It is possible to create linear as well as nonlinear pipelines as long as the data flow graph forms a <span class="strong"><strong>Directed Acyclic Graph</strong></span> (<span class="strong"><strong>DAG</strong></span>). Since pipelines can operate on DataFrames with varied types, they cannot use compile-time type checking. <code class="literal">Pipelines</code> and <code class="literal">PipelineModels</code> instead do runtime checking before actually running the pipeline. This type checking is done using the DataFrame schema, a description of the data types of columns in the DataFrame. Also, a pipeline's stages should be unique instances.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec233"></a>See also</h3></div></div></div><p>The algorithms, such as decision tree, random forest, gradient-boosted tree and so on have implementations in Spark ML pipelines.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch06"></a>ChapterÂ 6.Â NLP with Spark</h2></div></div></div><p>In this chapter, we will see how to run NLP algorithms over Spark. You will learn the following recipes:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Installing NLTK on Linux</p></li><li style="list-style-type: disc"><p>Installing Anaconda on Linux</p></li><li style="list-style-type: disc"><p>Anaconda for cluster management</p></li><li style="list-style-type: disc"><p>POS tagging with PySpark on an Anaconda cluster</p></li><li style="list-style-type: disc"><p>Named Entity Recognition with IPython over Spark</p></li><li style="list-style-type: disc"><p>Implementing openNLP - chunker over Spark</p></li><li style="list-style-type: disc"><p>Implementing openNLP - sentence detector over Spark</p></li><li style="list-style-type: disc"><p>Implementing stanford NLP - lemmatization over Spark</p></li><li style="list-style-type: disc"><p>Implementing sentiment analysis using stanford NLP over Spark</p></li></ul></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec64"></a>Introduction</h2></div></div><hr /></div><p>The study of natural language processing is called NLP. It is about the application of computers on different language nuances and building real-world applications using NLP techniques. NLP is analogous to teaching a language to a child. The most common tasks, such as understanding words and sentences, forming grammatically and structurally correct sentences are natural to humans. In NLP, some of these tasks translate to tokenization, chunking, parts of speech tagging, parsing, machine translation and speech recognition and these are tough challenges for computers.</p><p>Currently, NLP is one of the rarest skill sets that is required in the industry. With the advent of big data, the major challenge is that there is a need for people who are good with not just structured, but also with semi or unstructured data. Petabytes of weblogs, tweets, Facebook feeds, chats, e-mails and reviews are generated continuously. Companies are collecting all these different kinds of data for better customer targeting and to gather meaningful insights. To process all these unstructured data at large scale, people need to understand NLP as well as get acquainted with processing data in distributed fashion (big data processing). In order to achieve functionalities of NLP applications such as spelling correction (MS Word), search engines (Google, Bing), news feeds (Google, Yahoo!) and any other applications that perform basic NLP tasks preprocessing, there are many tools available, such as GATE, Mallet, Open NLP, UMA, Stanford ToolKit and <span class="strong"><strong>Natural Language ToolKit</strong></span> (<span class="strong"><strong>NLTK</strong></span>).</p><p>NLTK is a comprehensive Python library for natural language processing and text analytics. It is often used for rapid prototyping of text processing programs and it can even be used in production applications. The different areas of application are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><span class="strong"><strong>Searching:</strong></span> This identifies specific elements of text. It simply finds occurrence of a name in a document or involves finding synonyms and alternates spelling/mis-spelling to find entries which are close to the original search string.</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Machine translation:</strong></span> This involves translating one natural language to another.</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Summation:</strong></span> This summarizes paragraphs, articles, documents or collections.</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Named Entity Recognition (NER):</strong></span> This extracts names, locations, people and things from text.</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Parts of Speech Tagging (POS):</strong></span> This splits up the text into different grammatical elements such as nouns and verbs useful in analyzing text further.</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Sentiment analysis:</strong></span> People's feelings and attitudes regarding movies, books and other products is determined using this technique. It is useful in providing automated feedback on how well a product is perceived.</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Speech recognition:</strong></span> This is responsible for recognizing text from the speech.</p></li></ul></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec65"></a>Installing NLTK on Linux</h2></div></div><hr /></div><p>In this recipe, we will see how to install NLTK on Linux. Before proceeding with the installation, let's consider the version of Python we're going to use. There are two versions or flavors of Python, namely Python 2.7.x and Python 3.x. Although the latest version, Python 3.x, appears to be the better choice, for scientific, numeric, or data analysis work, Python 2.7 is recommended.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec234"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you need Ubuntu 14.04 (Linux flavor) installed on the machine. Python comes pre-installed. The <code class="literal">python --version</code> command gives the version of the Python installed. If the version seems to be 2.6.x, upgrade it to Python 2.7 as follows:</p><pre class="programlisting">
<span class="strong"><strong>    sudo apt-get install python2.7</strong></span>
</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec235"></a>How to do itâ€¦</h3></div></div></div><p>Let's see the installation process for NLTK:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Once the Python 2.7.x version is available, install NLTK as follows:</p><pre class="programlisting">
<span class="strong"><strong>      sudo pip install -U nltk</strong></span>
</pre></li><li><p>The preceding installation may throw an error such as the following:</p><pre class="programlisting">
<span class="strong"><strong>       Could not find any downloads that satisfy the requirement nltk
       Cleaning up...
       No distributions at all found for nltk
       Storing debug log for failure in /home/padmac/.pip/pip.log
</strong></span>
</pre></li><li><p>If so, please try to install <code class="literal">nltk</code> as follows:</p><pre class="programlisting">
<span class="strong"><strong>        sudo pip install nltk
        DownloaDing/unpacking nltk
        Cannot fetch index base URL https://pypi.python.org/
        Downloading nltk-3.2.tar.gz (1.2MB): 1.2MB downloaded
        Running setup.py (path:/tmp/pip_build_root/
        warning: no files found matching 'Makefile' under directory
        '*.txt'
        warning: no previously-included files matching '*~' found
        anywhere in distribution
        Installing collected packages: nltk
        Running setup.py install for nltk
        warning: no files found matching 'Makefile' under directory
        '*.txt'
        warning: no previously-included files matching '*~' found
        anywhere in distribution
        Successfully installed nltk
        Cleaning up...
</strong></span>
</pre></li><li><p>TheÂ <code class="literal">numpy</code> package is optional to install:</p><pre class="programlisting">
<span class="strong"><strong>        sudo pip install -U numpy</strong></span>
</pre></li><li><p>Now test the installation by invoking Python interpreter and type the following:</p><pre class="programlisting">
<span class="strong"><strong>        python
        Python 2.7.6 (default, Mar 22 2014, 22:59:56)
        [GCC 4.8.2] on linux2
        Type "help", "copyright", "credits" or "license" for more
        information.
        &gt;&gt;&gt; import nltk
        &gt;&gt;&gt;
</strong></span>
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec236"></a>How it worksâ€¦</h3></div></div></div><p>The preceding commands install the <code class="literal">nltk</code> and <code class="literal">numpy</code> (optional) libraries. For older versions of Python, it might be necessaryÂ install setup tools (see <a class="ulink" href="http://pypi.python.org/pypi/setuptools" target="_blank">http://pypi.python.org/pypi/setuptools</a>) and to install pip (<code class="literal">sudo easy_install pip</code>).</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec237"></a>There's moreâ€¦</h3></div></div></div><p>The NLTK library is available as a part of Anaconda (continuum analytics). Anaconda is a Python distribution for data analytics. It supports a variety of numerical and scientific packages, such as <code class="literal">numpy</code>, <code class="literal">pandas</code>, <code class="literal">scipy</code>, <code class="literal">matpotlib</code>Â and <code class="literal">ipython</code>, with over 250 more packages available via a simple <code class="literal">conda install &lt;package-name&gt;</code>. Please refer to the following recipes below for installing Anaconda and Anaconda for cluster management.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec238"></a>See also</h3></div></div></div><p>Please refer toÂ <a class="link" href="#" linkend="ch02">Chapter 2</a>, 
<span class="emphasis"><em>Tricky Statistics with Spark</em></span> to know how to work with Pandas on the Spark framework. Similarly, refer toÂ <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Big Data Analytics with Spark</em></span> to get familiar with Spark.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec66"></a>Installing Anaconda on Linux</h2></div></div><hr /></div><p>Anaconda is a free, enterprise-ready Python distribution for data analytics, processing and scientific computing. In this recipe, we will see how to install Anaconda on Linux. Before proceeding with the installation, let's consider the version of Python we're going to use. There are two versions or flavors of Python, namely Python 2.7.x and Python 3.x. Although the latest version, Python 3.x, appears to be the better choice, for scientific, numeric, or data analysis work, Python 2.7 is recommended.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec239"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you need Ubuntu 14.04 (Linux flavor) installed on the machine. Python comes pre-installed. <code class="literal">python --version</code> gives the version of the Python installed. If the version is 2.6.x, upgrade it to Python 2.7 as follows:</p><pre class="programlisting">
<span class="strong"><strong>    sudo apt-get install python2.7</strong></span>
</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec240"></a>How to do itâ€¦</h3></div></div></div><p>Once Python version 2.7.x is available, download the Anaconda installer from <a class="ulink" href="https://www.continuum.io/downloads" target="_blank">https://www.continuum.io/downloads</a> and type the following in the terminal window at the path where Anaconda has been downloaded:</p><pre class="programlisting">
<span class="strong"><strong>    bash ~/path/Anaconda2-2.5.0-Linux-x86_64.sh</strong></span>
</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec241"></a>How it worksâ€¦</h3></div></div></div><p>The preceding command installs Anaconda and the following line is added in <code class="literal">.bashrc</code> by the Anaconda installer as follows:</p><pre class="programlisting">
<span class="strong"><strong>    export PATH="/home/padma/anaconda2/bin:$PATH"</strong></span>
</pre><p>Now close and reopen the terminal for the changes to take effect. In order to ensure that <code class="literal">conda</code> is successfully installed, enter the following <code class="literal">conda -version</code>Â command which results in the version as <code class="literal">conda 3.19.1</code>.</p><p>To update <code class="literal">conda</code> to the current version, use the <code class="literal">update</code> command to update conda:</p><pre class="programlisting">
<span class="strong"><strong>    conda 3.19.1</strong></span>
</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec242"></a>There's moreâ€¦</h3></div></div></div><p>Anaconda includes an easy installation of Python and updates over scientific and analytic Python packages that include <code class="literal">numpy</code>, <code class="literal">pandas</code>, <code class="literal">scipy</code>, <code class="literal">matpotlib</code>, and <code class="literal">ipython</code>, with over 250 more packages available via a simple <code class="literal">conda install &lt;package-name&gt;</code>. The Anaconda platform is language-specific as well as extensible. It provides integrations such as Cloudera, Amazon AWS, Microsoft Azure, Docker, VM Depot, Vagrant and so on.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec243"></a>See also</h3></div></div></div><p>For more details on Anaconda (continuum analytics), please visit <a class="ulink" href="http://docs.continuum.io/anaconda/index" target="_blank">http://docs.continuum.io/anaconda/index</a>.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec67"></a>Anaconda for cluster management</h2></div></div><hr /></div><p>Anaconda for cluster management provides resource management tools which allow users to easily create, provision and manage bare-metal or cloud-based clusters. It enables the management of conda environments on clusters and provides integration, configuration and setup management of Hadoop services. This can be installed alongside enterprise Hadoop distributions such as Cloudera CDH or Hortonworks HDP and this is used to manage conda packages and environments across a cluster.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec244"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you need Ubuntu 14.04 (Linux flavor) installed on the machine. Python comes pre-installed. <code class="literal">python --version</code> gives the version of Python installed. If the version is 2.6.x, upgrade it to Python 2.7 as follows:</p><pre class="programlisting">
<span class="strong"><strong>    sudo apt-get install python2.7</strong></span>
</pre><p>For installing Anaconda, please refer to the earlier <span class="emphasis"><em>Installing Anaconda on Linux</em></span> recipe.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec245"></a>How to do itâ€¦</h3></div></div></div><p>Let's look at the installation process for installing Anaconda for cluster management:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>You can create a new environment in conda with Python 2.7 using the following command:</p><pre class="programlisting">
<span class="strong"><strong>         conda create -n acluster python=2.7</strong></span>
</pre></li><li><p>Activate the <code class="literal">acluster</code> environment as follows:</p><pre class="programlisting">
<span class="strong"><strong>        source activate acluster</strong></span>
</pre></li><li><p>Install the command-line client for Anaconda cloud as follows:</p><pre class="programlisting">
<span class="strong"><strong>        conda install anaconda-client</strong></span>
</pre></li><li><p>Log in to your Anaconda cloud account as follows:</p><pre class="programlisting">       (<span class="strong"><strong>acluster)padma@padma$ anaconda login
       Using Anaconda Cloud api site https://api.anaconda.org
       Username: padma
       padma's Password:
       login successful</strong></span>
</pre><p>If the login is unsuccessful, visit <a class="ulink" href="https://anaconda.org/account/register" target="_blank">https://anaconda.org/account/register</a> and register the account by providing the necessary details. Now try to log in with the created credentials:</p></li><li><p>Now install Anaconda for cluster management on the local machine using the following command:</p><pre class="programlisting">
<span class="strong"><strong>       conda install anaconda-cluster -c anaconda-cluster</strong></span>
</pre></li><li><p>The preceding code creates the <code class="literal">~/.acluster</code> directory which contains sample <code class="literal">~/.acluster/providers.yaml</code> and an example profile file located in <code class="literal">~/.acluster/profiles.d/</code>.</p></li><li><p>Edit the <code class="literal">~/.acluster/providers.yaml</code> file and replace the <code class="literal">private_key</code> location:</p><pre class="programlisting">
<span class="strong"><strong>       private_key: ~/.ssh/id_rsa</strong></span>
</pre></li><li><p>Edit <code class="literal">~/.acluster/profiles.d/aws_profile_sample</code> file and configure the following fields:</p><pre class="programlisting">
<span class="strong"><strong>        name: padma
        provider: aws_east
        num_nodes: 4
        node_id: bare_metal  # Ubuntu 14.04, us-east-1 region
        node_type: bare_metal
        user: ubuntu
        anaconda_url: http://localhost/miniconda/Miniconda-latest-
        Linux-x86_64.sh

        machines:
          head:
            - 192.168.1.1
          compute:
            - 192.168.1.2
            - 192.168.1.3
            - 192.168.1.4

        plugins:
          - spark-yarn
          - notebook

        default_channels: http://localhost/conda/anaconda
        conda_channels:
          - defaults
          - anaconda-cluster
          - blaze
          - pypi
          - username
          - https://conda.anaconda.org/username/

        security:
          disable_selinux: false
          flush_iptables: false</strong></span>
</pre></li><li><p>Rename <code class="literal">aws_profile_sample</code> to <code class="literal">profile-name</code>, <code class="literal">aws_profile_sample.yaml</code> to <code class="literal">profile-name.yaml</code>, <code class="literal">aws_profile_sample_spark_yarn.yaml</code> to <code class="literal">profile-name_spark_yarn.yaml</code>Â and <code class="literal">aws_profile_sample_spark_standalone.yaml</code> to <code class="literal">profile-name_spark_standalone.yaml</code>. Now create <code class="literal">demo_cluster</code> as follows:</p><pre class="programlisting">
<span class="strong"><strong>        acluster create demo_cluster --profile padma</strong></span>
</pre></li><li><p>Install the following <code class="literal">conda</code> packages:</p><pre class="programlisting">
<span class="strong"><strong>        acluster conda install numpy scipy pandas nltk</strong></span>
</pre></li><li><p>Also, the plugin for Ipython notebook is installed as follows:</p><pre class="programlisting">
<span class="strong"><strong>        acluster install notebook</strong></span>
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec246"></a>How it worksâ€¦</h3></div></div></div><p>The preceding commands show various steps to install Anaconda for cluster management, create an Anaconda cloud account, run the cluster with a specified number of instances and install packages such as <code class="literal">numpy</code>, <code class="literal">scipy</code>Â and <code class="literal">nltk</code>. If at any step of setting up the cluster, there is a failure then trace the dependency packages to be installed and install them.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec247"></a>There's moreâ€¦</h3></div></div></div><p>There are a variety of packages supported by Anaconda; please visit <a class="ulink" href="http://docs.continuum.io/anaconda/pkg-docs" target="_blank">http://docs.continuum.io/anaconda/pkg-docs</a> to see the list of packages supported. There are also plugins available to export Python objects to Excel and import the contents of Excel spreadsheets to perform calculations or visualizations in Python. Also, IDEs such as Spyder, PyCharm, Eclipse and PyDev can be integrated to run in Anaconda. For details, please refer to <a class="ulink" href="http://docs.continuum.io/anaconda/ide_integration" target="_blank">http://docs.continuum.io/anaconda/ide_integration</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec248"></a>See also</h3></div></div></div><p>For more details on Anaconda (continuumÂ analytics), please visit <a class="ulink" href="http://docs.continuum.io/anaconda/index" target="_blank">http://docs.continuum.io/anaconda/index</a>.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec68"></a>POS tagging with PySpark on an Anaconda cluster</h2></div></div><hr /></div><p>Parts-of-speech tagging is the process of converting a sentence in the form of a list of words, into a list of tuples, where each tuple is of the form (word, tag). The <span class="strong"><strong>tag</strong></span> is a part-of-speech tag and signifies whether the word is a noun, adjective, verb and so on. This is a necessary step before chunking. With parts-of-speech tags, a chunker knows how to identify phrases based on tag patterns. These POS tags are used for grammar analysis and word sense disambiguation.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec249"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark cluster either in pseudo distributed mode or in one of the distributed modes, that is, standalone, YARN, or Mesos. Also, have PySpark and Anaconda installed on the Linux machine, that is, Ubuntu 14.04. For installing Anaconda, please refer the earlier recipes.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec250"></a>How to do itâ€¦</h3></div></div></div><p>Let's see how to implement POS tagging using PySpark:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Activate the Anaconda cluster as follows:</p><pre class="programlisting">
<span class="strong"><strong>        source activate acluster</strong></span>
</pre></li><li><p>Install the Spark + YARN plugin on the cluster as follows:</p><pre class="programlisting">
<span class="strong"><strong>        acluster install spark-yarn</strong></span>
</pre></li><li><p>Once the installation of the plugin is done, we can view the YARN UI as follows:</p><pre class="programlisting">
<span class="strong"><strong>        acluster open yarn</strong></span>
</pre></li><li><p>Install the NLTK library as follows:</p><pre class="programlisting">
<span class="strong"><strong>        acluster conda install nltk</strong></span>
</pre></li><li><p>The following output is seen from each node, which indicates that the package was successfully installed across the cluster:</p><pre class="programlisting">
<span class="strong"><strong>        Node "ip-192-168-0-1.ec2.internal":
        Successful actions: 1/1
        Node "ip-192-168-0-2.ec2.internal":
        Successful actions: 1/1
        Node "ip-192-168-0-3.ec2.internal":
        Successful actions: 1/1</strong></span>
</pre></li><li><p>Now, download data for the NLTK project as follows:</p><pre class="programlisting">
<span class="strong"><strong>        acluster cmd 'sudo /opt/anaconda/bin/python -m
        nltk.downloader -d /usr/share/nltk_data all'</strong></span>
</pre></li><li><p>After a few minutes, the output is as below:</p><pre class="programlisting">
<span class="strong"><strong>      Execute command "sudo /opt/anaconda/bin/python -m
      nltk.downloader -d /usr/share/nltk_data all" target: "*" cluster:
      "d"
      All nodes (x3) response: [nltk_data] Downloading collection 'all'
      [nltk_data]    |
      [nltk_data]    | Downloading package abc to
      /usr/share/nltk_data...
      [nltk_data]    |   Unzipping corpora/abc.zip.
      [nltk_data]    | Downloading package alpino to
      /usr/share/nltk_data...
      [nltk_data]    |   Unzipping corpora/alpino.zip.
      [nltk_data]    | Downloading package biocreative_ppi to
      [nltk_data]    |     /usr/share/nltk_data...
      ....
      [nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.
      [nltk_data]    | Downloading package word2vec_sample to
      [nltk_data]    |     /usr/share/nltk_data...
      [nltk_data]    |   Unzipping models/word2vec_sample.zip.
      [nltk_data]    |
      [nltk_data]  Done downloading collection all</strong></span>
</pre></li><li><p>Now create a script in order to run NLTK's POS-tagger as follows:</p><pre class="programlisting">
<span class="strong"><strong>      # spark-nltk.py
      from pyspark import SparkConf
      from pyspark import SparkContext

      conf = SparkConf()
      conf.setMaster('yarn-client')
      conf.setAppName('spark-nltk')
      sc = SparkContext(conf=conf)

      data =
      sc.textFile('file:///usr/share/nltk_data/corpora/state_union/
      1972-Nixon.txt')
      import nltk
      words = data.flatMap(lambda x: nltk.word_tokenize(x))
      print words.take(10)
      pos_word = words.map(lambda x: nltk.pos_tag([x]))
      print pos_word.take(5)</strong></span>
</pre></li><li><p>Run the script on the Spark Cluster:</p><pre class="programlisting">
<span class="strong"><strong>     Using Spark's default log4j profile: org/apache/spark/log4j-
     defaults.properties
     15/06/13 05:14:29 INFO SparkContext: Running Spark version 1.6.0

     [...]

     ['Address',
        'on',
        'the',
        'State',
        'of',
        'the',
        'Union',
        'Delivered',
            'Before',
         'a']
     [...]
     [[('Address', 'NN')],
          [('on','IN')],
          [('the', 'DT')],
          [('State', 'NNP')],
          [('of', 'IN')]]</strong></span>
</pre></li></ol></div><p>The output shows the words that were returned from the Spark script, including the results from the <code class="literal">flatMap</code> operation and the POS-tagger.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec251"></a>How it worksâ€¦</h3></div></div></div><p>First, SparkContext is created. Note that Anaconda for cluster management will not create a SparkContext by default. In the preceding example, we use the YARN resource manager. After SparkContext is created, <code class="literal">sc.textFile('file:///usr/share/nltk_data/corpora/state_union/1972-Nixon.txt')</code> is used to load the data. Next, <code class="literal">nltk</code> is imported and the text is mapped to the <code class="literal">word_tokenize</code> function in NLTK. Finally, NLTK's POS-tagger, <code class="literal">nltk.pos_tag([x])</code>, is used to find the parts of speech for each word.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec252"></a>There's moreâ€¦</h3></div></div></div><p>Apart from the POS tagger, the significant NLP tasks such as chunker, <span class="strong"><strong>Named Entity Recognition</strong></span> (<span class="strong"><strong>NER</strong></span>), text classification using naive Bayes classifier, decision tree classifier, and maximum entropy classifier can be performed on large datasets as the framework Spark provides distributed computation. For details on NLP, please refer <a class="ulink" href="https://en.wikipedia.org/wiki/Natural_language_processing" target="_blank">https://en.wikipedia.org/wiki/Natural_language_processing</a> and <a class="ulink" href="http://nlp.stanford.edu/" target="_blank">http://nlp.stanford.edu/</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec253"></a>See also</h3></div></div></div><p>Please refer <span class="emphasis"><em>Installing Anaconda on Linux</em></span> and <span class="emphasis"><em>Anaconda for cluster management</em></span> recipes to get familiar with installations and continuum analytics.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec69"></a>NER with IPython over Spark</h2></div></div><hr /></div><p>Apart from POS, one of the most common labeling problems is finding entities in the text. Typically, NER constitutes name, location and organizations. There are NER systems that tag more entities than just these three such as labeling and named entities using the context and other features. There is a lot more research going on in this area of NLP, where people are trying to tag biomedical entities, product entities, and so on.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec254"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark cluster either in pseudo distributed mode or in one of the distributed modes, that is, standalone, YARN, or Mesos. Also, have PySpark and Ipython installed on the Linux machine, that is, Ubuntu 14.04. For installing IPython, please refer to the <span class="emphasis"><em>Using IPython with PySpark</em></span> recipe in the <a class="link" href="#" linkend="ch02">Chapter 2</a>,Â <span class="emphasis"><em>Tricky Statistics with Spark</em></span>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec255"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Download and install NLTK data correctly as follows:</p><pre class="programlisting">
<span class="strong"><strong>      ipython console -profile=pyspark
      In [1]:
      In [1]: from pyspark import SparkConf, SparkContext
      Welcome to
          ____              __
         / __/__  ___ _____/ /__
        _\ \/ _ \/ _ `/ __/  '_/
       /__ / .__/\_,_/_/ /_/\_\   version 1.6.0
          /_/

      Using Python version 2.7.11 (default, Dec  6 2015 18:08:32)
      SparkContext available as sc, HiveContext available as
      sqlContext.
      In [2]: import nltk
      In [3]: nltk.download('averaged_perceptron_tagger')
      [nltk_data] Downloading package averaged_perceptron_tagger to
      [nltk_data]     /home/padma/nltk_data...
      [nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.
      Out[3]: True
      In [4]: nltk.download('maxent_ne_chunker')
      [nltk_data] Downloading package maxent_ne_chunker to
      [nltk_data]     /home/padma/nltk_data...
      [nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.
      Out[4]: True
      In [5]: nltk.download('words')
      [nltk_data] Downloading package words to /home/padma/nltk_data...
      [nltk_data]   Unzipping corpora/words.zip.
      Out[5]: True
      In [6]: from pyspark import SparkConf, SparkContext
      In [7]: import nltk
      In [8]: from nltk import word_tokenize, ne_chunk
      In [9]: data = sc.parallelize("Mark is studying at Stanford
      University in California")
      In [10]: words = data.flatMap(lambda x: word_tokenize(x))
      In [11]: pos_tags = words.map(lambda x: nltk.pos)
      In [12]: ne_chunks = pos_tags.map(lambda x: ne_chunk(x,
      binary=False))
      In [13]: ne_chunks.take(2)
      16/03/09 09:36:04 INFO spark.SparkContext: Starting job:
      runJob at PythonRDD.scala:393
      16/03/09 09:36:04 INFO scheduler.DAGScheduler: Got job 1 (runJob
      at PythonRDD.scala:393) with 1 output partitions
      16/03/09 09:36:04 INFO scheduler.DAGScheduler: Final stage:
      ResultStage 1 (runJob at PythonRDD.scala:393)
      16/03/09 09:36:04 INFO scheduler.DAGScheduler: Parents of fina</strong></span>l
     <span class="strong"><strong> stage: List()
      16/03/09 09:36:04 INFO scheduler.DAGScheduler: Missing parents:
      List()
      16/03/09 09:36:04 INFO scheduler.DAGScheduler: Submitting
      ResultStage 1 (PythonRDD[2] at RDD at PythonRDD.scala:43), which
      has no missing parents
      16/03/09 09:36:04 INFO storage.MemoryStore: Block broadcast_1
      stored as values in memory (estimated size 4.5 KB, free 11.6 KB)
      16/03/09 09:36:04 INFO storage.MemoryStore: Block
      broadcast_1_piece0 stored as bytes in memory (estimated size 3.0
      KB, free 14.6 KB)
      16/03/09 09:36:04 INFO storage.BlockManagerInfo: Added
      broadcast_1_piece0 in memory on localhost:39038 (size: 3.0 KB,
      free: 511.5 MB)
      16/03/09 09:36:04 INFO spark.SparkContext: Created broadcast 1
      from broadcast at DAGScheduler.scala:1006
      16/03/09 09:36:04 INFO scheduler.DAGScheduler: Submitting 1
      missing tasks from ResultStage 1 (PythonRDD[2] at RDD at
      PythonRDD.scala:43)
      16/03/09 09:36:04 INFO scheduler.TaskSchedulerImpl: Adding task
      set 1.0 with 1 tasks
      16/03/09 09:36:04 INFO scheduler.TaskSetManager: Starting</strong></span>
<span class="strong"><strong>task
      0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL,
      2107 bytes)
      16/03/09 09:36:04 INFO executor.Executor: Running task 0.0 in
      stage 1.0 (TID 1)
      16/03/09 09:36:07 INFO python.PythonRunner: Times: total = 3267,
      boot = 4, init = 363, finish = 2900
      16/03/09 09:36:07 INFO executor.Executor: Finished task 0.0 in
      stage 1.0 (TID 1). 1112 bytes result sent to driver
      16/03/09 09:36:07 INFO scheduler.TaskSetManager: Finished task
      0.0</strong></span>
<span class="strong"><strong>in stage 1.0 (TID 1) in 3284 ms on localhost (1/1)
      16/03/09 09:36:07 INFO scheduler.TaskSchedulerImpl: Removed
      TaskSet 1.0, whose tasks have all completed, from pool
      16/03/09 09:36:07 INFO scheduler.DAGScheduler: ResultStage 1
      (runJob at PythonRDD.scala:393) finished in 3.286 s
      16/03/09 09:36:07 INFO scheduler.DAGScheduler: Job 1 finished:
      runJob at PythonRDD.scala:393, took 3.302134 s
      Out[13]: [Tree('S', [('M', 'NN')]), Tree('S', [('a', 'DT')])]</strong></span>
</pre></li><li><p>The following code snippet shows the use of the stanford NER tagger:</p><pre class="programlisting">
<span class="strong"><strong>      In [14]: from nltk.tag.stanford import NERTagger
      In [15]: st = NERTagger('&lt;PATH&gt;/stanford-
      ner/classifiers/all.3class.distsim.crf.ser.gz',...'&lt;PATH&gt;
      /stanford-ner/stanford-ner.jar')
      dataForStanford = sc.parallelize('Rami Eid is studying at Stony
      Brook University in NY')
      wordsSt = dataForStanford.flatMap(lambda x: x.split())
      wordsTagged = wordsSt.map(lambda x: st.tag(x))
      Out [15]: [('Rami', 'PERSON'), ('Eid', 'PERSON'), ('is', 'O'),
      ('studying', 'O'), ('at', 'O'), ('Stony', 'ORGANIZATION'),
      ('Brook', 'ORGANIZATION'), ('University', 'ORGANIZATION'), ('in',
      'O'), ('NY', 'LOCATION')]
</strong></span>
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec256"></a>How it worksâ€¦</h3></div></div></div><p>NLTK provides a method for named entity extraction, that is, <code class="literal">ne_chunk</code>. The preceding code snippet demonstrated how to use it for tagging any sentence. This method will require you to preprocess the text to tokenize for sentences, tokens and POS tags in the same order to be able to tag for named entities. NLTK used <code class="literal">ne_chunking</code>, where chunking is nothing but tagging multiple tokens to a call it a meaningful entity.</p><p>The <code class="literal">ne_chunking</code> method recognizes people (names), places (location) and organizations. If binary is set to <code class="literal">True</code> then it provides the output for the entire sentence tree and tags everything. Setting it to <code class="literal">False</code> will give us detailed person, location, and organization information.</p><p>Similar to the POS tagger, NLTK also has a wrapper around Stanford NER. This NER tagger has better accuracy. If you observe closely, even with a very small test sentence, we can say the Stanford tagger outperformed the NLTK <code class="literal">ne_chunk</code> tagger.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec257"></a>There's moreâ€¦</h3></div></div></div><p>There are two ways of tagging the NER using NLTK. One is by using the pre-trained NER model that just scores the test data, the other is to build a machine learning based model. NLTK provides the <code class="literal">ne_chunk()</code> method and a wrapper around the Stanford NER tagger for NER. The above taggers are a nice solution for a generic kind of entity tagging, but we have to train our own tagger, when it comes to tagging domain-specific entities such as biomedical and product names, so we have to build our own NER system. The NER Calais tagger is a recommended one. It has ways of tagging not just typical NER, but also some more entities. The performance of this tagger is also very good.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec258"></a>See also</h3></div></div></div><p>There are also pre-trained taggers such as the Sequential tagger, N-gram tagger, Regex tagger, and Brill tagger. The internals of either the NLTK or Stanford taggers are still a black box. For example, <code class="literal">pos_tag</code> internally uses a <span class="strong"><strong>Maximum Entropy Classifier</strong></span> (<span class="strong"><strong>MEC</strong></span>), while the <code class="literal">StanfordTagger</code> also uses a modified version of Maximum Entropy. These are discriminatory models. While there is <span class="strong"><strong>Hidden Markov Model</strong></span> (<span class="strong"><strong>HMM</strong></span>) and <span class="strong"><strong>Conditional Random Field</strong></span> (<span class="strong"><strong>CRF</strong></span>) based taggers, these are generative models. Please refer to the NLP class <a class="ulink" href="https://www.coursera.org/course/nlp" target="_blank">https://www.coursera.org/course/nlp</a> for greater understanding of these concepts.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec70"></a>Implementing openNLP - chunker over Spark</h2></div></div><hr /></div><p>Chunking is shallow parsing, where instead of retrieving deep structure of the sentence, we try to club some chunks of the sentences that constitute some meaning. A chunk is defined as the minimal unit that can be processed. The conventional pipeline in chunking is to tokenize the POS tag and the input string, before they are given to any chunker.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec259"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark cluster either in pseudo distributed mode or in one of the distributed modes, that is, standalone, YARN, or Mesos. For installing Spark on a standalone cluster, please refer to <a class="ulink" href="http://spark.apache.org/docs/latest/spark-standalone.html" target="_blank">http://spark.apache.org/docs/latest/spark-standalone.html</a>. Install Hadoop (optionally), Scala, and Java.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec260"></a>How to do itâ€¦</h3></div></div></div><p>Let's see how to run OpenNLP-Chunker over Spark:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Let's start an application named SparkNLP. Initially specify the following libraries in theÂ <code class="literal">build.sbt</code> file:</p><pre class="programlisting">     libraryDependencies ++= Seq(
     "org.apache.spark" %% "spark-core" % "1.6.0",
     "org.apache.spark" %% "spark-mllib" % "1.6.0",
     "org.apache.spark" %% "spark-sql" % "1.6.0",
     "org.apache.spark" %% "spark-streaming" % "1.6.0",
     "org.apache.opennlp" % "opennlp-tools" % "1.6.0",
     "org.apache.opennlp" % "opennlp-uima" % "1.6.0"
      )</pre></li><li><p>Here is the code for the Chunker which runs on the each record of Spark RDD and identifies the parts of speech:</p><pre class="programlisting">    import java.io.File
    import opennlp.tools.chunker.{ChunkerME, ChunkerModel}
    import opennlp.tools.tokenize.WhitespaceTokenizer
    import org.apache.spark.SparkConf
    import org.apache.spark.SparkContext
    import opennlp.tools.cmdline.postag.POSModelLoader
    import opennlp.tools.postag.POSModel
    import opennlp.tools.postag.POSSample
    import opennlp.tools.postag.POSTaggerME
    object Chunker_Demo {
    def main(args:Array[String]): Unit ={
    val conf = new SparkConf()
    .setAppName("Chunker_Application")
    .setMaster("spark://master:7077")
    .set("spark.serializer",
    "org.apache.spark.serializer.KryoSerializer")
    val sc = new SparkContext(conf)
    val textInput = sc.makeRDD(Array("I am Padma working in Fractal
    Analytics Company","I am a big data enthusiast",
    "I love cooking"),1)
    val modelFile = new File("/home/padmac/opennlp_models/en-pos-
     maxent.bin")
    val chunkerModelFile = new File("/home/padmac/opennlp_models/en
    chunker.bin")
    val model = new POSModelLoader().load(modelFile)
    val tagger = new POSTaggerME(model)
    val chunkerModel = new ChunkerModel(chunkerModelFile)
    val chunkerME= new ChunkerME(chunkerModel)
    val broadCastedChunkerME = sc.broadcast(chunkerME)
    val broadCastedTagger = sc.broadcast(tagger)
    val resultsAndSpan = textInput.map{sentence =&gt;
    val tokenizedLines =
    WhitespaceTokenizer.INSTANCE.tokenize(sentence)
    val tags = broadCastedTagger.value.tag(tokenizedLines)
    val result = broadCastedChunkerME.value.
    chunk(tokenizedLines,tags)
    val span = broadCastedChunkerME.value
    .chunkAsSpans(tokenizedLines,tags)
    (result,span)}
    val results = resultsAndSpan.flatMap{case(results,spans) =&gt;results}
    val spans = resultsAndSpan.flatMap{case(results,spans) =&gt;spans}
    println("Resultant Strings: ")
    results.foreach(println)
    println("Spans: ")
    spans.foreach(println)
    }
    }</pre><p>The following is the output:</p><p>Resultant strings:</p><pre class="programlisting">      B-NP
      B-VP
      B-NP
      I-NP
      B-PP
      B-NP
      I-NP
      I-NP
      B-NP
      B-VP
      B-NP
      I-NP
      I-NP
      I-NP
      B-NP
      B-VP
      I-VP
      Spans:
      [0..1) NP
      [1..2) VP
      [2..4) NP
      [4..5) PP
      [5..8) NP
      [0..1) NP
      [1..2) VP
      [2..6) NP
      [0..1) NP
      [1..3) VP</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec261"></a>How it worksâ€¦</h3></div></div></div><p>In the preceding code snippet, the models <code class="literal">en-pos-maxent.bin</code> (POS tagger model with tag dictionary) and <code class="literal">en-chunker.bin</code> (chunker model) are downloaded from the location <a class="ulink" href="http://opennlp.sourceforge.net/models-1.5/" target="_blank">http://opennlp.sourceforge.net/models-1.5/</a>. The <code class="literal">POSModelLoader().load(modelFile) </code>line loads the POS Model. Next, theÂ <code class="literal">POSTaggerME(model) </code>line initializes the <code class="literal">POSTagger</code> from the model. Also, <code class="literal">ChunkerModel(chunkerModelFile)</code> loads the chunker model and passing this as parameter, <code class="literal">ChunkerME</code> is initialized.</p><p>When broadcasting these models they might result in serialization exceptions. Hence setting the property, <code class="literal">org.apache.spark.serializer.KryoSerializer</code> serializes the models.Next, for each sentence in the RDD, <code class="literal">WhitespaceTokenizer.INSTANCE.tokenize</code>Â generates tokens. The generated array of tokens are passed to <code class="literal">POSTaggerME </code>which tags the parts of speech.</p><p>Now, <code class="literal">broadCastedChunkerME.value.chunk(tokenizedLines,tags) </code>generates chunks from the tokens and the corresponding tags. Finally from the tokenized sentences and tags, spans are generated asÂ <code class="literal">broadCastedChunkerME.value.</code></p><pre class="programlisting">chunkAsSpans(tokenizedLines,tags)</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec262"></a>There's moreâ€¦</h3></div></div></div><p>Chunker partitions a sentence into a set of chunks using the tokens generated by the tokenizer. For example, for the sentence <span class="emphasis"><em>President speaks about the health care reforms</em></span>Â can be broken into two chunks, one is <span class="emphasis"><em>the President</em></span>, which is noun dominated, and hence is called a <span class="strong"><strong>noun phrase</strong></span> (<span class="strong"><strong>NP</strong></span>). The remaining part of the sentence is dominated by a verb; hence it is called a <span class="strong"><strong>verb phrase</strong></span> (<span class="strong"><strong>VP</strong></span>). There is one more sub-chunk in the part that is, <span class="emphasis"><em>speaks about the health care reforms</em></span>. Here, one more NP exists that can be broken down again in <span class="emphasis"><em>speaks about</em></span>Â and <span class="emphasis"><em>health care reforms</em></span>. Chunking is also a processing interface to identify non-overlapping groups in unrestricted text. Regular chunker used the rule NP/VP, which defines different POS patterns called as verb/noun phrase.</p><p>Regular expression based chunkers rely on chunk rules defined manually to chunk the string. So, if we are able to write a universal rule that can incorporate most of the noun phrase patterns, we can use regex chunkers. Unfortunately, it's hard to come up with those kind of generic rules; the other approach is to use a machine learning way of doing chunking.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec263"></a>See also</h3></div></div></div><p>Please refer toÂ <span class="emphasis"><em>POS tagging with PySpark on an Anaconda cluster</em></span> and <span class="emphasis"><em>NER with IPython over Spark</em></span> recipes to know details on implementing POS tagging and NER using NLTK over Spark.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec71"></a>Implementing openNLP - sentence detector over Spark</h2></div></div><hr /></div><p>Partitioning text into sentences is called <span class="strong"><strong>Sentence Boundary Disambiguation</strong></span> (<span class="strong"><strong>SBD</strong></span>) or Sentence Detection. This process is useful for many downstream NLP tasks, which require analysis within sentences; for instance POS and phrase analysis. This Sentence Detection process is language dependent. Most search engines are not concerned with Sentence Detection. They are only interested in query's tokens and their respective positions. POS taggers and other NLP tasks that perform extraction of data will frequently process individual sentences. The detection of sentence boundaries will help separate phrases that might appear to span sentences.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec264"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark cluster either in pseudo distributed mode or in one of the distributed modes, that is, standalone, YARN, or Mesos. For installing Spark on a standalone cluster, please refer to <a class="ulink" href="http://spark.apache.org/docs/latest/spark-standalone.html" target="_blank">http://spark.apache.org/docs/latest/spark-standalone.html</a>. . Install Hadoop (optionally), Scala, and Java.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec265"></a>How to do itâ€¦</h3></div></div></div><p>Let's see how to detect sentences using OpenNLP over Spark:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Let's start an application named SparkNLP. Initially specify the following libraries in theÂ <code class="literal">build.sbt</code> file:</p><pre class="programlisting">       libraryDependencies ++= Seq(
       "org.apache.spark" %% "spark-core" % "1.6.0",
       "org.apache.spark" %% "spark-mllib" % "1.6.0",
       "org.apache.spark" %% "spark-sql" % "1.6.0",
       "org.apache.spark" %% "spark-streaming" % "1.6.0",
       "org.apache.opennlp" % "opennlp-tools" % "1.6.0",
       "org.apache.opennlp" % "opennlp-uima" % "1.6.0"
       )</pre></li><li><p>Here is the code for the Sentence Detector which runs on the each record of Spark RDD and identifies the sentence:</p><pre class="programlisting">       import java.io.File
       import opennlp.tools.sentdetect.{SentenceDetectorME,
       SentenceModel}
       import org.apache.spark.SparkConf
       import org.apache.spark.SparkContext
       object SentenceDetector_Demo {
       def main(args:Array[String]): Unit ={
       val conf = new SparkConf()
       .setAppName("SentenceDetector_Application")
       .setMaster("spark:master:7077")
       .set("spark.serializer",
       "org.apache.spark.serializer.KryoSerializer")
       val sc = new SparkContext(conf)
       val textInput = sc.makeRDD(Array("Hi Padma ! How are you ?",
       "He saw him in Boston at McKenzie's pub. At 3:00 where he",
       "He was the last person. To see Fred."),1)
       val sentenceDetectorModelFile = new
       File("/home/padmac/opennlp_models/en- sent.bin")
       val model = new SentenceModel(sentenceDetectorModelFile)
       val sdetector = new SentenceDetectorME(model)
       val broadCastedsdector = sc.broadcast(sdetector)
       val broadCastedsdector = sc.broadcast(sdetector)
       val results = textInput.map{record =&gt;
       (broadCastedsdector.value.sentDetect(record),
       broadCastedsdector.value.getSentenceProbabilities)
       }
       val detectedSentences = results.keys.flatMap(x =&gt; x)
       val probabilities = results.values.flatMap(x =&gt; x)
       println("Detected Sentences: ")
       detectedSentences.collect().foreach(println)
       println("Probabilities : ")
       probabilities.collect().foreach(println)
        }
       }</pre><p>The following is the output:</p><p>Detected sentences:</p><pre class="programlisting">
     Hi Padma !
     How are you ?
     He saw him in Boston at McKenzie's pub.
     At 3:00 where he
     He was the last person.
     To see Fred.
</pre><p>Probabilities:</p><pre class="programlisting">
     0.8702580316569011
     0.94117302373681
     0.9863984646806333
     1.0
     0.999763703383087
     0.9347401492739993
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec266"></a>How it worksâ€¦</h3></div></div></div><p>Initially the model <code class="literal">en-sent.bin</code> is downloaded fromÂ <a class="ulink" href="http://opennlp.sourceforge.net/models-1.5/" target="_blank">http://opennlp.sourceforge.net/models-1.5/</a>. The <code class="literal">SentenceModel(sentenceDetectorModelFile)</code> line, loads the <code class="literal">SentenceModel</code>. Next, theÂ <code class="literal">SentenceDetectorME(model)</code>Â line initializes the <code class="literal">SenetenceDetectorME</code> from the model. The <code class="literal">SenetenceDetectorME </code>created, is broadcasted which might result in serialization issues. Hence setting the property, <code class="literal">che.spark.sorg.apaerializer.KryoSerializer</code> serializes the model.</p><p>For each element in the RDD, <code class="literal">broadCastedsdector.value.sentDetect(record)</code> generates detected array of sentences. The <code class="literal">getSentenceProbabilities</code> method returns an array of doubles representing the confidence of the sentences detected from the last use of the <code class="literal">sentDetect</code> method. The detected sentences and the respective probabilities are finally displayed on console.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec267"></a>There's moreâ€¦</h3></div></div></div><p>Common approaches to detect sentences are to include a set of rules to train a model for detection. The simple rules could be that text should be terminated by a period, question mark or exclamation mark and also the period should not be preceded by an abbreviation or followed by a digit and so on. The detection of sentence boundaries helps separate phrases that might appear to span sentences. However, breaking text into sentences is difficult because punctuation is frequently ambiguous, abbreviations often contain periods and also sentences may be embedded within each other by the use of quotes.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec268"></a>See also</h3></div></div></div><p>Please refer <span class="emphasis"><em>POS tagging with PySpark on an Anaconda cluster</em></span>, <span class="emphasis"><em>NER with IPython over Spark </em></span>and<span class="emphasis"><em><span class="emphasis"><em>Implemeting OpenNLP-Chunker</em></span><span class="emphasis"><em>over</em></span></em></span><span class="emphasis"><em> Spark </em></span>recipes to know details on implementing POS tagging, NER and Chunker using NLTK and OpenNLP over Spark.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec72"></a>Implementing stanford NLP - lemmatization over Spark</h2></div></div><hr /></div><p>Lemmatization is one of the pre-processing steps which is a more methodical way of converting all the grammatical/inflected forms of the root of the word. It uses context and parts of speech to determine the inflected form of the word and applies different normalization rules for each part of speech to get the word (lemma). In this recipe, we'll see lemmatization of text using Stanford API.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec269"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark cluster either in pseudo distributed mode or in one of the distributed modes, that is, standalone, YARN, or Mesos. For installing Spark on a standalone cluster, please refer to <a class="ulink" href="http://spark.apache.org/docs/latest/spark-standalone.html" target="_blank">http://spark.apache.org/docs/latest/spark-standalone.html</a>. . Install Hadoop (optionally), Scala, and Java.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec270"></a>How to do itâ€¦</h3></div></div></div><p>Let's see how to apply lemmatization using Stanford NLP over Spark:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Let's start an application named SparkCoreNLP. Initially specify the following libraries in <code class="literal">build.sbt</code> file:</p><pre class="programlisting">    libraryDependencies ++= Seq(
    "org.apache.spark" %% "spark-core" % "1.6.0",
    "org.apache.spark" %% "spark-mllib" % "1.6.0",
    "org.apache.spark" %% "spark-sql" % "1.6.0",
    "org.apache.spark" %% "spark-streaming" % "1.6.0",
    "org.apache.opennlp" % "opennlp-tools" % "1.6.0",
    "org.apache.opennlp" % "opennlp-uima" % "1.6.0"
    "edu.stanford.nlp" % "stanford-corenlp" % "3.6.0",
    "com.google.protobuf" % "protobuf-java" % "2.6.1"
    )</pre></li><li><p>Here is the code for the Lemmatization using Stanford NLP API:</p><pre class="programlisting">    import java.util._
    import edu.stanford.nlp.ling.CoreAnnotations._
    import edu.stanford.nlp.ling.CoreAnnotations.TokensAnnotation
    import edu.stanford.nlp.ling.CoreAnnotations
    .LemmaAnnotation
    import edu.stanford.nlp.pipeline.StanfordCoreNLP
    import scala.collection.JavaConverters._
    object Lemmatization_Stanford {
    def main(args: Array[String]): Unit = {
    val conf = new SparkConf()
    .setMaster("spark:master:7077")
    .setAppName("Lemmatization_Demo")
    .set("spark.serializer",
    "org.apache.spark.serializer.KryoSerializer")
    val sc = new SparkContext(conf)
    val textInput = sc.makeRDD(Array("Hi Padma ! How are you ?", "He
    saw him in Boston at McKenzie's pub. At 3:00 where he", "He was the
    last person. To see Fred."), 1)
    val props = new Properties()
    props.put("annotators", "tokenize, ssplit, pos, lemma")
    val pipeline = new StanfordCoreNLP(props)
    val broadCastedPipeline = sc.broadcast(pipeline)
    val lemmas = textInput.flatMap{
    record =&gt;
    val document = broadCastedPipeline.value.process(record)
    val sentences =
    document.get(classOf[SentencesAnnotation]).asScala.toList
    val tokens = sentences.flatMap{sentence =&gt;
    sentence.get(classOf[TokensAnnotation]).asScala.toList}
    tokens.map{token =&gt;
    val word = token.get(classOf[TextAnnotation])
    val lemma = token.get(classOf[LemmaAnnotation])
    (word,lemma)}
      }
    lemmas.foreach(println)
      }
     }</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec271"></a>How it worksâ€¦</h3></div></div></div><p>The preceding code snippet shows how to use Stanford NLP API over Spark to lemmatize the text. Initially, properties object is created and the annotators Â <code class="literal">tokenize, ssplit, pos</code> and <code class="literal">lemma</code> are specified. <code class="literal">tokenize</code> performs tokenization, <code class="literal">ssplit</code> performs sentence splitting, <code class="literal">pos</code> does POS tagging and <code class="literal">lemma</code> performs lemmatization. The line <code class="literal">new StanfordCoreNLP(props)</code> creates <code class="literal">StanforCoreNLP</code> object which is the pipeline. Now, this <code class="literal">StanfordCoreNLP</code> instance is broadcasted. For each sentence in the RDD, the line <code class="literal">broadCastedPipeline.value.process(record)</code> generates <code class="literal">Annotation</code>. Using the generated <code class="literal">Annotation</code>, the line <code class="literal">document.get(classOf[SentencesAnnotation]).asScala.toList</code> generates sentences of the form <code class="literal">List[CoreMap].</code> For each sentence in turn, tokens are generated in the form - <code class="literal">List[CoreLabel]</code>. The generated tokens are iterated and for each token, <code class="literal">token.get(classOf[TextAnnotation])</code> returns the word and <code class="literal">token.get(classOf[LemmaAnnotation])</code> returns lemma.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec272"></a>There's moreâ€¦</h3></div></div></div><p>The goal of stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly, and often includes the removal of derivational affixes. Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.</p><p>If confronted with the token 'saw', stemming might return just 's', whereas lemmatization would attempt to return either 'see' or 'saw' depending on whether the use of the token is verb or a noun. These two may also differ in a way that - stemming most commonly collapses derivationally related words, whereas lemmatization commonly collapses the different inflectional forms of a lemma. Linguistic processing for stemming or lemmatization is often done by an additional plug-in component to the indexing process, and a number of such components exist, in both commercial and open-source</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec273"></a>See also</h3></div></div></div><p>Please refer toÂ <span class="emphasis"><em>Implemeting OpenNLP-Chunker</em></span><span class="emphasis"><em><span class="emphasis"><em>over</em></span></em></span><span class="emphasis"><em> Spark </em></span>and <span class="emphasis"><em> Implemeting OpenNLP - sentence detectorÂ </em></span><span class="emphasis"><em><span class="emphasis"><em>over</em></span></em></span><span class="emphasis"><em> Spark </em></span>recipes to know details on running Chunker and sentence detector using OpenNLP.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec73"></a>Implementing sentiment analysis using stanford NLP over Spark</h2></div></div><hr /></div><p>Sentiment analysis or opinion mining involves building a system to collect and categorize opinions about a product. This can be used in several ways that help marketers evaluate the success of an ad-campaign or new product launch, determine which versions of product or service are popular and also identify demographics that like or dislike product features. In this recipe we will see how the Stanford NLP API performs sentiment analysis.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec274"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark cluster either in pseudo distributed mode or in one of the distributed modes, that is, standalone, YARN, or Mesos. For installing Spark on a standalone cluster, please refer to <a class="ulink" href="http://spark.apache.org/docs/latest/spark-standalone.html" target="_blank">http://spark.apache.org/docs/latest/spark-standalone.html</a>. Install Hadoop (optionally), Scala, and Java.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec275"></a>How to do itâ€¦</h3></div></div></div><p>Let's see how to apply sentiment analysis using Stanford NLP over Spark:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Let's start an application named SparkCoreNLP. Initially specify the following libraries in theÂ <code class="literal">build.sbt</code> file:</p><pre class="programlisting">    libraryDependencies ++= Seq(
    "org.apache.spark" %% "spark-core" % "1.6.0",
    "org.apache.spark" %% "spark-mllib" % "1.6.0",
    "org.apache.spark" %% "spark-sql" % "1.6.0",
    "org.apache.spark" %% "spark-streaming" % "1.6.0",
    "org.apache.opennlp" % "opennlp-tools" % "1.6.0",
    "org.apache.opennlp"
    % "opennlp-uima" % "1.6.0"
    "edu.stanford.nlp" % "stanford-corenlp" % "3.6.0",
    "com.google.protobuf" % "protobuf-java" % "2.6.1"
    )</pre></li><li><p>Here is the code for the sentiment analysis using stanford NLP API:</p><pre class="programlisting">    import edu.stanford.nlp.ling.CoreAnnotations
    import edu.stanford.nlp.neural.rnn.RNNCoreAnnotations
    import edu.stanford.nlp.sentiment.
    SentimentCoreAnnotations
    import org.apache.spark.SparkConf
    import org.apache.spark.SparkContext
    import java.util._
    import edu.stanford.nlp.pipeline.{Annotation, StanfordCoreNLP}
    import scala.collection.JavaConverters._
    object SentimentAnalysis_StanfordAPI {
    def main(args:Array[String]): Unit =
    {
    val conf = new SparkConf()
    .setMaster("spark:master:7077")
    .setAppName("SentimentAnalysis_Demo")
    .set("spark.serializer","org.apache.spark.serializer.
    KryoSerializer")
    val sc = new SparkContext(conf)
    val textInput = sc.makeRDD(Array("An overly sentimental film with a
    somewhat problematic message, but its sweetness and charm are
    occasionally enough to approximate true depth and grace.",
    "Sam was an odd sort of fellow. Not prone to angry and not prone to
    merriment. Overall, an odd fellow.", "Mary thought that custard pie
    was the best pie in the world. However, she loathed chocolate
    pie"), 1)
    val sentimentText = scala.collection.immutable.List("Very
    Negative", "Negative", "Neutral", "Positive", "Very Positive")
    val props = new Properties()
    props.put("annotators", "tokenize, ssplit, parse, sentiment")
    val pipeline = new StanfordCoreNLP(props)
    val broadCastedPipeline = sc.broadcast(pipeline)
    val broadCastedSentimentText = sc.broadcast(sentimentText)
    val scoredSentiments = textInput.flatMap{
    record =&gt; val annotation = new Annotation(record)
    pipeline.annotate(annotation)
    val sentenceList =
    annotation.get(classOf[CoreAnnotations.SentencesAnnotation])
    .asScala.toList; sentenceList.map{sentence =&gt;
    val tree sentence.get(classOf[SentimentCoreAnnotations.
    SentimentAnnotatedTree])
    val score = RNNCoreAnnotations.getPredictedClass(tree)
    broadCastedSentimentText.value(score)
    }
    }
    scoredSentiments.foreach(println)
    }
    }</pre><p>The following is the output:</p><pre class="programlisting">
    Positive
    Neutral
    Negative
    Neutral
    Positive
    Neutral
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec276"></a>How it worksâ€¦</h3></div></div></div><p>The preceding code snippet shows how to use Stanford NLP API with Spark to extract sentiment out of the text. The <code class="literal">textInput</code> RDD contains different possible sentences. The list <code class="literal">sentimentText</code> holds strings for different possible sentiments. The properties object is created and the annotators--<code class="literal">tokenize, ssplit, parse</code> and <code class="literal">sentiment </code>are specified. <code class="literal">tokenize</code> performs tokenization, <code class="literal">ssplit</code> performs sentence splitting, <code class="literal">parse</code> does Parsing and <code class="literal">sentiment</code> extracts sentiment out of the text.</p><p>TheÂ <code class="literal">new StanfordCoreNLP(props)</code>Â line creates aÂ <code class="literal">StanforCoreNLP</code> object which is the pipeline. Now, this <code class="literal">StanfordCoreNLP</code> instance is broadcasted. For each sentence in the RDD, theÂ <code class="literal">new Annotation(record)</code>Â line creates annotation and then <code class="literal">pipeline.annotate(annotation)</code> line performs the actual processing behind the scene. The <code class="literal">Annotation</code> class get method returns list of objects of type <code class="literal">CoreMap</code> which represents the results of splitting the input text into sentences. For each sentence in turn, <code class="literal">sentence.get(classOf[SentimentCoreAnnotations.SentimentAnnotatedTree])</code> line generates an instance of <code class="literal">Tree</code> object which represents the tree structure containing parse of the text for the sentiment. The <code class="literal">getPredictedClass</code> method returns an index into the <code class="literal">sentimentText</code> array reflecting the sentiment of the text. Finally the sentiments are displayed on the console.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec277"></a>There's moreâ€¦</h3></div></div></div><p>Apart from sentiment analysis which is the most common use case, there are others like - spam classification, e-mail categorization, news categorization etc. Sentiment analysis is extremely useful in social media monitoring as it allows us to gain an overview of the wider public opinion behind certain topics. The applications of sentiment analysis are broad and powerful. The ability to extract insights from social data is a practice that is being widely adopted by organizations across the world. Shifts in sentiment on social media have been shown to correlate with shifts in the stock market.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec278"></a>See also</h3></div></div></div><p>Please refer <span class="emphasis"><em>Implementing OpenNLP - chunkerÂ </em></span><span class="emphasis"><em><span class="emphasis"><em>over</em></span></em></span><span class="emphasis"><em> Spark</em></span><span class="emphasis"><em>,<span class="emphasis"><em> Implemeting OpenNLP - sentence detector </em></span></em></span>and<span class="emphasis"><em> Implementing stanford NLP - lemmatization</em></span><span class="emphasis"><em><span class="emphasis"><em>over</em></span></em></span><span class="emphasis"><em> Spark </em></span>recipes to know details on running Chunker, sentence detector and lemmatization using OpenNLP and stanford NLP</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch07"></a>ChapterÂ 7.Â Working with Sparkling Water - H2O</h2></div></div></div><p>In this chapter, you will learn the following recipes:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Working with H2O on Spark</p><p>Downloading and installing H2O</p><p>Using H2O API in Spark</p></li><li style="list-style-type: disc"><p>Implementing k-means using H2O over Spark</p></li><li style="list-style-type: disc"><p>Implementing spam detection with Sparkling Water</p></li><li style="list-style-type: disc"><p>Deep learning with airlines and weather data</p></li><li style="list-style-type: disc"><p>Implementing a crime detection application</p></li><li style="list-style-type: disc"><p>Running SVM with H2O over Spark</p></li></ul></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec74"></a>Introduction</h2></div></div><hr /></div><p>H2O is a fast, scalable, open-source machine learning and deep learning library for smarter applications. Using in-memory compression, H2O handles billions of data rows in memory, even with a small cluster. In order to create complete analytic workflows, H2O's platform includes interfaces for R, Python, Scala, Java, JSON and CoffeeScript/JavaScript flows, as well as a built-in web interface. H2O is designed to run in standalone mode on Hadoop, or within a Spark Cluster. It includes many common machine learning algorithms, such as generalized linear modeling (linear regression, logistic regression, and so on), Naive Bayes, principal components analysis, k-means clustering and others.</p><p>H2O also implements best-in-class algorithms at scale, such as distributed random forest, gradient boosting and deep learning. Users can build thousands of models and compare the results to get the best predictions.</p><p>Sparkling Water allows users to combine the fast, scalable machine learning algorithms of H2O with the capabilities of Spark. With Sparkling Water, users can drive computation from Scala, R, or Python and use the H2O flow UI, providing an ideal machine learning platform for application developers. This integrates the two open source environments, namely Spark, which is an elegant and powerful general-purpose, open-source, in-memory platform and H2O, an in-memory application for machine learning.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec75"></a>Features</h2></div></div><hr /></div><p>Sparkling Water provides transparent integration for the H2O engine and its machine learning algorithms into Spark platforms, which enables the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Use of H2O algorithms in the Spark workflow</p></li><li style="list-style-type: disc"><p>Transformation between H2O and Spark data structures</p></li><li style="list-style-type: disc"><p>Use of Spark RDDs and DataFrames as input for H2O algorithms</p></li><li style="list-style-type: disc"><p>Use of H2O frames as input for MLlib algorithms</p></li><li style="list-style-type: disc"><p>Transparent execution of Sparkling Water applications on top of Spark</p></li></ul></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec76"></a>Working with H2O on Spark</h2></div></div><hr /></div><p>Sparkling Water is executed as a regular Spark application. It provides a way to initialize H2O services on each node in the Spark Cluster and toÂ access data stored in the data structures of Spark and H2O. The Sparkling Water application is launched inside a spark executor created after submitting the application. At this point, H2O starts the services, including the distributed <span class="strong"><strong>key value</strong></span> (<span class="strong"><strong>KV</strong></span>) storage and the memory manager.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec279"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark Cluster in any one of the following modes: Local, standalone, YARN, Mesos. You must also include the Spark MLlib package in the <code class="literal">build.sbt</code> file so that it downloads the related libraries and the API can be used. Install Hadoop (optionally), Scala, and Java.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec280"></a>How to do itâ€¦</h3></div></div></div><p>In this recipe, we'll learn how to download and install H2O services in a Spark Cluster. We'll also use the H2O API in Spark.</p><p>The list of sub-recipes in this section is as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Downloading and installing H2O</p></li><li style="list-style-type: disc"><p>Using H2O API in Spark</p></li></ul></div><p>The following are the steps for downloading and installing H2O:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Please download the Sparkling Water 1.6.1 from the following location: <a class="ulink" href="http://h2o-release.s3.amazonaws.com/sparkling-water/rel-1.6/1/index.html" target="_blank">http://h2o-release.s3.amazonaws.com/sparkling-water/rel-1.6/1/index.html</a>.</p></li><li><p>As per the latest addition of H2O, the compatible version of Spark is 1.6.0. The pre-requisite is to download and install Spark 1.6.0, point <code class="literal">SPARK_HOME</code> to the existing installation of Spark and also specify the <code class="literal">MASTER</code> variable in the <code class="literal">sparkling-env.sh</code> as follows:</p><div class="mediaobject"><img src="graphics/image_07_001.jpg" /></div></li><li><p>Now invokeÂ <code class="literal">sparkling-shell</code> as follows:</p><pre class="programlisting">
<span class="strong"><strong>         ./bin/sparkling-shell --conf "spark.executor.memory=1g"</strong></span>
</pre><div class="mediaobject"><img src="graphics/image_07_002-1024x292.jpg" /></div><div class="mediaobject"><img src="graphics/image_07_003-1024x248.jpg" /></div></li><li><p>Let's create <code class="literal">H2OContext</code> inside the Sparkling shell, as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/image_07_004-1024x343.jpg" /></div><div class="mediaobject"><img src="graphics/image_07_005.jpg" /></div></li><li><p>The <code class="literal">sparkling-shell</code> can also be launched on the <code class="literal">yarn-client</code> with specified executor properties, as follows:</p><pre class="programlisting">
<span class="strong"><strong>  bin/sparkling-shell --num-executors 3 --executor-memory 4g --
      driver-memory 4g   --master yarn-client</strong></span>
</pre></li></ol></div><p>The following are the steps for using H2O API in Spark:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Let's see how to use the H2O API in a standalone Spark application. Specify the following libraries in the <code class="literal">build.sbt</code> file:</p><pre class="programlisting">          libraryDependencies ++= Seq(
          "ai.h2o" % "sparkling-water-core_2.10" % "1.6.4",
          "ai.h2o" % "sparkling-water-ml_2.10" % "1.6.4" )
</pre></li><li><p>Please download the dataset from the following location <a class="ulink" href="https://github.com/ChitturiPadma/datasets/blob/master/chicagoCrimes.csv" target="_blank">https://github.com/ChitturiPadma/datasets/blob/master/chicagoCrimes.csv</a>. Here is the sample code which depicts the usage of H2O API:</p><pre class="programlisting">        import org.apache.spark._
        import org.apache.spark.sql._
        import org.apache.spark.h2o._
        import org.apache.spark.h2o.H2OContext
        object H2OSample_Demo {
          def main(args:Array[String]): Unit = {
          val conf = new SparkConf()
          .setMaster("spark://master:7077")
          .setAppName("H2O_SampleDemo")
          val sc = new SparkContext(conf)
          val sqlContext = new SQLContext(sc)
          val h2oContext = H2OContext.getOrCreate(sc)
          import h2oContext._
          import h2oContext.implicits._
          import sqlContext.implicits._
          val chicagoCrimesData =
          sqlContext.read.format("com.databricks.spark.csv")
          .option("header", "true")
          .option("inferSchema",
          "true").load("hdfs://namenode:9000/chicagoCrimes.csv")
          val h2oFrame = h2oContext.asH2OFrame(chicagoCrimesData)
          val means_H2OFrame = h2oFrame.means()
          means_H2OFrame.foreach(println)
           }
        }
</pre><p>The following is the output:</p><pre class="programlisting">    0.29282928292829286
    0.15231523152315232
    1159.6180618061808
    11.348988512757956
    22.954095409540987
    37.44764476447647
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec281"></a>How it worksâ€¦</h3></div></div></div><p>Initially, to install Sparkling Water, the version compatible with Spark 1.6.0 is downloaded and in the <code class="literal">sparkling-env.sh</code> file, the variables, such as <code class="literal">SPARK_HOME</code>, <code class="literal">MASTER</code>Â and <code class="literal">SPARK_WORKER_DIR</code>, are set and the Sparkling shell is invoked. This then initializes both <code class="literal">SparkContext</code> and <code class="literal">SQLContext</code> (as in <code class="literal">spark-shell</code>), and the corresponding H2O API is available from the shell by importing the <code class="literal">org.apache.spark.h2o</code> package.</p><p>Next, for a standalone application, the dependencies <code class="literal">sparkling-water-core_2.10</code> and <code class="literal">sparkling-water-ml_2.10</code> are included in the <code class="literal">build.sbt</code> file. In the preceding code snippet, the line <code class="literal">H2OContext.getOrCreate(sc)</code> creates the <code class="literal">H2OContext</code> if it doesn't exist, and retrieves if it already exists. Using the <code class="literal">SQLContext</code>, the CSV file is read and the line <code class="literal">h2oContext.asH2OFrame(chicagoCrimesData)</code> creates the <code class="literal">H2OFrame</code>. Finally, <code class="literal">h2oFrame.means()</code> displays the mean of the respective columns of the frame.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec282"></a>There's moreâ€¦</h3></div></div></div><p>In the next recipes, we'll see how we can run some of the machine learning algorithms using the H2O API.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec283"></a>See also</h3></div></div></div><p>For more details, please refer to the H2O documentation at <a class="ulink" href="http://docs.h2o.ai" target="_blank">http://docs.h2o.ai</a>.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec77"></a>Implementing k-means using H2O over Spark</h2></div></div><hr /></div><p>In this recipe, we'll look at how to run a k-means clustering algorithm on a dataset of figures concerning prostate cancer. Please download the dataset from <a class="ulink" href="https://github.com/ChitturiPadma/datasets/blob/master/prostate.csv" target="_blank">https://github.com/ChitturiPadma/datasets/blob/master/prostate.csv</a>. This is prostate cancer data that came from a study that examined the correlation between the level of prostate-specific antigen and a number of other clinical measures in men.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec284"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark Cluster in any one of the following modes: Local, standalone, YARN, Mesos. Include the Spark MLlib package in the <code class="literal">build.sbt</code> file so that it downloads the related libraries and the API can be used. Install Hadoop (optionally), Scala, and Java. Also, install Sparkling Water as discussed in the preceding recipe.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec285"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>The sample rows in the <code class="literal">prostate.csv</code> look like the following:</p><div class="mediaobject"><img src="graphics/Image5317.jpg" /></div></li><li><p>Here is the code to run k-means on the preceding dataset:</p><pre class="programlisting">        import org.apache.spark._
        import org.apache.spark.sql._
        import org.apache.spark.h2o._
        import hex.kmeans.KMeansModel.KMeansParameters
        import hex.kmeans.{KMeans, KMeansModel}
        import water._
        import water.support.SparkContextSupport
        object H2O_KmeansDemo {
        def main(args:Array[String]): Unit = {
        val conf = new SparkConf()
        .setMaster("spark://master:7077")
        .setAppName("H2O_KmeansDemo")
        val sc = new SparkContext(conf)
        val sqlContext = new SQLContext(sc)
        val h2oContext = H2OContext.getOrCreate(sc)
        import h2oContext._
        import h2oContext.implicits._
        import sqlContext.implicits._
        val prostateDf =
        sqlContext.read.format("com.databricks.spark.csv")
        .option("header", "true")
        .option("inferSchema",
         "true").load("hdfs://namenode:9000/prostate.csv")
         prostateDf.registerTempTable("prostate_table")
         val result = sqlContext.sql("SELECT * FROM prostate_table
         WHERE CAPSULE=1")
         val h2oFrame = h2oContext.asH2OFrame(result)
         /* Build a KMeans model, setting model parameters via a
         Properties */
         val model = runKmeans(h2oFrame)
         println(model)
         // Shutdown Spark cluster and H2O
         h2oContext.stop(stopSparkContext = true) }

         def runKmeans[T](trainDataFrame: H2OFrame): KMeansModel = {
         val params = new KMeansParameters
         params._train = trainDataFrame._key
         params._k = 3
         // Create a builder
         val job = new KMeans(params)
        // Launch a job and wait for the end.
        val kmm = job.trainModel.get
        // Print the JSON model
        println(new String(kmm._output.writeJSON(new
        AutoBuffer()).buf()))

        // Return a model
        kmm
         }
        }
</pre></li></ol></div><p>The following is the output:</p><div class="mediaobject"><img src="graphics/image_07_007-1024x344.jpg" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec286"></a>How it worksâ€¦</h3></div></div></div><p>Initially <code class="literal">SparkContext</code>, <code class="literal">SQLContext</code> and <code class="literal">H2OContext</code> are initialized and the <code class="literal">prostate.csv</code> file is loaded using the <code class="literal">sqlContext.read.format("com.databricks.spark.csv")</code>Â statement from HDFS. From the <code class="literal">prostateDf</code> DataFrame, a temporary table is created as <code class="literal">prostateDf.registerTempTable("prostate_table")</code>. The <code class="literal">sqlContext.sql("SELECT * FROM prostate_table WHERE CAPSULE=1")</code> statement fetches all the rows where <code class="literal">CAPSULE</code> is equal to <code class="literal">1</code>. Next, <code class="literal">H2OFrame</code> is created from the <code class="literal">h2oContext.asH2OFrame(result)</code> statement.</p><p>The <code class="literal">runKmeans</code> function takes anÂ <code class="literal">H2OFrame</code> as parameter, creates an instance of <code class="literal">KmeansParameters</code>Â and initializes the properties <code class="literal">_train</code> and <code class="literal">_k</code>. Next, the <code class="literal">Kmeans</code> object is created as a new <code class="literal">Kmeans(params)</code>. The <code class="literal">job.trainModel.get</code> line retrieves the <code class="literal">KmeansModel</code> and returns it.</p><p>The <code class="literal">runKmeans(h2oFrame)</code> line in the main function passes <code class="literal">H2OFrame</code> as the input, runs the <code class="literal">Kmeans</code> algorithm and returns <code class="literal">KmeansModel</code>. Finally, the <code class="literal">h2oContext.stop(stopSparkContext = true)</code> statement stops the <code class="literal">H2OContext</code> and also the <code class="literal">SparkContext</code>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec287"></a>There's moreâ€¦</h3></div></div></div><p>After submitting the resulting Sparkling Water application into a Spark Cluster, the application can create <code class="literal">H2OContext</code>, which initializes the H2O services on top of the Spark nodes. The application can then use any functionality provided by H2O, including its algorithms and interactive UI. H2O uses its own data structure called <span class="strong"><strong>H2OFrame</strong></span> to represent tabular data, but <code class="literal">H2OContext</code> allows H2O to share data with Spark's RDDs.</p><p>Also, the H2O API supports loading data directly into the H2OFrame from file(s) stored on local filesystems, HDFS, S3 and HTTP/HTTPS. Sparkling Water can read data stored in the CSV, SVMLight, and ARFF formats.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec288"></a>See also</h3></div></div></div><p>Please visit theÂ <span class="emphasis"><em>Working with H2O on SparkÂ </em></span>recipe to learn about the details of installing Sparkling Water and writing standalone Sparkling Water applications. For more details, please refer to the H2O documentation at <a class="ulink" href="http://docs.h2o.ai" target="_blank">http://docs.h2o.ai</a>.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec78"></a>Implementing spam detection with Sparkling Water</h2></div></div><hr /></div><p>In this recipe, we'll look at how to implement a spam detector by extracting data, transforming and tokenizing messages, building Spark's Tf-IDF model, and expanding messages to feature vectors. We'll also create and evaluate H2O's deep learning model. Lastly, we will use the models to detect spam messages.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec289"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark Cluster in any one of the following modes: Local, standalone, YARN, Mesos. Include the Spark MLlib package in the <code class="literal">build.sbt</code> file so that it downloads the related libraries and the API can be used. Install Hadoop (optionally), Scala, and Java. Also, install Sparkling Water as discussed in the preceding recipe.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec290"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Please download the dataset from <a class="ulink" href="https://github.com/ChitturiPadma/datasets/blob/master/smsData.txt" target="_blank">https://github.com/ChitturiPadma/datasets/blob/master/smsData.txt</a>. The records in the dataset look like the following:</p><pre class="programlisting">       ham   Ok... But they said i've got wisdom teeth hidden inside n
       mayb need 2 remove.
       ham   U thk of wat to eat tonight.
       ham   I dunno until when... Lets go learn pilates...
       spam Someonone you know is trying to contact you via our dating
       service! To find out who it could be call from your mobile or
       landline 09064015307 BOX334SK38ch
       ham   Ok c u then.
       spam  URGENT! We are trying to contact U. Todays draw shows that
       you have won a Â£800 prize GUARANTEED. Call 09050003091 from land
       line. Claim C52. Valid12hrs only spam  Not heard from U4 a
       while. Call 4 rude chat private line 01223585334 to cum. Wan 2C
       pics of me gettin shagged then text PIX to 8552. 2End send STOP
       8552 SAM xxx
       ham    staff.science.nus.edu.sg/~phyhcmk/teaching/pc1323
       ham    Thank god they are in bed!
</pre></li><li><p>Here is the code for spam detection:</p><pre class="programlisting">       import hex.ModelMetricsBinomial
       import hex.deeplearning.{DeepLearning, DeepLearningModel}
       import hex.deeplearning.DeepLearningModel.DeepLearningParameters
       import org.apache.spark.h2o._
       import org.apache.spark.mllib.feature.{HashingTF, IDF, IDFModel}
       import org.apache.spark.rdd.RDD
       import org.apache.spark.sql.{DataFrame, SQLContext}
       import org.apache.spark.{SparkConf, SparkContext, mllib}
       import water.support.{H2OFrameSupport, ModelMetricsSupport,
       SparkContextSupport}

       object H2O_SpamDetector extends SparkContextSupport with
       ModelMetricsSupport   with H2OFrameSupport {

       case class SMS(target: String, fv: mllib.linalg.Vector)

       val TEST_MSGS = Seq(
       "Michal, beer tonight in MV?",
       "We tried to contact you re your reply to our offer of a Video
       Handset? 750 anytime any networks mins? UNLIMITED TEXT?")

      def main(args:Array[String]): Unit = {
      val conf = new SparkConf()
               .setMaster("spark://master:7077")
               .setAppName("H2O_Spam_Detector")
      val sc = new SparkContext(conf)
      implicit val h2oContext = H2OContext.getOrCreate(sc)
      implicit val sqlContext = new SQLContext(sc)

      import h2oContext._
      import h2oContext.implicits._
      import sqlContext.implicits._

      // Loading Data
      val dataRdd = sc.textFile("hdfs://namenode:9000/smsData.txt")
      .map(l =&gt; l.split("\t")).filter(r =&gt; !r(0).isEmpty)

      //Extract response spam or ham
      val hamSpamRdd = dataRdd.map(r =&gt; r(0))
      val messageRdd = dataRdd.map(r =&gt; r(1))

      // Tokenize message content
      val tokensRdd = tokenize(messageRdd)

      // Build IDF model
      val (hashingTF, idfModel, tfidf) = buildIDFModel(tokensRdd)


     // Merge response with extracted vectors
     val resultDf: DataFrame = hamSpamRdd.zip(tfidf).map(v =&gt; SMS(v._1,
     v._2)).toDF

     val table:H2OFrame = h2oContext.asH2OFrame(resultDf)
     // Transform target column into
     table.replace(table.find("target"),
     table.vec("target").toCategoricalVec).remove()

     // Split table
     val keys = Array[String]("train.hex", "valid.hex")
     val ratios = Array[Double](0.8)
     val frs = split(table, keys, ratios)
     val (train, valid) = (frs(0), frs(1))
     table.delete()
     // Build a model
     val dlModel = buildDLModel(train, valid)
     // Collect model metrics
     val trainMetrics = modelMetrics[ModelMetricsBinomial](dlModel,
     train)
     val validMetrics = modelMetrics[ModelMetricsBinomial](dlModel,
     valid)

     println(
       s"""
          |AUC on train data = ${trainMetrics.auc}
          |AUC on valid data = ${validMetrics.auc}
        """.stripMargin)
     // Detect spam messages
     TEST_MSGS.foreach(msg =&gt; {
      println(s""" |"$msg" is ${if (isSpam(msg,sc, dlModel, hashingTF,
       idfModel)) "SPAM" else   "HAM"}
       """.stripMargin)
       })
      // Shutdown Spark cluster and H2O
     h2oContext.stop(stopSparkContext = true)
     }
     def tokenize(data: RDD[String]): RDD[Seq[String]] = {
     val ignoredWords = Seq("the", "a", "", "in", "on", "at", "as",
      "not", "for")
     val ignoredChars = Seq(',', ':', ';', '/', '&lt;', '&gt;', '"', '.',
     '(', ')', '?', '-',   ''','!','0', '1')
     val texts = data.map( r=&gt; {
     var smsText = r.toLowerCase
     for( c &lt;- ignoredChars) {
        smsText = smsText.replace(c, ' ')
      }
      val words =smsText.split(" ").filter(w =&gt;
      !ignoredWords.contains(w) &amp;&amp; w.length&gt;2).distinct
      words.toSeq
      })
      texts
     }

     /* Buil tf-idf model representing a text message. */
     def buildIDFModel(tokens: RDD[Seq[String]],
                    minDocFreq:Int = 4,
                    hashSpaceSize:Int = 1 &lt;&lt; 10): (HashingTF, IDFModel,
                    RDD[mllib.linalg.Vector]) = {
     // Hash strings into the given space
     val hashingTF = new HashingTF(hashSpaceSize)
     val tf = hashingTF.transform(tokens)
     // Build term frequency-inverse document frequency
     val idfModel = new IDF(minDocFreq = minDocFreq).fit(tf)
     val expandedText = idfModel.transform(tf)
     (hashingTF, idfModel, expandedText)
      }
      /** Builds DeepLearning model. */
      def buildDLModel(train: Frame, valid: Frame,
              epochs: Int = 10, l1: Double = 0.001, l2: Double = 0.0,
              hidden: Array[Int] = Array[Int](200, 200))
              (implicit h2oContext: H2OContext): DeepLearningModel = {
      import h2oContext.implicits._
      // Build a model
      val dlParams = new DeepLearningParameters()
      dlParams._train = train
      dlParams._valid = valid
      dlParams._response_column = 'target
      dlParams._epochs = epochs
      dlParams._l1 = l1
      dlParams._hidden = hidden

      // Create a job
      val dl = new DeepLearning(dlParams,water.Key.make("dlModel.hex"))
      dl.trainModel.get
       }

     /* Spam detector */
     def isSpam(msg: String,
             sc: SparkContext,
             dlModel: DeepLearningModel,
             hashingTF: HashingTF,
             idfModel: IDFModel,
             hamThreshold: Double = 0.5)
            (implicit sqlContext: SQLContext, h2oContext:
            H2OContext):Boolean = {
     import h2oContext.implicits._
     import sqlContext.implicits._
     val msgRdd = sc.parallelize(Seq(msg))
     val msgVector: DataFrame = idfModel.transform(
     hashingTF.transform (tokenize (msgRdd))).map(v =&gt;
     SMS("?", v)).toDF
     val msgTable: H2OFrame = msgVector
     msgTable.remove(0) // remove first column

     val prediction = dlModel.score(msgTable)
     //println(prediction)
     prediction.vecs()(1).at(0) &lt; hamThreshold
      }
     }
</pre><p>The following is the output:</p><pre class="programlisting">    AUC on train data = 0.9998817765671698
    AUC on valid data = 0.9821645021645021

    "Michal, beer tonight in MV?" is HAM
    "We tried to contact you re your reply to our offer of a Video
     Handset? 750 anytime any networks mins? UNLIMITED TEXT?" is SPAM
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec291"></a>How it worksâ€¦</h3></div></div></div><p>Initially, all the required libraries are imported. The <code class="literal">SparkContext</code> and <code class="literal">SQLContext</code> are initialized and the <code class="literal">smsData.txt</code> file is loaded using the <code class="literal">sc.textFile("hdfs://namenode:9000/smsData.txt").map(l =&gt; l.split("\t")).filter(r =&gt; !r(0).isEmpty)</code> line. The dataset consists of two columns, the first one being the label (<code class="literal">ham</code> or <code class="literal">spam</code> ) and the second one being the message itself. The H2O services are started using the <code class="literal">implicit val h2oContext = H2OContext.getOrCreate(sc)</code> line. Once the data is loaded, the first and second columns from the data are extracted as <code class="literal">val hamSpamRdd = dataRdd.map(r =&gt; r(0)) </code>and <code class="literal">val messageRdd = dataRdd.map(r =&gt; r(1))</code>.</p><p>Next, the <code class="literal">messageRDD</code> is sent as a parameter to the <code class="literal">tokenize</code> method, which then removes the stop words and some special characters. Once the messages are tokenized, the <code class="literal">tokensRDD</code> is sent as a parameter to <code class="literal">buildIDFModel</code>. This method instantiates Spark's <code class="literal">HashingTF</code> and vectorizes the words. The inverse document frequency object <code class="literal">IDF</code> is created as a new <code class="literal">IDF(minDocFreq = minDocFreq).fit(tf)</code>, and it creates a numerical representation of how much information a given word provides in the whole message. The extracted vectors are merged with <code class="literal">hamSpamRDD</code> as <code class="literal">hamSpamRdd.zip(tfidf).map(v =&gt; SMS(v._1, v._2)).toDF</code>.Â The H2OFrame is created from theÂ <code class="literal">h2oContext.asH2OFrame(result)</code>Â statement.</p><p>The dataset is split and the splits with specified keys are stored in H2O's distributed storage. The ratios are specified as <code class="literal">val ratios = Array[Double](0.8)</code>. Now, the table is split as per the specified keys and ratios using the <code class="literal">val frs = split(table, keys, ratios)</code> line. As per the split, <code class="literal">frs</code>, which is of the type <code class="literal">Array[Frame]</code>, contains two elements where the first element has 80% of the data and second element has 20% of the data. The 80% of the data becomes train and the remaining 20% becomes valid.</p><p>The <code class="literal">buildDLModel</code> method creates <code class="literal">DeepLearningParmeters</code> as <code class="literal">val dlParams = new DeepLearningParameters()</code>. Next, the properties of the <code class="literal">DeepLearningParmeters</code> are specified as <code class="literal">dlParams._train = train</code> and <code class="literal">dlParams._valid = valid</code>. The deep learning model is created as <code class="literal">val dl = new DeepLearning(dlParams, water.Key.make("dlModel.hex"))</code> and the trained model is returned as <code class="literal">dl.trainModel.get</code>. The model metrics from the training data (<code class="literal">train</code>) and from the validation data (<code class="literal">valid</code>) are obtained using the <code class="literal">val trainMetrics = modelMetrics[ModelMetricsBinomial](dlModel, train)</code> line.</p><p>The <code class="literal">isSpam</code> method predicts the unlabeled data. It uses the model generated by the pipeline. To make a prediction, we call the <code class="literal">transform</code> method. The prediction is obtained using the <code class="literal">dlModel.score(msgTable)</code> line, where <code class="literal">msgTable</code> is the <code class="literal">H2OFrame</code> created from the <code class="literal">msgVector</code> DataFrame.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec292"></a>There's moreâ€¦</h3></div></div></div><p>Sparkling Water enables transformation between different types of RDDs and H2O's H2OFrame, and vice versa. When converting from an H2OFrame to an RDD, a wrapper is created around the H2OFrame to provide an RDD-like API. In this case, data is not duplicated but served directly from the underlying H2OFrame. Converting from an RDD DataFrame to an H2OFrame requires data duplication because it transfers data from the RDD storage into the H2OFrame. However, data stored in an H2OFrame is heavily compressed and does not need to be preserved in RDD.</p><p>The <code class="literal">H2OContext</code> contains the necessary information for running H2O services and exposes methods for data transformation between the Spark RDD or DataFrame and the H2OFrame. Starting <code class="literal">H2OContext</code> involves a distributed operation that contacts all accessible Spark executor nodes and initializes H2O services.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec293"></a>See also</h3></div></div></div><p>Please visit the earlier <span class="emphasis"><em>Working with H2O on Spark</em></span> recipe to learn about the details of installing Sparkling Water and writing standalone Sparkling Water applications. You can also refer to the <span class="emphasis"><em>Implementing k-means using H2O over Spark</em></span> recipe. For more details, please refer to the H2O documentation at <a class="ulink" href="http://docs.h2o.ai" target="_blank">http://docs.h2o.ai</a>.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec79"></a>Deep learning with airlines and weather data</h2></div></div><hr /></div><p>In this recipe, we'll see how to run deep learning models on an airlines dataset.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec294"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark Cluster in any one of the following modes: Local, standalone, YARN, Mesos. Include the Spark MLlib package in the <code class="literal">build.sbt</code> file so that it downloads the related libraries and the API can be used. Install Hadoop (optionally), Scala, and Java. Also, install Sparkling Water as discussed in the preceding recipe.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec295"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Please download the dataset from <a class="ulink" href="https://github.com/ChitturiPadma/datasets/blob/master/allyears2k_headers.csv" target="_blank">https://github.com/ChitturiPadma/datasets/blob/master/allyears2k_headers.csv</a>. The sample records (with a few columns) in the dataset look like the following:</p><div class="mediaobject"><img src="graphics/Image317_7.jpg" /></div></li><li><p>Here is the code for loading the airline data and fetching records with the specific destination SFO:</p><pre class="programlisting">
      import hex.deeplearning.DeepLearning
      import hex.deeplearning.DeepLearningModel.DeepLearningParameters
      import org.apache.spark.{SparkContext, SparkConf, SparkFiles}
      import org.apache.spark.h2o.{DoubleHolder, H2OContext, H2OFrame}
      import org.apache.spark.sql.{SQLContext, Dataset}
      import water.support.{SparkContextSupport}

      object H2O_DeepLearning_AirlinesData {
      def main(args:Array[String]): Unit ={

      val conf = new SparkConf()
        .setMaster("spark://master:7077")
        .setAppName("H2O_DeepLearning_AirlinesData")
      val sc = new SparkContext(conf)
      val h2oContext = H2OContext.getOrCreate(sc)
      val sqlContext = new SQLContext(sc)

      import h2oContext._
      import h2oContext.implicits._
      import sqlContext.implicits._

      //Loading Data
      val airlinesData =
      sqlContext.read.format("com.databricks.spark.csv")
          .option("header", "true")
          .option("inferSchema",
      "true").load("hdfs://namenode:9000/allyears2k_headers.csv")

      //Create temporary table or view
      airlinesData.registerTempTable("airlinesTable")
      val result : H2OFrame = sqlContext.sql("SELECT * FROM
      airlinesTable WHERE Dest   LIKE 'SFO'")
      println(" Number of flights with destination in SFO:
      "+result.numRows())
       } }
</pre><p>The following is the output:</p><pre class="programlisting">          Number of flights with destination in SFO: 1331</pre></li><li><p>Now, let's see the code used to run the deep learning:</p><pre class="programlisting">      // Run Deep Learning
      // Training data
      val train = result('Year, 'Month, 'DayofMonth, 'DayOfWeek,
      'CRSDepTime, 'CRSArrTime,'UniqueCarrier, 'FlightNum, 'TailNum,
      'CRSElapsedTime, 'Origin,   'Dest,'Distance, 'IsDepDelayed )
       train.replace(train.numCols()-1
       train.lastVec().toCategoricalVec)
       train.update()

       // Configure Deep Learning algorithm
       val dlParams = new DeepLearningParameters()
       dlParams._train = train
       dlParams._response_column = 'IsDepDelayed
       val dl = new DeepLearning(dlParams)
       val dlModel = dl.trainModel.get

       // Use model for scoring
      println("Making prediction with help of DeepLearning model")
      val predictionH2OFrame = dlModel.score(result)
      predictions.vecs().map(_.at(0)).foreach(println)

      // Shutdown H2O
      h2oContext.stop(stopSparkContext = true)
</pre><p>The following is the output:</p><pre class="programlisting">      Making prediction with help of DeepLearning model

      1.0
      0.04475814491848565
      0.9552418550815144
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec296"></a>How it worksâ€¦</h3></div></div></div><p>Initially, all the required libraries are imported. The <code class="literal">SparkContext</code> and <code class="literal">SQLContext</code> are initialized and the <code class="literal">allyears2k_headers.csv</code> file is loaded using the <code class="literal">sqlContext.read.format("com.databricks.spark.csv")</code> line from HDFS. From the <code class="literal">airlinesData</code> DataFrame, a temporary table is created as <code class="literal">airlinesData.registerTempTable("airlinesTable")</code>. The <code class="literal">sqlContext.sql("SELECT * FROM airlinesTable WHERE Dest LIKE 'SFO'")</code> statement fetches all the airlines records whose <code class="literal">Dest</code> is <code class="literal">SFO</code>. Next, the result <code class="literal">H2OFrame</code>Â is created from the query result.</p><p>Next, for the model training, fields such as <code class="literal">'Year</code>, <code class="literal">'Month</code>, <code class="literal">'DayofMonth</code>, <code class="literal">'DayOfWeek</code>, <code class="literal">'CRSDepTime</code>, <code class="literal">'CRSArrTime</code>,<code class="literal">'UniqueCarrier</code>, <code class="literal">'FlightNum</code>, <code class="literal">'TailNum</code>, <code class="literal">'CRSElapsedTime</code>, <code class="literal">'Origin</code>, <code class="literal">'Dest</code>, <code class="literal">'Distance</code>Â and <code class="literal">'IsDepDelayed</code> are selected from the <code class="literal">H2OFrame</code> result. The <code class="literal">DeepLearningParameters</code> is created as <code class="literal">val dlParams = new DeepLearningParameters()</code>. Next, the properties of the <code class="literal">DeepLearningParameters</code> are specified as <code class="literal">dlParams._train = train and dlParams._response_column = 'IsDepDelayed</code>. From the <code class="literal">DeepLearningParameters</code>, the deep learning model is created as <code class="literal">val dl = new DeepLearning(dlParams)</code>. The trained model is obtained as <code class="literal">dl.trainModel.get</code>. The predictions are obtained using the line <code class="literal">dlModel.score(result)</code>, and finally the predictions are displayed.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec297"></a>There's moreâ€¦</h3></div></div></div><p>When launching Sparkling Water applications, both Spark and H2O are in-memory processes and all computation occurs in memory with minimal writing to disk, occurring only when specified by the user. Because all the data used in the modeling process needs to read into memory, the recommended method of launching Spark and H2O is through YARN, which dynamically allocates available resources. When the job is finished, you can tear down the Sparkling Water cluster and free up resources for other jobs.</p><p>All Spark and Sparkling Water applications launched with YARN will be tracked and listed in the history server that you can launch on Cloudera Manager. YARN will allocate the container to launch the application master in and when you launch with yarn-client, the Spark driver runs in the client process and the application master submits a request to the resource manager to spawn the Spark executor JVMs. Finally, after creating a Sparkling Water cluster, you have access to HDFS to read data into either H2O or Spark.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec298"></a>See also</h3></div></div></div><p>Please visit the earlier <span class="emphasis"><em>Working with H2O on Spark</em></span> recipe to find out about the details of installing Sparkling Water and writing standalone Sparkling Water applications. You can also refer to the recipes <span class="emphasis"><em>Implementing k-means using H2O over Spark</em></span> and <span class="emphasis"><em>Implementing spam detection with Sparkling Water</em></span>. For more details, please refer to the H2O documentation at <a class="ulink" href="http://docs.h2o.ai" target="_blank">http://docs.h2o.ai</a>.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec80"></a>Implementing a crime detection application</h2></div></div><hr /></div><p>In this recipe, we'll see how to run deep learning models on various sets of data to detect crime in the city of Chicago.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec299"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark Cluster in any one of the following modes: Local, standalone, YARN, Mesos. Include the Spark MLlib package in the <code class="literal">build.sbt</code> file so that it downloads the related libraries and the API can be used. Install Hadoop (optionally), Scala, and Java. Also, install Sparkling Water as discussed in the preceding recipe.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec300"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Please download the following datasets from the following locations:</p><p>Weather data: <a class="ulink" href="https://github.com/ChitturiPadma/datasets/blob/master/chicagoAllWeather.csv" target="_blank">https://github.com/ChitturiPadma/datasets/blob/master/chicagoAllWeather.csv</a>.</p><p>Census data:<a class="ulink" href="https://github.com/ChitturiPadma/datasets/blob/master/chicagoCensus.csv" target="_blank">https://github.com/ChitturiPadma/datasets/blob/master/chicagoCensus.csv</a>.</p><p>Crime data: <a class="ulink" href="https://github.com/ChitturiPadma/datasets/blob/master/chicagoCrimes10k.csv" target="_blank">https://github.com/ChitturiPadma/datasets/blob/master/chicagoCrimes10k.csv</a>.</p></li><li><p>The sample records (with a few columns) in the datasets look as follows:</p><p>The sample rows in weather data:</p><div class="mediaobject"><img src="graphics/Capture-3-1.jpg" /></div><p>The sample rows in census data:</p><div class="mediaobject"><img src="graphics/Capture-4-1.jpg" /></div><p>The sample rows in crime data:</p><div class="mediaobject"><img src="graphics/B05317_07_11.jpg" /></div></li><li><p>Include the dependency <code class="literal">ai.h2o" % "h2o-genmodel" % "3.10.0.7</code> in the <code class="literal">build.sbt</code> file.</p></li><li><p>The code for loading the datasets and fetching the required records for joining the preceding datasets <code class="literal">chicagoAllWeather.csv</code>, <code class="literal">chicagoCensus.csv</code>, <code class="literal">chicagoCrimes10k.csv</code>Â looks like the following:</p><pre class="programlisting">      import hex.deeplearning.DeepLearningModel
      import hex.deeplearning.DeepLearningModel.DeepLearningParameters
      import hex.deeplearning.DeepLearningModel.DeepLearningParameters
      .Activation
      import hex.tree.gbm.GBMModel
      import hex.{Model, ModelMetricsBinomial}
      import org.apache.spark.SparkContext
      import org.apache.spark.h2o.{H2OContext, H2OFrame}
      import org.apache.spark.sql.{DataFrame, SQLContext}
      import org.joda.time.DateTimeConstants._
      import org.joda.time.format.DateTimeFormat
      import org.joda.time.{DateTimeZone, MutableDateTime}
      import water.MRTask
      import water.fvec.{Chunk, NewChunk, Vec}
      import water.parser.{BufferedString, ParseSetup}
      import water.support.{H2OFrameSupport, ModelMetricsSupport,
      SparkContextSupport, SparklingWaterApp}
      import java.net._
      import org.apache.spark._

      class RefineDateColumn(val datePattern: String,
                             val dateTimeZone: String) extends
                              MRTask[RefineDateColumn] {
      // Entry point
      def doIt(col: Vec): H2OFrame = {
      val inputCol = if (col.isCategorical) col.toStringVec else col
      val result = new H2OFrame(
      doAll(Array[Byte](Vec.T_NUM, Vec.T_NUM, Vec.T_NUM, Vec.T_NUM,
      Vec.T_NUM, Vec.T_NUM, Vec.T_NUM, Vec.T_NUM),
      inputCol).outputFrame(
      Array[String]("Day", "Month", "Year", "WeekNum", "WeekDay",
      "Weekend", "Season","HourOfDay"),
      Array[Array[String]](null, null, null, null, null, null,
      H2O_ChicagoCrimeAppNew_H2O.SEASONS, null)))
         if (col.isCategorical) inputCol.remove()
      result
      }

      override def map(cs: Array[Chunk], ncs: Array[NewChunk]): Unit =
      {
      /* Initialize DataTime convertor (cannot be done in setupLocal
      since it is not H2O serializable */
      val dtFmt = DateTimeFormat.forPattern(datePattern)
      .withZone(DateTimeZone.forID(dateTimeZone))
      // Get input and output chunks
      val dateChunk = cs(0)
      val (dayNC, monthNC, yearNC, weekNC, weekdayNC, weekendNC,
      seasonNC, hourNC)
      = (ncs(0), ncs(1), ncs(2), ncs(3), ncs(4), ncs(5), ncs(6),
      ncs(7))
      val valStr = new BufferedString()
      val mDateTime = new MutableDateTime()
      for(row &lt;- 0 until dateChunk.len()) {
        if (dateChunk.isNA(row)) {
          addNAs(ncs)
        } else {

        // Extract data
          val ds = dateChunk.atStr(valStr, row).toString
          if (dtFmt.parseInto(mDateTime, ds, 0) &gt; 0) {
          val month = mDateTime.getMonthOfYear
          dayNC.addNum(mDateTime.getDayOfMonth, 0)
          monthNC.addNum(month, 0)
          yearNC.addNum(mDateTime.getYear, 0)
          weekNC.addNum(mDateTime.getWeekOfWeekyear)
          val dayOfWeek = mDateTime.getDayOfWeek
          weekdayNC.addNum(dayOfWeek)
          weekendNC.addNum(if (dayOfWeek == SUNDAY || dayOfWeek ==
          SATURDAY) 1 else 0, 0)
          seasonNC.addNum(ChicagoCrimeAppNew_H2O.getSeason(month), 0)
          hourNC.addNum(mDateTime.getHourOfDay)
          } else {
          addNAs(ncs)
          }  } } }
          private def addNAs(ncs: Array[NewChunk]): Unit =
          ncs.foreach(nc =&gt; nc.addNA())
          }
         object ChicagoCrimeAppNew_H2O {
         def SEASONS = Array[String]("Spring", "Summer", "Autumn",
         "Winter")
         def getSeason(month: Int) =
         if (month &gt;= MARCH &amp;&amp; month &lt;= MAY) 0 // Spring
         else if (month &gt;= JUNE &amp;&amp; month &lt;= AUGUST) 1 // Summer
         else if (month &gt;= SEPTEMBER &amp;&amp; month &lt;= OCTOBER) 2 // Autumn
         else 3 // Winter

         def loadData(datafile: String, modifyParserSetup: ParseSetup
         =&gt; ParseSetup =   identity[ParseSetup]): H2OFrame = {
         val uri = java.net.URI.create(datafile)
         val parseSetup =
         modifyParserSetup(water.fvec.H2OFrame.parserSetup(uri))
         new H2OFrame(parseSetup, new java.net.URI(datafile))
         }

         def createWeatherTable(datafile: String): H2OFrame = {
         val table = loadData(datafile)
         // Remove first column since we do not need it
         table.remove(0).remove()
         table.update()
         table
         }

         def createCensusTable(datafile: String): H2OFrame = {
         val table = loadData(datafile)
         // Rename columns: replace ' ' by '_'
         val colNames = table.names().map( n =&gt; n.trim.replace(' ',
         '_').replace('+','_'))
         table._names = colNames
         table.update()
         table
         }

         def createCrimeTable(datafile: String): H2OFrame = {
         val table = loadData(datafile, (parseSetup: ParseSetup) =&gt; {
         val colNames = parseSetup.getColumnNames
         val typeNames = parseSetup.getColumnTypes
         colNames.indices.foreach { idx =&gt;
         if (colNames(idx) == "Date") typeNames(idx) = Vec.T_STR
         }
         parseSetup
         })

         // Refine date into multiple columns
         val dateCol = table.vec(2)
         table.add(new RefineDateColumn("MM/dd/yyyy hh:mm:ss a",
         "Etc/UTC").doIt(dateCol))
         // Update names, replace all ' ' by '_'
         val colNames = table.names().map( n =&gt; n.trim.replace(' ',
          '_'))
         table._names = colNames
         // Remove Date column
         table.remove(2).remove()
         // Update in DKV
         table.update()
         table
         }
        def main(args:Array[String]): Unit = {
        val conf = new SparkConf()
        .setMaster("spark://master:7077")
        .setAppName("H2O_ChicagoCrimeApp")
        val sc = new SparkContext(conf)
        implicit val h2oContext = H2OContext.getOrCreate(sc)
        implicit val sqlContext = new SQLContext(sc)
        import h2oContext._
        import h2oContext.implicits._
        import sqlContext.implicits._
        // Loading Weather Data
        val weatherDataTable = asDataFrame(createWeatherTable
        ("hdfs://namenode:9000/chicagoAllWeather.csv"))
        val censusDataTable = asDataFrame(createCensusTable
        ("hdfs://namenode:9000/chicagoCensus.csv"))
        val crimeDataTable = asDataFrame(createCrimeTable
        ("hdfs://namenode:9000/chicagoCrimes10k.csv"))
        weatherDataTable.registerTempTable("chicagoWeather")
        censusDataTable.registerTempTable("chicagoCensus")
        crimeDataTable.registerTempTable("chicagoCrime")
        val crimeWeather = sqlContext.sql(
        """SELECT
        |a.Year, a.Month, a.Day, a.WeekNum, a.HourOfDay, a.Weekend,
        a.Season, a.WeekDay,
        |a.IUCR, a.Primary_Type, a.Location_Description,
        a.Community_Area, a.District,
        |a.Arrest, a.Domestic, a.Beat, a.Ward, a.FBI_Code,
        |b.minTemp, b.maxTemp, b.meanTemp,
        |c.PERCENT_AGED_UNDER_18_OR_OVER_64, c.PER_CAPITA_INCOME,
        c.HARDSHIP_INDEX,
        |c.PERCENT_OF_HOUSING_CROWDED,
        c.PERCENT_HOUSEHOLDS_BELOW_POVERTY,
        |c.PERCENT_AGED_16__UNEMPLOYED,
        c.PERCENT_AGED_25__WITHOUT_HIGH_SCHOOL_DIPLOMA
        |FROM chicagoCrime a
        |JOIN chicagoWeather b
        |ON a.Year = b.year AND a.Month = b.month AND a.Day = b.day
        |JOIN chicagoCensus c
        |ON a.Community_Area = c.Community_Area_Number""".stripMargin)

        val crimeWeatherDF:H2OFrame = crimeWeather
        // Transform all string columns into categorical
        val crimeWeatherDataFrame = asDataFrame(crimeWeatherDF)
        crimeWeatherDataFrame.select("Year","Month","Day","WeekNum",
        "Season","IUCR").show(10)
         }
       }
</pre><p>The following is the output:</p><pre class="programlisting">+----+-----+---+-------+------+----+
|Year|Month|Day|WeekNum|Season|IUCR|
+----+-----+---+-------+------+----+
|2015|    1| 23|      4|Winter|null|
|2015|    1| 23|      4|Winter|4625|
|2015|    1| 23|      4|Winter| 320|
|2015|    1| 23|      4|Winter|1310|
|2015|    1| 23|      4|Winter| 610|
|2015|    1| 23|      4|Winter|2210|
|2015|    1| 23|      4|Winter| 470|
|2015|    1| 23|      4|Winter|1305|
|2015|    1| 23|      4|Winter| 486|
|2015|    1| 23|      4|Winter| 820|
+----+-----+---+-------+------+----+
</pre></li></ol></div><p>Now, let's try to run the GBM model on the <code class="literal">crimeWeatherDF</code> and collect the model metrics with the following code:</p><pre class="programlisting">
  def main(args:Array[String]) {
  // Previous code
  H2OFrameSupport.allStringVecToCategorical(crimeWeatherDF)
  val keys = Array[String]("train.hex", "test.hex")
  val ratios = Array[Double](0.8, 0.2)
  val frs = H2OFrameSupport.split(crimeWeatherDF, keys, ratios)
  val (train, test) = (frs(0), frs(1))

  // Build GBM model and collect model metrics
  val gbmModel = GBMModel(train, test, 'Arrest)
  val (trainMetricsGBM, testMetricsGBM) = binomialMetrics(gbmModel, train, test)

  println(
    s"""Model performance:
        |  GBM:
        |    train AUC = ${trainMetricsGBM.auc}
        |    test  AUC = ${testMetricsGBM.auc}
    """.stripMargin) }

  def GBMModel(train: H2OFrame, test: H2OFrame, response: String, ntrees:Int =   10, depth:Int = 6, family: DistributionFamily = DistributionFamily.bernoulli)
            (implicit h2oContext: H2OContext) : GBMModel = {
    import h2oContext.implicits._
    import hex.tree.gbm.GBM
    import hex.tree.gbm.GBMModel.GBMParameters
    val gbmParams = new GBMParameters()
    gbmParams._train = train
    gbmParams._valid = test
    gbmParams._response_column = response
    gbmParams._ntrees = ntrees
    gbmParams._max_depth = depth
    val gbm = new GBM(gbmParams)
    val model = gbm.trainModel.get
    model
  }
  def binomialMetrics[M &lt;: Model[M,P,O], P &lt;: hex.Model.Parameters, O &lt;:   hex.Model.Output]
  (model: Model[M,P,O], train: H2OFrame, test: H2OFrame):(ModelMetricsBinomial,   ModelMetricsBinomial) = {
    (ModelMetricsSupport.modelMetrics(model,train),   ModelMetricsSupport.modelMetrics(model, test))
  }
</pre><p>The following is the output:</p><pre class="programlisting">  Model performance:
  GBM:
    train AUC = 0.9182010072171471
    test  AUC = 0.9366698161864178
</pre><p>In the following code snippet, we train a deep neural network to predict the likelihood of an arrest for a given crime as follows:</p><pre class="programlisting">  def main(args:Array[String]) {
  // Previous code
  // Build Deep Learning model and collect model metrics
  val dlModel = DLModel(train, test, 'Arrest)
  val (trainMetricsDL, testMetricsDL) = binomialMetrics(dlModel, train,
  test)
  println(
    s"""Model performance:
      |  DL:
      |    train AUC = ${trainMetricsDL.auc}
      |    test  AUC = ${testMetricsDL.auc}
    """.stripMargin) }
  def DLModel(train: H2OFrame, test: H2OFrame, response: String,
            epochs: Int = 10, l1: Double = 0.0001, l2: Double = 0.0001,
            activation: Activation = Activation.RectifierWithDropout,
  hidden:Array[Int] =   Array(200,200))
           (implicit h2oContext: H2OContext) : DeepLearningModel = {
    import h2oContext.implicits._
    import hex.deeplearning.DeepLearning
    val dlParams = new DeepLearningParameters()
    dlParams._train = train
    dlParams._valid = test
    dlParams._response_column = response
    dlParams._epochs = epochs
    dlParams._l1 = l1
    dlParams._l2 = l2
    dlParams._activation = activation
    dlParams._hidden = hidden
    // Create a job
    val dl = new DeepLearning(dlParams)
    val model = dl.trainModel.get
    model }
</pre><p>The output is as follows:</p><pre class="programlisting">Model performance:
DL:
train AUC = 0.9032203563709825
test AUC = 0.9358068689123377</pre><p>Let's look at the last building block of the application, which predicts the arrest rate probability for a new crime. The function combines the Spark API to enrich each incoming crime event with census information with H2O's deep learning model, which scores the event:</p><pre class="programlisting">  case class Crime(Year: Short, Month: Byte, Day: Byte, WeekNum: Byte,
  HourOfDay:Byte,
                 Weekend:Byte, Season: String, WeekDay: Byte,
                 IUCR: Short,
                 Primary_Type: String,
                 Location_Description: String,
                 Domestic: String,
                 Beat: Short,
                 District: Byte,
                 Ward: Byte,
                 Community_Area: Byte,
                 FBI_Code: Byte,
                 minTemp: Option[Byte],
                 maxTemp: Option[Byte],
                 meanTemp: Option[Byte])
    object Crime {
    def apply(date:String,
            iucr: Short,
            primaryType: String,
            locationDescr: String,
            domestic: Boolean,
            beat: Short,
            district: Byte,
            ward: Byte,
            communityArea: Byte,
            fbiCode: Byte,
            minTemp: Option[Byte] = None,
            maxTemp: Option[Byte] = None,
            meanTemp: Option[Byte] = None,
            datePattern: String = "MM/dd/yyyy hh:mm:ss a",
            dateTimeZone: String = "Etc/UTC"):Crime = {
    val dtFmt = DateTimeFormat.forPattern(datePattern).withZone
    (DateTimeZone.forID(dateTimeZone))
    val mDateTime = new MutableDateTime()
    dtFmt.parseInto(mDateTime, date, 0)
    val month = mDateTime.getMonthOfYear.toByte
    val dayOfWeek = mDateTime.getDayOfWeek
    Crime(mDateTime.getYear.toShort,
      month,
      mDateTime.getDayOfMonth.toByte,
      mDateTime.getWeekOfWeekyear.toByte,
      mDateTime.getHourOfDay.toByte,
      if (dayOfWeek == SUNDAY || dayOfWeek == SATURDAY) 1 else 0,
      ChicagoCrimeAppNew_H2O.SEASONS(ChicagoCrimeAppNew_H2O.getSeason
      (month)),
      mDateTime.getDayOfWeek.toByte,
      iucr, primaryType, locationDescr,
      if (domestic) "true" else "false" ,
      beat, district, ward, communityArea, fbiCode,
      minTemp, maxTemp, meanTemp)  } }

      def scoreEvent(crime: Crime, model: Model[_,_,_], censusTable:
      DataFrame)
  (implicit sqlContext: SQLContext, h2oContext: H2OContext): Float = {
  import h2oContext.implicits._
  import sqlContext.implicits._
  // Create a single row table
  val srdd: DataFrame =
  sqlContext.sparkContext.parallelize(Seq(crime)).toDF
  // Join table with census data
  val row: H2OFrame = censusTable.join(srdd).where('Community_Area ===
  'Community_Area_Number) //.printSchema
  // Transform all string columns into categorical
  H2OFrameSupport.allStringVecToCategorical(row)
  val predictTable = model.score(row)
  val probOfArrest = predictTable.vec("true").at(0)
  probOfArrest.toFloat }

   def main(args:Array[String]) {
  // Previous code

  // Test the arrest rate probability for a new Crime.
  val crimeExamples = Seq(
  Crime("02/08/2015 11:43:58 PM", 1811, "NARCOTICS", "STREET",false,
  422, 4, 7, 46, 18),
  Crime("02/08/2015 11:00:39 PM", 1150, "DECEPTIVE PRACTICE",
  "RESIDENCE",false, 923, 9, 14, 63, 11))
  for (crime &lt;- crimeExamples) {
  val arrestProbGBM = 100*scoreEvent(crime,
  gbmModel,
  censusDataTable)(sqlContext, h2oContext)
  val arrestProbDL = 100*scoreEvent(crime,
  dlModel,
  censusDataTable)(sqlContext, h2oContext)

  println(
    s"""
       |Crime: $crime
       |  Probability of arrest best on DeepLearning: ${arrestProbDL} %
       |  Probability of arrest best on GBM: ${arrestProbGBM} %
       |
    """.stripMargin)
 } }
</pre><p>The following is the output:</p><pre class="programlisting">  Crime:
  Crime(2015,2,8,6,23,1,Winter,7,1811,NARCOTICS,STREET,false,422,4,
  7,46,18,None,None,None)
  Probability of arrest best on DeepLearning: 99.97552 %
  Probability of arrest best on GBM: 74.49276 %

  Crime: Crime(2015,2,8,6,23,1,Winter,7,1150,DECEPTIVE
  PRACTICE,RESIDENCE,false,923,9,14,63,11,None,None,None)
  Probability of arrest best on DeepLearning: 1.6130093 %
  Probability of arrest best on GBM: 12.061813 %
</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec301"></a>How it worksâ€¦</h3></div></div></div><p>Initially, all the required libraries are imported. The <code class="literal">SparkContext</code> and <code class="literal">SQLContext</code> are initialized and the <code class="literal">chicagoAllWeather.csv</code>, <code class="literal">chicagoCensus.csv</code>Â and <code class="literal">chicagoCrimes10k.csv</code> files are loaded using the <code class="literal">hdfs://namenode:9000/chicagoAllWeather.csv</code>Â line. The <code class="literal">createWeatherTable</code>, <code class="literal">createCensusTable</code>, and <code class="literal">createCrimeTable</code> methods invoke <code class="literal">loadData</code>, which creates an <code class="literal">H2OFrame</code> from the <code class="literal">java.net.URI</code>.</p><p>The <code class="literal">RefineDateColumn</code> class formats the date and extracts the year, month and day. The <code class="literal">crimeWeatherDF</code> is an <code class="literal">H2OFrame</code> created by selecting the required fields from the DataFrame, and all strings are converted to categorical variables using the <code class="literal">H2OFrameSupport.allStringVecToCategorical(crimeWeatherDF)</code> line.</p><p>The dataset is split and the splits with specified keys are stored into H2O's distributed storage. The ratios are specified as <code class="literal">val ratios = Array[Double](0.8)</code>. Now, the table is split as per the specified keys and ratios using the <code class="literal">val frs = </code><code class="literal">H2OFrameSupport.</code><code class="literal">split(</code><code class="literal">crimeWeatherDF</code><code class="literal">, keys, ratios)</code> line. As per the split, <code class="literal">frs</code>, which is of the type <code class="literal">Array[Frame]</code>, contains two elements where the first element has 80% of the data and the second element has 20% of the data. The 80% of the data becomes train and the remaining 20% becomes valid.</p><p>The <code class="literal">GBMModel</code> is built from the train and test datasets and the model metrics are obtained by invoking the <code class="literal">binomialMetrics(dlModel, train, test)</code> method. Also, a deep neural network is trained to predict the likelihood of an arrest for a given crime. Finally, the <code class="literal">GBMModel</code> and <code class="literal">DLModel</code> accuracy is displayed.</p><p>Once the models are ready, the <code class="literal">scoreEvent</code> method is invoked, which predicts the arrest rate probability for a new crime using both <code class="literal">GBMModel</code> and <code class="literal">DeepLearningModel</code>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec302"></a>There's moreâ€¦</h3></div></div></div><p>There are some incredible applications of deep learning with respect to image recognition and machine translation, but the preceding specific use case shows how deep learning can be used to fight crime in the forward-thinking cities of San Francisco and Chicago. Since both are open cities, anybody can access city data ranging from transportation information to building maintenance records. The cities' data is joined with other external data, such as weather and socio-economic statistics, to predict the probability of an arrest for a crime.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec303"></a>See also</h3></div></div></div><p>Please visit the earlier <span class="emphasis"><em>Working with H2O on Spark</em></span> recipe to learn about the details of installing Sparkling Water and writing standalone Sparkling Water applications. You can also refer to the <span class="emphasis"><em>Implementing k-means using H2O over Spark</em></span> and <span class="emphasis"><em>Deep learning with airlines and weather data</em></span> recipes. For more details, please refer to the H2O documentation at <a class="ulink" href="http://docs.h2o.ai/" target="_blank">http://docs.h2o.ai</a>.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec81"></a>Running SVM with H2O over Spark</h2></div></div><hr /></div><p>In this recipe, we'll see how to run SVM to predict or classify a cancer.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec304"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark Cluster in any one of the following modes: Local, standalone, YARN, Mesos. Include the Spark MLlib package in the <code class="literal">build.sbt</code> file so that it downloads the related libraries and the API can be used. Install Hadoop (optionally), Scala, and Java. Also, install Sparkling Water as discussed in the preceding recipe.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec305"></a>How to do itâ€¦</h3></div></div></div><p>Please download the dataset from <a class="ulink" href="https://github.com/ChitturiPadma/datasets/blob/master/Breast_CancerData.csv" target="_blank">https://github.com/ChitturiPadma/datasets/blob/master/Breast_CancerData.csv</a>. While including the dependencies <code class="literal">sparkling-water-core</code> and <code class="literal">sparkling-water-ml</code>, please change the version to 1.6.8.</p><p>The sample records in the data (with a few columns) look as follows:</p><div class="mediaobject"><img src="graphics/B05317_07_12.jpg" /></div><p>Here, the last column <code class="literal">label</code> indicates whether the person has breast cancer (represented by <code class="literal">B</code>).</p><p>The code that runs SVM on the data is as follows:</p><pre class="programlisting">  import java.io._
  import org.apache.spark.ml.spark.models.svm._
  import org.apache.spark.h2o.H2OContext
  import org.apache.spark.sql.SQLContext
  import org.apache.spark.{SparkConf, SparkContext, SparkFiles}
  import water.fvec.H2OFrame
  import water.support.SparkContextSupport

  object H2O_SVM {
    def main(args: Array[String]): Unit = {
      val conf = new SparkConf()
        .setMaster("spark://master:7077")
        .setAppName("H2O_SVMDemo")
      val sc = new SparkContext(conf)
      implicit val h2oContext = H2OContext.getOrCreate(sc)
      implicit val sqlContext = new SQLContext(sc)
      val breastCancerData = new H2OFrame(new File)

      // Training data
  breastCancerData.replace(breastCancerData.numCols()-1,
  breastCancerData.lastVec().toCategoricalVec)
  breastCancerData.update()

  // Configure DeepLearning Algorithm
      val parms = new SVMParameters
      parms._train = breastCancerData.key

  parms._response_column = "label"
  val svm = new SVM(parms, h2oContext)
  val svmModel = svm.trainModel.get


    // Use model for scoring
   val predictionH2OFrame = svmModel.score(breastCancerData)
   val predictionsFromModel =
   h2oContext.asDataFrame(predictionH2OFrame).collect
   println(predictionsFromModel.mkString("\n===&gt; Model predictions: ",
   ",\n",  ", ...\n"))
   h2oContext.stop(stopSparkContext = true)} }
</pre><p>The following is the output:</p><div class="mediaobject"><img src="graphics/5317_10.jpg" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec306"></a>How it worksâ€¦</h3></div></div></div><p>Initially, all the required libraries are imported. The <code class="literal">SparkContext</code> and <code class="literal">H2OContext</code> are initialized and the file <code class="literal">Breast_CancerData.csv</code> is loaded using the <code class="literal">new H2OFrame(new File "(hdfs://namenode:9000/Breast_CancerData.csv"))</code> line. The <code class="literal">label</code> column is replaced with a categorical vector using the <code class="literal">breastCancerData.replace(breastCancerData.numCols()-1, breastCancerData.lastVec().toCategoricalVec)</code> line.</p><p>The <code class="literal">SVMParameters</code> is created, and the properties, such as <code class="literal">_train</code> and <code class="literal">_reponse_column</code>, are initialized. The object SVM is created as <code class="literal">val svm = new SVM(parms, h2oContext)</code>, and the <code class="literal">SVMModel</code> is obtained as <code class="literal">val svmModel = svm.trainModel.get</code>. The model is used for scoring the data as <code class="literal">val predictionH2OFrame = svmModel.score(breastCancerData)</code>. The predicted scores are obtained as <code class="literal">val predictionsFromModel = h2oContext.asDataFrame(predictionH2OFrame).collect</code>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec307"></a>There's moreâ€¦</h3></div></div></div><p>In the preceding recipe, we saw how to implement <code class="literal">SVMModel</code> on the data, which obtains the scores for the target variable.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec308"></a>See also</h3></div></div></div><p>You can look again at the earlier <span class="emphasis"><em>Working with H2O on Spark</em></span> recipe to learn about the details of installing Sparkling Water and writing standalone Sparkling Water applications. You can also refer to the <span class="emphasis"><em>Deep learning with airlines and weather data</em></span> and <span class="emphasis"><em>Implementing crime detection application</em></span> recipes. For more details, please refer to the H2O documentation at <a class="ulink" href="http://docs.h2o.ai/" target="_blank">http://docs.h2o.ai</a>.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch08"></a>ChapterÂ 8.Â Data Visualization with Spark</h2></div></div></div><p>In this chapter, you will learn the following recipes:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Visualization using Zeppelin</p></li><li style="list-style-type: disc"><p>Creating scatter plots with Bokeh-Scala</p></li><li style="list-style-type: disc"><p>Creating a time series MultiPlot with Bokeh-Scala</p></li><li style="list-style-type: disc"><p>Creating plots with the lightning visualization server</p></li><li style="list-style-type: disc"><p>Visualizing machine learning models with Databricks notebook</p></li></ul></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec82"></a>Introduction</h2></div></div><hr /></div><p>Visualizing large data is challenging. There are more data points than possible pixels and manipulating distributed data can take a long time. Along with the increase in volume, there are new kinds of datasets which are becoming more and more mainstream. The need to analyze user comments, sentiments, customer calls and various unstructured data has resulted in the use of new kinds of visualizations. The use of graph databases and visualization to represent unstructured data is an example of how things are changing because of increased variety.</p><p>There are a variety of tools developed recently which allow interactive analysis with Spark by reducing query latency to the range of human interactions through caching. Additionally, Spark's unified programming model and diverse programming interfaces enable smooth integration with popular visualization tools. We can use these to perform both exploratory and expository visualization over large data. In this chapter, we are going to look at the most widely used visualization techniques with Spark, such as Apache Zeppelin, Lightning, and highly active Scala bindings (Bokeh-Scala).</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec83"></a>Visualization using Zeppelin</h2></div></div><hr /></div><p>Apache Zeppelin is a nifty web-based tool that helps us visualize and explore large datasets. From a technical standpoint, Apache Zeppelin is a web application on steroids. We aim to use this application to render some neat, interactive, and shareable graphs and charts.</p><p>The interesting part of Zeppelin is that it has a bunch of built-in interpreters--ones that can interpret and invoke all API functions in Spark (with a SparkContext ) and Spark SQL (with a SQLContext ). The other interpreters that are built in are for Hive, Flink, Markdown and Scala. It also has the ability to run remote interpreters (outside of Zeppelin's own JVM) via Thrift. To look at the list of built-in interpreters, you can go through <code class="literal">conf</code>/<code class="literal">interpreter.json</code> in the Zeppelin installation directory. Alternatively, you can view and customize the interpreters from <code class="literal">http://localhost:8080/#/interpreter</code> once you start the Zeppelin daemon.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec309"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark Cluster in any one of the modes, that is, local, standalone, YARN, or Mesos. Also, include the Spark MLlib package in the <code class="literal">build.sbt</code> file so that it downloads the related libraries and the API can be used. Install Hadoop (optionally), Scala, and Java. For installing Zeppelin, choose a cluster node that does not contain the DataNode or NameNode. This is to ensure that Zeppelin has enough processing resources on that node. The prerequisites for installing Zeppelin are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Git</p></li><li style="list-style-type: disc"><p>Java 1.7</p></li><li style="list-style-type: disc"><p>OS Ubuntu 14.x</p></li><li style="list-style-type: disc"><p>Apache Maven</p></li><li style="list-style-type: disc"><p>Hadoop client</p></li><li style="list-style-type: disc"><p>Spark</p></li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec310"></a>How to do itâ€¦</h3></div></div></div><p>In this recipe, we'll be using the built-in SparkContext and SQLContext inside Zeppelin and transforming data using Spark. At the end, we'll register the transformed data as a table and use Spark SQL to query the data and visualize it.</p><p>The list of sub-recipes in this section are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Installing Zeppelin</p></li><li style="list-style-type: disc"><p>Customizing Zeppelin's server and websocket port</p></li><li style="list-style-type: disc"><p>Visualizing data on HDFS - parameterizing inputs</p></li><li style="list-style-type: disc"><p>Using custom functions during visualization</p></li><li style="list-style-type: disc"><p>Adding external dependencies to Zeppelin</p></li><li style="list-style-type: disc"><p>Pointing to an external Spark Cluster</p></li></ul></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec84"></a>Installing Zeppelin</h2></div></div><hr /></div><p>Zeppelin supports binary build as well as source build. Let's see how to build it from source. We just ought to run one command to install it to our local machine. At the end of this recipe, we'll see how to connect Zeppelin to an external Spark master. Here is the code:</p><pre class="programlisting">
<span class="strong"><strong>git clone https://github.com/apache/zeppelin.git
cd zeppelin/
mvn clean package -Pspark-1.6 -Phadoop-2.6 -Pyarn -Ppyspark -Psparkr -Pscala-2.10 -DskipTests

[INFO] Reactor Summary:
[INFO]
[INFO] Zeppelin .......................................... SUCCESS [1:39.666s]
[INFO] Zeppelin: Interpreter ............................. SUCCESS [1:40.830s]
[INFO] Zeppelin: Zengine ................................. SUCCESS [2:46.084s]
[INFO] Zeppelin: Display system apis ..................... SUCCESS [2:03.322s]
[INFO] Zeppelin: Spark dependencies ...................... SUCCESS [14:30.613s]
[INFO] Zeppelin: Spark ................................... SUCCESS [1:27.082s]
[INFO] Zeppelin: Markdown interpreter .................... SUCCESS [8.820s]
[INFO] Zeppelin: Angular interpreter ..................... SUCCESS [0.558s]
[INFO] Zeppelin: Shell interpreter ....................... SUCCESS [0.817s]
[INFO] Zeppelin: Livy interpreter ........................ SUCCESS [1:04.855s]
[INFO] Zeppelin: HBase interpreter ....................... SUCCESS [4:38.000s]
[INFO] Zeppelin: PostgreSQL interpreter .................. SUCCESS [36.218s]
[INFO] Zeppelin: JDBC interpreter ........................ SUCCESS [49.480s]
[INFO] Zeppelin: File System Interpreters ................ SUCCESS [37.278s]
[INFO] Zeppelin: Flink ................................... SUCCESS [1:59.856s]
[INFO] Zeppelin: Apache Ignite interpreter ............... SUCCESS [36.267s]
[INFO] Zeppelin: Kylin interpreter ....................... SUCCESS [1.000s]
[INFO] Zeppelin: Python interpreter ...................... SUCCESS [5.144s]
[INFO] Zeppelin: Lens interpreter ........................ SUCCESS [2:06.158s]
[INFO] Zeppelin: Apache Cassandra interpreter ............ SUCCESS [4:53.121s]
[INFO] Zeppelin: Elasticsearch interpreter ............... SUCCESS [2:27.213s]
[INFO] Zeppelin: BigQuery interpreter .................... SUCCESS [38.911s]
[INFO] Zeppelin: Alluxio intekrpreter ..................... SUCCESS [2:07.707s]
[INFO] Zeppelin: web Application ......................... SUCCESS [11:16.111s]
[INFO] Zeppelin: Server .................................. SUCCESS [2:55.576s]
[INFO] Zeppelin: Packaging distribution .................. SUCCESS [1.701s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 1:01:13.648s
[INFO] Finished at: Sun Oct 02 14:37:48 IST 2016
[INFO] Final Memory: 161M/836M</strong></span>
</pre><p>Once built, we can start the Zeppelin daemon using the following command:</p><pre class="programlisting">
<span class="strong"><strong>./bin/zeppelin-daemon.sh start &amp;

Log dir doesn't exist, create /home/padmac/bigdata/zeppelin/logs
Pid dir doesn't exist, create /home/padmac/bigdata/zeppelin/run
Zeppelin start                                            [  OK  ]</strong></span>
</pre><p>To stop the daemon, we can use the following command:</p><pre class="programlisting">
<span class="strong"><strong>./bin/zeppelin-daemon.sh stop
Zeppelin stop                                             [  OK  ]</strong></span>
</pre></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec85"></a>Customizing Zeppelin's server and websocket port</h2></div></div><hr /></div><p>Zeppelin runs on port <code class="literal">8080</code> by default, and it has a websocket port enabled at the +1 port <code class="literal">8081</code>Â by default. We can customize the port by copying <code class="literal">conf/zeppelin-site.xml.template</code> to <code class="literal">conf/zeppelin-site.xml</code> and changing the ports and various other properties, if necessary. Since the Spark standalone cluster master web UI also runs on <code class="literal">8080</code>, when we are running Zeppelin on the same machine as the Spark master, we have to change the ports to avoid conflicts:</p><div class="mediaobject"><img src="graphics/image_08_001.jpg" /></div><p>For now, let's change the port to <code class="literal">8180</code>Â by editing the configuration file shown in the following image. In order for this to take effect, let's restart Zeppelin using <code class="literal">bin/zeppelin-daemon restart</code>. Now Zeppelin can be viewed on the web browser by visiting the site <code class="literal">http://localhost:8180</code> and the web browser looks like the following screenshot:</p><div class="mediaobject"><img src="graphics/image_08_002.jpg" /></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec86"></a>Visualizing data on HDFS - parameterizing inputs</h2></div></div><hr /></div><p>Once we start the service, we can point our browser to <code class="literal">http://localhost:8080</code> (change the port as per your modified port configuration) to view the Zeppelin UI. Zeppelin organizes its contents as notes and paragraphs. A note is simply a list of all the paragraphs on a single web page.</p><p>Using data from HDFS simply means that we point to the HDFS location instead of the local file system location. Before we consume the file from HDFS, let's quickly check the Spark version that Zeppelin uses. This can be achieved by issuing <code class="literal">sc.version</code> on a paragraph. The <code class="literal">sc</code>Â variable is an implicit variable representing the SparkContext inside Zeppelin, which simply means that we need not programmatically create a SparkContext within Zeppelin:</p><pre class="programlisting">
<span class="strong"><strong>sc.version
res0: String = 1.6.0</strong></span>
</pre><p>Let's load the sample file <code class="literal">profiles.json</code>, convert it into a DataFrame, and print the schema and the first 20 rows (show) for verification. Let's also finally register the DataFrame as a table. Just like the implicit variable for SparkContext , SQLContext is represented by the <code class="literal">sqlc</code> implicit variable inside Zeppelin:</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note22"></a>Note</h3><p>The file is available at the following location: <a class="ulink" href="https://github.com/ChitturiPadma/datasets/blob/master/profiles.json" target="_blank">https://github.com/ChitturiPadma/datasets/blob/master/profiles.json</a>.</p></div><p>Please include the following code in Zeppelin notebook:</p><pre class="programlisting">val profilesJsonRdd = sqlc.jsonFile("hdfs://namenode:9000/profiles.json")
val profileDF=profilesJsonRdd.toDF()
profileDF.printSchema()
profileDF.show(5)
profileDF.registerTempTable("profiles")
</pre><p>The output looks like the following screenshot:</p><div class="mediaobject"><img src="graphics/image_08_003.jpg" /></div><div class="mediaobject"><img src="graphics/image_08_004.jpg" /></div><p>Let's now run a simple query to understand eye colors and their counts for men in the dataset:</p><pre class="programlisting">%sql select eyeColor, count(eyeColor) as count from profiles where
gender='male' group by eyeColor
</pre><p>The <code class="literal">%sql</code> at the beginning of the paragraph indicates to Zeppelin that we are about to execute a Spark SQL query in this paragraph:</p><div class="mediaobject"><img src="graphics/image_08_005.jpg" /></div><p>Now, if we wish to share this chart with someone or link it to an external website, we can do so by clicking on the gear icon in this paragraph and then clicking on <span class="strong"><strong>Link this paragraph</strong></span>, as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/image_08_006.jpg" /></div><p>We can actually parameterize the input for gender instead of altering our query every time. This is achieved by the use of <code class="literal">${PARAMETER PLACEHOLDER}</code>:</p><pre class="programlisting">%sql select eyeColor, count(eyeColor) as count from profiles where
gender="${gender}" group by eyeColor
</pre><div class="mediaobject"><img src="graphics/image_08_007.jpg" /></div><p>Finally, if parameterizing using free-form text isn't enough, we can use a drop-down list instead:</p><pre class="programlisting">%sql select eyeColor, count(eyeColor) as count from profiles where gender
="${gender=male,male|female}" group by eyeColor
</pre></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec87"></a>Running custom functions</h2></div></div><hr /></div><p>While Spark SQL doesn't support a range of functions as wide as ANSI SQL does, it has an easy and powerful mechanism for registering a normal Scala function and using it inside the SQL context.</p><p>Let's say we would like to find out how many profiles fall under each age group. We have a simple function called <code class="literal">ageGroup</code>. Given an age, it returns a string representing the age group:</p><pre class="programlisting">def fnGroupAge(age: Int, bucket:Int=10) = {
val buckets = Array("0-10", "11-20", "20-30", "31-40", "41-50", "51-60", "61-70", "71-80", "81-90", "91-100", "&gt;100")
val bucket = buckets((age-1)/10)
bucket
}
</pre><p>Now, in order to register this function to be used inside Spark SQL, all that we need to do is give it a name and call the <code class="literal">register</code> method of the SQLContext's user-defined function object:</p><pre class="programlisting">sqlc.udf.register("fnGroupAge", (age:Long)=&gt;ageGroup(age.toInt))
</pre><p>Let's fire our query and see the use of the function in action:</p><pre class="programlisting">%sql select fnGroupAge(age) as ageGroup, count(gender) as genderTotal from profiles where gender='${gender=male,male|female}' group by fnGroupAge(age), gender
</pre><p>The following is the output:</p><div class="mediaobject"><img src="graphics/image_08_008.jpg" /></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec88"></a>Adding external dependencies to Zeppelin</h2></div></div><hr /></div><p>Sooner or later, we will be depending on external libraries than that don't come bundled with Zeppelin. For instance, we might need, a library for CSV or import or RDBMS data import. Let's see how to load a MySQL database driver and visualize data from a table.</p><p>In order to load a <code class="literal">mysql</code> connector Java driver, we just need to specify the group ID, artifact ID, and version number, and the JAR gets downloaded from the Maven repository. <code class="literal">%dep</code> indicates that the paragraph adds a dependency, and the <code class="literal">z</code> implicit variable represents the Zeppelin context:</p><div class="mediaobject"><img src="graphics/image_08_009.jpg" /></div><p>The only thing that we need to watch out for while using <code class="literal">%dep</code> is that the dependency paragraph should be used before using the libraries that are being loaded. So it is generally advised to load the dependencies at the top of the Notebook.</p><p>Once we have loaded the dependencies, we need to construct the options required to connect to the MySQL database:</p><div class="mediaobject"><img src="graphics/image_08_010.jpg" /></div><p>We use the connection to create a DataFrame:</p><div class="mediaobject"><img src="graphics/image_08_011.jpg" /></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec89"></a>Pointing to an external Spark Cluster</h2></div></div><hr /></div><p>Running Zeppelin with built-in Spark is all good, but in most of our cases, we'll be executing the Spark jobs initiated by Zeppelin on a cluster of workers. Achieving this is pretty simple: we need to configure Zeppelin to point its Spark master property to an external Spark master URL. Let's take for example a simple and standalone external Spark cluster running on my local machine. Please note that we will have to run Zeppelin on a different port because of the Zeppelin UI port's conflict with the Spark standalone cluster master web UI over <code class="literal">8080</code>.</p><p>Let's bring up the Spark Cluster. From inside your Spark source, execute the following:</p><pre class="programlisting">
<span class="strong"><strong>sbin/start-all.sh</strong></span>
</pre><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec311"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Finally, let's modify <code class="literal">conf/interpreter.json</code> and <code class="literal">conf/zeppelin-env.sh</code> to point the <code class="literal">master</code> property to the host on which the Spark VM is running. In this case, it will be my localhost, with the port being <code class="literal">7077</code>, which is the default master port:</p></li><li><p>The <code class="literal">conf/interpreter.json</code> file looks like the following:</p><pre class="programlisting">       "2BXPPAVX1": {
       "id": "2BXPPAVX1",
       "name": "spark",
       "group": "spark",
       "properties": {
        "spark.executor.memory": "",
        "args": "",
       "zeppelin.spark.printREPLOutput": "true",
        "spark.cores.max": "",
        "zeppelin.dep.additionalRemoteRepository": "spark-
         packages,http://dl.bintray.com/spark-packages/maven,false;",
        "zeppelin.spark.importImplicit": "true",
        "zeppelin.spark.sql.stacktrace": "false",
        "zeppelin.spark.concurrentSQL": "false",
        "zeppelin.spark.useHiveContext": "true",
        "zeppelin.pyspark.python": "python",
        "zeppelin.dep.localrepo": "local-repo",
        "zeppelin.interpreter.localRepo": "local-rep",
        "zeppelin.R.knitr": "true",
        "zeppelin.spark.maxResult": "1000",
        "master": "spark://master:7077",
        "spark.app.name": "Zeppelin",
</pre></li><li><p>The <code class="literal">conf/zeppelin-env.sh</code> file should look like as follows:</p><pre class="programlisting">     export MASTER= spark://master:7077
</pre></li></ol></div><p>Now, when we rerun Spark SQL from Zeppelin, we can see that the job runs on the external Spark instance.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec312"></a>How it worksâ€¦</h3></div></div></div><p>In the preceding recipe, we saw how to installÂ Zeppelin, customize ports, visualize data from a distributed filesystem such as HDFS, write custom functions and invoke them from the Notebook, add external dependencies such as MySQL, and lastly, point Zeppelin towards an external Spark Cluster.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec313"></a>There's moreâ€¦</h3></div></div></div><p>In the next recipe, let's see how to visualize data using Bokeh.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec314"></a>See also</h3></div></div></div><p>Please also visit the subsequent recipeÂ <span class="emphasis"><em>Creating scatter plots with Bokeh-Scala</em></span></p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec90"></a>Creating scatter plots with Bokeh-Scala</h2></div></div><hr /></div><p>In this section, we'll take a brief look at the most popular visualizing framework in Python, called Bokeh, and use its (also fast-evolving) Scala bindings to the framework. Breeze also has a visualization API called <span class="strong"><strong>breeze-viz</strong></span>, which is built on JFreeChart. Bokeh is backed by a JavaScript visualization library, called <span class="strong"><strong>BokehJS</strong></span>. The Scala bindings library <code class="literal">bokeh-scala</code>Â not only gives an easier way to construct glyphs (lines, circles, and so on) out of Scala objects, but also translates glyphs into a format that is understandable by the BokehJS JavaScript components. The various terms in Bokeh actually mean the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><span class="strong"><strong>Glyph</strong></span>: All geometric shapes that we can think of--circles, squares, lines, and so on - are glyphs. This is just the UI representation and doesn't hold any data. All the properties related to this object just help us modify the UI properties: <code class="literal">color</code>, <code class="literal">x</code>, <code class="literal">y</code>, width and so on.</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Plot</strong></span>: A plot is like a canvas on which we arrange various objects relevant to the visualization, such as the legend, <span class="emphasis"><em>x</em></span> and <span class="emphasis"><em>y</em></span> axes, grid, tools and obviously, the core of the graph--the data itself. We construct various accessory objects and finally add them to the list of renderers in the plot object.</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Document</strong></span>: The document is the component that does the actual rendering.</p></li></ul></div><p>The Bokeh-Scala accepts the plot as an argument and when we call the <code class="literal">save</code> method in the document, it uses all the child renderers in the plot object and constructs a JSON from the wrapped elements. This JSON is eventually read by the BokehJS widgets to render the data in a visually pleasing manner. More than one plot can be rendered in the document by adding it to a grid plot (we'll look at how this is done in the next recipe, <span class="emphasis"><em>Creating a time series MultiPlot with Bokeh-Scala</em></span>).</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec315"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark Cluster in any one of the modes, that is, local, standalone, YARN, or Mesos. Also, include the Spark MLlib package in the <code class="literal">build.sbt</code> file so that it downloads the related libraries and the API can be used. Install Hadoop (optionally), Scala and Java.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec316"></a>How to do itâ€¦</h3></div></div></div><p>Initially, specify the following libraries in the <code class="literal">build.sbt</code> file as follows:</p><pre class="programlisting">  libraryDependencies ++= Seq(
      "io.continuum.bokeh" % "bokeh_2.10" % "0.5",
      "org.scalanlp" %% "breeze" % "0.5",
      "org.scalanlp" %% "breeze-viz" % "0.5" )
</pre><p>In this recipe, we will be creating a scatter plot using iris data (<a class="ulink" href="https://archive.ics.uci.edu/ml/datasets/Iris" target="_blank">https://archive.ics.uci.edu/ml/datasets/Iris</a>), which has the length and width attributes of flowers belonging to three different species of the same plant. Drawing a scatter plot on this dataset involves a series of interesting sub steps. For the purpose of representing the iris data in a Breeze matrix, I have naÃ¯vely transformed the species categories into numbers:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Iris setosa: 0</p></li><li style="list-style-type: disc"><p>Iris versicolor: 1</p></li><li style="list-style-type: disc"><p>Iris virginica: 2</p></li></ul></div><p>This is available in <code class="literal">irisNumeric.csv</code>. Later, we'll see how we can load the original iris data (<code class="literal">iris.data</code>) into a Spark DataFrame and use that as a source for plotting. A plot is a composition of multiple widgets/glyphs. This consists of a series of steps:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Preparing our data.</p></li><li><p>Creating the plot, a point (marker object), and a renderer for it.</p></li><li><p>Setting the <span class="emphasis"><em>x</em></span> and <span class="emphasis"><em>y</em></span> axes' data range for the plot.</p></li><li><p>Drawing the <span class="emphasis"><em>x</em></span> and <span class="emphasis"><em>y</em></span> axes.</p></li><li><p>Creating the document object and viewing the plot.</p></li><li><p>Adding tools to the plot.</p></li><li><p>Adding grid lines.</p></li><li><p>Adding a legend to the plot.</p></li></ol></div><p>The following describes the preceding steps:</p><p><span class="strong"><strong>Preparing our data</strong></span></p><p>Bokeh plots require our data to be in a format that it understands, but it's really easy to do this. All that we need to do is create a new source object that inherits from <code class="literal">ColumnDataSource</code>. The other options are <code class="literal">AjaxDataSource</code> and <code class="literal">RemoteDataSource</code>. So, let's overlay our Breeze data source on <code class="literal">ColumnDataSource</code> as follows:</p><pre class="programlisting">  import breeze.linalg._
  import breeze.plot.Figure
  import io.continuum.bokeh._
  object IrisSource extends ColumnDataSource {
    val colormap = Map[Int, Color](0 -&gt; Color.Red, 1 -&gt;
      Color.Green, 2 -&gt; Color.Blue)
    val iris = csvread(file = new
      File("/home/padmacuser/data/iris.csv"), separator = ',')
    val sepalLength = column(iris(::, 0))
    val sepalWidth = column(iris(::, 1))
    val petalLength = column(iris(::, 2))
    val petalWidth = column(iris(::, 3)) }
</pre><p><span class="strong"><strong>Creating the plot, a point (marker object) and a renderer for it</strong></span></p><p>Let's have our image's title as <code class="literal">Iris Petal Length vs Width</code>. Also, create a marker object that marks the data point. There are a variety of marker objects to choose from: <code class="literal">Asterisk</code>, <code class="literal">Circle</code>, <code class="literal">CircleCross</code>, <code class="literal">CircleX</code>, <code class="literal">Cross</code>, <code class="literal">Diamond</code>, <code class="literal">DiamondCross</code>, <code class="literal">InvertedTriangle</code>, <code class="literal">PlainX</code>, <code class="literal">Square</code>, <code class="literal">SquareCross</code>, <code class="literal">SquareX</code> , and <code class="literal">Triangle</code>. Let's choose <code class="literal">Diamond</code> for our purposes:</p><pre class="programlisting">
object Bokeh_Scala extends  App{
      import IrisSource.{colormap,sepalLength,sepalWidth,petalLength,petalWidth}

    val plot = new Plot().title("Iris Petal Length vs Width")
    val diamond = new Diamond()
    .x(petalLength)
    .y(petalWidth)
    .fill_color(Color.Blue)
    .fill_alpha(0.5)
    .size(5)
    val dataPointRenderer = new GlyphRenderer().data_source(IrisSource).
    glyph(diamond)
</pre><p>While constructing the marker object, other than the UI attributes, we also say what the <span class="emphasis"><em>x</em></span> and the <span class="emphasis"><em>y</em></span> coordinates for it are. Note that we have also mentioned that the color of this marker is <code class="literal">blue</code>. We'll change that in a while using the color map.</p><p><span class="strong"><strong>Setting the x and y axes' data range for the plot</strong></span></p><p>The plot needs to know what the <span class="emphasis"><em>x</em></span> and <span class="emphasis"><em>y</em></span> data ranges of the plot are before rendering. Let's do that by creating two DataRange objects and setting them to the plot:</p><pre class="programlisting">  val xRange = new DataRange1d().sources(petalLength :: Nil)
  val yRange = new DataRange1d().sources(petalWidth :: Nil)
  plot.x_range(xRange).y_range(yRange)
</pre><p><span class="strong"><strong>Drawing the x and the y axes</strong></span></p><p>Let's now draw the axes, set their bounds, and add them to the plot's renderers. We also need to let the plot know which location each axis belongs to:</p><pre class="programlisting">  //X and Y Axis
  val xAxis = new LinearAxis().plot(plot).axis_label("Petal Length").
    bounds((1.0, 7.0))
  val yAxis = new LinearAxis().plot(plot).axis_label("Petal Width").
    bounds((0.0, 2.5))
  plot.below &lt;&lt;= (listRenderer =&gt; (xAxis :: listRenderer))
  plot.left &lt;&lt;= (listRenderer =&gt; (yAxis :: listRenderer))
  //Add the renderer to the plot
  plot.renderers := List[Renderer](xAxis, yAxis, dataPointRenderer)
</pre><p><span class="strong"><strong>Creating the document object and viewing the plot</strong></span></p><p>Now, create a document object so that we can save the final HTML by the name <code class="literal">IrisBokehBreeze.html</code>. Since we haven't specified the full path of the target file in the <code class="literal">save</code> method, the file will be saved in the same directory as the project itself as follows:</p><pre class="programlisting">  val document = new Document(plot)
  val file =
  document.save("/home/padmacuser/data/IrisBokehBreeze.html")
  file.view() } // Object Bokeh_Scala ends here
</pre><p>The following is the output:</p><div class="mediaobject"><img src="graphics/image_08_012.jpg" /></div><p><span class="strong"><strong>Adding tools to the plot</strong></span></p><p>Let's add some tools to the image. Bokeh has some nice tools that can be attached to the image: <code class="literal">BoxSelectTool</code>, <code class="literal">BoxZoomTool</code>, <code class="literal">CrosshairTool</code>, <code class="literal">HoverTool</code>, <code class="literal">LassoSelectTool</code>, <code class="literal">PanTool</code>, <code class="literal">PolySelectTool</code>, <code class="literal">PreviewSaveTool</code>, <code class="literal">ResetTool</code>, <code class="literal">ResizeTool</code>, <code class="literal">SelectTool</code>, <code class="literal">TapTool</code>, <code class="literal">TransientSelectTool</code>Â and <code class="literal">WheelZoomTool</code>.</p><p>Let's add some of the tools as follows:</p><pre class="programlisting">  val panTool = new PanTool().plot(plot)
  val wheelZoomTool = new WheelZoomTool().plot(plot)
  val previewSaveTool = new PreviewSaveTool().plot(plot)
  val resetTool = new ResetTool().plot(plot)
  val resizeTool = new ResizeTool().plot(plot)
  val crosshairTool = new CrosshairTool().plot(plot)
  plot.tools := List(panTool, wheelZoomTool, previewSaveTool,
  resetTool,   resizeTool, crosshairTool)
</pre><p>The following is the output:</p><div class="mediaobject"><img src="graphics/image_08_013.jpg" /></div><p><span class="strong"><strong>Adding grid lines</strong></span></p><p>While we have the crosshair tool, which helps us locate the exact <code class="literal">x</code> and <code class="literal">y</code> values of a particular data point, it would be nice to have a data grid too. Let's add two data grids, one for the <code class="literal">x</code> axis and one for the <code class="literal">y</code> axis as follows:</p><pre class="programlisting">  val xgrid = new Grid().plot(plot).axis(xAxis).dimension(0)
  val ygrid = new Grid().plot(plot).axis(yAxis).dimension(1)
  //Add the renderer to the plot
  plot.renderers := List[Renderer](xAxis, yAxis, dataPointRenderer,
  xgrid, ygrid)
</pre><p>The following is the output:</p><div class="mediaobject"><img src="graphics/image_08_014.jpg" /></div><p><span class="strong"><strong>Adding a legend to the plot</strong></span></p><p>This step is a bit tricky in the Scala binding of Bokeh due to the lack of high-level graphing objects, such as scatter. For now, let's cook up our own legend. The <code class="literal">legends</code> property of the <code class="literal">Legend</code> object accepts a list of tuples--a label and a <code class="literal">GlyphRenderer</code> pair. Let's explicitly create three <code class="literal">GlyphRenderer</code> wrapping diamonds of three colors, which represent the species. We then add them to the plot as follows:</p><pre class="programlisting">  //Adding Legend
val setosa = new Diamond().fill_color(Color.Red).size(10).fill_alpha(0.5)
  val setosaGlyphRnd=new GlyphRenderer().glyph(setosa)
  val versicolor = new Diamond().fill_color(Color.Green).size(10).fill_alpha(0.5)
  val versicolorGlyphRnd=new GlyphRenderer().glyph(versicolor)
  val virginica = new Diamond().fill_color(Color.Blue).size(10).fill_alpha(0.5)
  val virginicaGlyphRnd=new GlyphRenderer().glyph(virginica)
val legends = List("setosa" -&gt; List(setosaGlyphRnd), "versicolor" -&gt; List(versicolorGlyphRnd), "virginica" -&gt; List(virginicaGlyphRnd))
  val legend = new   Legend().orientation(LegendOrientation.TopLeft).plot(plot).legends(legends)
  plot.renderers := List[Renderer](xAxis, yAxis, dataPointRenderer, xgrid, ygrid,   legend, setosaGlyphRnd, virginicaGlyphRnd, versicolorGlyphRnd)
</pre><p>The following is the output:</p><div class="mediaobject"><img src="graphics/image_08_015.jpg" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec317"></a>How it worksâ€¦</h3></div></div></div><p>In the preceding recipe, we saw the steps in creating scatter plots with Bokeh-Scala, such as preparing data using the Breeze library, creating marker objects (representation for the data points), setting the <code class="literal">x</code> and <code class="literal">y</code> axes data, creating document objects for viewing the plots, and adding tools, grid lines and legends.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec318"></a>There's moreâ€¦</h3></div></div></div><p>In the next recipe, let's see how to create time series plots using Bokeh.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec319"></a>See also</h3></div></div></div><p>Please visit theÂ <span class="emphasis"><em>Visualization using Zeppelin</em></span> and <span class="emphasis"><em>Creating scatter plots with Bokeh-ScalaÂ </em></span>recipes, which show plots from various tools.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec91"></a>Creating a time series MultiPlot with Bokeh-Scala</h2></div></div><hr /></div><p>In this second recipe on plotting using Bokeh, we'll see how to plot a time series graph with a dataset borrowed from <code class="literal">https://archive.ics.uci.edu/ml/datasets/Dow+Jones+Index</code>. We will also see how to plot multiple charts in a single document.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec320"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark Cluster in any one of the modes, that is, local, standalone, YARN, or Mesos. Also, include the Spark MLlib package in the <code class="literal">build.sbt</code> file so that it downloads the related libraries and the API can be used. Install Hadoop (optionally), Scala, and Java.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec321"></a>How to do itâ€¦</h3></div></div></div><p>Initially, specify the following libraries in the <code class="literal">build.sbt</code> file as follows:</p><pre class="programlisting">  libraryDependencies ++= Seq(
      "io.continuum.bokeh" % "bokeh_2.10" % "0.5",
      "org.scalanlp" %% "breeze" % "0.5",
      "org.scalanlp" %% "breeze-viz" % "0.5" )
</pre><p>We'll be using only two fields from the dataset: the closing price of the stock at the end of the week, and the last business day of the week. The dataset is comma separated. Let's take a look at some samples, as shown here:</p><div class="mediaobject"><img src="graphics/image_08_016.jpg" /></div><p><span class="strong"><strong>Preparing data</strong></span></p><p>In contrast to the previous recipe, where we used the Breeze matrix to construct the Bokeh <code class="literal">ColumnDataSource</code>, we'll use the Spark DataFrame to construct the source this time. The <code class="literal">getSource</code> method accepts a ticker (MSFT-Microsoft and CAT-Caterpillar) and a SQLContext. It runs a Spark SQL, fetches the data from the table, and constructs a <code class="literal">ColumnDataSource</code> from it as follows:</p><pre class="programlisting">  import org.joda.time.format.DateTimeFormat
  import org.apache.spark.sql._
  import io.continuum.bokeh._
  import org.apache.spark._
  object StockSource {
    val formatter = DateTimeFormat.forPattern("MM/dd/yyyy")
    def getSource(ticker: String, sqlContext: SQLContext) = {
      val stockDf = sqlContext.sql(s"select stock, date, close from
    stocks where stock= '$ticker'")
      stockDf.cache()
      val dateData: Array[Double] =
    stockDf.select("date").collect.map(eachRow =&gt;
    formatter.parseDateTime(eachRow.getString(0)).getMillis().toDouble)
      val closeData: Array[Double] =
    stockDf.select("close").collect.map(eachRow =&gt;
    eachRow.getString(0).drop(1).toDouble)
      object source extends ColumnDataSource {
      val date = column(dateData)
      val close = column(closeData)
      }
      source
     }  }
</pre><p>Let's construct the SQLContext and register the DataFrame as a table like this:</p><pre class="programlisting">
  object TimeSeries_MultiPlot extends App {
    import StockSource.{getSource}
    val conf = new SparkConf()
    .setAppName("TimeSeriesPlot").setMaster("spark://master:7077")
    val sc = new SparkContext(conf)
    val sqlContext = new SQLContext(sc)
    import sqlContext.implicits._
    val stocks= sqlContext.read.format("com.databricks.spark.csv")
      .option("header", "true")
      .option("inferSchema","true").load("hdfs://namenode:9000/
      dow_jones  _index.csv")
    stocks.registerTempTable("stocks")
</pre><p><span class="strong"><strong>Creating a line that joins all the data points</strong></span></p><p>As we saw in the previous recipe with the diamond marker, we'll have to pass the <code class="literal">x</code> and the <code class="literal">y</code> positions of the data points. Also, we will need to wrap the line glyph into a renderer so that we can add it to the plot as follows:</p><pre class="programlisting">  val sourceObject = getSource("AA", sqlContext)
  val line = new  Line().x(sourceObject.date).y(sourceObject.close)
  .line_color(Color.Blue).line_width(2)
  val lineGlyph = new
   GlyphRenderer().data_source(sourceObject).glyph(line)
</pre><p><span class="strong"><strong>Setting the x and y axes' data range for the plot</strong></span></p><p>The plot needs to know what the <span class="emphasis"><em>x</em></span> and <span class="emphasis"><em>y</em></span> data ranges are before rendering. Let's do that by creating two DataRange objects and setting them to the plot as follows:</p><pre class="programlisting">  val xdr = new DataRange1d().sources(List(sourceObject.date))
  val ydr = new DataRange1d().sources(List(sourceObject.close))
</pre><p><span class="strong"><strong>Creating the plot</strong></span></p><p>Let's create the plot as follows:</p><pre class="programlisting">  //Create Plot
  val plot = new Plot().title("Ticker").x_range(xdr).y_range(ydr)
  .width(800).height(400)
</pre><p><span class="strong"><strong>Drawing the axes and the grids</strong></span></p><p>Drawing the axes and the grids is the same as before. We add some labels to the axes, format the display of the x axis and then add them to the plot:</p><pre class="programlisting">  //Drawing x-axis and y-axis
  val xformatter = new
  DatetimeTickFormatter().formats(Map(DatetimeUnits.Months -&gt; List
  ("%b    %Y")))
  val xAxis = new DatetimeAxis().plot(plot).formatter(xformatter)
  .axis_label("Month")
  val yAxis = new LinearAxis().plot(plot).axis_label("Price")
  plot.below &lt;&lt;= (xAxis :: _)
  plot.left &lt;&lt;= (yAxis :: _)
  val xgrid = new Grid().plot(plot).dimension(0).axis(xAxis)
  val ygrid = new Grid().plot(plot).dimension(1).axis(yAxis)
</pre><p><span class="strong"><strong>Adding tools</strong></span></p><p>Let's add some tools to the image and to the plot:</p><pre class="programlisting">   //Tools
  val panTool = new PanTool().plot(plot)
  val wheelZoomTool = new WheelZoomTool().plot(plot)
  val previewSaveTool = new PreviewSaveTool().plot(plot)
  val resetTool = new ResetTool().plot(plot)
  val resizeTool = new ResizeTool().plot(plot)
  val crosshairTool = new CrosshairTool().plot(plot)
  plot.tools := List(panTool, wheelZoomTool, previewSaveTool,
    resetTool, resizeTool, crosshairTool)
</pre><p><span class="strong"><strong>Adding a legend to the plot</strong></span></p><p>Here is the code to add legend to the plot:</p><pre class="programlisting">  //Legend
  val legends = List("AA" -&gt; List(lineGlyph))
  val legend = new Legend().plot(plot).legends(legends)
</pre><p><span class="strong"><strong>Adding renderer, creating document object and viewing the plot</strong></span></p><p>Here is the code for adding renderer and creating document object:</p><pre class="programlisting">  //Add the renderer to the plot
  plot.renderers := List[Renderer](xAxis, yAxis, xgrid, ygrid, lineGlyph, legend)

  //Creating document object
  val document = new Document(plot)
  val file = document.save("/home/padmacuser/data/TimeSeriesMultiPlot.html")
  file.view()
</pre><p>The following is the output:</p><div class="mediaobject"><img src="graphics/image_08_017.jpg" /></div><p><span class="strong"><strong>Multiple plots in the document</strong></span></p><p>Creating multiple plots in the same document is child's play. All that we need to do is create all the plots, such as <code class="literal">microsoftPlot</code>, <code class="literal">bofaPlot</code>, <code class="literal">caterPillarPlot</code>, and <code class="literal">mmmPlot</code>, and then add them into a grid. Finally, instead of passing our individual plot object into the document, we pass in <code class="literal">GridPlot</code>:</p><pre class="programlisting"> val children = List(List(microsoftPlot, bofaPlot),
  List(caterPillarPlot,  mmmPlot))
 val grid = new GridPlot().children(children)
 val document = new Document(grid)
 val html =
 document.save("/home/padmac/data/multipleTimeSeriesPlots.html")
</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec322"></a>How it workâ€¦</h3></div></div></div><p>In the preceding recipe, we saw the procedure for creating time series plots in which the data is loaded using Spark. Although the data resides in a distributed system, not all the records can be visualized. We fetched the records pertaining to a specific 'stock' and visualized them using time series plot. While creating multiplots, it is assumed that the user creates the plots <code class="literal">microsoftPlot</code>, <code class="literal">bofaPlot</code>, <code class="literal">caterPillarPlot</code>Â and <code class="literal">mmmPlot</code> corresponding to specific stock.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec323"></a>There's moreâ€¦</h3></div></div></div><p>In the next recipe, let's see how to visualize data using lightning visualization server.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec324"></a>See also</h3></div></div></div><p>Please visit the earlier recipes <span class="emphasis"><em>Visualization using Zeppelin </em></span>and<span class="emphasis"><em> Creating scatter plots with Bokeh-Scala</em></span>, which show plots from various tools.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec92"></a>Creating plots with the lightning visualization server</h2></div></div><hr /></div><p>Lightning is a framework for interactive data visualization, including a server, visualizations, and client libraries. The lightning server provides API-based access to reproducible, web-based visualizations. It includes a core set of visualization types, but is built for extendibility and customization. It can be deployed in many ways, including Heroku, Docker, a public server, a local app for OS X and even a serverless version well suited to notebooks such as Jupyter.</p><p>Lightning can expose a single visualization to all the languages of data science. Client libraries are available in multiple languages, including Python, Scala, JavaScript, and rstats, with many more in future.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec325"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark Cluster in any one of the modes, that is, local, standalone, YARN, or Mesos. Install Hadoop (optionally), Scala, and Java. Lightning is designed to support a variety of use cases. The first option is to use a pre-built server, run own server, or use locally without a server. In this recipe, we'll use a public server to generate the visualizations.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec326"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Install lightning on your local machine as follows.</p></li><li><p>Clone the <code class="literal">git</code> repository for the <code class="literal">lightning-viz</code> repository:</p><pre class="programlisting">
<span class="strong"><strong>      git clone git@github.com:lightning-viz/lightning.git
      npm install
</strong></span>
</pre></li><li><p>Install <code class="literal">nvm</code> as follows:</p><pre class="programlisting">
<span class="strong"><strong>      wget -qO-
      https://raw.githubusercontent.com/creationix/nvm/v0.32.0/
      install.sh | bash
      source ~/.bashrc
      export NVM_NODEJS_ORG_MIRROR=http://nodejs.org/dist</strong></span>
</pre></li><li><p>Install Node 4.0 as follows:</p><pre class="programlisting">
<span class="strong"><strong>      nvm install 4.0
      npm install sqlite3
      cd lightning/
      npm start (starts the lightning server)
</strong></span>
</pre><div class="mediaobject"><img src="graphics/image_08_018.jpg" /></div></li><li><p>Also install the lightning Scala client and build the project as follows:</p><pre class="programlisting">
<span class="strong"><strong>       git clone https://github.com/lightning-viz/lightning-scala.git
       sbt assembly
</strong></span>
</pre><div class="mediaobject"><img src="graphics/image_08_019.jpg" /></div></li><li><p>Here is the code for creating a visualization on a public server:</p><pre class="programlisting">      import org.viz.lightning.types.Make
      import org.viz.lightning.{Visualization, Lightning}
      object Lightning_Demo {
        def main(args:Array[String]): Unit =
        {
          val lgn = Lightning(host="http://public.lightning-viz.org")
          lgn.createSession("SimpleDemo")
          val viz = lgn.line(Array(Array(1.0,1.0,2.0,3.0,9.0,20.0)))
         println(viz.getPermalinkURL) }
</pre></li></ol></div><p>The following is the output:</p><pre class="programlisting">
<span class="strong"><strong>http://public.lightning-viz.org/visualizations/8cd065a6-c416-44ad-addf- 5a0718a5c500
</strong></span>
</pre><p>Now on visiting the preceding URL by adding/public at the end, that is, <code class="literal">http://public.lightning-viz.org/visualizations/8cd065a6-c416-44ad-addf- 5a0718a5c500/public</code>, the visualization looks like the following:</p><div class="mediaobject"><img src="graphics/image_08_020.jpg" /></div><p>Let's see how to create a scatter streaming plot using Lightning. Here is the code for the same:</p><pre class="programlisting">   val lgn = Lightning(host="http://localhost:3000")
   lgn.createSession("ScatterStreaming")
   val viz = lgn.scatterStreaming(x = Make.gaussian(n = 50, scale = 5),
    y = Make.gaussian(n = 50, scale = 5),
    label = Make.labels(n = 50),
    size = Make.sizes(n = 50),
    alpha = Make.alphas(n = 50))
    println(viz.getPermalinkURL)
</pre><p>The following is the output:</p><div class="mediaobject"><img src="graphics/image_08_021.jpg" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec327"></a>How it worksâ€¦</h3></div></div></div><p>In the preceding recipe, we saw how to install the Lightning framework and create line plots as well as streaming plots. When processing the data using RDDs, the data needs to be collected on the driver (since the lightning libraries do not accept RDD as input) in Array format which can be input for the lightning API. This way, the visualization will be on a single node (the driver node) which requires good hardware and software configurations.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec328"></a>There's moreâ€¦</h3></div></div></div><p>In the next recipes, let's see how to visualize data using Spark's built-in libraries.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec329"></a>See also</h3></div></div></div><p>Please visit the earlier recipes <span class="emphasis"><em>Visualization using Zeppelin</em></span>, <span class="emphasis"><em>Creating scatter plots with Bokeh-Scala</em></span>Â and <span class="emphasis"><em>Creating plots with lightning visualization server</em></span>, which show plots from various tools.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec93"></a>Visualize machine learning models with Databricks notebook</h2></div></div><hr /></div><p>Databricks provides flexibility to visualize machine learning models using the built-in <code class="literal">display()</code> command that displays DataFrames as a table and creates convenient one-click plots. In the following recipe we'll, we'll see how to visualize data with DatabricksÂ notebook.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec330"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark cluster in any one of the modes, that is, local, standalone, YARN, or Mesos. Install Hadoop (optionally), Scala, and Java. Create a user account in Databricks and get access for the Notebook.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec331"></a>How to do itâ€¦</h3></div></div></div><p>The fitted versus residuals plot is available for linear regression and logistic regression models. The Databricks fitted versus residuals plot is analogous to R's residuals versus fitted plot for linear models. Linear regression computes a prediction as a weighted sum of the input variables. The fitted versus residuals plot can be used to assess a linear regression model's goodness of fit. The dataset <code class="literal">diabetes</code> is default available in the notebook. If not available, please download the dataset from the following location:</p><p><a class="ulink" href="https://github.com/ChitturiPadma/datasets/blob/master/diabetes.csv" target="_blank">https://github.com/ChitturiPadma/datasets/blob/master/diabetes.csv</a></p><p>Here is the plot:</p><pre class="programlisting">
  import org.apache.spark._
  import org.apache.spark.sql._
  import org.apache.spark.ml.regression.LinearRegression
  import org.apache.spark.mllib.util.MLUtils
  import org.apache.spark.mllib.linalg.Vectors
  import org.apache.spark.mllib.regression.LabeledPoint
  import org.apache.spark.mllib.linalg.{Vector, Vectors}
  import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}
  val data = sqlContext.sql("select * from diabetes")")
  val lr = new LinearRegression()
    .setMaxIter(20)
    .setRegParam(0.3)
    .setElasticNetParam(1.0)
  val lrModel = lr.fit(data)
  &gt; display(lrModel, data, "fittedVsResiduals")
</pre><p>The following is the output:</p><div class="mediaobject"><img src="graphics/image_08_022.jpg" /></div><p>Let's also see how to visualize clusters. K-means tries to separate data points into clusters by minimizing the sum of squared errors between data points and their nearest cluster centers. We can now visualize clusters and plot feature grids to identify trends and correlations. Each plot in the grid corresponds to two features, and data points are colored by their respective cluster labels. The plots can be used to visually assess how well your data has been clustered (the dataset iris is default available in the notebook):</p><pre class="programlisting">  val data = sqlContext.sql("select * from iris")")
  // The MLLib package requires an RDD[Vector] instead of a dataframe.
  We need to   manually extract the vector.
  // This is not necessary when using the ml package instead.
  val features = data.map(_.getAs[Vector]("features"))
  val clusters = KMeans.train(features, 3, 10)
  &gt; display(clusters, data)
</pre><p>The following is the output:</p><div class="mediaobject"><img src="graphics/image_08_023.jpg" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec332"></a>How it worksâ€¦</h3></div></div></div><p>In the preceding recipe, we saw how to visualize regression models and clusters using the <code class="literal">display()</code> built-in command available in Databricks notebook.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec333"></a>There's moreâ€¦</h3></div></div></div><p>Although the visualization tools are not fully matured with respect to distributed systems, tools such as Zeppelin, Lightning server and Bokeh are incorporating a lot of features, giving more scope for big data developers to visualize the data.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec334"></a>See also</h3></div></div></div><p>Please visit the earlierÂ <span class="emphasis"><em>Creating scatter plots with Bokeh-Scala</em></span> and <span class="emphasis"><em>Creating plots with Lightning Visualization Server</em></span> recipes, which show plots from various tools.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch09"></a>ChapterÂ 9.Â Deep Learning on Spark</h2></div></div></div><p>In this chapter, we'll cover the following recipes:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Installing CaffeOnSpark</p></li><li style="list-style-type: disc"><p>Working with CaffeOnSpark</p></li><li style="list-style-type: disc"><p>Running a feed-forward neural network with DeepLearning4j over Spark</p></li><li style="list-style-type: disc"><p>Running an RBM with DeepLearning4j over Spark</p></li><li style="list-style-type: disc"><p>Running a CNN for learning MNIST with DeepLearning4j over Spark</p></li><li style="list-style-type: disc"><p>Installing TensorFlow</p></li><li style="list-style-type: disc"><p>Working with Spark TensorFlow</p></li></ul></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec94"></a>Introduction</h2></div></div><hr /></div><p>Deep learning is a new area of machine learning which has been introduced with the objective of moving machine learning closer to one of its original goals, which is <span class="strong"><strong>Artificial Intelligence</strong></span> (<span class="strong"><strong>AI</strong></span>). It is becoming an important AI paradigm for pattern recognition, image/video processing and fraud detection applications in finance.</p><p>Deep learning is the implementation of neural networks with more than a single hidden layer of neurons. The <span class="emphasis"><em>deep</em></span> architectures vary quite considerably, with different implementations being optimized for different tasks or goals. To get familiar with neural networks, please get acquainted with the fundamentals of neural networks at <a class="ulink" href="http://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/" target="_blank">http://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/</a> and <a class="ulink" href="http://neuralnetworksanddeeplearning.com/" target="_blank">http://neuralnetworksanddeeplearning.com/</a>. The deep networks use many layers of non-linear information processing that are hierarchical in nature.</p><p>The deep models are capable of extracting useful, high-level, structured representations which in turn extract complex statistical dependencies from high-dimensional sensory input and effectively learn the representations by reusing and combining intermediate concepts, allowing these models to generalize well across a wide variety of tasks. These learned high-level representations give state-of-the-art results in challenging problems, including visual object recognition, information retrieval, natural language processing and speech perception. The list of complex models includes deep belief networks, deep Boltzmann machines, deep auto-encoders and sparse coding-based methods.</p><p>Apart from addressing computationally intensive problems, deep neural networks can also take precious time and resources to train. Hence, leveraging the existing distributed data processing frameworks such as Hadoop or Spark would parallelize the training phase of the network and reduce the training time. As Spark offers in-memory processing power, it is ideal for iterative workloads. Since the deep network includes training over the same dataset iteratively, Spark would be a great fit when taking advantage of distributed data processing frameworks.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec95"></a>Installing CaffeOnSpark</h2></div></div><hr /></div><p>Caffe is a fully open source deep learning framework which provides access to deep architectures. The code is written in C++ with CUDA used for GPU computation and supports bindings to Python/NumPy and MATLAB. In Caffe, multimedia scientists and practitioners have an orderly and extensible toolkit for state-of-the-art deep learning algorithms. It provides a complete toolkit for training, testing, fine-tuning and deploying models. It offers expressive architecture, modularity, Python and MATLAB bindings. Caffe also provides reference models for visual tasks.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec335"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you need Ubuntu 14.04 (Linux flavor) installed on the machine. Also, have Apache Hadoop 2.6 and Apache Spark 1.6.0 installed.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec336"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Before installing CaffeOnSpark, install the caffe prerequisites as follows:</p><pre class="programlisting">
<span class="strong"><strong>      sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev
      libopencv-dev libhdf5-serial-dev protobuf-compiler
      sudo apt-get install --no-install-recommends libboost-all-dev
</strong></span>
</pre></li><li><p>Also install the dependent packages <code class="literal">gflags</code>, <code class="literal">glogs</code>, <code class="literal">lmdb</code>Â and <code class="literal">atlas</code> as follows:</p><pre class="programlisting">
<span class="strong"><strong>      sudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev</strong></span>
</pre></li><li><p>Now, clone the <code class="literal">CaffeOnSpark</code> code as follows:</p><pre class="programlisting">
<span class="strong"><strong>      git clone https://github.com/yahoo/CaffeOnSpark.git -recursive
</strong></span>
</pre></li><li><p>Add the environment variable <code class="literal">CAFFE_ON_SPARK</code> in the <code class="literal">.bashrc</code> file as follows:</p><pre class="programlisting">
<span class="strong"><strong>      export CAFFE_ON_SPARK=$(pwd)/CaffeOnSpark
</strong></span>
</pre></li><li><p>Create <code class="literal">CaffeOnSpark/caffe-public/Makefile.config</code> as follows:</p><pre class="programlisting">
<span class="strong"><strong>       pushd ${CAFFE_ON_SPARK}/caffe-public
       cp Makefile.config.example Makefile.config
       echo "INCLUDE_DIRS += ${JAVA_HOME}/include"
       &gt;&gt; Makefile.config</strong></span>
</pre></li><li><p>Now, in <code class="literal">Makefile.config</code>, uncomment the line <code class="literal">CPU_ONLY := 1</code> as
we'll run Caffe on a CPU. Now, build <code class="literal">CaffeOnSpark</code> as follows:</p><pre class="programlisting">
<span class="strong"><strong>       pushd ${CAFFE_ON_SPARK}
       make build
</strong></span>
</pre></li><li><p>If the build fails with any dependency issue in <code class="literal">numpy</code> packages, re-install <code class="literal">numpy</code> as follows:</p><pre class="programlisting">
<span class="strong"><strong>       sudo apt-get install python-numpy
</strong></span>
</pre></li><li><p>The build may also require Maven; hence, install Maven and add the environment variable <code class="literal">M2_HOME</code> in <code class="literal">.bashrc</code> as follows:</p><pre class="programlisting">
<span class="strong"><strong>      export M2_HOME=$(pwd)/apache-maven-3.2.1
      export PATH = $PATH:$M2_HOME/bin</strong></span>
</pre></li><li><p>Once Apache Hadoop 2.6 and Apache Spark 1.6.0 have been installed, load <code class="literal">mnist</code> and <code class="literal">cifar10</code> datasets into HDFS as follows:</p><pre class="programlisting">
<span class="strong"><strong>      hadoop fs -mkdir -p /projects/machine_learning/image_dataset

      ${CAFFE_ON_SPARK}&gt;./scripts/setup-mnist.sh</strong></span>
</pre><p>This will download the <code class="literal">mnist_test_lmdb</code> and <code class="literal">mnist_train_lmdb</code> datasets.</p><pre class="programlisting">
<span class="strong"><strong>      ${CAFFE_ON_SPARK}&gt;./scripts/setup-cifar10.sh</strong></span>
</pre><p>This downloads the <code class="literal">cifar10_train_lmdb</code> and <code class="literal">cifar10_test_lmdb</code> datasets.</p></li><li><p>Now move both the preceding datasets into HDFS as follows:</p><pre class="programlisting">
<span class="strong"><strong>      hadoop fs -put -f ${CAFFE_ON_SPARK}/data/mnist_*_lmdb
      hdfs:/projects/machine_learning/image_dataset/
      hadoop fs -put -f ${CAFFE_ON_SPARK}/data/cifar10_*_lmdb
      hdfs:/projects/machine_learning/image_dataset/</strong></span>
</pre><p>Also, change the solver mode in theÂ <code class="literal">data/lenet_memory_solver.prototxt</code> and <code class="literal">data/cifar10_quick_solver.prototxt</code> files as follows:</p><pre class="programlisting">
<span class="strong"><strong>       solver_mode: CPU
</strong></span>
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec337"></a>How it worksâ€¦</h3></div></div></div><p>The preceding commands install the Caffe prerequisites, build the <code class="literal">CaffeOnSpark</code> package, and download the related <code class="literal">mnist</code> and <code class="literal">cifar10</code> datasets. If there are any dependency issues with <code class="literal">numpy</code>, install the <code class="literal">python-numpy</code> package and proceed.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec338"></a>There's moreâ€¦</h3></div></div></div><p>The Hadoop and Spark installation can be done from <code class="literal">CaffeOnSpark</code> as it contains <code class="literal">local-setup-hadoop.sh</code> and <code class="literal">local-setup-spark.sh</code> which downloads the Hadoop and Spark archived <code class="literal">.tar</code>Â files. Extract these TAR files and set the <code class="literal">HADOOP_HOME</code>, <code class="literal">YARN_CONF_DIR</code>Â and <code class="literal">SPARK_HOME</code> variables.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec339"></a>See also</h3></div></div></div><p>Please refer to <a class="link" href="#" linkend="ch03">Chapter 1</a>, <span class="emphasis"><em>Big Data Analytics with Spark</em></span> to get familiar with Spark. Also, refer to <a class="ulink" href="http://deeplearning.net/tutorial/" target="_blank">http://deeplearning.net/tutorial/</a> and <a class="ulink" href="http://neuralnetworksanddeeplearning.com/" target="_blank">http://neuralnetworksanddeeplearning.com/</a> for details on deep learning.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec96"></a>Working with CaffeOnSpark</h2></div></div><hr /></div><p>CaffeOnSpark brings deep learning to Hadoop and Spark clusters. By combining salient features from the deep learning framework Caffe and Big DataFrame works such as Apache Spark and Apache Hadoop, CaffeOnSpark enables distributed deep learning on a cluster of GPU and CPU servers. As a distributed extension of Caffe, CaffeOnSpark supports neural network model training, testing and feature extraction.</p><p>This is a Spark deep learning package. The API supports DataFrames so that the application can interface with a training dataset that was prepared using a Spark application and extract the predictions from the model or features from intermediate layers for results and data analysis using MLlib or SQL.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec340"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark Cluster either in pseudo distributed mode or in one of the distributed modes, that is, standalone, YARN, or Mesos and CaffeOnSpark ready to be run on a Spark/YARN cluster.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec341"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>A deep learning neural network can be trained using <code class="literal">CaffeOnSpark</code> with two Spark executors with an Ethernet connection as follows:</p></li><li><p>Edit the <code class="literal">~/.bashrc</code> file and copy the following lines:</p><pre class="programlisting">
<span class="strong"><strong>        export SPARK_WORKER_INSTANCES=2
        export DEVICES=1</strong></span>
</pre></li><li><p>Now submit the application to train the DNN as follows:</p><pre class="programlisting">
<span class="strong"><strong>        spark-submit --master spark://master:7077 \
        --num-executors ${SPARK_WORKER_INSTANCES} \
        --files /PATH/CaffeOnSpark/data/lenet_memory_solver.prototxt, \
        /PATH/CaffeOnSpark/data/lenet_memory_train_test.prototxt \
        --conf spark.driver.extraLibraryPath="${LD_LIBRARY_PATH}" \
        --conf spark.executorEnv.LD_LIBRARY_PATH="${LD_LIBRARY_PATH}" \
        --class com.yahoo.ml.caffe.CaffeOnSpark \
       /PATH/CaffeOnSpark/caffe-grid/target/caffe-grid-0.1-SNAPSHOT-</strong></span>
<span class="strong"><strong>        jar-with-dependencies.jar
        -train  \
        -features accuracy,loss -label label  \
        -conf lenet_memory_solver.prototxt  \
        -devices ${DEVICES} \
        -connection ethernet  \
        -model hdfs://&lt;namenode&gt;:54310/models  \
        -output hdfs://&lt;namenode&gt;:54310/mnist_features_result
</strong></span>
</pre></li><li><p>The training will produce a model and various snapshots as follows:</p><pre class="programlisting">
<span class="strong"><strong>       -rw-r--r--   3 root supergroup    1725052 2016-04-08 00:57
       /mnist_lenet.model
       -rw-r--r--   3 root supergroup    1725052 2016-04-08 00:57
       /mnist_lenet_iter_10000.caffemodel
      -rw-r--r--   3 root supergroup    1724462 2016-04-08 00:57
       /mnist_lenet_iter_10000.solverstate
      -rw-r--r--   3 root supergroup    1725052 2016-04-08 00:56
      /mnist_lenet_iter_5000.caffemodel
      -rw-r--r--   3 root supergroup    1724461 2016-04-08 00:56
      /mnist_lenet_iter_5000.solverstate</strong></span>
</pre></li><li><p>The feature result file looks as follows:</p><pre class="programlisting">
<span class="strong"><strong>        {"SampleID":"00009597","accuracy":[1.0],"loss":
           [0.028171852],"label":  [2.0]}
        {"SampleID":"00009598","accuracy":[1.0],"loss":
           [0.028171852],"label":  [6.0]}
        {"SampleID":"00009599","accuracy":[1.0],"loss":
           [0.028171852],"label":  [1.0]}
        {"SampleID":"00009600","accuracy":[0.97],"loss":
           [0.0677709],"label":  [5.0]}
        {"SampleID":"00009601","accuracy":[0.97],"loss":
           [0.0677709],"label":  [0.0]}
        {"SampleID":"00009602","accuracy":[0.97],"loss":
           [0.0677709],"label":  [1.0]}
        {"SampleID":"00009603","accuracy":[0.97],"loss":
           [0.0677709],"label":  [2.0]}
        {"SampleID":"00009604","accuracy":[0.97],"loss":
           [0.0677709],"label":  [3.0]}
        {"SampleID":"00009605","accuracy":[0.97],"loss":
           [0.0677709],"label":  [4.0]}
</strong></span>
</pre></li><li><p>Here is the Spark application which uses <code class="literal">CaffeOnSpark</code> to train a dataset on HDFS and MLlib to perform non-deep learning, that is, logistic regression for classification:</p><pre class="programlisting">       val conf = new SparkConf()
       .setMaster("spark://master:7077")
       .setAppName("Caffe_Spark_Application")
       val sc = new SparkContext(conf)
       val cos = new CaffeOnSpark(sc)
       val config = new Config(sc,args)
       val dl_train_source = DataSource.getSource(config, true)
       cos.train(dl_train_source)

       val lr_raw_source  = DataSource.getSource(config, false)
       val extracted_df = cos.features(lr_raw_source)
       val lr_input_df = extracted_df.withColumn("Label",
         cos.floatarray2doubleUDF(extracted_df(config.label)))
         .withColumn("Feature",
         cos.floatarray2doublevectorUDF(extracted_df(config.features(0)
         )))

       //Learn a LogisticRegression model via Apache MLlib
       val lr = new LogisticRegression()
           .setLabelCol("Label")
           .setFeaturesCol("Feature")
       val lr_model = lr.fit(lr_input_df)

       //save the LogisticRegression classification model onto HDFS
       lr_model.write.overwrite().save(config.outputPath)
</pre></li><li><p>The preceding code is submitted to the Spark cluster as follows:</p><pre class="programlisting">      spark-submit \   -files
      caffenet_train_solver.prototxt,caffenet_train_net.prototxt \   -
      num-executors 2  \   -class
      com.yahoo.ml.caffe.examples.MyMLPipeline \
      caffe-grid-0.1-SNAPSHOT-jar-with-dependencies.jar \
      -features fc8 \   -label label \   -conf
      caffenet_train_solver.prototxt \   -model
      hdfs:///sample_images.model   \   -output
      hdfs:///image_classifier_model \   -devices 2
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec342"></a>How it worksâ€¦</h3></div></div></div><p>Initially, <code class="literal">CaffeOnSpark</code> is used to train the deep learning network with two spark executors and stores the model onto HDFS. It also uses the configuration files for solvers and neural networks as in standard Caffe. The second application uses <code class="literal">CaffeOnSpark</code> and MLlib. First, the Spark context is initialized and creates <code class="literal">CaffeOnSpark</code> and a configuration object <code class="literal">caffe.Config</code>. Next, <code class="literal">CaffeOnSpark</code> conducts DNN training with the training dataset on HDFS. The learned DL model is applied to extract features from a feature dataset on HDFS. MLlib uses the extracted features to perform non-deep learning. In the preceding example, logistic regression trains the model on extracted features and the classification model is saved onto HDFS.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec343"></a>There's moreâ€¦</h3></div></div></div><p><span class="strong"><strong>Deep learning</strong></span> (<span class="strong"><strong>DL</strong></span>) is a critical capability demanded by many Yahoo products. Yahoo's Flickr team applied deep learning for scene detection, object recognition, and computational aesthetics. To benefit from the capabilities of DL on large scale data, it has been introduced into Hadoop clusters. When DL is conducted on Hadoop clusters, unnecessary movement between Hadoop clusters and deep learning clusters is avoided. DL can be made to run on GPU/CPU nodes in Hadoop clusters. When launched using Spark, each executor is given a partition of HDFS-based training data and launches multiple Caffe-based training threads. Each training thread is executed by a particular GPU/CPU.</p><p>After back-propagation in processing of a batch of training examples, the training threads exchange the gradients of model parameters. The exchanged gradient is carried out in an MPI Allreduce fashion across all GPUs on multiple servers. Caffe is also enhanced to use multiple GPUs on a server and benefit from RDMA to synchronize DL models.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec344"></a>See also</h3></div></div></div><p>Please refer toÂ <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Big Data Analytics with Spark</em></span> to get familiar with Spark. Also, refer to <a class="ulink" href="http://deeplearning.net/tutorial/" target="_blank">http://deeplearning.net/tutorial/</a> and <a class="ulink" href="http://neuralnetworksanddeeplearning.com/" target="_blank">http://neuralnetworksanddeeplearning.com/</a> for details on deep learning.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec97"></a>Running a feed-forward neural network with DeepLearning 4j over Spark</h2></div></div><hr /></div><p><span class="strong"><strong>DeepLearning4j</strong></span> (<span class="strong"><strong>DL4J</strong></span>) is an open source deep learning library written in Java and Scala and which is used in business environments. This can be easily integrated with GPU and scaled on Hadoop or Spark. It supports a stack of neural networks for image recognition, text analysis and speech to text. Hence, it has implementations for algorithms such as binary and continuous restricted Boltzmann machines, deep belief networks, de-noising auto-encoders, convolutional networks and recursive neural tensor networks.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec345"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark cluster either in pseudo distributed mode or in one of the distributed modes, that is, standalone, YARN, or Mesos. Also, get familiar with ND4S, that is, n-dimensional arrays for Scala (Scala bindings for ND4J). ND4J and ND4S are scientific computing libraries for the JVM. Please visit <a class="ulink" href="http://nd4j.org/" target="_blank">http://nd4j.org/</a> for details. The pre-requisites to be installed are Java 7, IntelliJ, and the Maven or SBT build tool.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec346"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Start an application named <code class="literal">FeedForwardNetworkWithSpark</code>. Initially, specify the following libraries in the <code class="literal">build.sbt</code> file as follows:</p><pre class="programlisting">      libraryDependencies ++= Seq(
      "org.apache.spark" %% "spark-core" % "1.6.0",
      "org.apache.spark" %% "spark-mllib" % "1.6.0",
      "org.deeplearning4j" % "deeplearning4j-core" % "0.4-rc3.8",
      "org.deeplearning4j" % "deeplearning4j-nlp" % "0.4-rc3.8",
      "org.deeplearning4j" % "dl4j-spark" % "0.4-rc3.8",
      "org.nd4j" % "nd4j-x86" % "0.4-rc3.8",
      "org.nd4j" % "nd4j-jcublas-7.0" % "0.4-rc3.8",
      "org.nd4j" % "nd4j-api" % "0.4-rc3.8",
      "org.nd4j" % "canova-api" % "0.0.0.14"
        )
</pre></li><li><p>Now take theÂ iris dataset, which consists of the measurements of four attributes of 150 iris flowers from three types of irises. The sample iris dataset looks as follows:</p><pre class="programlisting">      5.1,3.5,1.4,0.2,Iris-setosa
      4.9,3.0,1.4,0.2,Iris-setosa
      7.0,3.2,4.7,1.4,Iris-versicolor
      6.4,3.2,4.5,1.5,Iris-versicolor
</pre></li><li><p>To have a look at the dataset, please visit the following site: <a class="ulink" href="https://archive.ics.uci.edu/ml/datasets/Iris" target="_blank">https://archive.ics.uci.edu/ml/datasets/Iris</a>. Now the normalized dataset looks as follows:</p><pre class="programlisting">    0.194444444,0.625,0.101694915,0.208333333,0
    0.444444444,0.416666667,0.694915254,0.708333333,2
    0.694444444,0.416666667,0.762711864,0.833333333,2
    0.277777778,0.708333333,0.084745763,0.041666667,0
    0.083333333,0.583333333,0.06779661,0.083333333,0
    0.416666667,0.291666667,0.491525424,0.458333333,1
</pre><p>In the preceding sample, the final attribute 0, 1 ,2... represents the label. We can download the normalizedÂ iris dataset from the preceding specified location:</p></li><li><p>Here is the code for a feed-forward network which use the normalized iris dataset for classification:</p><pre class="programlisting">      import scala.collection.mutable.ListBuffer
      import org.apache.spark.SparkConf
      import org.apache.spark.SparkContext
      import org.canova.api.records.reader.RecordReader
      import org.canova.api.records.reader.impl.CSVRecordReader
      import org.deeplearning4j.nn.api.OptimizationAlgorithm
      import org.deeplearning4j.nn.conf.MultiLayerConfiguration
      import org.deeplearning4j.nn.conf.NeuralNetConfiguration
      import org.deeplearning4j.nn.conf.layers.DenseLayer
      import org.deeplearning4j.nn.conf.layers.OutputLayer
      import org.deeplearning4j.nn.multilayer.MultiLayerNetwork
      import org.deeplearning4j.nn.weights.WeightInit
      import org.deeplearning4j.spark.impl.multilayer.
      SparkDl4jMultiLayer
      import org.nd4j.linalg.io.ClassPathResource
      import org.nd4j.linalg.lossfunctions.LossFunctions

           object FeedForwardNetworkWithSpark {
           def main(args:Array[String]): Unit ={
           val recordReader:RecordReader = new CSVRecordReader(0,",")
           val conf = new SparkConf()
           .setMaster("spark://master:7077")
           .setAppName("FeedForwardNetwork-Iris")
           val sc = new SparkContext(conf)
           val numInputs:Int = 4
           val outputNum = 3
           val iterations =1
           val multiLayerConfig:MultiLayerConfiguration = new
             NeuralNetConfiguration.Builder()
             .seed(12345)
             .iterations(iterations)
            .optimizationAlgo(OptimizationAlgorithm
                              .STOCHASTIC_GRADIENT_DESCENT)
             .learningRate(1e-1)
             .l1(0.01).regularization(true).l2(1e-3)
             .list(3)
             .layer(0, new DenseLayer.Builder().nIn(numInputs).nOut(3)
             .activation("tanh")
             .weightInit(WeightInit.XAVIER)
             .build())
             .layer(1, new DenseLayer.Builder().nIn(3).nOut(2)
             .activation("tanh")
             .weightInit(WeightInit.XAVIER)
             .build())
             .layer(2, new
              OutputLayer.Builder(LossFunctions.LossFunction.MCXENT)
               .weightInit(WeightInit.XAVIER)
               .activation("softmax")
               .nIn(2).nOut(outputNum).build())
               .backprop(true).pretrain(false)
               .build
           val network:MultiLayerNetwork = new
           MultiLayerNetwork(multiLayerConfig)
           network.init
           network.setUpdater(null)
           val sparkNetwork:SparkDl4jMultiLayer = new
           SparkDl4jMultiLayer(sc,network)
           val nEpochs:Int = 6
           val listBuffer = new ListBuffer[Array[Float]]()
           (0 until nEpochs).foreach{i =&gt;
           val net:MultiLayerNetwork =
           sparkNetwork.fit("file:///&lt;path&gt;/
           iris_shuffled_normalized_csv.txt",4,recordReader)
           listBuffer +=(net.params.data.asFloat().clone())
           }
           println("Parameters vs. iteration Output: ")
           (0 until listBuffer.size).foreach{i =&gt;
           println(i+"\t"+listBuffer(i).mkString)}
         }
      }
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec347"></a>How it worksâ€¦</h3></div></div></div><p>The preceding example shows how to use a feed-forward network for classifying the <code class="literal">Iris</code> dataset. At first, <code class="literal">SparkConf</code> and <code class="literal">SparkContext</code> are initialized and then the configuration parameters such as <code class="literal">numInputs</code>, <code class="literal">outputNum</code>Â and <code class="literal">iterations</code> have been initialized. The <code class="literal">val multiLayerConfig:MultiLayerConfiguration = new NeuralNetConfiguration.Builder()</code> line runs a builder pattern useful for many parameter objects, on a <code class="literal">NeuralNetConfiguration</code> (it can create a single layer if needed). The <code class="literal">NeuralNetConfiguration</code> object can construct many layers and these in turn make a deeper neural network.</p><p>The line <code class="literal">.seed(12345)</code> is used for weight initialization. The line<code class="literal">.iterations(iterations)</code> specifies the maximum number of iterations the training algorithm will train. The <code class="literal">learningRate(1e-1)</code> specifies the learning rate, which is the size of the adjustments made to weights with each iteration. A too high or too low learning rate could affect negatively the convergence of the training process. The <code class="literal">.optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)</code> line specifies the optimization algorithm as stochastic gradient descent. Next, the regularization parameter is set to <code class="literal">true</code> and L2 regularization is chosen. The line <code class="literal">.list(3)</code> specifies the number of neural net layers . We are specifying the layers as <code class="literal">DenseLayer</code> and the setting <code class="literal">.nIn(numInputs).nOut(3)</code> sets the number of input nodes as <code class="literal">numInputs</code> and output nodes as 3 for a layer. The <code class="literal">.activation("tanh")</code> line sets the activation function to a <code class="literal">tanh</code> function and the <code class="literal">.weightInit(WeightInit.XAVIER)</code> line initializes the weights. Also, back-propagation is set to true as <code class="literal">.backprop(true)</code> and pre-training is set to <code class="literal">false</code> as <code class="literal">.pretrain(false)</code>.</p><p>The line <code class="literal">.build</code> builds the configuration settings and this is passed as parameter to the instance of the <code class="literal">MultiLayerNetwork</code> model. The settings <code class="literal">network.init</code> and <code class="literal">network.setUpdater(null)</code>initialize the network and then set the updater to null. Next, <code class="literal">SparkDl4jMultiLayer</code> is initialized and the <code class="literal">sparkNetwork.fit("file:///home/padma/data/iris_shuffled_normalized_csv.txt",4, recordReader)</code> line makes the neural network learn on the dataset.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec348"></a>There's moreâ€¦</h3></div></div></div><p>In the preceding code, we worked with a feed-forward neural network. Using the <code class="literal">NeuralNetConfiguration</code> object, we can create a <span class="strong"><strong>Convolutional Neural Network</strong></span> (<span class="strong"><strong>CNN</strong></span>) as well as <span class="strong"><strong>Restricted Boltzmann Machines</strong></span> (<span class="strong"><strong>RBMs</strong></span>). A <code class="literal">NeuralNetConfiguration</code> object is the fundamental one to construct deeper layers. Datasets are transformed as they are processed by each layer - before and after each layer, they undergo additional pre- or post-processing such as normalization. When initializing the seed, suppose the algorithm is run many times, as new random weights get initialized each time the network's F1 score varies, leading to different local minima of the error scape. When weights are kept the same, the effect of adjusting hyper-parameters is clearly seen.</p><p>The <code class="literal">iterations </code>parameter specifies the number of times the algorithm trains to classify samples with a corrected weight update. An iteration is applicable to an <code class="literal">epoch</code> (a completed pass through a dataset). Too few iterations might truncate the learning and too many might slow down the learning. When specifying the learning rate, it is essential to initialize the optimal rate, because a high learning rate causes the network to traverse the error scape and it is prone to overshoot the minima, whereas with a lower learning rate, the training doesn't converge. In the case of regularization, L1 and L2 are two ways to avoid over-fitting by decreasing the size of the model's weights.</p><p>Also, when initializing weights, <code class="literal">Xavier initialization</code> keeps weights from becoming too small or too large. Xavier initializes a given neuron's weight by making the variance of that weight equal to one over the number of neurons feeding into it. Apart from training the model, it is also essential to evaluate the model and display statistics such as accuracy and F1 score. F1 score is the metric which determines how well a classifier works. It's between 0 and 1 and explains how well network performed during the training. It is basically the probability that the network guesses are correct.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec349"></a>See also</h3></div></div></div><p>Please refer toÂ <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Big Data Analytics with Spark</em></span> to get familiar with Spark. Also, refer to <a class="ulink" href="http://deeplearning.net/tutorial/" target="_blank">http://deeplearning.net/tutorial/</a> and <a class="ulink" href="http://neuralnetworksanddeeplearning.com/" target="_blank">http://neuralnetworksanddeeplearning.com/</a> for details on deep learning.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec98"></a>Running an RBM with DeepLearning4j over Spark</h2></div></div><hr /></div><p>In this recipe, we'll see how to run a restricted Boltzmann machine for classifying the iris dataset.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec350"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark cluster either in pseudo distributed mode or in one of the distributed modes, that is, standalone, YARN, or Mesos. Also, get familiar with ND4S, that is, n-dimensional arrays for Scala (Scala bindings for ND4J). ND4J and ND4S are scientific computing libraries for the JVM. Please visit <a class="ulink" href="http://nd4j.org/" target="_blank">http://nd4j.org/</a> for details. The pre-requisites to be installed are Java 7, IntelliJ, and the Maven or SBT build tool.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec351"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Start an application named <code class="literal">RBMWithSpark</code>. Initially, specify the following libraries in the <code class="literal">build.sbt</code> file:</p><pre class="programlisting">      libraryDependencies ++= Seq(
      "org.apache.spark" %% "spark-core" % "1.6.0",
      "org.apache.spark" %% "spark-mllib" % "1.6.0",
      "org.deeplearning4j" % "deeplearning4j-core" % "0.4-rc3.8",
      "org.deeplearning4j" % "deeplearning4j-nlp" % "0.4-rc3.8",
      "org.deeplearning4j" % "dl4j-spark" % "0.4-rc3.8",
      "org.nd4j" % "nd4j-x86" % "0.4-rc3.8",
      "org.nd4j" % "nd4j-jcublas-7.0" % "0.4-rc3.8",
      "org.nd4j" % "nd4j-api" % "0.4-rc3.8",
      "org.nd4j" % "canova-api" % "0.0.0.14"
      )
</pre></li><li><p>Here is the code for a restricted Boltzmann machine which uses the iris dataset for classification:</p><pre class="programlisting">      import org.deeplearning4j.datasets.iterator.impl.
      IrisDataSetIterator
      import org.deeplearning4j.eval.Evaluation
      import org.deeplearning4j.nn.api.{Layer, OptimizationAlgorithm}
      import org.deeplearning4j.nn.conf.{Updater,
      NeuralNetConfiguration}
      import org.deeplearning4j.nn.conf.layers.{OutputLayer, RBM}
      import org.deeplearning4j.nn.multilayer.MultiLayerNetwork
      import org.deeplearning4j.nn.weights.WeightInit
      import org.nd4j.linalg.factory.Nd4j
      import org.nd4j.linalg.lossfunctions.LossFunctions

      object RBM_IrisDataset {
      def main(args: Array[String]) {
      Nd4j.MAX_SLICES_TO_PRINT = -1
      Nd4j.MAX_ELEMENTS_PER_SLICE = -1
      Nd4j.ENFORCE_NUMERICAL_STABILITY = true
      val inputNum = 4
      var outputNum = 3
      var numSamples = 150
      var batchSize = 150
      var iterations = 1000
      var seed = 321
      var listenerFreq = iterations / 5
      val learningRate = 1e-6
      println("Loading data....")
      val iter = new IrisDataSetIterator(batchSize, numSamples)
      val iris = iter.next()
      iris.shuffle()
      iris.normalizeZeroMeanZeroUnitVariance()
      val testAndTrain = iris.splitTestAndTrain(0.80)
      val train = testAndTrain.getTrain
      val test = testAndTrain.getTest
      println("Building model....")
      val RMSE_XENT = LossFunctions.LossFunction.RMSE_XENT
      val conf = new NeuralNetConfiguration.Builder()
        .seed(seed)
        .iterations(iterations)
        .learningRate(learningRate)
        .l1(1e-1).regularization(true).l2(2e-4)
        .optimizationAlgo(OptimizationAlgorithm
        .CONJUGATE_GRADIENT)
        .useDropConnect(true)
        .list(2)
        .layer(0, new RBM.Builder(RBM.HiddenUnit.RECTIFIED,
        RBM.VisibleUnit.GAUSSIAN)
         .nIn(inputNum).nOut(3).k(1).activation("relu")
         .weightInit(WeightInit.XAVIER).lossFunction(RMSE_XENT)
            .updater(Updater.ADAGRAD).dropOut(0.5)
            .build())
            .layer(1, new OutputLayer.Builder(LossFunctions
                   .LossFunction.MCXENT)
            .nIn(3).nOut(outputNum).activation("softmax").build())
            .build()

      val model = new MultiLayerNetwork(conf)
      model.init()

      println("Train the model....")
      model.fit(train.getFeatureMatrix)
      println("Evaluating the model....")
      val eval = new Evaluation(outputNum)
      val output = model.output(test.getFeatureMatrix,
                             Layer.TrainingMode.TEST)
      (0 until output.rows()).foreach { i =&gt;
      val actual = train.getLabels.getRow(i).toString.trim()
      val predicted = output.getRow(i).toString.trim()
      println("actual " + actual + " vs predicted " + predicted)
      }
      eval.eval(test.getLabels, output)
      println(eval.stats())
       }
     }
</pre></li><li><p>The output looks as follows:</p><pre class="programlisting">        Evaluate model....
        actual [ 1.00, 0.00, 0.00] vs predicted [ 0.39, 0.32, 0.29]
        actual [ 1.00, 0.00, 0.00] vs predicted [ 0.35, 0.34, 0.31]
        actual [ 1.00, 0.00, 0.00] vs predicted [ 0.35, 0.34, 0.30]
        actual [ 1.00, 0.00, 0.00] vs predicted [ 0.34, 0.34, 0.31]
        actual [ 0.00, 0.00, 1.00] vs predicted [ 0.36, 0.35, 0.28]
        actual [ 1.00, 0.00, 0.00] vs predicted [ 0.35, 0.34, 0.31]
        actual [ 0.00, 1.00, 0.00] vs predicted [ 0.35, 0.35, 0.30]
        actual [ 1.00, 0.00, 0.00] vs predicted [ 0.35, 0.34, 0.31]
        .
        .
        .
        Examples labeled as 0 classified by model as 0: 5 times
        Examples labeled as 1 classified by model as 0: 11 times
        Examples labeled as 2 classified by model as 0: 14 times
        Warning: class 1 was never predicted by the model. This class
        was excluded from the average precision
        Warning: class 2 was never predicted by the model. This class
        was excluded from the average precision
        ==========================Scores================
          Accuracy:  0.1667
          Precision: 0.1667
          Recall:    0.3333
          F1 Score:  0.2222
        ==============================================
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec352"></a>How it worksâ€¦</h3></div></div></div><p>The Iris flower dataset is widely used in machine learning to test classification techniques. Here, the same is used for testing the effectiveness of a restricted Boltzmann machine. Initially, the configuration parameters such as <code class="literal">inputNum</code>, <code class="literal">outputNum</code>, <code class="literal">numSamples</code>, <code class="literal">batchSize</code>, <code class="literal">seed</code>Â and <code class="literal">iterations</code> have been initialized. The dataset is available as a CSV file. The <code class="literal">IrisDataSetIterator</code> is used to traverse the dataset. The <code class="literal">val iris = iter.next()</code> line assigns each data point to a <code class="literal">DataSet</code> object with which the neural network trains. Next, the <code class="literal">iris.splitTestAndTrain(0.80)</code> line splits the dataset into training and testing samples.</p><p>The <code class="literal">val conf= new NeuralNetConfiguration.Builder()</code> line runs a builder pattern useful for many parameter objects, on a <code class="literal">NeuralNetConfiguration</code> (it can create a single layer if needed). The <code class="literal">NeuralNetConfiguration</code> object can construct many layers and these in turn make a deeper neural network. Once the <code class="literal">NeuralNetConfiguration</code> object is created, various configuration parameters such as <code class="literal">seed</code>, <code class="literal">iterations</code>, <code class="literal">learningRate</code>, <code class="literal">regularization</code>Â and <code class="literal">optimizationAlgo</code> are specified. The <code class="literal">.useDropConnect(true)</code> line ensures that the <code class="literal">DropConnect</code> is used (a regularization technique which randomly sets to zero a subset of activations within each layer).Â The <code class="literal">.list(2)</code> line specifies the number of neural net layers. We are specifying the layers as RBM which is instantiated as new <code class="literal">RBM.Builder(RBM.HiddenUnit.RECTIFIED</code>, <code class="literal">RBM.VisibleUnit.GAUSSIAN)</code> and the setting <code class="literal">.nIn(inputNum).nOut(3)</code> sets the number of input nodes as <code class="literal">inputNum</code> and output nodes as 3 for a layer. The <code class="literal">.activation("relu")</code> line sets the activation function to a rectified linear transform. The <code class="literal">LossFunctions.LossFunction.MCXENT</code> line sets the loss function.</p><p>When all the layers are instantiated, the <code class="literal">.build()</code> line calls build on the configuration and this instance is passed as a parameter to theÂ <code class="literal">MultiLayerNetwork</code>. The <code class="literal">model.init</code> initializes the model. The <code class="literal">model.fit(train.getFeatureMatrix)</code> line makes the neural network learn by passing the training set. Finally, the model is evaluated and the output displays how well the samples are labeled by each classification. The output also displays the accuracy and F1 score.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec353"></a>There's moreâ€¦</h3></div></div></div><p>In the preceding code, we worked with an RBM. Using the <code class="literal">NeuralNetConfiguration</code> object, we can create a CNN as well. A <code class="literal">NeuralNetConfiguration</code> object is the fundamental one to construct deeper layers. The <code class="literal">DataSetIterator</code> fetches one or more examples with each iteration and loads these examples into a <code class="literal">DataSet</code> object. The <code class="literal">DataSetIterator</code> takes two parameters - <code class="literal">batchSize</code> (specifies the number of examples to be fetched with each step) and <code class="literal">numSamples</code> (the total number of input data examples). After the <code class="literal">DataSet</code> is created, it is normalized to another scale, which is likely known as feature-scaling. The neural net learns on the training set and it verifies the guessed labels against the ground truth of the test set. When initializing the seed, suppose the algorithm is run many times, as new random weights get initialized each time the network's F1 score varies leading to different local minima of the error scape. When the weights are kept the same, the effect of adjusting hyper-parameters is clearly seen.</p><p>The <code class="literal">iterations</code> parameter specifies the number of times the algorithm trains to classify samples with a corrected weight update. An iteration is applicable to an epoch (a completed pass through a dataset). Too few iterations might truncate the learning and too many might slow down the learning rate. When specifying the learning rate, it is essential to initialize the optimal rate, because a high learning rate causes the network to traverse the errorscape and is prone to overshoot the minima, whereas a lower learning rate doesn't converge. In the case of regularization, L1 and L2 are two ways to avoid over-fitting by decreasing the size of the model's weights.</p><p>Also, DropConnect allows a neural network to generalize from training data by randomly cancelling out the interlayer edges between nodes. The layer is an RBM which is a shallow, building-block layer that is stacked to become a deep belief network. A Gaussian transform is applied and the Gaussian white noise is applied to normalize a distribution of continuous data. The <span class="strong"><strong>Rectified Linear Unit</strong></span> (<span class="strong"><strong>ReLU</strong></span>) creates more robust activations and this also improves the F1 score. ReLU applies fixed offset to the bias of each node. Apart from training the it is also essential to evaluate the model and display statistics such as accuracy and F1 score. F1 score is the metric which determines how well a classifier works. It's between 0 and 1 and explains how well the network performed during the training. It is basically the probability that the network guesses are correct.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec354"></a>See also</h3></div></div></div><p>Please refer toÂ 
<a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Big Data Analytics with Spark</em></span> to get familiar with Spark. Also, refer to <a class="ulink" href="http://deeplearning.net/tutorial/" target="_blank">http://deeplearning.net/tutorial/</a> and <a class="ulink" href="http://neuralnetworksanddeeplearning.com/" target="_blank">http://neuralnetworksanddeeplearning.com/</a> for details on deep learning.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec99"></a>Running a CNN for learning MNIST with DeepLearning4j over Spark</h2></div></div><hr /></div><p>In this recipe, we'll see how to run a CNN for classifying the iris dataset.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec355"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark cluster either in pseudo distributed mode or in one of the distributed modes, that is, standalone, YARN, or Mesos. Also, get familiar with ND4S, that is, n-dimensional arrays for Scala (Scala bindings for ND4J). ND4J and ND4S are scientific computing libraries for the JVM. Please visit <a class="ulink" href="http://nd4j.org/" target="_blank">http://nd4j.org/</a> for details. The prerequisites to be installed are Java 7, IntelliJ, and the Maven or SBT build tool.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec356"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>The MNIST database is a large set of handwritten digits used to train neural networks and other algorithms in image recognition. This dataset has 60,000 images in its training set and 10,000 in its test set. Each image is a 28X28 pixel.</p></li><li><p>Here is the code for a convolutional neural network which uses the MNIST dataset for digit recognition:</p><pre class="programlisting">      object CNN_MNIST {

        def main(args:Array[String]): Unit ={

          val nCores =2
          val conf = new SparkConf()
         .setMaster("spark://master:7077")
         .setAppName("MNIST_CNN")
         .set(SparkDl4jMultiLayer.AVERAGE_EACH_ITERATION,
          String.valueOf(true))
          val sc = new SparkContext(conf)
          val nChannels = 1
          val outputNum = 10
          val numSamples = 60000
          val nTrain = 50000
          val nTest = 10000
          val batchSize = 64
          val iterations = 1
          val seed = 123
          val mnistIter = new MnistDataSetIterator(1,numSamples, true)
          val allData = new ListBuffer[DataSet]()
          while(mnistIter.hasNext) allData.+=(mnistIter.next)
          new Random(12345).shuffle(allData)
            val iter = allData.iterator
            var c =0
            val train = new ListBuffer[DataSet]()
            val test = new ListBuffer[DataSet]()
          while(iter.hasNext) {
            if(c &lt;= nTrain) {
              train.+=(iter.next)
              c +=1
            }
            else test.+=(iter.next)
          }
          val sparkDataTrain = sc.parallelize(train)
          sparkDataTrain.persist(StorageLevel.MEMORY_ONLY)
          println("Building model ....")
          val builder = new NeuralNetConfiguration.Builder()
          .seed(seed)
          .iterations(iterations)
          .regularization(true).l2(0.0005)
          .learningRate(0.01)
          .optimizationAlgo(OptimizationAlgorithm
          .STOCHASTIC_GRADIENT_DESCENT)
          .updater(Updater.ADAGRAD)
          .list(6)
          .layer(0, new ConvolutionLayer.Builder(5, 5)
          .nIn(nChannels)
          .stride(1, 1)
          .nOut(20)
          .weightInit(WeightInit.XAVIER)
          .activation("relu")
          .build())
          .layer(1, new SubsamplingLayer.Builder(SubsamplingLayer
          .PoolingType.MAX, Array(2, 2))
          .build())
          .layer(2, new ConvolutionLayer.Builder(5, 5)
          .nIn(20)
          .nOut(50)
          .stride(2,2)
          .weightInit(WeightInit.XAVIER)
          .activation("relu")
          .build())
          .layer(3, new
          SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX,
          Array(2, 2))
         .build())
         .layer(4, new DenseLayer.Builder().activation("relu")
         .weightInit(WeightInit.XAVIER)
         .nOut(200).build())
         .layer(5, new OutputLayer.Builder
         (LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)
         .nOut(outputNum)
         .weightInit(WeightInit.XAVIER)
         .activation("softmax")
         .build())
         .backprop(true).pretrain(false);
         new ConvolutionLayerSetup(builder,28,28,1);
         val multiLayerConf:MultiLayerConfiguration= builder.build()
         val net:MultiLayerNetwork = new
         MultiLayerNetwork(multiLayerConf)
         net.init()
         net.setUpdater(null)
         val sparkNetwork = new SparkDl4jMultiLayer(sc, net)
         val nEpochs = 5
         (0 until nEpochs).foreach{i =&gt;
         val network = sparkNetwork.fitDataSet(sparkDataTrain,
         nCores*batchSize)
        //Evaluate the model
        val eval = new Evaluation()
        for(ds &lt;- test)
           {
          val output = network.output(ds.getFeatureMatrix)
                eval.eval(ds.getLabels, output)
              }
            println("Statistics..."+eval.stats())
          }
        }
      }
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec357"></a>How it worksâ€¦</h3></div></div></div><p>The preceding code shows how to use a convolutional neural network for recognizing the handwritten digits in the MNIST dataset. Initially, the configuration parameters such as <code class="literal">nChannels</code>, <code class="literal">outputNum</code>, <code class="literal">numSamples</code>, <code class="literal">nTrain</code>, <code class="literal">nTest</code>, <code class="literal">batchSize</code>, <code class="literal">seed</code> and <code class="literal">iterations</code> have been initialized. The <code class="literal">MnistDataSetIterator</code> initializes the MNIST dataset and this dataset is split into training and testing sets.</p><p>The <code class="literal">val builder= new NeuralNetConfiguration.Builder()</code> line runs a builder pattern useful for many parameter objects, on a <code class="literal">NeuralNetConfiguration</code> (it can create a single layer if needed). The <code class="literal">NeuralNetConfiguration</code> object can construct many layers and these in turn make a deeper neural network. Once the <code class="literal">NeuralNetConfiguration</code> object is created, various configuration parameters such as <code class="literal">seed</code>, <code class="literal">iterations</code>, <code class="literal">learningRate</code>, <code class="literal">regularization</code>Â and <code class="literal">optimizationAlgo</code> are specified.</p><p>We are have specified six layers, in which the input layer is <code class="literal">ConvolutionalLayer</code>, initialized as <code class="literal">-new ConvolutionLayer.Builder(5, 5)</code>, three hidden layers are initialized as sub-sampling layers <code class="literal">new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX, Array(2, 2)</code>, Â and a 4th layer is initialized as <code class="literal">new DenseLayer.Builder().activation("relu"))</code>.The output layer is initialized as new <code class="literal">OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)</code>.</p><p>The setting <code class="literal">.nIn(20).nOut(50)</code> sets the number of input nodes as 20 and output nodes as 50 for a layer. The line <code class="literal">.activation("relu")</code> sets the activation function to a rectified linear transform. The <code class="literal">LossFunctions.LossFuncti.NEGATIVELOGLIKELIHOOD</code> line sets the loss function. The <code class="literal">.weightInit(WeightInit.XAVIER)</code> line initializes the weights. For the output layer, the activation function is softmax. Also, backpropagation-propogation is set to <code class="literal">true</code> as <code class="literal">.backprop(true)</code> and pre-training is set to <code class="literal">false</code> as <code class="literal">.pretrain(false)</code>.</p><p>The <code class="literal">.build</code> line builds the configuration settings and this is passed as parameter to the instance of <code class="literal">MultiLayerNetwork</code> model. The settings <code class="literal">network.init</code> and <code class="literal">network.setUpdater(null)</code> initialize the network and then set the updater to null. Next, <code class="literal">SparkDl4jMultiLayer</code> is initialized and the line <code class="literal">sparkNetwork.fitDataSet(sparkDataTrain,nCores*batchSize)</code> makes the neural network learn on the dataset. Finally, the model is evaluated and the output displays how well the samples are labeled by each classification. The output displays the statistics.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec358"></a>There's moreâ€¦</h3></div></div></div><p>Using the <code class="literal">NeuralNetConfiguration</code> object, we created a CNN (convolutional neural network). A <code class="literal">NeuralNetConfiguration</code> object is the fundamental one to construct deeper layers. Convolutional networks perform object recognition with images. They can identify faces, individuals, street signs, and many other aspects of visual data. These networks overlap with character recognition; they are also useful when analyzing words as discrete textual units.</p><p>The efficiency of convolutional nets <code class="literal">ConvNets</code>Â in image recognition is one of the main reasons why the world has up to deep learning. They are powering major advances in machine vision, which has obvious applications for self-driving cars, robotics, drones, and treatments for the visually impaired. Convolutional nets ingest and process images as tensors and tensors are matrices of numbers with additional dimensions. Tensors are formed by arrays nested within arrays, and that nesting can go on infinitely, accounting for an arbitrary number of dimensions far greater than what we can visualize spatially.</p><p>Convolutional networks perceive images as volumes, that is, three-dimensional objects, rather than flat canvases to be measured only by width and height. That's because digital color images have a <span class="strong"><strong>red-blue-green</strong></span> (<span class="strong"><strong>RGB</strong></span>) encoding, mixing those three colors to produce the color spectrum humans perceive. A convolutional network ingests such images as three separate strata of color stacked one on top of the other. To know more about convolutional neural networks, please visit <a class="ulink" href="http://ufldl.stanford.edu/tutorial/supervised/ConvolutionalNeuralNetwork/" target="_blank">http://ufldl.stanford.edu/tutorial/supervised/ConvolutionalNeuralNetwork/</a> and <a class="ulink" href="http://deeplearning.net/tutorial/lenet.html" target="_blank">http://deeplearning.net/tutorial/lenet.html</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec359"></a>See also</h3></div></div></div><p>Please refer toÂ <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Big Data Analytics with Spark</em></span> to get familiar with Spark. Also, refer to <a class="ulink" href="http://deeplearning.net/tutorial/" target="_blank">http://deeplearning.net/tutorial/</a> and <a class="ulink" href="http://neuralnetworksanddeeplearning.com/" target="_blank">http://neuralnetworksanddeeplearning.com/</a> for details on deep learning.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec100"></a>Installing TensorFlow</h2></div></div><hr /></div><p>TensorFlow is an interface for expressing machine learning algorithms, and it's an implementation for executing such algorithms. The TensorFlow computation can be expressed with little or no change on a wide variety of heterogeneous systems, such as mobile phones, tablets, and large-scale distributed systems of hundreds of machines. It is flexible and can express a wide variety of algorithms, such as training and inference algorithms for deep neural network models. It is also used for deploying machine learning systems into production across many areas, such as speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and so on.</p><p>The application of tensors and their networks is a relatively new (but fast-evolving) approach in machine learning. Tensors, if you recall your algebra classes, are simply n-dimensional data arrays (so a scalar is a 0th order tensor, a vector is 1st order, and a matrix 2nd order). A simple practical example of this is a color image's RGB layers (essentially three 2D matrices combined into a 3rd order tensor). Or a more business-minded example - if your data source generates a table (a 2D array) every hour, you can look at the full dataset as a 3rd order tensor--time being the extra dimension.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec360"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark Cluster either in pseudo distributed mode or in one of the distributed modes, that is, standalone, YARN, or Mesos.</p><p>Python comes pre-installed. <code class="literal">python --version</code> gives the version of the Python installed. If the version seems to be 2.6.x, upgrade it to Python 2.7 as follows:</p><pre class="programlisting">
<span class="strong"><strong>    sudo apt-get install python2.7</strong></span>
</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec361"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Install pip as follows:</p><pre class="programlisting">
<span class="strong"><strong>     sudo apt-get install python-pip python-dev</strong></span>
</pre></li><li><p>Install TensorFlow as follows:</p><pre class="programlisting">
<span class="strong"><strong>      sudo pip install --upgrade
      https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-
      0.8.0rc0-cp27-none-linux_x86_64.whl
      Downloading/unpacking
      https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-
      0.8.0rc0-cp27-none-linux_x86_64.whl
      Downloading tensorflow-0.8.0rc0-cp27-none-linux_x86_64.whl
      (22.2MB): 22.2MB downloaded
      Requirement already up-to-date: protobuf==3.0.0b2 in
      /usr/local/lib/python2.7/dist-packages (from
      tensorflow==0.8.0rc0)
      Requirement already up-to-date: wheel in
      /usr/local/lib/python2.7/dist-packages (from
      tensorflow==0.8.0rc0)
      Requirement already up-to-date: numpy&gt;=1.8.2 in
      /usr/local/lib/python2.7/dist-packages (from
      tensorflow==0.8.0rc0)
      Requirement already up-to-date: six&gt;=1.10.0 in
      /usr/local/lib/python2.7/dist-packages (from
      tensorflow==0.8.0rc0)
      Installing collected packages: tensorflow
      Successfully installed tensorflow
      Cleaning up...
</strong></span>
</pre></li><li><p>Once TensorFlow is installed, try the following code from the command line to ensure that itis working:</p><pre class="programlisting">
<span class="strong"><strong>      python
      Python 2.7.6 (default, Mar 22 2014, 22:59:56)
      [GCC 4.8.2] on linux2
      Type "help", "copyright", "credits" or "license" for more
      information.
      &gt;&gt;&gt; import tensorflow as tf
      &gt;&gt;&gt; hello = tf.constant('Hello, TensorFlow!')
      &gt;&gt;&gt; sess = tf.Session()
      &gt;&gt;&gt; print(sess.run(hello))
      Hello, TensorFlow!
      &gt;&gt;&gt; a = tf.constant(10)
      &gt;&gt;&gt; b = tf.constant(32)
      &gt;&gt;&gt; print(sess.run(a + b))
      42
</strong></span>
</pre></li><li><p>TheÂ TensorFlow package will be available at the following location: <code class="literal">/usr/local/lib/python2.7/dist-packages/tensorflow</code></p></li><li><p>Now, from the preceding location, run the model for classifying handwritten digits from the MNIST dataset available in the sub-directory <code class="literal">models/image/mnist/convolutional.py</code> as follows:</p><pre class="programlisting">
<span class="strong"><strong>        $ cd models/image/mnist
        /&lt;TensorFlow_Package_Path&gt;/models/image/mnist$ python
        convolutional.py
        Extracting data/train-images-idx3-ubyte.gz
        Extracting data/train-labels-idx1-ubyte.gz
        Extracting data/t10k-images-idx3-ubyte.gz
        Extracting data/t10k-labels-idx1-ubyte.gz
        Initialized!
        Step 0 (epoch 0.00), 4.4 ms
        Minibatch loss: 12.054, learning rate: 0.010000
        Minibatch error: 90.6%
        Validation error: 84.6%
        Step 100 (epoch 0.12), 401.4 ms
        Minibatch loss: 3.289, learning rate: 0.010000
        Minibatch error: 6.2%
        Validation error: 7.0%
        Step 200 (epoch 0.23), 404.5 ms</strong></span>
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec362"></a>How it worksâ€¦</h3></div></div></div><p>Initially, the installation commands install the pip and  TensorFlow packages. The basic TensorFlow program initializes a string constant as <code class="literal">tf.constant('Hello, TensorFlow!')</code>. The <code class="literal">sess = tf.Session()</code> line creates the TensoFlow session and then <code class="literal">print(sess.run(hello))</code> starts the TensorFlow session and displays the constant string message.Â 
Then, for other constants <code class="literal">a</code> and <code class="literal">b</code>, the sum is calculated and the result is displayed.</p><p>Next, the TensorFlow demo model runs the simple convolutional neural network which makes the network learn the MNIST data. It achieves a validation error of 7.0%.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec363"></a>There's moreâ€¦</h3></div></div></div><p>TensorFlow is the dataflow graph representing computations. Nodes represent operations and the edges represent tensors (multi-dimensional arrays, the backbone of TensorFlow). The entire dataflow graph is a complete description of computations which occur within a session, and are executed on devices such as CPUs or GPUs. TensorFlow supports the Python API where tensors are represented as NumPy <code class="literal">ndarray</code> objects. It also has support for C++ API.</p><p>Graphs are constructed from nodes that don't require input which then pass their output to further operations, which in turn performing computations on these output tensors. These operations are performed asynchronously and optionally in parallel.</p><p>For more details, please visit: <a class="ulink" href="https://www.tensorflow.org/" target="_blank">https://www.tensorflow.org/</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec364"></a>See also</h3></div></div></div><p>Please refer toÂ <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Big Data Analytics with Spark</em></span> to get familiar with Spark. Also, refer to <a class="ulink" href="http://deeplearning.net/tutorial/" target="_blank">http://deeplearning.net/tutorial/</a> and <a class="ulink" href="http://neuralnetworksanddeeplearning.com/" target="_blank">http://neuralnetworksanddeeplearning.com/</a> for details on deep learning.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec101"></a>Working with Spark TensorFlow</h2></div></div><hr /></div><p>As Spark offers distributed computation, it can be used to perform neural network training on large data and the model deployment could be done at scale. The distributed training cuts down the training time, improves accuracy and also speeds up the model validation over a single-node model validation. The ability to scale model selection and neural network tuning by adopting tools such as Spark and TensorFlow may be a boon for the data science and machine learning communities because of the increasing availability of cloud computing and parallel resources to a wider range of engineers.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec365"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark cluster either in pseudo distributed mode or in one of the distributed modes, that is, standalone, YARN, or Mesos. Also, haveÂ <span class="emphasis"><em>Installing TensorFlow</em></span> recipe for details on the installation.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec366"></a>How to do itâ€¦</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Here is the Python code to run TensorFlow in distributed mode:</p><pre class="programlisting">      import numpy as np
      import tensorflow as tf
      import os
      from tensorflow.python.platform import gfile
      import os.path
      import re
      import sys
      import tarfile
      from subprocess import Popen, PIPE, STDOUT
      from pyspark import SparkContext
      def run(cmd):
      p = Popen(cmd, shell=True, stdin=PIPE, stdout=PIPE,
          stderr=STDOUT, close_fds=True)
      return p.stdout.read()
      model_dir = '/tmp/imagenet'
      image_file = ""
      num_top_predictions = 5
      DATA_URL =
      'http://download.tensorflow.org/models/image/imagenet/inception-
      2015-12-05.tgz'
      IMAGES_INDEX_URL = 'http://image-
      net.org/imagenet_data/urls/imagenet_fall11_urls.tgz'
</pre></li><li><p>The number of images to process is as follows:</p><pre class="programlisting">    image_batch_size = 3
    max_content = 1000L
    sc = SparkContext("local", "Distributed_tensorFlow")
    def read_file_index():
    from six.moves import urllib
    content = urllib.request.urlopen(IMAGES_INDEX_URL)
    data = content.read(max_content)
    tmpfile = "/tmp/imagenet.tgz"
    with open(tmpfile, 'wb') as f:
    f.write(data)
    run("tar -xOzf %s &gt; /tmp/imagenet.txt" % tmpfile)
    with open("/tmp/imagenet.txt", 'r') as f:
    lines = [l.split() for l in f]
    input_data = [tuple(elts) for elts in lines if len(elts) == 2]
    return [input_data[i:i+image_batch_size] for i in
    range(0,len(input_data),
    image_batch_size)]
    class NodeLookup(object):
        def __init__(self,
                   label_lookup_path=None,
                   uid_lookup_path=None):
        if not label_lookup_path:
         label_lookup_path = os.path.join(
            model_dir, 'imagenet_2012_challenge_label_map_proto.pbtxt')
        if not uid_lookup_path:
        uid_lookup_path = os.path.join(
              model_dir, 'imagenet_synset_to_human_label_map.txt')
        self.node_lookup = self.load(label_lookup_path,
                           uid_lookup_path)

        def load(self, label_lookup_path, uid_lookup_path):
        if not gfile.Exists(uid_lookup_path):
          tf.logging.fatal('File does not exist %s', uid_lookup_path)
        if not gfile.Exists(label_lookup_path):
          tf.logging.fatal('File does not exist %s', label_lookup_path)
</pre></li><li><p>Loads mapping from string UID to human-readable string:</p><pre class="programlisting">     proto_as_ascii_lines = gfile.GFile(uid_lookup_path).readlines()
        uid_to_human = {}
        p = re.compile(r'[n\d]*[ \S,]*')
        for line in proto_as_ascii_lines:
        parsed_items = p.findall(line)
        uid = parsed_items[0]
        human_string = parsed_items[2]
        uid_to_human[uid] = human_string
</pre></li><li><p>Loads mapping from string UID to integer node ID:</p><pre class="programlisting">    node_id_to_uid = {}
    proto_as_ascii = gfile.GFile(label_lookup_path).readlines()
    for line in proto_as_ascii:
      if line.startswith('  target_class:'):
        target_class = int(line.split(': ')[1])
      if line.startswith('  target_class_string:'):
        target_class_string = line.split(': ')[1]
    node_id_to_uid[target_class] = target_class_string[1:-2]
</pre></li><li><p>Loads the final mapping of integer node ID to human-readable string:</p><pre class="programlisting">        node_id_to_name = {}
        for key, val in node_id_to_uid.items():
          if val not in uid_to_human:
          tf.logging.fatal('Failed to locate: %s', val)
          name = uid_to_human[val]
          node_id_to_name[key] = name
        return node_id_to_name
        def id_to_string(self, node_id):
        if node_id not in self.node_lookup:
          return ''
        return self.node_lookup[node_id]
        def create_graph():
        with gfile.FastGFile(os.path.join(
          model_dir, 'classify_image_graph_def.pb'), 'rb') as f:
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(f.read())
        _ = tf.import_graph_def(graph_def, name='')
        def run_inference_on_image(image):
        if not gfile.Exists(image):
        tf.logging.fatal('File does not exist %s', image)
        image_data = gfile.FastGFile(image, 'rb').read()
        create_graph()
        with tf.Session() as sess:
        softmax_tensor = sess.graph.get_tensor_by_name('softmax:0')
        predictions = sess.run(softmax_tensor,
                               {'DecodeJpeg/contents:0': image_data})
        predictions = np.squeeze(predictions)
</pre></li><li><p>Creates <code class="literal">node ID --&gt; English</code> string lookup:</p><pre class="programlisting">    node_lookup = NodeLookup()
    top_k = predictions.argsort()[-num_top_predictions:][::-1]
    for node_id in top_k:
      human_string = node_lookup.id_to_string(node_id)
      score = predictions[node_id]
      print('%s (score = %.5f)' % (human_string, score))
      def maybe_download_and_extract():
      from six.moves import urllib
      dest_directory = model_dir
      if not os.path.exists(dest_directory):
        os.makedirs(dest_directory)
      filename = DATA_URL.split('/')[-1]
      filepath = os.path.join(dest_directory, filename)
      if not os.path.exists(filepath):
        filepath2, _ = urllib.request.urlretrieve(DATA_URL, filepath)
        print("filepath2", filepath2)
        statinfo = os.stat(filepath)
        print('Succesfully downloaded', filename, statinfo.st_size,
                'bytes.')
        tarfile.open(filepath, 'r:gz').extractall(dest_directory)
          else:
         print('Data already downloaded:', filepath,
         os.stat(filepath))
      maybe_download_and_extract()
      batched_data = read_file_index()
      label_lookup_path = os.path.join(model_dir,
        'imagenet_2012_challenge_label_map_proto.pbtxt')
      uid_lookup_path = os.path.join(model_dir,
       'imagenet_synset_to_human_label_map.txt')
      def load_lookup():
      if not gfile.Exists(uid_lookup_path):
         tf.logging.fatal('File does not exist %s', uid_lookup_path)
      if not gfile.Exists(label_lookup_path):
         tf.logging.fatal('File does not exist %s', label_lookup_path)
</pre></li><li><p>The following code loads mapping from string UID to human-readable string:</p><pre class="programlisting">      proto_as_ascii_lines = gfile.GFile(uid_lookup_path).readlines()
      uid_to_human = {}
      p = re.compile(r'[n\d]*[ \S,]*')
      for line in proto_as_ascii_lines:
        parsed_items = p.findall(line)
        uid = parsed_items[0]
        human_string = parsed_items[2]
        uid_to_human[uid] = human_string
</pre></li><li><p>The following code loads mapping from string UID to integer node ID:</p><pre class="programlisting">      node_id_to_uid = {}
      proto_as_ascii = gfile.GFile(label_lookup_path).readlines()
      for line in proto_as_ascii:
        if line.startswith('  target_class:'):
          target_class = int(line.split(': ')[1])
        if line.startswith('  target_class_string:'):
          target_class_string = line.split(': ')[1]
        node_id_to_uid[target_class] = target_class_string[1:-2]
</pre></li><li><p>The following code loads the final mapping of integer node ID to human-readable string:</p><pre class="programlisting">      node_id_to_name = {}
      for key, val in node_id_to_uid.items():
        if val not in uid_to_human:
          tf.logging.fatal('Failed to locate: %s', val)
        name = uid_to_human[val]
        node_id_to_name[key] = name

      return node_id_to_name
      node_lookup = load_lookup()
      node_lookup_bc = sc.broadcast(node_lookup)
      model_path = os.path.join(model_dir,
      'classify_image_graph_def.pb')
      with gfile.FastGFile(model_path, 'rb') as f:
      model_data = f.read()
      model_data_bc = sc.broadcast(model_data)
      def run_image(sess, img_id, img_url, node_lookup):
      from six.moves import urllib
      from urllib2 import HTTPError
      try:
        image_data = urllib.request.urlopen(img_url,
                     timeout=1.0).read()
        except HTTPError:
        return (img_id, img_url, None)
        except:
        return (img_id, img_url, None)
        scores = []
        softmax_tensor = sess.graph.get_tensor_by_name('softmax:0')
        predictions = sess.run(softmax_tensor,
                             {'DecodeJpeg/contents:0': image_data})
        predictions = np.squeeze(predictions)
        top_k = predictions.argsort()[-num_top_predictions:][::-1]
        scores = []
        for node_id in top_k:
        if node_id not in node_lookup:
          human_string = ''
        else:
          human_string = node_lookup[node_id]
        score = predictions[node_id]
        scores.append((human_string, score))
        return (img_id, img_url, scores)
        def apply_batch(batch):
        with tf.Graph().as_default() as g:
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(model_data_bc.value)
        tf.import_graph_def(graph_def, name='')
        with tf.Session() as sess:
          labelled = [run_image(sess, img_id, img_url,
              node_lookup_bc.value) for (img_id, img_url) in batch]
          return [tup for tup in labelled if tup[2] is not None]
       urls = sc.parallelize(batched_data)
       labelled_images = urls.flatMap(apply_batch)
       local_labelled_images = labelled_images.collect()
       local_labelled_images
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec367"></a>How it worksâ€¦</h3></div></div></div><p>The constants <code class="literal">model_dir</code>, <code class="literal">image_file</code>, <code class="literal">num_top_predictions</code>, <code class="literal">DATA_URL</code>Â and <code class="literal">IMAGES_INDEX_URL</code> are initialized before start of the program. The <code class="literal">read_file_index</code> function reads the images, writes them to a temporary file, and returns the images and image batch size. The <code class="literal">NodeLookup</code> class contains related methods which convert integer node IDs to human-readable labels. The <code class="literal">__init__</code> method initializes the <code class="literal">label_lookup_path</code> from <code class="literal">model_dir</code> and also initializes <code class="literal">uid_lookup_path</code>. The <code class="literal">load</code> method loads a human-readable English name for each softmax node. The <code class="literal">create_graph</code> method creates a graph from the saved <code class="literal">GraphDef</code> file and returns it. The <code class="literal">run_inference_on_image</code> method loads the image from file and creates a graph out of it by invoking the <code class="literal">create_graph</code> method and runs the softmax tensor by feeding the <code class="literal">image_data</code> as input to the graph. The <code class="literal">maybe_download_and_extract</code> method downloads and extracts the modelTAR file.</p><p>The model is first distributed to the workers of the clusters, using Spark's built-in broadcasting mechanism, as <code class="literal">model_data_bc =sc.broadcast(model_data)</code>. The model is loaded on each node and applied to images by invoking the <code class="literal">apply_batch(image_url)</code> method. The <code class="literal">labelled_images = urls.flatMap(apply_batch)</code> line applies the model to each image. Finally, the outcome is returned to the driver using <code class="literal">labelled_images.collect()</code>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec368"></a>There's moreâ€¦</h3></div></div></div><p>Spark is excellent for iterated MapReduce problems, but training neural networks is something different from the MapReduce paradigm. In the <code class="literal">Spark_TensorFlow</code> architecture, downpourSGD is a data-parallel setup, which means each worker has the entire model and is operating on data different from the other workers (data-parallel) instead of having different parts of the model on different machines (model-parallelism). The gradient descent method is taken and is split into two <span class="emphasis"><em>Compute Gradient</em></span>Â followed by <span class="emphasis"><em>Apply Gradients (Descent)</em></span>Â and insert a network boundary between them.</p><p>The Spark workers are computing gradients asynchronously, periodically sending their gradients back to the driver (parameter server), which combines all the worker's gradients and sends the resulting parameters back to the workers as the workers ask for them. The current implementation of Spark-TensorFlow is best for large datasets and small models, since the model size is linearly related to network overhead. Future work will involve model parallelism and model compression to improve performance.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec369"></a>See also</h3></div></div></div><p>Please refer toÂ <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Big Data Analytics with Spark</em></span> to get familiar with Spark. Also, refer to <a class="ulink" href="http://deeplearning.net/tutorial/" target="_blank">http://deeplearning.net/tutorial/</a> and <a class="ulink" href="http://neuralnetworksanddeeplearning.com/" target="_blank">http://neuralnetworksanddeeplearning.com/</a> for details on deep learning.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch10"></a>ChapterÂ 10.Â Working with SparkR</h2></div></div></div><p>In this chapter, we'll cover the following recipes:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Introduction</p></li><li style="list-style-type: disc"><p>Installing R</p></li><li style="list-style-type: disc"><p>Interactive analysis with the SparkR shell</p></li><li style="list-style-type: disc"><p>Creating a SparkR standalone application from RStudio</p></li><li style="list-style-type: disc"><p>Creating SparkR DataFrames</p></li><li style="list-style-type: disc"><p>SparkR DataFrame operations</p></li><li style="list-style-type: disc"><p>Applying user-defined functions in SparkR</p></li><li style="list-style-type: disc"><p>Running SQL queries from SparkR and caching DataFrames</p></li><li style="list-style-type: disc"><p>Machine learning with SparkR</p></li></ul></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec102"></a>Introduction</h2></div></div><hr /></div><p>R is a flexible, open source, and powerful statistical programming language. It is preferred by many professional statisticians and researchers in a variety of fields. It has extensive statistical and graphical capabilities. R combines the aspects of functional and object-oriented programming. One of the key features of R is implicit looping, which yields compact, simple code and frequently leads to faster execution. It provides a command-line interpreted statistical computing environment with built-in scripting language.</p><p>R is an integrated suite of software facilities for data manipulation, calculation, and graphical display. Its key strengths are effective data handling and storage facility, and a collection of tools for data analysis. It provides a number of extensions that support data processing and machine learning tasks. However, interactive analysis in R is limited as the runtime is single-threaded and can only process datasets that fit in a single machine's memory.</p><p>The SparkR project was started in the AMPLab as an effort to explore different techniques to integrate the usability of R with the scalability of Spark. Based on these efforts, the first developer preview was open sourced in January 2014. The project was then developed in the AMPLab for the next year and many performance and usability improvements were made through open source contributions. It is an R package that offers flexibility to use Apache Spark from R. From 1.6.0, SparkR provides distributed DataFrames and the related operations on large datasets and it also supports distributed machine learning using MLlib.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec103"></a>Installing R</h2></div></div><hr /></div><p>In this recipe, we will see how to install R on Linux.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec370"></a>Getting readyâ€¦</h3></div></div></div><p>To step through this recipe, you need Ubuntu 14.04 (Linux flavor) installed on the machine.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec371"></a>How to do itâ€¦</h3></div></div></div><p>Here are the steps in the installation of R:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>The <span class="strong"><strong>Comprehensive R Archive Network</strong></span> (<span class="strong"><strong>CRAN</strong></span>) contains precompiled binary distributions of the base system and contributed packages. It also contains source code for all the platforms. Add the security key as follows:</p><pre class="programlisting">
<span class="strong"><strong>sudo apt-key adv --keyserver
       keyserver.ubuntu.com --recv-keys
       E084DAB9</strong></span>
</pre></li><li><p>Add the CRAN repository to the end of <code class="literal">/etc/apt/sources.list</code>:</p><pre class="programlisting">
<span class="strong"><strong>   deb https://cran.cnr.berkeley.edu/bin/linux/ubuntu trusty/</strong></span>
</pre></li><li><p>Install R as follows:</p><pre class="programlisting">
<span class="strong"><strong>  sudo apt-get update</strong></span>
<span class="strong"><strong>  sudo apt-get install r-base r-base-dev</strong></span>
</pre></li></ol></div><p>This will install R and the recommended packages, and additional packages can be installed using <code class="literal">install.packages("&lt;package&gt;")</code>. The packages on CRAN are updated on a regular basis and the most recent versions will usually be available within a couple of days of their release. The advantage of using the CRAN repository is that older versions of packages are available.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec372"></a>How it worksâ€¦</h3></div></div></div><p>The preceding commands install the R package. If the installation fails with unmet dependencies, install the required dependencies. The instructions for installing R in Debian are similar to Ubuntu. Append the CRAN repository to the Debian list to update the available R version as follows:</p><pre class="programlisting">
<span class="strong"><strong> sudo sh -c 'echo "deb
     http://cran.rstudio.com/bin/linux/debianlenny-cran/"
     &gt;&gt; /etc/apt/sources.list</strong></span>
</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec373"></a>There's moreâ€¦</h3></div></div></div><p>In order to get R running on RHEL 6, we'll need to add an additional repository that allows us to install the new package EPEL. This is done as follows: <code class="literal">su -c 'rpm -Uvh http://download.fedoraproject.org/pub/epel/5/i386/epel-release-5.4.noarch.rpm'</code> and <code class="literal">sudo yum install R</code>. Search for additional R packages in the terminal as follows: <code class="literal">yum list R-\*</code>. We can also install Rstudio on Fedora/RHEL/CentOS as follows: <code class="literal">sudo yum install http://download1.rstudio.org/rstudio-0.97.320-x86_64.rpm</code>. If installation through the terminal fails, download the package from the Rstudio website. Open the file in the Ubuntu software center. Click <span class="strong"><strong>Install</strong></span> and it will be done.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec374"></a>See also</h3></div></div></div><p>Please refer to <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Tricky Statistics with Spark</em></span> and <a class="link" href="#" linkend="ch06">Chapter 6</a>, <span class="emphasis"><em>NLP with Spark</em></span> to learn how to work with Pandas and about using NLTK, OpenNLP on the Spark framework. Similarly, refer to <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Big Data Analytics with Spark</em></span> to get familiar with Spark.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec104"></a>Interactive analysis with the SparkR shell</h2></div></div><hr /></div><p>The entry point into SparkR is the SparkContext which connects the R program to a Spark Cluster. When working with the SparkR shell, SQLContext and SparkContext are already available. SparkR's shell provides a simple way to learn the API, as well as a powerful tool to analyze data interactively.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec375"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark Cluster either in pseudo distributed mode or in one of the distributed modes, that is, standalone, YARN, or Mesos.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec376"></a>How to do itâ€¦</h3></div></div></div><p>In this recipe, weâ€™ll see how to start SparkR interactive shell using Spark 1.6.0:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Start the SparkR shell by running the following in the SparkR package directory:</p><pre class="programlisting">
<span class="strong"><strong>      /bigdata/spark-1.6.0-bin-hadoop2.6$ ./bin/sparkR --master
      spark://192.168.0.118:7077</strong></span>
<span class="strong"><strong>  R version 3.2.3 (2015-12-10) -- "Wooden Christmas-Tree"</strong></span>
<span class="strong"><strong>  Copyright (C) 2015 The R Foundation for Statistical Computing</strong></span>
<span class="strong"><strong>  Platform: x86_64-pc-linux-gnu (64-bit)</strong></span>
<span class="strong"><strong>  R is free software and comes with ABSOLUTELY NO WARRANTY.</strong></span>
<span class="strong"><strong>  You are welcome to redistribute it under certain conditions.    </strong></span>
<span class="strong"><strong>  Type 'license()' or 'licence()' for distribution details.</strong></span>
<span class="strong"><strong>  Natural language support but running in an English locale</strong></span>
<span class="strong"><strong>  R is a collaborative project with many contributors.</strong></span>
<span class="strong"><strong>  Type 'contributors()' for more information and</strong></span>
<span class="strong"><strong>  'citation()' on how to cite R or R packages in publications.</strong></span>
<span class="strong"><strong>  Type 'demo()' for some demos, 'help()' for on-line help, or</strong></span>
<span class="strong"><strong>  'help.start()' for an HTML browser interface to help.</strong></span>
<span class="strong"><strong>  Type 'q()' to quit R.                </strong></span>
<span class="strong"><strong>Launching java with spark-submit command
      /home/padmac/bigdata/spark-1.6.0-bin-hadoop2.6/bin/spark-submit
      "sparkr-shell" /tmp/RtmpKBOXGW/backend_port125f6707fc13   </strong></span>
<span class="strong"><strong>16/03/18 10:04:40 INFO spark.SparkContext: Running Spark version
      1.6.0</strong></span>
<span class="strong"><strong>16/03/18 10:04:40 WARN util.NativeCodeLoader: Unable to load
      native-hadoop library for your platform... using builtin-java
      classes where applicable</strong></span>
<span class="strong"><strong>16/03/18 10:04:41 INFO spark.SecurityManager: Changing view acls
      to: padmac</strong></span>
<span class="strong"><strong>16/03/18 10:04:41 INFO spark.SecurityManager: Changing modify
      acls to: padmac</strong></span>
<span class="strong"><strong>16/03/18 10:04:41 INFO spark.SecurityManager: SecurityManager:
      authentication disabled; ui acls disabled; users with view
      permissions: Set(padmac); users with modify permissions:
      Set(padmac)</strong></span>
<span class="strong"><strong>16/03/18 10:04:41 INFO util.Utils: Successfully started service
      'sparkDriver' on port 54515.</strong></span>
<span class="strong"><strong>16/03/18 10:04:42 INFO slf4j.Slf4jLogger: Slf4jLogger started</strong></span>
<span class="strong"><strong>16/03/18 10:04:42 INFO Remoting: Starting remoting</strong></span>
<span class="strong"><strong>16/03/18 10:04:42 INFO Remoting: Remoting started; listening on
      addresses [akka.tcp://sparkDriverActorSystem@172.16.171.91:48197]</strong></span>
<span class="strong"><strong>16/03/18 10:04:42 INFO util.Utils: Successfully started service
      'sparkDriverActorSystem' on port 48197.</strong></span>
<span class="strong"><strong>16/03/18 10:04:42 INFO spark.SparkEnv: Registering
      MapOutputTracker</strong></span>
<span class="strong"><strong>16/03/18 10:04:42 INFO spark.SparkEnv: Registering
      BlockManagerMaster</strong></span>
<span class="strong"><strong>16/03/18 10:04:42 INFO storage.DiskBlockManager: Created local
      directory at /tmp/blockmgr-f178c98f-125f-4607-8748-a80e5d1e5a08</strong></span>
<span class="strong"><strong>16/03/18 10:04:42 INFO storage.MemoryStore: MemoryStore started
      with capacity 511.5 MB</strong></span>
<span class="strong"><strong>16/03/18 10:04:42 INFO spark.SparkEnv: Registering
      OutputCommitCoordinator</strong></span>
<span class="strong"><strong>16/03/18 10:04:42 INFO server.Server: jetty-8.y.z-SNAPSHOT</strong></span>
<span class="strong"><strong>16/03/18 10:04:42 INFO server.AbstractConnector: Started
      SelectChannelConnector@0.0.0.0:4040</strong></span>
<span class="strong"><strong>16/03/18 10:04:42 INFO util.Utils: Successfully started service
      'SparkUI' on port 4040.</strong></span>
<span class="strong"><strong>16/03/18 10:04:42 INFO ui.SparkUI: Started SparkUI at
      http://172.16.171.91:4040</strong></span>
<span class="strong"><strong>16/03/18 10:04:43 INFO executor.Executor: Starting executor ID
      driver on host localhost</strong></span>
<span class="strong"><strong>16/03/18 10:04:43 INFO util.Utils: Successfully started service
     'org.apache.spark.network.netty.NettyBlockTransferService' on port
      36055.</strong></span>
<span class="strong"><strong>16/03/18 10:04:43 INFO netty.NettyBlockTransferService: Server
      created on 36055</strong></span>
<span class="strong"><strong>16/03/18 10:04:43 INFO storage.BlockManagerMaster: Trying to
      register BlockManager</strong></span>
<span class="strong"><strong>16/03/18 10:04:43 INFO storage.BlockManagerMasterEndpoint:
      Registering block manager localhost:36055 with 511.5 MB RAM,
      BlockManagerId(driver, localhost, 36055)</strong></span>
<span class="strong"><strong>16/03/18 10:04:43 INFO storage.BlockManagerMaster: Registered
      BlockManager</strong></span>
<span class="strong"><strong>Welcome to
         ____              __
        / __/__  ___ _____/ /__
       _\ \/ _ \/ _ `/ __/  '_/
      /__ / .__/\_,_/_/ /_/\_\   version 1.6.0
         /_/</strong></span>
</pre><p>Spark context is available as <code class="literal">sc</code>, SQL context is available as <code class="literal">sqlContext</code></p></li><li><p>Spark's primary abstraction is a distributed collection of items called a <span class="strong"><strong>Resilient Distributed Dataset</strong></span> (<span class="strong"><strong>RDD</strong></span>). Let's make a new RDD from the text of the readme file in the <code class="literal">SparkR-pkg</code> source directory as follows:</p><pre class="programlisting">
<span class="strong"><strong>  &gt; textfile = SparkR:::textFile(sc, "~/&lt;path to spark&gt;/spark-
      1.6.0-bin-  hadoop2.6/README.md")</strong></span>
<span class="strong"><strong>  &gt; count(textfile)</strong></span>
<span class="strong"><strong>  [1] 95</strong></span>
<span class="strong"><strong>  &gt; take(textfile,1)</strong></span>
<span class="strong"><strong>  [[1]]</strong></span>
<span class="strong"><strong>  [1] "# Apache Spark"</strong></span>
</pre></li><li><p>We will use the <code class="literal">filterRDD</code> transformation to return a new RDD with a subset of the items in the file as follows:</p><pre class="programlisting">
<span class="strong"><strong>  &gt; linesWithSpark &lt;- filterRDD(textFile, function(line){
      grepl("Spark",   line)})</strong></span>
<span class="strong"><strong>  &gt; count(linesWithSpark)</strong></span>
<span class="strong"><strong>  [1] 17</strong></span>
</pre></li><li><p>We can chain together transformations and actions as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt;count(SparkR:::filterRDD(textfile, function(line){
        grepl("Spark", line)}))</strong></span>
<span class="strong"><strong>16/03/18 11:02:39 INFO SparkContext: Starting job: collect at
       NativeMethodAccessorImpl.java:-2</strong></span>
<span class="strong"><strong>16/03/18 11:02:39 INFO DAGScheduler: Got job 3 (collect at
       NativeMethodAccessorImpl.java:-2) with 2 output partitions</strong></span>
<span class="strong"><strong>16/03/18 11:02:39 INFO DAGScheduler: Final stage: ResultStage 3
       (collect at NativeMethodAccessorImpl.java:-2)</strong></span>
<span class="strong"><strong>16/03/18 11:02:39 INFO DAGScheduler: Parents of final stage:
       List()</strong></span>
<span class="strong"><strong>16/03/18 11:02:39 INFO DAGScheduler: Missing parents: List()</strong></span>
<span class="strong"><strong>16/03/18 11:02:39 INFO DAGScheduler: Submitting ResultStage 3
       (RRDD[4] at RDD at RRDD.scala:36), which has no missing parents</strong></span>
<span class="strong"><strong>16/03/18 11:02:39 INFO MemoryStore: Block broadcast_4 stored as
       values in memory (estimated size 16.1 KB, free 216.9 KB)</strong></span>
<span class="strong"><strong>16/03/18 11:02:39 INFO MemoryStore: Block broadcast_4_piece0
       stored as bytes in memory (estimated size 5.2 KB, free 222.1 KB)</strong></span>
<span class="strong"><strong>16/03/18 11:02:39 INFO BlockManagerInfo: Added
       broadcast_4_piece0 in memory on localhost:46499 (size: 5.2 KB,
       free: 511.5 MB)</strong></span>
<span class="strong"><strong>16/03/18 11:02:39 INFO SparkContext: Created broadcast 4 from
       broadcast at DAGScheduler.scala:1006</strong></span>
<span class="strong"><strong>16/03/18 11:02:39 INFO DAGScheduler: Submitting 2 missing tasks
       from ResultStage 3 (RRDD[4] at RDD at RRDD.scala:36)</strong></span>
<span class="strong"><strong>16/03/18 11:02:39 INFO TaskSchedulerImpl: Adding task set 3.0
       with 2 tasks16/03/18 11:02:39 INFO TaskSetManager: Starting task
       0.0 in stage 3.0 (TID 5, localhost, partition 0,PROCESS_LOCAL,
       2163 bytes)</strong></span>
<span class="strong"><strong>16/03/18 11:02:39 INFO TaskSetManager: Starting task 1.0 in
       stage
       3.0 (TID 6, localhost, partition 1,PROCESS_LOCAL, 2163 bytes)</strong></span>
<span class="strong"><strong>16/03/18 11:02:39 INFO Executor: Running task 0.0 in stage 3.0
       (TID 5)</strong></span>
<span class="strong"><strong>16/03/18 11:02:39 INFO Executor: Running task 1.0 in stage 3.0
       (TID 6)</strong></span>
<span class="strong"><strong>16/03/18 11:02:39 INFO HadoopRDD: Input split:
       file:/home/padmac/bigdata/spark-1.6.0-bin-
       hadoop2.6/README.md:0+1679</strong></span>
<span class="strong"><strong>16/03/18 11:02:39 INFO HadoopRDD: Input split:
       file:/home/padmac/bigdata/spark-1.6.0-bin-
       hadoop2.6/README.md:1679+1680</strong></span>
<span class="strong"><strong>16/03/18 11:02:39 INFO RRDD: Times: boot = 0.011 s, init = 0.004
       s, broadcast = 0.001 s, read-input = 0.000 s, compute = 0.002
       s, write-output = 0.001 s, total = 0.019 s</strong></span>
<span class="strong"><strong>16/03/18 11:02:39 INFO Executor: Finished task 1.0 in stage 3.0
       (TID 6). 2077 bytes result sent to driver</strong></span>
<span class="strong"><strong>16/03/18 11:02:39 INFO TaskSetManager: Finished task 1.0 in
       stage
       3.0 (TID 6) in 32 ms on localhost (1/2)</strong></span>
<span class="strong"><strong>16/03/18 11:02:39 INFO RRDD: Times: boot = 0.010 s, init = 0.011
       s, broadcast = 0.000 s, read-input = 0.000 s, compute = 0.003 s,
       write-output = 0.001 s, total = 0.025 s</strong></span>
<span class="strong"><strong>16/03/18 11:02:39 INFO Executor: Finished task 0.0 in stage 3.0
       (TID 5). 2077 bytes result sent to driver</strong></span>
<span class="strong"><strong>16/03/18 11:02:39 INFO TaskSetManager: Finished task 0.0 in
       stage
       3.0 (TID 5) in 40 ms on localhost (2/2)</strong></span>
<span class="strong"><strong>16/03/18 11:02:39 INFO TaskSchedulerImpl: Removed TaskSet 3.0,
       whose tasks have all completed, from pool </strong></span>
<span class="strong"><strong>16/03/18 11:02:39 INFO DAGScheduler: ResultStage 3 (collect at
       NativeMethodAccessorImpl.java:-2) finished in 0.042 s</strong></span>
<span class="strong"><strong>16/03/18 11:02:39 INFO DAGScheduler: Job 3 finished: collect at
       NativeMethodAccessorImpl.java:-2, took 0.052781 s</strong></span>
<span class="strong"><strong>[1] 17</strong></span>
</pre></li><li><p>RDD actions and transformations can be used for more complex computations. Let's say we want to find the line with the most words:</p><pre class="programlisting">
<span class="strong"><strong>&gt; SparkR:::reduce( SparkR:::lapply( textfile, function(line) {
        length(strsplit(unlist(line), " ")[[1]])}), function(a, b) { if
        (a &gt; b) { a } else { b }})</strong></span>
<span class="strong"><strong>[1] 14</strong></span>
</pre></li><li><p>We can define a max function to make this code easier to understand:</p><pre class="programlisting">
<span class="strong"><strong>&gt;  max &lt;- function(a, b) {if (a &gt; b) { a } else { b }}</strong></span>
<span class="strong"><strong>&gt; SparkR:::reduce(SparkR:::map(textfile, function(line) {
       length(strsplit(unlist(line), " ")[[1]])}), max)</strong></span>
<span class="strong"><strong>[1] 14</strong></span>
</pre></li><li><p>Now, let's define the word count program using the <code class="literal">flatMap</code>, <code class="literal">lapply</code>, and <code class="literal">reduceByKey</code> transformations as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; words &lt;- SparkR:::flatMap(textfile, </strong></span>
<span class="strong"><strong> function(line){</strong></span>
<span class="strong"><strong>   strsplit(line," ")[[1]]  })</strong></span>
<span class="strong"><strong>wordCount &lt;- SparkR:::lapply(words, function(word){
       list(word,   1L)
       })</strong></span>
<span class="strong"><strong>counts &lt;- SparkR:::reduceByKey(wordCount, "+", 2L)</strong></span>
</pre></li><li><p>To collect the word counts in our shell, we can use the collect action:</p><pre class="programlisting">
<span class="strong"><strong>output &lt;- collect(counts)</strong></span>
<span class="strong"><strong>&gt; for (wordcount in output) {</strong></span>
<span class="strong"><strong>   cat(wordcount[[1]], ": ", wordcount[[2]], "\n")</strong></span>
<span class="strong"><strong>   }</strong></span>
<span class="strong"><strong>how :  2 </strong></span>
<span class="strong"><strong>Thriftserver :  1 </strong></span>
<span class="strong"><strong>detailed :  2 </strong></span>
<span class="strong"><strong>its :  1 </strong></span>
<span class="strong"><strong>other :  1 </strong></span>
<span class="strong"><strong>Alternatively, :  1 </strong></span>
<span class="strong"><strong>refer :  2 </strong></span>
<span class="strong"><strong>"yarn" :  1 </strong></span>
<span class="strong"><strong>runs. :  1 </strong></span>
<span class="strong"><strong>start :  1 </strong></span>
<span class="strong"><strong>[...]</strong></span>
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec377"></a>How it worksâ€¦</h3></div></div></div><p>Spark RDDs can be created from Hadoop <code class="literal">InputFormats</code> (HDFS files) or by transforming other RDDs. <code class="literal">SparkR:::textFile</code> makes a new RDD from the text of the README file in the SparkR-package source directory. The actions such as count and take return values. The <code class="literal">filterRDD</code> transformation returns a new RDD with a subset of the items in the file. We have also seen how we chain transformations and actions together. The complex computation <code class="literal">SparkR:::reduce( SparkR:::lapply( textfile, function(line) { length(strsplit(unlist(line), " ")[[1]])}), function(a, b) { if (a &gt; b) { a } else { b }})</code> works as the inner function <code class="literal">lapply</code> maps a line to an integer value, creating a new RDD. The outer function reduce is invoked on the new RDD to find the largest line count. In this case, the arguments to both functions are passed as anonymous functions. We also defined R functions such as max beforehand and passed them as arguments to RDD functions.</p><p>When performing a word count, we combined the <code class="literal">flatMap</code>, <code class="literal">lapply</code>, and <code class="literal">reduceByKey</code> transformations to compute the per-word counts in the file as an RDD of (<code class="literal">string,int</code>) pairs. We used the collect action to get the word counts.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec378"></a>There's moreâ€¦</h3></div></div></div><p>The preceding code snippet shows the use of the SparkR shell for interactive data analysis. We can also write and execute a standalone application in SparkR, which is shown in the coming recipes. The central component in SparkR is the SparkR DataFrame, a distributed DataFrame implemented on top of Spark. The SparkR DataFrames API is similar to R DataFrames but it can scale to large datasets using support for distributed computation in Spark. There is also support for caching datasets, which allows the data to be accessed repeatedly. The upcoming release of SparkR will offer extensive support for running high-level machine learning algorithms and SQL queries over SparkR DataFrames.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec379"></a>See also</h3></div></div></div><p>Please refer to <span class="emphasis"><em><a class="link" href="#" linkend="ch01">Chapter 1</a></em></span>, <span class="emphasis"><em>Big Data Analytics with Spark</em></span> to get familiar with Spark.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec105"></a>Creating a SparkR standalone application from RStudio</h2></div></div><hr /></div><p>In this recipe, we'll look at the process of writing and executing a standalone application in SparkR.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec380"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark Cluster either in pseudo distributed mode or in one of the distributed modes, that is, standalone, YARN, or Mesos. Also, install RStudio. Please refer to the <span class="emphasis"><em>Installing R</em></span> recipe for details on the installation of R.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec381"></a>How to do itâ€¦</h3></div></div></div><p>In this recipe, we'll create standalone application using Spark-1.6.0 and Spark-2.0.2:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Before working with SparkR, make sure that <code class="literal">SPARK_HOME</code> is set in environment as follows:</p><pre class="programlisting">
<span class="strong"><strong>   if (nchar(Sys.getenv("SPARK_HOME")) &lt; 1) {</strong></span>
<span class="strong"><strong>   Sys.setenv(SPARK_HOME = "/home/padmac/bigdata/spark-1.6.0-bin-
       hadoop2.6")</strong></span>
<span class="strong"><strong>   }</strong></span>
</pre></li><li><p>Now, load the <code class="literal">SparkR</code> package and invoke <code class="literal">sparkR.init</code> as follows:</p><pre class="programlisting">
<span class="strong"><strong>  library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"),
      "R", "lib")))</strong></span>
<span class="strong"><strong>  sc &lt;- sparkR.init(master = "spark://192.168.0.118:7077",
      sparkEnvir =   list(spark.driver.memory="2g"))</strong></span>
</pre></li><li><p>Here is the word count example written in a standalone application:</p><pre class="programlisting">
<span class="strong"><strong>  lines &lt;- SparkR:::textFile(sc,
      "hdfs://&lt;namenode&gt;:9000/data/README.md")</strong></span>
<span class="strong"><strong>  words &lt;- SparkR:::flatMap(lines, function(line){</strong></span>
<span class="strong"><strong>    strsplit(line," ")[[1]]</strong></span>
<span class="strong"><strong>  })</strong></span>
<span class="strong"><strong>  wordCount &lt;- SparkR:::lapply(words, function(word){</strong></span>
<span class="strong"><strong>    list(word,1L)</strong></span>
<span class="strong"><strong>  })</strong></span>
<span class="strong"><strong>  counts &lt;- SparkR:::reduceByKey(wordCount,"+",numPartitions = 2)</strong></span>
<span class="strong"><strong>  output &lt;- collect(counts)</strong></span>
<span class="strong"><strong>  for (wordcount in output) {</strong></span>
<span class="strong"><strong>    cat(wordcount[[1]], ": ", wordcount[[2]], "\n")</strong></span>
<span class="strong"><strong>  }</strong></span>
<span class="strong"><strong>  how :  2 </strong></span>
<span class="strong"><strong>  Thriftserver :  1 </strong></span>
<span class="strong"><strong>  detailed :  2 </strong></span>
<span class="strong"><strong>  its :  1 </strong></span>
<span class="strong"><strong>  other :  1 </strong></span>
<span class="strong"><strong>  Alternatively, :  1 </strong></span>
<span class="strong"><strong>  refer :  2 </strong></span>
<span class="strong"><strong>  "yarn" :  1 </strong></span>
<span class="strong"><strong>  runs. :  1 </strong></span>
<span class="strong"><strong>  start :  1 </strong></span>
<span class="strong"><strong>  [...]</strong></span>
</pre></li><li><p>Here is another example:</p><pre class="programlisting">
<span class="strong"><strong>  SampleApp.R</strong></span>
<span class="strong"><strong>  if (nchar(Sys.getenv("SPARK_HOME")) &lt; 1) {</strong></span>
<span class="strong"><strong>    Sys.setenv(SPARK_HOME = "/home/padmac/bigdata/spark-1.6.0-bin-
      hadoop2.6")</strong></span>
<span class="strong"><strong>  }</strong></span>
<span class="strong"><strong>  library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"),
      "R", "lib")))</strong></span>
<span class="strong"><strong>  sc &lt;- sparkR.init(master = "spark://192.168.0.118:7077",
      sparkEnvir =   list(spark.driver.memory="2g"))</strong></span>
<span class="strong"><strong>  logFile &lt;- "hdfs://&lt;namenode&gt;:9000/&lt;path&gt;/README.md"</strong></span>
<span class="strong"><strong>  logData &lt;- cache(SparkR:::textFile(sc, logFile))</strong></span>
<span class="strong"><strong>  numAs &lt;- count(SparkR:::filterRDD(logData, function(s) {
      grepl("a",   s) }))</strong></span>
<span class="strong"><strong>  numBs &lt;- count(SparkR:::filterRDD(logData, function(s) {
      grepl("b",   s) }))</strong></span>
<span class="strong"><strong>  paste("Lines with a: ", numAs, ", Lines with b: ", numBs, sep="")</strong></span>
<span class="strong"><strong>  [1] "Lines with a: 58, Lines with b: 26"</strong></span>
</pre></li><li><p>Let's see how to create a standalone using Spark 2.0.2. Here is the code for the same -</p><pre class="programlisting">
<span class="strong"><strong>       if (nchar(Sys.getenv("SPARK_HOME")) &lt; 1) {</strong></span>
<span class="strong"><strong>       Sys.setenv(SPARK_HOME = "/home/padmac/bigdata/spark-2.0.2-bin-
       hadoop2.6")</strong></span>
<span class="strong"><strong>       }</strong></span>
<span class="strong"><strong>       library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"),
       "R", "lib")))</strong></span>
<span class="strong"><strong>       sparkR.session(master = "spark://master:7077", sparkConfig =
       list(spark.driver.memory = "2g"))</strong></span>
</pre><p>The following is the output:</p><pre class="programlisting">
<span class="strong"><strong>Attaching package: â€˜SparkRâ€™</strong></span>
<span class="strong"><strong>The following objects are masked from â€˜package:statsâ€™:cov, filter, lag, na.omit, predict, sd, var, window</strong></span>
<span class="strong"><strong>The following objects are masked from â€˜package:baseâ€™:</strong></span>
<span class="strong"><strong>as.data.frame, colnames, colnames&lt;-, drop, endsWith, intersect, rank, rbind, sample, startsWith, subset, summary, transform, union</strong></span>
<span class="strong"><strong>&gt; if (nchar(Sys.getenv("SPARK_HOME")) &lt; 1) {</strong></span>
<span class="strong"><strong>+ Sys.setenv(SPARK_HOME = "/home/padmac/bigdata/spark-2.0.2-bin-hadoop2.6")</strong></span>
<span class="strong"><strong>+ }</strong></span>
<span class="strong"><strong>&gt; library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"), "R","lib")))</strong></span>
<span class="strong"><strong>&gt; sparkR.session(master = "spark://master:7077", sparkConfig = list(spark.driver.memory = "2g"))</strong></span>
<span class="strong"><strong>Spark package found in SPARK_HOME: /home/padmac/bigdata/spark-2.0.2-bin-hadoop2.6</strong></span>
<span class="strong"><strong>Launching java with spark-submit command /home/padmac/bigdata/spark-2.0.2-bin- hadoop2.6/bin/spark-submit --driver-memory "2g" sparkr-shell /tmp/Rtmpg7HYyp/backend_port258d105c19c0 </strong></span>
<span class="strong"><strong>Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties</strong></span>
<span class="strong"><strong>Setting default log level to "WARN".</strong></span>
<span class="strong"><strong>To adjust logging level use sc.setLogLevel(newLevel).</strong></span>
<span class="strong"><strong>16/11/20 21:34:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</strong></span>
<span class="strong"><strong>16/11/20 21:34:09 WARN Utils: Your hostname, F01022 resolves to a loopback address:127.0.0.1; using 192.168.0.5 instead (on interface wlan3)</strong></span>
<span class="strong"><strong>16/11/20 21:34:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address</strong></span>
<span class="strong"><strong>Java ref type org.apache.spark.sql.SparkSession id 1</strong></span>
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec382"></a>How it worksâ€¦</h3></div></div></div><p>The preceding code snippet shows how to write a standalone application in <code class="literal">SparkR</code>. Initially, <code class="literal">Sys.getenv</code> checks whether <code class="literal">SPARK_HOME</code> is set in the environment. The module <code class="literal">library</code> is used to load the <code class="literal">SparkR</code> package. <code class="literal">sparkR.init</code> is used to initialize the <code class="literal">SparkContext</code>. It also takes Spark driver properties as parameters using <code class="literal">sparkEnvir</code>. We combined <code class="literal">flatMap</code>, <code class="literal">lapply</code>, and <code class="literal">reduceByKey</code> transformations to compute the per word counts in the file.</p><p>The program <code class="literal">SampleApp.R</code> counts the number of lines containing <code class="literal">a</code> and the number containing <code class="literal">b</code> in a text file and returns the counts as a string on the command line. We can pass R functions to Spark, where they are automatically serialized along with any variables they reference.</p><p>We also saw how to create standalone application using Spark 2.0.2. The <code class="literal">sparkR.session(master = "spark://master:7077", sparkConfig = list(spark.driver.memory = "2g"))</code> line creates Spark Session which is used in operations such as creating DataFrames from local DataFrames, various data sources such as CSV, JSON, Parquet and also from Hive.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec383"></a>There's moreâ€¦</h3></div></div></div><p>There is also support for caching datasets, which allows the data to be accessed repeatedly. Similar to R DataFrames, SparkR provides distributed DataFrames which can be constructed from a variety of sources, such as structured data files, Hive tables, databases, and R DataFrames. In the coming recipes, we'll see how to work with DataFrames created from a variety of data sources.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec384"></a>See also</h3></div></div></div><p>Please refer to <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Big Data Analytics with Spark</em></span> to get familiar with Spark. Also, see the <span class="emphasis"><em>Installing R</em></span> recipe to learn the installation details of R.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec106"></a>Creating SparkR DataFrames</h2></div></div><hr /></div><p>A DataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a DataFrame in R, but with rich optimizations. SparkR DataFrames scale to large datasets using the support for distributed computation in Spark. In this recipe, we'll see how to create SparkR DataFrames from different sources, such as JSON, CSV, local R DataFrames, and Hive tables.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec385"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark Cluster either in pseudo distributed mode or in one of the distributed modes, that is, standalone, YARN, or Mesos. Also, install RStudio. Please refer to the <span class="emphasis"><em>Installing R</em></span> recipe for details on the installation of R. Please refer to the <span class="emphasis"><em>Creating a SparkR standalone application from Rstudio</em></span> recipe for details on working with the SparkR package.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec386"></a>How to do itâ€¦</h3></div></div></div><p>In this recipe, we'll see how to create SparkR data frames in Spark 1.6.0 as well as Spark 2.0.2:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Use <code class="literal">createDataFrame</code> to pass a local R DataFrame and create a SparkR DataFrame:</p><pre class="programlisting">
<span class="strong"><strong>    CreateSparkRDataFrame.R</strong></span>
<span class="strong"><strong>    if (nchar(Sys.getenv("SPARK_HOME")) &lt; 1) {</strong></span>
<span class="strong"><strong>    Sys.setenv(SPARK_HOME = "/home/padmac/bigdata/spark-1.6.0-bin-
        hadoop2.6")</strong></span>
<span class="strong"><strong>    }</strong></span>
<span class="strong"><strong>    library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"),
        "R", "lib")))</strong></span>
<span class="strong"><strong>    sc &lt;- sparkR.init(master = "local[*]", sparkEnvir =
        list(spark.driver.memory="2g"))</strong></span>
<span class="strong"><strong>    sqlContext &lt;- sparkRSQL.init(sc)</strong></span>
<span class="strong"><strong>    df &lt;- createDataFrame(sqlContext, iris)</strong></span>
<span class="strong"><strong>    head(df)</strong></span>
</pre><p>The following is the output:</p><pre class="programlisting">
<span class="strong"><strong>  16/03/19 15:20:14 INFO Executor: Finished task 0.0 in stage 4.0
      (TID 5). 1794   bytes result sent to driver</strong></span>
<span class="strong"><strong>  16/03/19 15:20:14 INFO TaskSetManager: Finished task 0.0 in stage
      4.0 (TID 5) in   765 ms on localhost (1/1)</strong></span>
<span class="strong"><strong>  16/03/19 15:20:14 INFO DAGScheduler: ResultStage 4 (dfToCols at
      NativeMethodAccessorImpl.java:-2) finished in 0.765 s</strong></span>
<span class="strong"><strong>  16/03/19 15:20:14 INFO TaskSchedulerImpl: Removed TaskSet 4.0,
      whose tasks have   all completed, from pool </strong></span>
<span class="strong"><strong>  16/03/19 15:20:14 INFO DAGScheduler: Job 4 finished: dfToCols at
      NativeMethodAccessorImpl.java:-2, took 0.781704 s</strong></span>
<span class="strong"><strong>    Sepal_Length Sepal_Width Petal_Length Petal_Width Species</strong></span>
<span class="strong"><strong>  1          5.1         3.5          1.4         0.2  setosa</strong></span>
<span class="strong"><strong>  2          4.9         3.0          1.4         0.2  setosa</strong></span>
<span class="strong"><strong>  3          4.7         3.2          1.3         0.2  setosa</strong></span>
<span class="strong"><strong>  4          4.6         3.1          1.5         0.2  setosa</strong></span>
<span class="strong"><strong>  5          5.0         3.6          1.4         0.2  setosa</strong></span>
<span class="strong"><strong>  6          5.4         3.9          1.7         0.4  setosa</strong></span>
</pre></li><li><p>Now, create a DataFrame from a JSON file. The sample JSON file <code class="literal">people.json</code> contains the following content:</p><pre class="programlisting">
<span class="strong"><strong>  {"name":"Michael"}</strong></span>
<span class="strong"><strong>  {"name":"Andy", "age":30}</strong></span>
<span class="strong"><strong>  {"name":"Justin", "age":19}</strong></span>
</pre></li><li><p>Here is the code snippet for creating a DataFrame from JSON content forÂ <code class="literal">ReadingJSON.R</code>:</p><pre class="programlisting">
<span class="strong"><strong>  if (nchar(Sys.getenv("SPARK_HOME")) &lt; 1) {</strong></span>
<span class="strong"><strong>    Sys.setenv(SPARK_HOME = "~/bigdata/spark-1.6.0-bin-hadoop2.6")</strong></span>
<span class="strong"><strong>  }</strong></span>
<span class="strong"><strong>  library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"),
      "R", "lib")))</strong></span>
<span class="strong"><strong>  sc &lt;- sparkR.init(master = "local[*]", sparkEnvir =
      list(spark.driver.memory="2g"))</strong></span>
<span class="strong"><strong>  sqlContext &lt;- sparkRSQL.init(sc)</strong></span>
<span class="strong"><strong>  people &lt;- read.df(sqlContext, "~/&lt;path&gt;/people.json","json")</strong></span>
<span class="strong"><strong>  head(people)</strong></span>
</pre><p>The following is the output:</p><pre class="programlisting">
<span class="strong"><strong>Launching java with spark-submit command
      /home/padmac/bigdata/spark-1.6.0-bin-  hadoop2.6/bin/spark-submit
      --driver-memory "2g" sparkr-shell
      /tmp/Rtmphk0btR/backend_portad92e8be16e </strong></span>
<span class="strong"><strong>  log4j:WARN No appenders could be found for logger
      (io.netty.util.internal.logging.InternalLoggerFactory).</strong></span>
<span class="strong"><strong>  log4j:WARN Please initialize the log4j system properly.</strong></span>
<span class="strong"><strong>  log4j:WARN See
      http://logging.apache.org/log4j/1.2/faq.html#noconfig for more
      info.</strong></span>
<span class="strong"><strong>  Using Spark's default log4j profile: org/apache/spark/log4j-
      defaults.properties</strong></span>
<span class="strong"><strong>  16/03/19 15:09:17 INFO SparkContext: Running Spark version 1.6.0</strong></span>
<span class="strong"><strong>  16/03/19 15:09:17 WARN NativeCodeLoader: Unable to load native-
      hadoop library for   your platform... using builtin-java classes
      where applicable</strong></span>
<span class="strong"><strong>  [...]</strong></span>
<span class="strong"><strong>  &gt; head(people)</strong></span>
<span class="strong"><strong>  16/03/19 15:09:21 INFO MemoryStore: Block broadcast_2 stored as
      values in memory (estimated size 86.4 KB, free 338.3 KB)</strong></span>
<span class="strong"><strong>  16/03/19 15:09:21 INFO MemoryStore: Block broadcast_2_piece0
      stored as bytes in memory (estimated size 19.3 KB, free 357.6 KB)</strong></span>
<span class="strong"><strong>  16/03/19 15:09:21 INFO BlockManagerInfo: Added broadcast_2_piece0
      in memory on localhost:55348 (size: 19.3 KB, free: 1247.6 MB)</strong></span>
<span class="strong"><strong>  16/03/19 15:09:21 INFO SparkContext: Created broadcast 2 from
      dfToCols at NativeMethodAccessorImpl.java:-2</strong></span>
<span class="strong"><strong>  16/03/19 15:09:22 INFO MemoryStore: Block broadcast_3 stored as
      values in memory (estimated size 225.7 KB, free 583.3 KB)</strong></span>
<span class="strong"><strong>  16/03/19 15:09:22 INFO MemoryStore: Block broadcast_3_piece0
      stored as bytes in memory (estimated size 19.4 KB, free 602.8 KB)</strong></span>
<span class="strong"><strong>  16/03/19 15:09:22 INFO BlockManagerInfo: Added broadcast_3_piece0
      in memory on localhost:55348 (size: 19.4 KB, free: 1247.6 MB)</strong></span>
<span class="strong"><strong>  16/03/19 15:09:22 INFO SparkContext: Created broadcast 3 from
      dfToCols at NativeMethodAccessorImpl.java:-2</strong></span>
<span class="strong"><strong>  16/03/19 15:09:22 INFO FileInputFormat: Total input paths to
      process : 1</strong></span>
<span class="strong"><strong>  16/03/19 15:09:22 INFO SparkContext: Starting job: dfToCols at
      NativeMethodAccessorImpl.java:-2</strong></span>
<span class="strong"><strong>  16/03/19 15:09:22 INFO DAGScheduler: Got job 1 (dfToCols at
      NativeMethodAccessorImpl.java:-2) with 1 output partitions</strong></span>
<span class="strong"><strong>  16/03/19 15:09:22 INFO DAGScheduler: Final stage: ResultStage 1
      (dfToCols at NativeMethodAccessorImpl.java:-2)</strong></span>
<span class="strong"><strong>  16/03/19 15:09:22 INFO DAGScheduler: Parents of final stage:
      List()</strong></span>
<span class="strong"><strong>  16/03/19 15:09:22 INFO DAGScheduler: Missing parents: List()</strong></span>
<span class="strong"><strong>  16/03/19 15:09:22 INFO DAGScheduler: Submitting ResultStage 1
      (MapPartitionsRDD[9] at dfToCols at
      NativeMethodAccessorImpl.java:-2), which has no missing parents</strong></span>
<span class="strong"><strong>  [...]</strong></span>
<span class="strong"><strong>    age    name</strong></span>
<span class="strong"><strong>  1  NA Michael</strong></span>
<span class="strong"><strong>  2  30    Andy</strong></span>
<span class="strong"><strong>  3  19  Justin</strong></span>
</pre></li><li><p>Let's create SparkR DataFrames from Hive tables. The <code class="literal">students.txt</code> file contains the following content:</p><pre class="programlisting">
<span class="strong"><strong>  101  padma</strong></span>
<span class="strong"><strong>  102  priya</strong></span>
<span class="strong"><strong>  103  chitturi</strong></span>
</pre></li><li><p>Here is the code snippet for creating DataFrames from Hive tables:</p><pre class="programlisting">
<span class="strong"><strong>   CreatingDFHive.R</strong></span>
<span class="strong"><strong>   if (nchar(Sys.getenv("SPARK_HOME")) &lt; 1) {</strong></span>
<span class="strong"><strong>   Sys.setenv(SPARK_HOME = "/home/padmac/bigdata/spark-1.6.0-bin-
       hadoop2.6")</strong></span>
<span class="strong"><strong>    }</strong></span>
<span class="strong"><strong>   library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"),
       "R", "lib")))</strong></span>
<span class="strong"><strong>   sc &lt;- sparkR.init(master = "local[*]", sparkEnvir =
       list(spark.driver.memory="2g"))</strong></span>
<span class="strong"><strong>  hiveContext &lt;- sparkRHive.init(sc)</strong></span>
<span class="strong"><strong>   sql(hiveContext, "CREATE TABLE IF NOT EXISTS Student (id INT,
       name STRING)")</strong></span>
<span class="strong"><strong>   sql(hiveContext, "LOAD DATA LOCAL INPATH '~/&lt;path&gt;/students.txt'
       INTO TABLE Student")</strong></span>
<span class="strong"><strong>   results &lt;- sql(hiveContext, "FROM src SELECT key, value")</strong></span>
<span class="strong"><strong>   head(results)</strong></span>
</pre><p>The following is the output:</p><pre class="programlisting">
<span class="strong"><strong>    id     name</strong></span>
<span class="strong"><strong>  101    padma</strong></span>
<span class="strong"><strong>  102  priya</strong></span>
<span class="strong"><strong>  103  chitturi  </strong></span>
</pre></li><li><p>Let's try to read a CSV file as follows:</p><pre class="programlisting">
<span class="strong"><strong>  library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"),
      "R", "lib")))</strong></span>
<span class="strong"><strong>  sc &lt;- sparkR.init(master = "local[*]", sparkEnvir =
      list(spark.driver.memory="2g"),
      sparkPackages="com.databricks:spark-  csv_2.10:1.4.0")</strong></span>
<span class="strong"><strong>  sqlContext &lt;- sparkRSQL.init(sc)</strong></span>
<span class="strong"><strong>  people &lt;- read.df(sqlContext, "~/&lt;path&gt;/people.csv", "csv")</strong></span>
<span class="strong"><strong>  head(people)</strong></span>
</pre><p>The following is the output:</p><pre class="programlisting">
<span class="strong"><strong>  age    name</strong></span>
<span class="strong"><strong>  1  NA      Michael</strong></span>
<span class="strong"><strong>  2  30     Andy</strong></span>
<span class="strong"><strong>  3  19     Justin</strong></span>
</pre></li><li><p>Now, let's see how to create SparkDataFrame using Spark 2.0.2 in the following ways:</p><pre class="programlisting">
<span class="strong"><strong>     #Create SparkDateFrame from a local data frame
     df &lt;- as.DataFrame(faithful)
     head(df)
     #From Data Sources
     people &lt;- read.df("/home/padmac/bigdata/spark-2.0.2-bin-</strong></span>
<span class="strong"><strong>     hadoop2.6/examples/src/main/resources/people.json","json")
     head(people)
     printSchema(people)</strong></span>
</pre><p>The following is the output:</p><pre class="programlisting">
<span class="strong"><strong>       &gt; #Create SparkDateFrame from a local data frame
       &gt; df &lt;- as.DataFrame(faithful)
       &gt; head(df)
       eruptions waiting
       1 3.600 79
       2 1.800 54
       3 3.333 74
       4 2.283 62
       5 4.533 85
       6 2.883 55
       &gt;
       &gt; #From Data Sources
       &gt; people &lt;- read.df("/home/padmac/bigdata/spark-2.0.2- </strong></span>
<span class="strong"><strong>         binhadoop2.6/examples/src/main/resources/people.json","json")
       &gt; head(people)

       age name
       1 NA Michael
       2 30 Andy
       3 19 Justin
       &gt; printSchema(people)
        root
       |-- age: long (nullable = true)
       |-- name: string ( nullable = true)</strong></span>
</pre></li><li><p>Let's also see how to create SparkDataFrame from csv and parquet files as below. Please download the csv file from the location: <a class="ulink" href="https://github.com/ChitturiPadma/datasets/blob/master/Breast_CancerData.csv" target="_blank">https://github.com/ChitturiPadma/datasets/blob/master/Breast_CancerData.csv</a>:</p><pre class="programlisting">
<span class="strong"><strong>       #Reading CSV File
       df &lt;- read.df("/home/padmac/cookbook/Problems/
       Breast_CancerData.csv", "csv", header = "true", inferSchema =
       "true", na.strings = "NA")
       head(subset(df, select=c("clump_thickness","label")))
       #Write SparkDataFrame in parquet format
       write.df(people, path = "people.parquet", source = "parquet",
       mode = "overwrite")</strong></span>
</pre><p>The following is the output:</p><pre class="programlisting">
<span class="strong"><strong>        &gt; #Reading CSV File
        &gt; df &lt;-read.df("/home/padmac/cookbook/Problems</strong></span>
<span class="strong"><strong>        /Breast_CancerData.csv", "csv", header = "true", inferSchema = </strong></span>
<span class="strong"><strong>        "true", na.strings = "NA")
        &gt; head(subset(df, select=c("clump_thickness","label")))
        clump_thickness label
        1Â Â Â Â Â Â Â Â Â Â Â Â Â Â  5Â Â Â Â  Â Â Â Â Â Â Â  B
        2Â Â Â Â Â Â Â Â Â Â Â Â Â Â  5Â Â Â Â  Â Â Â Â Â Â Â  B
        3Â Â Â Â Â Â Â Â Â Â Â Â Â Â  3Â Â Â Â  Â Â Â Â Â Â Â  B
        4Â Â Â Â Â Â Â Â Â Â Â Â Â Â  6Â Â Â Â  Â Â Â Â Â Â Â  B
        5Â Â Â Â Â Â Â Â Â Â Â Â Â Â  4Â Â Â Â  Â Â Â Â Â Â Â  B
        6Â Â Â Â Â Â Â Â Â Â Â Â Â Â  8Â Â Â Â  Â Â Â Â Â Â Â  M
        &gt; #Write SparkDataFrame in parquet format
        &gt; write.df(people, path =   </strong></span>
<span class="strong"><strong>        "/home/padmac/cookbook/output/people.parquet", source =   </strong></span>
<span class="strong"><strong>        "parquet", mode = "overwrite")</strong></span>
</pre></li><li><p>We can also create SparkDataFrames from Hive tables. For this, we need to create a SparkSession with Hive support which can access tables in the Hive MetaStore:</p><pre class="programlisting">
<span class="strong"><strong>    sparkR.session()
    sql("CREATE TABLE IF NOT EXISTS src (key INT, value STRING)")
    sql("LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt'
    INTO TABLE src")
    # Queries can be expressed in HiveQL.
    results &lt;- sql("FROM src SELECT key, value")
    # results is now a SparkDataFrame
    head(results)</strong></span>
</pre><p>The following is the output:</p><pre class="programlisting">
<span class="strong"><strong>keyÂ Â  value
1 Â Â Â  238Â Â  val_238
2Â  Â Â  86Â Â Â  val_86
3 Â Â Â  311Â Â  val_311</strong></span>
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec387"></a>How it worksâ€¦</h3></div></div></div><p>The preceding code snippets show how to create SparkR DataFrames from a variety of data sources. Initially, <code class="literal">sqlContext &lt;- sparkRSQL.init(sc)</code> initializes the SQLContext. Using <code class="literal">createDataFrame</code>, a SparkR DataFrame is created from the existing dataset iris available in R. Next, we saw how to create DataFrames using <code class="literal">read.df</code>. It takes SQLContext and the path for the file to load and the type of data source as parameters. For reading a CSV file, we specified the packages with the packages argument while initializing SparkContext.</p><p>Also, we have seen how to create SparkR DataFrames from hive tables. For this, we initialized HiveContext, using which tables can be accessed in the Hive Metastore. To initialize HiveContext, Spark must be built with Hive support.</p><p>As in Spark 2.0.2, we created Spark Session which is used for creating SparkDataFrame from local data frames (available in R), from JSON, CSV and Hive tables. The statement <code class="literal">write.df(people, path = "/home/padmac/cookbook/output/people.parquet", source = "parquet", mode = "overwrite")</code> writes the SparkDataFrame in the specified file format. Â  Â  Â </p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec388"></a>There's moreâ€¦</h3></div></div></div><p>SparkR supports operating on a variety of data sources, such as JSON, Parquet, and Avro files natively through Spark packages (<a class="ulink" href="http://spark-packages.org/" target="_blank">http://spark-packages.org/</a>). We can find data source connectors for popular file formats such as CSV and Avro. These can be added by specifying packages with <code class="literal">spark-submit</code> or <code class="literal">sparkR</code> commands, or can be specified with the packages argument while initializing <code class="literal">SparkContext</code> using <code class="literal">init</code>. Also, while creating DataFrames from Hive tables, Spark must be built with Hive support (<a class="ulink" href="https://spark.apache.org/docs/1.6.0/building-spark.html#building-with-hive-and-jdbc-support" target="_blank">https://spark.apache.org/docs/1.6.0/building-spark.html#building-with-hive-and-jdbc-support</a>). The upcoming release of SparkR will have support for other sets of data sources as well.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec389"></a>See also</h3></div></div></div><p>Please refer to <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Big Data Analytics with Spark</em></span> to get familiar with Spark. Also, visit the <span class="emphasis"><em>Interactive analysis with the SparkR shell</em></span> recipe to get familiar with SparkR.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec107"></a>SparkR DataFrame operations</h2></div></div><hr /></div><p>SparkR DataFrames support a number of operations to do structured data processing. In this recipe, we'll see a good number of examples, such as selection, grouping, aggregation, and so on.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec390"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark Cluster either in pseudo distributed mode or in one of the distributed modes, that is, standalone, YARN, or Mesos. Also, install RStudio. Please refer to the <span class="emphasis"><em>Installing R</em></span> recipe for details on the installation of R and the <span class="emphasis"><em>Creating SparkR DataFrames</em></span> recipe to get acquainted with the creation of DataFrames from a variety of data sources.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec391"></a>How to do itâ€¦</h3></div></div></div><p>In this recipe, we'll see how to perform various operations SparkR data frames:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Let's see how to select a column from a DataFrame:</p><pre class="programlisting">
<span class="strong"><strong>  library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"),
      "R", "lib")))</strong></span>
<span class="strong"><strong>  sc &lt;- sparkR.init(master = "local[*]", sparkEnvir =
      list(spark.driver.memory="2g"))</strong></span>
<span class="strong"><strong>  sqlContext &lt;- sparkRSQL.init(sc)</strong></span>
<span class="strong"><strong>  df &lt;- createDataFrame(sqlContext, faithful)</strong></span>
<span class="strong"><strong>  head(select(df, df$eruptions))</strong></span>
<span class="strong"><strong>  [...]</strong></span>
<span class="strong"><strong>  16/03/19 21:52:11 INFO TaskSchedulerImpl: Adding task set 4.0
      with 1 tasks</strong></span>
<span class="strong"><strong>  16/03/19 21:52:11 INFO TaskSetManager: Starting task 0.0 in stage
      4.0 (TID 4,   localhost, partition 0,PROCESS_LOCAL, 12976 bytes)</strong></span>
<span class="strong"><strong>  16/03/19 21:52:11 INFO Executor: Running task 0.0 in stage 4.0
      (TID 4)</strong></span>
<span class="strong"><strong>  16/03/19 21:52:11 INFO Executor: Finished task 0.0 in stage 4.0
      (TID 4). 1522   bytes   result sent to driver</strong></span>
<span class="strong"><strong>  16/03/19 21:52:11 INFO TaskSetManager: Finished task 0.0 in stage
      4.0 (TID 4) in   27 ms on localhost (1/1)</strong></span>
<span class="strong"><strong>  16/03/19 21:52:11 INFO TaskSchedulerImpl: Removed TaskSet 4.0,
      whose tasks have   all completed, from pool </strong></span>
<span class="strong"><strong>  16/03/19 21:52:11 INFO DAGScheduler: ResultStage 4 (dfToCols at
      NativeMethodAccessorImpl.java:-2) finished in 0.029 s</strong></span>
<span class="strong"><strong>  16/03/19 21:52:11 INFO DAGScheduler: Job 4 finished: dfToCols at
      NativeMethodAccessorImpl.java:-2, took 0.041186 s</strong></span>
<span class="strong"><strong>    eruptions</strong></span>
<span class="strong"><strong>  1     3.600</strong></span>
<span class="strong"><strong>  2     1.800</strong></span>
<span class="strong"><strong>  3     3.333</strong></span>
<span class="strong"><strong>  4     2.283</strong></span>
<span class="strong"><strong>  5     4.533</strong></span>
<span class="strong"><strong>  6     2.883</strong></span>
</pre></li><li><p>We can also select a column by passing the column name as a string as follows:</p><pre class="programlisting">
<span class="strong"><strong>  head(select(df, "eruptions"))</strong></span>
</pre></li><li><p>Next, filter the DataFrame to retain rows with wait times shorter than <code class="literal">50</code> minutes:</p><pre class="programlisting">
<span class="strong"><strong>  head(filter(df, df$waiting &lt; 50))</strong></span>
<span class="strong"><strong>    eruptions waiting</strong></span>
<span class="strong"><strong>  1     1.750      47</strong></span>
<span class="strong"><strong>  2     1.750      47</strong></span>
<span class="strong"><strong>  3     1.867      48</strong></span>
<span class="strong"><strong>  4     1.750      48</strong></span>
<span class="strong"><strong>  5     2.167      48</strong></span>
<span class="strong"><strong>  6     2.100      49</strong></span>
</pre></li><li><p>Now we'll see how to group and perform aggregations over data:</p><pre class="programlisting">
<span class="strong"><strong>  waiting_counts &lt;- summarize(groupBy(df, df$waiting), count =
      n(df$waiting))</strong></span>
<span class="strong"><strong>  head(waiting_counts)</strong></span>
</pre><p>The following is the output:</p><pre class="programlisting">
<span class="strong"><strong> waiting count</strong></span>
<span class="strong"><strong> 1 81 13</strong></span>
<span class="strong"><strong> 2 60 6</strong></span>
<span class="strong"><strong> 3 93 2</strong></span>
<span class="strong"><strong> 4 68 1</strong></span>
<span class="strong"><strong> 5 47 4</strong></span>
<span class="strong"><strong> 6 80 8</strong></span>
</pre></li><li><p>Next, sort the output from the preceding aggregated results as follows:</p><pre class="programlisting">
<span class="strong"><strong>  head(arrange(waiting_counts, desc(waiting_counts$count)))</strong></span>
<span class="strong"><strong>   waiting count</strong></span>
<span class="strong"><strong>  1      78    15</strong></span>
<span class="strong"><strong>  2      83    14</strong></span>
<span class="strong"><strong>  3      81    13</strong></span>
<span class="strong"><strong>  4      77    12</strong></span>
<span class="strong"><strong>  5      82    12</strong></span>
<span class="strong"><strong>  6      84    10</strong></span>
</pre></li><li><p>Here is how to apply arithmetic functions on columns:</p><pre class="programlisting">
<span class="strong"><strong>df$waiting_secs &lt;- df$waiting * 60</strong></span>
<span class="strong"><strong>  eruptions waiting waiting_secs</strong></span>
<span class="strong"><strong>  1     3.600      79         4740</strong></span>
<span class="strong"><strong>    2     1.800      54         3240</strong></span>
<span class="strong"><strong>    3     3.333      74         4440</strong></span>
<span class="strong"><strong>  4     2.283      62         3720</strong></span>
<span class="strong"><strong>  5     4.533      85         5100</strong></span>
<span class="strong"><strong>  6     2.883      55         3300</strong></span>
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec392"></a>How it worksâ€¦</h3></div></div></div><p>The preceding code snippets show how to perform various DataFrame operations. The lines <code class="literal">select(df, df$eruptions)</code> and <code class="literal">select(df, "eruptions")</code> select the <code class="literal">eruptions</code> column. The line <code class="literal">filter(df, df$waiting &lt; 50)</code> filters the DataFrame such that it contains rows with wait times shorter than 50 minutes. The line <code class="literal">summarize(groupBy(df, df$waiting), count = n(df$waiting))</code> groups the rows in the DataFrame by waiting time and counts the number of times each waiting time appears. Now,Â <code class="literal">arrange(waiting_counts, desc(waiting_counts$count))</code> sorts the output from the aggregation to get the most common waiting times. We have also seen how to apply arithmetic functions on columns for data processing and during aggregation.</p><p>The grouping and aggregation operations are same in Spark 2.0.2 as well except that we have to create spark session as <code class="literal">sparkR.session(master = "spark://master:7077", sparkConfig = list(spark.driver.memory = "2g"))</code>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec393"></a>There's moreâ€¦</h3></div></div></div><p>There is also support for SparkR DataFrames to be registered as tables which allows to run SQL queries over the data. The SQL queries return results as a DataFrame. In the coming recipe, we'll see how to apply user-defined functions, SQL queries and also machine learning libraries over SparkR.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec394"></a>See also</h3></div></div></div><p>Please refer to <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Big Data Analytics with Spark</em></span> to get familiar with Spark. Also, visit the <span class="emphasis"><em>Interactive analysis with the SparkR shell</em></span> and <span class="emphasis"><em>Creating SparkR DataFrames</em></span> recipes to get familiar with SparkR.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec108"></a>Applying user-defined functions in SparkR</h2></div></div><hr /></div><p>In this recipe we'll see how to apply the functions such as dapply, gapply and lapply over the Spark DataFrame.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec395"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark Cluster either in pseudo distributed mode or in one of the distributed modes that is, standalone, YARN, or Mesos. Also, install RStudio. Please refer the <span class="emphasis"><em>Installing R </em></span>recipeÂ for details on the installation of R andÂ <span class="emphasis"><em>Creating SparkR DataFrames</em></span> recipe to get acquainted with the creation of DataFrames from a variety of data sources.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec396"></a>How to do itâ€¦</h3></div></div></div><p>In this recipe, we'll see how to apply the user defined functions available as of Spark 2.0.2.</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Here is the code which applies <code class="literal">dapply</code> on the Spark DataFrame.</p><pre class="programlisting">
<span class="strong"><strong>      schema &lt;- structType(structField("eruptions", "double"),
      structField("waiting", "double"), structField("waiting_secs",
      "double"))
      df1 &lt;- dapply(df, function(x) { x &lt;- cbind(x, x$waiting * 60) },
      schema)
      head(collect(df1))</strong></span>
</pre><p>The following is the output:</p><pre class="programlisting">
<span class="strong"><strong> eruptions waiting waiting_secs
        1 3.600 79 4740
        2 1.800 54 3240
        3 3.333 74 4440
        4 2.283 62 3720
        5 4.533 85 5100
        6 2.883 55 3300</strong></span>
</pre></li><li><p>Let's see how to use <code class="literal">dapplyCollect</code>:</p><pre class="programlisting">
<span class="strong"><strong> ldf &lt;- dapplyCollect(
      df,
      function(x){ x &lt;- cbind(x, "waiting_secs" = x$waiting * 60)
       })
      head(ldf,4)</strong></span>
</pre><p>The following is the output:</p><pre class="programlisting">
<span class="strong"><strong> eruptions waiting waiting_secs
        1 3.600 79 4740
        2 1.800 54 3240
        3 3.333 74 4440
        4 2.283 62 3720</strong></span>
</pre></li><li><p>Let's see how to use <code class="literal">gapply</code>Â and <code class="literal">gapplyCollect</code>:</p><pre class="programlisting">
<span class="strong"><strong>      #gapply
      schema &lt;- structType(structField("waiting", "double"),
      structField("max_eruption", "double"))
      result &lt;- gapply(
      df,
      "waiting",
      function(key, x) {
      y &lt;- data.frame(key, max(x$eruptions))
      }, schema)
      print("gapply output:")
      head(collect(arrange(result, "max_eruption", decreasing = TRUE)))
      print("gapplyCollect output:")
      result &lt;- gapplyCollect( df, "waiting",
      function(key, x) {
      y &lt;- data.frame(key, max(x$eruptions))
      colnames(y) &lt;- c("waiting", "max_eruption")
      y })
      head(result[order(result$max_eruption, decreasing = TRUE), ])</strong></span>
</pre><p>The following is the <code class="literal">gapply</code> output:
</p><pre class="programlisting">
<span class="strong"><strong>waiting max_eruption
          1 96 5.100
          2 76 5.067
          3 77 5.033
          4 88 5.000
          5 86 4.933
          6 82 4.900</strong></span>
</pre><p>The following is the <code class="literal">gapply</code> Â Collect output:</p><pre class="programlisting">
<span class="strong"><strong>          waiting max_eruption
        10 96 5.100
        49 76 5.067
        15 77 5.033
        4 88 5.000
        12 86 4.933
        29 82 4.900</strong></span>
</pre></li><li><p>Now, let's see the code which runs <code class="literal">lapply</code> over a list of elements:</p><pre class="programlisting">
<span class="strong"><strong>        print("lapply")
        families &lt;- c("gaussian", "poisson")
        train &lt;- function(family) {
        model &lt;- glm(Sepal.Length ~ Sepal.Width + Species, iris, family
        = family)
        summary(model)}
        # Return a list of model's summaries
        model.summaries &lt;- spark.lapply(families, train)
        # Print the summary of each model
        print(model.summaries)</strong></span>
</pre><p>The following is the output:</p><pre class="programlisting">
<span class="strong"><strong>[[1]]
        Call:
        glm(formula = Sepal.Length ~ Sepal.Width + Species, family =
        family, data = iris)
        Deviance Residuals:
        Min 1Q Median 3Q Max
        -1.30711 -0.25713 -0.05325 0.19542 1.41253
        Coefficients:
        Estimate Std. Error t value Pr(&gt;|t|)
        (Intercept) 2.2514 0.3698 6.089 9.57e-09 ***
        Sepal.Width 0.8036 0.1063 7.557 4.19e-12 ***
        Speciesversicolor 1.4587 0.1121 13.012 &lt; 2e-16 ***
        Speciesvirginica 1.9468 0.1000 19.465 &lt; 2e-16 ***
        ---
        Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
        (Dispersion parameter for gaussian family taken to be
         0.1918059)
        Null deviance: 102.168 on 149 degrees of freedom
        Residual deviance: 28.004 on 146 degrees of freedom
        AIC: 183.94
        Number of Fisher Scoring iterations: 2
        [[2]]
        Call:
        glm(formula = Sepal.Length ~ Sepal.Width + Species, family =
        family, data = iris)
        Deviance Residuals:
        Min 1Q Median 3Q Max
        -0.52652 -0.10966 -0.01230 0.07755 0.56101
        Coefficients:
        Estimate Std. Error z value Pr(&gt;|z|)
        (Intercept) 1.13033 0.35454 3.188 0.001432 **
        Sepal.Width 0.13971 0.10119 1.381 0.167361
        Speciesversicolor 0.26277 0.10901 2.410 0.015931 *
        Speciesvirginica 0.33842 0.09587 3.530 0.000416 ***
        ---
        Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
        (Dispersion parameter for poisson family taken to be 1)
        Null deviance: 17.3620 on 149 degrees of freedom
        Residual deviance: 4.5202 on 146 degrees of freedom
        AIC: Inf
        Number of Fisher Scoring iterations: 3</strong></span>
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec397"></a>HowÂ it worksâ€¦</h3></div></div></div><p>In the preceding code snippets, we applied various user-defined functions over the Spark DataFrameÂ 
<code class="literal">df</code>Â (created from the R DataFrameÂ 
<code class="literal">faithful</code>
). TheÂ 
<code class="literal">schema</code><code class="literal">&lt;- structType(structField("eruptions", "double"), structField("waiting", "double"), structField("waiting_secs", "double"))</code>Â line creates schema and the function <code class="literal">dapply</code> is applied to each partition of the Spark DataFrame.Â 
The function defined asÂ 
<code class="literal">function(x) { x &lt;- cbind(x, x$waiting * 60) }</code>Â takes one parameter which is applied to each partition and the schema specifies the row format of the resultingÂ 
<code class="literal">SparkDataFrame</code>
. Next
,Â 
<code class="literal">dapplyCollect</code>Â is also applied on each partition and the results are collected back on the driver.</p><p>TheÂ 
<code class="literal">gapply</code>Â functionÂ is applied to each group of theÂ 
<code class="literal">SparkDataFrame</code>Â which takes two parameters: grouping key and 
<code class="literal">Rdata.frame</code>Â corresponding to that key. The groups are chosen fromÂ 
<code class="literal">SparkDataFrame</code>Â column(s). The output of the function should be aÂ 
<code class="literal">data.frame</code>. Schema specifies the row format of the resultingÂ 
<code class="literal">SparkDataFrame</code>. The 
<code class="literal">gapplyCollect</code> functionÂ works the same asÂ 
<code class="literal">gapply</code>, but doesn't require schema to be passed as parameter and collects the result back to 
<code class="literal">Rdata.frame</code>. Similar toÂ 
<code class="literal">lapply</code>Â in R, 
<code class="literal">spark.lapply</code>Â runs a function over a list of elements and distributes the computations with Spark. It also applies a function in a manner that is similar to 
<code class="literal">doParallel</code>Â orÂ 
<code class="literal">lapply</code>Â to elements of a list.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec398"></a>There's moreâ€¦</h3></div></div></div><p>When applyingÂ 
<code class="literal">dapplyCollect</code>Â andÂ 
<code class="literal">gapplyCollect</code>Â functions over the Spark DataFrame, they can fail if the output of UDF execution on all the partitions cannot fit in the driver memory. Also when running 
<code class="literal">lapply</code>
, if the results of all the computations do not fit in a single machine, create DataFrame from the list asÂ 
<code class="literal">dfcreateDataFrame(list)</code>Â and then useÂ 
<code class="literal">dapply</code>
.
There is also support for SparkR DataFrame to be registered as table and allows to run SQL queries over its data. The SQL queries return results as a DataFrame. In the coming recipe, we'll see how to apply SQL queries and machine learning libraries over SparkR.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec399"></a>See also</h3></div></div></div><p>Please refer toÂ <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Big Data Analytics with SparkÂ </em></span>to get familiar with Spark. Also, visit the <span class="emphasis"><em>Interactive analysis with the SparkR shell</em></span> , <span class="emphasis"><em>Creating SparkR DataFramesÂ </em></span>andÂ <span class="emphasis"><em>SparkR DataFrame operations </em></span>recipes to get familiar with SparkR and to learn the type byÂ operations performed of Spark DataFrames.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec109"></a>Running SQL queries from SparkR and caching DataFrames</h2></div></div><hr /></div><p>In this recipe, we'll see how to run SQL queries over SparkR DataFrames and cache the datasets.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec400"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark Cluster either in pseudo distributed mode or in one of the distributed modes, that is, standalone, YARN, or Mesos. Also, install RStudio. Please refer to the Installing R recipe for details on the installation of R and the <span class="emphasis"><em>Creating SparkR DataFrames</em></span> recipe to get acquainted with the creation of DataFrames from a variety of data sources.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec401"></a>How to do itâ€¦</h3></div></div></div><p>The following code shows how to apply SQL queries over SparkR data frames using Spark 1.6.0. As per Spark 2.0.2, the methods would remain same except that spark session is used instead of SQLContext:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Let's create a DataFrame from a JSON file. The sample JSON file <code class="literal">people.json</code> contains the following content:</p><pre class="programlisting">
<span class="strong"><strong>  {"name":"Michael"}</strong></span>
<span class="strong"><strong>  {"name":"Andy", "age":30}</strong></span>
<span class="strong"><strong>  {"name":"Justin", "age":19}</strong></span>
<span class="strong"><strong>  Here is the code snippet for creating a data frame from JSON
      content:</strong></span>
<span class="strong"><strong>  ReadingJSON.R</strong></span>
<span class="strong"><strong>  if (nchar(Sys.getenv("SPARK_HOME")) &lt; 1) {</strong></span>
<span class="strong"><strong>    Sys.setenv(SPARK_HOME = "~/bigdata/spark-1.6.0-bin-hadoop2.6")</strong></span>
<span class="strong"><strong>  }</strong></span>
<span class="strong"><strong>  library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"),
      "R", "lib")))</strong></span>
<span class="strong"><strong>  sc &lt;- sparkR.init(master = "local[*]", sparkEnvir =
      list(spark.driver.memory="2g"))</strong></span>
<span class="strong"><strong>  sqlContext &lt;- sparkRSQL.init(sc)</strong></span>
<span class="strong"><strong>  people &lt;- read.df(sqlContext, "~/&lt;path&gt;/people.json","json")</strong></span>
</pre></li><li><p>Next, run SQL queries on the DataFrame as follows:</p><pre class="programlisting">
<span class="strong"><strong>registerTempTable(people, "people")</strong></span>
<span class="strong"><strong>  teenagers &lt;- sql(sqlContext, "SELECT name, age FROM people WHERE
      age &gt;= 10)</strong></span>
<span class="strong"><strong>  head(teenagers)</strong></span>
<span class="strong"><strong>  16/03/20 21:05:12 INFO TaskSchedulerImpl: Adding task set 26.0
      with 1 tasks</strong></span>
<span class="strong"><strong>  16/03/20 21:05:12 INFO TaskSetManager: Starting task 0.0 in stage
      26.0 (TID 37, localhost, partition 1,PROCESS_LOCAL, 2193 bytes)</strong></span>
<span class="strong"><strong>  16/03/20 21:05:12 INFO Executor: Running task 0.0 in stage 26.0
      (TID 37)</strong></span>
<span class="strong"><strong>  16/03/20 21:05:12 INFO HadoopRDD: Input split:
      file:/home/padmac/bigdata/spark-  1.6.0-bin-
      hadoop2.6/examples/src/main/resources/people.json:36+37</strong></span>
<span class="strong"><strong>  16/03/20 21:05:12 INFO Executor: Finished task 0.0 in stage 26.0
      (TID 37). 2638   bytes result sent to driver</strong></span>
<span class="strong"><strong>  16/03/20 21:05:12 INFO TaskSetManager: Finished task 0.0 in stage
      26.0 (TID 37) in   15 ms on localhost (1/1)</strong></span>
<span class="strong"><strong>  16/03/20 21:05:12 INFO TaskSchedulerImpl: Removed TaskSet 26.0,
      whose tasks have   all completed, from pool </strong></span>
<span class="strong"><strong>  16/03/20 21:05:12 INFO DAGScheduler: ResultStage 26 (dfToCols at
      NativeMethodAccessorImpl.java:-2) finished in 0.016 s</strong></span>
<span class="strong"><strong>  16/03/20 21:05:12 INFO DAGScheduler: Job 26 finished: dfToCols at
      NativeMethodAccessorImpl.java:-2, took 0.025842 s</strong></span>
<span class="strong"><strong>      name age</strong></span>
<span class="strong"><strong>  1   Andy  30</strong></span>
<span class="strong"><strong>  2 Justin  19</strong></span>
</pre></li><li><p>Here is the code snippet to cache the DataFrame:</p><pre class="programlisting">
<span class="strong"><strong>cache(teenagers)</strong></span>
<span class="strong"><strong>  system.time(count(teenagers))</strong></span>
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec402"></a>How it worksâ€¦</h3></div></div></div><p>In the preceding code snippet, a SparkR DataFrame is registered as a temporary table in Spark SQL so that SQL queries can be run over the data. TheÂ <code class="literal">registerTempTable</code>Â function registers the DataFrame as a table. The <code class="literal">sql</code> method is used to run <code class="literal">sql</code> statements over the DataFrame. We have also seen that cache (teenagers) is used to cache the DataFrame so that it can be used repeatedly.Â In order to run the code using Spark 2.0.2, please refer the recipe, <span class="emphasis"><em>Creating SparkR standalone application from RStudio</em></span>Â for details on creating Spark session.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec403"></a>There's moreâ€¦</h3></div></div></div><p>Apart from running SQL queries over DataFrames, as Dataset is the new interface added in Spark 1.6, the upcoming versions of SparkR will also integrate with the DataSet API. In the coming recipe, we'll see how to run machine learning models over DataFrames.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec404"></a>See also</h3></div></div></div><p>Please refer to <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Big Data Analytics with Spark</em></span> to get familiar with spark. Also, visit the <span class="emphasis"><em>Interactive analysis with the SparkR</em></span> shell and <span class="emphasis"><em>Creating SparkR DataFrames</em></span> recipes to get familiar with SparkR.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec110"></a>Machine learning with SparkR</h2></div></div><hr /></div><p>SparkR is integrated with Spark's MLlib machine learning library so that algorithms can be parallelized seamlessly without specifying manually which part of the algorithm can be run in parallel. MLlib is one of the fastest-growing machine learning libraries; hence, the ability to use R with MLlib will create a huge number of contributions to MLlib from R users. As of Spark 1.6, there is support for generalized linear models (Gaussian and binomial) over DataFrames and as per Spark 2.0.2, the algorithms such as Naive Bayes and KMeans are available.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec405"></a>Getting ready</h3></div></div></div><p>To step through this recipe, you will need a running Spark Cluster either in pseudo distributed mode or in one of the distributed modes, that is, standalone, YARN, or Mesos. Also, install RStudio. Please refer toÂ <span class="emphasis"><em>Installing R</em></span> recipe for details on the installation of R and the <span class="emphasis"><em>Creating SparkR DataFrames</em></span> recipe to get acquainted with the creation of DataFrames from a variety of data sources.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec406"></a>How to do itâ€¦</h3></div></div></div><p>Here let's see how to apply gaussian linear model and Kmeans model as per Spark 2.0.2:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Let's use a Gaussian linear model over the SparkR DataFrame as follows:</p><pre class="programlisting">
<span class="strong"><strong>  GLMExample.R
      if (nchar(Sys.getenv("SPARK_HOME")) &lt; 1) {
      Sys.setenv(SPARK_HOME = "/home/padmac/bigdata/spark-2.0.2-bin-
      hadoop2.6")
      }
      library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"),
      "R", "lib")))
      sparkR.session(master = "spark://master:7077", sparkConfig =
      list(spark.driver.memory = "2g"))
      irisDF &lt;- as.DataFrame(iris)
      # Fit a generalized linear model of family "gaussian" with
      #spark.glm
       gaussianDF &lt;- irisDF
       gaussianTestDF &lt;- irisDF
       gaussianGLM &lt;- spark.glm(gaussianDF, Sepal_Length ~ Sepal_Width
       + Species, family = "gaussian")
       #Model summary
       summary(gaussianGLM)

       # Prediction
       gaussianPredictions &lt;- predict(gaussianGLM, gaussianTestDF)
       showDF(gaussianPredictions,5)

       # Fit a generalized linear model of family "binomial" with
       # spark.glm
       binomialDF &lt;- filter(irisDF, irisDF$Species != "setosa")
       binomialTestDF &lt;- binomialDF
       binomialGLM &lt;- spark.glm(binomialDF, Species ~ Sepal_Length +
       Sepal_Width, family = "binomial")

       # Model summary
       summary(binomialGLM)

       # Prediction
       binomialPredictions &lt;- predict(binomialGLM, binomialTestDF)
       showDF(binomialPredictions,5)
</strong></span>
</pre><p>The following is the output:</p><pre class="programlisting">
<span class="strong"><strong>     &gt; #Model summary</strong></span>
<span class="strong"><strong>     &gt; summary(gaussianGLM)</strong></span>
<span class="strong"><strong>     Deviance Residuals: </strong></span>
<span class="strong"><strong>     (Note: These are approximate quantiles with relative error &lt;=
     0.01)</strong></span>
<span class="strong"><strong>     Min 1Q Median 3Q Max </strong></span>
<span class="strong"><strong>    -1.30711 -0.26011 -0.06189 0.19111 1.41253  </strong></span>
<span class="strong"><strong>    Coefficients:</strong></span>
<span class="strong"><strong>    Estimate Std. Error t value Pr(&gt;|t|) </strong></span>
<span class="strong"><strong>    (Intercept) 2.2514 0.36975 6.0889 9.5681e-09</strong></span>
<span class="strong"><strong>    Sepal_Width 0.80356 0.10634 7.5566 4.1873e-12</strong></span>
<span class="strong"><strong>    Species_versicolor 1.4587 0.11211 13.012 0 </strong></span>
<span class="strong"><strong>    Species_virginica 1.9468 0.10001 19.465 0</strong></span>
<span class="strong"><strong>    (Dispersion parameter for gaussian family taken to be 0.1918059)</strong></span>
<span class="strong"><strong>    Null deviance: 102.168 on 149 degrees of freedom</strong></span>
<span class="strong"><strong>    Residual deviance: 28.004 on 146 degrees of freedom</strong></span>
<span class="strong"><strong>    AIC: 183.9</strong></span>
<span class="strong"><strong>    Number of Fisher Scoring iterations: 1</strong></span>
<span class="strong"><strong>    &gt; # Prediction</strong></span>
<span class="strong"><strong>    &gt; gaussianPredictions &lt;- predict(gaussianGLM, gaussianTestDF)</strong></span>
<span class="strong"><strong>    &gt; showDF(gaussianPredictions,5)</strong></span>
<span class="strong"><strong>+------------+-----------+------------+-----------+-------+------+ |Sepal_Length|Sepal_Width|Petal_Length|Petal_Width|Species|label | </strong></span>
<span class="strong"><strong>+------------+-----------+------------+-----------+-------+------+</strong></span>
<span class="strong"><strong>| 5.1        | 3.5       | 1.4        | 0.2       | setosa| 5.1  | </strong></span>
<span class="strong"><strong>| 4.9        | 3.0       | 1.4        | 0.2       | setosa| 4.9  | </strong></span>
<span class="strong"><strong>| 4.7        | 3.2       | 1.3        | 0.2       | setosa| 4.7  | </strong></span>
<span class="strong"><strong>| 4.6        | 3.1       | 1.5        | 0.2       | setosa| 4.6  | </strong></span>
<span class="strong"><strong>| 5.0        | 3.6       | 1.4        | 0.2       | setosa| 5.0  | </strong></span>
<span class="strong"><strong>+------------+-----------+------------+-----------+-------+------+</strong></span>
<span class="strong"><strong>+------------------+</strong></span>
<span class="strong"><strong>|prediction        |</strong></span>
<span class="strong"><strong>+------------------+</strong></span>
<span class="strong"><strong>|5.063856384860281 |</strong></span>
<span class="strong"><strong>|4.662075934441678 |</strong></span>
<span class="strong"><strong>|4.82278811460912  |</strong></span>
<span class="strong"><strong>|4.7424320245253995|</strong></span>
<span class="strong"><strong>|5.144212474944002 |</strong></span>
<span class="strong"><strong>+------------------+</strong></span>
<span class="strong"><strong> &gt; # Fit a generalized linear model of family "binomial" with spark.glm</strong></span>
<span class="strong"><strong> &gt; binomialDF &lt;- filter(irisDF, irisDF$Species != "setosa")</strong></span>
<span class="strong"><strong> &gt; binomialTestDF &lt;- binomialDF</strong></span>
<span class="strong"><strong> &gt; binomialGLM &lt;- spark.glm(binomialDF, Species ~ Sepal_Length +
 Sepal_Width, family = "binomial")</strong></span>
<span class="strong"><strong> 16/11/30 18:10:14 WARN WeightedLeastSquares: regParam is zero, which
 might cause numerical instability and overfitting.</strong></span>
<span class="strong"><strong> 16/11/30 18:10:14 WARN WeightedLeastSquares: regParam is zero, which
 might cause numerical instability and overfitting.</strong></span>
<span class="strong"><strong> 16/11/30 18:10:14 WARN WeightedLeastSquares: regParam is zero, which
 might cause numerical instability and overfitting.</strong></span>
<span class="strong"><strong> 16/11/30 18:10:14 WARN WeightedLeastSquares: regParam is zero, which
 might cause numerical instability and overfitting.</strong></span>
<span class="strong"><strong> 16/11/30 18:10:14 WARN WeightedLeastSquares: regParam is zero, which
 might cause numerical instability and overfitting.</strong></span>
<span class="strong"><strong> 16/11/30 18:10:15 WARN WeightedLeastSquares: regParam is zero, which
 might cause numerical instability and overfitting.</strong></span>
<span class="strong"><strong> &gt; </strong></span>
<span class="strong"><strong> &gt; # Model summary</strong></span>
<span class="strong"><strong> &gt; summary(binomialGLM)</strong></span>
<span class="strong"><strong> Deviance Residuals: </strong></span>
<span class="strong"><strong> (Note: These are approximate quantiles with relative error &lt;= 0.01)</strong></span>
<span class="strong"><strong> Min 1Q Median 3Q Max </strong></span>
<span class="strong"><strong> -1.87365 -0.93236 -0.35150 0.96084 2.35669</strong></span>
<span class="strong"><strong>Coefficients:</strong></span>
<span class="strong"><strong> Estimate Std. Error t value Pr(&gt;|t|) </strong></span>
<span class="strong"><strong> (Intercept) -13.046 3.0974 -4.2119 2.5319e-05</strong></span>
<span class="strong"><strong> Sepal_Length 1.9024 0.51692 3.6802 0.00023303</strong></span>
<span class="strong"><strong> Sepal_Width 0.40466 0.86283 0.46899 0.63908</strong></span>
<span class="strong"><strong>(Dispersion parameter for binomial family taken to be 1)</strong></span>
<span class="strong"><strong>Null deviance: 138.63 on 99 degrees of freedom</strong></span>
<span class="strong"><strong> Residual deviance: 110.33 on 97 degrees of freedom</strong></span>
<span class="strong"><strong> AIC: 116.3</strong></span>
<span class="strong"><strong>Number of Fisher Scoring iterations: 5</strong></span>
<span class="strong"><strong> &gt; </strong></span>
<span class="strong"><strong> &gt; # Prediction</strong></span>
<span class="strong"><strong> &gt; binomialPredictions &lt;- predict(binomialGLM, binomialTestDF)</strong></span>
<span class="strong"><strong> &gt; showDF(binomialPredictions,5)</strong></span>
<span class="strong"><strong> +------------+-----------+------------+-----------+----------+----+</strong></span>
<span class="strong"><strong> |Sepal_Length|Sepal_Width|Petal_Length|Petal_Width| Species label| </strong></span>
<span class="strong"><strong> +------------+-----------+------------+-----------+----------+----+</strong></span>
<span class="strong"><strong> | 7.0        | 3.2       | 4.7        | 1.4       |versicolor|0.0 | </strong></span>
<span class="strong"><strong> | 6.4        | 3.2       | 4.5        | 1.5       |versicolor|0.0 | </strong></span>
<span class="strong"><strong> | 6.9        | 3.1       | 4.9        | 1.5       |versicolor|0.0 | </strong></span>
<span class="strong"><strong> | 5.5        | 2.3       | 4.0        | 1.3       |versicolor|0.0 | </strong></span>
<span class="strong"><strong> | 6.5        | 2.8       | 4.6        | 1.5       |versicolor|0.0 | </strong></span>
<span class="strong"><strong> +------------+-----------+------------+-----------+----------+-----</strong></span>
<span class="strong"><strong> +--------------------+</strong></span>
<span class="strong"><strong> | prediction         |</strong></span>
<span class="strong"><strong> +--------------------+</strong></span>
<span class="strong"><strong> |0.8271421517601683  |</strong></span>
<span class="strong"><strong> | 0.6044595910412891 |</strong></span>
<span class="strong"><strong> | 0.7916340858282183 |</strong></span>
<span class="strong"><strong> | 0.16080518180591663|</strong></span>
<span class="strong"><strong> |0.6112229217050481  |</strong></span>
<span class="strong"><strong> +--------------------+</strong></span>
</pre></li><li><p>Now, let's see how to run Kmeans over the SparkDataFrame:</p><pre class="programlisting">       Â  <span class="strong"><strong>#Run KMeans
         kmeansDF &lt;- irisDF
         kmeansTestDF &lt;- irisDF
         kmeansModel &lt;- spark.kmeans(kmeansDF, ~ Sepal_Length +
         Sepal_Width +Petal_Length + Petal_Width,k = 3)
         # Model summary
         summary(kmeansModel)
         # Get fitted result from the k-means model
         showDF(fitted(kmeansModel),5)
         # Prediction
         kmeansPredictions &lt;- predict(kmeansModel, kmeansTestDF)
         showDF(kmeansPredictions,5)
</strong></span>
</pre><p>The following is the output:</p><pre class="programlisting">
<span class="strong"><strong>     &gt; #Run KMeans
     &gt; kmeansDF &lt;- irisDF
     &gt; kmeansTestDF &lt;- irisDF
     &gt; kmeansModel &lt;- spark.kmeans(kmeansDF, ~ Sepal_Length +
     Sepal_Width +Petal_Length+ Petal_Width, k = 3)
     16/11/30 18:29:35 WARN KMeans: The input data is not directly
     cached, which may hurt performance if its parent RDDs are also
     uncached.
     16/11/30 18:29:38 WARN KMeans: The input data was not directly
     cached, which may hurt performance if its parent RDDs are also
     uncached.
     [Stage 184:================================&gt; (124 + 5) / 200]
     [Stage 184:===========================================&gt; (166 + 4)
     / 200] &gt;
     &gt; # Model summary
     &gt; summary(kmeansModel)
     $coefficients
     Sepal_Length Sepal_Width Petal_Length Petal_Width
     1 5.006 3.428 1.462 0.246
     2 6.853846 3.076923 5.715385 2.053846
     3 5.883607 2.740984 4.388525 1.434426

    $size
    $size[[1]]
    [1] 50
    $size[[2]]
    [1] 39
    $size[[3]]
    [1] 61
    $cluster
    SparkDataFrame[prediction:int]
    $is.loaded
    [1] FALSE

    &gt;
    &gt; # Get fitted result from the k-means model
    &gt; showDF(fitted(kmeansModel),5)
+------------+-----------+------------+-----------+-------+-----------+
|Sepal_Length|Sepal_Width|Petal_Length|Petal_Width|Species|prediction | +-------------+-----------+------------+-----------+-------+----------+
| 5.1        | 3.5       | 1.4        | 0.2        | setosa| 0        |
| 4.9        | 3.0       | 1.4        | 0.2        | setosa| 0        |
| 4.7        | 3.2       | 1.3        | 0.2        | setosa| 0        |
| 4.6        | 3.1       | 1.5        | 0.2        | setosa| 0        |
| 5.0        | 3.6       | 1.4        | 0.2        | setosa| 0        |
+------------+-----------+------------+-----------+-------+-----------+


    &gt;
    &gt; # Prediction
    &gt; kmeansPredictions &lt;- predict(kmeansModel, kmeansTestDF)
    &gt; showDF(kmeansPredictions,5)
+------------+-----------+------------+-----------+-------+---------+
|Sepal_Length|Sepal_Width|Petal_Length|Petal_Width|Species|prediction
+------------+-----------+------------+-----------+-------+---------+
| 5.1        | 3.5       |  1.4       | 0.2       | setosa| 0       |
| 4.9        | 3.0       | 1.4        | 0.2       | setosa| 0       |
| 4.7        | 3.2       | 1.3        | 0.2       | setosa| 0       |
| 4.6        | 3.1       | 1.5        | 0.2       | setosa| 0       |
| 5.0        | 3.6       | 1.4        | 0.2       | setosa| 0       |
+------------+-----------+------------+-----------+-------+---------+
</strong></span>
</pre></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec407"></a>How it worksâ€¦</h3></div></div></div><p>The preceding code snippets show how to fit generalized Gaussian and binomial linear models over DataFrames. The Gaussian GLM model returns a list with <code class="literal">devianceResiduals</code> and <code class="literal">coefficients</code> components. <code class="literal">devianceResiduals</code> gives the <code class="literal">min</code>/<code class="literal">max</code> deviance residuals of the estimation; <code class="literal">coefficients</code> gives the estimated coefficients and their estimated standard errors, t-values and p-values. (This is only available when the model is fitted by a normal solver.) The binomial GLM also returns <code class="literal">devianceResiduals</code> and <code class="literal">coefficients</code> components similar to gaussian GLM.</p><p>Similarly we applied k-means over the SparkDataFrame. The <code class="literal">kmeansModel &lt;- spark.kmeans(kmeansDF, ~ Sepal_Length + Sepal_Width + Petal_Length + Petal_Width, k = 3)</code> line generates the k-means model by running k-means over the SparkDataFrame with <code class="literal">k=3</code>. The <code class="literal">summary(kmeansModel)</code> line display the model summary that is, the centroids of the clusters and size of each cluster (number of data points in each cluster). Once the model is ready, the <code class="literal">kmeansPredictions &lt;- predict(kmeansModel, kmeansTestDF)</code> line predicts the cluster for each data point in theÂ  <code class="literal">kmeansTestDF.</code></p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec408"></a>There's moreâ€¦</h3></div></div></div><p>SparkR implements the interpretation of R model formulas as an MLlib feature transformer (<a class="ulink" href="http://spark.apache.org/docs/latest/ml-guide.html#transformers" target="_blank">http://spark.apache.org/docs/latest/ml-guide.html#transformers</a>), and provides integration with the ML pipelines API. The RFormula transformer provides a convenient way to specify feature transformations as in R. In Spark 2.0.2, there is additional support for more advanced features of R model formulas, including feature interactions, more model families, link functions, and better summary support.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec409"></a>See also</h3></div></div></div><p>Please refer to <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Big Data Analytics with Spark</em></span> to get familiar with Spark. Also, visit the <span class="emphasis"><em>Interactive analysis with the SparkR shell</em></span>, <span class="emphasis"><em>Creating SparkR DataFrame</em></span>s, and <span class="emphasis"><em>SparkR DataFrame operations</em></span> recipes to get familiar with SparkR.</p></div></div></div></div></div>
</div></div></div></body></html>
