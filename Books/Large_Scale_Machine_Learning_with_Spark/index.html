<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="">

    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0"> <!--320-->
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no, user-scalable=no">

    <link rel="icon" href="../../mapt/images/favicon.ico">

    <link rel="stylesheet" href="../../mapt/css/font-awesome.css">
    <link rel="stylesheet" href="../../mapt/css/google-fonts.css">
    <link rel="stylesheet" href="../../mapt/css/devicon.css">

    <link rel="stylesheet" href="../../mapt/css/bootstrap.css">
    <link rel="stylesheet" href="../../mapt/css/bootstrap-xl.css">
    <link rel="stylesheet" href="../../mapt/css/magnific-popup.css">
    <link rel="stylesheet" href="../../mapt/css/prism.css">
    <link rel="stylesheet" href="../../mapt/css/hljs-github.css">

    <link rel="stylesheet" href="../../mapt/css/mapt.css">
    <link rel="stylesheet" href="../../mapt/css/custom.css">

    <script src="../../mapt/js/jquery.js"></script>
    <script src="../../mapt/js/bootstrap.js"></script>
    <script src="../../mapt/js/jquery.magnific-popup.js"></script>
    <script src="../../mapt/js/highlight.min.js"></script>

    <script src="../../mapt/js/custom.js"></script>
    
    <title>Large Scale Machine Learning with Spark</title>
</head>

<body class="home-body">
    <div id="wrapper">
        <div id="sidebar-wrapper">    
            <ul class="sidebar-nav">
                <div class="list-group" id="sidebar-nav" role="tablist">
                    <li>
                        <a href="../../index.html" class="sidenav-menu-holder back-btn" id="back-link">
                            <span class="sidenav-menu">Book List</span>
                            <span class="pull-left mr5"><i class="fa fa-chevron-left"></i></span>
                        </a>
                    </li>
                    
                    <li class="book-info copyright">
                        <span class="info text-nowrap"><span class="copyleft">&copy;</span><span><strong>RuTracker</strong>.org</span></span>
                    </li>          
                    <li class="book-info copyright">
                        <span class="info text-nowrap">Pub date: <strong>27 Oct 2016</strong></span>
                    </li>         
                    <li class="book-info">
                        <span class="info text-nowrap">Price: €<strong>34.99</strong></span>
                        <span class="info text-nowrap">ISBN: <strong>9781785888748</strong></span>
                    </li>     
            
                    <li>
                        <a href="graphics/cover.jpg" class="sidenav-menu-holder cover-img">
                            <img src="graphics/cover.jpg" class="cover-image">
                        </a>
                    </li>        
            
                    <div class="book_navigation">
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse1">
                                <div class="section-name">1: Introduction to Data Analytics with Spark</div>
                            </a>
                        </li>
                        <div id="collapse1" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="1" class="sub-nav">
                                <a href="#ch01">
                                    <div class="section-name">Chapter 1: Introduction to Data Analytics with Spark</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec7" class="sub-nav">
                                <a href="#ch01lvl1sec7">                    
                                    <div class="section-name">Spark overview</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec8" class="sub-nav">
                                <a href="#ch01lvl1sec8">                    
                                    <div class="section-name">New computing paradigm with Spark</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec9" class="sub-nav">
                                <a href="#ch01lvl1sec9">                    
                                    <div class="section-name">Spark ecosystem</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec10" class="sub-nav">
                                <a href="#ch01lvl1sec10">                    
                                    <div class="section-name">Spark machine learning libraries</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec11" class="sub-nav">
                                <a href="#ch01lvl1sec11">                    
                                    <div class="section-name">Installing and getting started with Spark</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec12" class="sub-nav">
                                <a href="#ch01lvl1sec12">                    
                                    <div class="section-name">Packaging your application with dependencies</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec13" class="sub-nav">
                                <a href="#ch01lvl1sec13">                    
                                    <div class="section-name">Running a sample machine learning application</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec14" class="sub-nav">
                                <a href="#ch01lvl1sec14">                    
                                    <div class="section-name">References</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec15" class="sub-nav">
                                <a href="#ch01lvl1sec15">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse2">
                                <div class="section-name">2: Machine Learning Best Practices</div>
                            </a>
                        </li>
                        <div id="collapse2" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="2" class="sub-nav">
                                <a href="#ch02">
                                    <div class="section-name">Chapter 2: Machine Learning Best Practices</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec16" class="sub-nav">
                                <a href="#ch02lvl1sec16">                    
                                    <div class="section-name">What is machine learning?</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec17" class="sub-nav">
                                <a href="#ch02lvl1sec17">                    
                                    <div class="section-name">Machine learning tasks</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec18" class="sub-nav">
                                <a href="#ch02lvl1sec18">                    
                                    <div class="section-name">Practical machine learning problems</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec19" class="sub-nav">
                                <a href="#ch02lvl1sec19">                    
                                    <div class="section-name">Most widely used machine learning problems</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec20" class="sub-nav">
                                <a href="#ch02lvl1sec20">                    
                                    <div class="section-name">Large scale machine learning APIs in Spark</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec21" class="sub-nav">
                                <a href="#ch02lvl1sec21">                    
                                    <div class="section-name">Practical machine learning best practices</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec22" class="sub-nav">
                                <a href="#ch02lvl1sec22">                    
                                    <div class="section-name">Choosing the right algorithm for your application</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec23" class="sub-nav">
                                <a href="#ch02lvl1sec23">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse3">
                                <div class="section-name">3: Understanding the Problem by Understanding the Data</div>
                            </a>
                        </li>
                        <div id="collapse3" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="3" class="sub-nav">
                                <a href="#ch03">
                                    <div class="section-name">Chapter 3: Understanding the Problem by Understanding the Data</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec24" class="sub-nav">
                                <a href="#ch03lvl1sec24">                    
                                    <div class="section-name">Analyzing and preparing your data</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec25" class="sub-nav">
                                <a href="#ch03lvl1sec25">                    
                                    <div class="section-name">Resilient Distributed Dataset basics</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec26" class="sub-nav">
                                <a href="#ch03lvl1sec26">                    
                                    <div class="section-name">Dataset basics</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec27" class="sub-nav">
                                <a href="#ch03lvl1sec27">                    
                                    <div class="section-name">Dataset from string and typed class</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec28" class="sub-nav">
                                <a href="#ch03lvl1sec28">                    
                                    <div class="section-name">Spark and data scientists workflow</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec29" class="sub-nav">
                                <a href="#ch03lvl1sec29">                    
                                    <div class="section-name">Deeper into Spark</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec30" class="sub-nav">
                                <a href="#ch03lvl1sec30">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse4">
                                <div class="section-name">4: Extracting Knowledge through Feature Engineering</div>
                            </a>
                        </li>
                        <div id="collapse4" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="4" class="sub-nav">
                                <a href="#ch04">
                                    <div class="section-name">Chapter 4: Extracting Knowledge through Feature Engineering</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec31" class="sub-nav">
                                <a href="#ch04lvl1sec31">                    
                                    <div class="section-name">The state of the art of feature engineering</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec32" class="sub-nav">
                                <a href="#ch04lvl1sec32">                    
                                    <div class="section-name">Best practices in feature engineering</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec33" class="sub-nav">
                                <a href="#ch04lvl1sec33">                    
                                    <div class="section-name">Feature engineering with Spark</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec34" class="sub-nav">
                                <a href="#ch04lvl1sec34">                    
                                    <div class="section-name">Advanced feature engineering</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec35" class="sub-nav">
                                <a href="#ch04lvl1sec35">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse5">
                                <div class="section-name">5: Supervised and Unsupervised Learning by Examples</div>
                            </a>
                        </li>
                        <div id="collapse5" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="5" class="sub-nav">
                                <a href="#ch05">
                                    <div class="section-name">Chapter 5: Supervised and Unsupervised Learning by Examples</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec36" class="sub-nav">
                                <a href="#ch05lvl1sec36">                    
                                    <div class="section-name">Machine learning classes</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec37" class="sub-nav">
                                <a href="#ch05lvl1sec37">                    
                                    <div class="section-name">Supervised learning with Spark - an example</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec38" class="sub-nav">
                                <a href="#ch05lvl1sec38">                    
                                    <div class="section-name">Unsupervised learning</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec39" class="sub-nav">
                                <a href="#ch05lvl1sec39">                    
                                    <div class="section-name">Recommender system</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec40" class="sub-nav">
                                <a href="#ch05lvl1sec40">                    
                                    <div class="section-name">Advanced learning and generalizations</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec41" class="sub-nav">
                                <a href="#ch05lvl1sec41">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse6">
                                <div class="section-name">6: Building Scalable Machine Learning Pipelines</div>
                            </a>
                        </li>
                        <div id="collapse6" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="6" class="sub-nav">
                                <a href="#ch06">
                                    <div class="section-name">Chapter 6: Building Scalable Machine Learning Pipelines</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec42" class="sub-nav">
                                <a href="#ch06lvl1sec42">                    
                                    <div class="section-name">Spark machine learning pipeline APIs</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec43" class="sub-nav">
                                <a href="#ch06lvl1sec43">                    
                                    <div class="section-name">Cancer-diagnosis pipeline with Spark</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec44" class="sub-nav">
                                <a href="#ch06lvl1sec44">                    
                                    <div class="section-name">Cancer-prognosis pipeline with Spark</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec45" class="sub-nav">
                                <a href="#ch06lvl1sec45">                    
                                    <div class="section-name">Market basket analysis with Spark Core</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec46" class="sub-nav">
                                <a href="#ch06lvl1sec46">                    
                                    <div class="section-name">OCR pipeline with Spark</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec47" class="sub-nav">
                                <a href="#ch06lvl1sec47">                    
                                    <div class="section-name">Topic modeling using Spark MLlib and ML</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec48" class="sub-nav">
                                <a href="#ch06lvl1sec48">                    
                                    <div class="section-name">Credit risk analysis pipeline with Spark</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec49" class="sub-nav">
                                <a href="#ch06lvl1sec49">                    
                                    <div class="section-name">Scaling the ML pipelines</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec50" class="sub-nav">
                                <a href="#ch06lvl1sec50">                    
                                    <div class="section-name">Tips and performance considerations</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec51" class="sub-nav">
                                <a href="#ch06lvl1sec51">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse7">
                                <div class="section-name">7: Tuning Machine Learning Models</div>
                            </a>
                        </li>
                        <div id="collapse7" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="7" class="sub-nav">
                                <a href="#ch07">
                                    <div class="section-name">Chapter 7: Tuning Machine Learning Models</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec52" class="sub-nav">
                                <a href="#ch07lvl1sec52">                    
                                    <div class="section-name">Details about machine learning model tuning</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec53" class="sub-nav">
                                <a href="#ch07lvl1sec53">                    
                                    <div class="section-name">Typical challenges in model tuning</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec54" class="sub-nav">
                                <a href="#ch07lvl1sec54">                    
                                    <div class="section-name">Evaluating machine learning models</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec55" class="sub-nav">
                                <a href="#ch07lvl1sec55">                    
                                    <div class="section-name">Validation and evaluation techniques</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec56" class="sub-nav">
                                <a href="#ch07lvl1sec56">                    
                                    <div class="section-name">Parameter tuning for machine learning models</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec57" class="sub-nav">
                                <a href="#ch07lvl1sec57">                    
                                    <div class="section-name">Hypothesis testing</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec58" class="sub-nav">
                                <a href="#ch07lvl1sec58">                    
                                    <div class="section-name">Machine learning model selection</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec59" class="sub-nav">
                                <a href="#ch07lvl1sec59">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse8">
                                <div class="section-name">8: Adapting Your Machine Learning Models</div>
                            </a>
                        </li>
                        <div id="collapse8" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="8" class="sub-nav">
                                <a href="#ch08">
                                    <div class="section-name">Chapter 8: Adapting Your Machine Learning Models</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec60" class="sub-nav">
                                <a href="#ch08lvl1sec60">                    
                                    <div class="section-name">Adapting machine learning models</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec61" class="sub-nav">
                                <a href="#ch08lvl1sec61">                    
                                    <div class="section-name">The generalization of ML models</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec62" class="sub-nav">
                                <a href="#ch08lvl1sec62">                    
                                    <div class="section-name">Adapting through incremental algorithms</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec63" class="sub-nav">
                                <a href="#ch08lvl1sec63">                    
                                    <div class="section-name">Adapting through reusing ML models</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec64" class="sub-nav">
                                <a href="#ch08lvl1sec64">                    
                                    <div class="section-name">Machine learning in dynamic environments</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec65" class="sub-nav">
                                <a href="#ch08lvl1sec65">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse9">
                                <div class="section-name">9: Advanced Machine Learning with Streaming and Graph Data</div>
                            </a>
                        </li>
                        <div id="collapse9" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="9" class="sub-nav">
                                <a href="#ch09">
                                    <div class="section-name">Chapter 9: Advanced Machine Learning with Streaming and Graph Data</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec66" class="sub-nav">
                                <a href="#ch09lvl1sec66">                    
                                    <div class="section-name">Developing real-time ML pipelines</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec67" class="sub-nav">
                                <a href="#ch09lvl1sec67">                    
                                    <div class="section-name">Time series and social network analysis</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec68" class="sub-nav">
                                <a href="#ch09lvl1sec68">                    
                                    <div class="section-name">Movie recommendation using Spark</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec69" class="sub-nav">
                                <a href="#ch09lvl1sec69">                    
                                    <div class="section-name">Developing a real-time ML pipeline from streaming</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec70" class="sub-nav">
                                <a href="#ch09lvl1sec70">                    
                                    <div class="section-name">ML pipeline on graph data and semi-supervised graph-based learning</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec71" class="sub-nav">
                                <a href="#ch09lvl1sec71">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse10">
                                <div class="section-name">10: Configuring and Working with External Libraries</div>
                            </a>
                        </li>
                        <div id="collapse10" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="10" class="sub-nav">
                                <a href="#ch10">
                                    <div class="section-name">Chapter 10: Configuring and Working with External Libraries</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec72" class="sub-nav">
                                <a href="#ch10lvl1sec72">                    
                                    <div class="section-name">Third-party ML libraries with Spark</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec73" class="sub-nav">
                                <a href="#ch10lvl1sec73">                    
                                    <div class="section-name">Using external libraries with Spark Core</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec74" class="sub-nav">
                                <a href="#ch10lvl1sec74">                    
                                    <div class="section-name">Time series analysis using the Cloudera Spark-TS package</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec75" class="sub-nav">
                                <a href="#ch10lvl1sec75">                    
                                    <div class="section-name">Configuring SparkR with RStudio</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec76" class="sub-nav">
                                <a href="#ch10lvl1sec76">                    
                                    <div class="section-name">Configuring Hadoop run-time on Windows</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec77" class="sub-nav">
                                <a href="#ch10lvl1sec77">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                    </div>
                </div>
            </ul>
        </div>
        
        <div id="page-content-wrapper" class="book-page">
            <a href="#" id="menu-toggle" class="toggle-nav"><i class="fa fa-bars fa-2x mr5"></i></a>
            
            <a href="#" id="back_to_top" class="back-to-top"><img src="../../mapt/images/kopimi.svg"></a>
            
            <div class="col-sm-12 col-xl-offset-2 col-xl-8 col-lg-offset-1 col-lg-10">
                <div class="btn-group pull-right mt15 mb30" role="group">
                    <a href="#home" class="btn btn-default">
                        <i class="fa fa-share fa-lg no-text-padding"></i>
                        <span class="hidden-xs ml5">Book Home</span>
                    </a>
                    <button class="btn btn-default" data-nid="25988" id="code-download">
                        <i class="fa fa-file fa-lg"></i>
                        <span class="hidden-xs ml5">Download Code Files</span>
                    </button>
                </div>
            </div>
            <div class="clearfix"></div>
            
            <div id="book-wrapper" class="container-fluid">
                <div class="col-sm-12 col-xl-offset-2 col-xl-8 col-lg-offset-1 col-lg-10" id="home">
                    <h2 class="product-title">Large Scale Machine Learning with Spark</h2>
                    <hr>
                    <div class="row">
                        <div class="col-sm-12">
                            <h5 class="mt10">By Md. Rezaul Karim, Md. Mahedi Kaysar</h5>
                            <div>
                                <p class="mb20"><b>Discover everything you need to build robust machine learning applications with Spark 2.0</b></p>
                                <a href="#ch01" class="btn btn-info btn-lg pull-right hidden-xs">
                                    Start Reading <i class="fa fa-chevron-right ml5"></i>
                                </a>
                                <a href="#ch01" class="btn btn-info btn-lg btn-block mt20 mb20 visible-xs">
                                    Start Reading <i class="fa fa-chevron-right ml5"></i>
                                </a>
                                <div class="clearfix"></div>
                                <div class="col-sm-12">
                                    <ul id="myTabs" class="nav nav-tabs nav-justified hidden-xs mt20" role="tablist">
                                        <li class="active">
                                            <a href="#info" role="tab" id="info-tab" data-toggle="tab">
                                                <h5>Info</h5>
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#content" role="tab" id="content-tab" data-toggle="tab">
                                                <h5>Contents</h5>
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#author" role="tab" id="author-tab" data-toggle="tab">
                                                <h5>Author</h5>
                                            </a>
                                        </li>
                                    </ul>
                
                                    <ul id="myTabsMobile" class="nav nav-pills text-center nav-stacked visible-xs mb60" role="tablist">
                                        <li class="active">
                                            <a href="#info" role="tab" id="info-tab-responsive" data-toggle="tab">
                                                Info
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#content" role="tab" id="content-tab-responsive" data-toggle="tab">
                                                Contents
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#author" role="tab" id="author-tab-responsive" data-toggle="tab">
                                                Author
                                            </a>
                                        </li>
                                    </ul>
                
                                    <div id="myTabContent" class="tab-content pt30">
                                    
                                        <div role="tabpanel" class="tab-pane active fade in" id="info">
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">Features</h5>
                                            </div>
                                            <div class="hidden-xs">
                                                <h5>Features</h5>
                                            </div>
                                            <hr>
                                            <div>
                                                <ul>
                <li>Get the most up-to-date book on the market that focuses on design, engineering, and scalable solutions in machine learning with Spark 2.0.0</li>
                <li>Use Spark’s machine learning library in a big data environment</li>
                <li>You will learn how to develop high-value applications at scale with ease and a develop a personalized design</li>
                </ul>
                                            </div>
                                            <br>
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">Learning</h5>
                                            </div>
                                            <div class="hidden-xs">
                                                <h5>Learning</h5>
                                            </div>
                                            <hr>
                                            <div>
                                                <ul>
                <li>Get solid theoretical understandings of ML algorithms</li>
                <li>Configure Spark on cluster and cloud infrastructure to develop applications using Scala, Java, Python, and R</li>
                <li>Scale up ML applications on large cluster or cloud infrastructures</li>
                <li>Use Spark ML and MLlib to develop ML pipelines with recommendation system, classification, regression, clustering, sentiment analysis, and dimensionality reduction</li>
                <li>Handle large texts for developing ML applications with strong focus on feature engineering</li>
                <li>Use Spark Streaming to develop ML applications for real-time streaming</li>
                <li>Tune ML models with cross-validation, hyperparameters tuning and train split</li>
                <li>Enhance ML models to make them adaptable for new data in dynamic and incremental environments</li>
                </ul>
                                            </div>
                                            <br>
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">About</h5>
                                            </div>
                                            <div class="hidden-xs">
                                                <h5>About</h5>
                                            </div>
                                            <hr>
                                            <div>
                                                <p>Data processing, implementing related algorithms, tuning, scaling up and finally deploying are some crucial steps in the process of optimising any application.</p>
                <p>Spark is capable of handling large-scale batch and streaming data to figure out when to cache data in memory and processing them up to 100 times faster than Hadoop-based MapReduce.This means predictive analytics can be applied to streaming and batch to develop complete machine learning (ML) applications a lot quicker, making Spark an ideal candidate for large data-intensive applications.</p>
                <p>This book focuses on design engineering and scalable solutions using ML with Spark. First, you will learn how to install Spark with all new features from the latest Spark 2.0 release. Moving on, you’ll explore important concepts such as advanced feature engineering with RDD and Datasets. After studying developing and deploying applications, you will see how to use external libraries with Spark.</p>
                <p>In summary, you will be able to develop complete and personalised ML applications from data collections,model building, tuning, and scaling up to deploying on a cluster or the cloud.</p>
                                            </div>
                                        </div>
                                        
                                        <div role="tabpanel" class="tab-pane fade in" id="content">
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">Contents</h5>
                                                <hr>
                                            </div>
                                            <ul>
                                                <div>
                                                    <li data-chapter="1">
                                                        <div class="section-name">1: Introduction to Data Analytics with Spark</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="1" class="chapter-section">
                                                                    <a href="#ch01">        
                                                                        <div class="section-name">Chapter 1: Introduction to Data Analytics with Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec7" class="chapter-section">
                                                                    <a href="#ch01lvl1sec7">                    
                                                                        <div class="section-name">Spark overview</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec8" class="chapter-section">
                                                                    <a href="#ch01lvl1sec8">                    
                                                                        <div class="section-name">New computing paradigm with Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec9" class="chapter-section">
                                                                    <a href="#ch01lvl1sec9">                    
                                                                        <div class="section-name">Spark ecosystem</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec10" class="chapter-section">
                                                                    <a href="#ch01lvl1sec10">                    
                                                                        <div class="section-name">Spark machine learning libraries</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec11" class="chapter-section">
                                                                    <a href="#ch01lvl1sec11">                    
                                                                        <div class="section-name">Installing and getting started with Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec12" class="chapter-section">
                                                                    <a href="#ch01lvl1sec12">                    
                                                                        <div class="section-name">Packaging your application with dependencies</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec13" class="chapter-section">
                                                                    <a href="#ch01lvl1sec13">                    
                                                                        <div class="section-name">Running a sample machine learning application</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec14" class="chapter-section">
                                                                    <a href="#ch01lvl1sec14">                    
                                                                        <div class="section-name">References</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec15" class="chapter-section">
                                                                    <a href="#ch01lvl1sec15">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="2">
                                                        <div class="section-name">2: Machine Learning Best Practices</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="2" class="chapter-section">
                                                                    <a href="#ch02">        
                                                                        <div class="section-name">Chapter 2: Machine Learning Best Practices</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec16" class="chapter-section">
                                                                    <a href="#ch02lvl1sec16">                    
                                                                        <div class="section-name">What is machine learning?</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec17" class="chapter-section">
                                                                    <a href="#ch02lvl1sec17">                    
                                                                        <div class="section-name">Machine learning tasks</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec18" class="chapter-section">
                                                                    <a href="#ch02lvl1sec18">                    
                                                                        <div class="section-name">Practical machine learning problems</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec19" class="chapter-section">
                                                                    <a href="#ch02lvl1sec19">                    
                                                                        <div class="section-name">Most widely used machine learning problems</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec20" class="chapter-section">
                                                                    <a href="#ch02lvl1sec20">                    
                                                                        <div class="section-name">Large scale machine learning APIs in Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec21" class="chapter-section">
                                                                    <a href="#ch02lvl1sec21">                    
                                                                        <div class="section-name">Practical machine learning best practices</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec22" class="chapter-section">
                                                                    <a href="#ch02lvl1sec22">                    
                                                                        <div class="section-name">Choosing the right algorithm for your application</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec23" class="chapter-section">
                                                                    <a href="#ch02lvl1sec23">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="3">
                                                        <div class="section-name">3: Understanding the Problem by Understanding the Data</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="3" class="chapter-section">
                                                                    <a href="#ch03">        
                                                                        <div class="section-name">Chapter 3: Understanding the Problem by Understanding the Data</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec24" class="chapter-section">
                                                                    <a href="#ch03lvl1sec24">                    
                                                                        <div class="section-name">Analyzing and preparing your data</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec25" class="chapter-section">
                                                                    <a href="#ch03lvl1sec25">                    
                                                                        <div class="section-name">Resilient Distributed Dataset basics</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec26" class="chapter-section">
                                                                    <a href="#ch03lvl1sec26">                    
                                                                        <div class="section-name">Dataset basics</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec27" class="chapter-section">
                                                                    <a href="#ch03lvl1sec27">                    
                                                                        <div class="section-name">Dataset from string and typed class</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec28" class="chapter-section">
                                                                    <a href="#ch03lvl1sec28">                    
                                                                        <div class="section-name">Spark and data scientists workflow</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec29" class="chapter-section">
                                                                    <a href="#ch03lvl1sec29">                    
                                                                        <div class="section-name">Deeper into Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec30" class="chapter-section">
                                                                    <a href="#ch03lvl1sec30">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="4">
                                                        <div class="section-name">4: Extracting Knowledge through Feature Engineering</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="4" class="chapter-section">
                                                                    <a href="#ch04">        
                                                                        <div class="section-name">Chapter 4: Extracting Knowledge through Feature Engineering</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec31" class="chapter-section">
                                                                    <a href="#ch04lvl1sec31">                    
                                                                        <div class="section-name">The state of the art of feature engineering</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec32" class="chapter-section">
                                                                    <a href="#ch04lvl1sec32">                    
                                                                        <div class="section-name">Best practices in feature engineering</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec33" class="chapter-section">
                                                                    <a href="#ch04lvl1sec33">                    
                                                                        <div class="section-name">Feature engineering with Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec34" class="chapter-section">
                                                                    <a href="#ch04lvl1sec34">                    
                                                                        <div class="section-name">Advanced feature engineering</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec35" class="chapter-section">
                                                                    <a href="#ch04lvl1sec35">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="5">
                                                        <div class="section-name">5: Supervised and Unsupervised Learning by Examples</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="5" class="chapter-section">
                                                                    <a href="#ch05">        
                                                                        <div class="section-name">Chapter 5: Supervised and Unsupervised Learning by Examples</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec36" class="chapter-section">
                                                                    <a href="#ch05lvl1sec36">                    
                                                                        <div class="section-name">Machine learning classes</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec37" class="chapter-section">
                                                                    <a href="#ch05lvl1sec37">                    
                                                                        <div class="section-name">Supervised learning with Spark - an example</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec38" class="chapter-section">
                                                                    <a href="#ch05lvl1sec38">                    
                                                                        <div class="section-name">Unsupervised learning</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec39" class="chapter-section">
                                                                    <a href="#ch05lvl1sec39">                    
                                                                        <div class="section-name">Recommender system</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec40" class="chapter-section">
                                                                    <a href="#ch05lvl1sec40">                    
                                                                        <div class="section-name">Advanced learning and generalizations</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec41" class="chapter-section">
                                                                    <a href="#ch05lvl1sec41">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="6">
                                                        <div class="section-name">6: Building Scalable Machine Learning Pipelines</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="6" class="chapter-section">
                                                                    <a href="#ch06">        
                                                                        <div class="section-name">Chapter 6: Building Scalable Machine Learning Pipelines</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec42" class="chapter-section">
                                                                    <a href="#ch06lvl1sec42">                    
                                                                        <div class="section-name">Spark machine learning pipeline APIs</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec43" class="chapter-section">
                                                                    <a href="#ch06lvl1sec43">                    
                                                                        <div class="section-name">Cancer-diagnosis pipeline with Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec44" class="chapter-section">
                                                                    <a href="#ch06lvl1sec44">                    
                                                                        <div class="section-name">Cancer-prognosis pipeline with Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec45" class="chapter-section">
                                                                    <a href="#ch06lvl1sec45">                    
                                                                        <div class="section-name">Market basket analysis with Spark Core</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec46" class="chapter-section">
                                                                    <a href="#ch06lvl1sec46">                    
                                                                        <div class="section-name">OCR pipeline with Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec47" class="chapter-section">
                                                                    <a href="#ch06lvl1sec47">                    
                                                                        <div class="section-name">Topic modeling using Spark MLlib and ML</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec48" class="chapter-section">
                                                                    <a href="#ch06lvl1sec48">                    
                                                                        <div class="section-name">Credit risk analysis pipeline with Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec49" class="chapter-section">
                                                                    <a href="#ch06lvl1sec49">                    
                                                                        <div class="section-name">Scaling the ML pipelines</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec50" class="chapter-section">
                                                                    <a href="#ch06lvl1sec50">                    
                                                                        <div class="section-name">Tips and performance considerations</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec51" class="chapter-section">
                                                                    <a href="#ch06lvl1sec51">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="7">
                                                        <div class="section-name">7: Tuning Machine Learning Models</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="7" class="chapter-section">
                                                                    <a href="#ch07">        
                                                                        <div class="section-name">Chapter 7: Tuning Machine Learning Models</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec52" class="chapter-section">
                                                                    <a href="#ch07lvl1sec52">                    
                                                                        <div class="section-name">Details about machine learning model tuning</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec53" class="chapter-section">
                                                                    <a href="#ch07lvl1sec53">                    
                                                                        <div class="section-name">Typical challenges in model tuning</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec54" class="chapter-section">
                                                                    <a href="#ch07lvl1sec54">                    
                                                                        <div class="section-name">Evaluating machine learning models</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec55" class="chapter-section">
                                                                    <a href="#ch07lvl1sec55">                    
                                                                        <div class="section-name">Validation and evaluation techniques</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec56" class="chapter-section">
                                                                    <a href="#ch07lvl1sec56">                    
                                                                        <div class="section-name">Parameter tuning for machine learning models</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec57" class="chapter-section">
                                                                    <a href="#ch07lvl1sec57">                    
                                                                        <div class="section-name">Hypothesis testing</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec58" class="chapter-section">
                                                                    <a href="#ch07lvl1sec58">                    
                                                                        <div class="section-name">Machine learning model selection</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec59" class="chapter-section">
                                                                    <a href="#ch07lvl1sec59">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="8">
                                                        <div class="section-name">8: Adapting Your Machine Learning Models</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="8" class="chapter-section">
                                                                    <a href="#ch08">        
                                                                        <div class="section-name">Chapter 8: Adapting Your Machine Learning Models</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec60" class="chapter-section">
                                                                    <a href="#ch08lvl1sec60">                    
                                                                        <div class="section-name">Adapting machine learning models</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec61" class="chapter-section">
                                                                    <a href="#ch08lvl1sec61">                    
                                                                        <div class="section-name">The generalization of ML models</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec62" class="chapter-section">
                                                                    <a href="#ch08lvl1sec62">                    
                                                                        <div class="section-name">Adapting through incremental algorithms</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec63" class="chapter-section">
                                                                    <a href="#ch08lvl1sec63">                    
                                                                        <div class="section-name">Adapting through reusing ML models</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec64" class="chapter-section">
                                                                    <a href="#ch08lvl1sec64">                    
                                                                        <div class="section-name">Machine learning in dynamic environments</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec65" class="chapter-section">
                                                                    <a href="#ch08lvl1sec65">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="9">
                                                        <div class="section-name">9: Advanced Machine Learning with Streaming and Graph Data</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="9" class="chapter-section">
                                                                    <a href="#ch09">        
                                                                        <div class="section-name">Chapter 9: Advanced Machine Learning with Streaming and Graph Data</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec66" class="chapter-section">
                                                                    <a href="#ch09lvl1sec66">                    
                                                                        <div class="section-name">Developing real-time ML pipelines</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec67" class="chapter-section">
                                                                    <a href="#ch09lvl1sec67">                    
                                                                        <div class="section-name">Time series and social network analysis</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec68" class="chapter-section">
                                                                    <a href="#ch09lvl1sec68">                    
                                                                        <div class="section-name">Movie recommendation using Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec69" class="chapter-section">
                                                                    <a href="#ch09lvl1sec69">                    
                                                                        <div class="section-name">Developing a real-time ML pipeline from streaming</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec70" class="chapter-section">
                                                                    <a href="#ch09lvl1sec70">                    
                                                                        <div class="section-name">ML pipeline on graph data and semi-supervised graph-based learning</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec71" class="chapter-section">
                                                                    <a href="#ch09lvl1sec71">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="10">
                                                        <div class="section-name">10: Configuring and Working with External Libraries</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="10" class="chapter-section">
                                                                    <a href="#ch10">        
                                                                        <div class="section-name">Chapter 10: Configuring and Working with External Libraries</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec72" class="chapter-section">
                                                                    <a href="#ch10lvl1sec72">                    
                                                                        <div class="section-name">Third-party ML libraries with Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec73" class="chapter-section">
                                                                    <a href="#ch10lvl1sec73">                    
                                                                        <div class="section-name">Using external libraries with Spark Core</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec74" class="chapter-section">
                                                                    <a href="#ch10lvl1sec74">                    
                                                                        <div class="section-name">Time series analysis using the Cloudera Spark-TS package</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec75" class="chapter-section">
                                                                    <a href="#ch10lvl1sec75">                    
                                                                        <div class="section-name">Configuring SparkR with RStudio</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec76" class="chapter-section">
                                                                    <a href="#ch10lvl1sec76">                    
                                                                        <div class="section-name">Configuring Hadoop run-time on Windows</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec77" class="chapter-section">
                                                                    <a href="#ch10lvl1sec77">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                            </ul>
                                        </div>
                                        
                                        <div role="tabpanel" class="tab-pane fade" id="author">
                                            <div class="visible-xs">
                                                <h4 class="mobile-title">About the Author</h4>
                                                <hr>
                                            </div>
                                            <p><strong>Md. Rezaul Karim</strong></p>
                                            <div>
                                                <p>Md. Rezaul Karim has more than 8 years of experience in the area of research and development with a solid knowledge of algorithms and data structures, focusing C, C++, Java, R, and Python and big data technologies such as Spark, Kafka, DC/OS, Docker, Mesos, Hadoop, and MapReduce.</p>
                <p>He was first enchanted by machine learning while studying an Advanced Artificial Intelligence post-graduate course by applying the combined technique of Hadoop-based MapReduce and machine learning together for market basket analysis on large-scale business-oriented transactional databases in back 2010. Consequently, his research interests include machine learning, data mining, Semantic Web, big data, and bioinformatics. He has published more than 30 research papers in renowned peer-reviewed international journals and conferences focusing on the areas of data mining, machine learning, and bioinformatics, with good citations.</p>
                <p>He is a Software Engineer and Researcher currently working at the Insight Centre for Data Analytics, Ireland (the largest data analytics center in Ireland and the largest Semantic Web research institute in the world) as a PhD Researcher. He is also a PhD candidate at the National University of Ireland, Galway. He also holds an ME (Master of Engineering) degree in Computer Engineering from the Kyung Hee University, Korea, majoring in data mining and knowledge discovery. And he has a BS (Bachelor of Science) degree in Computer Science from the University of Dhaka, Bangladesh.</p>
                <p>Before joining the Insight Center for Data Analytics, he had been working as a Lead Software Engineer with Samsung Electronics, where he worked with the distributed Samsung R&amp;D centers across the world, including Korea, India, Vietnam, Turkey, UAE, Brazil, and Bangladesh. Before that, he worked as a Graduate Research Assistant in the Database Lab at Kyung Hee University, Korea, while working towards his Master's degree. He also worked as an R&amp;D Engineer with BMTech21 Worldwide, Korea. Even before that, he worked as a Software Engineer with i2SoftTechnology, Dhaka, Bangladesh.</p>
                                            </div>
                                            <p><strong>Md. Mahedi Kaysar</strong></p>
                                            <div>
                                                <p>Md. Mahedi Kaysar is a Software Engineer and Researcher at the Insight Center for Data Analytics (the largest data analytics center across the Ireland and the largest semantic web research institute in the world), Dublin City University (DCU), Ireland. Before joining the Insight Center at DCU, he worked as a Software Engineer at the Insight Center for Data Analytics, National University of Ireland, Galway and Samsung Electronics, Bangladesh.</p>
                <p>He has more than 5 years of experience in research and development with a strong background in algorithms and data structures concentrating on C, Java, Scala, and Python. He has lots of experience in enterprise application development and big data analytics.</p>
                <p>He obtained a BSc in Computer Science and Engineering from the Chittagong University of Engineering and Technology, Bangladesh. Now, he has started his postgraduate research in Distributed and Parallel Computing at the Dublin City University, Ireland.</p>
                <p>His research interests include Distributed Computing, Semantic Web, Linked Data, big data, Internet of Everything, and machine learning. Moreover, he was involved in a research project in collaboration with CISCO Systems Inc. in the area of Internet of Everything and Semantic Web Technologies. His duties were to develop an IoT-enabled meeting management system, a scalable system for stream processing, designing, and showcasing the use cases of a project.</p>
                                            </div>
                                        </div>
                                        
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    
                    <div class="next-wrapper">
                        <div class="row ns">
                            <hr />
                            <span class="hidden-xs">
                                <h4 class="pull-left">
                                    <strong>Up Next: </strong><span class="section-title"></span>
                                </h4>
                                <a href="#" class="btn btn-primary pull-right btn-lg">
                                    Next Section
                                </a>
                            </span>
                            <span class="visible-xs">
                                <a href="#" class="btn btn-primary btn-block btn-lg">
                                    Next Section
                                </a>
                            </span>
                        </div>
                        <div class="row ns">
                            <hr>
                        </div>
                    </div>
                </div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch01"></a>Chapter 1. Introduction to Data Analytics with Spark</h2></div></div></div><p>This chapter covers an overview of Apache Spark, its computing paradigm, and installation to getting started. It will briefly describe the main components of Spark and focus on its new computing advancements. A description of the <span class="strong"><strong>Resilient Distributed Datasets </strong></span>(<span class="strong"><strong>RDD</strong></span>) and Dataset will be discussed as a base knowledge for the rest of this book. It will then focus on the Spark machine learning libraries. Installing and packaging a simple machine learning application with Spark and Maven will be demonstrated then before getting on board. In a nutshell, the following topics will be covered in this chapter:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Spark overview</p></li><li style="list-style-type: disc"><p>New computing paradigm with Spark</p></li><li style="list-style-type: disc"><p>Spark ecosystem</p></li><li style="list-style-type: disc"><p>Spark machine learning libraries</p></li><li style="list-style-type: disc"><p>Installing and getting started with Spark</p></li><li style="list-style-type: disc"><p>Packaging your application with dependencies</p></li><li style="list-style-type: disc"><p>Running a simple machine learning application</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec7"></a>Spark overview</h2></div></div><hr /></div><p>This section describes Spark (<a class="ulink" href="https://spark.apache.org/" target="_blank">https://spark.apache.org/</a>) basics followed by the issues with the traditional parallel and distributed computing, then how Spark was evolved, and it then brings a new computing paradigm across the big data processing and analytics on top of that. In addition, we also presented some exciting features of Spark that easily attract the big data engineers, data scientists, and researchers, including:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Simplicity of data processing and computation</p></li><li style="list-style-type: disc"><p>Speed of computation</p></li><li style="list-style-type: disc"><p>Scalability and throughput across large-scale datasets</p></li><li style="list-style-type: disc"><p>Sophistication across diverse data types</p></li><li style="list-style-type: disc"><p>Ease of cluster computing and deployment with different cluster managers</p></li><li style="list-style-type: disc"><p>Working capabilities and supports with various big data storage and sources</p></li><li style="list-style-type: disc"><p>Diverse APIs are written in widely used and emerging programming languages</p></li></ul></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec5"></a>Spark basics</h3></div></div></div><p>Before praising Spark and its many virtues, a short overview is in the mandate. Apache Spark is a fast, in-memory, big data processing, and general-purpose cluster computing framework with a bunch of sophisticated APIs for advanced data analytics. Unlike the Hadoop-based MapReduce, which is only suited for batch jobs in speed and ease of use, Spark could be considered as a general execution engine that is suitable for applying advanced analytics on both static (batch) as well as real-time data:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Spark was originally developed at the University of California, Berkeley's AMPLab based on <span class="strong"><strong>Resilient Distributed Datasets</strong></span> (<span class="strong"><strong>RDDs</strong></span>), which provides a fault-tolerant abstraction for in-memory cluster computing facilities. However, later on Spark's code base was bequeathed to the Apache Software Foundation making it open source, since then open source communities are taking care of it. Spark provides an interface to perform data analytics on entire clusters at scale with implicit data parallelism and fault-tolerance through its high-level APIs written in Java, Scala, Python, and R.</p></li></ul></div><p>In Spark 2.0.0, elevated libraries (most widely used data analysis algorithms) are implemented, including:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Spark SQL for querying and processing large-scale structured data</p></li><li style="list-style-type: disc"><p>SparkR for statistical computing that provides distributed computing using programming language R at scale</p></li><li style="list-style-type: disc"><p>MLlib for machine learning (ML) applications, which is internally divided into two parts; MLlib for RDD-based machine learning application development and Spark ML for a high-level abstraction to develop complete computational data science and machine learning workflows</p></li><li style="list-style-type: disc"><p>GraphX for large-scale graph data processing</p></li><li style="list-style-type: disc"><p>Spark Streaming for handling large-scale real-time streaming data to provide a dynamic working environment to static machine learning</p></li></ul></div><p>Since its first stable release, Spark has already experienced dramatic and rapid development as well as wide adoptions through active initiatives from a wide range of IT solution providers, open source communities, and researchers around the world. Recently it has emerged as one of the most active, and the largest open source project in the area of big data processing and cluster computing, not only for its comprehensive adoptions, but also deployments and surveys by IT peoples, data scientists, and big data engineers worldwide. As quoted by <span class="emphasis"><em>Matei Zaharia</em></span>, founder of Spark and the CTO of <span class="emphasis"><em>Databricks</em></span> on the <span class="emphasis"><em>Big Data analytics</em></span> news website at: <a class="ulink" href="http://bigdataanalyticsnews.com/apache-spark-3-real-world-use-cases/" target="_blank">http://bigdataanalyticsnews.com/apache-spark-3-real-world-use-cases/</a>:</p><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>It's an interesting thing. There hasn't been as much noise about it commercially, but the actual developer community votes with its feet and people are actually getting things done and working with the project.</em></span></p></blockquote></div><p>Even though many Tech Giants such as Yahoo, Baidu, Conviva, ClearStory, Hortonworks, Gartner, and Tencent are already using Spark in production - on the other hand, IBM, DataStax, Cloudera, and BlueData provide the commercialized Spark distribution for the enterprise. These companies have enthusiastically deployed Spark applications at a massive scale collectively for processing multiple petabytes of data on clusters of 8,000 nodes, which is the largest known cluster of Spark.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec6"></a>Beauties of Spark</h3></div></div></div><p>Are you planning to develop a machine learning (ML) application? If so, you probably already have some data to perform preprocessing before you train a model on that data, and finally, you will be using the trained model to make predictions on new data to see the adaptability. That's all you need? We guess no, since you have to consider other parameters as well. Obviously, you will desire your ML models to be working perfectly in terms of accuracy, execution time, memory usage, throughput, tuning, and adaptability. Wait! Still not done yet; what happens if you would like to make your application handle large training and new datasets at scale? Or as a data scientist, what if you could build your ML models to overcome these issues as a multi-step journey from data incorporation through train and error to production by running the same machine learning code on the big cluster and the personal computer without breaking down further? You can simply rely on Spark and close your eyes.</p><p>Spark has several advantages over other big data technologies such as MapReduce (you can refer to <a class="ulink" href="https://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html" target="_blank">https://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html</a> for MapReduce tutorials and the research paper <span class="emphasis"><em>MapReduce: Simplified Data Processing on Large Clusters, Jeffrey Dean et al, In proc of OSDI, 2004</em></span> to get to know more) and Storm, which is a free and open source distributed real-time computation system (please refer to <a class="ulink" href="http://storm.apache.org/" target="_blank">http://storm.apache.org/</a> for more on Storm-based distributed computing). First of all, Spark gives a comprehensive, unified engine to manage big data processing requirements with a variety of datasets such as text and tabular to graph data as well as the source of data (batch and real-time streaming data) that are diverse in nature. As a user (data science engineers, academicians, or developers), you can be likely benefited from Spark's rapid application development through simple and easy-to-understand APIs across batches, interactive, and real-time streaming applications.</p><p>Working and programming with Spark is easy and simple. Let us show you an example of that. Yahoo is one of the contributors and an early adopter of Spark, who implemented an ML algorithm with 120 lines of Scala code. With just 30 minutes of training on a large dataset with a hundred million records, the Scala ML algorithm was ready for business. Surprisingly, the same algorithm was written using C++ in 15,000 lines of code previously (please refer to the following URL for more at: <a class="ulink" href="https://www.datanami.com/2014/03/06/apache_spark_3_real-world_use_cases/" target="_blank">https://www.datanami.com/2014/03/06/apache_spark_3_real-world_use_cases/</a>). You can develop your applications using Java, Scala, R, or Python with a built-in set of over 100 high-level operators (mostly supported after Spark release 1.6.1) for transforming datasets and getting the familiarity with the data frame APIs for manipulating semi-structured, structured, and streaming data. In addition to the Map and Reduce operations, it supports SQL queries, streaming data, machine learning, and graph data processing. Moreover, Spark also provides an interactive shell written in Scala and Python for executing your codes sequentially (such as SQL or R style).</p><p>The main reason Spark adopts so quickly is because of two main factors: speed and sophistication. Spark provides order-of-magnitude performance for many applications using coarse-grained, immutable, and sophisticated data called Resilient Distributed Datasets that are spread across the cluster and that can be stored in memory or disks. An RDD offers fault-tolerance, which is resilient in a sense that it cannot be changed once created. Moreover, Spark's RDD has the property of recreating from its lineage if it is lost in the middle of computation. Furthermore, the RDD can be distributed automatically across the clusters by means of partitions and it holds your data. You can also keep it on your data on memory by the caching mechanism of Spark, and this mechanism enables big data applications in Hadoop-based MapReduce clusters to execute up to 100 times faster for in-memory if executed iteratively and even 10 times faster for disk-based operation.</p><p>Let's look at a surprising statistic about Spark and its computation powers. Recently, Spark took over Hadoop-based MapReduce by completing the 2014 Gray Sort Benchmark in the 100 TB category, which is an industry benchmark on how fast a system can sort 100 TB of data (1 trillion records) (please refer to <a class="ulink" href="http://spark.apache.org/news/spark-wins-daytona-gray-sort-100tb-benchmark.html" target="_blank">http://spark.apache.org/news/spark-wins-daytona-gray-sort-100tb-benchmark.html</a> and <a class="ulink" href="http://sortbenchmark.org/" target="_blank">http://sortbenchmark.org/</a>). Finally, it becomes the open source engine (please refer to the following URL for more information <a class="ulink" href="https://databricks.com/blog/2014/11/05/spark-officially-sets-a-new-record-in-large-scale-sorting.html" target="_blank">https://databricks.com/blog/2014/11/05/spark-officially-sets-a-new-record-in-large-scale-sorting.html</a>) for sorting at petabyte scale. In comparison, the previous world record set by Hadoop MapReduce had to use 2100 machines, taking 72 minutes of execution time, which implies Spark sorted the same data three times faster using 10 times fewer machines. Moreover, you can combine multiple libraries seamlessly to develop large-scale machine learning and data analytics pipelines to execute the job on various cluster managers such as Hadoop YARN, Mesos, or in the cloud by accessing data storage and sources such as <span class="strong"><strong>HDFS</strong></span>, <span class="strong"><strong>Cassandra</strong></span>, <span class="strong"><strong>HBase</strong></span>, <span class="strong"><strong>Amazon S3</strong></span>, or even <span class="strong"><strong>RDBMs</strong></span>. Moreover, the job can be executed as a standalone mode on a local PC or cluster, or even on <span class="strong"><strong>AWS EC2</strong></span>. Therefore, deployment of a Spark application on the cluster is easy (we will show more on how to deploy a Spark application on the cluster later in this chapter).</p><p>The other beauties of Spark are: it is open source and platform independent. These two are also its greatest advantage, which is it's free to use, distribute, and modify and develop an application on any platform. An open source project is also more secure as the code is accessible to everyone and anyone can fix bugs as they are found. Consequently, Spark has evolved so rapidly that it has become the largest open source project concerning big data solutions with 750+ contributors from 200+ organizations.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec8"></a>New computing paradigm with Spark</h2></div></div><hr /></div><p>In this section, we will show a chronology of Spark that will provide a concept of how it was evolved and emerged as a revolution for big data processing and cluster computing. In addition to this, we will also describe the Spark ecosystem in brief to understand the features and facilities of Spark in more details.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec7"></a>Traditional distributed computing</h3></div></div></div><p>The traditional data processing paradigm is commonly referred to as a client-server model, which people used to move data to the code. The database server (or simply the server) was mainly responsible for performing data operations and then returning the results to the client-server (or simply the client) program. However, when the number of task to be computed is increased, a variety of operations and client devices also started to increase exponentially. As a result, a progressively complex array of computing endpoint in servers also started in the background. So to keep this type of computing model we need to increase the application (client) servers and database server in balance for storing and processing the increased number of operations. Consequently, the data propagation between nodes and data transfer back and forth across this network also increases drastically. Therefore, the network itself becomes a performance bottleneck. As a result, the performance (in terms of both the scalability and throughput) in this kind of computing paradigm also decreases undoubtedly. It is shown in the following diagram:</p><div class="mediaobject"><img src="graphics/image_01_001.jpg" /><div class="caption"><p>Figure 1: Traditional distributed processing in action.</p></div></div><p>After the successful completion of human genome projects in life sciences, real-time IOT data, sensor data, data from mobile devices, and data from the Web are creating the data-deluge and contributing for the big data, which has mostly evolved the data-intensive computing. The data-intensive computing nowadays is now flattering increasingly in an emerging way, which requires an integrated infrastructure or computing paradigm, so that the computational resources and data could be brought in a common platform and perform the analytics on top of it. The reasons are diverse because big data is really huge in terms of complexity (<span class="strong"><strong>volume</strong></span>, <span class="strong"><strong>variety</strong></span>, and <span class="strong"><strong>velocity</strong></span>), and from the operational perspective four ms (that is, <span class="strong"><strong>move</strong></span>, <span class="strong"><strong>manage</strong></span>, <span class="strong"><strong>merge</strong></span>, and <span class="strong"><strong>munge</strong></span>).</p><p>In addition, since we will be talking about large-scale machine learning applications in this book, we also need to consider some addition and critical assessing parameters such as validity, veracity, value, and visibility to grow the business. Visibility is important, because suppose you have a big dataset with a size of 1 PB; however, if there is no visibility, everything is a black hole. We will explain more on big data values in upcoming chapters.</p><p>It may not be feasible to store and process these large-scale and complex big datasets in a single system; therefore, they need to be partitioned and stored across multiple physical machines. Well, big datasets are partitioned or distributed, but to process and analyze these rigorously complex datasets, both the database servers as well as application servers might need to be increased to intensify the processing power at a large-scale. Again, the same performance bottleneck issues arrive at worst in multi-dimension that requires a new and more data-intensive big data processing and related computing paradigm.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec8"></a>Moving code to the data</h3></div></div></div><p>To overcome the issues mentioned previously, a new computing paradigm is desperately needed so that instead of moving data to the code/application, we could move the code or application to the data and perform the data manipulation, processing, and associated computing at home (that is, where the data is stored). As you understand the motivation and objective, now the reverts programming model can be called <span class="strong"><strong>move code to data and do parallel processing on distributed system</strong></span>, which can be visualized in the following diagram:</p><div class="mediaobject"><img src="graphics/image_01_002.jpg" /><div class="caption"><p>Figure 2: New computing (move code to data and do parallel processing on distributed system).</p></div></div><p>To understand the workflows illustrated in <span class="emphasis"><em>Figure 2</em></span>, we can envisage a new programming model described as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Execution of a big data processing using your application initiated at your personal computer (let's name it <span class="strong"><strong>Driver Program</strong></span>), which coordinates the execution in action remotely across multiple computing nodes within a cluster or grid, or a more openly speaking cloud.</p></li><li style="list-style-type: disc"><p>Now what you need is to transfer your developed application/algorithm/code segments (could be invoked or revoked using command-line or shell scripting as a simple programming language notation) to the computing/worker nodes (having large storage, main memory, and processing capability). We can simply imagine that the data to be computed or manipulated is already stored in those computing nodes as partitions or blocks.</p></li><li style="list-style-type: disc"><p>It is also understandable that the bulk data no longer needs to be transferred (upload/download) to your driver program because of the network or computing bottleneck, but it only holds the data reference in its variable instead, which is basically an address (hostname/IP address with a port) to locate the physical data stored in the computing nodes in a cluster, for example (of course bulk-upload could be performed using other solutions, such as scalable provisioning that is to be discussed in later chapters).</p></li><li style="list-style-type: disc"><p>So what do the remote computing nodes have? They have the data as well as code to perform the data computations and necessary processing to materialize the output or modified data without leaving their home (more technically, the computing nodes).</p></li><li style="list-style-type: disc"><p>Finally, upon your request, only the results could be transferred across the network to your driver program for validation or other analytics since there are many subsets of the original datasets.</p></li></ul></div><p>It's worth noticing that by moving the code to the data, the computing structure has been changed drastically. Most interestingly, the volume of data transfer across the network has significantly reduced. The justification here is that you will be transferring only a small chunk of software code to the computing nodes and receiving a small subset of the original data as results in return. This was the most important paradigm shifting for big data processing that Spark brought to us with the concept of RDD, datasets, DataFrame, and other lucrative features that imply great revolution in the history of big data engineering and cluster computing. However, for brevity, in the next section we will only discuss the concepts of RDD and the other computing features will be discussed in upcoming sections</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec9"></a>RDD – a new computing paradigm</h3></div></div></div><p>To understand the new computing paradigm, we need to understand the concept of <span class="strong"><strong>Resilient Distributed Datasets</strong></span> (<span class="strong"><strong>RDDs</strong></span>), by which and how Spark has implemented the data reference concept. As a result, it has been able to shift the data processing easily to take it at scale. The basic thing about RDD is that it helps you to treat your input datasets almost like any other data objects. In other words, it brings the diversity of input data types, which you greatly missed in the Hadoop-based MapReduce framework.</p><p>An RDD provides the fault-tolerance capability in a resilient way in a sense that it cannot be changed once created and the Spark engine will try to iterate the operation once failed. It is distributed because once it has created performed partition operations, RDDs are automatically distributed across the clusters by means of partitions. RDDs let you play more with your input datasets since RDDs can also be transformed into other forms rapidly and robustly. In parallel, RDDs can also be dumped through an action and shared across your applications that are logically co-related or computationally homogeneous. This is achievable because it is a part of Spark's general-purpose execution engine to gain massive parallelism, so it can virtually be applied in any type of datasets.</p><p>However, to make the RDD and related operation on your inputs, Spark engines require you to make a distinguishing borderline between the data pointer (that is, the reference) and the input data itself. Basically, your driver program will not hold data, but only the reference of the data where the data is actually located on the remote computing nodes in a cluster.</p><p>To make the data processing faster and easier, Spark supports two types of operations, which can be performed on RDDs: transformations and actions (please refer to <span class="emphasis"><em>Figure 3</em></span>). A transformation operation basically creates a new dataset from an existing one. An action, on the other hand, materializes a value to the driver program after a successful computation on input datasets on the remote server (computing nodes to be more exact).</p><p>The style of data execution initiated by the driver program builds up a graph as a <span class="strong"><strong>Directed Acyclic Graph</strong></span> (<span class="strong"><strong>DAG</strong></span>) style; where nodes represent RDDs and the transformation operations are represented by the edges. However, the execution itself does not start in the computing nodes in a Spark cluster until an action operation is performed. Nevertheless, before starting the operation, the driver program sends the execution graph (that signifies the style of operation for the data computation pipelining or workflows) and the code block (as a domain-specific script or file) to the cluster and each worker/computing node receives a copy from the cluster manager node:</p><div class="mediaobject"><img src="graphics/image_01_003.jpg" /><div class="caption"><p>Figure 3: RDD in action (transformation and action operation).</p></div></div><p>Before proceeding to the next section, we argue you to learn about the action and transformation operation in more detail. Although we will discuss these two operations in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Understanding the Problem by Understanding the Data</em></span> in detail. There are two types of transformation operations currently supported by Spark. The first one is the narrow transformation, where data mingle is unnecessary. Typical Spark narrow transformation operations are performed using the <code class="literal">filter()</code>, <code class="literal">sample()</code>, <code class="literal">map()</code>, <code class="literal">flatMap()</code>, <code class="literal">mapPartitions()</code> , and other methods. The wide transformation is essential to make a wider change to your input datasets so that the data could be brought in a common node out of multiple partitions of data shuffling. Wide transformation operations include <code class="literal">groupByKey()</code>, <code class="literal">reduceByKey()</code>, <code class="literal">union()</code>, <code class="literal">intersection()</code>, <code class="literal">join()</code>, and so on.</p><p>An action operation returns the final results of RDD computations from the transformation by triggering execution as a <span class="strong"><strong>Directed Acyclic Graph</strong></span> (<span class="strong"><strong>DAG</strong></span>) style to the Driver Program. But the materialized results are actually written on the storage, including the intermediate transformation results of the data objects and return the final results. Common actions include: <code class="literal">first()</code>, <code class="literal">take()</code>, <code class="literal">reduce()</code>, <code class="literal">collect()</code>, <code class="literal">count()</code>, <code class="literal">saveAsTextFile()</code>, <code class="literal">saveAsSequenceFile()</code>, and so on. At this point we believe that you have gained the basic operation on top of RDDs, so we can now define an RDD and related programs in a natural way. A typical RDD programming model that Spark provides can be described as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>From an environment variable, Spark Context (Spark shell or Python Pyspark provides you with a Spark Context or you can make your own, this will be described later in this chapter) creates an initial data reference RDD object.</p></li><li style="list-style-type: disc"><p>Transform the initial RDD to create more RDDs objects following the functional programming style (to be discussed later on).</p></li><li style="list-style-type: disc"><p>Send the code/algorithms/applications from the driver program to the cluster manager nodes. Then the cluster manager provides a copy to each computing node.</p></li><li style="list-style-type: disc"><p>Computing nodes hold a reference of the RDDs in its partition (again, the driver program also holds a data reference). However, computing nodes could have the input dataset provided by the cluster manager as well.</p></li><li style="list-style-type: disc"><p>After a transformation (via either narrow or wider transformation), the result to be generated is a brand new RDD, since the original one will not be mutated. Finally, the RDD object or more (specifically data reference) is materialized through an action to dump the RDD into the storage.</p></li><li style="list-style-type: disc"><p>The Driver Program can request the computing nodes for a chunk of results for the analysis or visualization of a program.</p></li></ul></div><p>Wait! So far we have moved smoothly. We suppose you will ship your application code to the computing nodes in the cluster. Still you will have to upload or send the input datasets to the cluster to be distributed among the computing nodes. Even during the bulk-upload you will have to transfer the data across the network. We also argue that the size of the application code and results are negligible or trivial. Another obstacle is if you/we want Spark to process the data at scale computation, it might require data objects to be merged from multiple partitions first. That means we will need to shuffle data among the worker/computing nodes that are usually done by <code class="literal">partition()</code>, <code class="literal">intersection()</code>, and <code class="literal">join()</code> transformation operations.</p><p>So frankly speaking, the data transfer has not been eliminated completely. As we and you understand the overheads being contributed especially for the bulk upload/download of these operations, their corresponding outcomes are as follows:</p><div class="mediaobject"><img src="graphics/image_01_004.jpg" /><div class="caption"><p>Figure 4: RDD in action (the caching mechanism).</p></div></div><p>Well, it's true that we have been compromised with these burdens. However, situations could be tackled or reduced significantly using the caching mechanism of Spark. Imagine you are going to perform an action multiple times on the same RDD objects, which have a long lineage; this will cause an increase in execution time as well as data movement inside a computing node. You can remove (or at least reduce) this redundancy with the caching mechanism of Spark (<span class="emphasis"><em>Figure 4</em></span>) that stores the computed result of the RDD in the memory. This eliminates the recurrent computation every time. Because, when you cache on an RDD, its partitions are loaded into the main memory instead of a disk (however, if there is not enough space in the memory, the disk will be used instead) of the nodes that hold it. This technique enables big data applications on Spark clusters to outperform MapReduce significantly for each round of parallel processing. We will discuss more on Spark data manipulations and other techniques in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Understanding the Problem by Understanding the Data</em></span> in detail.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec9"></a>Spark ecosystem</h2></div></div><hr /></div><p>To provide more enhancements and additional big data processing capabilities, Spark can be configured and run on top of existing Hadoop-based clusters. As already stated, although Hadoop provides the <span class="strong"><strong>Hadoop Distributed File System</strong></span> (<span class="strong"><strong>HDFS</strong></span>) for efficient and operational storing of large-scale data cheaply; however, MapReduce provides the computation fully disk-based. Another limitation of MapReduce is that; only simple computations can be executed with a high-latency batch model, or static data to be more specific. The core APIs in Spark, on the other hand, are written in Java, Scala, Python, and R. Compared to MapReduce, with the more general and powerful programming model, Spark also provides several libraries that are part of the Spark ecosystems for redundant capabilities in big data analytics, processing, and machine learning areas. The Spark ecosystem consists of the following components, as shown in <span class="emphasis"><em>Figure 5</em></span>:</p><div class="mediaobject"><img src="graphics/image_01_005.jpg" /><div class="caption"><p>Figure 5: Spark ecosystem (till date up to Spark 1.6.1).</p></div></div><p>As we have already stated, it is very much possible to combine these APIs seamlessly to develop large-scale machine learning and data analytics applications. Moreover, the job can be executed on various cluster managers such as Hadoop YARN, Mesos, standalone, or in the cloud by accessing data storage and sources such as HDFS, Cassandra, HBase, Amazon S3, or even RDBMs.</p><p>Nevertheless, Spark is enriched with other features and APIs. For example, recently Cisco has announced to invest $150M in the Spark ecosystem towards Cisco Spark Hybrid Services (<a class="ulink" href="http://www.cisco.com/c/en/us/solutions/collaboration/cloud-collaboration/index.html" target="_blank">http://www.cisco.com/c/en/us/solutions/collaboration/cloud-collaboration/index.html</a>). So Cisco Spark open APIs could boost its popularity with developers in higher cardinality (highly secure collaboration and connecting smartphone systems to the cloud). Beyond this, Spark has recently integrated Tachyon (<a class="ulink" href="http://ampcamp.berkeley.edu/5/exercises/tachyon.html" target="_blank">http://ampcamp.berkeley.edu/5/exercises/tachyon.html</a>), a distributed in-memory storage system that economically fits in memory to further improve Spark's performance.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec10"></a>Spark core engine</h3></div></div></div><p>Spark itself is written in Scala, which is functional, as well as <span class="strong"><strong>Object Oriented Programming Language</strong></span> (<span class="strong"><strong>OOPL</strong></span>) which runs on top of JVM. Moreover, as mentioned in <span class="emphasis"><em>Figure 5</em></span>, Spark's ecosystem is built on top of the general and core execution engine, which has some extensible API's implemented in different languages. The lower level layer or upper level layer also uses the Spark core engine as a general execution job performing engine and it provides all other functionality on top. The Spark Core is written in Scala as already mentioned, and it runs on <span class="strong"><strong>Java Virtual Machine</strong></span> (<span class="strong"><strong>JVM</strong></span>) and the high-level APIs (that is, Spark MLlib, SparkR, Spark SQL, Dataset, DataFrame, Spark Streaming, and GraphX) that use the core in the execution time.</p><p>Spark has brought the in-memory computing mode to a great visibility. This concept (in-memory computing) enables the Spark core engine to leverage speed through a generalized execution model to develop diverse applications.</p><p>The low-level implementation of general purpose data computing and machine learning algorithms written in Java, Scala, R, and Python are easy to use for big data application development. The Spark framework is built on Scala, so developing ML applications in Scala can provide access to the latest features that might not be available in other Spark languages initially. However, that is not a big problem, open source communities also take care of the necessity of developers around the globe. Therefore, if you do need a particular machine learning algorithm to be developed, and you want to add it to the Spark library, you can contribute it to the Spark community. The source code of Spark is openly available on GitHub at <a class="ulink" href="https://github.com/apache/spark" target="_blank">https://github.com/apache/spark</a> as Apache Spark mirror. You can do a pull out request and the open source community will review your changes or algorithm before adding it to the master branch. For more information, please check the Spark Jira confluence site at <a class="ulink" href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark" target="_blank">https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark</a>.</p><p>Python was a great arsenal for data scientists previously, and the contribution of Python in Spark is also not different. That means Python also has some excellent libraries for data analysis and processing; however, it is comparatively slower than Scala. R on the other hand, has a rich environment for data manipulation, data pre-processing, graphical analysis, machine learning, and statistical analysis, which can help to increase the developer's productivity. Java is definitely a good choice for developers who are coming from the Java and Hadoop background. However, Java also has the similar problem as Python, since Java is also slower than Scala.</p><p>A recent survey presented on the Databricks website at <a class="ulink" href="http://go.databricks.com/2015-spark-survey" target="_blank">http://go.databricks.com/2015-spark-survey</a> on Spark users (66% users evaluated the Spark languages where 41% were data engineers and 22% were data scientists) shows that 58% are using Python, 71% are using Scala, 31% are using Java, and 18% are using R for developing their Spark applications. However, in this book, we will try to provide the examples mostly in Java and a few in Scala if needed for the simplicity. The reason for this is that many of the readers are very familiar with Java-based MapReduce. Nevertheless, we will provide some hints of using the same examples in Python or R in the appendix at the end.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec11"></a>Spark SQL</h3></div></div></div><p>Spark SQL is a Spark component for querying and structured data processing. The demand was obvious since many data science engineers and business intelligence analysts also rely on interactive SQL queries for exploring data from RDBMS. Previously, MS SQL server, Oracle, and DB2 were used frequently by the enterprise. However, these tools were not scalable or interactive. Therefore, to make it easier, Spark SQL provides a programming abstraction called DataFrames and datasets that work as distributed SQL query engines, which support unmodified Hadoop Hive queries to execute 100 times faster on existing deployments and data. Spark SQL is a powerful integration with the rest of the Spark ecosystem.</p><p>Recently, Spark has offered a new experimental interface, commonly referred to as datasets (to be discussed in more details in the next section), which provide the same benefits of RDDs to use the <code class="literal">lambda</code> functions strongly. Lambda evolves from the Lambda Calculus (<a class="ulink" href="http://en.wikipedia.org/wiki/Lambda_calculus" target="_blank">http://en.wikipedia.org/wiki/Lambda_calculus</a>) that refers to anonymous functions in computer programming. It is a flexible concept in modern programming language that allows you to write any function quickly without naming them. In addition, it also provides a nice way to write closures. For example, in Python:</p><pre class="programlisting">def adder(x):
    return lambda y: x + y
add6 = adder(6)
add4(4)
</pre><p>It returns the result as <code class="literal">10</code>. On the other hand, in Java, it can be similarly written if an integer is odd or even:</p><pre class="programlisting">Subject&lt;Integer&gt; sub = x -&gt; x % 2 = 0; // Tests if the parameter is even.
boolean result = sub.test(8);
</pre><p>The previous <code class="literal">lambda</code> function checks if the parameter is even and returns either <code class="literal">true</code> or <code class="literal">false</code>. For example, the preceding snippet would return <code class="literal">true</code> since <code class="literal">8</code> is divisible by <code class="literal">2</code>.</p><p>Please note that in Spark 2.0.0, the Spark SQL has substantially been improved with the SQL functionalities with SQL 2003 support. Therefore, Spark SQL can now be executed with all the 99 TPC-DS queries. More importantly, now a native SQL parser supports ANSI_SQL and Hive QL. Native DDL is a command that can also be executed, it also now supports sub querying of SQL and the view of canonicalization support.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec12"></a>DataFrames and datasets unification</h3></div></div></div><p>In the latest Spark release 2.0.0, in Scala and Java, the DataFrame and dataset have been unified. In other words, the DataFrame is just a type alias for a dataset of rows. However, in Python and R, given the lack of type safety, DataFrame is the main programming interface. And for Java, DataFrame is no longer supported, but only the Dataset and RDD-based computations are supported, and DataFrame has become obsolete (note that it has become obsolete - not depreciated). Although SQLContext and HiveContext are kept for backward compatibility; however, in Spark 2.0.0 release, the new entry point that replaces the old SQLContext and HiveContext for DataFrame and dataset APIs is SparkSession.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec13"></a>Spark streaming</h3></div></div></div><p>You might want your applications to have the ability to process and analyze not only static datasets, but also real-time streams data. To make your wish easier, Spark Streaming provides the facility to integrate your application with popular batch and streaming data sources. The most commonly used data sources include HDFS, Flume, Kafka, and Twitter, and they can be used through their public APIs. This integration allows users to develop powerful interactive and analytical applications on both streaming and historical data. In addition to this, the fault tolerance characteristics are achieved through Spark streaming.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec14"></a>Graph computation – GraphX</h3></div></div></div><p><span class="strong"><strong>GraphX</strong></span> is a resilient distributed graph computation engine built on top of Spark. GraphX brought a revolution to the users who want to interactively build, transform, and reason graph structured data with millions of nodes and vertices at scale. As a developer you will enjoy the simplicity so that a large-scale graph (social network graph, normal network graph, or astrophysics) could be represented using a small chunk of code written in Scala, Java, or Python. GraphX enables the developers to take the advantages of both data-parallel and graph-parallel systems by efficiently expressing graph computation with ease and speed. Another beauty added in the cabinet of GraphX is that it can be used to build an end-to-end graph analytical pipeline on real-time streaming data, where the graph-space partitioning is used to handle large-scale directed multigraph with properties associated with each vertex and edge. Well, some fundamental graph operators are used to make this happen such as subgraph, joinVertices and aggregateMessages as well as an optimized variant of the Pregel API in particular.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec15"></a>Machine learning and Spark ML pipelines</h3></div></div></div><p>Traditional machine learning applications were used to build using R or Matlab that lacks scalability issues. Spark has brought two emerging APIs, Spark MLlib and Spark ML. These APIs make the machine learning as an actionable insight for engineering big data to remove the scalability constraint. Built on top of Spark, MLlib is a scalable machine learning library that is enriched with numerous high-quality algorithms with a high-accuracy performance that mainly works for RDDs. Spark provides many language options for the developers that are functioning in Java, Scala, R, and Python to develop complete workflows. Spark ML, on the other hand, is an ALPHA component that enhances a new set of machine learning algorithms to let data scientists quickly assemble and configure practical machine learning pipelines on top of DataFrames.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec16"></a>Statistical computation – SparkR</h3></div></div></div><p>SparkR is an R package specially designed for the data scientists who are familiar with R language and want to analyze large datasets and interactively run jobs from the R shell, which supports all the major Spark DataFrame operations such as aggregation, filtering, grouping, summary statistics, and much more. Similarly, users also can create SparkR DataFrames from local R data frames, or from any Spark supported data sources such as Hive, HDFS, Parquet, or JSON. Technically speaking, the concept of Spark DataFrame is a tabular data object akin to R's native DataFrame (<a class="ulink" href="https://cran.r-project.org/web/packages/dplyr/vignettes/data_frames.html" target="_blank">https://cran.r-project.org/web/packages/dplyr/vignettes/data_frames.html</a>), which on the other hand, is syntactically similar to <code class="literal">dplyr</code> (an R package, refer to <a class="ulink" href="https://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html" target="_blank">https://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html</a>), but is stored in the cluster setting instead.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec10"></a>Spark machine learning libraries</h2></div></div><hr /></div><p>In this section, we will describe two main machine learning libraries (Spark MLib and Spark ML) and the most widely used implemented algorithms. The ultimate target is to provide you with some familiarization about the machine learning treasures of Spark since many people still think that Spark is only a general-purpose in-memory big data processing or cluster computing framework. However, this is not like that, rather this information would help you to understand what could be done with the Spark machine learning APIs. In addition, this information would help you to explore and will increase the usability while deploying real-life machine learning pipelines using Spark MLlib and Spark ML.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec17"></a>Machine learning with Spark</h3></div></div></div><p>In the pre-Spark era, big data modelers typically used to build their ML models. Where a model is prepared through a training process where it is required to make predictions and is corrected when those predictions are wrong. In short, an ML model is an object that takes an input, does some processing, and finally produces the output. Those models were commonly constructed using statistical languages such as R and SAS. Then the data engineers used to re-implement the same model in Java to deploy on Hadoop. However, this kind of workflow lacks efficiency, scalability, throughput, and accuracy with extended execution time. Using Spark, the same ML model can be built, adopted, and deployed, making the whole workflow much more efficient, robust, and faster and that allows you to provide hands-on insight to increase the performance. The main goal of Spark machine learning libraries is to make practical machine learning applications scalable, faster, and easy. It consists of common and widely used machine learning algorithms and their utilities, including classification, regression, clustering, collaborative filtering, and dimensionality reduction. It is divided into two packages: Spark MLlib (<code class="literal">spark.mllib</code>) and Spark ML (<code class="literal">spark.ml</code>).</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec18"></a>Spark MLlib</h3></div></div></div><p>MLlib is the machine learning library of Spark. It is a distributed, low-level library written with Scala, Java, and Python against Spark core runtime. MLlib mainly focus on learning algorithms and their proper utilities to not only provide machine learning analytical capabilities. The major learning utilities include classification, regression, clustering, recommender system, and dimensionality reduction. In addition, it also aids to optimize the general purpose primitives for developing large-scale machine learning pipelines. As stated earlier, MLlib comes with some exciting APIs written in Java, Scala, R, and Python. The main components of Spark MLlib are described in the following sections.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec0"></a>Data types</h4></div></div></div><p>Spark provides support of local vectors and matrix data types stored on a single machine, as well as distributed matrices backed by one or multiple RDDs. Local vectors and matrices are simple data models that serve as public interfaces. The vector and matrix operations are heavily dependent on the linear algebra operation, and you are recommended to gain some background knowledge before using these data types.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec1"></a>Basic statistics</h4></div></div></div><p>Spark not only provides a column summary and basic statistics to be performed on RDDs, but it also supports calculating the correlation between two series of data or more complex correlation operations, such as pairwise correlations among many series of data, which are a common operation in statistics. However, currently Pearson's and Spearman's correlations are only supported and more are to be added in future Spark releases. Unlike the other statistical function, stratified sampling is also supported by Spark and can be performed on RDD's as key-value pairs; however, some functionalities are yet to be added to Python developers.</p><p>Spark provides only the Pearson's chi-squared test for hypothesis testing for its goodness of fit and independence of a claim hypothesis, which is a powerful technique in statistics that determines whether a result is statistically significant to satisfy the claim. Spark also provides online implementations of some tests to support use cases such as A/B testing as streaming significance testing typically performed on real-time streaming data. Another exciting feature of Spark is the factory methods to generate random double RDDs or vector RDDs that are useful for randomized algorithms, prototyping, performance, and hypothesis testing. Other functionality in the current Spark MLib provides computation facilities of kernel density estimation from sample RDDs, which is a useful technique for visualizing empirical probability distributions.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec2"></a>Classification and regression</h4></div></div></div><p>Classification is a typical process that helps new data objects and components to be organized, differentiated, and understood or belong in a certain way on the basis of training data. In statistical computing, two types of classification exist, binary classification (also commonly referred to as binomial classification) and multiclass classification. Binary classification is the task of classifying data objects of a given observation into two groups. <span class="strong"><strong>Support Vector Machines</strong></span> (<span class="strong"><strong>SVMs</strong></span>), logistic regression, decision trees, random forests, gradient-boosted trees, and Naive Bayes have been implemented up to the latest release of Spark.</p><p>Multiclass classification, on the other hand, is the task of classifying data objects of a given observation into more than two groups. The logistic regression, decision trees, random forests, and naive Bayes are implemented as multiclass classification. However, more complex classification algorithms such as multi-level classification and multiclass perceptron have not been implemented yet. The regression analysis is also a statistical process that estimates relationships among variables or observation. Other than the classification process, regression analysis involves several techniques for modeling and analyzing data objects. Currently, the following algorithms are supported by Spark MLlib library:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Linear least squares</p></li><li style="list-style-type: disc"><p>Lasso</p></li><li style="list-style-type: disc"><p>Ridge regression</p></li><li style="list-style-type: disc"><p>Decision trees</p></li><li style="list-style-type: disc"><p>Random forests</p></li><li style="list-style-type: disc"><p>Gradient-boosted trees</p></li><li style="list-style-type: disc"><p>Isotonic regression</p></li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec3"></a>Recommender system development</h4></div></div></div><p>An intelligent and scalable recommender system is an emerging application that is currently being developed by many enterprises to expand their business and cost towards automating recommendation for customers. The collaborative filtering approach is the most widely used algorithm in the recommender system, aiming to fill in the missing entries of a user-item association matrix. For example, Netflix is an example who could manage to reduce their movie recommendation by several million dollars. However, the current implementation of Spark MLlib provides only the model-based collaborative filtering technique.</p><p>The pros of a model-based collaborative filtering algorithm are users and products that can be described by a small set of latent factors to predict missing entries using the <span class="strong"><strong>Alternating Least Squares</strong></span> (<span class="strong"><strong>ALS</strong></span>) algorithm. The con is that user rating or feedback cannot be taken into consideration for predicting an interest. Interestingly, open source developers are also working to develop a memory-based collaborative filtering technique to be incorporated into Spark MLib in which user rating data could be used to compute the similarity between users or items making the ML model more versatile.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec4"></a>Clustering</h4></div></div></div><p>Clustering is an unsupervised machine learning problem/technique. The aims are to group subsets of entities with one another based on some notion of similarity that is often used for exploratory analysis and for developing hierarchical supervised learning pipelines. Spark MLib provides support for various clustering models such as K-means, Gaussian matrix, <span class="strong"><strong>Power Iteration Clustering</strong></span> (<span class="strong"><strong>PIC</strong></span>), <span class="strong"><strong>Latent Dirichlet Allocation</strong></span> (<span class="strong"><strong>LDA</strong></span>), Bisecting K-means, and Streaming K-means from real time streaming data. We will discuss more on supervised/unsupervised and reinforcement learning in upcoming chapters.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec5"></a>Dimensionality reduction</h4></div></div></div><p>Working with high-dimensional data is cool and demanding to meet the big data related complexities. However, one of the problems with high-dimensional data is unwanted features or variables. Since all of the measured variables might not be important for building the model, to answer the questions of interest you might need to reduce the search space. Therefore, based on certain considerations or requirements, we need to reduce the dimension of the original data before creating any model without sacrificing the original structure.</p><p>The current implementation of MLib API supports two types of dimensionality reduction techniques: <span class="strong"><strong>Singular Value Decomposition</strong></span> (<span class="strong"><strong>SVD</strong></span>) and <span class="strong"><strong>Principal Component Analysis</strong></span> (<span class="strong"><strong>PCA</strong></span>) for tall-and-skinny matrices that are stored in row-oriented formats and for any vectors. The SVD technique has some performance issues; however, PCA is the most widely used technique in dimensionality reduction. These two techniques are very useful in large scale ML applications, but they require strong background knowledge of linear algebra.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec6"></a>Feature extraction and transformation</h4></div></div></div><p>Spark provides different techniques for making the feature engineering easy to use through the <span class="strong"><strong>Term frequency-inverse document frequency</strong></span> (<span class="strong"><strong>TF-IDF</strong></span>), <span class="strong"><strong>Word2Vec</strong></span>, <span class="strong"><strong>StandardScaler</strong></span>, <span class="strong"><strong>ChiSqSelector</strong></span>, and so on. If you are working or planning to work in the area of mining towards building a text mining ML application, TF-IDF would be an interesting option from Spark MLlib. TF-IDF provides a feature vectorization method to reflect the importance of a term to a document in the corpus that is very helpful to develop a text analytical pipeline.</p><p>In addition, you might be interested in using the Word2Vec computers distributed vector representation of the words or corpus on your ML application for text analysis. This feature of Word2Vec will eventually make your generalization and model estimation more robust in the area of the novel patterns. You also have the StandardScaler to normalize the extracted features by scaling to the unit variance or by removing the mean based on column summary statistics. It is needed in the pre-processing step while building a scalable ML application typically performed on the samples in the training dataset. Well, suppose you have extracted features through this method, now you will need to select the features to be incorporated into your ML model. Therefore, you might also be fascinated in the ChiSqSelector algorithm of Spark MLlib for feature selection. ChiSqSelector tries to identify relevant features during the ML model building. The reason is obviously to reduce the size of the feature space as well as the search space in a tree-based approach and to improve both speed and statistical learning behavior in the reinforcement learning algorithms.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec7"></a>Frequent pattern mining</h4></div></div></div><p>Mining frequent items, maximal frequent patterns/itemsets, contiguous frequent patterns or subsequences, or other substructures is usually among the first steps to analyze a large-scale dataset before starting to build your ML models. The current implementation of Spark MLib provides a parallel implementation of FP-growth for mining frequent patterns and the association rules. It also provides the implementation of another popular algorithm, PrefixSpan, for mining sequence patterns. However, you will have to customize the algorithm for mining maximal frequent patterns accordingly. We will provide a scalable ML application for mining privacy, and preserving maximal frequent patterns in upcoming chapters.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec19"></a>Spark ML</h3></div></div></div><p>Spark ML is an ALPHA component that adds a new set of machine learning APIs to let users quickly assemble and configure practical machine learning pipelines on top of DataFrames. Before praising the features and advantages of Spark ML, we should know about the DataFrames machine learning techniques that can be applied and developed to a wide variety of data types, such as vectors, unstructured (that is, raw texts), images, and structured data. In order to support a variety of data types to make the application development easier, recently, Spark ML has adopted the DataFrame and Dataset from Spark SQL.</p><p>A DataFrame or Dataset can be created either implicitly or explicitly from an RDD of objects that supports the basic and structured types. The goal of Spark ML is to provide a uniform set of high-level APIs built on top of DataFrames and datasets rather than RDDs. It helps the users to create and tune practical machine learning pipelines. The Spark ML also provides for the feature estimators and transformers for developing scalable ML pipelines. Spark ML systematizes many ML algorithms and APIs to make it even easier to combine multiple algorithms into a single pipeline, or data workflow that uses the concept of DataFrame and datasets.</p><p>The three basic steps in feature engineering are feature extraction, feature transformation, and selection. Spark ML provides implementation of several algorithms to make these steps easier. Extraction provides the facility for extracting features from raw data, whereas transformation provides the facility of scaling, converting, or modifying features that are found from the extraction step and the selection helps to select a subset from a larger set of features from the second step. Spark ML also provides several classifications (logistic regression, decision tree classifier, random forest classifier, and more), regression (liner regression, decision tree regression, random forest regression, survival regression, and gradient-boosted tree regression), decision tree and tree ensembles (random forest and gradient-boosted trees), as well as clustering (K-means and LDA) algorithms implemented for developing ML pipelines on top of DataFrames. We will discuss more on RDDs and DataFrames and their underlying operations in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Understanding the Problem by Understanding the Data</em></span>.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec11"></a>Installing and getting started with Spark</h2></div></div><hr /></div><p>Spark is Apache Hadoop's successor. Therefore, it would be better to install and work Spark into a Linux-based system even though you can also try on Windows and Mac OS. It is also very possible to configure your Eclipse environment to work with Spark as a Maven project on any OS and bundle your applications as a jar file with all the dependencies. Secondly, you can try running an application from the Spark shell (Scala shell to be more specific) following the same fashion as SQL or R programming:</p><p>The third way is from the command line (Windows)/Terminal (Linux/Mac OS). At first you need to write your ML application using Scala or Java and prepare the jar file with the required dependencies. Then the jar file can be submitted to a cluster to compute a Spark job.</p><p>We will show how to develop and deploy a Spark ML application in three ways. However, the very first perquisite is to prepare your Spark application development environment. You can install and configure Spark on a number of operating systems, including:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Windows (XP/7/8/10)</p></li><li style="list-style-type: disc"><p>Mac OS X (10.4.7+)</p></li><li style="list-style-type: disc"><p>Linux distribution (including Debian, Ubuntu, Fedora, RHEL, CentOS, and so on)</p></li></ul></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note3"></a>Note</h3><p>Please check, the Spark website at <a class="ulink" href="https://spark.apache.org/documentation.html" target="_blank">https://spark.apache.org/documentation.html</a> for the Spark version and OS related documentation. The following steps show you how to install and configure Spark on Ubuntu 14.04 (64-bit). Please note that Spark 2.0.0 runs on Java 7+, Python 2.6+/3.4+, and R 3.1+. For the Scala API, Spark 2.0.0 uses Scala 2.11. Therefore, you will need to use a compatible Scala version (2.11.x).</p></div><p><span class="strong"><strong>Step 1: Java installation</strong></span></p><p>Java installation should be considered as one of the mandatory requirements in installing Spark since Java and Scala-based APIs require having a Java virtual machine installed on the system. Try the following command to verify the Java version:</p><pre class="programlisting">
<span class="strong"><strong>$ java -version </strong></span>
</pre><p>If Java is already installed on your system, you should see the following message:</p><pre class="programlisting">
<span class="strong"><strong>java version "1.7.0_80"</strong></span>
<span class="strong"><strong>Java(TM) SE Runtime Environment (build 1.7.0_80-b15)</strong></span>
<span class="strong"><strong>Java HotSpot(TM) 64-Bit Server VM (build 24.80-b11, mixed mode)</strong></span>
</pre><p>In case you do not have Java installed on your system, make sure you install Java before proceeding to the next step. Please note that to avail and enjoy the lambda expression support it is recommended to install Java 8 on your system, preferably JDK and JRE both. Although for Spark 1.6.2 and prior releases Java 7 should be enough:</p><pre class="programlisting">
<span class="strong"><strong>$ sudo apt-add-repository ppa:webupd8team/java</strong></span>
<span class="strong"><strong>$ sudo apt-get update</strong></span>
<span class="strong"><strong>$ sudo apt-get install oracle-java8-installer</strong></span>
</pre><p>After installing, don't forget to set <code class="literal">JAVA_HOME</code>. Just apply the following commands (we assume Java is installed at <code class="literal">/usr/lib/jvm/java-8-oracle</code>):</p><pre class="programlisting">
<span class="strong"><strong>$ echo "export JAVA_HOME=/usr/lib/jvm/java-8-oracle" &gt;&gt; ~/.bashrc  </strong></span>
<span class="strong"><strong>$ echo "export PATH=$PATH:$JAVA_HOME/bin" &gt;&gt; ~/.bashrc</strong></span>
</pre><p>You can add these environmental variables manually in the<code class="literal">.bashrc</code> file located in the home directory. If you cannot find the file, probably it is hidden so it needs to be explored. Just go to the <span class="strong"><strong>view </strong></span>tab and enable the <span class="strong"><strong>Show hidden file</strong></span>.</p><p><span class="strong"><strong>Step 2: Scala installation</strong></span></p><p>Spark is written in Scala itself; therefore, you should have Scala installed on your system. Checking this is so straight forward by using the following command:</p><pre class="programlisting">
<span class="strong"><strong>$ scala -version</strong></span>
</pre><p>If Scala is already installed on your system, you should get the following message on the terminal:</p><pre class="programlisting">
<span class="strong"><strong>Scala code runner version 2.11.8 -- Copyright 2002-2016, LAMP/EPFL</strong></span>
</pre><p>Note that during the writing of this installation, we used the latest version of Scala, that is 2.11.8. In case you do not have Scala installed on your system, make sure you install it, so before proceeding to the next step, you can download the latest version of Scala from the Scala website at <a class="ulink" href="http://www.scala-lang.org/download/" target="_blank">http://www.scala-lang.org/download/</a>. After the download has finished, you should find the Scala <code class="literal">tar</code> file in the download folder:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Extract the Scala <code class="literal">tar</code> file by extracting, from its location or type the following command for extracting the Scala tar file from the terminal:</p><pre class="programlisting">
<span class="strong"><strong>    $ tar -xvzf scala-2.11.8.tgz </strong></span>
</pre></li><li><p>Now move the Scala distribution to the user’s perspective (for example, <code class="literal">/usr/local/scala</code>) by the following command or do it manually:</p><pre class="programlisting">
<span class="strong"><strong>    $ cd /home/Downloads/ </strong></span>
<span class="strong"><strong>    $ mv scala-2.11.8 /usr/local/scala </strong></span>
</pre></li><li><p>Set the Scala home:</p><pre class="programlisting">
<span class="strong"><strong>$ echo "export SCALA_HOME=/usr/local/scala/scala-2.11.8" &gt;&gt;
        ~/.bashrc  </strong></span>
<span class="strong"><strong>$ echo "export PATH=$PATH:$SCALA_HOME/bin" &gt;&gt; ~/.bashrc</strong></span>
</pre></li><li><p>After installation has been completed, you should verify it using the following command:</p><pre class="programlisting">
<span class="strong"><strong>    $ scala -version</strong></span>
</pre></li><li><p>If Scala has successfully been configured on your system, you should get the following message on your terminal:</p><pre class="programlisting">
<span class="strong"><strong>Scala code runner version 2.11.8 -- Copyright 2002-2016, LAMP/EPFL</strong></span>
</pre></li></ol></div><p><span class="strong"><strong>Step 3: Installing Spark</strong></span></p><p>Download the latest version of Spark from the Apace Spark website at <a class="ulink" href="https://spark.apache.org/downloads.html" target="_blank">https://spark.apache.org/downloads.html</a>. For this installation step, we used the latest Spark stable release 2.0.0 version pre-built for Hadoop 2.7 and later. After the download has finished, you will find the Spark <code class="literal">tar</code> file in the download folder:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Extract the Scala <code class="literal">tar</code> file by extracting it from its location or type the following command for extracting the Scala <code class="literal">tar</code> file from the terminal:</p><pre class="programlisting">
<span class="strong"><strong>    $ tar -xvzf spark-2.0.0-bin-hadoop2.7.tgz  </strong></span>
</pre></li><li><p>Now move the Scala distribution to the user's perspective (for example, <code class="literal">/usr/local/spark</code>) by the following command or do it manually:</p><pre class="programlisting">
<span class="strong"><strong>    $ cd /home/Downloads/ </strong></span>
<span class="strong"><strong>    $ mv spark-2.0.0-bin-hadoop2.7 /usr/local/spark </strong></span>
</pre></li><li><p>To set after Spark installing, just apply the following commands:</p><pre class="programlisting">
<span class="strong"><strong>$ echo "export SPARK_HOME=/usr/local/spark/spark-2.0.0-bin-hadoop2.7" &gt;&gt;
      ~/.bashrc  </strong></span>
<span class="strong"><strong>$ echo "export PATH=$PATH:$SPARK_HOME/bin" &gt;&gt; ~/.bashrc</strong></span>
</pre></li></ol></div><p><span class="strong"><strong>Step 4: Making all the changes permanent</strong></span></p><p>Source the <code class="literal">~/.bashrc</code> file using the following command to make the changes permanent:</p><pre class="programlisting">
<span class="strong"><strong>$ source ~/.bashrc</strong></span>
</pre><p>If you execute the <code class="literal">$ vi ~/. bashrc</code> command, you will see the following entry in your <code class="literal">bashrc</code> file as follows:</p><pre class="programlisting">
<span class="strong"><strong>export JAVA_HOME=/usr/lib/jvm/java-8-oracle</strong></span>
<span class="strong"><strong>export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/jvm/java-8-oracle/bin:/usr/lib/jvm/java-8-oracle/db/bin:/usr/lib/jvm/java-8-oracle/jre/bin: /usr/lib/jvm/java-8-oracle/bin</strong></span>
<span class="strong"><strong>export SCALA_HOME=/usr/local/scala/scala-2.11.8</strong></span>
<span class="strong"><strong>export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/jvm/java-8-oracle/bin:/usr/lib/jvm/java-8-oracle/db/bin:/usr/lib/jvm/java-8-oracle/jre/bin: /bin</strong></span>
<span class="strong"><strong>export SPARK_HOME=/usr/local/spark/spark-2.0.0-bin-hadoop2.7</strong></span>
<span class="strong"><strong>export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/jvm/java-8-oracle/bin:/usr/lib/jvm/java-8-oracle/db/bin:/usr/lib/jvm/java-8-oracle/jre/bin: /bin</strong></span>
</pre><p><span class="strong"><strong>Step 5: Verifying the Spark installation</strong></span></p><p>The verification of the Spark installation is shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/image_01_006.jpg" /><div class="caption"><p>Figure 6: The Spark shell confirms the successful Spark installation.</p></div></div><p>Write the following command to open the Spark shell to verify if Spark has been configured successfully:</p><pre class="programlisting">
<span class="strong"><strong>$ spark-shell</strong></span>
</pre><p>If Spark is installed successfully, you should see the following message (<span class="emphasis"><em>Figure 6</em></span>).</p><p>The Spark server will start on localhost at port <code class="literal">4040</code>, more precisely at <code class="literal">http://localhost:4040/</code> (<span class="emphasis"><em>Figure 7</em></span>). Just move there to make sure if it's really running:</p><div class="mediaobject"><img src="graphics/image_01_007.jpg" /><div class="caption"><p>Figure 7: Spark is running as a local web server.</p></div></div><p>Well done! Now you are ready to start writing the Scala code on the Spark shell.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec12"></a>Packaging your application with dependencies</h2></div></div><hr /></div><p>Now we will show you how to package the applications as a Java archive (<code class="literal">JAR</code>) file with all the required dependencies on Eclipse, which is an <span class="strong"><strong>Integrated Development Environment</strong></span> (<span class="strong"><strong>IDE</strong></span>) and an open source tool for Java development as an Apache Maven project (<a class="ulink" href="https://maven.apache.org/" target="_blank">https://maven.apache.org/</a>). Maven is a software project management and comprehension tool like Eclipse. Based on the concept of a <span class="strong"><strong>Project Object Model</strong></span> (<span class="strong"><strong>POM</strong></span>), Maven can manage a project's build, reporting and documenting from a central piece of information.</p><p>Note that it is possible to export an ML application written in Java or Scala as an archive/executable jar file using Command Prompt. However, for the simplicity and faster application development we will use the same as the Maven project using Eclipse so that readers can enjoy the same facility to submit the application to the master node for computation. Now let's move to the discussion of exporting frequent pattern mining applications as a jar file with all the dependencies.</p><p><span class="strong"><strong>Step 1: Creating a Maven project in Eclipse</strong></span></p><p>On successful creation of a sample Maven project, you will see the following project structure in Eclipse shown in <span class="emphasis"><em>Figure 8</em></span>:</p><div class="mediaobject"><img src="graphics/image_01_008.jpg" /><div class="caption"><p>Figure 8: Maven project structure in Eclipse.</p></div></div><p><span class="strong"><strong>Step 2: Application development</strong></span></p><p>Create a Java class and copy the following source code to under the <code class="literal">src/main/java</code> directory for the mining frequent pattern. Here, inputting the filename has been specified by the filename string to be provided through the command line argument or by specifying the source manually. For the time being, we have just provided line comments, however, you will get to know details from <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Understanding the Problem by Understanding the Data</em></span> and onwards:</p><pre class="programlisting">import java.util.Arrays;
import java.util.List;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.mllib.fpm.FPGrowth;
import org.apache.spark.mllib.fpm.FPGrowthModel;
import org.apache.spark.rdd.RDD;
import org.apache.spark.sql.SparkSession;

public class JavaFPGrowthExample {
  public static void main(String[] args) {
   //Specify the input transactional as command line argument
   String fileName = "input/input.txt";
   //Configure a SparkSession as spark by specifying the application name, master URL, Spark config, and Spark warehouse directory
  SparkSession spark = SparkSession
                  .builder()
                  .appName("JavaFPGrowthExample")
                  .master("local[*]")
                  .config("spark.sql.warehouse.dir", "E:/Exp/")
                  .getOrCreate();

   //Create an initial RDD by reading the input database
   RDD&lt;String&gt; data = spark.sparkContext().textFile(fileName, 1);

   //Read the transactions by tab delimiter &amp; mapping RDD(data)
   JavaRDD&lt;List&lt;String&gt;&gt; transactions = data.toJavaRDD().map(
                   new Function&lt;String, List&lt;String&gt;&gt;(){
                   public List&lt;String&gt; call(String line) {
                          String[] parts = line.split(" ");
                          return Arrays.asList(parts);
                                 }
                             });

  //Create FPGrowth object by min. support &amp; no. of partition
  FPGrowth fpg = new  FPGrowth()
                       .setMinSupport(0.2)
                       .setNumPartitions(10);

  //Train and run your FPGrowth model using the transactions
  FPGrowthModel&lt;String&gt; model = fpg.run(transactions);

  //Convert and then collect frequent patterns as Java RDD. After that print the frequent patterns along with their support
    for (FPGrowth.FreqItemset&lt;String&gt; itemset :
          model.freqItemsets().toJavaRDD().collect()) {
       System.out.println(itemset.javaItems()
                             + "==&gt; " + itemset.freq());
      }
    }
  }
</pre><p><span class="strong"><strong>Step 3: Maven configuration</strong></span></p><p>Now you need to configure Maven specifying related dependencies and configurations. First, edit your existing <code class="literal">pom.xml</code> file to copy each XML source code snippets inside the <code class="literal">&lt;dependencies&gt;</code> tag. Please note that your dependencies might be different based on Spark release so change the version accordingly:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Spark core dependency for Spark context and configuration:</p><pre class="programlisting">      &lt;dependency&gt;
      &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
      &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;
      &lt;version&gt;2.0.0&lt;/version&gt;
     &lt;/dependency&gt;
</pre></li><li><p>Spark MLib dependency for the FPGrowth:</p><pre class="programlisting">    &lt;dependency&gt;
      &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
      &lt;artifactId&gt;spark-mllib_2.11&lt;/artifactId&gt;
      &lt;version&gt;2.0.0&lt;/version&gt;
     &lt;/dependency&gt;
</pre></li></ol></div><p>Now you need to add the build requirements. Copy the following code snippets immediately after the <code class="literal">&lt;/dependencies&gt;</code> tag. Here we are specifying the <code class="literal">&lt;groupId&gt;</code> as maven plugins, <code class="literal">&lt;artifactId&gt;</code> as maven shade plugins, and specifying the jar file naming convention using the <code class="literal">&lt;finalName&gt;</code> tag. Make sure that you have specified the source code download plugin, set the compiler level, and set the assembly plugin for the Maven, described as follows:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Specify the source code download plugin with Maven:</p><pre class="programlisting">       &lt;plugin&gt;
        &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
        &lt;artifactId&gt;maven-eclipse-plugin&lt;/artifactId&gt;
        &lt;version&gt;2.9&lt;/version&gt;
        &lt;configuration&gt;
          &lt;downloadSources&gt;true&lt;/downloadSources&gt;
          &lt;downloadJavadocs&gt;false&lt;/downloadJavadocs&gt;
        &lt;/configuration&gt;
      &lt;/plugin&gt;
</pre></li><li><p>Set the compiler level for Maven:</p><pre class="programlisting">      &lt;plugin&gt;
        &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
        &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
        &lt;version&gt;2.3.2&lt;/version&gt;
      &lt;/plugin&gt;
      &lt;plugin&gt;
        &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
        &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;
        &lt;configuration&gt;
          &lt;shadeTestJar&gt;true&lt;/shadeTestJar&gt;
        &lt;/configuration&gt;
      &lt;/plugin&gt;
</pre></li><li><p>Set the Maven assembly plugin:</p><pre class="programlisting">      &lt;plugin&gt;
        &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
        &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;
        &lt;version&gt;2.4.1&lt;/version&gt;
        &lt;configuration&gt;
          &lt;!-- get all project dependencies --&gt;
          &lt;descriptorRefs&gt;
            &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;
          &lt;/descriptorRefs&gt;
          &lt;!-- MainClass in mainfest make a executable jar --&gt;
          &lt;archive&gt;
            &lt;manifest&gt;              &lt;mainClass&gt;com.example.SparkFPGrowth.JavaFPGrowthExample&lt;/mainClass&gt;            &lt;/manifest&gt;
          &lt;/archive&gt;
          &lt;property&gt;
            &lt;name&gt;oozie.launcher.mapreduce.job.user.classpath.first&lt;/name&gt;
            &lt;value&gt;true&lt;/value&gt;
          &lt;/property&gt;
          &lt;finalName&gt;FPGrowth-${project.version}&lt;/finalName&gt;
        &lt;/configuration&gt;
        &lt;executions&gt;
          &lt;execution&gt;
            &lt;id&gt;make-assembly&lt;/id&gt;
            &lt;!-- bind to the packaging phase --&gt;
            &lt;phase&gt;package&lt;/phase&gt;
            &lt;goals&gt;
              &lt;goal&gt;single&lt;/goal&gt;
            &lt;/goals&gt;
          &lt;/execution&gt;
        &lt;/executions&gt;
      &lt;/plugin&gt;
</pre></li></ol></div><p>The complete <code class="literal">pom.xml</code> file, input data, and Java source file can be downloaded from our GitHub repositories at <a class="ulink" href="https://github.com/rezacsedu/PacktMLwithSpark" target="_blank">https://github.com/rezacsedu/PacktMLwithSpark</a>. Please note that we used Eclipse Mars Eclipse IDE for Java Developers, and the version was Mars Release (4.5.0). You can go for this version or another distribution such as Eclipse Luna.</p><p><span class="strong"><strong>Step 4: The Maven build</strong></span></p><p>In this section, we will describe how to create a Maven friendly project on Eclipse. After you have followed all the steps, you will be able to run a Maven project successfully. The steps should be in a chronological order as follows:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Run your project as Maven install.</p></li><li><p>If your code and maven configuration file are okay, the maven build will be successful.</p></li><li><p>Build the maven project.</p></li><li><p>Right-click on your project and run the maven project as <span class="strong"><strong>Maven build...</strong></span> and write <code class="literal">clean package</code> in the <span class="strong"><strong>Goals</strong></span> option.</p></li><li><p>Check the Maven dependencies.</p></li><li><p>Expand the Maven dependencies tree and check if the required jar files have been installed.</p></li><li><p>Check if the jar file is generated with dependencies.</p></li><li><p>As we specified, you should find two jar files under the <code class="literal">/target</code> directory tree (refer to <span class="emphasis"><em>Figure 9</em></span>). The packaging file should contain exactly the same name as specified in the <code class="literal">&lt;finalName&gt;</code> tag. Now move your code (jar file) to a directory that aligns our experiment (that is, <code class="literal">/user/local/code</code>) and your data (that is, <code class="literal">/usr/local/data/</code>). We will use this jar file to execute the Spark job on an AWS EC2 cluster in a later stage. We will discuss the input dataset in the next step.</p><div class="mediaobject"><img src="graphics/image_01_009.jpg" /><div class="caption"><p>Figure 9: Maven project with the jar generated with all the required dependencies on Eclipse.</p></div></div></li></ol></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec13"></a>Running a sample machine learning application</h2></div></div><hr /></div><p>In this section, we will describe how to run a sample machine learning application from the Spark shell, on the local machine as stand-alone mode, and finally we will show you how to deploy and run the application on the Spark cluster using Amazon EC2 (<a class="ulink" href="https://aws.amazon.com/ec2/" target="_blank">https://aws.amazon.com/ec2/</a>).</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec20"></a>Running a Spark application from the Spark shell</h3></div></div></div><p>Please note that this is just an exercise that checks the installation and running of a sample code. Details on machine learning application development will be covered from <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Understanding the Problem by Understanding the Data</em></span> to <span class="emphasis"><em><a class="link" href="#" linkend="ch09">Chapter 9</a></em></span>, <span class="emphasis"><em>Advanced Machine Learning with Streaming and Graph Data</em></span>.</p><p>Now we will further proceed with one of the popular machine learning problem also called frequent pattern mining using the Frequent Pattern-growth or FP-growth. Suppose we have a transactional database as shown in the following table. Each line indicates a transaction done by a particular customer. Our target is to find the frequent patterns from the database, which is the prerequisite for calculating association rules (<a class="ulink" href="https://en.wikipedia.org/wiki/Association_rule_learning" target="_blank">https://en.wikipedia.org/wiki/Association_rule_learning</a>) from customer purchase rules. Save this database as <code class="literal">input.txt</code> in the <code class="literal">/usr/local/data</code> directory without transaction IDs:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Transaction ID</strong></span></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Transaction</strong></span></p>
</td></tr><tr><td style="border-right: 0.5pt solid ; ">
<p>1</p><p>
</p><p>2</p><p>
</p><p>3</p><p>
</p><p>4</p><p>
</p><p>5</p><p>
</p><p>6</p><p>
</p><p>7</p><p>
</p><p>8</p><p>
</p><p>9</p><p>
</p><p>10</p>
</td><td style="">
<p>A B C D F</p><p>
</p><p>A B C E</p><p>
</p><p>B C D E F</p><p>
</p><p>A C D E</p><p>
</p><p>C D F</p><p>
</p><p>D E F</p><p>
</p><p>D E</p><p>
</p><p>C D F</p><p>
</p><p>C F</p><p>
</p><p>A C D E</p>
</td></tr></tbody></table></div><p>Table 1: A transactional database.</p><p>Now let's move to the Spark shell by specifying the master and number of the computational core to use as standalone mode (here are four cores, for example):</p><pre class="programlisting">
<span class="strong"><strong>$ spark-shell --master "local[4]" </strong></span>
</pre><p><span class="strong"><strong>Step 1: Loading packages</strong></span></p><p>Load the required FPGrowth package and other dependent packages:</p><pre class="programlisting">
<span class="strong"><strong>scala&gt;import org.apache.spark.mllib.fpm.FPGrowth</strong></span>
<span class="strong"><strong>scala&gt;import org.apache.spark.{SparkConf, SparkContext}</strong></span>
</pre><p><span class="strong"><strong>Step 2: Creating Spark context</strong></span></p><p>To create a Spark context, at first you need to configure the Spark session by mentioning the application name and master URL. Then you can use the Spark configuration instance variable to create a Spark context as follows:</p><pre class="programlisting">
<span class="strong"><strong>val conf = new SparkConf().setAppName(s"FPGrowthExample with $params")</strong></span>
<span class="strong"><strong>val sc = new SparkContext(conf)</strong></span>
</pre><p><span class="strong"><strong>Step 3: Reading the transactions</strong></span></p><p>Let's read the transactions as RDDs on the created Spark Context (<code class="literal">sc</code>) (see <span class="emphasis"><em>Figure 6</em></span>):</p><pre class="programlisting">
<span class="strong"><strong>scala&gt; val transactions = sc.textFile(params.input).map(_.split(" ")).cache()</strong></span>
</pre><p><span class="strong"><strong>Step 4: Checking the number of transactions</strong></span></p><p>Here is the code for checking the number of transactions:</p><pre class="programlisting">
<span class="strong"><strong>Scala&gt;println(s"Number of transactions: ${transactions.count()}")</strong></span>
<span class="strong"><strong>Number of transactions: 22</strong></span>
<span class="strong"><strong>Scala&gt;</strong></span>
</pre><p><span class="strong"><strong>Step 5: Creating an FPGrowth model</strong></span></p><p>Create the model by specifying the minimum support threshold (see also <a class="ulink" href="https://en.wikipedia.org/wiki/Association_rule_learning" target="_blank">https://en.wikipedia.org/wiki/Association_rule_learning</a>) and the number of partitions:</p><pre class="programlisting">
<span class="strong"><strong>scala&gt;val model = new FPGrowth()</strong></span>
<span class="strong"><strong>                   .setMinSupport(0.2)</strong></span>
<span class="strong"><strong>                   .setNumPartitions(2)</strong></span>
<span class="strong"><strong>                   .run(transactions)</strong></span>
</pre><p><span class="strong"><strong>Step 6: Checking the number of frequent patterns</strong></span></p><p>The following code explains how to check the number of frequent patterns:</p><pre class="programlisting">
<span class="strong"><strong>scala&gt; println(s"Number of frequent itemsets:
    ${model.freqItemsets.count()}")</strong></span>
<span class="strong"><strong>Number of frequent itemsets: 18</strong></span>
<span class="strong"><strong>Scala&gt;</strong></span>
</pre><p><span class="strong"><strong>Step 7: Printing patterns and support</strong></span></p><p>Print the frequent pattern and their corresponding support/frequency counts (see <span class="emphasis"><em>Figure 10</em></span>). Spark job will be running on localhost (refer to <span class="emphasis"><em>Figure 11</em></span>):</p><pre class="programlisting">
<span class="strong"><strong>scala&gt; model.freqItemsets.collect().foreach { itemset =&gt; println(itemset.items.mkString("[", ",", "]") + ", " + itemset.freq)}</strong></span>
</pre><div class="mediaobject"><img src="graphics/image_01_010.jpg" /><div class="caption"><p>Figure 10: Frequent patterns.</p></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec21"></a>Running a Spark application on the local cluster</h3></div></div></div><p>Once a user application is bundled as either a jar file (written in Scala or Java) or a Python file, it can be launched using the <code class="literal">spark-submit</code> script located under the bin directory in the Spark distribution.</p><p>As per the API documentation provided by the Spark website at <a class="ulink" href="http://spark.apache.org/docs/2.0.0-preview/submitting-applications.html" target="_blank">http://spark.apache.org/docs/2.0.0-preview/submitting-applications.html</a>, this script takes care of setting up the class path with Spark and its dependencies, and can support different cluster managers and deploys models that Spark supports. In a nutshell, Spark job submission syntax is as follows:</p><pre class="programlisting">
<span class="strong"><strong>$spark-submit [options] &lt;app-jar | python-file&gt; [app arguments]</strong></span>
</pre><p>Here, <code class="literal">[options]</code> can be: <code class="literal">--class &lt;main-class&gt;</code><code class="literal">--master &lt;master-url&gt;</code><code class="literal">--deploy-mode &lt;deploy-mode&gt;</code>, and a number of other options.</p><p>To be more specific, <code class="literal">&lt;main-class&gt;</code> is the name of the main class name, which is the entry point for our application. <code class="literal">&lt;master-url&gt;</code> specifies the master URL for the cluster (for example, <code class="literal">spark://HOST:PORT</code> for connecting to the given Spark standalone cluster master, local for running Spark locally with one worker thread with no parallelism at all, <code class="literal">local [k]</code> for running a Spark job locally with K worker threads, which is the number of cores on your machine, <code class="literal">local[*]</code> for running a Spark job locally with as many worker threads as logical cores on your machine have, and <code class="literal">mesos://IP:PORT</code> for connecting to the available Mesos cluster, and even you could submit your job to the Yarn cluster - for more, see <a class="ulink" href="http://spark.apache.org/docs/latest/submitting-applications.html#master-urls" target="_blank">http://spark.apache.org/docs/latest/submitting-applications.html#master-urls</a>).</p><p><code class="literal">&lt;deploy-mode&gt;</code> is used to deploy our driver on the worker nodes (cluster) or locally as an external client (client). <code class="literal">&lt;app-jar&gt;</code> is the jar file we just built, including all the dependencies. <code class="literal">&lt;python-file&gt;</code> is the application main source code written using Python. <code class="literal">[app-arguments]</code> could be an input or output argument specified by an application developer:</p><div class="mediaobject"><img src="graphics/image_01_011.jpg" /><div class="caption"><p>Figure 11: Spark job running on localhost</p></div></div><p>Therefore, for our case, the job submit syntax would be as follows:</p><pre class="programlisting">
<span class="strong"><strong>$./bin/spark-submit --class com.example.SparkFPGrowth.JavaFPGrowthExample --master local[4] FPGrowth-0.0.1-SNAPSHOT-jar-with-dependencies.jar input.txt</strong></span>
</pre><p>Here, <code class="literal">JavaFPGrowthExample</code> is the main class file written in Java; local is the master URL; <code class="literal">FPGrowth-0.0.1-SNAPSHOT-jar-with-dependencies.jar</code> is the application <code class="literal">jar</code> file we just generated by maven project; <code class="literal">input.txt</code> is the transactional database as the text file, and output is the directory where the output to be generated (in our case, the output will be shown on the console). Now let's submit this job to be executed locally.</p><p>If it is executed successfully, you will find the following message including the output in <span class="emphasis"><em>Figure 12</em></span> (abridged):</p><div class="mediaobject"><img src="graphics/B05243_01_12.jpg" /><div class="caption"><p>Figure 12: Spark job output on the terminal.</p></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec22"></a>Running a Spark application on the EC2 cluster</h3></div></div></div><p>In the previous section, we illustrated how to submit spark jobs in local or standalone mode. Here, we are going to describe how to run a spark application in cluster mode. To make our application run on the spark cluster mode, we consider the <span class="strong"><strong>Amazon Elastic Compute Cloud</strong></span> (<span class="strong"><strong>EC2</strong></span>) services, as <span class="strong"><strong>Infrastructure as a Service</strong></span> (<span class="strong"><strong>IaaS</strong></span>) or <span class="strong"><strong>Platform as a Service</strong></span> (<span class="strong"><strong>PaaS</strong></span>). For pricing and related information, please refer to this URL <a class="ulink" href="https://aws.amazon.com/ec2/pricing/" target="_blank">https://aws.amazon.com/ec2/pricing/</a>.</p><p><span class="strong"><strong>Step 1: Key pair and access key configuration</strong></span></p><p>We assume you have EC2 accounts already created. The first requirement is to create EC2 key pairs and AWS access keys. The EC2 key pair is the private key that you need when you will make a secure connection through SSH to your EC2 server or instances. To make the key, you have to go through the AWS console at <a class="ulink" href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html#having-ec2-create-your-key-pair" target="_blank">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html#having-ec2-create-your-key-pair</a>. Please refer to <span class="emphasis"><em>Figure 13</em></span>, which shows the key pair creation page for an EC2 account:</p><div class="mediaobject"><img src="graphics/B05243_01_13.jpg" /><div class="caption"><p>Figure 13: AWS key-pair generation window.</p></div></div><p>Name it <code class="literal">my-key-pair.pem</code> once you have downloaded it and saved it on your local machine. Then ensure the permission by executing the following command (you should store this file in a secure location for security purposes, say <code class="literal">/usr/local/key</code>):</p><pre class="programlisting">
<span class="strong"><strong>$ sudo chmod  400  /usr/local/key/my-key-pair.pem</strong></span>
</pre><p>Now what you need is the AWS access keys, the credentials of your account, which are needed if you want to submit your Spark job to compute nodes from your local machine using spark-ec2 script. To generate and download the keys, login to your AWS IAM services at <a class="ulink" href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html#Using_CreateAccessKey" target="_blank">http://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html#Using_CreateAccessKey</a>. Upon the download completion (that is, <code class="literal">/usr/local/key</code>), you need to set two environment variables in your local machine. Just execute the following commands:</p><pre class="programlisting">
<span class="strong"><strong>$ echo "export AWS_ACCESS_KEY_ID=&lt;access_key_id&gt;" &gt;&gt; ~/.bashrc  </strong></span>
<span class="strong"><strong>$ echo " export AWS_SECRET_ACCESS_KEY=&lt;secret_access_key_id&gt;" &gt;&gt; ~/.bashrc  </strong></span>
</pre><p><span class="strong"><strong>Step 2: Configuring the Spark cluster on EC2</strong></span></p><p>Spark distribution (that is, <code class="literal">/usr/local/spark</code><code class="literal">/ec2</code>) provides a script called <code class="literal">spark-ec2</code> for launching Spark Clusters in EC2 instances from your local machine (driver program), which helps in launching, managing, and shutting down the Spark Cluster.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note4"></a>Note</h3><p>Please note that starting a cluster on AWS will cost money. Therefore, it is always a good practice to stop or destroy a cluster when the computation is done. Otherwise, it will incur additional costs. For more about AWS pricing, please refer to this URL <a class="ulink" href="https://aws.amazon.com/ec2/pricing/" target="_blank">https://aws.amazon.com/ec2/pricing/</a>.</p></div><p>Once you execute the following command to launch a new instance, it sets up Spark, HDFS, and other dependencies on the cluster automatically:</p><pre class="programlisting">
<span class="strong"><strong>$./spark-ec2 --key-pair=&lt;name_of_the_key_pair&gt; --identity-file=&lt;path_of_the key_pair&gt;  --instance-type=&lt;AWS_instance_type &gt; --region=&lt;region&gt; zone=&lt;zone&gt; --slaves=&lt;number_of_slaves&gt; --hadoop-major-version=&lt;Hadoop_version&gt; --spark-version=&lt;spark_version&gt; launch &lt;cluster-name&gt;</strong></span>
</pre><p>We believe that these parameters are self-explanatory, or alternatively, please see details at <a class="ulink" href="http://spark.apache.org/docs/latest/ec2-scripts.html" target="_blank">http://spark.apache.org/docs/latest/ec2-scripts.html</a>. For our case, it would be something like this:</p><pre class="programlisting">
<span class="strong"><strong>$./spark-ec2 --key-pair=my-key-pair --identity-file=/usr/local/key/my-key-pair.pem  --instance-type=m3.2xlarge --region=eu-west-1 --zone=eu-west-1a --slaves=2 --hadoop-major-version=yarn --spark-version=1.6.0 launch ec2-spark-cluster-1</strong></span>
</pre><p>It is shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/image_01_014.jpg" /><div class="caption"><p>Figure 14: Cluster home.</p></div></div><p>After the successful completion, spark cluster will be instantiated with two workers (slave) nodes on your EC2 account. This task; however, sometimes might take half an hour approximately depending on your Internet speed and hardware configuration. Therefore, you'd love to have a coffee break. Upon successful competition of the cluster setup, you will get the URL of the Spark cluster on the terminal.</p><p>To check to make sure if the cluster is really running, check this URL <code class="literal">https://&lt;master-hostname&gt;:8080</code> on your browser, where the master hostname is the URL you receive on the terminal. If everything was okay, you will find your cluster is running, see cluster home in <span class="emphasis"><em>Figure 14</em></span>.</p><p><span class="strong"><strong>Step 3: Running and deploying Spark job on Spark Cluster</strong></span></p><p>Execute the following command to the SSH remote Spark cluster:</p><pre class="programlisting">
<span class="strong"><strong>$./spark-ec2 --key-pair=&lt;name_of_the_key_pair&gt; --identity-file=&lt;path_of_the _key_pair&gt; --region=&lt;region&gt; login &lt;cluster-name&gt;   </strong></span>
</pre><p>For our case, it should be something like this:</p><pre class="programlisting">
<span class="strong"><strong>$./spark-ec2 --key-pair=my-key-pair --identity-file=/usr/local/key/my-key-pair.pem --region=eu-west-1 login ec2-spark-cluster-1 </strong></span>
</pre><p>Now copy your application (the jar we generated as the Maven project on Eclipse) to a remote instance (that is, <code class="literal">ec2-52-48-119-121.eu-west-1.compute.amazonaws.com</code> in our case) by executing the following command (in a new terminal):</p><pre class="programlisting">
<span class="strong"><strong>$ scp -i /usr/local/key/my-key-pair.pem  /usr/local/code/FPGrowth-0.0.1-SNAPSHOT-jar-with-dependencies.jar ec2-user@ec2-52-18-252-59.eu-west-1.compute.amazonaws.com:/home/ec2-user/</strong></span>
</pre><p>Then you need to copy your data (<code class="literal">/usr/local/data/input.txt</code> in our case) to the same remote instance by executing the following command:</p><pre class="programlisting">
<span class="strong"><strong>$ scp -i /usr/local/key/my-key-pair.pem /usr/local/data/input.txt ec2-user@ec2-52-18-252-59.eu-west-1.compute.amazonaws.com:/home/ec2-user/ </strong></span>
</pre><div class="mediaobject"><img src="graphics/image_01_015.jpg" /><div class="caption"><p>Figure 15: Job running status at Spark cluster.</p></div></div><p>Well done! You are almost done! Now, finally you will have to submit your Spark job to be computed by the slaves or worker nodes. To do so, just execute the following commands:</p><pre class="programlisting">
<span class="strong"><strong>$./bin/spark-submit --class com.example.SparkFPGrowth.JavaFPGrowthExample --master spark://ec2-52-48-119-121.eu-west-1.compute.amazonaws.com:7077 /home/ec2-user/FPGrowth-0.0.1-SNAPSHOT-jar-with-dependencies.jar /home/ec2-user/input.txt</strong></span>
</pre><p>Upon successful completion of the job computation, you are supposed to see the status of your job at port 8080 like <span class="emphasis"><em>Figure 15</em></span> (the output will be shown on the terminal).</p><p><span class="strong"><strong>Step 4: Pausing and restarting spark cluster</strong></span></p><p>To stop your clusters, execute the following command from your local machine:</p><pre class="programlisting">
<span class="strong"><strong>$./ec2/spark-ec2 --region=&lt;ec2-region&gt; stop &lt;cluster-name&gt;</strong></span>
</pre><p>For our case, it would be:</p><pre class="programlisting">
<span class="strong"><strong>$./ec2/spark-ec2 --region=eu-west-1 stop ec2-spark-cluster-1</strong></span>
</pre><p>To restart the cluster later on, execute the following command:</p><pre class="programlisting">
<span class="strong"><strong>$./ec2/spark-ec2 -i &lt;key-file&gt; --region=&lt;ec2-region&gt; start &lt;cluster-name&gt;</strong></span>
</pre><p>For our case, it will be something as follows:</p><pre class="programlisting">
<span class="strong"><strong>$./ec2/spark-ec2 --identity-file=/usr/local/key/my-key-pair.pem --region=eu-west-1 start ec2-spark-cluster-1</strong></span>
</pre><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip5"></a>Tip</h3><p>To terminate your spark cluster:<code class="literal">$./spark-ec2 destroy &lt;cluster-name&gt;</code></p><p>In our case, it would be:<code class="literal"> <span class="strong"><strong>$./spark-ec2 --region=eu-west-1 destroy ec2-spark-cluster-1 </strong></span>
</code></p></div><p>If you want your application to scale up for large-scale datasets, the fastest way is to load them from Amazon S3 or an Amazon EBS device into an instance of the <span class="strong"><strong>Hadoop Distributed File System</strong></span> (<span class="strong"><strong>HDFS</strong></span>) on your nodes. We will discuss this technique in later chapters throughout practical machine learning examples.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec14"></a>References</h2></div></div><hr /></div><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="emphasis"><em>Resilient Distributed Datasets</em></span>: <span class="emphasis"><em>A Fault-Tolerant Abstraction for In-Memory Cluster Computing</em></span>, <span class="emphasis"><em>Zaharia, Mosharaf Chowdhury, Tathagata Das, Ankur Dave, Justin Ma, Murphy McCauley, Michael J</em></span>. <span class="emphasis"><em>Franklin, Scott Shenker, Ion Stoica.</em></span>

<span class="emphasis"><em>NSDI 2012</em></span>

<span class="emphasis"><em>. April 2012.</em></span>
</p></li><li style="list-style-type: disc"><p>
<span class="emphasis"><em>Spark</em></span>: <span class="emphasis"><em>Cluster Computing with Working</em></span>
<span class="emphasis"><em>Sets</em></span>, <span class="emphasis"><em>Matei Zaharia, Mosharaf Chowdhury, Michael J</em></span>. <span class="emphasis"><em>Franklin, Scott Shenker, Ion Stoica,</em></span>

<span class="emphasis"><em>HotCloud 2010</em></span>

<span class="emphasis"><em>. June 2010.</em></span>
</p></li></ul></div><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="emphasis"><em>Spark SQL</em></span>: <span class="emphasis"><em>Relational Data Processing in Spark</em></span>, <span class="emphasis"><em>Michael Armbrust, Reynold S. Xin, Cheng Lian, Yin Huai, Davies Liu, Joseph K. Bradley, </em></span><span class="emphasis"><em>Xiangrui Meng</em></span>, <span class="emphasis"><em>Tomer Kaftan</em></span>, <span class="emphasis"><em>Michael J. Franklin</em></span>, <span class="emphasis"><em>Ali Ghodsi</em></span>, <span class="emphasis"><em>Matei Zaharia</em></span>, <span class="emphasis"><em>SIGMOD</em></span> <span class="emphasis"><em>
2015. June 2015.</em></span>
</p></li><li style="list-style-type: disc"><p>
<span class="emphasis"><em>Discretized Streams</em></span>: <span class="emphasis"><em>Fault-Tolerant Streaming Computation at Scale,</em></span> <span class="emphasis"><em>Matei Zaharia, Tathagata Das, Haoyuan Li, Timothy Hunter, Scott Shenker, Ion Stoica.</em></span>

<span class="emphasis"><em>SOSP 2013</em></span>

<span class="emphasis"><em>. November 2013.</em></span>
</p></li><li style="list-style-type: disc"><p>
<span class="emphasis"><em>Discretized Streams</em></span>: <span class="emphasis"><em>An Efficient and Fault-Tolerant Model for Stream Processing on Large Clusters</em></span>. <span class="emphasis"><em>Matei Zaharia, Tathagata Das, Haoyuan Li, Scott Shenker, Ion Stoica.</em></span>

<span class="emphasis"><em>HotCloud 2012</em></span>

<span class="emphasis"><em>. June 2012.</em></span>
</p></li><li style="list-style-type: disc"><p>
<span class="emphasis"><em>GraphX</em></span>: <span class="emphasis"><em>Unifying Data-Parallel and Graph-Parallel Analytics</em></span>. <span class="emphasis"><em>Reynold S. Xin</em></span>, <span class="emphasis"><em>Daniel Crankshaw, Ankur Dave, Joseph E.</em></span>
<span class="emphasis"><em>Gonzalez, Michael J. Franklin</em></span>, <span class="emphasis"><em>Ion Stoica.</em></span>

<span class="emphasis"><em>OSDI 2014</em></span>

<span class="emphasis"><em>. October 2014.</em></span>
</p></li><li style="list-style-type: disc"><p>
<span class="emphasis"><em>MLlib</em></span>: <span class="emphasis"><em>Machine Learning in Apache Spark</em></span>, <span class="emphasis"><em>Meng et al</em></span>. <span class="emphasis"><em>arXiv:1505.06807v1, [cs.LG], 26 May 2015.</em></span>
</p></li><li style="list-style-type: disc"><p>
<span class="emphasis"><em>Recommender</em></span>: <span class="emphasis"><em>An Analysis of Collaborative Filtering Techniques</em></span>, <span class="emphasis"><em>Christopher R. Aberger</em></span>, <span class="emphasis"><em>Stanford publication, 2014.</em></span>
</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec15"></a>Summary</h2></div></div><hr /></div><p>This ends our rather quick tour of Spark. We have tried to cover some of the most basic features of Spark, its computing paradigm, and getting started with Spark by installing and configuring. Use of Spark ML is recommended if they fit the ML pipeline concept well (for example, feature extraction, transformation, and selection) since it is more versatile and flexible with DataFrames and Datasets. However, according to Apache's documentations, they will keep supporting and contributing Spark MLib along with the active development of Spark ML.</p><p>On the other hand, data science developers should be comfortable with using Spark MLlib's features and should expect more features in the future. However, some algorithms are not available or are yet to be added to Spark ML, most notably, dimensionality reduction. Nevertheless, developers can seamlessly combine the implementation of these techniques found in Spark MLib with the rest of the algorithms found in Spark ML as hybrid or interoperable ML applications. We also showed some basic techniques to deploy ML applications on clusters and cloud services, though you can also try other deployment options available.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip6"></a>Tip</h3><p>For more updates, interested readers should refer to the Spark website at <a class="ulink" href="http://spark.apache.org/docs/latest/mllib-guide.html" target="_blank">http://spark.apache.org/docs/latest/mllib-guide.html</a> for release dates, APIs, and specifications. As Spark's open source community and developers from all over the globe are continually enriching and updating the implementation, therefore, it is better to be updated.</p></div><p>In the next chapter (<a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Machine Learning Best Practices</em></span>), we will discuss some best practices while developing advanced machine learning with Spark, including machine learning tasks and classes, some practical machine learning problems and their related discussion, some best practices in machine learning application development, choosing the right algorithm for the ML application, and so on.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch02"></a>Chapter 2. Machine Learning Best Practices</h2></div></div></div><p>The purpose of this chapter is to provide a conceptual introduction to statistical <span class="strong"><strong>machine learning</strong></span> (<span class="strong"><strong>ML</strong></span>) techniques for those who might not normally be exposed to such approaches during their typical required statistical training. This chapter also aims to take a newcomer from minimal knowledge of machine learning all the way to a knowledgeable practitioner in a few steps. The second part of the chapter is focused on giving some recommendations for choosing the right machine learning algorithms depending on the application types and requirements. It will then lead through some best practices when applying large-scale machine learning pipelines. In a nutshell, the following topics will be discussed in this chapter: </p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>What is machine learning?</p></li><li style="list-style-type: disc"><p>Machine learning tasks</p></li><li style="list-style-type: disc"><p>Practical machine learning problems</p></li><li style="list-style-type: disc"><p>Large scale machine learning APIs in Spark</p></li><li style="list-style-type: disc"><p>Practical machine learning best practices</p></li><li style="list-style-type: disc"><p>Choosing the right algorithm for your application</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec16"></a>What is machine learning?</h2></div></div><hr /></div><p>In this section, we will try to define the term machine learning from the computer science, statistics and data analytical perspectives. Then we will show the steps of analytical machine learning applications. Finally, we will discuss some typical and emerging machine learning tasks and then name some practical machine learning problems that need to be addressed.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec23"></a>Machine learning in modern literature</h3></div></div></div><p>Let's see how a renowned professor of machine learning, Tom Mitchell, Chair of the CMU Machine Learning Department and Professor at the Carnegie Mellon University defines the term machine learning in his literature (<span class="emphasis"><em>Tom M. Mitchell, The Discipline of Machine Learning, CMU-ML-06-108, July 2006</em></span>, <a class="ulink" href="http://www.cs.cmu.edu/~tom/pubs/MachineLearning.pdf" target="_blank">http://www.cs.cmu.edu/~tom/pubs/MachineLearning.pdf</a>):</p><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>Machine Learning is a natural outgrowth of the intersection of Computer Science and Statistics. We might say the defining question of Computer Science is 'How can we build machines that solve problems, and which problems are inherently tractable/intractable?' The question that largely defines Statistics is 'What can be inferred from data plus a set of modelling assumptions, with what reliability?' The defining question for Machine Learning builds on both, but it is a distinct question. Whereas Computer Science has focused primarily on how to manually program computers, Machine Learning focuses on the question of how to get computers to program themselves (from experience plus some initial structure). Whereas Statistics has focused primarily on what conclusions can be inferred from data, Machine Learning incorporates additional questions about what computational architectures and algorithms can be used to most effectively capture, store, index, retrieve and merge these data, how multiple learning subtasks can be orchestrated in a larger system, and questions of computational tractability.</em></span></p></blockquote></div><p>We believe that this definition from Prof. Tom is self-explanatory. However, we will provide some clearer understanding of machine learning in the next two sub-sections from the computer science, statistics, and data analytical perspectives.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip7"></a>Tip</h3><p>Interested readers should follow other resources to get more insights about machine learning and its theoretical perspective. Here we have provided some links as follows: <span class="emphasis"><em>Machine learning</em></span>: <a class="ulink" href="https://en.wikipedia.org/wiki/Machine_learning" target="_blank">https://en.wikipedia.org/wiki/Machine_learning</a>.</p><p><span class="emphasis"><em>Machine learning: what it is and why matters</em></span> - <a class="ulink" href="http://www.sas.com/en_us/insights/analytics/machine-learning.html" target="_blank">http://www.sas.com/en_us/insights/analytics/machine-learning.html</a>.</p><p><span class="emphasis"><em>A Gentle Introduction To Machine Learning</em></span>: <a class="ulink" href="https://www.youtube.com/watch?v=NOm1zA_Cats" target="_blank">https://www.youtube.com/watch?v=NOm1zA_Cats</a>.</p><p><span class="emphasis"><em>What is machine learning, and how does it work</em></span>: <a class="ulink" href="https://www.youtube.com/watch?v=elojMnjn4kk" target="_blank">https://www.youtube.com/watch?v=elojMnjn4kk</a>.</p><p><span class="emphasis"><em>Introduction to Data Analysis using Machine Learning</em></span>: <a class="ulink" href="https://www.youtube.com/watch?v=U4IYsLgNgoY" target="_blank">https://www.youtube.com/watch?v=U4IYsLgNgoY</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec8"></a>Machine learning and computer science</h4></div></div></div><p>Machine learning is a branch of computer science that studies the design of algorithms that can learn from its heuristics that typically evolved from the study of pattern recognition and computational learning theory in artificial intelligence. An interesting question came into the mind of Alan Turing about the machine, which is, <span class="emphasis"><em>Can a machine think?</em></span> In fact, there are some good reasons to believe a sufficiently complex machine could one day pass the unrestricted Turing test; let's postpone this question until the Turing test, but gets passed. However, machines can learn at least. Subsequently, Arthur Samuel was the first man who defined the term <span class="strong"><strong>machine learning</strong></span> as a f<span class="emphasis"><em>ield of study that gives computers the ability to learn without being explicitly programmed</em></span> in 1959. Typical machine learning tasks are concept learning, predictive modeling, classification, regression, clustering, dimensionality reduction, recommender system, deep learning and finding useful patterns from the large-scale dataset.</p><p>The ultimate goal is to improve the learning in such a way that it becomes automatic, so that no human interactions are needed any more, or the level of human interaction is reduced as much as possible. Although machine learning is sometimes conflated with <span class="strong"><strong>Knowledge Discovery and Data Mining</strong></span> (<span class="strong"><strong>KDDM</strong></span>), the latter sub-field on the other hand focuses more on exploratory data analysis and is known as unsupervised learning - such as clustering analysis, anomaly detection, <span class="strong"><strong>Artificial Neural Networks</strong></span> (<span class="strong"><strong>ANN</strong></span>), and so on.</p><p>Other machine learning techniques include supervised learning, where a learning algorithm analyzes the training data and produces an inferred function that can be used for mapping new examples towards prediction. Classification and regression analysis are two typical examples of supervised learning. Reinforcement learning, on the other hand, is inspired by behaviorist psychology (see also <a class="ulink" href="https://en.wikipedia.org/wiki/Behaviorism" target="_blank">https://en.wikipedia.org/wiki/Behaviorism</a>), which is is typically concerned with how a software agent performs an action in a new <span class="emphasis"><em>environment</em></span> by maximizing the <code class="literal">reward</code> function. Dynamic programming and intelligent agent are two examples of reinforcement learning.</p><p>Typical machine learning applications can be classified into scientific knowledge discovery and more commercial applications, ranging from Robotic or <span class="strong"><strong>Human Computer Interaction</strong></span> (<span class="strong"><strong>HCI</strong></span>) to anti-spam filtering and recommender systems.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec9"></a>Machine learning in statistics and data analytics</h4></div></div></div><p>Machine learning reconnoitres the study and construction of algorithms (see also <a class="ulink" href="https://en.wikipedia.org/wiki/Algorithm" target="_blank">https://en.wikipedia.org/wiki/Algorithm</a>) that can learn (see also <a class="ulink" href="https://en.wikipedia.org/wiki/Learning" target="_blank">https://en.wikipedia.org/wiki/Learning</a>) from the heuristics and make meaningful predictions on data. However, in order to make data-driven predictions or decisions, such algorithms operate by building a model (see also <a class="ulink" href="https://en.wikipedia.org/wiki/Mathematical_model" target="_blank">https://en.wikipedia.org/wiki/Mathematical_model</a>) from training datasets, quicker than following a stringently static program or instructions. Machine learning is also closely related and often overlaps with the nature of computational statistics. Computational statistics is, on the other hand, an applied field of statistics that focuses on making predictions through a computerised approach. In addition, it has strong stalemates to mathematical optimisation, which delivers methods and computing tasks along with theory and application domains. The tasks that are not feasible in mathematics due to the demands for a strong background knowledge of mathematics, machine learning suits best and can be applied as the alternative to that.</p><p>Within the field of data analytics, on the other hand, machine learning is a method used to devise complex models and algorithms that advance themselves towards prediction for a future outcome. These analytical models allow researchers, data scientists, engineers, and analysts to produce reliable, repeatable, and reproducible results and mine hidden insights through learning from past relationships (heuristics) and trends in the data. Again we will refer to a famous definition from Prof. Tom, where he explained what learning really means from the computer science perspective in the literature (<span class="emphasis"><em>Tom M. Mitchell, The Discipline of Machine Learning, CMU-ML-06-108, July 2006</em></span>, <a class="ulink" href="http://www.cs.cmu.edu/~tom/pubs/MachineLearning.pdf" target="_blank">http://www.cs.cmu.edu/~tom/pubs/MachineLearning.pdf</a>):</p><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.</em></span></p></blockquote></div><p>Therefore, we can conclude that a computer program or machines can:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Learn from data and histories</p></li><li style="list-style-type: disc"><p>Can be improved with experience</p></li><li style="list-style-type: disc"><p>Interactively enhance a model that can be used to predict the outcomes of questions</p></li></ul></div><p>Furthermore, the following diagram helps us to understand the whole process of machine learning:</p><div class="mediaobject"><img src="graphics/image_02_001.jpg" /><div class="caption"><p>Figure 1: Machine learning at a glance.</p></div></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec24"></a>Typical machine learning workflow</h3></div></div></div><p>A typical machine learning application involving several steps from input, processing to output that form a scientific workflow is shown in Figure 2. The following steps are involved in typical machine learning applications:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Load the sample data.</p></li><li><p>Parse the data into the input format for the algorithm.</p></li><li><p>Pre-process the data and handle the missing values.</p></li><li><p>Split the data into two sets, one for building the model (training dataset) and one for testing the model (test dataset or validation dataset).</p></li><li><p>Run the algorithm to build or train your ML model.</p></li><li><p>Make predictions with the training data and observe the results.</p></li><li><p>Test and evaluate the model with the test data or alternatively validate the model with some cross-validator technique using the third dataset, called the validation dataset.</p></li><li><p>Tune the model for better performance and accuracy.</p></li><li><p>Scale-up the model so that it can handle massive datasets in the future.</p></li><li><p>Deploy the ML model in commercialization:</p><div class="mediaobject"><img src="graphics/image_02_002.jpg" /><div class="caption"><p>Figure 2: Machine learning workflow.</p></div></div></li></ol></div><p>Often the machine learning algorithms have some ways to handle the skewness in the datasets; that skewness can sometimes be immensely skewed though. In step 4, the experimental dataset is split often into a training set and test sets randomly, which is called sampling. The training dataset is used to train the model, whereas the test dataset is used to evaluate the performance of the best model at the very end. The better practice is to use the training dataset as much as you can to increase the generalization performance. On the other side, it is recommended to use the test dataset only once to avoid the overfitting and underfitting problem while computing the prediction error and the related metrics.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip8"></a>Tip</h3><p>Overfitting is a statistical property by which random error and noise is described apart from the normal and underlying relationships. It mostly occurs when there are too many hyperparameters relative to the number of observations or features. Under fitting on the other hand refers to a model that can neither model the training data nor generalize to new data towards the model evaluation or adaptability.</p></div><p>However, these steps consist of several techniques and we will discuss those in <a class="link" href="#" linkend="ch05">Chapter 5</a>, <span class="emphasis"><em>Supervised and Unsupervised Learning by Examples</em></span> in detail. Step 9 and 10 are usually considered as advanced steps, and they consequently will be discussed in later chapters.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec17"></a>Machine learning tasks</h2></div></div><hr /></div><p>Machine learning tasks or machine learning processes are typically classified into three broad categories, depending on the nature of the learning feedback available to a learning system. Supervised learning, unsupervised learning, and reinforcement learning; these three kinds of machine learning tasks are shown in <span class="emphasis"><em>Figure 3</em></span>, and will be discussed in this section:</p><div class="mediaobject"><img src="graphics/image_02_003.jpg" /><div class="caption"><p>Figure 3: Machine learning tasks.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec25"></a>Supervised learning</h3></div></div></div><p>A <span class="strong"><strong>supervised learning</strong></span> application makes predictions based on a set of examples, and the goal is to learn general rules that map inputs to outputs aligning with the real world. For example, a dataset for spam filtering usually contains spam messages as well as non-spam messages. Therefore, we could know which messages in a training set are spams or non-spams. Nevertheless, we might have the opportunity to use this information to train our model in order to classify new and unseen messages. Figure 4 shows the schematic diagram of the supervised learning.</p><p>In other words, the dataset for training the ML model in this case is labeled with the value of interest and a supervised learning algorithm looks for patterns in those value labels. After the algorithm has found the required patterns, those patterns can be used to make predictions for unlabeled test data. This is the most popular and useful type of machine learning tasks, which is not an exception for Spark as well, where most of the algorithms are a supervised learning technique:</p><div class="mediaobject"><img src="graphics/image_02_004.jpg" /><div class="caption"><p>Figure 4: Supervised learning in action.</p></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec26"></a>Unsupervised learning</h3></div></div></div><p>In <span class="strong"><strong>unsupervised learning</strong></span>, data points have no labels related or in other words, the correct classes of the training dataset in unsupervised learning are unknown, as shown in <span class="emphasis"><em>Figure 5</em></span>. As a result, classes have to be inferred from the unstructured datasets, which implies that the goal of an unsupervised learning algorithm is to pre-process the data in some structured ways by describing its structure.</p><p>To overcome this obstacle in unsupervised learning, clustering techniques are used typically to group the unlabeled samples based on certain similarity measures, mining hidden patterns towards feature learning. More technically, we can write down a generative model, and then tell the data to find parameters that explain the data to us. Now what will happen next if we are not satisfied with the possibility of this elucidation? The answer is that we should tell the data to do it again until we are using some efficient algorithms or techniques.</p><p>Now a new question may arise in your mind, why do we have to put labels on the data? Or cannot we just appreciate the data in its current order recognizing that each datum is unique and pre snowflake? In other words, with a little supervision, our data can grow up to be whatever it wants to be! So why should the unlabeled data be taken into consideration too?</p><p>Well, there are some deeper issues regarding this. For example, most of the variation in the data comes from phenomena that are irrelevant to our desired labeling scheme. A more realistic example would be how Gmail classifies e-mails as spam and ham using the supervised learning technique, where the data might use its parameters to explain its semantics, when all we care about is its syntactic properties:</p><div class="mediaobject"><img src="graphics/image_02_005.jpg" /><div class="caption"><p>Figure 5: Unsupervised learning.</p></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec27"></a>Reinforcement learning</h3></div></div></div><p><span class="strong"><strong>Reinforcement learning</strong></span> is the technique where the model itself learns from a series of actions or behaviors. Complexity of datasets or sample complexity is very important in the reinforcement learning needed for the algorithms to learn a target function successfully. Moreover, in response to each data point for achieving the ultimate goal, maximization of the reward function should be ensured while interacting with an external environment, as demonstrated in <span class="emphasis"><em>Figure 6</em></span>. To make the maximization easier, the reward function can either be exploited by penalizing the bad actions or rewarding for the good actions.</p><p>In order to achieve the highest reward, the algorithm should be modified with a strategy that also allows the machine or software agent to learn its behavior periodically. These behaviors can be learned once and for all, or the machine learning model can keep adapting as times passes:</p><div class="mediaobject"><img src="graphics/image_02_006.jpg" /><div class="caption"><p>Figure 6: Reinforcement learning.</p></div></div><p>For example, reinforcement learning is common in robotics; the algorithm must choose the robot's next action based on a set of sensor readings. It is also a natural fit for <span class="strong"><strong>Internet of Things</strong></span> (<span class="strong"><strong>IoT</strong></span>) applications, where a computer program interacts with a dynamic environment in which it must perform a certain goal, without an explicit mentor. Another example is the game <span class="strong"><strong>Flappy Bird</strong></span>, which has been trained to play itself.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec28"></a>Recommender system</h3></div></div></div><p>A recommender system is an emerging application, which is a subclass of information filtering system use for making a prediction of the rating or preference from the users that they usually provide to an item. The concept of recommender systems has become very common in recent years and subsequently applied in different applications. The most popular ones are probably products (for examples, movies, music, books, research articles, news, search queries, social tags, and so on). Recommender systems can be typed into four categories typically:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Collaborative filtering system, where accumulation of a consumer's preferences and recommendations to other users is based on likeness in behavioral patterns.</p></li><li style="list-style-type: disc"><p>Content-based systems, where the supervised machine learning is used to persuade a classifier to distinguish between interesting and uninteresting items for the users.</p></li><li style="list-style-type: disc"><p>Hybrid recommender systems is a recent research and hybrid approach (that is, combining collaborative filtering and content-based filtering). Netflix is a good example of such a recommendation system that uses <span class="strong"><strong>Restricted Boltzmann Machines</strong></span> (<span class="strong"><strong>RBM</strong></span>) and a form of the Matrix Factorization algorithm for large movie databases, such as IMDb. This recommendation, which simply recommends movies or dramas or streaming by comparing the watching and searching habits of similar users, is called rating prediction.</p></li><li style="list-style-type: disc"><p>Knowledge-based systems, where knowledge about users and products is used to reason what fulfills the user's requirements, using the perception tree, decision support systems, and case-based reasoning.</p></li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec29"></a>Semi-supervised learning</h3></div></div></div><p>Between supervised and unsupervised learning, there is a small place for <span class="strong"><strong>semi-supervised learning</strong></span>; where the ML model usually receives an incomplete training signal. More statistically, the ML model receives a training set with some of the target outputs missing. The semi-supervised learning is more or less assumption-based and often uses three kinds of assumption algorithms as the learning algorithm for the unlabeled datasets. The following assumptions are used: smoothness, cluster, and manifold assumption.</p><p>In other words, semi-supervised learning can furthermore be denoted as a <span class="strong"><strong>weakly supervised </strong></span>or <span class="strong"><strong>bootstrapping</strong></span> technique for using the hidden wealth of unlabeled examples to enhance the learning from a small amount of labeled data. Emerging examples include <span class="emphasis"><em>semi-supervised expectation minimization, and concept learning in human cognition and transitive SVMs</em></span>.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec18"></a>Practical machine learning problems</h2></div></div><hr /></div><p>What does machine learning really mean? We already saw some convincing definitions of this term as well as the meaning of the term <span class="emphasis"><em>learning</em></span> at the very beginning of this chapter. However, the reality is machine learning itself is defined by the problems to be resolved. In this section, we will first emphasize the machine learning classes and then we will list some well-known and popularly-used examples of real world machine learning problems. The typical classes include classification, clustering, rule extraction, and regression, which will all be discussed.</p><p>In addition, we will also discuss those problems based on the main taxonomy of standard machine learning problems. This is important, since knowing the type of problems we could face allows us to think about the data we need. Another important fact is that before knowing some practical machine learning problems, you might face difficulties in having an idea about developing your machine learning applications. In other words, to know the problem we need to know the data in the very first place. Therefore, the types of algorithm and their optimality to be addressed will be discussed throughout this chapter; data manipulation, however, will be discussed to dig-down the problems in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Understanding the Problem by Understanding the Data</em></span>.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec30"></a>Machine learning classes</h3></div></div></div><p>The problem classes we mentioned above are standards for most of the problems we refer to in everyday life while doing and applying machine learning techniques. However, knowing only the ML classes is not enough we also need to know what type of problems machines are learning, since you will find many problems that are simply problem solving that does not help a ML model or agent to learn at all.</p><p>When you think a problem is a machine learning problem, more technically, you are thinking of a decision problem that needs to be modeled from data that could be termed as a machine learning problem. In other words, as a data scientist or human expert, if you have enough time to answer a particular question by knowing the available dataset, you can more or less apply a suitable machine learning problem. Therefore, we can assume that a solvable problem using some ML algorithms would have mainly two parts - the data itself, which could be used to point to specific observations of the problem, and secondly the quantitative measurement of the quality of an available solution. Once you have succeeded in identifing a problem as an ML problem, you would probably be able to think about what types of problems you could formulate with it easily, or the type of aftermath your client will be asking for, or what sorts of requirements are to be satisfied. As already stated in the above section, the more frequently used machine learning classes are: classification, clustering, regression, and rule extraction. We will now provide a short overview of each class.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec10"></a>Classification and clustering</h4></div></div></div><p>If the experimental dataset is labeled, it means a class has been assigned to it already. For instance, spam/non-spam during spam e-mail detection or fraud/non-fraud during credit card fraud identification. However, if the dataset based on which the fundamental decision will be made or modeled is unlabeled, new labels need to be made manually or algorithmically. This might be difficult, and can be thought of a judgment problem. On the contrary, sculpting the differences or resemblances between several groups might be computationally harder.</p><p>Clustering, on the other hand, handles the data that is not labeled or un-labeled. However, it still can be divided into groups based on similarity and other measures of natural structure in the data you have. Organizing pictures from a digital album by faces only without names could be an example, where the human users like us have to assign names to groups manually. Again, the same computational complexity might arise to label multiple image files manually; we will provide some examples in later chapters of how Spark provides several APIs to solve these issues.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec31"></a>Rule extraction and regression</h3></div></div></div><p>From the given dataset, propositional rules can be generated by means of antecedent and consequent in the <span class="emphasis"><em>if...then</em></span> style that defines the behavior of a machine learning agent. This type of rule generation technique is commonly referred to as <span class="emphasis"><em>rule extraction</em></span>. You might be wondering if such rules might exist, however, they are typically not directed. That means the methods used to discover statistically meaningful or statistically significant relationships between attributes in your data.</p><p>An example of rule extraction is the mining association rules between items from business oriented transactional databases. Non-technically, a practical example could be the discovery of the relationship or association between the purchase of beer and diapers, which is illustrative of the desire and opportunity for the customers. However, some situation, might arise where some predictions out of the rules or data are not necessarily involved directly.</p><p>Now let's talk about the regression where the data is labeled with a real value. To be more exact, some floating point value rather than having labels in the data. The easiest way to understand an example would be time series data similar to the price of a stock or currency that changes over time. In these types of data, the regression task is to make a prediction for new and unpredicted data by some regression modeling techniques.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec19"></a>Most widely used machine learning problems</h2></div></div><hr /></div><p>You will find an extensive amount of examples of the use of machine learning related problems in daily life, since they solve the difficult parts of the available problems that are widely used techniques or algorithms. We often use many desktop or web-based applications that solve your problems out of the data even without knowing that what underlying techniques have been used. You will be wondered to know that many of them actually use widely used machine learning algorithms to make your life easier. There are many machine learning problems around. Here we will mention some example problems that really represent what machine learning is all about:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Spam detection or spam filtering</strong></span>: Given some e-mails in an inbox, the task is to identify those e-mails that are spam and those that are non-spam (often called ham) e-mail messages. Now the challenging part is to develop an ML application that can be applied so that it can identify only the non-spam e-mails to stay in the inbox. and move the spam emails to the corresponding spam folder or delete them permanently from the email account. A typical example could be what you may do while using Gmail manually, but if you have an ML application, that application will do it automatically.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Anomaly detection or outlier detection</strong></span>: The anomaly detection deals with the identification of items, events, or observations that are unexpected or non-confirming to the expected patterns in a dataset; in other words, the identification of suspect patterns. The most common example is network anomaly detection using some machine learning applications. Now the challenging task is to develop an ML application that can be applied successfully to simply identify the unusual data points from the data propagating across the network.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Credit card fraud detection</strong></span>: Credit card fraud is very common nowadays. Stealing credit card related information from online shopping and using it in an illegal way happens in many countries. Suppose you have a transactional database for a customer for a particular month. Now the challenging task is to develop an ML application to identify those transactions that were made by the customer themselves and those done by others illegally.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Voice recognition</strong></span>: Recognizing a voice and converting it into a corresponding text command and later performing some actions, as an intelligent agent does. The most widely used applications include Apple Siri, Samsung S-Voice, Amazon's Echo (consumer space), and Microsoft Cortana (especially because Cortana has SDKs for extensibility and integration, and so on). Another example would be locking or unlocking your smartphone by using the recognised voice.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Digit/character recognition</strong></span>: Suppose you have a handwritten zip code or address or message on/inside an envelope, now the task of digit/character recognition is to identify and classify the digits or characters for each handwritten character that is made by different people. An efficient ML application could help in this regard to read and understand handwritten zip codes or characters and sort the contents of the envelope by the geographic region, or more technically, by the image segmentations.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Internet of Things</strong></span>: Large-scale sensor data analytics for prediction and classification from real-time streamed data. For example, smart living room monitoring including water level checking, room temperature checking, home appliances controlling, and so on.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Gaming analytics</strong></span>: Analytics for sports, games, and console-based gaming profiles in order to predict upsell and target in-app purchases and modifications.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Face detection</strong></span>: Given a digital photo album of hundreds or thousands of photographs, the task is to identify those photos that resemble a given person. An efficient ML application, in this case, could help to organise photos by person.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Product recommendation</strong></span>: Provided a purchase history of a customer along with a large inventory of products, the target is to identify those products that the customer will likely be interested in purchasing with an ML system. Business and tech giants such as Amazon, Facebook, and Google Plus have this recommended feature for the users.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Stock trading</strong></span>: Given the current and historical prices for a stock market, predict whether stock should be bought or sold in order to profit with the help of an ML system.</p></li></ul></div><p>The following are some examples of machine learning that are emerging and the demands of current research:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Privacy preserving data mining</strong></span>: Mining customer's purchase rules from the maximal frequent pattern and association rules from business oriented retail databases to increase purchases in the future</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Author name disambiguation</strong></span>: Disambiguation performance is evaluated with manual verification of random samples of pairs from clustering results from a list of authors from a set of given publications</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Recommendation systems</strong></span>: Recommender system based on click stream data using association rule mining</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Text mining</strong></span>: Plagiarism checking from a given text corpus for example</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Sentiment analysis</strong></span>
: A lot of decisions these days are being made by business and tech companies based on the opinion of others, and it will be a good place to innovate machine learning
</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Speech understanding</strong></span>: Given an utterance from a user, the target is to identify the specific request made by the user. A model of this problem would allow a program to understand and make an attempt to fulfill that request. For example, iPhone with Siri and Samsung Voice Recorder in meeting mode have this feature implemented</p></li></ul></div><p>Some of these problems are the hardest problems in artificial intelligence, natural language processing, and computer vision that can be addressed and solved using ML algorithms. Similarly, we will try to develop some ML applications emphasizing these problems in upcoming chapters.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec20"></a>Large scale machine learning APIs in Spark</h2></div></div><hr /></div><p>In this section, we will describe two key concepts introduced by the Spark machine learning libraries (Spark MLlib and Spark ML) and the most widely used implemented algorithms that align with the supervised and unsupervised learning techniques we discussed in the above sections.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec32"></a>Spark machine learning libraries</h3></div></div></div><p>As already stated, in the pre-Spark era, big data modelers typically used to build their ML models using statistical languages such as R, STATA, and SAS. Then the data engineers used to re-implement the same model in Java, for example, to deploy on Hadoop.</p><p>However, this kind of workflow lacks efficiency, scalability, throughput, and accuracy as well as extended execution time.</p><p>Using Spark, the same ML model can be re-built, adopted, and deployed, making the whole workflow much more efficient, robust, and faster, which allows you to provide hands-on insight to increase the performance. The Spark machine learning libraries are divided into two packages: Spark MLlib (<code class="literal">spark.mllib</code>) and Spark ML (<code class="literal">spark.ml</code>).</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec11"></a>Spark MLlib</h4></div></div></div><p>MLlib is Spark's scalable machine learning library, which is the extension of the Spark Core API that provides a library of easy to use machine learning algorithms. Algorithms are implemented and written in Java, Scala, and Python. Spark provides support for local vectors and matrix data types stored on a single machine, as well as distributed matrices backed by one or multiple RDDs:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Spark MLlib</strong></span></p>
</td><td class="auto-generated" style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "> </td><td class="auto-generated" style="border-bottom: 0.5pt solid ; "> </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>ML tasks</strong></span></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Discrete</strong></span></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Continuous</strong></span></p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Supervised</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Classification:</p><p>
</p><p>Logistic regression</p><p>
</p><p>and regularized variants</p><p>
</p><p>Linear SVM</p><p>
</p><p>Naïve Bayes</p><p>
</p><p>Decision trees</p><p>
</p><p>Random forests</p><p>
</p><p>Gradient-boosted trees</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Regression:</p><p>
</p><p>Linear regression</p><p>
</p><p>and regularized variants</p><p>
</p><p>Linear least squares</p><p>
</p><p>Lasso and ridge regression</p><p>
</p><p>Isotonic regression</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Unsupervised</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Clustering:</p><p>
</p><p>K-means</p><p>
</p><p>Gaussian matrix</p><p>
</p><p><span class="strong"><strong>Power iteration clustering</strong></span> (<span class="strong"><strong>PIC</strong></span>)</p><p>
</p><p><span class="strong"><strong>Latent Dirichlet Allocation</strong></span> (<span class="strong"><strong>LDA</strong></span>)</p><p>
</p><p>Bisecting K-means</p><p>
</p><p>Streaming K-means</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Dimensionality reduction, matrix factorization:</p><p>
</p><p>Principal components analysis</p><p>
</p><p>Singular value decomposition</p><p>
</p><p>Alternate least square</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Reinforcement</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>N/A</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>N/A</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; ">
<p>Recommender systems</p>
</td><td style="border-right: 0.5pt solid ; ">
<p>Collaborative filtering:</p><p>
</p><p>Netflix recommendation</p>
</td><td style="">
<p>N/A</p>
</td></tr></tbody></table></div><p>Table 1: Spark MLlib at a glance.</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Legend</strong></span>: Continuous: making predictions about continuous variables, for example, prediction of the maximum temperature for the upcoming days</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Discrete</strong></span>: Assigning discrete class labels to particular observations as outcomes of a prediction, for example, in weather forecasting it could be the prediction of a sunny, rainy, or snowy day</p></li></ul></div><p>The beauty of Spark MLlib is numerous. For example, the algorithms implemented using Scala, Java, and Python are highly-scalable and leverage Spark's ability to work with a massive amount of data. They are fast towards designed for parallel computing with in-memory based operation, which is 100 times faster compared to MapReduce data processing (they also support disk-based operation that is 10 times faster than what MapReduce has as normal data processing) using Dataset, DataFrame, or <span class="strong"><strong>Directed Acyclic Graph</strong></span> (<span class="strong"><strong>DAG</strong></span>)-based RDD APIs.</p><p>They are also diverse, since they cover common machine learning algorithms for regression analysis, classification, clustering, recommender systems, text analytics, frequent pattern mining, and they obviously cover all the steps required to build scalable machine learning applications.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec12"></a>Spark ML</h4></div></div></div><p>Spark ML adds a new set of machine learning APIs to let users quickly assemble and configure practical machine learning pipelines on top of Datasets. Spark ML targets to offer a uniform set of high-level APIs built on top of DataFrames rather than RDDs that help users create and tune practical machine learning pipelines. Spark ML API standardizes machine learning algorithms to make the learning tasks easier to combine multiple algorithms into a single pipeline or data workflow for data scientists.</p><p>Spark ML uses the concept of DataFrame (although it's obsolete in Java but still the main programming interface in Python and R), which is introduced in the Spark 1.3.0 release from Spark SQL as machine learning Datasets. The Datasets hold diverse data types such as columns storing text, feature vectors, and true labels for the data. In addition to this, Spark ML also uses the transformer to transform one DataFrame into another or vice-versa, where the concept of the estimator is used to fit on a DataFrame to produce a new transformer. The pipeline API, on the other hand, can restrain multiple transformers and estimators together to specify an ML data-workflow. The concept of the parameter was introduced to specify all the transformers and estimators to share a common API under an umbrella during the development of an ML application:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Spark ML</strong></span></p>
</td><td class="auto-generated" style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; "> </td><td class="auto-generated" style="border-bottom: 0.5pt solid ; "> </td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>ML tasks</strong></span></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Discrete</strong></span></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Continuous</strong></span></p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Supervised</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Classification:</p><p>
</p><p>Logistic regression</p><p>
</p><p>Decision tree classifier</p><p>
</p><p>Random forest classifier</p><p>
</p><p>Gradient-boosted tree classifier</p><p>
</p><p>Multilayer perception classifier</p><p>
</p><p>One-vs-Rest classifier</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Regression:</p><p>
</p><p>Linear regression</p><p>
</p><p>Decision tree regression</p><p>
</p><p>Random forest regression</p><p>
</p><p>Gradient-boosted tree regression</p><p>
</p><p>Survival regression</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Unsupervised</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Clustering:</p><p>
</p><p>K-means</p><p>
</p><p><span class="strong"><strong>Latent Dirichlet allocation</strong></span> (<span class="strong"><strong>LDA</strong></span>)</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Tree Ensembles:</p><p>
</p><p>Random forests</p><p>
</p><p>Gradient-boosted Trees</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Reinforcement</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>N/A</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>N/A</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; ">
<p>Recommender systems</p>
</td><td style="border-right: 0.5pt solid ; ">
<p>N/A</p>
</td><td style="">
<p>N/A</p>
</td></tr></tbody></table></div><p>Table 2: Spark ML at a glance (legend same as Table 1).</p><p>As shown in table 2, Spark ML also provides several classifications, regression, decision trees, and tree ensembles as well as a clustering algorithm implemented for developing ML pipelines on top of DataFrames. The optimization algorithm under active implementation is called <span class="strong"><strong>Orthant-Wise Limited-memory QuasiNewton</strong></span> (<span class="strong"><strong>OWL-QN</strong></span>), which is also an advanced algorithm that is an extension of L-BFGS that can effectively handle L1 regularization and elastic net (see also at Spark ML Advanced topic, <a class="ulink" href="https://spark.apache.org/docs/latest/ml-advanced.html" target="_blank">https://spark.apache.org/docs/latest/ml-advanced.html</a>).</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec13"></a>Important notes for practitioners</h4></div></div></div><p>However, currently only Pearson's and Spearman's correlation are supported and more are to be added in future Spark releases. Unlike the other statistical functions, stratified sampling is also supported by Spark and it can be performed on RDDs as key-value pairs; however, some functionalities are yet to be added to Python developers. Currently there are no reinforcement learning algorithm modules in Spark Machine Learning libraries (please refer to <span class="emphasis"><em>Table 1</em></span> and <span class="emphasis"><em>Table 2</em></span>). The current implementation of Spark MLlib provides a parallel implementation of FP-growth for mining frequent patterns and the association rules. However, you will have to customize the algorithm for mining maximal frequent patterns accordingly. We will provide a scalable ML application for mining privacy preserving maximal frequent pattern in upcoming chapters.</p><p>Another fact is that the current implementation of the collaborative based recommendation system in Spark does not support the use of real time stream data, however, in later chapters we will try to show a practical recommender system based on click stream data using association rule mining (see Mitchell, Tom M. <span class="emphasis"><em>The Discipline of Machine Learning</em></span>, 2006, <a class="ulink" href="http://www.cs.cmu.edu/" target="_blank">http://www.cs.cmu.edu/</a>. CMU. Web. Dec. 2014). However, some algorithms are not available or are yet to be added to Spark ML, most notably dimensionality reduction is such an example.</p><p>However, developers can seamlessly combine the implementation of these techniques found in Spark MLlib with the rest of the algorithms found in Spark ML as hybrid or interoperable ML applications. Spark's neural networks and perception are brain-inspired learning algorithms covering multiclass, two-class, and regression problems that are not yet implemented in Spark ML APIs.</p></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec21"></a>Practical machine learning best practices</h2></div></div><hr /></div><p>In this section, we will describe some good machine learning practices that need to be followed before developing a machine learning application of particular interest, as described in <span class="emphasis"><em>Figure 7</em></span>:</p><div class="mediaobject"><img src="graphics/image_02_007.jpg" /><div class="caption"><p>Figure 7: Machine learning systematic process.</p></div></div><p>A scalable and accurate ML application demand for following a systematic approach to its development from problem definition to presenting results can be summarized into four steps: problem definition and formulation, data preparation, finding suitable algorithms for machine learning, and finally, presenting the results after the machine learning model deployment. Well, these steps can be depicted as shown in <span class="emphasis"><em>Figure 6</em></span>.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec33"></a>Best practice before developing an ML application</h3></div></div></div><p>The learning of a machine learning system can be formulated as the sum of representation, evaluation, and optimisation. In other words, according to Pedro D et al. (Pedro Domingos, <span class="emphasis"><em>A Few Useful Things to Know about Machine Learning</em></span>, <a class="ulink" href="https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf" target="_blank">https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf</a>):</p><p><span class="emphasis"><em>Learning = Representation + Evaluation + Optimization</em></span></p><p>Taking this formulation into consideration, we will provide some recommendations for practitioners before getting into ML application development.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec14"></a>Good machine learning and data science worth huge</h4></div></div></div><p>So what do we need for an effective machine learning applications development? We actually need four arsenals before we start developing an ML application; including:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The data primitives (or the experimental data to be more frank).</p></li><li style="list-style-type: disc"><p>A pipeline synthesis tool (to understand the data and control flow during the machine learning steps).</p></li><li style="list-style-type: disc"><p>An effective and robust error analysis tools.</p></li><li style="list-style-type: disc"><p>A verification or validation tool (to verify or validate the prediction accuracy or performance of the ML model). However, most importantly, without some strong theoretical basement with good data science that is worth a huge amount, the whole process will be in vain. In fact, many data scientists and machine learning experts often quote something like this statement: <span class="emphasis"><em>if you can pose your problem as a simple optimization problem then you is almost done</em></span> (see <span class="emphasis"><em>Data Analytics &amp; R</em></span>, <a class="ulink" href="http://advanceddataanalytics.net/2015/01/31/condensed-news-7/" target="_blank">http://advanceddataanalytics.net/2015/01/31/condensed-news-7/</a>).</p></li></ul></div><p>That means before you start your machine learning voyage, if you can identify if your problem is a machine learning problem, you will be able to find some suitable algorithms to develop your ML application altogether. Of course, in practice, most machine learning applications can't be changed into simple optimization problems. Therefore, it's the duty of a data scientist like you to manage and maintain complex datasets. After that, you will have to handle other issues such as the analytical problems that evolve when engineering the machine learning pipeline to tackle those issues we mentioned earlier.</p><p>Therefore, the best practice is to use Spark MLlib, Spark ML, GraphX, and Spark Core APIs along with the best practice data science heuristics for developing your machine learning applications together. Now you might think of getting benefits out of it; yes, the benefits are obvious, and they are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Built-in distributed algorithms</p></li><li style="list-style-type: disc"><p>In-memory and disk-based data computation and processing</p></li><li style="list-style-type: disc"><p>In-memory capabilities for iterative workloads</p></li><li style="list-style-type: disc"><p>Algorithmic accuracy and performance</p></li><li style="list-style-type: disc"><p>Faster data cleaning, feature engineering and feature selection, training, and testing</p></li><li style="list-style-type: disc"><p>Real-time visualization of the predictive results</p></li><li style="list-style-type: disc"><p>Tuning towards better performance</p></li><li style="list-style-type: disc"><p>Adaptability for new datasets</p></li><li style="list-style-type: disc"><p>Scalability with the increasing datasets</p></li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec15"></a>Best practice – feature engineering and algorithmic performance</h4></div></div></div><p>In best practice, feature engineering should be considered as one of the most important parts of machine learning. The thing is to find a better representation of features out of the experimental dataset non-technically. In parallel to this, which learning algorithms or techniques are to be used are also important. Parameter tuning, of course in addition, however, the final choice is more about  experimentation through the ML model you will be developing.</p><p>In practice, however, it is trivial to grasp the naive performance baseline by means of an <span class="strong"><strong>out-of-the-box</strong></span> method (also referred to as functionality or <span class="strong"><strong>OOTB</strong></span> in short, which is a feature of a product of interest that works straight away after installing or configuring) and good data pre-processing. Therefore, you might be doing it continually in order to know where the baseline is and whether this performance is of a satisfactory level or good enough for your requirements.</p><p>Once you've trained all of your out-of-the-box methods, it's always recommended and is a good idea to try bagging them together. Moreover, in order to solve the ML problems, very often you might need to know the reality that computationally hard problems (shown in section 2, for example) need either domain-specific knowledge or lots of digging down in the data or both. Consequently, the combination of a widely accepted feature engineering technique and domain-specific knowledge would help your ML algorithm/application/system to solve prediction related problems.</p><p>In a nutshell, if you have the required dataset and a robust algorithm that can take the advantages of the dataset by learning the complex features, it's almost guaranteed that you will be successful. Furthermore, sometimes domain experts might be wrong in selecting the good features; therefore, incorporation of multiple domain experts (problem domain expert), more well-structured data, and ML expertise is always helpful.</p><p>Last but not least, sometimes it is recommended from our side to consider the error rate rather than only the accuracy. For example, suppose an ML system with 99% accuracy and 50% errors is worse than the one with 90% accuracy but 25% errors, for example.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec16"></a>Beware of overfitting and underfitting</h4></div></div></div><p>A common mistake often made by novice data scientists is subject to the overfitting issue that might evolve while building your ML model by hearing without generalizing. More technically, if you evaluate your model on the training data instead of test or validated data, you probably won't be able to articulate whether your model is overfitting or not. The common symptoms are:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Predictive accuracy of the data used for training can be over accurate (that is, sometimes even 100%)</p></li><li style="list-style-type: disc"><p>And the model might show a little better compared to the random prediction for new data</p></li></ul></div><p>Sometimes the ML model itself becomes under-fit for a particular tuning or data point, which means the model has become too simplistic. Our recommendation (like others as well we believe) is as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Split the dataset into two sets to detect overfitting situations, the first one being for training and model selection, called the training set; the second one is the test set for evaluating the model stated in place of the ML workflow section</p></li><li style="list-style-type: disc"><p>Alternatively, you also could void the overfitting by consuming simpler models (for example, linear classifiers in preference to Gaussian kernel SVM) or by swelling the regularisation parameters of your ML model (if available)</p></li><li style="list-style-type: disc"><p>Tune the model with a correct data value of parameters to avoid both overfitting as well as underfitting</p></li></ul></div><p>Hastie et al. (Hastie Trevor, Tibshirani Robert, Friedman Jerome, <span class="emphasis"><em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</em></span>, Second Edition, 2009) on the other hand, have recommended splitting the large-scale dataset into three sets: Training set (50%), Validation set (25%), and Test set (25%) (roughly). They also suggested building the model using the training set and calculating the prediction errors using the validation set. The test set was recommended to be used to assess the generalization error of the final model.</p><p>If the amount of labeled data available during the supervised learning is smaller, it is not recommended to split the datasets. In that case, use cross-validation or Train split techniques (this will be discussed in <a class="link" href="#" linkend="ch07">Chapter 7</a>, <span class="emphasis"><em>Tuning Machine Learning Models,</em></span> with several examples). More specifically, divide the data set into 10 parts of (roughly) equal size, after that for each of these ten parts, train the classifier iteratively and use the 10th part to test the model.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec17"></a>Stay tuned and combining Spark MLlib with Spark ML</h4></div></div></div><p>The first step of the pipeline designing is to create the building blocks (as a directed or undirected graph consisting of nodes and edges) and make a link between those blocks. Nevertheless, as a data scientist, you should be focused on scaling and optimizing nodes (primitives) too, so that you are able to scale-up your application for handling large-scale datasets in the later stage to make your ML pipeline consistently perform. The pipeline process will also help you to make your model adaptive for new datasets. However, some of these primitives might be explicitly defined to particular domains and data types (for example, text, images, video, audio, and spatiotemporal).</p><p>And beyond these types of data, the primitives should also be working for the general purpose domain statistics or mathematics. The casting of your ML model in terms of these primitives will make your workflow more transparent, interpretable, accessible, and explainable. A recent example would be the ML-Matrix, which is a distributed matrix library that can be used on top of Spark:</p><div class="mediaobject"><img src="graphics/image_02_008.jpg" /><div class="caption"><p>Figure 8: Stay tune and interoperate ML, MLlib, and GraphX.</p></div></div><p>As we already stated in the previous section, as a developer you can seamlessly combine the implementation techniques in Spark MLlib along with the algorithms developed in Spark ML, Spark SQL, GraphX, and Spark Streaming as hybrid or interoperable ML applications on top of RDD, DataFrame, and Datasets, as shown in Figure 8. For example, an IoT-based real-time application could be developed using a hybrid model. Therefore, the recommendation here is to stay tuned or synchronized with the latest technologies around you for the betterment of your ML application.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec18"></a>Making ML applications modular and simplifying pipeline synthesis</h4></div></div></div><p>Another good and often used practice when building your ML pipeline is to make the ML system modular. Some supervised learning problems can be solved using very simple models commonly referred to as generalized linear models. However, it depends on the data you will be using and others simply don't.</p><p>Therefore, to conglomerates a series of simple linear binary classifiers, try to employ a lightweight modular architecture. This might be at the workflow stems or at the algorithms level. The advantages are obvious, since the modular architecture of your application handles massive amounts of data flow in a parallel and distributed way. Consequently, we suggest you have the three key innovative mechanisms: weighted threshold sampling, logistic calibration, and intelligent data partitioning as mentioned in the literature (for example, Yu Jin; Nick Duffield; Jeffrey Erman; Patrick Haffner; Subhabrata Sen; Zhi Li Zhang, <span class="emphasis"><em>A Modular Machine Learning System for Flow-Level Traffic Classification in Large Networks</em></span>, ACM Transactions on Knowledge Discovery from Data, V-6, Issue-1, March 2012). The target is to achieve scalability and high-throughput while attaining a high accuracy of the predicted results from your ML application/system. While primitives can serve as building blocks, you still need some other tools that enable users to build ML pipelines.</p><p>Subsequently, workflow tools have become more common these days, and such tools exist for data engineers, data scientists, and even for business analysts such as Alteryx, RapidMiner, Alpine Data, and Dataiku. At this point, we are talking about and stressing the business analysts since at the very last phase your target customer will be a business company who will value your ML model, right? The latest release of Spark comes with Spark ML APIs for building machine learning pipelines and making a domain specific language (see <a class="ulink" href="https://en.wikipedia.org/wiki/Domain-specific_language" target="_blank">https://en.wikipedia.org/wiki/Domain-specific_language</a>) for pipelines.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec19"></a>Thinking of an innovative ML system</h4></div></div></div><p>However, in order to develop the algorithms to learn the ML models continuously with the help of available data, the viewpoint behind the machine learning is to automate the creation of analytical models. Unremittingly evolving models produce increasingly positive results and reduce the need for human interaction. This enables the ML models to automatically produce reliable and repeatable predictions.</p><p>More technically, suppose you are planning to develop a recommender system using ML algorithms. So, what is the target of developing that recommender system? And what are some innovative ideas for product development in machine learning? These two are typical questions that should be considered before you start developing your ML application or system. Consistent innovation might be challenging, especially when stirring advancing with new ideas, it can also be tough to comprehend where the greatest benefit lies. Machine learning can provision innovation from end to end of a variety of paths, such as determining weaknesses with current products, predictive analysis, or identifying previously concealed patterns.</p><p>As a result, you will have to think of large-scale computing to train your ML model offline, and later on your recommender system has to be able to work as a conventional search engine analysis for online recommendations. Thus, your ML application will be valued by a business company if your system:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Can forecast buying items using your machine learning application</p></li><li style="list-style-type: disc"><p>Can do product analysis</p></li><li style="list-style-type: disc"><p>Can work as an emerging trend in production</p></li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec20"></a>Thinking and becoming smarter about Big Data complexities</h4></div></div></div><p>As shown in Figure 9, new business models are the unavoidable extension of the available data utilisation, so consideration of big data and its business values can make the business analyst's job, life and thinking smarter, which results in your targeted company delivering value to customers. In addition to this, you will also have to investigate (analyze to be more exact) rival or better companies.</p><p>Now the question is, how do you collect and use enterprise data? Big data is not only about the size (volume), it is also related to its velocity, veracity, variety, and value. For these types of complexities, for example, velocity can be addressed using Spark Streaming since streaming-based data is also big data that needs a real-time analytical approach. Other parameters such as volume and variety can be handled using Spark Core and Spark MLlib/ML towards big data processing.</p><p>Well, you will have to manage the data by hook or by crook. If you are able to manage the data, the insights from the data can really shake up the way businesses operate with the useful features of big data:</p><div class="mediaobject"><img src="graphics/image_02_009.jpg" /><div class="caption"><p>Figure 9: Machine learning in Big Data best practice.</p></div></div><p>At this point, data alone is not enough (see Pedro Domingos, <span class="emphasis"><em>A Few Useful Things to Know about Machine Learning, </em></span><a class="ulink" href="https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf" target="_blank">https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf</a>), but extracting meaningful features from the data and putting semantics of data into the model is more important. This is like what most of the tech giants such as LinkedIn are developing through large-scale machine learning frameworks from feature targeting for their community, which is more or less a supervised learning technique. The workflow is as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Fetch the data, extract the feature, and set the target</p></li><li style="list-style-type: disc"><p>Feature and target join</p></li><li style="list-style-type: disc"><p>Create a snapshot from the concatenated data</p></li><li style="list-style-type: disc"><p>Partition the snapshot into two parts: training set and test set</p></li><li style="list-style-type: disc"><p>From the training set, prepare the sample data by sampling techniques</p></li><li style="list-style-type: disc"><p>Train the model using the sampled data</p></li><li style="list-style-type: disc"><p>Scoring</p></li><li style="list-style-type: disc"><p>Evaluate the model from the previously developed persistent model, as well as the test data prepared in step 4</p></li><li style="list-style-type: disc"><p>If the best model is found</p></li><li style="list-style-type: disc"><p>Deploy the model for the target audience</p></li></ul></div><p>So what's next? Your model also should be adaptable to large-scale dynamic data such as real-time streaming IoT data PLUS real-time feedback is also important so that your ML system can learn from the mistakes. The next sub-section discusses that.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec21"></a>Applying machine learning to dynamic data</h4></div></div></div><p>The reasons are obvious, since machine learning brings concrete and dynamic aspects to IoT projects. Recently, machine learning has experienced a pep talk in popularity amongst industrial companies and they profit out of the box. As a result, all but every IT vendor are precipitously announcing IoT platforms and consulting services. But achieving financial benefits through IoT data is not an easy job. Moreover, many businesses have failed to clearly determine what areas will change with the implementation of an IoT strategy.</p><p>Considering these positive and negative issues together, your ML model should adapt to large dynamic data since the large-scale data means billions of records, large feature spaces, and low positive rates from the sparsity issue. Nevertheless, data is dynamic so consequently, the ML models have to be adaptive enough; otherwise you will have to face a bad experience or be lost in the black hole.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec34"></a>Best practice after developing an ML application</h3></div></div></div><p>The typical steps that are best practice after an ML model/system has been developed are: visualization for understanding the predictive values, model validation, error and accuracy analysis, model tuning, model adapting, and scaling up for handling large-scale datasets with ease.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec22"></a>How to enable real-time ML visualization</h4></div></div></div><p>Visualization provides an interactive interface to stay tune the ML model itself. Therefore, without visualizing the predictive results, it merely becomes difficult to further improve the performance of an ML application. The best practice could be something like this:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Incorporate some third-party tools along with GraphX for your visualization for large-scale graph related data (more to be discussed in <span class="emphasis"><em><a class="link" href="#" linkend="ch09">Chapter 9</a></em></span>, <span class="emphasis"><em>Advanced Machine Learning with Streaming and Graph Data</em></span>)</p></li><li style="list-style-type: disc"><p>For non-graph data, a call-back interface for the Spark ML algorithm to send and receive messages by incorporating other tools like Apache Kafka:
</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Algorithms decide when and what message to send</p></li><li style="list-style-type: disc"><p>Algorithms don't care how the message is delivered</p></li></ul></div><p>
</p></li><li style="list-style-type: disc"><p>A task channel to handle the message delivery service from the Spark Driver program to Spark Client or Spark cluster nodes. The task channel would be communicating using Spark Core at a lower level of abstraction:
</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>It does not care about the content of the message or recipient of the message</p></li></ul></div><p>
</p></li><li style="list-style-type: disc"><p>The message is delivered from Spark Client to the browser or visualization client:
</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>We recommend using HTML5 <span class="strong"><strong>Server-Sent Events</strong></span> (<span class="strong"><strong>SSE</strong></span>) and HTTP Chunked Response (PUSH) together. Incorporation of Spark with this type of technology will be discussed in <a class="link" href="#" linkend="ch10">Chapter 10</a>, <span class="emphasis"><em>Configuring and Working with External Libraries</em></span>
</p></li><li style="list-style-type: disc"><p>Pull is possible; however, it requires a message queue</p></li></ul></div><p>
</p></li><li style="list-style-type: disc"><p>Visualization using JavaScript frameworks such as <code class="literal">Plot.ly</code> (please refer to <a class="ulink" href="https://plot.ly/" target="_blank">https://plot.ly/</a>) and <code class="literal">D3.js</code> (please refer to <a class="ulink" href="https://d3js.org/" target="_blank">https://d3js.org/</a>)</p></li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec23"></a>Do some error analysis</h4></div></div></div><p>As algorithms become more prevalent, we need better tools for building complex hitherto, robust, and stable machine learning systems. A popular distributed framework like Apache Spark takes these ideas to extremely large datasets for the wider audience. Therefore, it would be better if we could bind approximation errors and convergence rates for the layered pipelines.</p><p>Assuming we can compute error bars for nodes, the next step would be to have a mechanism for extracting error bars for these pipelines. However, in practice, when the ML model is deployed for the production, we might need tools to confirm that the pipeline will work and will not do make malfunction or stop halfway through and that it can provide some expected measure of the errors.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec24"></a>Keeping your ML application tuned</h4></div></div></div><p>Devising one or two algorithms that perform solidly well on a simple problem can be considered as a good kick-off. However, sometimes you may be thirsty to get the best accuracy, by even sacrificing your valuable time and available computational resources. This would be a smarter way, and it will help you not only to squeeze out extra performance, but also to improve the results in terms of accuracy that you were receiving out of the machine learning algorithms you designed previously. In order to do that, when you tune the model and related algorithm, essentially, you must have a high confidence in the results.</p><p>Obviously, those results will be available after you specify the testing and validation. This means you should only be using those techniques that reduce the variance of the performance measure so that you can assess the algorithms that are running more smoothly.</p><p>In parallel, like most data practitioners, we also suggest you to use the cross-validation technique (also often called rotation estimation) with a reasonably high number of folds (that is, K-fold cross-validation, where a single subsample is used as the validation dataset for testing the model itself , and the remaining K-1 subsamples are used to train the data). Although the exact number of folds, or K, depends on your dataset, however, 10-fold cross-validation is commonly used, but most often the value of K remains unfixed. We will mention three strategies here that you will need to tune your machine learning model:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Algorithm tuning</strong></span>: Makes your machine learning algorithm parameterized. After that, adjust the value of those parameters (if they have multiple parameters) to influence the outcome of the overall learning process.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Ensembles</strong></span>: Sometimes it is good to be naïve! Therefore, in order to get improved results, keep trying to combine the outcomes from multiple machine learning methods or algorithms.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Extreme feature engineering</strong></span>: If your data has complex and multi-dimensional structures embedded in it, ML algorithms know how to find and exploit it to make decisions.</p></li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec25"></a>Keeping your ML application adaptive and scale-up</h4></div></div></div><p>As shown in Figure 10, the adaptive learning conglomerates the previous generations of rule-based, simple machine learning, and deep learning approaches to machine intelligence according to Rob Munro:</p><div class="mediaobject"><img src="graphics/image_02_010.jpg" /><div class="caption"><p>Figure 10: Four generation of machine intelligence (Figure courtesy of Rob Munro).</p></div></div><p>The fourth generation of machine learning: adaptive learning, (<code class="literal">http://idibon.com/the-fourth-generation-of-machine-learning-adaptive-learning/#comment-175958</code>).</p><p>Research also shows that adaptive learning is 95% accurate in predicting people's intention to purchase a car, for example (please refer to Rob Munro, <span class="emphasis"><em>The fourth generation of machine learning: Adaptive learning</em></span>, <code class="literal">http://idibon.com/the-fourth-generation-of-machine-learning-adaptive-learning/#comment-175958</code>). Moreover, if your ML application is adaptive with the new environment and new data, it is expected that if enough infrastructure is provided, your ML system can be scaled-up for the increasing data loads.</p></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec22"></a>Choosing the right algorithm for your application</h2></div></div><hr /></div><p><span class="emphasis"><em>What machine learning algorithm should I use?</em></span> is a very frequently asked question for the Naive machine learning practitioners, but the answer is always i<span class="emphasis"><em>t depends on</em></span>. More elaborately:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>It depends on the volume, quality, complexity, and the nature of the data that has to be tested/used</p></li><li style="list-style-type: disc"><p>It depends on external environments and parameters such as your computing system's configuration or underlying infrastructures</p></li><li style="list-style-type: disc"><p>It depends on what you want to do with the answer</p></li><li style="list-style-type: disc"><p>It depends on how the mathematical and statistical formulation of the algorithm was translated into machine instructions for the computer</p></li><li style="list-style-type: disc"><p>And it depends on how much time you have</p></li><li style="list-style-type: disc"><p>
<span class="emphasis"><em>Figure 11</em></span> provides a complete work-flow for choosing the right algorithm for your ML problem. However, note that some tricks might not work-flow depending upon data and problem types:</p><div class="mediaobject"><img src="graphics/B05243_02_11-1-976x1024.jpg" /><div class="caption"><p>Figure 11: A work-flow for choosing the right algorithm</p></div></div></li></ul></div><p>The reality is, even the most experienced data scientists or data engineers can't give a straight recommendation about which ML algorithm performs best before trying them all together. Most of the statements of agreement/disagreement begins with <span class="emphasis"><em>It depends on...hmm...</em></span>Habitually, you might be contemplative if there are cheat sheets of machine learning algorithms and if so, how to use that cheat sheet. Several data scientists we talked to said that the only sure way to find the very best algorithm is to try all of them; therefore, there is no shortcut dude! Let's make it clear, suppose you do have a set of data and you want to do some clustering. Thus, technically, this could be classification or regression if your data is labeled/unlabeled or values or training set data. Now, the first concern that evolves in your mind is:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Which factors should I consider before choosing an appropriate algorithm? Or should I just choose an algorithm randomly?</p></li><li style="list-style-type: disc"><p>How do I choose any data pre-processing algorithm or tools that can be applied to my data?</p></li><li style="list-style-type: disc"><p>What sort of feature engineering techniques should I be using to extract the useful features?</p></li><li style="list-style-type: disc"><p>What factors can improve the performance of my ML model?</p></li><li style="list-style-type: disc"><p>How can I adopt my ML application for new data types?</p></li><li style="list-style-type: disc"><p>Can I scale-up my ML application for large-scale datasets? And so on.</p></li></ul></div><p>You will always expect the best answer that is much more justified and explains everything that someone should consider. In this section, we will try to answer these questions with our little machine learning knowledge.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec35"></a>Considerations when choosing an algorithm</h3></div></div></div><p>The recommendation or suggestions we are providing here are for the novice data scientist with learner machine learning to expert data scientists who are trying to choose an optimal algorithm to start with the Spark ML APIs. That means, it makes some overviews and oversimplifications, but it will point you in a safe direction, believe us! Suppose you are planning to develop an ML system to answer the following question based on the rule:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><code class="literal">IF</code> feature X has property Z <code class="literal">THEN</code> do Y</p></li></ul></div><p>Affirmatively, there should be such rules:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>IF X <code class="literal">THEN</code> it is sensible to try Y using property Z and avoid W</p></li></ul></div><p>However, what is sensible and what is not depends on:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Your application and the expected complexity of the problem.</p></li><li style="list-style-type: disc"><p>Size of the data set (that is, how many rows/columns, how many independent cases).</p></li><li style="list-style-type: disc"><p>Is your dataset labeled or unlabeled?</p></li><li style="list-style-type: disc"><p>Type of data and the kind of measurement, since different nature of data suggests a different order or structure, right?</p></li><li style="list-style-type: disc"><p>And obviously in practice your experience in applying different methods efficiently and intelligently.</p></li></ul></div><p>Moreover, if you want to have a general answer to a general problem, we recommend the Elements of Statistical Learning (Hastie Trevor, Tibshirani Robert, Friedman Jerome, <span class="emphasis"><em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</em></span>, Second Edition, 2009) for a fresh start. Nevertheless, we also recommend going with the following algorithmic properties that:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Show excellent accuracy</p></li><li style="list-style-type: disc"><p>Have fast training times</p></li><li style="list-style-type: disc"><p>And the use of linearity</p></li></ul></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec26"></a>Accuracy</h4></div></div></div><p>Getting the most accurate results from your ML application isn't always indispensable. Depending on what you want to use it for, sometimes an approximation is adequate enough. If the situation is something like this, you may be able to reduce the processing time drastically by incorporating the better-estimated methods. When you are familiar with the workflow with the Spark machine learning APIs, you will enjoy the advantage of having more approximation methods, because those approximation methods will tend to avoid the overfitting problem out of your ML model automatically.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec27"></a>Training time</h4></div></div></div><p>The execution time requires finishing the data preprocessing or building the model and varies a great deal across different algorithms, the inherited complexities, and of course the robustness. The training time is often closely related to the accuracy. In addition, often you will discover that some of the algorithms you will be using are elusive to the number of data points compared to others. However, when your time is sufficient and especially when the dataset is larger, for doing all the formalities, it can get-up-and-go the choice of algorithm. Therefore, if you are concerned particularly with the time, try to sacrifice the accuracy or performance and use a simple algorithm that fulfils your minimum requirements.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec28"></a>Linearity</h4></div></div></div><p>There are many machine learning algorithms developed recently that make use of linearity (also available in the Spark MLlib and Spark ML). For example, the linear classification algorithms allow classes to be separated by plotting a differentiating straight line or otherwise by the higher-dimensional equivalents of the datasets. A linear regression algorithm, on the other hand, assumes that data trends follow a simple straight line. This assumption is not naive for some machine learning problems; however, there might be some other cases where the accuracy will be down. Despite their hazards, linear algorithms are very popular for the data engineers or data scientists as the first line of the outbreak. Moreover, these algorithms also tend to be algorithmically simple and fast to train your models during the whole process.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec36"></a>Talking to your data when choosing an algorithm</h3></div></div></div><p>You will find many machine learning datasets available for free here at <a class="ulink" href="http://machinelearningmastery.com/tour-of-real-world-machine-learning-problems/" target="_blank">http://machinelearningmastery.com/tour-of-real-world-machine-learning-problems/</a> or at the UC Irvine Machine Learning Repository (at <a class="ulink" href="http://archive.ics.uci.edu/ml/" target="_blank">http://archive.ics.uci.edu/ml/</a>). The following data properties should also be placed first:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Number of parameters</p></li><li style="list-style-type: disc"><p>Number of features</p></li><li style="list-style-type: disc"><p>Size of the training dataset</p></li></ul></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec29"></a>Number of parameters</h4></div></div></div><p>Parameters or data properties are the handholds for a data scientist like you that gets to turn when setting up an algorithm. They are numbers that affect the algorithm's performance, such as error tolerance or the number of iterations, or options between variants of how the algorithm acts. The training time and accuracy of the algorithm can sometimes be quite sensitive to getting the right settings. Typically, algorithms with a large number of parameters require trial and error to find an optimal combination.</p><p>Despite the fact that this is a great way to span the parameter space, the model building or training time increases exponentially with the increased number of parameters. This is a dilemma as well as a time-performance trade-off. The positive sides are having many parameters characteristically indicates greater flexibility of the ML algorithms. And secondly, your ML application achieves much better accuracy.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec30"></a>How large is your training set?</h4></div></div></div><p>If your training set is smaller, high bias with low variance classifiers such as Naive Bayes have an advantage over low bias with high variance classifiers such as kNN. Therefore, the latter will over fit. But low bias with high variance classifiers, on the other hand, start to win out as your training set grows linearly or exponentially since they have lower asymptotic errors. This is because high bias classifiers aren't powerful enough to provide accurate models. You can also think of this as a trade-off between generative models versus discriminative model distinction.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec31"></a>Number of features</h4></div></div></div><p>For certain types of experimental datasets, the number of extracted features can be very large compared to the number of data points itself. This is often the case with genomics, biomedical, or textual data. A large number of features can swamp some learning algorithms, making training time ridiculously high. Support vector machines are particularly well suited in this case for its high accuracy, nice theoretical guarantees regarding overfitting, and an appropriate kernel.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec37"></a>Special notes on widely used ML algorithms</h3></div></div></div><p>In this section, we will provide some special notes for the most commonly used machine learning algorithm or techniques. The techniques we will emphasis are logistic regression, linear regression, recommender system, SVM, decision tree, random forest, Bayesian method and decision forests, decision jungles, and variants. Table 3 shows the pros and cons of some widely used algorithms including where and when to chose these algorithms.</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Algorithm</strong></span></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Pros</strong></span></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Cons</strong></span></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Better at</strong></span></p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Linear regression (LR)</strong></span></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Very fast and often runs in a constant time</p><p>
</p><p>Easy to understand the modelling</p><p>
</p><p>Less prone to overfitting and underfitting</p><p>
</p><p> Intrinsically simple</p><p>
</p><p>Very fast so less model building time</p><p>
</p><p>Less prone to overfitting and underfitting</p><p>
</p><p>Has low variance</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Often unable for complex data modelling</p><p>
</p><p>Often unable to conceptualize the nonlinear relationships without transforming the input Dataset</p><p>
</p><p>Not suitable for complex modelling</p><p>
</p><p>Works better with only single decision boundary</p><p>
</p><p> Requires large sample size to achieve stable results</p><p>
</p><p>High bias</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Numerical dataset with large collection of features</p><p>
</p><p>Widely used in biological, behavioral and social sciences to predict possible relationships among variables</p><p>
</p><p>Works well for numerical as well as categorical variables</p><p>
</p><p>Used in various fields, including the medical and social sciences</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Decision trees (DT)</strong></span></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p> Less model building and prediction time</p><p>
</p><p>Robust against the noise and missing values</p><p>
</p><p>High accuracy</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Interpretation is hard with large and complex trees</p><p>
</p><p>Duplication may occur within the same sub-tree</p><p>
</p><p>Possible issues with diagonal decision boundaries</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p> Targeting high accurate classification</p><p>
</p><p>Medical diagnosis and prognosis</p><p>
</p><p>Credit risk analytics</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Neural networks (NN)</strong></span></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p> Extremely powerful and robust</p><p>
</p><p>Capable of modelling very complex relationships</p><p>
</p><p>Can be working without knowing the underlying data</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Highly overfitting and underfitting prone</p><p>
</p><p>High training and prediction time</p><p>
</p><p>Computationally expensive requiring significant computing power</p><p>
</p><p>Model is not readable or reusable</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p> Image processing</p><p>
</p><p>Video processing</p><p>
</p><p>Human-intelligence</p><p>
</p><p>Robotics</p><p>
</p><p>Deep learning</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Random forest (RF)</strong></span></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Good for bagged trees</p><p>
</p><p>Low variance</p><p>
</p><p>High accuracy</p><p>
</p><p>Can handle the overfitting problem</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Not as easy to visually and interpret</p><p>
</p><p>High training and prediction time</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>When dealing with multiple features which may be correlated</p><p>
</p><p>Biomedical diagnosis and prognosis</p><p>
</p><p>Can be applied both for classification and regression</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Support vector machines (SVM)</strong></span></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>High accuracy</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Susceptible to overfitting and underfitting</p><p>
</p><p>No numerical stability</p><p>
</p><p>Computationally expensive requiring large computing power</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Image classification</p><p>
</p><p>Handwriting recognition</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>K-nearest neighbors (K-NN)</strong></span></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Simple and powerful</p><p>
</p><p>Lazy training involved</p><p>
</p><p>Can be applied for both multiclass classification and regression</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>High training and prediction time</p><p>
</p><p>Need to have accurate distance function</p><p>
</p><p>Low performance with high dimensional dataset</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Low-dimensional datasets</p><p>
</p><p>Anomaly detection like outlier detection</p><p>
</p><p>Fault detection in semiconductor</p><p>
</p><p>Gene expression</p><p>
</p><p>Protein-protein interaction</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>K-means</strong></span></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Linear execution time</p><p>
</p><p>Perform better than hierarchical clustering</p><p>
</p><p>Excellent with hyper-spherical  clusters</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Repeatable and lack consistency</p><p>
</p><p>Requires prior knowledge of K</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Is not a good choice if the natural clusters occurring in the dataset are non-spherical</p><p>
</p><p>Good for large dataset</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Latent Dirichilet Allocation (LDA)</strong></span></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Can be applied for large-scale text datasets</p><p>
</p><p>Can overcome the overfitting problem of pLSA</p><p>
</p><p>Can be applied for both document classification and clustering through topic modelling</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Cannot be applied with high dimensional and complex texts databases</p><p>
</p><p>Requires the specification of the number of topics</p><p>
</p><p>Cannot find the granularity at optimum level</p><p>
</p><p>Hierarchical Dirichlet Process (HDP) is the better choice</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Document classification and clustering through topic modelling from large-scale text dataset</p><p>
</p><p>Can be applied in NLP and other text analytics</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Naive Bayes (NB)</strong></span></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Computationally fast</p><p>
</p><p>Simple to implement</p><p>
</p><p>Works well with high dimensions</p><p>
</p><p>Can handle missing values</p><p>
</p><p>Is adaptable since the model can be modified with new training data without rebuilding the model</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Relies on independence assumption so performs badly if the assumption does not met</p><p>
</p><p>Relatively low accuracy</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>When data has lots of missing values</p><p>
</p><p>Dependencies of features from each other are similar between features</p><p>
</p><p>Spam filtering and classification</p><p>
</p><p>Classifying a news article about technology, politics, or sports</p><p>
</p><p>Text mining</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; ">
<p><span class="strong"><strong>Singular Value decomposition (SVD) and Principal Component Analysis (PCA)</strong></span></p>
</td><td style="border-right: 0.5pt solid ; ">
<p>Reflects the real intuitions about the data</p><p>
</p><p>Allows estimation probabilities in high-dimensional data</p><p>
</p><p>Dramatic reduction in size of data</p><p>
</p><p>Both are based on strong linear algebra</p>
</td><td style="border-right: 0.5pt solid ; ">
<p>Too expensive for many applications like Twitter and web analytics</p><p>
</p><p>Disastrous for task with fine-grained classes</p><p>
</p><p>Need proper understanding of the linearity</p><p>
</p><p>Often complexity is cubic</p><p>
</p><p>Computationally slower</p>
</td><td style="">
<p>SVD is applied for low-rank matrix approximation, image processing, bioinformatics,  signal processing,  NLP</p><p>
</p><p>PCA is used for interest rate derivatives portfolios, neuroscience and so on</p><p>
</p><p>Both are suitable for the dataset having high dimension and multivariate data</p>
</td></tr></tbody></table></div><p>Table 3: Pros and cons of some widely used algorithms</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec32"></a>Logistic regression and linear regression</h4></div></div></div><p>Logistic regression is a powerful tool developed around the globe for its two-class and multiclass classification since it's fast as well as simple. The fact is that it uses an <span class="emphasis"><em>S</em></span>-shaped curve instead of a straight line. making it a natural fit for partitioning data into groups. It provides linear class boundaries, so that when you use it, make sure a linear approximation is something you can survive with. Unlike the decision trees or SVMs, you also have a nice probabilistic interpretation, so you will be able to update your model to adapt for new datasets easily.</p><p>Therefore, the recommendation is, use it if you want to have a flavor of probabilistic framework or if you expect to receive more training data in the future to be incorporated into your model. As mentioned previously, linear regression fits a line, plane, or hyperplane to the dataset. It's a workhorse, simple and fast, but it may be overly simplistic for some problems.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec33"></a>Recommendation systems</h4></div></div></div><p>We already talked about the accuracy and performance issues of mostly used ML algorithms and tools. However, beyond the accuracy research on recommender systems is concern about finding another environmental factor or/and parameter diversity. Therefore, a recommendation system with good accuracy and higher intra-list diversity will be the winner. As a result, your product will be precious to your target customers. It would be, however, more effective to let the users re-rate the items, rather than showing new items only. If your clients have some extra requirements that need to be fulfilled, such as privacy or security, your system has to be able to deal with the privacy related issues.</p><p>This is particularly important because customers have to provide some personal information as well, so it is recommended not to expose that sensitive information publicly.</p><p>Building user profiles using some robust techniques or algorithms such as collaborative filtering, on the other hand, could be problematic from the privacy perspective. Moreover, research in this area has found that user demographics information may influence how satisfied the other users are with recommendations (see also in Joeran Beel, Stefan Langer, Andreas Nürnberger, Marcel Genzmehr, <span class="emphasis"><em>The Impact of Demographics (Age and Gender) and Other User Characteristics on Evaluating Recommender Systems</em></span>. In Trond Aalberg and Milena Dobreva and Christos Papatheodorou and Giannis Tsakonas and Charles Farrugia. <span class="emphasis"><em>Proceedings of the 17th International Conference on Theory and Practice of Digital Libraries, Springer, pp. 400-404, Retrieved 1 November 2013</em></span>).</p><p>Although the serendipity is a crucial measure of how surprising the recommendations are, ultimately trust needs to be built using the recommender system. This can be made possible by explaining how it generates the recommendations, and why it recommends an item even with little demographic information, from the user.</p><p>Therefore, if the user does not trust the system at all, they will not provide any demographic information or will not re-rate the items. A SVMs, according to <span class="emphasis"><em>Cowley et al</em></span>. (G. C. Cawley and N. L. C. Talbot, <span class="emphasis"><em>Over-fitting in model selection and subsequent selection bias in performance evaluation, Journal of Machine Learning Research, vol. 11, pp. 2079-2107, July 2010</em></span>), there are several advantages of Support Vector Machines:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>You can tackle the problem of the over-fitting problem since SVMs provide you with a regularization parameter</p></li><li style="list-style-type: disc"><p>SVM use the kernel trick that helps to build the machine learning model via engineering the kernel with ease</p></li><li style="list-style-type: disc"><p>An SVM algorithm is developed, designed, and defined based on a convex optimization problem, therefore, there is no concept of local minima</p></li><li style="list-style-type: disc"><p>It is a ballpark figure to a bound on the test error rate, where there is a significant and well-studied theory that works</p></li></ul></div><p>These promising features of SVM really would help you, and it is suggested that it should be used frequently. On the other hand, the cons are:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The theory only can really cover determination of the parameters for a given value of the regularization and kernel parameters. Therefore, you could only choose the kernel.</p></li><li style="list-style-type: disc"><p>There might be a worse scenario as well, where the kernel model itself can be quite sensitive to over-fitting during the model selection criterion.</p></li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec34"></a>Decision trees</h4></div></div></div><p>Decision trees are cool because of their usability they are easy to interpret and explain the machine learning problem around. In parallel, they can easily be handled for the feature related interactions. Most importantly, they are often non-parametric. Therefore, even if you are an ordinary data scientist with limited working proficiencies, you don't need to be worried about the issues such as outliers, parameter setting, and tuning. Sometimes fundamentally, you can relay with the decision trees so that they will make your stress for handling issue of the data linearity, or more technically, whether your data is linearly separable or not, you need not be worried. On the contrary, there are some cons as well. For example:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>In some cases, the decision tree will not be suitable, sometimes they don't support online learning for real-time datasets. In that case, you have to rebuild your tree when new examples or datasets come; more technically, gaining model adaptability would not be possible.</p></li><li style="list-style-type: disc"><p>Secondly, if you are not aware, they will easily become over-fitting.</p></li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec35"></a>Random forests</h4></div></div></div><p>Random forests are quite popular and are a winner for the data scientist, since they are divine for a package with plenty of classification problems. They are usually slightly ahead of SVMs in terms of usability and have faster operation for most of the classification problems. In addition to this, they are also scalable when increasing the datasets you have available. In parallel, you don't need to be worried about tuning a cluster of parameters. On the contrary, you need to take care of many parameters and tuning when handling your data.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec36"></a>Decision forests, decision jungles, and variants</h4></div></div></div><p>Decision forests, decision jungles, and boosted decision trees are all based on decision trees, a foundational machine learning concept that is less used. There are many variants of decision trees are there; nonetheless, they all do the same thing, which is subdividing the feature space into regions with the same label. In order to avoid the over-fitting problem, a large set of trees are constructed with mathematical and statistical formulations, where the trees are not correlated at all.</p><p>The average of this is referred to as a decision forest; which is a tree that avoids the overfitting problem as stated earlier. However, the disadvantage is that decision forests can use a lot of memory. Decision jungles, on the other hand, are a variant that consume less memory by sacrificing a slightly longer training time. Fortunately, the boosted decision trees avoid overfitting by limiting the number of subdivision and the number of permitted data points in each region.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec37"></a>Bayesian methods</h4></div></div></div><p>When the experimental or sample dataset size is large, the Bayesian method often provides results for parametric models that are very similar to the results produced by other classical statistical methods. Some potential advantages of using the Bayesian method was summarized by Elam et al (W. T. Elam, B. Scruggs, F. Eggert, and J. A. Nicolosi, <span class="emphasis"><em>Advantages and Disadvantages of Methods for Obtaining XRF NET Intensities</em></span>, Copyright ©JCPDS-International Centre for Diffraction Data 2011 ISSN 1097-0002). For example, it provides a natural way of combining prior information with data. Therefore, as a data scientist, you can incorporate that past information regarding the parameters and form a prior distribution for future analysis for new datasets. It also provides inferences that are conditional on the data without the need of asymptotic approximation of the algorithm.</p><p>It provides some suitable settings for a wide range of models, such as hierarchical models and missing data problems. There are also disadvantages of using Bayesian analysis. For example, it does not tell you how to select a prior over world models or even that there is no correct way to choose a prior. Therefore, if you do not proceed with caution, you might generate many false positive or false negative results that often come with a high computational cost, if the number of parameters in a model is large.</p></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec23"></a>Summary</h2></div></div><hr /></div><p>This ends our rather quick tour of machine learning and the best practice that needs to be followed. Although we have tried to cover some of the most basic things to remember, suitable data often beats better algorithms and better demand. Most importantly, to design good features out of your data might take a long time; however, it would very much aid you. However, if you have a large-scale dataset to be applied to your machine learning algorithms or model, whichever classification, clustering, or regression algorithm you use might not be a matter of fact concerning the machine learning classes and their respective classification performance.</p><p>Therefore, it would be a wise decision to choose an appropriate machine learning algorithm that can fulfill requirements such as speed, memory usage, throughput, scalability, or usability. In addition to going over what we said in the sections above, if you are really concerned about achieving the accuracy, you should undoubtedly try a group of different classifiers to find the best one using the cross-validation technique or just use an ensemble method to choose them alltogether.</p><p>You can also be motivated and take a lesson from the Netflix Prize PLUS. We spoke at length about the Spark machine learning APIs, some best practice in ML application development, machine learning tasks and classes, some widely used best practices, and so on. However, we have not shown in depth analysis of the machine learning techniques. We intend to talk about this in more detail in <a class="link" href="#" linkend="ch04">Chapter 4</a>, <span class="emphasis"><em>Extracting Knowledge through Feature Engineering</em></span>.</p><p>In the next chapter, we will cover in detail the DataFrame, Dataset, and <span class="strong"><strong>Resilient Distributed Dataset</strong></span> (<span class="strong"><strong>RDD</strong></span>) APIs for working with structured data targeting to provide a basic understanding of machine learning problems with the available data. Therefore, at the end, you will be able to apply from basic to complex data manipulation with ease.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch03"></a>Chapter 3. Understanding the Problem by Understanding the Data</h2></div></div></div><p>This chapter will cover in details of the DataFrame, Datasets, and <span class="strong"><strong>Resilient Distributed Dataset</strong></span> (<span class="strong"><strong>RDD</strong></span>) APIs for working with structured data targeting to provide a basic understanding of machine learning problems with the available data. At the end of the chapter you will be able to apply basic to complex data manipulation with ease. Some comparisons will be made available with basic abstractions in Spark using RDD, DataFrame, and Dataset based data manipulation to show both gains in terms of programming and performance. In addition, we will guide you on the right track so that you will be able to use Spark to persist an RDD or data objects in memory, allowing it to be reused efficiently across the parallel operations in the later stage. In a nutshell, the following topics will be covered throughout this chapter:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Analyzing and preparing your data</p></li><li style="list-style-type: disc"><p>Resilient Distributed Dataset (RDD) basics</p></li><li style="list-style-type: disc"><p>Dataset basics</p></li><li style="list-style-type: disc"><p>Dataset from string and typed class</p></li><li style="list-style-type: disc"><p>Spark and data scientists, workflow</p></li><li style="list-style-type: disc"><p>Deeper into Spark</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec24"></a>Analyzing and preparing your data</h2></div></div><hr /></div><p>In practice, several factors affect the success of <span class="strong"><strong>machine learning</strong></span> (<span class="strong"><strong>ML</strong></span>) applications on a given task. Therefore, the representation and quality of the experimental dataset is first and foremost considered as the first class entities. It is always advisable to have better data. For example, irrelevant and redundant data, data features with null values or noisy data result in unreliable source of information. The bad properties in datasets make the knowledge discovery process during the machine learning model training phase more tedious and time inefficient.</p><p>As a result, the data preprocessing will contribute a considerable amount of computational time across the total ML workflow steps. As we stated in the previous chapter, unless you know your available data, it would be difficult to understand the problem itself. Moreover, knowing the data will help you to formulate your problem. In parallel, and more importantly, before trying to apply an ML algorithm to a problem, first you have to identify if the problem is really a machine learning problem and whether an ML algorithm could directly be applied to solve the problem. The next step that you need to take is to know the machine learning classes. More technically, you need to know if an identified problem falls under classification, clustering, rule retraction, or regression classes.</p><p>For the sake of simplicity, we assume you have a machine learning problem. Now you need to do some data pre-processing that includes some steps like data cleaning, normalization, transformation, feature extraction, and selection. The product of a data pre-processing workflow step is the final training set that is typically used to build/train the ML model.</p><p>In the previous chapter, we also argued that a machine learning algorithm learns from the data and activities during the model building and feed backing. It is critical that you feed your algorithm with the right data for the problem you want to solve. Even if you have good data (or well-structured data to be more precise), you need to make sure that the data is in an appropriate scale, with a well-known format to be parsed by the programming languages and, most importantly, if the most meaningful features are also included.</p><p>In this section, you will learn how to prepare your data so that your machine-learning algorithm becomes spontaneous towards best performance. The overall data processing is a huge topic; however, we will try to cover essential techniques to make some large scale machine learning applications in <span class="emphasis"><em><a class="link" href="#" linkend="ch06">Chapter 6</a></em></span>, <span class="emphasis"><em>Building Scalable Machine Learning Pipelines</em></span>.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec38"></a>Data preparation process</h3></div></div></div><p>If you are more focused and disciplined during the data handling and preparation steps, you are likely to get more consistent and better results in the first place. However, the data preparation is a tedious process consisting of several steps. Nevertheless, the process for getting data ready for a machine learning algorithm can be summarized in three steps:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Data selection</p></li><li style="list-style-type: disc"><p>Data pre-processing</p></li><li style="list-style-type: disc"><p>Data transformation</p></li></ul></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec38"></a>Data selection</h4></div></div></div><p>This step will focus on selecting the subset of all available datasets that you will be using and working with within your machine learning application development and deployment. There is always a strong urge to include all the available data in machine learning application development since more data will provide more features. In other words, by holding the well-known aphorism, <span class="emphasis"><em>more is better</em></span>. However, essentially, this might not be true in all cases. You need to consider what data you need to have before you actually answer the question. The ultimate goal is to provide a solution of a particular hypothesis. You might be doing some assumptions about the data as well in the first place. Although it is difficult, if you are a domain expert of that problem, you can make some assumption to know at least some insights before applying your ML algorithms. However, be careful to record those assumptions so that you can test them at a later stage when required. We will present some common question to help you out in thinking through the data selection process:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The first question would be, <span class="emphasis"><em>what is the extent of the data you have available?</em></span> For example, the extent could be the throughout time, database tables, connected system files, and so on. Therefore, the better practice is to ensure that you have a clear understanding and low-level structure of everything that you can use, or holding informally the available resources (while of course including the available data and computational resources).</p></li><li style="list-style-type: disc"><p>The second question is a little bit weird! <span class="emphasis"><em>What data are not yet available but important to solve the problem?</em></span> In this case, you might have to wait for the data to be available or alternatively you can at least generate or simulate these types of data using some generator or software.</p></li><li style="list-style-type: disc"><p>The third question might be: <span class="emphasis"><em>what data don't you need to address the problem?</em></span> That means again the redundancies so excluding these redundant or unwanted data is almost always easier than including it altogether. You might be wondering whether or not to note down the data you excluded and why? We think it should be yes since you might need some trivial data in the later stages.</p></li></ul></div><p>Moreover, in practice in this case small problems or games, toy competition data will already have been selected for you; therefore, you don't need to be worried at all!</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec39"></a>Data pre–processing</h4></div></div></div><p>After you have selected the data you will be working with, you need to consider how you could use the data and the proper utilization required. This pre-processing step will address some steps or techniques for getting the selected data into a form that you can work and apply during your model building and validation steps. The three most common data pre-processing steps that are used are formatting, cleaning, and sampling the data:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Formatting</strong></span>: The selected data may not be in a good shape so might not be suitable for you to work with directly. Very often, your data might be in a raw data format (a flat file format such as a text format or a less used proprietary format) and if you are lucky enough then data might be in a relational database. If this is the case, then it would better be to apply some conversion steps (that is, converting a relational database to its format for example, since using Spark you cannot make any conversion). As already stated, the beauty of Spark is its support for diverse file formats. Therefore, we will be able to take advantage in the following sections.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Cleaning</strong></span>: Very often the data you will be using comes with many unwanted records or sometimes with missing entries against a record. This cleaning process deals with the removal or fixing of missing data. There may be always some trivial data objects that are insignificant or incomplete and addressing them should be the first priority. Consequently, these instances may need to be removed, ignored or deleted from the datasets to get rid of this problem. Additionally, if the privacy or security is a concern because of the presence of the sensitive information against some attributes, those attributes need to be anonymized or removed from the data entirely (if appropriate).</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Sampling</strong></span>: The third step would be the sampling over the top of the formatted and cleaned datasets. Sampling is often required since there might be a time when the available data size is large or a number of records are huge. However, we argue to use the data as much as possible. Another reason is that more data can result in a longer execution time during the whole machine learning process. If this is the case, this also increases the running times of the algorithms and requires a more powerful computational infrastructure. Therefore, you can take a smaller representative sample of the selected data that may be much faster for exploring and prototyping the machine learning solution before considering the whole dataset. It is obvious that whatever the machine learning tools you apply for your machine learning application development and commercialization, data will influence the pre-processing you will be required to perform.</p></li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec40"></a>Data transformation</h4></div></div></div><p>After selecting appropriate data sources and pre-processing those data, the final step is to transform the processed data. Your specific ML algorithm and knowledge of the problem domain will be influenced in this step. Three common data transformations techniques are scaling attributes, decompositions and attribute aggregations. This step is also commonly referred to as feature engineering that will be discussed in more details in the next chapter:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Scaling</strong></span>: The pre-processed data may contain attributes with a mixture of scales for various quantities and units, for example dollars, kilograms, and sales volume. However, the machine-learning methods have the data attributes within the same scale such as between 0 and 1 for the smallest and largest value for a given feature. Therefore, consider any feature scaling you may need to perform the proper scaling of the processed data.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Decomposition</strong></span>: The data might have some features that represent a complex concept that provides a more powerful response from the machine learning algorithms when you split the datasets into the fundamental parts. For example, consider a day that is composed of 24 hours, 1,440 minutes, and 86,400 seconds that in turn could be split out further. Probably some specific hours or only the hours in a day are relevant to the problem which to be investigated and resolved. Therefore, consider an appropriate feature extraction and selection to perform the proper decomposition of the processed data.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Aggregation</strong></span>: Often segregated or scattered features may be trivial on their. However, those features can be aggregated into a single feature that would be more meaningful to the problem you are trying to solve. For example, several data instances can be presented in an online shopping website for each time a customer logged on the site. These data objects could be aggregated into a count for the number of logins by discarding additional instances. Therefore, consider appropriate feature aggregation to process the data properly.</p></li></ul></div><p>Apache Spark has its distributed data structures includes RDD, DataFrame, and Datasets by which you can perform the data pre-processing efficiently. These data structures have different advantages and performance for processing the data. In the next sections, we will describe those data structures individually and also show examples of how to process the large Dataset using them.</p></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec25"></a>Resilient Distributed Dataset basics</h2></div></div><hr /></div><p>In <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Introduction to Data Analytics with Spark</em></span>, we have described the Resilient Distributed Datasets in brief including the data transformation and action as well as the caching mechanism. We also stated that RDDs are basically an immutable collection of records that can only be created by operations such as map, filter, group by, and so on. In this chapter, we are going to use this native data structure of Spark for data manipulation and data pre-processing for a practical machine learning application commonly referred to as spam-filtering. Spark provides another two higher label APIs such as DataFrame and Datasets for data manipulation.</p><p>We will, however, show all the APIs including RDD here because you might need this API to handle more complex data manipulation. We have referred to some commonly used definitions regarding Spark actions and operations from the Spark programming guide.</p><p>As we already discussed some basics of RDD operations using action and transformations. The RDDs can be created by both stable storages such as the <span class="strong"><strong>Hadoop Distributed File System</strong></span> (<span class="strong"><strong>HDFS</strong></span>) and by transformations on existing RDDs. Spark periodically logs those transformations while creating RDDs over a set of transformations, rather than actual data, therefore, technically the original RDD and the Datasets do not get changed.</p><p>A transformed Dataset can be created from an existing one; however, the reverse is not possible in Spark. After finishing a computation on the Dataset, an action returns a value to the driver program. For example, according to the Spark programming guidelines, the map is a transformation that passes each Dataset element using a function and returns a brand new RDD that represents and holds the results. In contrast, reduce is also an action that aggregates all the elements of an RDD by using a function and returns a brand new RDD too as the final result to the driver program.</p><p>More technically, suppose we have a text file that contains a sequence of number separated by commas (that is, a CSV file). Now after reading the same you will have an RDD and consequently, you might want to count the frequencies of each number. For doing this, you need to convert the RDD into key value pairs, where the key is the number and the value will be the frequency of each number.</p><p>On the other hand, you might need to collect the result in the driver program by doing some operations. In the next few sections, we will provide more details on some useful topics such as transformations and actions by showing some examples based on a practical machine learning problem.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec39"></a>Reading the Datasets</h3></div></div></div><p>For reading Datasets from different data sources like a local filesystem, HDFS, Cassandra, HBase, and more, Spark provides different APIs that are easy to use. It supports the different representation of data including text file, sequence file, Hadoop input format, CSV, TSV, TXT, MD, JSON, and other data formats. The input API or methods support running on compressed files, directories and wildcards. For example, <span class="emphasis"><em>Table 1</em></span> shows the list of reading formats. The <code class="literal">textFile()</code> method reads different file formats such as <code class="literal">.txt</code> and <code class="literal">.gz</code> from the directory <code class="literal">/my/directory:</code></p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /></colgroup><tbody><tr><td style="">
<p>textFile("/my/directory"),</p><p>
</p><p>textFile("/my/directory/*.txt"),</p><p>
</p><p>textFile("/my/directory/*.gz").</p>
</td></tr></tbody></table></div><p>Table 1: Reading files formats</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec41"></a>Reading from files</h4></div></div></div><p>You might need to read a Dataset from local or HDFS. The following code show the different methods for creating RDDs from a given Dataset stored on your local machine or in HDFS.</p><p>However, before reading and writing with Spark, we need to create the Spark entry point by means of a Spark session that can be instantiated as follows:</p><pre class="programlisting">static SparkSession spark = SparkSession
      .builder()
      .appName("JavaLDAExample")
      .master("local[*]")
      .config("spark.sql.warehouse.dir", "E:/Exp/")
      .getOrCreate();
</pre><p>Here, the Spark SQL warehouse is set to as <code class="literal">E:/Exp/</code> path. You should set your path accordingly based on OS types you are on. Well, now we have our Spark session as variable <code class="literal">spark</code>, let's see how to use it with ease for reading from a text file.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch03lvl4sec0"></a>Reading from a text file</h5></div></div></div><p>It uses <code class="literal">textFile()</code> methods of <code class="literal">SparkContext()</code> and returns an RDD of a string containing a collection of lines. In <a class="link" href="#" linkend="ch01">Chapter 1</a>, 
<span class="emphasis"><em>Introduction to Data Analytics with Spark</em></span>, we explained what SparkContext is. Nevertheless, Spark Context is the entry point of a Spark application. Suppose we have a Dataset called <code class="literal">1.txt</code> containing some tweets data as unstructured texts. You can download the data from the Packt materials and store under the <code class="literal">project_path/input/test/</code> directory, defined as follows:</p><pre class="programlisting">String csvFile = "input/test/1.txt";
RDD&lt;String&gt; distFile = spark.sparkContext().textFile(csvFile, 2);
</pre><p>Here we have created RDDs of a string that is stored with the variable <code class="literal">distFile</code> in two partitions. However, to work with Java, the RDDs have to be converted into JavaRDD. Let's do it by calling the <code class="literal">toJavaRDD()</code> method as follows:</p><pre class="programlisting">JavaRDD&lt;String&gt; distFile2 = distFile.toJavaRDD();
</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch03lvl4sec1"></a>Reading multiple text files from a directory</h5></div></div></div><p>It will return RDD as (filename and content) pairs. Suppose we have multiple files stored to read in the directory <code class="literal">csvFiles/</code> is defined as follows:</p><pre class="programlisting">RDD&lt;Tuple2&lt;String, String&gt;&gt; distFile = spark.sparkContext().wholeTextFiles("csvFiles/*.txt", 2);
JavaRDD&lt;Tuple2&lt;String, String&gt;&gt; distFile2 = distFile.toJavaRDD();
</pre><p>Please note, when the data objects in an RDD do not hold in the main memory or HDD, we need to perform a partition on the RDD to increase the parallelism.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec42"></a>Reading from existing collections</h4></div></div></div><p>The second source for creating an RDD is from the collections of your driver program such as list a containing integers. Before going deeper into this, let's initialize Spark in an alternative way as follows:</p><pre class="programlisting">SparkConf conf = new SparkConf().setAppName("SampleAppliation").setMaster("local[*]");
JavaSparkContext sc = new JavaSparkContext(conf);
</pre><p>Here Java Spark context is available as a variable <code class="literal">sc</code>. This time, we have created the Spark context so we will be able to create the Java RDDs of string without using the <code class="literal">toJavaRDD()</code> method.</p><p>Now, you can do it by using the parallelized method of Spark Context as shown here:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Reading list of integers</strong></span>: It returns a parallelized RDD of integers:</p><pre class="programlisting">      List&lt;Integer&gt; list = Arrays.asList(1,2,3,4,5);
      JavaRDD&lt;Integer&gt; rdd = sc.parallelize(list);
</pre></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Reading list of pairs</strong></span>: It returns a parallelized <code class="literal">pairRDD</code> of the list of pairs (integer, string):</p><pre class="programlisting">      List&lt;Tuple2&lt;Integer, String&gt;&gt; pairs = Arrays.asList(
                new Tuple2&lt;&gt;(1, "Hello"),
                new Tuple2&lt;&gt;(2, "World"),
                new Tuple2&lt;&gt;(3, "How are you?"));
      JavaPairRDD&lt;Integer, String&gt; pairRDD = sc.parallelizePairs(pairs);
</pre></li></ul></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec40"></a>Pre–processing with RDD</h3></div></div></div><p>To continue the discussion about the data pre-processing that we started in the previous section, we will show an example of a machine learning problem and how to pre-process the Dataset using RDD in this section.</p><p>We are considering the <span class="strong"><strong>Spam filter</strong></span> application that is a popular example of s supervised learning problem. The problem is to predict and identify the spam messages from the incoming e-mails (please refer to <span class="emphasis"><em>Table 2</em></span>). As usual, to train the model, you have to train a model by using the historical data (the historical e-mail that you have received over a couple of days, hours or months an even year). The final output of pre-processing tasks is to make the feature vectors or extract the features including its labels or classes. Typically, you might be doing the following steps:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Stop word removal</strong></span>: The text file might contain some word that is useless or redundant for the feature vector such as <span class="emphasis"><em>and</em></span>, <span class="emphasis"><em>the</em></span>, and <span class="emphasis"><em>of</em></span>, since these are very common in all forms of English sentences. Another reason is they are not very meaningful in deciding spam or ham status or they may contain trivial significance. These words, therefore, need to be filtered from the e-mail Dataset before moving the next step.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Lemmatization</strong></span>: Some words possessing the same meaning but with different endings, need to be readjusted in order to make them consistent across the data set and if they all carry the same form will be easier to make them transform into feature vectors. For example, <span class="emphasis"><em>attached</em></span>, <span class="emphasis"><em>attachment</em></span>, and <span class="emphasis"><em>attach</em></span> could all be represented and later on interpreted as e-mail <span class="emphasis"><em>attachments</em></span>. The <code class="literal">SMSSpamCollection</code> Dataset was downloaded from the UCI ML repositories at <a class="ulink" href="https://archive.ics.uci.edu/ml/machine-learning-databases/00228/" target="_blank">https://archive.ics.uci.edu/ml/machine-learning-databases/00228/</a>.</p><p>Please note, all the words in the email body are usually converted to lowercase for simplicity in this phase. Now, let's take a look at the following table:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /></colgroup><tbody><tr><td style="">
<p>ham: What you doing? how are you?</p><p>
</p><p>ham: Ok lar... Joking wif u oni.</p><p>
</p><p>ham: dun say so early hor... U c already then say.</p><p>
</p><p>ham: MY NO. IN LUTON 0125698789 RING ME IF UR AROUND! H*</p><p>
</p><p>spam: FreeMsg: Txt: CALL to No: 86888 &amp; claim your reward of 3 hours talk time to use from your phone now! ubscribe6GBP/ mnth inc 3hrs 16 stop?txtStop</p><p>
</p><p>ham: Siva is in hostel aha.</p><p>
</p><p>ham: Cos i was out shopping wif darren jus now n i called him 2 ask wat present he wan lor. Then he started guessing who i was wif n he finally guessed darren lor.</p><p>
</p><p>spam: Sunshine Quiz! Win a super Sony DVD recorder if you can name the capital of Australia? Text MQUIZ to 82277. B</p>
</td></tr></tbody></table></div><p>Table 2: Test file for training set containing ham and spam messages</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Removal of non-words</strong></span>: Numbers and punctuation have to be removed too. However, we will not show here all the possible transformation for pre-processing data due to page limitation and brevity, but we will try to show some basic transformations and actions for pre-processing segment of the Dataset that contains some labels data as spam or ham presented in <span class="emphasis"><em>Table 2</em></span>. Where the Dataset or e-mails are labelled as ham or spam followed by the message or e-mails. Ham means non-spam and spams are identified as junk emails messages.</p><p>The overall pre-processing using RDD can be described using the following steps. The first step we need is to prepare the feature vectors using RDD operations and transformations. The remaining steps are given as follows:</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Reading Dataset</strong></span>: The following code is for reading Dataset that creates a <code class="literal">linesRDD</code> of strings from the <code class="literal">SMSSpamCollection</code> Dataset. Please download this Dataset from the Packt materials and store it in your disk or HDFS in  <code class="literal">Project+path/input/</code> directory. A detailed description of this Dataset will be provided later on:</p><pre class="programlisting">      String filePath = "input/SMSSpamCollection.txt";
      JavaRDD&lt;String&gt; linesRDD = sc.textFile(filePath);
</pre><p>However, the <code class="literal">linesRDD</code> contains both the spam as well as the ham messages. Therefore, we need to separate the spam and ham message from the files.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Filter out the spam messages</strong></span>: To filter the data from existing RDDs, Spark provides a method called <code class="literal">filter()</code>, which returns a new Dataset containing only the selected elements. In the following code you can see we have passed <code class="literal">new Function()</code> as a parameter that takes two arguments of the type String and Boolean of the<code class="literal">filter()</code> method. Basically, Spark APIs heavily rely on passing functions to the driver program for running on the cluster. There are two ways to create a function that includes:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Implementing the function interfaces either creating anonymous inner class or named one and passing an instance of it to Spark</p></li><li style="list-style-type: disc"><p>Using lambda expressions (you will have to have Java 8 installed to take advantage of lambda expressions though)</p></li><li style="list-style-type: disc"><p>The following code segments that we have used explain the concept of anonymous class as a parameter that contains a  <code class="literal">call()</code> method that returns <code class="literal">true</code> if the line contains the word <code class="literal">spam</code>:</p></li></ul></div><pre class="programlisting">      JavaRDD&lt;String&gt; spamRDD = linesRDD.filter(new Function&lt;String,
        Boolean&gt;() {
        @Override
        public Boolean call(String line) throws Exception {
          return line.split("\t")[0].equals("spam");}});
</pre></li><li style="list-style-type: disc"><p>Filter out the ham messages: similarly, we can filter out the ham messages as shown here:</p><pre class="programlisting">      JavaRDD&lt;String&gt; hamRDD = linesRDD.filter(new Function&lt;String,
       Boolean&gt;() {
       @Override
       public Boolean call(String line) throws Exception {
         return line.split("\t")[0].equals("ham");
        }
     });
</pre></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Split the words from the lines</strong></span>: To extract the features and labels from each line, we need to split those using space or tab characters. After that, we can have the lines without the spam or ham words.</p><p>The following code segments show the separation of spam and ham features from the lines. We have used the <code class="literal">map</code> transformation that returns a new RDD formed by passing each line of the existing RDD through a function <code class="literal">call</code>. Here the <code class="literal">call</code> method always returns a single item. You will find a difference with <code class="literal">flatMap</code> in the later section:</p><pre class="programlisting">      JavaRDD&lt;String&gt; spam = spamRDD.map(new Function&lt;String, String&gt;() {
        @Override
        public String call(String line) throws Exception {
         return line.split("\t")[1];
        }
     });
</pre></li></ul></div><p>Output: <code class="literal">ham.collect()</code>:</p><div class="mediaobject"><img src="graphics/B05243_03_01.jpg" /><div class="caption"><p>Figure 1: A snapshot of the spam RDD</p></div></div><pre class="programlisting">      JavaRDD&lt;String&gt; ham = hamRDD.map(new Function&lt;String, String&gt;() {
        @Override
        public String call(String line) throws Exception {
          return line.split("\t")[1];
      }
      });
</pre><p>Output: <code class="literal">ham.collect()</code>:</p><div class="mediaobject"><img src="graphics/B05243_03_02.jpg" /><div class="caption"><p>Figure 2: A snapshot of the ham RDD</p></div></div><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Split the words from the lines of spam RDD</strong></span>: After we get the feature lines against the spam and ham RDDs separately, we have to split the words for makinga  feature vector in the future. The following codes do this split with a space by returning the wordlist RDD. The call method returns a list of words for each line:
</p></li></ul></div><pre class="programlisting">      JavaRDD&lt;ArrayList&lt;String&gt;&gt; spamWordList = spam.map(new
        Function&lt;String, ArrayList&lt;String&gt;&gt;() {
          @Override
      public ArrayList&lt;String&gt; call(String line) throws Exception{
            ArrayList&lt;String&gt; words = new ArrayList&lt;&gt;();
            words.addAll(Arrays.asList(line.split(" ")));
            return words;
          }});
      JavaRDD&lt;ArrayList&lt;String&gt;&gt; hamWordList = ham.map(new Function&lt;String,
        ArrayList&lt;String&gt;&gt;() {
          @Override
      public ArrayList&lt;String&gt; call(String line) throws Exception{
            ArrayList&lt;String&gt; words = new ArrayList&lt;&gt;();
            words.addAll(Arrays.asList(line.split(" ")));
            return words;}
      });
</pre><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Make label and feature pair RDD</strong></span>
: Now we have two RDDs for spam and ham. We want to label them by 1.0 or 0.0 for spam and ham words or features respectively. For ease of use, we can again create a new RDD-containing tuple of a label and features or wordlist for each line. In the following code we have used <code class="literal">Tuple2</code> for making a pair. You can also use <code class="literal">JavaPairRDD</code> for making a pair of labels and features:</p><pre class="programlisting">      JavaRDD&lt;Tuple2&lt;Double, ArrayList&lt;String&gt;&gt;&gt; spamWordsLabelPair =
      spamWordList.map(new Function&lt;ArrayList&lt;String&gt;, Tuple2&lt;Double,
          ArrayList&lt;String&gt;&gt;&gt;() {
          @Override
            public Tuple2&lt;Double, ArrayList&lt;String&gt;&gt; call(
            ArrayList&lt;String&gt; v1) throws Exception {
            return new Tuple2&lt;Double, ArrayList&lt;String&gt;&gt;(1.0, v1);
          }});
      JavaRDD&lt;Tuple2&lt;Double, ArrayList&lt;String&gt;&gt;&gt; hamWordsLabelPair =
      hamWordList.map(new Function&lt;ArrayList&lt;String&gt;, Tuple2&lt;Double,
          ArrayList&lt;String&gt;&gt;&gt;() {
          @Override
          public Tuple2&lt;Double, ArrayList&lt;String&gt;&gt; call(
            ArrayList&lt;String&gt; v1) throws Exception {
            return new Tuple2&lt;Double, ArrayList&lt;String&gt;&gt;(0.0, v1);
          }});

      [Output: print spamWordsLabelPair2.collect() using for loop]
      1.0: [FreeMsg:, Txt:, CALL, to, No:, 86888, &amp;, claim, your, reward,
      of, 3, hours, talk, time, to, use, from, your, phone, now!,
      ubscribe6GBP/, mnth, inc, 3hrs, 16, stop?txtStop]
      1.0: [Sunshine, Quiz!, Win, a, super, Sony, DVD, recorder,
      if, you, canname, the, capital, of, Australia?, Text, MQUIZ,
      to, 82277., B]
</pre></li><li style="list-style-type: disc"><p><span class="strong"><strong>Union of the two RDDs</strong></span>: Now we have two labels for the Dataset in <span class="emphasis"><em>Table 2</em></span>, the feature pair RDD of spam and ham. Now to make the training Dataset, we can join these two RDDs into one. Spark has <code class="literal">union()</code>method for doing this that returns a new RDD, containing the union of the Dataset and the argument or another Dataset:</p><pre class="programlisting">      JavaRDD&lt;Tuple2&lt;Double, ArrayList&lt;String&gt;&gt;&gt; train_set =
      spamWordsLabelPair.union(hamWordsLabelPair);
</pre></li><li style="list-style-type: disc"><p>Counting all the lines from the preceding operations, are called a transformation. This returns a new Dataset from existing one in the worker nodes in each case. If you want to bring the return in the driver program or print the results that would be called an action operation. Spark supports several built-in methods as actions. The <code class="literal">count()</code> method counts the number of elements in the Dataset:</p><pre class="programlisting">      System.out.println(train_set.count());
      The following output is 8
</pre></li><li style="list-style-type: disc"><p><span class="strong"><strong>Printing the RDD</strong></span>: The <code class="literal">collect()</code> and <code class="literal">take()</code> are also action method that are used to print or collect the Dataset as an array in the driver program, where, <code class="literal">take()</code> takes an argument of, say <span class="emphasis"><em>n</em></span> that returns the first n elements of that Dataset. The following code segments print the first 10 elements or tuples out of the train set:</p><pre class="programlisting">      for (Tuple2&lt;Double, ArrayList&lt;String&gt;&gt; tt : train_set.collect()) {
          System.out.println(tt._1 + ": " + tt._2.toString()); }
</pre><p>The output is as follows:</p><pre class="programlisting">      1.0: [FreeMsg:, Txt:, CALL, to, No:, 86888,
       &amp;, claim, your, reward, of, 3, hours, talk, time, to, use, from,
       your, phone, now!, ubscribe6GBP/, mnth, inc, 3hrs, 16, stop?txtStop]
      1.0: [Sunshine, Quiz!, Win, a, super, Sony, DVD, recorder, if,
      you, canname, the, capital, of, Australia?, Text, MQUIZ,
      to, 82277., B]
      0.0: [What, you, doing?, how, are, you?]
      0.0: [Ok, lar..., Joking, wif, u, oni...]
      0.0: [dun, say, so, early, hor..., U, c, already, then, say...]
      0.0: [MY, NO., IN, LUTON, 0125698789, RING, ME, IF, UR, AROUND!, H*]
      0.0: [Siva, is, in, hostel, aha:-.]
      0.0: [Cos, i, was, out, shopping, wif, darren, jus, now,
       n, i, called,
      him, 2, ask, wat, present, he, wan, lor., Then, he,
      started, guessing,
      who, i, was, wif, n, he, finally, guessed, darren, lor.]
</pre></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Save the result in the local filesystem</strong></span>: Sometimes you might need to save the RDD in the filesystem as text. You can use the following code for saving your RDDs straight away:</p><pre class="programlisting">      train_set.saveAsTextFile("output.txt");
</pre></li></ul></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec43"></a>Getting insight from the SMSSpamCollection dataset</h4></div></div></div><p>The following source code shows the basic ham and spam statistics:</p><pre class="programlisting">String path = "input/SMSSpamCollection.txt";
RDD&lt;String&gt; lines = spark.sparkContext().textFile(path, 2);
System.out.println(lines.take(10));

JavaRDD&lt;Row&gt; rowRDD = lines.toJavaRDD().map( new Function&lt;String, Row&gt;() {
    public Row call(String line) throws Exception {
      return RowFactory.create(line);
      }});
System.out.println(rowRDD.collect());
List&lt;StructField&gt; fields = new ArrayList&lt;StructField&gt;();
fields.add(DataTypes.createStructField("line", DataTypes.StringType, true));
org.apache.spark.sql.types.StructType schema = DataTypes.createStructType(fields);
Dataset&lt;Row&gt; df = spark.sqlContext().createDataFrame(rowRDD, schema);
df.select("line").show();
Dataset&lt;Row&gt; spam = df.filter(df.col("line").like("%spam%"));
Dataset&lt;Row&gt; ham = df.filter(df.col("line").like("%ham%"));
System.out.println(spam.count());
System.out.println(ham.count());
spam.show();
</pre><p>The preceding code generates the following spam and ham counts:</p><pre class="programlisting">747
4831
</pre><p>This means that out of 5,578 emails <code class="literal">747</code> e-mails are spam and <code class="literal">4,831</code> e-mails are labelled as ham or non-spam. In other words, the spam and ham ratio is 13.40% and 86.6%.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec41"></a>Working with the key/value pair</h3></div></div></div><p>This subsection describes the key/value pair that is frequently needed in the data analytics, especially in the text processing.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec44"></a>mapToPair()</h4></div></div></div><p>This method will return a Dataset of (K, V) pair where K is key and V is value. For example, if you have an RDD with a list of integer then you want to count the number of duplicate entries in the list then the first task is to map each number to 1. After that you can do the reduce operation on it. The code produces the output and cache as shown here:</p><pre class="programlisting">JavaRDD&lt;Integer&gt; rdd = sc.parallelize(Arrays.asList(1,2,1,3,4,5));
JavaPairRDD&lt;Integer, Integer&gt; pairs = rdd.mapToPair(
  new PairFunction&lt;Integer, Integer, Integer&gt;() {
    @Override
    public Tuple2&lt;Integer, Integer&gt; call(Integer x) {
      return new Tuple2&lt;&gt;(x, 1);
    }
}).cache();

[Output: pairs.collect()]
[(1,1), (2,1), (1,1), (3,1), (4,1), (5,1)]
</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec42"></a>More about transformation</h3></div></div></div><p>In this section, you can see more about transformation including some differences between similar types of methods. Mainly <code class="literal">map</code> and <code class="literal">flatMap</code>, <code class="literal">groupByKey</code>, <code class="literal">reduceByKey</code> and <code class="literal">aggregateByKey</code>, <code class="literal">sortByKey</code> and <code class="literal">sortBy</code> will be discussed in this section. However, interested readers can refer to the Spark programming guidelines for RDD operation in [2].</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec45"></a>map and flatMap</h4></div></div></div><p>The <code class="literal">flatMap</code> is similar to the map we have showed in the preceding examples, but each input item or each time calling the <code class="literal">call()</code> method of the anonymous class can be mapped to zero or more output items. So ,the <code class="literal">call()</code> function returns a Sequence rather than a single item like a map. For example, for input of following RDD, the output should be as follows:</p><pre class="programlisting">JavaRDD&lt;String&gt; rdd = sc.parallelize(Arrays.asList("Hello World!",
  "How are you."));
JavaRDD&lt;String&gt; words = rdd
  .flatMap(new FlatMapFunction&lt;String, String&gt;() {
    @Override
    public Iterable&lt;String&gt; call(String t) throws Exception {
      return Arrays.asList(t.split(" "));
    }});

[output: words.collect()]
[Hello, World!, How, are, you.]
</pre><p>For the previous example, you could not do the map operation because the <code class="literal">call()</code> method of map return only one object rathers than a sequence of the objects.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec46"></a>groupByKey, reduceByKey, and aggregateByKey</h4></div></div></div><p>In order to perform some operation while pre-processing your Dataset you might need to do some aggregation such as sum and average based on key values. Spark provides some methods for doing these kinds of operations. Let's say, you have the following pairs of RDD and you want to group the values based on the keys and do some aggregations:</p><pre class="programlisting">JavaPairRDD&lt;Integer, Integer&gt; rdd_from_integer = sc
.parallelizePairs(Arrays.asList( new Tuple2&lt;&gt;(1, 1),
new Tuple2&lt;&gt;(1, 1), new Tuple2&lt;&gt;(3, 2),
new Tuple2&lt;&gt;(5, 1), new Tuple2&lt;&gt;(5, 3)), 2);
</pre><p>The aggregations you want to do can be done by the three methods of Spark including <code class="literal">groupByKey</code>, <code class="literal">reduceByKey</code>, and <code class="literal">aggregateByKey</code>. But they differ in term of performance, efficiency and flexibility to do an operation such as counting, computing summary statistics, finding unique elements from a data set and so on. The <code class="literal">groupByKey</code> method returns a Dataset of (k, <code class="literal">Iterable&lt;v&gt;</code>) pairs where k is the key and <code class="literal">Iterable&lt;v&gt;</code> is the sequence of values of the key k. The output of previous Dataset using this method is given as follows and it shows the collection values of each key:</p><pre class="programlisting">[Output: pairs.groupByKey(2).collect() ]
</pre><div class="mediaobject"><img src="graphics/image_03_003.jpg" /><div class="caption"><p>Figure 3: Pairs using groupBykey</p></div></div><p>In order to make the sum of values of each unique key, <code class="literal">groupByKey</code> is inefficient in terms of performance because it does not perform the map side in combination. You have to make more transformation to do this summation explicitly. So, it increases the network I/O and shuffle size. Better performance can be gained by <code class="literal">reduceByKey</code> or <code class="literal">aggregateByKey</code> because they perform the map side combination.</p><p>The methods return the Dataset with the result of each key aggregation such as summations of the values of each key. The following code show the operation of those methods which return the Dataset of (k,v) pairs where values (v) of the keys are aggregated by the given functions.</p><p>The <code class="literal">reduceByKey</code> takes one function that reduces the values of each key while the <code class="literal">aggregateByKey</code> takes two functions where the first function is for specifying how the aggregation will take place inside each partition and the second function is for specifying how the aggregation will take place between the partitions:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Code: <code class="literal">reduceByKey()</code>:</p><pre class="programlisting">      JavaPairRDD&lt;Integer, Integer&gt; counts = rdd
        .reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() {
          @Override
          public Integer call(Integer a, Integer b) {
            return a + b;}});
</pre></li><li style="list-style-type: disc"><p>Code: <code class="literal">aggregateByKey()</code>:</p><pre class="programlisting">      JavaPairRDD&lt;Integer, Integer&gt; counts = pairs.aggregateByKey(0,
          new Function2&lt;Integer, Integer, Integer&gt;() {
            @Override
            public Integer call(Integer v1, Integer v2) {
              return v1 + v2;
            }
            }, new Function2&lt;Integer, Integer, Integer&gt;() {
            @Override
            public Integer call(Integer v1, Integer v2) {
                  return v1 + v2;
                }
              });
</pre></li></ul></div><p>For both cases, the output will be as shown here:</p><p>Output: <code class="literal">counts.collect()</code>:</p><div class="mediaobject"><img src="graphics/image_03_004.jpg" /><div class="caption"><p>Figure 4: RDD using count</p></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec47"></a>sortByKey and sortBy</h4></div></div></div><p>Sorting is a common operation in data pre-processing. Spark provides two methods that transform one Dataset to another sorted paired Dataset that includes <code class="literal">sortByKey</code> and <code class="literal">sortBy</code>. For instance, we have a Dataset as shown here:</p><pre class="programlisting">List&lt;Tuple2&lt;Integer, Integer&gt;&gt; pairs = new ArrayList&lt;&gt;();
pairs.add(new Tuple2&lt;&gt;(1, 5));
pairs.add(new Tuple2&lt;&gt;(4, 2));
pairs.add(new Tuple2&lt;&gt;(-1, 1));
pairs.add(new Tuple2&lt;&gt;(1, 1));
</pre><p>The <code class="literal">sortByKey()</code> method performs on (k,v) pairs and returns (k,v) pairs sorted by keys in ascending or descending order. You can also customize the sorting by providing a comparator as parameters. The following code shows the sorting by key of the preceding Dataset:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Code: <code class="literal">sortByKey()</code>:</p></li></ul></div><pre class="programlisting">      JavaPairRDD&lt;Integer, Integer&gt; rdd = sc.parallelizePairs(pairs);
      JavaPairRDD&lt;Integer, Integer&gt; sortedRDD=rdd.sortByKey(Collections.
      &lt;Integer&gt; reverseOrder(), false);
      [Output: sortedRDD.collect()]
</pre><div class="mediaobject"><img src="graphics/image_03_005.jpg" /><div class="caption"><p>Figure 5: Pairs using sortByKey</p></div></div><p>The <code class="literal">sortBy()</code> method takes a function as a parameter where you can specify the sorting method either by key or by value. The following code shows the sorting by values of the preceding Dataset:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Code: <code class="literal">sortBy()</code>:</p></li></ul></div><pre class="programlisting">      JavaRDD&lt;Tuple2&lt;Integer, Integer&gt;&gt; rdd_new = sc.parallelize(pairs);
      JavaRDD&lt;Tuple2&lt;Integer, Integer&gt;&gt; sortedRDD=rdd.sortBy(
      new Function&lt;Tuple2&lt;Integer, Integer&gt;, Integer&gt;() {
      @Override
          public Integer call(Tuple2&lt;Integer, Integer&gt; t) {
              return t._2();
          }
      ,} true, 2);</pre><p>Output: <code class="literal">sortedRDD.collect()</code>:</p><div class="mediaobject"><img src="graphics/image_03_006.jpg" /><div class="caption"><p>Figure 6: Pairs using sortBy</p></div></div></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec26"></a>Dataset basics</h2></div></div><hr /></div><p>As discussed in <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Introduction to Data Analytics with Spark</em></span> that in Spark 2.0.0 release, the DataFrame remains the primary computation abstraction for the Scala, Python and R, however, while using Java the same will be replaced with Dataset. Consequently, Dataset of type Row will be used throughout this book.</p><p>The Dataset is a distributed collection of data is structured the Rows. This is this is one of the more convenient ways for interacting with Spark SQL module.</p><p>In other words, it can be considered as an equivalent entity to a tabular data like a <span class="strong"><strong>Relational Database</strong></span> (<span class="strong"><strong>RDB</strong></span>) format.. The Like the other data abstractions like DataFrame and RDD, the Dataset can also be created from various data sources like structured data files (TSV, CSV, JSON, and TXT), Hive tables, secondary storages, external databases, or existing RDDs and DataFrames. However, upon the Spark 2.0.0 release, the Java based computation does not support the DataFrame but you are developing your applications using Python, Scala or R, still you will be able making use of the DataFrames.</p><p>In the next few sections, you will find the operations and actions using Dataset and how to create a Dataset from different sources.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec43"></a>Reading datasets to create the Dataset</h3></div></div></div><p>As mentioned above, the Dataset is a component of Spark SQL module introduced from the Spark 1.5.0 release. Therefore, all the entry point of all functionally starts from the initialization of Spark <code class="literal">SQLContext</code> . Basically, Spark SQL is used for executing SQL queries written either as a basic SQL syntax or HiveQL.</p><p>A Dataset object will be returning when running SQL within another programming language. The following code segment will initialize the <code class="literal">SQLContext</code> within Spark Context. On the other hand, you might require having the <code class="literal">HiveContext</code> initialized for reading a data set from the Hive. You can also create a different context like <code class="literal">HiveContext</code> which provides a superset of basic functionalities of <code class="literal">SQLContext</code>:</p><pre class="programlisting">JavaSparkContext sc = new JavaSparkContext("local","DFDemo");
SQLContext sqlContext = new org.apache.spark.sql.SQLContext(sc);
</pre><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec48"></a>Reading from the files</h4></div></div></div><p>For example, you have a JSON file as shown here. Now you want to read this file using SQL context which basically returns a DataFrame which you can perform all the basic SQL operations and other DSL operations of Spark:</p><pre class="programlisting">[Input File]
{"name":"Michael"}
{"name":"Andy", "age":30}
{"name":"Justin", "age":19}
[Code]
Dataset&lt;Row&gt; df = sqlContext.read().json("people.json");

[Output: df.show()]
+----+-------+
| age|   name|
+----+-------+
|null|Michael|
|  30|   Andy|
|  19| Justin|
+----+-------+
</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec49"></a>Reading from the Hive</h4></div></div></div><p>The following code connects with Hive context where one table is created and people JSON file is loaded into hive create. The output of the DataFrame will be the same as above:</p><pre class="programlisting">The code is as follows:]hiveContext.sql("CREATE TEMPORARY TABLE people USING
org.apache.spark.sql.json OPTIONS ( path "people.json" )");
Dataset&lt;Row&gt; results = hiveContext.sql("SELECT * FROM people ");
results.show();
</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec44"></a>Pre-processing with Dataset</h3></div></div></div><p>In the previous section, we have described the pre-processing with RDD for a practical machine learning application. Now we will do the same example using <span class="strong"><strong>DataFrame</strong></span> (<span class="strong"><strong>DF</strong></span>) API. You will find it very easy to manipulate the <code class="literal">SMSSpamCollection</code> Dataset (see at <a class="ulink" href="http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/" target="_blank">http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/</a>). We will show the same example by tokenizing the spam and ham messages for preparing a training set:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Reading a Dataset:</strong></span>
 You can read that Dataset using the Spark session variable <code class="literal">spark</code>
that we have to initialize before using it. After reading the file as Dataset the output will be a tabular format of a single column. The default name of this column is <code class="literal">value</code>:</p><pre class="programlisting">      Dataset&lt;Row&gt; df = spark.read().load("input/SMSSpamCollection.txt");
      df.show();
</pre><p>Output:</p><div class="mediaobject"><img src="graphics/B05243_03_07.jpg" /><div class="caption"><p>Figure 7: A snapshot of the SMS spam dataset</p></div></div></li><li style="list-style-type: disc"><p><span class="strong"><strong>Create Row RDD from existing Dataset</strong></span>: From the preceding output you can see one column containing all the lines together. In order to make two columns such as label and features, we have to split it. Since Dataset is immutable you cannot modify the existing columns or Dataset. So you have to create new Dataset using the existing Dataset. Here the code converts the Dataset to RDD that is the collection of Row dataset. The row is an interface, which represents one row of output from a relational operator. You can create a new Row using <code class="literal">RowFactory</code> class of Spark:</p><pre class="programlisting">         JavaRDD&lt;Row&gt; rowRDD = df.toJavaRDD();
</pre></li><li style="list-style-type: disc"><p><span class="strong"><strong>Create new row RDD from an existing row RDD</strong></span>: After having the Row RDD you can perform normal map operation which is all contains Row Dataset but having two values. The following code split the each row and returns a new one:</p><pre class="programlisting">      JavaRDD&lt;Row&gt; splitedRDD = rowRDD.map(new Function&lt;Row, Row&gt;() {
           @Override
          public Row call(Row r) throws Exception {
            String[] split = r.getString(0).split("\t");
            return RowFactory.create(split[0],split[1]);
          }});
</pre></li><li style="list-style-type: disc"><p><span class="strong"><strong>Create Dataset from Row RDD</strong></span>: Now you have Row RDD, which contains two values for each Row. For creating a DF, you have to define the column names or schemas and its data types. There are two methods to define including inferring the schema using reflection and programmatically specify the schema. The methods are as follows:
</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The 1st method basically uses the POJO classes and fields names will be the schema</p></li><li style="list-style-type: disc"><p>The 2nd method create list of StruchFields by defining the datatypes and create the structype. For this example, we have used the 2nd method for creating DF from existing row RDD as shown here:</p><pre class="programlisting">      List&lt;StructField&gt; fields  = new ArrayList&lt;&gt;();
      fields.add(DataTypes.createStructField("labelString",
      DataTypes.StringType, true));
      fields.add(DataTypes.createStructField("featureString",
      DataTypes.StringType, true));
      org.apache.spark.sql.types.StructType schema = DataTypes
      .createStructType(fields);
      Dataset&lt;Row&gt; schemaSMSSpamCollection = sqlContext
      .createDataFrame(splitedRDD, schema);
      schemaSMSSpamCollection.printSchema();
      [Output: schemaSMSSpamCollection.printSchema()]
</pre><div class="mediaobject"><img src="graphics/B05243_03_08.jpg" /><div class="caption"><p>Figure 8: Schema of the collection</p></div></div></li></ul></div><p>
</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Adding a new column</strong></span>: Now that we have the DF of two columns. But we want to add new columns which convert the <code class="literal">labledSting</code> to <code class="literal">labedDouble</code> and <code class="literal">featureString</code> to <code class="literal">featureTokens</code>. You can do it similarly as previous code. After adding to new fields create new schema. Then create new DF after having normal map transformation in existing DF. The following code gives output of new DF having four columns:</p><pre class="programlisting">      fields.add(DataTypes.createStructField("labelDouble",
      DataTypes.DoubleType, true));
      fields.add(DataTypes.createStructField("featureTokens",
      DataTypes.StringType, true));
      org.apache.spark.sql.types.StructType schemaUpdated =
      DataTypes.createStructType(fields);
      Dataset Row&gt; newColumnsaddedDF = sqlContext
      .createDataFrame(schemaSMSSpamCollection.javaRDD().map(
      new Function&lt;Row, Row&gt;() {
          @Override
          public Row call(Row row) throws Exception {
            double label;
            if(row.getString(0).equalsIgnoreCase("spam"))
              label = 1.0;
            else
              label = 0.0;
            String[] split = row.getString(1).split(" ");
            ArrayList&lt;String&gt; tokens = new ArrayList&lt;&gt;();
            for(String s:split)
              tokens.add(s.trim());
            return RowFactory.create(row.getString(0),
       row.getString(1),label, tokens.toString());
          }}), schemaUpdated);
      [Output: newColumnsaddedDF.show()]
</pre><div class="mediaobject"><img src="graphics/B05243_03_09.jpg" /><div class="caption"><p>Figure 9: The dataset after adding a new column</p></div></div></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Some Dataset operations</strong></span>: For data manipulation DF provides domain specific language in Java, Scala and others. You can do select, counting, filter, <code class="literal">groupBy</code> and so on operations into a DF. The following codes show some operations on the above DF:</p><pre class="programlisting">      newColumnsaddedDF.select(newColumnsaddedDF.col("labelDouble"),
      newColumnsaddedDF.col("featureTokens")).show();
</pre><div class="mediaobject"><img src="graphics/B05243_03_10.jpg" /><div class="caption"><p>Figure 10: Dataset showing the label and features</p></div></div><pre class="programlisting">      newColumnsaddedDF.filter(newColumnsaddedDF.col
      ("labelDouble").gt(0.0)).show();
</pre><div class="mediaobject"><img src="graphics/B05243_03_11.jpg" /><div class="caption"><p>Figure 11: Dataset showing that the label has been converted into double value</p></div></div><pre class="programlisting">      newColumnsaddedDF.groupBy("labelDouble").count().show();
</pre><div class="mediaobject"><img src="graphics/image_03_012.jpg" /><div class="caption"><p>Figure 12: showing the Dataset statistics after manipulations</p></div></div></li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec45"></a>More about Dataset manipulation</h3></div></div></div><p>This section will describe how to use SQL queries on DF and different way to create Datasets across the datasets. Mainly running the SQL queries on DataFrame and the creating DataFrame from the JavaBean will be discussed in this section. However, interested readers can refer Spark programing guidelines for SQL operation in [3].</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec50"></a>Running SQL queries on Dataset</h4></div></div></div><p>The <code class="literal">SQLContext</code> of Spark has <code class="literal">sql</code> method enables applications to run SQL queries. This method returns a DataFrame as a result:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>[<code class="literal">FilternewColumnsAddedDF.createOrReplaceTempView</code>(<code class="literal">SMSSpamCollection</code>)]:</p><pre class="programlisting">      Dataset&lt;Row&gt; spam = spark.sqlContext().sql("SELECT * FROM
      SMSSpamCollection
      WHERE labelDouble=1.0");
      spam.show();
</pre><p>The following is the output of the preceding code:</p><div class="mediaobject"><img src="graphics/B05243_03_13.jpg" /><div class="caption"><p>Figure 13: using SQL query to retrieve same result as Figure 11</p></div></div></li><li style="list-style-type: disc"><p>Count:</p><pre class="programlisting">      Dataset&lt;Row&gt; counts = sqlContext.sql("SELECT labelDouble, COUNT(*)
      AS count FROM SMSSpamCollection GROUP BY labelDouble");
      counts.show();
</pre><p>Output:</p><div class="mediaobject"><img src="graphics/image_03_014.jpg" /><div class="caption"><p>Figure 14: Showing the Dataset statistics</p></div></div></li></ul></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec46"></a>Creating Dataset from the Java Bean</h3></div></div></div><p>You can create Dataset from a Java Bean; where you don't need to define the schemas programmatically. For example, you can see <span class="strong"><strong>Plain Old Java Object</strong></span> (<span class="strong"><strong>POJO</strong></span>) named as Bean in the following code:</p><pre class="programlisting">public class SMSSpamBean implements Serializable {
  private String labelString;
  private String featureString;
public SMSSpamBean(String labelString, String featureString) {
    super();
    this.labelString = labelString;
    this.featureString = featureString;
  }
  public String getLabelString() {
    return labelString;
  }
  public void setLabelString(String labelString) {
    this.labelString = labelString;
  }
  public String getFeatureString() {
    return featureString;
  }  public void setFeatureString(String featureString) {    this.featureString = featureString;
  }}
</pre><p>Create DF:</p><pre class="programlisting">JavaRDD&lt;SMSSpamBean&gt; smsSpamBeanRDD =  rowRDD.map(new Function&lt;Row, SMSSpamBean&gt;() {
      @Override
    public SMSSpamBean call(Row r) throws Exception {
        String[] split = r.getString(0).split("\t");
        return new SMSSpamBean(split[0],split[1]);
      }});
Dataset&lt;Row&gt; SMSSpamDF = spark.sqlContext().createDataFrame(smsSpamBeanRDD, SMSSpamBean.class);
SMSSpamDF.show();
</pre><p>The following output is as follows:</p><div class="mediaobject"><img src="graphics/B05243_03_15.jpg" /><div class="caption"><p>Figure 15: Corresponding feature and label string</p></div></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec27"></a>Dataset from string and typed class</h2></div></div><hr /></div><p>As already mentioned that the Dataset is a typed and immutable collection of objects. Datasets are basically mapped to a relational schema. With the Dataset abstraction, a new concept has been brought in Spark called an encoder. The encoder helps in entity conversion for example conversion between the JVM objects and the corresponding tabular representation. You will find this API quite similar to RDDs transformations such as <code class="literal">map, mapToPair, flatMap</code> or <code class="literal">filter.</code></p><p>We will show the spam filter example using Datasets API in the following section. It reads the text file using and returns a Dataset as a tabular format. Then perform map transformation like RDDs for making (label, tokens) columns with adding an additional encoder parameter. Here, we have used the bean encoder with <code class="literal">SMSSpamTokenizedBean</code> class.</p><p>In this sub-section, we will show how to create Dataset from string and typed class <code class="literal">SMSSpamTokenizedBean</code>. Let's create the Spark session at first place as follows:</p><pre class="programlisting">static SparkSession spark = SparkSession.builder()
      .appName("DatasetDemo")
      .master("local[*]")
      .config("spark.sql.warehouse.dir", "E:/Exp/")
      .getOrCreate();
</pre><p>Now create a new Dataset of type String from the <code class="literal">smm</code> filtering Dataset that means <code class="literal">Dataset&lt;String&gt;</code> and show the result as follows:</p><pre class="programlisting">Dataset&lt;String&gt; ds = spark.read().text("input/SMSSpamCollection.txt").as(org.apache.spark.sql.Encoders.STRING());
ds.show();
</pre><p>Here is the output of the preceding code:</p><div class="mediaobject"><img src="graphics/B05243_03_16.jpg" /><div class="caption"><p>Figure 16: Showing the snapshot of the spam filtering dataset using Dataset</p></div></div><p>Now let's create a second Dataset from the typed class <code class="literal">SMSSpamTokenizedBean</code> by mapping the Dataset of string we created immediate before as follows:</p><pre class="programlisting">Dataset&lt;SMSSpamTokenizedBean&gt; dsSMSSpam = ds.map(
new MapFunction&lt;String, SMSSpamTokenizedBean&gt;() {
          @Override
public SMSSpamTokenizedBean call(String value) throws Exception {
      String[] split = value.split("\t");
      double label;
      if(split[0].equalsIgnoreCase("spam"))
          label = 1.0;
      else
          label=0.0;
ArrayList&lt;String&gt; tokens = new ArrayList&lt;&gt;();
  for(String s:split)
    tokens.add(s.trim());
      return new SMSSpamTokenizedBean(label, tokens.toString());
         }
}, org.apache.spark.sql.Encoders.bean(SMSSpamTokenizedBean.class));
</pre><p>Now let's print the Dataset along with its schema as follows:</p><pre class="programlisting">dsSMSSpam.show();
dsSMSSpam.printSchema();
</pre><p>The following output is:</p><div class="mediaobject"><img src="graphics/B05243_03_17.jpg" /><div class="caption"><p>Figure 17: Showing the token and label and the lower side the schema</p></div></div><p>Now if you would like to convert this typed Dataset as type Row then you can use the <code class="literal">toDF()</code> method and to further create a temporary view out of the new <code class="literal">Dataset&lt;Row&gt;</code> you can use the <code class="literal">createOrReplaceTempView()</code> method with ease as follows:</p><pre class="programlisting">Dataset&lt;Row&gt; df = dsSMSSpam.toDF();
df.createOrReplaceTempView("SMSSpamCollection");
</pre><p>Similarly, might want to view the same Dataset by calling show <code class="literal">method()</code> as follows:</p><pre class="programlisting">df.show();
</pre><p>Output:</p><div class="mediaobject"><img src="graphics/B05243_03_18.jpg" /><div class="caption"><p>Figure 18: Corresponding labels and tokens. Labels are converted into double value</p></div></div><p>Now let's explore the typed class <code class="literal">SMSSpamTokenizedBean</code>. The class works as a Java tokenized bean class for the labeling the texts. More technically, the class takes the input then it sets the labels and after that gets the labels. Secondly, it also sets and gets the token for spam filtering. Including the setter and methods, here is the class:</p><pre class="programlisting">public class SMSSpamTokenizedBean implements Serializable {
private Double labelDouble;
private String tokens;
public SMSSpamTokenizedBean(Double labelDouble, String tokens) {
  super();
  this.labelDouble = labelDouble;
  this.tokens = tokens;
  }
  public Double getLabelDouble() {
    return labelDouble;
  }
  public void setLabelDouble(Double labelDouble) {
    this.labelDouble = labelDouble;
  }
  public String getTokens() {
    return tokens;
  }
  public void setTokens(String tokens) {
    this.tokens = tokens;
  }}
</pre><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec47"></a>Comparison between RDD, DataFrame and Dataset</h3></div></div></div><p>There are some objectives to bring Dataset as a new Data Structure of Spark. Although RDD API is very flexible, it is sometimes harder to optimize the processing. On the other hand, the DataFrame API is very easier to optimize but it lacks some of the nice features of RDD. So, the goal of the Datasets is to allow the users to easily express transformations on objects and also providing the advantages (performance and robustness) of the Spark SQL execution engine.</p><p>The Dataset can perform many operations such as sorting or shuffling without de-serializing of an object. For doing this it requires an explicit Encoder that is used to serialize the object into a binary format. It is capable of mapping the schema of a given object (Bean) to the Spark SQL type system. On the other hand, RDDs are based on run-time reflection based serialisation and the operations that change the types of object of a Dataset also need an encoder for the new type.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec28"></a>Spark and data scientists workflow</h2></div></div><hr /></div><p>As already stated that, a common task for a data scientist is to select the data, data pre-processing (formatting, cleaning and sampling) and data transformation (scaling, decomposition and aggregation) the raw data into a format that can be passed into machine learning models to build the models. As the size of the experimental datasets increases, the traditional single-node databases will not be feasible to handle these kinds of datasets, therefore, you need to switch a big data processing computing like Spark. Fortunately, we have the Spark to be an excellent option as a scalable distributed computing system to coup with your datasets.</p><div class="mediaobject"><img src="graphics/image_03_019.jpg" /><div class="caption"><p>Figure-19: Data scientist's workflow for using the Spark</p></div></div><p>Now let's move to the exact point, as a data scientist at first you will have to read the Dataset available in diverse formats. Then reading the Datasets will provide you with the concept of RDDs, DataFrames and Datasets that we already describe. You can cache the Dataset into the main memory; you can transform the read data sets from the DataFrame, SQL or as Datasets. And finally, you will perform an action to dump your data to the disks, computing nodes or clusters. The step what we describe here essentially forms a workflow that you will follow for the basic data processing using Spark that showed in <span class="emphasis"><em>Figure 1</em></span>.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec29"></a>Deeper into Spark</h2></div></div><hr /></div><p>In this section, we will show you the advanced features of Spark including the use of shared variables (both the broadcast variables and accumulators) and their underlying concept will be discussed. However, we will discuss the data partition in later chapters.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec48"></a>Shared variables</h3></div></div></div><p>The concept of shared variables in the context programming is not new. The variables that are required to use by many functions, and methods in parallel are called shared variables. Spark has some mechanism to use or implement the shared variables. In spark, the functions are passed to a spark operation like a map or reduce is executed on remote cluster nodes. The codes or functions work as a separate copy of variables on the nodes and no updates of the results are propagated back to the driver program. However, Spark provides two types of shared variables for two common usage patterns: broadcast variables and accumulators.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec51"></a>Broadcast variables</h4></div></div></div><p>Broadcast variables provide the facility to persist a read-only to be variable cached on local machine rather than sending a copy to the computing nodes or driver program. Providing the copy of large input Dataset to every node in an efficient manner of spark. It also reduces the communication cost because Spark uses an efficient broadcast. Broadcast variables can be created from a variable <code class="literal">v</code> by calling <code class="literal">SparkContext.broadcast(v)</code>. The following code shows this:</p><pre class="programlisting">Broadcast&lt;int[]&gt; broadcastVariable=sc.broadcast(new int[] {2,3,4});
int[] values = broadcastVariable.value();
for(int i:values){
  System.out.println(i);}
</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec52"></a>Accumulators</h4></div></div></div><p>Accumulators are another shared variable can be used to implement counters (as in MapReduce) or sums. Spark provides the supports for accumulators to be of numeric types only. However, you can also add support for new data types using existing techniques [1]. It is created from an initial value say <code class="literal">val</code> by calling:</p><pre class="programlisting">SparkContext. accumulator(val)
</pre><p>The following code shows the uses of accumulator for adding the elements of an array:</p><pre class="programlisting">Accumulator&lt;Integer&gt; accumulator = sc.accumulator(0);
sc.parallelize(Arrays.asList(1, 5, 3, 4))
.foreach(x -&gt; accumulator.add(x));
System.out.println(accumulator.value());
</pre><p>There are many types and method in the Spark APIs needed to be known. However, more and details discussion is out of the scope of this book.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip9"></a>Tip</h3><p>Interested readers should refer Spark and related materials on the following web pages:</p><p>Spark programming guide: <a class="ulink" href="http://spark.apache.org/docs/latest/programming-guide.html" target="_blank">http://spark.apache.org/docs/latest/programming-guide.html</a>.</p><p>Spark RDD operation: <a class="ulink" href="http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations" target="_blank">http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations</a>.</p><p>Spark SQL operation: <a class="ulink" href="http://spark.apache.org/docs/latest/sql-programming-guide.html" target="_blank">http://spark.apache.org/docs/latest/sql-programming-guide.html</a>.</p></div></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec30"></a>Summary</h2></div></div><hr /></div><p>In this Chapter, we applied the basic data manipulations with RDDs, Dataset and DataFrame APIs. We also learn how to do some complex data manipulation through these APIs. We tried to focus on data manipulations, to understand a practical machine learning problem Spam-filtering. In addition to these, we showed how to read the data from different sources. Analyzing and preparing your data to understand the spam filtering as an example.</p><p>However, we did not develop any complete machine learning application, since our target was just to show you the basic data manipulation on the experimental Datasets. We intended to develop complete ML application in <a class="link" href="#" linkend="ch06">Chapter 6</a>, <span class="emphasis"><em>Building Scalable Machine Learning Pipelines</em></span>.</p><p>Which features should be used to create a predictive model is not only a vital question but also a difficult question that may require deep knowledge of the problem domain to be answered. It is possible to automatically select those features in data that are most useful or most relevant for the problem someone is working on. Considering these questions, the next chapter covers the feature engineering in detail, explaining the reasons why to apply it along with some best practices in feature engineering. Some topics which are still unclear will be clearer in the next chapter.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch04"></a>Chapter 4. Extracting Knowledge through Feature Engineering</h2></div></div></div><p>Which features should be used to create a predictive model is not only a vital question, but also a difficult question that may require deep knowledge of the problem domain to be answered. It is possible to automatically select those features in data that are most useful or most relevant for the problem someone is working on. Considering these questions, this chapter covers Feature Engineering in detail, explaining the reasons why to apply it along with some best practices in feature engineering.</p><p>In addition to this, we will provide the theoretical descriptions and examples of feature extraction, transformations, and selection applied in large scale machine learning techniques, using both Spark MLlib and Spark ML APIs. Furthermore, this chapter also covers the basic idea of advanced feature engineering (also known as extreme feature engineering).</p><p>Please note that you will require having R and RStudio installed on your machine prior to proceeding with this chapter since an example towards exploratory data analysis will be shown using R.</p><p>In a nutshell, the following topics will be covered throughout this chapter:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The state of the art of feature engineering</p></li><li style="list-style-type: disc"><p>Best practices in feature engineering</p></li><li style="list-style-type: disc"><p>Feature engineering with Spark</p></li><li style="list-style-type: disc"><p>Advanced feature engineering</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec31"></a>The state of the art of feature engineering</h2></div></div><hr /></div><p>Even though feature engineering is an informal topic, however, it is considered as an essential part in applied machine learning. Andrew Ng, who is one of the leading scientists in the area of machine learning, defined the term feature engineering in his book <span class="emphasis"><em>Machine Learning and AI via Brain simulations</em></span> (see also, feature engineering defined at: <a class="ulink" href="https://en.wikipedia.org/wiki/Andrew_Nghttps://en.wikipedia.org/wiki/Andrew_Ng" target="_blank">https://en.wikipedia.org/wiki/Andrew_Nghttps://en.wikipedia.org/wiki/Andrew_Ng</a>) as follows:</p><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>Coming up with features is difficult, time-consuming, requires expert knowledge. Applied machine learning is basically feature engineering.</em></span></p></blockquote></div><p>Based on the preceding definition, we can argue that feature engineering is actually human intelligence, not artificial intelligence. Moreover, we will explain what feature engineering is from other perspectives. Feature engineering also can be defined as the process of converting raw data into useful features (also often called feature vectors). The features help you in better representation of the underlying problem to the predictive models eventually; so that the predictive modeling can be applied to new data types to avail high predictive accuracy.</p><p>Alternatively, we can define the term feature engineering as a software engineering process of using or reusing someone's advanced domain knowledge about the underlying problem and the available data to create features, which makes the machine learning algorithms work with ease.</p><p>This is how we define the term feature engineering. If you read it carefully, you will see four dependencies in these definitions:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The problem itself</p></li><li style="list-style-type: disc"><p>The raw data you will be working with to find out useful patterns or features</p></li><li style="list-style-type: disc"><p>The type of the machine learning problem or classes</p></li><li style="list-style-type: disc"><p>The predictive models you'll be using</p></li></ul></div><p>Now based on these four dependencies, we can conclude a workflow out of this. First, you have to understand your problem itself, then you have to know your data and if it is in good order, if not, process your data to find a certain pattern or features so that you can build your model.</p><p>Once you have identified the features, you need to know which categories your problem falls under. In other words, you have to be able to identify if it is a classification, clustering, or a regression problem based on the features. Finally, you will build the model to make a prediction on the test set or validation set using a well-known method such as random forest or <span class="strong"><strong>Support Vector Machine </strong></span>(<span class="strong"><strong>SVMs</strong></span>).</p><p>Throughout this chapter, you will see and argue that feature engineering is an art that deals with uncertain and often unstructured data. It's also true that there are many well-defined procedures of applying the classification, clustering, regression model, or methods such as SVMs that are both methodical and provable; however, the data is a variable and comes often with a variety of characteristics at different times.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec49"></a>Feature extraction versus feature selection</h3></div></div></div><p>You will get to know when and how you might be good at deciding which procedures to be followed by practice from the empirical apprenticeship. The main tasks involved in feature engineering are:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Data exploration and feature extraction</strong></span>: This is the process of uncovering the hidden treasure in the raw data. Generally, this process does not vary much by algorithms consuming the features. However, a better understanding of the hands-on experience, business domain, and intuition play a vital role in this regard.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Feature selection</strong></span>: This is the process for deciding which features to be selected based on the machine learning problem you are dealing with. You can use diverse techniques for selecting the features; however, it may vary in algorithms and using the features.</p></li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec50"></a>Importance of feature engineering</h3></div></div></div><p>When the ultimate goal is to achieve the most accurate and reliable results from a predictive model, you have to invest your best in what you have. The best investment, in this case, would be the three parameters: time and patience, data and availability, and best algorithm. However, <span class="emphasis"><em>how do you get the most valuable treasures out of your data for the predictive modeling?</em></span> is the problem that the process and practice of feature engineering solves in an emerging way.</p><p>In fact, the success of most of the machine learning algorithms depends on how you properly and intelligently utilize value and present your data. It is often agreed that the hidden treasure (that is, features or patterns) out of your data will directly stimulate the results of the predictive model.</p><p>Therefore, better features (that is, what you extract and select from the datasets) mean better results (that is, the results you will achieve from the model). However, please remember one thing before you generalize the earlier statement for your machine learning model, you need a great feature which is true nonetheless with the properties that describe the structures inherent in your data.</p><p>In summary, better features signify three pros: flexibility, tuning, and better results:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Better features (better flexibility)</strong></span>
: If you are successful in extracting and selecting the better features, you will get better results for sure, even if you choose a non-optimal or wrong model. In fact, optimal or most suitable models can be selected or picked up based on the good structure of the original data you have. In addition to this, good features will allow you to use less complex but efficient, faster, easily understandable, and easy to maintain models eventually.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Better features (better tuning)</strong></span>
: As we already stated, if you do not choose your machine learning model intelligently or if your features are not in good shape, you are more likely to get worse results out of the ML model. However, even if you choose some wrong parameters during building the model and if you do have some well-engineered features, still you can expect better results out of the model. Furthermore, you don't need to worry much or even work harder to choose the most optimal models and related parameters. The reason is simple, which is the good feature, you have actually understood the problem well and ready to use the better represented by all the data by characterizing the problem itself.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Better features (better results)</strong></span>: You are most likely to get better results even if you spent most of your efforts in feature engineering towards better features selections.</p></li></ul></div><p>We also suggest readers not to be overconfident with only the features. The preceding statements are often true; however, sometimes they are misleading. We would like to clear the preceding statements further. Actually, if you receive the best predictive results from a model, it is actually of three factors: the model you selected, the data you had, and the features you had prepared.</p><p>Therefore, if you have enough time and computational resources, always try to use the standard model since often the simplicity does not imply better accuracy. Nonetheless, better features will contribute the most out of these three factors. One thing you should know is that, unfortunately, even if you master feature engineering emanates with many hands-on practices, and research what others are doing well in the state of the arts, some machine learning projects fail at the very end.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec51"></a>Feature engineering and data exploration</h3></div></div></div><p>Very often, an intelligent choice for both training and test samples out of better features leads to better solutions. Although in the previous section we argued that there are two tasks in the feature engineering: feature extraction from the raw data and feature selection. However, there is no definite or fixed path for feature engineering.</p><p>Conversely, the whole step in feature engineering is very much directed by the available raw data. If the data is well-structured you would be feeling lucky. Nonetheless, the reality is often that the raw data comes from diverse sources in multiple formats. Therefore, exploring this data is very important before you proceed to feature extraction and feature selection.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip10"></a>Tip</h3><p>We suggest you to figure out the data skewness and kurtosis using the histogram and outliers using the box-plot and bootstrapping the data using Data Sidekick techniques (introduced by Abe Gong) in the literature (see at: <a class="ulink" href="https://curiosity.com/paths/abe-gong-building-for-resilience-solid-2014-keynote-oreilly/#abe-gong-building-for-resilience-solid-2014-keynote-oreilly" target="_blank">https://curiosity.com/paths/abe-gong-building-for-resilience-solid-2014-keynote-oreilly/#abe-gong-building-for-resilience-solid-2014-keynote-oreilly</a>).</p></div><p>The following questions need to be answered and known by means of data exploration before applying the feature engineering:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>What is the percentage of the total data being present or not having null or missing values for all the available fields? Then try to handle those missing values and interpret them well without losing the data semantics.</p></li><li style="list-style-type: disc"><p>What is the correlation between the fields? What is the correlation of each field with the predicted variable? What values do they take (that is, categorical or non-categorical, numerical or alpha-numerical, and so on)?</p></li><li style="list-style-type: disc"><p>Then find out if the data distribution is skewed or not. You can identify the skewness by seeing the outliers or long tail (slightly skewed to the right or positively skewed, slightly skewed to the left or negatively skewed, as shown in <span class="emphasis"><em>Figure 1</em></span>). Now identify if the outliers contribute towards making the prediction or not.</p></li><li style="list-style-type: disc"><p>After that, observe the data kurtosis. More technically, check if your kurtosis is mesokurtic (less than but almost equal to 3), leptokurtic (more than 3), or platykurtic (less than 3). Note, the kurtosis of any univariate normal distribution is considered to be 3.</p></li><li style="list-style-type: disc"><p>Now play with the tail and observe (do the predictions get better?) what happens when you remove the long tail?
</p><div class="mediaobject"><img src="graphics/image_04_001.jpg" /><div class="caption"><p>Figure 1: Skewness of the data distribution (x-axis = data, y-axis = density).</p></div></div></li></ul></div><p>You can use simple visualization tools such as density plots for doing this, as explained by the following example.</p><p>Example 1. Suppose you are interested in fitness walking and you walked at a sports ground or countryside in the last four weeks (excluding the weekends). You spent the following time (in minutes to finish a 4 KM walking track):15, 16, 18, 17.16, 16.5, 18.6, 19.0, 20.4, 20.6, 25.15, 27.27, 25.24, 21.05, 21.65, 20.92, 22.61, 23.71, 35, 39, and 50. Now let's compute and interpret the skewness and kurtosis of these values using R.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip11"></a>Tip</h3><p>We will show how to configure and work with SparkR in <a class="link" href="#" linkend="ch10">Chapter 10</a>, <span class="emphasis"><em>Configuring and Working with External Libraries</em></span> and show how to execute the same code on SparkR. The reason behind this is some plotting packages such as <code class="literal">ggplot2</code> are still not implemented in the current version of Spark used for SparkR directly. However, the <code class="literal">ggplot2</code> is available as a combined package named <code class="literal">ggplot2.SparkR</code> on GitHub, which can be installed and configured using the following command:</p><p>
</p><p><span class="strong"><strong><code class="literal">devtools::install_github("SKKU-SKT/ggplot2.SparkR")</code>
</strong></span></p><p>
</p><p>However, there are numerous dependencies that need to be ensured before and during the configuration process. Therefore, we should resolve this issue in a later chapter instead. For the time being, we assume you have basic knowledge of using R and if you have R installed and configured on your computer then please use the following steps. However, a step-by-step example on how to install and configure SparkR using RStudio will be shown in <a class="link" href="#" linkend="ch10">Chapter 10</a>, <span class="emphasis"><em>Configuring and Working with External Libraries</em></span>.</p></div><p>Now just copy the following code snippets and try to execute to make sure you have the correct value of the Skewness and Kurtosis.</p><p>Install the <code class="literal">moments</code> package for calculating Skewness and Kurtosis:</p><pre class="programlisting">install.packages("moments")
</pre><p>Use the <code class="literal">moments</code> package:</p><pre class="programlisting">library(moments)
</pre><p>Make a vector for the time you have taken during the workout:</p><pre class="programlisting">time_taken &lt;- c (15, 16, 18, 17.16, 16.5, 18.6, 19.0, 20.4, 20.6, 25.15, 27.27, 25.24, 21.05, 21.65, 20.92, 22.61, 23.71, 35, 39, 50)
</pre><p>Convert the time into DataFrame:</p><pre class="programlisting">df&lt;- data.frame(time_taken)
</pre><p>Now calculate the <code class="literal">skewness</code>:</p><pre class="programlisting">skewness(df)
[1]1.769592
</pre><p>Now calculate the <code class="literal">kurtosis</code>:</p><pre class="programlisting">&gt; kurtosis(df)
[1]5.650427
</pre><p><span class="strong"><strong>Interpretation of the result</strong></span>: The skewness of your workout time is 1.769592, which means your data is skewed to the right or positively skewed. The kurtosis, on the other hand, is 5.650427, which means the distribution of the data is leptokurtic. Now to check the outliers or tails check the following histogram. Again, for simplicity, we will use R to plot the density plot that will interpret your workout time.</p><p>Install <code class="literal">ggplot2package</code> for plotting the histogram:</p><pre class="programlisting">
<span class="strong"><strong>install.packages("ggplot2") </strong></span>
</pre><p>Use the <code class="literal">moments</code> package:</p><pre class="programlisting">
<span class="strong"><strong>library(ggplot2)</strong></span>
</pre><p>Now plot the histogram using the <code class="literal">qplot()</code> method of <code class="literal">ggplot2</code>:</p><pre class="programlisting">ggplot(df, aes(x = time_taken)) + stat_density(geom="line", col=
"green", size = 1, bw = 4) + theme_bw()
</pre><div class="mediaobject"><img src="graphics/image_04_002.jpg" /><div class="caption"><p>Figure 2. Histogram of the workout time (right-skewed).</p></div></div><p>The interpretation presented in <span class="emphasis"><em>Figure 2</em></span> of the distribution of data (workout times) shows the density plot is skewed to the right so is leptokurtic. Besides the density plot, you can also look at the box-plots for each individual feature. Where the box plot displays the data distribution based on five-number summaries: <span class="strong"><strong>minimum</strong></span>, <span class="strong"><strong>first quartile</strong></span>, median, <span class="strong"><strong>third quartile</strong></span>, and <span class="strong"><strong>maximum</strong></span>, as shown in <span class="emphasis"><em>Figure 3</em></span>, where we can look for outliers beyond three (3) <span class="strong"><strong>Inter-Quartile Range</strong></span> (<span class="strong"><strong>IQR</strong></span>):</p><div class="mediaobject"><img src="graphics/image_04_003.jpg" /><div class="caption"><p>Figure 3. Histogram of the workout time (figure courtesy of Box Plot: Display of Distribution, <a class="ulink" href="http://www.physics.csbsju.edu/stats/box2.htmlhttp://www.physics.csbsju.edu/stats/box2.html" target="_blank">http://www.physics.csbsju.edu/stats/box2.htmlhttp://www.physics.csbsju.edu/stats/box2.html</a>).</p></div></div><p>Bootstrapping the datasets also sometimes offers insights on outliers. If the data volume is too large (that is, big data) doing the Data Sidekick, evaluations and predictions are also useful. The idea of Data Sidekick is to use a small part of the available data to figure out what insights can be concluded from the datasets and it is also commonly referred to as <span class="emphasis"><em>using small data to multiply the value of big data</em></span>.</p><p>It is very useful for large-scale text analytics. For example, suppose you have a huge corpus of text, and of course you can use a small portion of it to test various sentiment analysis models and choose the one which gives the best results in terms of performance (computation time, memory usage, scalability, and throughput).</p><p>Now we would like to draw your attention to the other aspects of feature engineering. Moreover, converting continuous variables into categorical variables (with a certain combination of features) results in better predictor variables.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip12"></a>Tip</h3><p>In statistical language, a variable in your data either represents measurements on some continuous scale, or on some categorical or discrete characteristics. For example, weight, height, and age of an athlete would represent the continuous variables. Alternatively, the survival or failure in terms of time is also considered as continuous variables. A person's gender, occupation, or marital status, on the other hand, is categorical or discrete variables. Statistically, some variables could be considered in either way. For example, a movie viewer's rating of a move on a 10 point scale may be considered a continuous variable, or we may consider it as a discrete variable with 10 categories. Time series data or real-time streaming data are usually collected for continuous variables until a certain time.</p></div><p>In parallel, considering the square or cube or even using the non-linear models of the features can also provide better insights. Also, consider the forward selection or backwards selection wisely since both of them are computationally expensive.</p><p>Finally, when the number of features becomes significantly large it is a wise decision to use the <span class="strong"><strong>Principal Component Analysis</strong></span> (<span class="strong"><strong>PCA</strong></span>) or <span class="strong"><strong>Singular Value Decomposition</strong></span> (<span class="strong"><strong>SVD</strong></span>) technique to find the right combination of features.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec52"></a>Feature extraction – creating features out of data</h3></div></div></div><p>Feature extraction is the automatic way of constructing new features from the raw data you have or will be collecting. During the feature extraction process, reducing the dimensionality of complex raw data is usually done by making the observation into a much smaller set automatically that can be modeled into later stages. Projection methods such as PCA and unsupervised clustering methods are used for tabular data in TXT, CSV, TSV, or RDB format. However, feature extraction from another data format is very complex. Specially parsing many data formats such as XML and SDRF is a tedious process if the number of fields to extract is huge.</p><p>For multimedia data such as image data, the most common type of technique includes line or edge detection or image segmentation. However, subject to the domain and image, video and audio observations advance themselves to many of the same types of <span class="strong"><strong>Digital Signal Processing</strong></span> (<span class="strong"><strong>DSP</strong></span>) methods where typically the analogue observations are stored in digital formats.</p><p>The most positive pros and the key to feature extraction are that the methods that are developed and available are automatic; therefore, thay can solve the problem of unmanageable high dimensional data. As we stated in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Understanding the Problem by Understanding the Data</em></span> that more data exploration and better feature extraction eventually increases the performance of your ML model (since feature extraction also involves feature selection). The reality is more data will provide more insights towards the performance of the predictive models eventually. However, the data has to be useful and dumping unwanted data will kill your valuable time; therefore, think of the meaning of the statement before collecting your data.</p><p>There are several steps involved in the feature extraction process; including the data transformation and feature transformation. As we stated several times, a machine learning model is likely to provide a better result if the model is well trained with better features out of the raw data. Optimized for learning and generalization is a key characteristic of good data. Therefore, the process of putting together the data in this optimal format is achieved through some data processing steps such as cleaning, missing values handling, and some intermediate transformation like from a text document to words transformation.</p><p>The methods that help to create new features as predictor variables are called feature transformation, which is actually a group of methods. Feature transformation is essentially required for the dimension reduction. Usually, when the transformed features have a descriptive dimension, it is likely to have better order compared to the original features.</p><p>Therefore, less descriptive features can be dropped from the training or test samples when building the machine learning models. The most common tasks included in the feature transformation are non-negative matrix factorization, principal component analysis, and factor analysis using scaling, decomposition, and aggregation operations.</p><p>Examples of feature extraction include the extraction of contours in images, extraction of diagrams from a text, extraction of phonemes from the recording of spoken text, and so on. Feature extraction involves a transformation of the features, which is often not reversible because some information is lost eventually in the process of dimensionality reduction.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec53"></a>Feature selection – filtering features from data</h3></div></div></div><p>Feature selection is a process for preparing the training datasets or validation dataset for predictive modeling and analytics. Feature selection has practical implication in most of the machine learning problem types including classification, clustering, dimensionality reduction, collaborative filtering, regression, and so on.</p><p>Therefore, the ultimate goal is to select a subset from the large collection of features from the original data set. And often dimensionality reduction algorithms are applied, such as <span class="strong"><strong>Singular Value Decomposition</strong></span> (<span class="strong"><strong>SVD</strong></span>) and <span class="strong"><strong>Principal Component Analysis</strong></span> (<span class="strong"><strong>PCA</strong></span>).</p><p>An interesting power of the feature selection technique is that a minimal feature set can be applied to represent the maximum amount of variance in the available data. In other words, the minimal subset of the feature is enough to train your machine learning model quite efficiently.</p><p>This subset of features is used to train the model. There are two types of feature selection techniques, namely forward selection and backwards selection. The forward selection starts with the strongest feature and keeps adding more features. On the contrary, the backwards selection starts with all the features and removes the weakest features. However, both techniques are computationally expensive.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl3sec53"></a>Importance of feature selection</h4></div></div></div><p>Since not all the features are equally important; consequently, you will find some features with more importance than others for making the model more accurate. Therefore, those attributes can be treated as irrelevant to the problem. As a result, you need to remove those features before preparing the training and test sets. Sometimes, the same technique might be applied to the validation sets.</p><p>In parallel to importance, you will always find some features that will be redundant in the context of other features. Feature selection is not only involved with removing irrelevant or redundant features, it also serves other purposes that are important to increase the model's accuracy, as stated here:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Feature selection increases the predictive accuracy of the model you are using by eliminating irrelevant, null/missing, and redundant features. It also deals with highly correlated features.</p></li><li style="list-style-type: disc"><p>Feature selection techniques make the model training process more robust and faster by decreasing the number of features.</p></li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl3sec54"></a>Feature selection versus dimensionality reduction</h4></div></div></div><p>Although by using the feature selection technique it is quietly possible to reduce the number of features by selecting certain features in the dataset. And later on, the subset is used to train the model. However, the entire process usually, cannot be used interchangeably with the term <span class="strong"><strong>dimensionality reduction</strong></span>.</p><p>The reality is that the feature selection methods are used to extract a subset from the total set in the data without changing their underlying properties.</p><p>In contrast, the dimensionality reduction method, on the other hand, employs already engineered features that can transform the original features into corresponding feature vectors by reducing the number of variables under certain considerations and requirements of the machine learning problem.</p><p>Thus, it actually modifies the underlying data, extracts the original features from raw and noisy features by compressing the data, but maintains the original structure and most of the time is irreversible. Typical examples of dimensionality reduction methods include <span class="strong"><strong>Principal Component Analysis</strong></span> (<span class="strong"><strong>PCA</strong></span>), <span class="strong"><strong>Canonical Correlation Analysis</strong></span> (<span class="strong"><strong>CCA</strong></span>), and <span class="strong"><strong>Singular Value Decomposition</strong></span> (<span class="strong"><strong>SVD</strong></span>).</p><p>Other feature selection techniques use the filter-based, wrapper methods and embedded methods feature selection by evaluating the correlation between each feature and the target attribute in a supervised context. These methods apply some statistical measures to assign a score to each feature also known as filter methods.</p><p>The features are then ranked based on the scoring system that can help to eliminate the specific features. Examples of such techniques are information gain, correlation coefficient scores, and Chi-squared test. An example of wrapper methods, which is a feature selection process as a search problem, is the recursive feature elimination algorithm. On the other hand, <span class="strong"><strong>Least Absolute Shrinkage and Selection Operator</strong></span> (<span class="strong"><strong>LASSO</strong></span>), Elastic Net, and Ridge Regression are typical examples of embedded methods of feature selection, which is also known as regularizations methods.</p><p>The current implementation of Spark MLlib provides the support for dimensionality reduction on the <code class="literal">RowMatrix</code> class only for the SVD and PCA. On the other hand, some typical steps from raw data collection to feature selection are feature extractions, feature transformation, and feature selection.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip13"></a>Tip</h3><p>Interested readers are suggested to read the API documentation for the feature selection and dimensionality reduction at: <a class="ulink" href="http://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html" target="_blank">http://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html</a>.</p></div></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec32"></a>Best practices in feature engineering</h2></div></div><hr /></div><p>In this section, we have figured out some good practices while performing the feature engineering on your available data. Some best practices of machine learning were described in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Machine Learning Best Practices</em></span>. However, those were too general for the overall machine learning state of the arts. Those best practices, of course, would be useful in the feature engineering, too. Moreover, we will provide more concrete examples concerning feature engineering in the following sub-sections.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec54"></a>Understanding the data</h3></div></div></div><p>Although the term feature engineering is more technical, however, it is an art that helps you to understand where the features come from. Now some vital questions evolve too, which need to be answered before understanding the data:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>What are the provenances of those features? Is the data real-time or coming from the static sources?</p></li><li style="list-style-type: disc"><p>Are the features continuous, discrete, or none?</p></li><li style="list-style-type: disc"><p>What is the distribution of the features? Does the distribution largely depend on what subset of examples is being considered?</p></li><li style="list-style-type: disc"><p>Do these features contain missing values (that is, NULL)? If so, is it possible to handle those values? Is it possible to eliminate them in the present, future, or upcoming data?</p></li><li style="list-style-type: disc"><p>Is there duplicate or redundant entries?</p></li><li style="list-style-type: disc"><p>Should we go for manual feature creation that proves to be useful? If so, how hard would it be to incorporate those features in the model training stage?</p></li><li style="list-style-type: disc"><p>Are there features that can be used as standard features?</p></li></ul></div><p>Knowing the answers to the preceding questions is important. Since data provenance would help you to prepare your feature engineering techniques a bit faster. You need to know if your features are discrete or continuous or if the requests are a real-time response or not. Moreover, you need to know the data distribution along with their skewness and kurtosis to handle the outliers.</p><p>You need to be prepared for the missing or null values whether they could be removed or need to be filled with alternative values. Besides, you need to remove duplicates entries in the first place, which is extremely important, since duplicate data points might significantly affect the results of model validation if not properly excluded. Finally, you need to know your machine learning problem itself since knowing the problem type would help you to label your data accordingly.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec55"></a>Innovative way of feature extraction</h3></div></div></div><p>Be innovative while extracting and selecting the features. Here we provide eight tips altogether that will help you to generalize the same during your machine learning application development.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip14"></a>Tip</h3><p>Create the input by rolling up existing data fields to a broader level or category.</p></div><p>To be more specific, let's give you some examples. Obviously, you can categorize your colleagues based on their title into strategic or tactical. For instance, you can code the employee with <span class="emphasis"><em>Vice President or VP</em></span> or above as strategic and the <span class="emphasis"><em>Director</em></span> and below could be encoded as tactical.</p><p>Collating several industries into a higher-level industry could be another example of such categorization. Collate oil and gas companies with commodity companies; gold, silver, or platinum as precious metal companies; high-tech giants and telecommunications industries as <span class="emphasis"><em>technology</em></span>; define the companies with more than $1B revenue as <span class="emphasis"><em>large</em></span> and <span class="emphasis"><em>small</em></span> with net asset $1M for instance.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip15"></a>Tip</h3><p>Split data into separate categories or bins.</p></div><p>To be more specific, let's give you some examples. Suppose you are doing some analytics on the companies with an annual that ranges from $50 M to over $1 B. Therefore, obviously, you can split the revenue into some sequential bins, such as $50-$200M, $201-$500M, $501M-$1B, and $1B+, for instance. Now how do you represent the features in a presentable format? It's so simple, try to put a value one whenever a company falls with the revenue bin; otherwise, the value is zero. There are now four new data fields created from the annual revenue field, right?</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip16"></a>Tip</h3><p>Think of an innovative way to combine existing data fields into new ones.</p></div><p>To be more specific, let's give you some examples. In the very first tip, we argue how to create new inputs by rolling up existing fields into broader fields. Now, suppose if you want to create a Boolean flag that identifies whether someone falls in a VP or higher category with more than 10 years of experience. Therefore, in this case, you are actually creating new fields by multiplying, dividing, adding, or subtracting one data field by another.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip17"></a>Tip</h3><p>Think about the problem at hand and be creative simultaneously.</p></div><p>In previous tips, suppose you have created enough bins and fields or inputs. Now, don't worry much about creating too many variables in the first place. It would be wise to just let the brainstorming flow a normal flow for the feature selection step.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip18"></a>Tip</h3><p>Don't be a fool.</p></div><p>Be cautious about creating unnecessary fields; since creating too many features out of a small amount of data may overfit your model, which can lead to spurious results. When you face the data correlation, remember that correlation does not always imply causation. Our logic to this common point is that modeling observational data can only show us that two variables are related, but it cannot tell us the reason.</p><p>Research articles in the book <span class="emphasis"><em>Freakonomics</em></span> (see also at <span class="emphasis"><em>Steven D. Levitt, Stephen J. Dubner, Freakonomics: A Rogue Economist Explores the Hidden Side of Everything</em></span>, <a class="ulink" href="http://www.barnesandnoble.com/w/freakonomics-steven-d-levitt/1100550563http://www.barnesandnoble.com/w/freakonomics-steven-d-levitt/1100550563" target="_blank">http://www.barnesandnoble.com/w/freakonomics-steven-d-levitt/1100550563http://www.barnesandnoble.com/w/freakonomics-steven-d-levitt/1100550563</a>) has found that data from public school's test scores indicates that children living at home with a higher number of books have a tendency of having higher standardized test scores compared to those with a lower number of books at home.</p><p>Therefore, be cautious before creating and constructing unnecessary features, which implies that don't be a fool.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip19"></a>Tip</h3><p>Don't over engineer.</p></div><p>It is trivial to judge the difference whether an iteration takes a few minutes or half a day during the feature engineering phase. Since the most productive time during the feature engineering phase is usually spent on the whiteboard. Therefore, the most productive way to make sure it is done right is to ask the right questions to your data. It's true that nowadays the term big data is taking over the term feature engineering. There is no room for hacking, so for the over engineering:</p><div class="mediaobject"><img src="graphics/image_04_004.jpg" /><div class="caption"><p>Figure 4: Real interpretation of false positive and false negative.</p></div></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip20"></a>Tip</h3><p>Beware of false positives and false negatives.</p></div><p>Another important aspect is comparing the false negatives and false positives. Depending on the problem, getting a higher accuracy on one or the other is important. For instance, if you are doing research in the healthcare section and trying to develop a machine learning model that will work towards the disease prediction, getting false positives might be better than getting the false negative results. Therefore, our suggestion in this regard would be to look at the confusion matrix that will help you to see the predictions made by a classifier in a visual way.</p><p>The rows indicate the true class of each observation while the columns correspond to the class predicted by the model itself, as shown in <span class="emphasis"><em>Figure 4</em></span>. However, <span class="emphasis"><em>Figure 5</em></span> would provide more insight. Note that the diagonal elements, also called correct decision, are marked in bold. The last column, <span class="strong"><strong>Acc</strong></span>, signifies the accuracy for each key as follows:</p><div class="mediaobject"><img src="graphics/B05243_04_05.jpg" /><div class="caption"><p>Figure 5: A simple confusion matrix.</p></div></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip21"></a>Tip</h3><p>Think about precision and recall before selecting features.</p></div><p>Finally, two more important quantities to consider are the precision and recall. More technically, how often your classifier predicts a +ve outcome correctly is called recall. On the contrary, when your classifier predicts a +ve output and how often it is actually true is the precision.</p><p>It's true that it's really difficult to predict these two values. However, a careful feature selection would help you to get both the values better in the last place.</p><p>You will find more interesting and some excellent descriptions about the feature selection in a research paper written by <span class="emphasis"><em>Matthew Shardlow</em></span> (see also at Matthew Shardlow, <span class="emphasis"><em>An Analysis of Feature Selection Techniques</em></span>, <a class="ulink" href="https://studentnet.cs.manchester.ac.uk/pgt/COMP61011/goodProjects/Shardlow.pdf" target="_blank">https://studentnet.cs.manchester.ac.uk/pgt/COMP61011/goodProjects/Shardlow.pdf</a>). Now let's have a journey to the realm of Spark's feature engineering features in the next section.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec33"></a>Feature engineering with Spark</h2></div></div><hr /></div><p>Machine learning based on big data is a deep and broad area and it needs a new recipe and the ingredients would be feature engineering and stable optimization of the model out of the data. The optimized model can be called Big Models (see also at <span class="emphasis"><em>S. Martinez</em></span>, <span class="emphasis"><em>A. Chen</em></span>, <span class="emphasis"><em>G. I. Webb</em></span>, and <span class="emphasis"><em>N. A. Zaidi</em></span>, <span class="emphasis"><em>Scalable learning of Bayesian network classifiers</em></span>, accepted to be published in <span class="emphasis"><em>Journal of Machine Learning Research</em></span>) that can learn from big data and holds the key to a breakthrough other than big data.</p><p>Big model also signifies that your results out of diverse and complex big data would be with low bias (see at <span class="emphasis"><em>D. Brain and G. I. Webb</em></span>, <span class="emphasis"><em>The need for low bias algorithms in classification learning from small data sets</em></span>, <span class="emphasis"><em>in PKDD</em></span>, <span class="emphasis"><em>pp. 62, 73, 2002</em></span>) and out-of-core (see out-of-core learning defined at, <a class="ulink" href="https://en.wikipedia.org/wiki/Out-of-core_algorithm" target="_blank">https://en.wikipedia.org/wiki/Out-of-core_algorithm</a> and <a class="ulink" href="https://en.wikipedia.org/wiki/Out-of-core_algorithm" target="_blank">
https://en.wikipedia.org/wiki/Out-of-core_algorithm</a>) using multi-class machine learning algorithms with minimal tuning parameters.</p><p>Spark introduces this big model for us to deploy our machine learning application at scale. In this section, we will describe how Spark developed machine learning libraries and Spark core to handle the advanced features of feature engineering for large-scale datasets and different data structures efficiently.</p><p>As we already stated, Spark's machine learning module contains two APIs including <code class="literal">spark.mllib</code> and <code class="literal">spark.ml</code>. The MLlib package is built on top of RDD, whereas the ML package is built on top of DataFrame and Dataset that provides a higher label API for constructing an ML pipeline. The next few sections will show you the details of the ML (MLlib will be discussed in<a class="link" href="#" linkend="ch05">
Chapter 5
</a>, <span class="emphasis"><em>Supervised and Unsupervised Learning by Examples</em></span>) package with examples concluding with a practical machine learning problem.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec56"></a>Machine learning pipeline – an overview</h3></div></div></div><p>Spark's ML package provides a uniform set of higher-level APIs that helps to create a practical machine learning pipeline. The main concept of this pipeline is to combine multiple algorithms of machine learning together to make a complete workflow. In the machine learning arena, it is often common practice to run a sequence of algorithms to process and learn from the available data.</p><p>For example, suppose you want to develop a text analytics machine learning application. The total process could be split into several stages for a collection of some simple text document. Naturally, the processing workflow might include several stages. In the very first step, you need to split the text into words from each document. Once you have the split words, you should convert those words into numerical feature vectors for the words from each document.</p><p>Finally, you might want to learn a prediction model using the features vector you got in stage 2 and also want to label each vector to use supervised machine learning algorithms. In brief, these four stages can be summarised as follows. For each document, do the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Split the texts=&gt; words</p></li><li style="list-style-type: disc"><p>Convert words =&gt; numerical feature vectors</p></li><li style="list-style-type: disc"><p>Numerical feature vectors =&gt; labeling</p></li><li style="list-style-type: disc"><p>Build an ML model as a prediction model using vectors and labels</p></li></ul></div><p>These four stages could be considered as a workflow. The Spark ML represents these kinds of workflows as pipelines that consists of a sequence of PipelineStages; where a Transformer and an Estimator contribute in each stage of the pipeline to be run in a certain order. The Transformer is actually an algorithm to transform one Dataset to another Dataset.</p><p>On the other hand, an Estimator is also an algorithm, which is liable for fitting on a Dataset to produce a Transformer. Technically, an Estimator implements a method called <code class="literal">fit()</code>, which accepts a Dataset and produces a model, which is a Transformer.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip22"></a>Tip</h3><p>Interested readers should refer to this URL <a class="ulink" href="http://spark.apache.org/docs/latest/ml-pipeline.html" target="_blank">http://spark.apache.org/docs/latest/ml-pipeline.html</a> for more details on the Transformer, an Estimator in pipelines.</p></div><div class="mediaobject"><img src="graphics/image_04_006.jpg" /><div class="caption"><p>Figure 6: Pipeline is an Estimator.</p></div></div><p>To be more specific, let's draw an example, suppose a machine learning algorithm such as Logistic Regression (or the Linear Regression) is used as an Estimator. Now by calling the <code class="literal">fit()</code> method , which trains a <span class="strong"><strong>Logistic Regression Model</strong></span> (which itself is a model, and hence a Transformer). Technically, a Transformer implements a method, namely <code class="literal">transform()</code>, which converts one Dataset into another.</p><p>During the conversion, one more column is depending upon the selection and column position. It is to be noted that the pipeline concept that Spark has developed is mostly inspired by the Scikit-learn project, which is a simple and efficient tool for data mining and data analysis (see also at Scikit-learn project, <a class="ulink" href="http://scikit-learn.org/stable/" target="_blank">http://scikit-learn.org/stable/</a>).</p><p>As discussed in <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Introduction to Data Analytics with Spark</em></span>, Spark has implemented RDD operation as <span class="strong"><strong>Directed Acyclic Graph</strong></span> (<span class="strong"><strong>DAG</strong></span>) style. The same fashion is also applicable on pipelining as well; wherein each DAG Pipeline, stages are specified as an ordered array. The text-processing pipeline we previously described as an example with four stages is actually a linear Pipeline; in which each stage consumes the data produced by the previous stage. It is also possible to create the non-linear pipelines as long as the data flow of the feature engineering graph forms and aligns in a DAG style.</p><p>It is to be noted that, if a Pipeline forms a DAG, then the stages need to be specified in topological order essentially. The pipeline we are talking about can be operated on top of Dataset including various file types, therefore run-time and compile-time checking from the pipelining consistencies is required. Unfortunately, the current implementation of Spark Pipeline does not provide the use compile-time type checking. However, Spark provides the run-time checking that is used by the Pipelines and PipelineModels, which is done using the Dataset schema.</p><p>Since the concept of RDD is immutable, that means once an RDD is created, it's not possible to change the contents of the RDD, similarly, uniqueness in Pipeline stages should be persistent (please refer to <span class="emphasis"><em>Figure 6</em></span> and <span class="emphasis"><em>Figure 7</em></span> for the clear view) with unique IDs. For simplicity, the preceding text processing workflow can be visualized as like Figure 5; where we have shown the text processing pipeline with three stages. The <span class="strong"><strong>Tokenizer</strong></span> and <span class="strong"><strong>HashingTF</strong></span> are two unique Transformers.</p><p>On the other hand, LogisticRegression is an Estimator. In the bottom row, a cylinder indicates a Dataset. The <code class="literal">fit()</code> method from pipeline is called on the original Dataset containing the documents of text with labels. Now the <code class="literal">Tokenizer.transform()</code> method splits the raw text documents into words and the <code class="literal">HashingTF.transform()</code> method on the other hand converts the words column into feature vectors.</p><p>Please note in each case, a column on the Dataset is added. Now the <code class="literal">LogisticRegression.fit()</code> method is called to produce a <code class="literal">LogisticRegressionModel</code>:</p><div class="mediaobject"><img src="graphics/image_04_007.jpg" /><div class="caption"><p>Figure 7: Pipeline is an Estimator.</p></div></div><p>In <span class="emphasis"><em>Figure 7</em></span>, the PipelineModel has the same number of stages as the original Pipeline. However, in this case, all the Estimators from the original Pipeline need to be converted into Transformers.</p><p>When the <code class="literal">transform()</code> method from the <span class="strong"><strong>PipelineModel</strong></span> is called on a test Dataset (that is, numeric feature vectors), the data is passed through the fitted pipeline in a certain order.</p><p>In a nutshell, Pipelines and PipelineModel help to ensure that training and test data go through identical feature processing steps. The following section shows a practical example of the preceding pipelining process we have described.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec57"></a>Pipeline – an example with Spark ML</h3></div></div></div><p>This section will show a practical machine-learning problem called <span class="strong"><strong>Spam Filtering</strong></span>, which was introduced in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Understanding the Problem by Understanding the Data</em></span> with Spark's pipeline. We will use the <code class="literal">SMSSpamCollection</code> dataset downloaded from <a class="ulink" href="https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection" target="_blank">https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection</a> to show the feature engineering with Spark. The following code reads a sample dataset as a Dataset using a <span class="strong"><strong>Plain Old Java Object</strong></span> (<span class="strong"><strong>POJO</strong></span>) class (see more at <a class="ulink" href="https://en.wikipedia.org/wiki/Plain_Old_Java_Object" target="_blank">https://en.wikipedia.org/wiki/Plain_Old_Java_Object</a>). Note that the <code class="literal">SMSSpamHamLabelDocument</code> class contains the label (<code class="literal">label: double</code>) and SMS lines (<code class="literal">text: String</code>).</p><p>To run the code, just create a Maven project in your Eclipse IDE by specifying the master URL and dependencies on the provided <code class="literal">pom.xml</code> file under the Maven project and package the application as a jar file. Alternatively, run the example on Eclipse as a standalone Java application.</p><p>The code for Spark session creation is as follows:</p><pre class="programlisting">  static SparkSession spark = SparkSession
      .builder().appName("JavaLDAExample")
      .master("local[*]")
      .config("spark.sql.warehouse.dir", "E:/Exp/")
      .getOrCreate();
</pre><p>Here the Spark SQL warehouse is set to the <code class="literal">E:/Exp/</code> directory for Windows. Set your path accordingly based on the OS type.</p><p>The code for the <code class="literal">smsspamdataset</code> sample is as follows:</p><pre class="programlisting">public static void main(String[] args) {
 // Prepare training documents, which are labelled.
 Dataset&lt;Row&gt; smsspamdataset = spark.createDataFrame(Arrays.asList(
      new SMSSpamHamLabelDocument(0.0, "What you doing how are you"),
      new SMSSpamHamLabelDocument(0.0, "Ok lar Joking wif u oni"),
      new SMSSpamHamLabelDocument(1.0, "FreeMsg Txt CALL to No 86888 &amp; claim your reward of 3 hours talk time to use from your phone now ubscribe6GBP mnth inc 3hrs 16 stop txtStop"),
      new SMSSpamHamLabelDocument(0.0, "dun say so early hor U c already then say"),
      new SMSSpamHamLabelDocument(0.0, "MY NO IN LUTON 0125698789 RING ME IF UR AROUND H"),
      new SMSSpamHamLabelDocument(1.0, "Sunshine Quiz Win a super Sony DVD recorder if you canname the capital of Australia Text MQUIZ to 82277 B")
    ), SMSSpamHamLabelDocument.class);
</pre><p>Now let's see the structure of the Dataset by calling the <code class="literal">show()</code> method as follows:</p><pre class="programlisting">Smsspamdataset.show();
</pre><p>The output will look as follows:</p><div class="mediaobject"><img src="graphics/image_04_008.jpg" /><div class="caption"><p>The code for the POJO Class is as follows:</p></div></div><pre class="programlisting">public class SMSSpamHamLabelDocument implements Serializable {
    private double label;
    private String wordText;
    public SMSSpamHamLabelDocument(double label, String wordText) {
      this.label = label;
      this.wordText = wordText;
    }
    public double getLabel() { return this.label; }
    public void setLabel(double id) { this.label = label; }
    public String getWordText() { return this.wordText; }    public void setWordText(String wordText) { this.wordText = wordText; }
}  }
</pre><p>Now, let's split the dataset into <code class="literal">trainingData</code> (60%) and <code class="literal">testData</code> (40%) for the model training purpose.</p><p>The code for splits is as follows:</p><pre class="programlisting">Dataset&lt;Row&gt;[] splits = smsspamdataset.randomSplit(new double[] { 0.6, 0.4 });
Dataset&lt;Row&gt; trainingData = splits[0];
Dataset&lt;Row&gt; testData = splits[1];
</pre><p>The objective of the dataset is to build a predictive model using a classification algorithm, as we know from the dataset; there are two types of messages. One is spam, which is represented as 1.0, and another one is ham, represented as 0.0 labels. We can consider here the LogisticRegression or linear regression algorithm for training a model for the simplicity of training and using.</p><p>However, more complex classifiers using regression such as generalized regression will be discussed in <a class="link" href="#" linkend="ch08">Chapter 8</a>, <span class="emphasis"><em>Adapting Your Machine Learning Models</em></span>. Consequently, our workflow or pipeline will be like the following, according to our dataset:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Tokenize the text lines into words from the training data</p></li><li style="list-style-type: disc"><p>Extracting the features using the hashing technique</p></li><li style="list-style-type: disc"><p>Applying a LogisticRegression Estimator for building a model</p></li></ul></div><p>The preceding three steps can be done easily by Spark's pipeline component. You can define all the stages into a single Pipeline class that will build a model in an efficient way. The following code shows the whole pipeline for building the predictive model. The Tokenizer class defines the input and output column (for example, <code class="literal">wordText</code> to words), the <code class="literal">HashTF</code> class defines how to extract the features from the words of the Tokenizer class.</p><p>The <code class="literal">LogisticRegression</code> class configures its parameter. Finally, you can see the Pipeline class that takes the preceding methods into a PipelineStage array and returns an Estimator. After applying the <code class="literal">fit()</code> method to the training set it will return the final model, which is ready for prediction. You can see the output of the test data after applying a model for predicting.</p><p>The code for Pipeline is as follows:</p><pre class="programlisting">Tokenizer tokenizer = new Tokenizer()
      .setInputCol("wordText")
      .setOutputCol("words");
HashingTF hashingTF = new HashingTF()
      .setNumFeatures(100)
      .setInputCol(tokenizer.getOutputCol())
      .setOutputCol("features");
LogisticRegression logisticRegression = new LogisticRegression()
      .setMaxIter(10)
      .setRegParam(0.01);
Pipeline pipeline = new Pipeline().setStages(new PipelineStage[] {tokenizer, hashingTF, logisticRegression});
    // Fit the pipeline to training documents.
PipelineModel model = pipeline.fit(trainingData);
Dataset&lt;Row&gt; predictions = model.transform(testData);
for (Row r: predictions.select("label", "wordText", "prediction").collectAsList()) {
  System.out.println("(" + r.get(0) + ", " + r.get(1) + ") --&gt; prediction=" + r.get(2));
    } }
</pre><p>The output is as follows:</p><pre class="programlisting">(0.0, What you doing how are you)
--&gt; prediction=0.0
(0.0, MY NO IN LUTON 0125698789 RING ME IF UR AROUND H)
--&gt; prediction=0.0
(1.0, Sunshine Quiz Win a super Sony DVD recorder if you canname the capital of Australia Text MQUIZ to 82277 B)
--&gt; prediction=0.0
</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec58"></a>Feature transformation, extraction, and selection</h3></div></div></div><p>The preceding section showed you the overall process of the pipeline. This pipeline or workflow is basically the collection of some operation such as transformation to one dataset of another data set, extracting the features, and selecting the features. These are the basic operators for feature engineering that we already described in previous sections. This section will show you the details about those operations using Spark machine learning packages. Spark provides some efficient APIs for feature engineering including MLlib and ML.</p><p>In this section, we will start with the ML package by continuing the Spam Filter examples. Let's read a large dataset from the text file as Dataset, which contains the lines starting with ham or spam words. The sample output of this Dataset is given here. Now we will use this dataset for feature extracting the features and building a model with Spark's APIs.</p><p>The code for <code class="literal">Input DF</code> is as follows:</p><pre class="programlisting">Dataset&lt;Row&gt; df = spark.read().text("input/SMSSpamCollection.txt");
df.show();
</pre><p>The output is as follows:</p><div class="mediaobject"><img src="graphics/image_04_009.jpg" /></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl3sec55"></a>Transformation – RegexTokenizer</h4></div></div></div><p>From the preceding output, you can see that we have to transform it into two columns for identifying the spam and ham messages. For doing this, we can use the <code class="literal">RegexTokenizer</code> Transformer that can take input from a regular expression (<code class="literal">regex</code>) and transform it to a new dataset. This code produces <code class="literal">labelFeatured</code>. For example, refer the Dataset shown in the following output:</p><pre class="programlisting">// Feature Transformers (RegexTokenizer)
RegexTokenizer regexTokenizer1 = new RegexTokenizer()
        .setInputCol("value")
        .setOutputCol("labelText")
        .setPattern("\\t.*$");
Dataset&lt;Row&gt; labelTextDataFrame = regexTokenizer1.transform(df);
RegexTokenizer regexTokenizer2 = new RegexTokenizer()
        .setInputCol("value").setOutputCol("text").setPattern("\\W");
Dataset&lt;Row&gt; labelFeatureDataFrame = regexTokenizer2
        .transform(labelTextDataFrame);
for (Row r : labelFeatureDataFrame.select("text", "labelText").collectAsList()) {
      System.out.println( r.getAs(1) + ": " + r.getAs(0));
    }
</pre><p>Here is the output of <code class="literal">labelFeature</code> Dataset:</p><pre class="programlisting">WrappedArray(ham): WrappedArray(ham, what, you, doing, how, are, you)
WrappedArray(ham): WrappedArray(ham, ok, lar, joking, wif, u, oni)
WrappedArray(ham): WrappedArray(ham, dun, say, so, early, hor, u, c, already, then, say)
WrappedArray(ham): WrappedArray(ham, my, no, in, luton, 0125698789, ring, me, if, ur, around, h)
WrappedArray(spam): WrappedArray(spam, freemsg, txt, call, to, no, 86888, claim, your, reward, of, 3, hours, talk, time, to, use, from, your, phone, now, ubscribe6gbp, mnth, inc, 3hrs, 16, stop, txtstop)
WrappedArray(ham): WrappedArray(ham, siva, is, in, hostel, aha)
WrappedArray(ham): WrappedArray(ham, cos, i, was, out, shopping, wif, darren, jus, now, n, i, called, him, 2, ask, wat, present, he, wan, lor, then, he, started, guessing, who, i, was, wif, n, he, finally, guessed, darren, lor)
WrappedArray(spam): WrappedArray(spam, sunshine, quiz, win, a, super, sony, dvd, recorder, if, you, canname, the, capital, of, australia, text, mquiz, to, 82277, b)
</pre><p>Now let's create a new Dataset from the <code class="literal">labelFeatured</code> Dataset that we just created by selecting the label text as follows:</p><pre class="programlisting">Dataset&lt;Row&gt; newDF = labelFeatureDataFrame.withColumn("labelTextTemp",        labelFeatureDataFrame.col("labelText").cast(DataTypes.StringType))        .drop(labelFeatureDataFrame.col("labelText"))        .withColumnRenamed("labelTextTemp", "labelText");
</pre><p>Now let's further explore the contents in the new Dataset by calling the <code class="literal">show()</code> method as follows:</p><div class="mediaobject"><img src="graphics/image_04_010.jpg" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl3sec56"></a>Transformation – StringIndexer</h4></div></div></div><p>The preceding output has the classification of ham and spam messages, but we have to make the ham and spam text as double values. The <code class="literal">StringIndexer</code> Transformer can do it easily. It can encode a string column of labels into indices in another column. The indices are ordered by label frequencies. <code class="literal">StringIndexer</code> produces two indices, 0.0 and 1.0 for our dataset:</p><pre class="programlisting">// Feature Transformer (StringIndexer)
StringIndexer indexer = new StringIndexer().setInputCol("labelText")
        .setOutputCol("label");
Dataset&lt;Row&gt; indexed = indexer.fit(newDF).transform(newDF);
    indexed.select(indexed.col("labelText"), indexed.col("label"), indexed.col("text")).show();
</pre><p>The following is the output for the <code class="literal">indexed.show()</code> function:</p><div class="mediaobject"><img src="graphics/image_04_011.jpg" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl3sec57"></a>Transformation – StopWordsRemover</h4></div></div></div><p>The preceding output contains words or tokens, but some words are not as important as features. Therefore, we need to remove those words. For making this task easier, Spark provides the list of stop words through the <code class="literal">StopWordsRemover</code> class that will be discussed more in <a class="link" href="#" linkend="ch06">Chapter 6</a>, <span class="emphasis"><em>Building Scalable Machine Learning Pipelines</em></span>.</p><p>We can use those words to filter unwanted words. Additionally, we will remove the ham and spam words from the text column. The <code class="literal">StopWordsRemover</code> class will transform the preceding Dataset into a filtered Dataset by removing the stop works from the features. The following output will show us the words without spam and ham word tokens:</p><pre class="programlisting">// Feature Transformers (StopWordsRemover)
StopWordsRemover remover = new StopWordsRemover();
String[] stopwords = remover.getStopWords();
String[] newStopworks = new String[stopwords.length+2];
newStopworks[0]="spam";
newStopworks[1]="ham";
for(int i=2;i&lt;stopwords.length;i++){
      newStopworks[i]=stopwords[i];}
remover.setStopWords(newStopworks).setInputCol("text").setOutputCol("filteredWords");
Dataset&lt;Row&gt; filteredDF = remover.transform(indexed);
filteredDF.select(filteredDF.col("label"), filteredDF.col("filteredWords")).show();
</pre><p>The output is as follows:</p><div class="mediaobject"><img src="graphics/image_04_012.jpg" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl3sec58"></a>Extraction – TF</h4></div></div></div><p>Now we have the Dataset containing a label with a double value and filtered words or tokens. The next task is to vectorize (make numeric values) the features or extract the features from the words or tokens.</p><p><span class="strong"><strong>TF-IDF</strong></span> (<code class="literal">HashingTF</code> and <code class="literal">IDF</code>; also known as <span class="strong"><strong>Term Frequency-Inverse Document Frequency</strong></span>) is a feature vectorization method widely used for extracting the features, which basically calculates the importance of a term to a document in the corpus.</p><p><code class="literal">TF</code> counts the frequency of the terms in a document or line and <code class="literal">IDF</code> counts the document or line frequency, that is, number of document or lines containing a particular term. The following code explains the term frequency of the preceding dataset using the efficient <code class="literal">HashingTF</code> class of Spark. <code class="literal">HashingTF</code> is a Transformer that takes sets of terms; and converts those sets into fixed-length feature vectors. The output of the featured data is also shown:</p><pre class="programlisting">// Feature Extractors (HashingTF transformer)
int numFeatures = 100;
HashingTF hashingTF = new HashingTF().setInputCol("filteredWords")
        .setOutputCol("rawFeatures").setNumFeatures(numFeatures);
Dataset&lt;Row&gt; featurizedData = hashingTF.transform(filteredDF);
    for (Row r : featurizedData.select("rawFeatures", "label").collectAsList()) {
Vector features = r.getAs(0); ////Problematic line
Double label = r.getDouble(1);
System.out.println(label + "," + features);
    }
</pre><p>The output is as follows:</p><pre class="programlisting">0.0,(100,[19],[1.0])
0.0,(100,[9,16,17,48,86,96],[1.0,1.0,1.0,1.0,1.0,1.0])
0.0,(100,[17,37,43,71,99],[1.0,1.0,2.0,1.0,2.0])
0.0,(100,[4,41,42,47,92],[1.0,1.0,1.0,1.0,1.0])
1.0,(100,[3,12,19,26,28,29,34,41,46,51,71,73,88,93,94,98],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0])
0.0,(100,[19,25,38],[1.0,1.0,1.0])
0.0,(100,[8,10,16,30,37,43,48,49,50,55,76,82,89,95,99],[1.0,4.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0])
1.0,(100,[0,24,36,39,42,48,53,58,67,86,95,97,98],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0])
</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl3sec59"></a>Extraction – IDF</h4></div></div></div><p>Similarly, we can apply <code class="literal">IDF</code> on the featured data to count the document frequency. <code class="literal">IDF</code> is an Estimator that fits on the preceding dataset and produces an <code class="literal">IDFModel</code> that transforms to a rescaled dataset containing features and labels:</p><pre class="programlisting">// Feature Extractors (IDF Estimator)
IDF idf = new IDF().setInputCol("rawFeatures").setOutputCol("features");
IDFModel idfModel = idf.fit(featurizedData);
Dataset&lt;Row&gt; rescaledData = idfModel.transform(featurizedData);
for (Row r : rescaledData.select("features", "label").collectAsList()) {
Vector features = r.getAs(0);
Double label = r.getDouble(1);
System.out.println(label + "," + features);
    }
</pre><p>The output is as follows:</p><pre class="programlisting">0.0,(100,[19],[0.8109302162163288])
0.0,(100,[9,16,17,48,86,96],[1.5040773967762742,1.0986122886681098,1.0986122886681098,0.8109302162163288,1.0986122886681098,1.5040773967762742])
0.0,(100,[17,37,43,71,99],[1.0986122886681098,1.0986122886681098,2.1972245773362196,1.0986122886681098,2.1972245773362196])
0.0,(100,[4,41,42,47,92],[1.5040773967762742,1.0986122886681098,1.0986122886681098,1.5040773967762742,1.5040773967762742])
1.0,(100,[3,12,19,26,28,29,34,41,46,51,71,73,88,93,94,98],[1.5040773967762742,1.5040773967762742,0.8109302162163288,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.0986122886681098,1.5040773967762742,1.5040773967762742,1.0986122886681098,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,2.1972245773362196])
0.0,(100,[19,25,38],[0.8109302162163288,1.5040773967762742,1.5040773967762742])
0.0,(100,[8,10,16,30,37,43,48,49,50,55,76,82,89,95,99],[1.5040773967762742,6.016309587105097,2.1972245773362196,1.5040773967762742,1.0986122886681098,2.1972245773362196,0.8109302162163288,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.0986122886681098,2.1972245773362196])
1.0,(100,[0,24,36,39,42,48,53,58,67,86,95,97,98],[1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.0986122886681098,0.8109302162163288,1.5040773967762742,1.5040773967762742,3.0081547935525483,1.0986122886681098,1.0986122886681098,3.0081547935525483,1.0986122886681098])
</pre><p>The preceding output extracts features from the raw texts. The very first entry is the label and the rest are the feature vector extracted.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl3sec60"></a>Selection – ChiSqSelector</h4></div></div></div><p>The preceding output is ready for training using a classification algorithm such as <code class="literal">LogisticRegression</code>. But we can use the more important feature from the categorical features. For doing this, Spark provides some feature selector APIs such as <code class="literal">ChiSqSelector</code>. The <code class="literal">ChiSqSelector</code> is called <span class="strong"><strong>Chi-Squared feature selection</strong></span>.</p><p>It operates on labeled data with categorical features. It orders features based on a Chi-Squared test, which is independent from the class, and then filters the top features which the class label depends on the most. This selector is useful for improving the predictive power of a model. The following code will select the top three features from the feature vectors, along with the output given:</p><pre class="programlisting">org.apache.spark.ml.feature.ChiSqSelector selector = new org.apache.spark.ml.feature.ChiSqSelector();
selector.setNumTopFeatures(3).setFeaturesCol("features")
        .setLabelCol("label").setOutputCol("selectedFeatures");
Dataset&lt;Row&gt; result = selector.fit(rescaledData).transform(rescaledData);
    for (Row r : result.select("selectedFeatures", "label").collectAsList()) {
  Vector features = r.getAs(0);
  Double label = r.getDouble(1);
  System.out.println(label + "," + features);
    }
</pre><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip23"></a>Tip</h3><p>We will discuss more on the <code class="literal">ChiSqSelector</code>, <code class="literal">IDFModel</code>, <code class="literal">IDF</code>, <code class="literal">StopWordsRemover</code>, and <code class="literal">RegexTokenizer</code> classes in <a class="link" href="#" linkend="ch06">Chapter 6</a>, <span class="emphasis"><em>Building Scalable Machine Learning Pipelines</em></span>.</p></div><p>The output is as follows:</p><pre class="programlisting">0.0,(3,[],[])
0.0,(3,[],[])
0.0,(3,[],[])
0.0,(3,[],[])
1.0,(3,[1,2],[1.5040773967762742,2.1972245773362196])
0.0,(3,[],[])
0.0,(3,[],[])
1.0,(3,[0,2],[1.5040773967762742,1.0986122886681098])
</pre><p>Now, you can apply <code class="literal">LogisticRegression</code> when building a model with the feature vectors. Spark provides lots of different APIs for feature engineering. However, we have not used the other machine learning of Spark (that is, Spark MLlib) for the brevity and page limitation. We will discuss the feature engineering using <code class="literal">spark.mllib</code> gradually with examples in future chapters.</p></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec34"></a>Advanced feature engineering</h2></div></div><hr /></div><p>In this section, we will discuss some advanced features that are also involved in the feature engineering process such as manual feature construction, feature learning, iterative process of feature engineering, and deep learning.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec59"></a>Feature construction</h3></div></div></div><p>The best results come down to you through the manual feature engineering or feature construction. Therefore, manual construction is the process of creating new features from the raw data. Feature selection based on the feature's importance can inform you about the objective utility of features; however, those features have to come from somewhere else. In fact, sometimes, you need to manually create them.</p><p>In contrast to the feature selection, the feature construction technique requires spending a lot of effort and time with not the aggregation or picking the feature, but on the actual raw data so that new features can be constructive towards increasing the predictive accuracies of the model. Therefore, it also involves thinking of the underlying structure of the data along with the ML problem.</p><p>In this regard, to construct new features from the complex and high dimensional dataset, you need to know the overall structure of the data. In addition to this, how to use and apply them in predictive modeling algorithms. There will be three aspects in terms of tabular, textual, and multimedia datasets:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Handling and manual creation from the tabular data often means a mixture of combining features to create new features. You might also need the decomposing or splitting of some original features to create new features.</p></li><li style="list-style-type: disc"><p>With textual data, it often means devising document or context-specific indicators relevant to the problem. For example, when you are applying text analytics on large raw data such as data from Twitter hashtags.</p></li><li style="list-style-type: disc"><p>With multimedia data such as image data, it can often mean enormous amounts of time are passed to pick out relevant structures in a manual way.</p></li></ul></div><p>Unfortunately, the feature construction technique is not only manual, but the whole process is slower, requiring lots of research involvement from humans like you and us. However, it can make a big difference in the long run. In fact, feature engineering and feature selection are not mutually exclusive; however, both of them are important in the realm of machine learning.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec60"></a>Feature learning</h3></div></div></div><p>Is it possible to avoid the manual process of prescribing how to construct or extract features from raw data? Feature learning helps you to get rid of this. Therefore, feature learning is an advanced process; alternatively, an automatic identification and use of features from raw data. This is also referred to as representation learning that helps your machine learning algorithm to identify useful features.</p><p>The feature learning technique is commonly used in deep learning algorithms. As a result, recent deep learning techniques are achieving some success in this area. The auto-encoders and restricted Boltzmann machines are such an example where the concept of feature learning was used. The key idea behind feature learning is the automatic and abstract representations of the features in a compressed form using unsupervised or semi-supervised learning algorithms.</p><p>Speech recognition, image classification, and object recognition are some successful examples; where researchers have found supported state-of-the-art results. Further details could not have been represented in this book due to the brevity.</p><p>Unfortunately, Spark has not implemented any APIs for the automatic feature extraction or construction.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec61"></a>Iterative process of feature engineering</h3></div></div></div><p>The whole process of feature engineering is not a standalone, but more or less iterative. Since you are actually interplaying with the form the data selection to model evaluation again and again until you are completely satisfied or you are running out of time. The iteration could be imagined as a four-step workflow that iteratively runs over time. When you are aggregating or collecting the raw data, you might not be doing enough brainstorming. However, when you start exploring the data, you are really getting into the problem into deeper.</p><p>After that you will be looking at a lot of data, studying the best technique of feature engineering and the related problems presented in the state of the arts and you will see how much you are able to steal. When you have done enough brainstorming, you will start devising the required features or extracting the features depending on your problem type or class. You might use the automatic feature extraction or manual feature construction (or both sometimes). If you are not satisfied with the performance you might redo the feature extraction process for improvement. Please refer to <span class="emphasis"><em>Figure 7</em></span> for a clear view of the iterative process of feature engineering:</p><div class="mediaobject"><img src="graphics/image_04_013.jpg" /><div class="caption"><p>Figure 8: The iterative processing in feature engineering.</p></div></div><p>When you have devised or extracted the feature, you need to select the features. You might apply a different scoring or ranking mechanism based on feature importance. Similarly, you might iterate the same process such as devising the feature to improve the model. And finally, you will evaluate your model to estimate the model's accuracy on new data to make your model adaptive.</p><p>You also need a well-defined problem that will help you to stop the whole iteration. When finished, you can move on to try other models. There will be gain waiting for you in the future once you plateau on ideas or the accuracy delta out of your ML pipeline.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec62"></a>Deep learning</h3></div></div></div><p>One of the most interesting and promising moves in data representation we would say is deep learning. It is very popular on the tensor computing application and the <span class="strong"><strong>Artificial Intelligent Neural Network </strong></span>(<span class="strong"><strong>AINN</strong></span>) system. Using the deep learning technique, the network learns how to represent data at different levels.</p><p>Therefore, you will have an exponential ability to represent the linear data you have. Spark can take this advantage and it can be used to improve deep learning. For more general discussion, please refer to the following URL at <a class="ulink" href="https://en.wikipedia.org/wiki/Deep_learning" target="_blank">
https://en.wikipedia.org/wiki/Deep_learning
</a>and to learn how to deploy pipelines on a cluster with TensorFlow, see <a class="ulink" href="https://www.tensorflow.org/" target="_blank">https://www.tensorflow.org/</a>.</p><p>A recent research and development at Databricks (also see <a class="ulink" href="https://databricks.com/" target="_blank">https://databricks.com/</a>) has shown that Spark can also be used to find the best set of hyperparameters for AINN training. The advantage is that Spark will do the computation 10X faster than a normal deep learning or neural network algorithm.</p><p>Consequently, your model training time will drastically reduce up to 10 times and the error rate will be 34% lower. Moreover, Spark can be applied to a trained AINN model on a large amount of data so you can deploy your ML model at scale. We will discuss more on deep learning in later chapters as advanced machine learning.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec35"></a>Summary</h2></div></div><hr /></div><p>Feature engineering, feature selection, and feature construction are the three most commonly used steps while preparing the training and test set towards building a machine learning model. Usually, the feature engineering is applied first to generate additional features from the available dataset. After that, the feature selection technique is applied to eliminate irrelevant, missing or null, redundant, or even highly correlated features so that high predictive accuracy can be availed.</p><p>In contrast, feature construction is an advanced technique applied to construct new features that are either absent or trivial in the raw dataset.</p><p>Note that it is not always necessary to perform feature engineering or feature selection. Whether to perform feature selection and construction depends on the data you have or collected, what kind of ML algorithm you have picked, and the objective of the experiment itself.</p><p>In this chapter, we have described all of the three steps in detail with practical Spark examples. In the next chapter, we will describe in detail some practical examples of supervised and unsupervised learning using two machine learning APIs: Spark MLlib and Spark ML.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch05"></a>Chapter 5.   Supervised and Unsupervised Learning by Examples  </h2></div></div></div><p>In <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Machine Learning Best Practices</em></span> readers, learned some theoretical underpinnings of basic machine learning techniques. Whereas, <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Understanding the Problem by Understanding the Data,</em></span> describes the basic data manipulation using Spark's APIs such as RDD, DataFrame, and Datasets. <a class="link" href="#" linkend="ch04">Chapter 4</a>, <span class="emphasis"><em>Extracting Knowledge through Feature Engineering</em></span>, on the other hand, describes feature engineering from both the theoretical and practical point of view. However, in this chapter, the reader will learn the practical know-how needed quickly and powerfully to apply supervised and unsupervised techniques on the available data to the new problems through some widely used examples based on the understandings from the previous chapters. These examples we are talking about will be demonstrated from the Spark perspective. In a nutshell, the following topics will be covered throughout this chapter:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Machine learning classes</p></li><li style="list-style-type: disc"><p>Supervised learning</p></li><li style="list-style-type: disc"><p>Unsupervised learning</p></li><li style="list-style-type: disc"><p>Recommender system</p></li><li style="list-style-type: disc"><p>Advanced learning and generalization</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec36"></a>Machine learning classes</h2></div></div><hr /></div><p>As stated in <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Introduction to Data Analytics with Spark</em></span> and <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Machine</em></span>
<span class="emphasis"><em> Learning Best Practices</em></span>, machine learning techniques can be categorized further into three major classes of algorithms: supervised learning, unsupervised learning, and the recommender system. Where classification and regression algorithms are widely used in the supervised learning application development, clustering, on the other hand, falls in the category of unsupervised learning. In this section, we will describe some examples of the supervised learning technique.</p><p>Then we will provide some example of the same example presented using Spark. On the other hand, an example of the clustering technique will be discussed in the section: <span class="emphasis"><em>Unsupervised learning</em></span>, where a regression technique often models the past relationship between variables to predict their future changes (up or down). Here we show two real-life examples of classification and regression algorithms respectively. In contrast, a classification technique takes a set of data with known labels and learns how to label new records based on that information:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Example (classification)</strong></span>: Gmail uses a machine learning technique called classification to designate if an e-mail is spam or not, based on the data of an e-mail.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Example (regression)</strong></span>: As an example, suppose you are an online currency trader and you work on Forex or Fortrade. Right now you have two currency pairs in mind to buy or sell say: GBP/USD and USD/JPY. If you look at these two pairs carefully, USD is a common in these two pairs. Now if you look at the historical prices of USD, GBP, or JPY you can predict the future outcome of whether you should open the trade in buy or sell. These types of problems can be resolved with supervised learning techniques using regression analysis:</p><div class="mediaobject"><img src="graphics/image_05_001.jpg" /><div class="caption"><p>Figure 1: Classification, clustering, and collaborative filtering-the big picture</p></div></div></li></ul></div><p>On the other hand, clustering and dimensionality reduction are commonly used for unsupervised learning. Here are some examples:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Example (clustering)</strong></span>: Google News uses a technique called clustering to group news articles into different categories, based on title and content. Clustering algorithms discover groupings that occur in collections of data.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Example (collaborative filtering)</strong></span>: The collaborative filtering algorithm is often used in the recommendation system development. Renowned companies such as Amazon and Netflix use a machine learning technique called collaborative filtering, to determine which products users will like based on their history and similarity to other users.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Example (dimensionality reduction)</strong></span>: Dimensionality reduction is often used to make the available dataset that is high dimensional. For example, suppose you have an image of size 2048x1920, and you would like to reduce the dimension to 1080x720 without sacrificing the quality much. In this case, popular algorithms such as <span class="strong"><strong>Principal Component Analysis</strong></span> (<span class="strong"><strong>PCA</strong></span>) or <span class="strong"><strong>Singular Value Decomposition</strong></span> (<span class="strong"><strong>SVD</strong></span>) can be used although you can also implement the SVD to implement the PCA. This is why SVD is more widely used.</p></li></ul></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec63"></a>Supervised learning</h3></div></div></div><p>As already stated, a supervised learning application makes predictions based on a set of examples and the goal is to learn general rules that map inputs to outputs aligning with the real world. For example, a dataset for spam filtering usually contains spam messages as well as non-spam messages. Consequently, we could know which messages in the training set are spams or non-spam. Therefore, supervised learning is the machine learning technique of inferring a function from the labeled training data. The following steps are involved in supervised learning tasks:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Train the ML model with the training dataset</p></li><li style="list-style-type: disc"><p>Use the test dataset to test the model performance</p></li></ul></div><p>Therefore, the dataset for training the ML model, in this case, is labeled with the value of interest and a supervised learning algorithm looks for patterns in those value labels. After the algorithm has found the required patterns, those patterns can be used to make predictions for unlabeled test data.</p><p>A typical use of the supervised learning is diverse and commonly used in the bioinformatics, cheminformatics, database marketing, handwriting recognition, information retrieval, object recognition in computer vision, optical character recognition, spam detection, pattern recognition, speech recognition, and so on, and in these applications mostly the classification technique is used. On the other hand, supervised learning is a special case of downward causation in biological systems.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip24"></a>Tip</h3><p>More on how the supervised learning technique works from the theoretical perspective can be found on these books: Vapnik, V. N. <span class="emphasis"><em>The Nature of Statistical Learning Theory (2nd Ed.)</em></span>, Springer Verlag, 2000; and Mehryar M., Afshin R. Ameet T. (2012) Foundations of Machine Learning, The MIT Press ISBN 9780262018258.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl3sec61"></a>Supervised learning example</h4></div></div></div><p>Classification is a family of supervised machine learning algorithms that designate input as belonging to one of the several pre-defined classes. Some common use cases for classification include:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Credit card fraud detection</p></li><li style="list-style-type: disc"><p>E-mail spam detection</p></li></ul></div><p>Classification data is labeled, for example, as spam/non-spam or fraud/non-fraud. Machine learning assigns a label or class to new data. You classify something based on pre-determined features. Features are the <span class="emphasis"><em>if questions</em></span> that you ask. The label is the answer to those questions. For example, if an object walks, swims, and quacks like a duck, then the label would be <span class="emphasis"><em>duck</em></span>. Or suppose for a flight is delayed on to be a departure or arrival by more than say 1 hour, it would be a delay; otherwise not a delay.</p></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec37"></a>Supervised learning with Spark - an example</h2></div></div><hr /></div><p>We will demonstrate an example by analyzing an air-flight delay. The dataset named <code class="literal">On_Time_Performance_2016_1.csv</code> from the United Department of Transportation website at <a class="ulink" href="http://www.transtats.bts.gov/" target="_blank">http://www.transtats.bts.gov/</a> will be used.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec64"></a>Air-flight delay analysis using Spark</h3></div></div></div><p>We are using flight information for 2016. For each flight, we have the following information presented in <span class="emphasis"><em>Table 1</em></span> (we have presented only a few fields out of 444,827 rows and 110 columns as of May 17, 2016):</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Data field</strong></span></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Description</strong></span></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Example value</strong></span></p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">DayofMonth</code></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Day of month</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>2</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">DayOfWeek</code></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Day of week</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>5</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">TailNum</code></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Tail number for the plane</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>N505NK</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">FlightNum</code></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Flight number</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>48</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">AirlineID</code></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Airline ID</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>19805</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">OriginAirportID</code></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Origin airport ID</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>JFK</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">DestAirportID</code></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Destination airport ID</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>LAX</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">Dest</code></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Destination airport code</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>1424</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">CRSDepTime</code></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Schedule departure time</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>10:00</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">DepTime</code></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Actual departure time</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>10:30</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">DepDelayMinutes</code></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Departure delay in minutes</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>30</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">CRSArrTime</code></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Schedule arrival time</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>22:45</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">ArrTime</code></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Actual arrival time</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>23:45</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">ArrDelayMinutes</code></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Arrival delay in minutes</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>60</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">CRSElapsedTime</code></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Elapsed time</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>825</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; ">
<p><code class="literal">Distance</code></p>
</td><td style="border-right: 0.5pt solid ; ">
<p>Total distance</p>
</td><td style="">
<p>6200</p>
</td></tr></tbody></table></div><p>Table 1: Sample data from the "On Time On Time Performance 2016_1" dataset</p><p>In this scenario, we will build a tree to predict the label of delayed or not delayed based on the following features shown in the figure, which is small snapshot of an air flight dataset. Here <code class="literal">ArrDelayMinutes</code> is 113 which should be classified in delayed (1.0) and other rows are less than 60 minutes so the label should be 0.0 (not delayed). From this dataset, we will do some operation such as feature extraction, transformation, and selection. <span class="emphasis"><em>Table 2</em></span> shows the top five rows related to the features we will be considering for this example as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Label</strong></span>: Delayed and not delayed - delayed if delay &gt;60 minutes</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Features</strong></span>: {<code class="literal">DayOfMonth</code>, <code class="literal">WeekOfday</code>, <code class="literal">CRSdeptime</code>, <code class="literal">CRSarrtime</code>, <code class="literal">Carrier</code>, <code class="literal">CRSelapsedtime</code>, <code class="literal">Origin</code>, <code class="literal">Dest</code>, <code class="literal">ArrDelayMinutes</code>}</p><div class="mediaobject"><img src="graphics/image_05_002.jpg" /><div class="caption"><p>Figure 2: Selected feature for air-flight delay prediction</p></div></div></li></ul></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl3sec62"></a>Loading and parsing the Dataset</h4></div></div></div><p>Before performing the feature extraction, we need to load and parse the dataset. This step also includes: loading packages and related dependencies, reading the dataset as a DataFrame, making the POJO or Bean class, and adding the new label column based on requirements.</p><p><span class="strong"><strong>Step 1: Load required packages and dependencies</strong></span></p><p>For reading csv files, we used the csv reader provided by the Databricks:</p><pre class="programlisting">import org.apache.log4j.Level; 
import org.apache.log4j.Logger; 
import org.apache.spark.api.java.JavaRDD; 
import org.apache.spark.api.java.function.Function; 
import org.apache.spark.ml.Pipeline; 
import org.apache.spark.ml.PipelineModel; 
import org.apache.spark.ml.PipelineStage; 
import org.apache.spark.ml.classification.DecisionTreeClassificationModel; 
import org.apache.spark.ml.classification.DecisionTreeClassifier; 
import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator; 
import org.apache.spark.ml.feature.IndexToString; 
import org.apache.spark.ml.feature.LabeledPoint; 
import org.apache.spark.ml.feature.StringIndexer; 
import org.apache.spark.ml.feature.StringIndexerModel; 
import org.apache.spark.ml.feature.VectorAssembler; 
import org.apache.spark.ml.feature.VectorIndexer; 
import org.apache.spark.ml.feature.VectorIndexerModel; 
import org.apache.spark.ml.linalg.Vector; 
import org.apache.spark.rdd.RDD; 
import org.apache.spark.sql.Dataset; 
import org.apache.spark.sql.Row; 
import org.apache.spark.sql.SparkSession; 
import scala.Tuple2; 
</pre><p><span class="strong"><strong>Step 2: Create the Spark session</strong></span></p><p>Here is the code to create a Spark session:</p><pre class="programlisting">  static SparkSession spark = SparkSession 
      .builder() 
      .appName("JavaLDAExample") 
      .master("local[*]") 
      .config("spark.sql.warehouse.dir", "E:/Exp/") 
      .getOrCreate(); 
</pre><p><span class="strong"><strong>Step 3: Read and parse the csv file using Dataset</strong></span></p><p>This dataset contains many columns that we will not include as a feature in this example. So we will select from the DataFrame only the features that we have mentioned previously. This DataFrame output has already been shown in <span class="emphasis"><em>Figure 2</em></span>:</p><pre class="programlisting">String csvFile = "input/On_Time_On_Time_Performance_2016_1.csv"; 
Dataset&lt;Row&gt; df = spark.read().format("com.databricks.spark.csv").option("header", "true").load(csvFile);  
RDD&lt;Tuple2&lt;String, String&gt;&gt; distFile = spark.sparkContext().wholeTextFiles("input/test/*.txt", 2); 
JavaRDD&lt;Tuple2&lt;String, String&gt;&gt; distFile2 = distFile.toJavaRDD(); 
JavaRDD&lt;Row&gt; rowRDD = df.toJavaRDD(); 
Dataset&lt;Row&gt; newDF = df.select(df.col("ArrDelayMinutes"), 
df.col("DayofMonth"), df.col("DayOfWeek"), 
df.col("CRSDepTime"), df.col("CRSArrTime"), df.col("Carrier"), 
df.col("CRSElapsedTime"), df.col("Origin"), df.col("Dest")); 
newDF.show(5); 
</pre><p>Here is a the output of top 5 rows:</p><div class="mediaobject"><img src="graphics/image_05_003.jpg" /></div><p><span class="strong"><strong>Step 4: Making a POJO or Bean class</strong></span></p><p>The POJO class we have developed is called <code class="literal">Flight</code> where the required features and label field will be defined with the corresponding setter and getter.</p><pre class="programlisting">public class Flight implements Serializable { 
  double label; 
  double monthDay; 
  double weekDay; 
  double crsdeptime; 
  double crsarrtime; 
  String carrier; 
  double crselapsedtime; 
  String origin; 
  String dest; 
 
public Flight(double label, double monthDay, double weekDay, double crsdeptime, double crsarrtime, String carrier, 
      double crselapsedtime, String origin, String dest) { 
    super(); 
    this.label = label; 
    this.monthDay = monthDay; 
    this.weekDay = weekDay; 
    this.crsdeptime = crsdeptime; 
    this.crsarrtime = crsarrtime; 
    this.carrier = carrier; 
    this.crselapsedtime = crselapsedtime; 
    this.origin = origin; 
    this.dest = dest; 
  } 
  public double getLabel() { 
    return label; 
  } 
  public void setLabel(double label) { 
    this.label = label; 
  } 
  public double getMonthDay() { 
    return monthDay; 
  } 
  public void setMonthDay(double monthDay) { 
    this.monthDay = monthDay; 
  } 
  public double getWeekDay() { 
    return weekDay; 
  } 
  public void setWeekDay(double weekDay) { 
    this.weekDay = weekDay; 
  } 
  public double getCrsdeptime() { 
    return crsdeptime; 
  } 
  public void setCrsdeptime(double crsdeptime) { 
    this.crsdeptime = crsdeptime; 
  } 
  public double getCrsarrtime() { 
    return crsarrtime; 
  } 
  public void setCrsarrtime(double crsarrtime) { 
    this.crsarrtime = crsarrtime; 
  } 
  public String getCarrier() { 
    return carrier; 
  } 
  public void setCarrier(String carrier) { 
    this.carrier = carrier; 
  } 
  public double getCrselapsedtime() { 
    return crselapsedtime; 
  } 
  public void setCrselapsedtime(double crselapsedtime) { 
    this.crselapsedtime = crselapsedtime; 
  } 
  public String getOrigin() { 
    return origin; 
  } 
  public void setOrigin(String origin) { 
    this.origin = origin; 
  } 
  public String getDest() { 
    return dest; 
  } 
  public void setDest(String dest) { 
    this.dest = dest; 
  } 
  @Override 
  public String toString() { 
    return "Flight [label=" + label + ", monthDay=" + monthDay + ", weekDay="
       + weekDay + ", crsdeptime=" 
        + crsdeptime + ", crsarrtime=" + crsarrtime + ", carrier=" + 
      carrier + ", crselapsedtime=" 
        + crselapsedtime + ", origin=" + origin + ", dest=" +
       dest + "]"; 
  } 
</pre><p>We believe the preceding class is self-explanatory, which is used for setting and getting the feature values from the original dataset.</p><p><span class="strong"><strong>Step 5: Adding the new label column based on the delay column</strong></span></p><p>If the delay is greater than 40 then the label should be 1 otherwise it should be 0. Create a new Dataset using the Flight bean class. This dataset can contain an empty string for the <code class="literal">ArrDelayMinutes</code> column. So before mapping we filtered the rows containing the empty string from the dataset:</p><pre class="programlisting">JavaRDD&lt;Flight&gt; flightsRDD = newDF.toJavaRDD().filter(new Function&lt;Row, Boolean&gt;() { 
          @Override 
          public Boolean call(Row v1) throws Exception { 
            return !v1.getString(0).isEmpty(); 
          } 
        }).map(new Function&lt;Row, Flight&gt;() { 
          @Override 
          public Flight call(Row r) throws Exception { 
            double label; 
            double delay = Double.parseDouble(r.getString(0)); 
            if (delay &gt; 60) 
              label = 1.0; 
else 
      label = 0.0; 
double monthday = Double.parseDouble(r.getString(1)) - 1; 
double weekday = Double.parseDouble(r.getString(2)) - 1; 
double crsdeptime = Double.parseDouble(r.getString(3)); 
double crsarrtime = Double.parseDouble(r.getString(4)); 
String carrier = r.getString(5); 
double crselapsedtime1 = Double.parseDouble(r.getString(6)); 
String origin = r.getString(7); 
String dest = r.getString(8); 
Flight flight = new Flight(label, monthday, weekday,crsdeptime, crsarrtime, carrier,crselapsedtime1, origin, dest); 
        return flight; 
    }}); 
</pre><p>Now create a new Dataest from the RDD we created above as follows:</p><pre class="programlisting">Dataset&lt;Row&gt; flightDelayData = spark.sqlContext().createDataFrame(flightsRDD,Flight.class); 
flightDelayData.printSchema(); 
</pre><p>Now show the top 5 rows from the data frame <code class="literal">flightDelayData</code> in following <span class="emphasis"><em>Figure 3</em></span>:</p><pre class="programlisting">flightDelayData.show(5); 
</pre><p>[Output:]</p><div class="mediaobject"><img src="graphics/image_05_004.jpg" /><div class="caption"><p>Figure 3:The DataFrame showing the new label column</p></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl3sec63"></a>Feature extraction</h4></div></div></div><p>For extracting the feature, we have to make numerical values and if there are any text values, then we have to make a labeled vector for applying a machine learning algorithm.</p><p><span class="strong"><strong>Step 1 :Transformation towards feature extraction</strong></span></p><p>Here we will transform the columns containing text into double values columns. Here we use <code class="literal">StringIndexer</code> for making a unique index for each unique text:</p><pre class="programlisting">StringIndexer carrierIndexer = new StringIndexer().setInputCol("carrier").setOutputCol("carrierIndex"); 
Dataset&lt;Row&gt; carrierIndexed = carrierIndexer.fit(flightDelayData).transform(flightDelayData); 
StringIndexer originIndexer = new StringIndexer().setInputCol("origin").setOutputCol("originIndex"); 
Dataset&lt;Row&gt; originIndexed = originIndexer.fit(carrierIndexed).transform(carrierIndexed); 
StringIndexer destIndexer = new StringIndexer().setInputCol("dest").setOutputCol("destIndex"); 
Dataset&lt;Row&gt; destIndexed = destIndexer.fit(originIndexed).transform(originIndexed); 
destIndexed.show(5); 
</pre><p>[Output]:</p><div class="mediaobject"><img src="graphics/image_05_005.jpg" /><div class="caption"><p>Figure 4: Uunique indices for each unique text</p></div></div><p><span class="strong"><strong>Step 2: Making the feature vectors using the vector assembler</strong></span></p><p>Make the feature vector with the vector assembler and transform it to the labelled vector for applying the machine learning algorithm (decision tree). Note, here we used the decision tree to show just an example since it shows better classification accuracies. Based on the algorithm and model selection and tuning, you will be further able to explore and use other classifiers:</p><pre class="programlisting">VectorAssembler assembler = new VectorAssembler().setInputCols( 
        new String[] { "monthDay", "weekDay", "crsdeptime", 
            "crsarrtime", "carrierIndex", "crselapsedtime", 
            "originIndex", "destIndex" }).setOutputCol( 
        "assembeledVector"); 
</pre><p>Now transform the assembler into a Dataset of row as follows:</p><pre class="programlisting">Dataset&lt;Row&gt; assembledFeatures = assembler.transform(destIndexed); 
</pre><p>Now convert the Dataset into <code class="literal">JavaRDD</code> for making the feature vectors as follows:</p><pre class="programlisting">JavaRDD&lt;Row&gt; rescaledRDD = assembledFeatures.select("label","assembeledVector").toJavaRDD(); 
</pre><p>Map the RDD for <code class="literal">LabeledPoint</code> as follows:</p><pre class="programlisting">JavaRDD&lt;LabeledPoint&gt; mlData = rescaledRDD.map(new Function&lt;Row, LabeledPoint&gt;() { 
          @Override 
          public LabeledPoint call(Row row) throws Exception { 
            double label = row.getDouble(0); 
            Vector v = row.getAs(1); 
            return new LabeledPoint(label, v); 
          } 
        }); 
</pre><p>Now print the first five values as follows:</p><pre class="programlisting">System.out.println(mlData.take(5));  
</pre><p>[Output]:</p><div class="mediaobject"><img src="graphics/image_05_006.jpg" /><div class="caption"><p>Figure 5: The corresponding assembled vectors</p></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl3sec64"></a>Preparing the training and testing set</h4></div></div></div><p>Here we will prepare the training dataset from the dataset of the labeled vector. Initially, we will make a training set where 15% of records will be non-delayed and 85% will be delayed records. Finally, the training and testing dataset will be prepared as 70% and 30% respectively.</p><p><span class="strong"><strong>Step 1: Make training and test set from the whole Dataset</strong></span></p><p>First, create a new RDD by filtering the RDD based on the labels (that is, 1 and 0) we created previously as follows:</p><pre class="programlisting">JavaRDD&lt;LabeledPoint&gt; splitedData0 = mlData.filter(new Function&lt;LabeledPoint, Boolean&gt;() { 
          @Override 
          public Boolean call(LabeledPoint r) throws Exception { 
              return r.label() == 0; 
          } 
        }).randomSplit(new double[] { 0.85, 0.15 })[1]; 
 
    JavaRDD&lt;LabeledPoint&gt; splitedData1 = mlData.filter(new Function&lt;LabeledPoint, Boolean&gt;() { 
          @Override 
          public Boolean call(LabeledPoint r) throws Exception { 
            return r.label() == 1; 
          } 
        }); 
   
    JavaRDD&lt;LabeledPoint&gt; splitedData2 = splitedData1.union(splitedData0); 
    System.out.println(splitedData2.take(1)); 
</pre><p>Now union the two RDDs using the <code class="literal">union()</code> method as follows:</p><pre class="programlisting">JavaRDD&lt;LabeledPoint&gt; splitedData2 = splitedData1.union(splitedData0); 
System.out.println(splitedData2.take(1)); 
</pre><p>Now further convert the combined RDD into Dataset of Row as follows (max categories is set to be 4):</p><pre class="programlisting">Dataset&lt;Row&gt; data = spark.sqlContext().createDataFrame(splitedData2, LabeledPoint.class); 
data.show(100); 
</pre><p>Now we need to do the vector indexer for the categorical variables as follows:</p><pre class="programlisting">VectorIndexerModel featureIndexer = new VectorIndexer() 
          .setInputCol("features") 
          .setOutputCol("indexedFeatures") 
          .setMaxCategories(4) 
          .fit(data); 
</pre><p>Now that we have the feature indexer using the <code class="literal">VectorIndexerModel</code> estimator. Now the next task is to do the string indexing using the <code class="literal">StringIndexerModel</code> estimator as follows:</p><pre class="programlisting">StringIndexerModel labelIndexer = new StringIndexer() 
          .setInputCol("label") 
          .setOutputCol("indexedLabel") 
          .fit(data); 
</pre><p>Finally, split the Dataset of Row into training and test (70% and 30% respectively but you should adjust the values based on your requirements) set as follows:</p><pre class="programlisting">Dataset&lt;Row&gt;[] splits = data.randomSplit(new double[]{0.7, 0.3}); 
Dataset&lt;Row&gt; trainingData = splits[0]; 
Dataset&lt;Row&gt; testData = splits[1]; 
</pre><p>Well done! Now our dataset is ready to train the model, right? For the time being, we will naively select a classifier to say let's use the decision tree classifier to solve our purpose. You can try this with other multiclass classifiers based on examples provided in <a class="link" href="#" linkend="ch06">Chapter 6</a>, <span class="emphasis"><em>Building Scalable Machine Learning Pipelines</em></span>, <a class="link" href="#" linkend="ch07">Chapter 7</a>, <span class="emphasis"><em>Tuning Machine Learning Models</em></span>, and <a class="link" href="#" linkend="ch08">Chapter 8</a>, <span class="emphasis"><em>Adapting Your Machine Learning Models</em></span>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl3sec65"></a>Training the model</h4></div></div></div><p>As shown in <span class="emphasis"><em>Figure 2</em></span>, training and test data will be collected from the raw data. After the feature engineering process has been done, the RDD of feature vectors with labels or ratings will be used next to be processed by the classification algorithm before building the predictive model (as shown in <span class="emphasis"><em>Figure 6</em></span>) and at the end the test data will be used for testing the model performance:</p><div class="mediaobject"><img src="graphics/image_05_007.jpg" /><div class="caption"><p>Figure 6: Supervised learning using Spark</p></div></div><p>Next, we prepare the values for the parameters that will be required for the decision tree. You might have wondered why we are talking about the decision tree. The reason is simple since using the decision tree (that is, the <span class="strong"><strong>binary decision tree</strong></span>)we observed better prediction accuracy compared to the Naive Bayes approaches. Refer to <span class="emphasis"><em>Table 2</em></span> which describes the categorical features and their significance as follows:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Categorical features</strong></span></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Mapping</strong></span></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Significant</strong></span></p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>categoricalFeaturesInfo</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>0 -&gt; 31</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Specifies that the feature index 0 (which represents the day of the month) has 31 categories [values {0, ..., 31}]</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>categoricalFeaturesInfo</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>1 -&gt; 7</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Represents days of the week, and specifies that the feature index 1 has seven categories</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; ">
<p>Carrier</p>
</td><td style="border-right: 0.5pt solid ; ">
<p>0 -&gt; N</p>
</td><td style="">
<p>N signifies the numbers from 0 to up to the number of distinct carriers</p>
</td></tr></tbody></table></div><p>Table 2: Categorical features and their significance</p><p>Now we will describe the approach of the decision tree construction in brief. We will use the CategoricalFeaturesInfo that specifies which features are categorical and how many categorical values each of those features can take during the tree construction process. This is given as a map from the feature index to the number of categories for that feature.</p><p>However, the model is trained by making associations between the input features and the labeled output associated with those features. We train the model using the <code class="literal">DecisionTreeClassifier</code> method which returns a <code class="literal">DecisionTreeModel</code> eventually as shown in <span class="emphasis"><em>Figure 7</em></span>. The detailed source code for constructing the tree will be shown later in this section:</p><div class="mediaobject"><img src="graphics/B05243_05_08-1024x574.jpg" /><div class="caption"><p>Figure 7: The binary decision tree generated for the air-flight delay analysis (partially shown)</p></div></div><p><span class="strong"><strong>Step 1: Train the decision tree model</strong></span></p><p>To train the decision tree classifier model, we need to have the necessary labels and features:</p><pre class="programlisting">DecisionTreeClassifier dt = new DecisionTreeClassifier() 
      .setLabelCol("indexedLabel") 
      .setFeaturesCol("indexedFeatures"); 
</pre><p><span class="strong"><strong>Step 2: Convert the indexed labels back to original labels</strong></span></p><p>To create a decision tree pipeline, we need to have the original labels apart from the indexed labels. So, let's do it as follows:</p><pre class="programlisting">IndexToString labelConverter = new IndexToString() 
      .setInputCol("prediction") 
      .setOutputCol("predictedLabel")         
        .setLabels(labelIndexer.labels());  
</pre><p><span class="strong"><strong>Step 3: Chain the indexer and tree in a single pipeline</strong></span></p><p>Create a new pipeline where the stages are as follows: <code class="literal">labelIndexer</code>, <code class="literal">featureIndexer</code>, <code class="literal">dt</code>, <code class="literal">labelConverter</code> as follows:</p><pre class="programlisting">Pipeline pipeline = new Pipeline() 
      .setStages(new PipelineStage[]{labelIndexer,  
        featureIndexer, dt, labelConverter}); 
</pre><p>Now fit the pipeline using the training set we created in <span class="emphasis"><em>Step 8</em></span> as follows:</p><pre class="programlisting">PipelineModel model = pipeline.fit(trainingData); 
</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl3sec66"></a>Testing the model</h4></div></div></div><p>Here we will test the models as shown in the following steps:</p><p><span class="strong"><strong>Step 1: Make the prediction on the test dataset</strong></span></p><p>Make the prediction on the test set by transforming the <code class="literal">PipelineModel</code> and show the performance parameters as follows:</p><pre class="programlisting">Dataset&lt;Row&gt; predictions = model.transform(testData); 
predictions.select("predictedLabel", "label", "features").show(5); 
</pre><p><span class="strong"><strong>Step 2: Evaluate the model</strong></span></p><p>Evaluate the model by the multiclass classification evaluator and print the accuracy and test error as follows:</p><pre class="programlisting">MulticlassClassificationEvaluator evaluator = new MulticlassClassificationEvaluator() 
      .setLabelCol("indexedLabel") 
      .setPredictionCol("prediction") 
      .setMetricName("accuracy"); 
    double accuracy = evaluator.evaluate(predictions); 
    System.out.println("accuracy: "+accuracy); 
    System.out.println("Test Error = " + (1.0 - accuracy)); 
</pre><p>The preceding code segment produces the classification accuracy and test error as follows:</p><pre class="programlisting">Accuracy: 0.7540472721385786 
Test Error = 0.24595272786142142 
</pre><p>Please note that since we randomly split the dataset into training and testing, you might get a different result. The classification accuracy is 75.40%, which is not good, we believe.</p><p>However, now it's your turn to use a different classifier and tune before deploying the model. A more details discussion will be carried out in <a class="link" href="#" linkend="ch07">Chapter 7</a>, <span class="emphasis"><em>Tuning Machine Learning Models,</em></span> regarding tuning the ML models.</p><p><span class="strong"><strong>Step 3: Print the decision tree</strong></span></p><p>Here is the code to print the decision tree:</p><pre class="programlisting">DecisionTreeClassificationModel treeModel = 
      (DecisionTreeClassificationModel) (model.stages()[2]); 
System.out.println("Learned classification tree model:\n" + treeModel.toDebugString()); 
</pre><p>This code segment produces a decision tree as shown in <span class="emphasis"><em>Figure 7</em></span>.</p><p><span class="strong"><strong>Step 4: Stop the Spark session</strong></span></p><p>Stop the Spark session using the <code class="literal">stop()</code> method of Spark as follows:</p><pre class="programlisting">spark.stop();</pre><p>This is a good practice that you initiate a Spark session and close or stop it properly to avoid the memory leak in your applications.</p></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec38"></a>Unsupervised learning</h2></div></div><hr /></div><p>In unsupervised learning, data points have no labels related to them; therefore, we need to put labels on them algorithmically. In other words, the correct classes of the training dataset in unsupervised learning are unknown.</p><p>Consequently, classes have to be inferred from the unstructured datasets which implies that the goal of an unsupervised learning algorithm is to pre-process the data in some structured ways by describing its structure. The main objective of the unsupervised learning algorithms or techniques is to explore the unknown patterns of the input data that are mostly unlabeled. In this way, it is closely related to the problem of density estimation used in theoretical and applied statistics.</p><p>Unsupervised learning, however, also comprehends many other techniques to summarize and explain the key features of the data including exploratory data analysis for finding these hidden patterns, even grouping the data points or features and applying the unsupervised learning technique based on data mining methods for the data pre-processing.</p><p>To overcome this obstacle in unsupervised learning, clustering techniques are used typically to group the unlabeled samples based on certain similarity measures, mining hidden patterns towards feature learning.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip25"></a>Tip</h3><p>For in-depth theoretical knowledge, how the unsupervised algorithms work, please refer to these three books: Bousquet, O.; von Luxburg, U.; Raetsch, G., eds. (2004). <span class="emphasis"><em>Advanced Lectures on Machine Learning</em></span>. Springer-Verlag. ISBN 978-3540231226. Or Duda, Richard O.; Hart, Peter E.; Stork, David G. (2001). <span class="emphasis"><em>Unsupervised Learning and Clustering</em></span>. <span class="emphasis"><em>Pattern Classification (2nd Ed.)</em></span>. Wiley. ISBN 0-471-05669-3 and Jordan, Michael I.; Bishop, Christopher M. (2004). <span class="emphasis"><em>Neural Networks</em></span>. In Allen B. Tucker. Computer <span class="emphasis"><em>Science Handbook, Second Edition (Section VII: Intelligent Systems)</em></span>. Boca Raton, FL: Chapman &amp; Hall/CRC Press LLC. ISBN 1-58488-360-X.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec65"></a>Unsupervised learning example</h3></div></div></div><p>In clustering, an algorithm groups objects into categories by analyzing similarities between input examples where similar objects or features are clustered and marked using circles around them.</p><p>Clustering uses include: <span class="strong"><strong>Search results grouping</strong></span> such as grouping of customers, <span class="strong"><strong>anomaly detection</strong></span> for suspicious pattern finding, <span class="strong"><strong>text categorization</strong></span> for finding useful patterns in the tests, <span class="strong"><strong>social network analysis</strong></span> for finding coherent groups, <span class="strong"><strong>data center computing clusters</strong></span> for finding a way of putting related computers together to improve performance, <span class="strong"><strong>astronomic data analysis</strong></span> for galaxy formation, and <span class="strong"><strong>real estate data analysis</strong></span> for identifying neighborhoods based on similar features. Moreover, clustering uses unsupervised algorithms, which do not have the outputs in advance.</p><p>Clustering using the K-means algorithm begins by initializing all the coordinates to centroids. Note that Spark also supports other clustering algorithms such as Gaussian mixture, <span class="strong"><strong>Power Iteration Clustering </strong></span>(<span class="strong"><strong>PIC</strong></span>), <span class="strong"><strong>Latent Dirichlet Allocation </strong></span>(<span class="strong"><strong>LDA</strong></span>), Bisecting k-means, and Streaming k-means. Whereas, the Gaussian mixture is mainly used for expectation minimization as an optimization algorithm, the LDA, on the other hand, is used for the document classification and clustering. PIC is used for the clustering vertices of a graph given pairwise similarities as edge properties. Bisecting K-means is faster than the regular K-means, but it will generally produce a different clustering. Therefore, to keep the discussion simpler we will use the K-means algorithm for our purposes.</p><p>Interested readers should refer the Spark ML and Spark MLlib based clustering techniques at <a class="ulink" href="https://spark.apache.org/docs/latest/ml-clustering.html" target="_blank">https://spark.apache.org/docs/latest/ml-clustering.html</a> and <a class="ulink" href="https://spark.apache.org/docs/latest/mllib-clustering.html" target="_blank">https://spark.apache.org/docs/latest/mllib-clustering.html</a> web pages respectively to get more insights. With every pass of the algorithm, each point is assigned to its nearest centroid based on some distance metric, usually the <span class="strong"><strong>Euclidean distance</strong></span>.</p><p>Note that there are other ways to calculate the distance, for example, the <span class="strong"><strong>Chebyshev distance</strong></span> is used to measure the distance by considering only the most significant dimensions. The <span class="strong"><strong>Hamming distance algorithm</strong></span> is used to identify the difference bit by bit of two strings. The <span class="strong"><strong>Mahalanobis distance</strong></span> is used to normalize the covariance matrix to make the distance metric scale-invariant.</p><p>The <span class="strong"><strong>Manhattan distance</strong></span> is used to measure the distance following only axis-aligned directions. The <span class="strong"><strong>Minkowski distance algorithm</strong></span> is used to make the Euclidean distance, Manhattan distance, and Chebyshev distance generalize. The <span class="strong"><strong>Haversine distance</strong></span> is used to measure the great-circle distances between two points on a sphere from their longitudes and latitudes. Considering these distance measuring algorithms, it is clear that the Euclidean distance algorithm would be the most appropriate to solve our problem.</p><p>The centroids are then updated to be the centers of all the points assigned to it in that pass. This repeats until there is a minimum change in the centers. The k-means algorithm is an iterative algorithm and works in two steps:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Cluster assignment step</strong></span>: This algorithm will go through each data point and, depending upon which centroid it is nearer to, it will be assigned that centroid and, in turn, the cluster it represents</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Move centroid step</strong></span>: This algorithm will take each centroid and move it to the mean of the data points in the cluster</p></li></ul></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl3sec67"></a>Unsupervised learning with Spark - an example</h4></div></div></div><p>We will use the <span class="emphasis"><em>Saratoga NY Homes</em></span> downloaded from the URL <a class="ulink" href="http://course1.winona.edu/bdeppa/Stat%20425/Datasets.html" target="_blank">http://course1.winona.edu/bdeppa/Stat%20425/Datasets.html</a> to demonstrate an example of clustering as an unsupervised learning technique using Spark in Java. The dataset contains several features as follows: Price, Lot Size, Waterfront, Age, Land Value, New Construct, Central Air, Fuel Type, Heat Type, Sewer Type, Living Area, Pct.College, Bedrooms, Fireplaces, Bathrooms, and the number of Rooms. However, among those columns, we have shown only some selected columns in <span class="emphasis"><em>Table 3</em></span>. Note that the original dataset was downloaded and later on converted into a corresponding text file as a tab delimiter:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /><col /><col /><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Price</strong></span></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Lot Size</strong></span></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Water Front</strong></span></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Age</strong></span></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Land Value</strong></span></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Rooms</strong></span></p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>132500</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>0.09</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>0</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>42</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>5000</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>5</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>181115</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>0.92</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>0</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>0</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>22300</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>6</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>109000</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>0.19</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>0</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>133</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>7300</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>8</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>155000</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>0.41</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>0</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>13</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>18700</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>5</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>86060</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>0.11</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>0</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>0</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>15000</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>3</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>120000</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>0.68</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>0</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>31</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>14000</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>8</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>153000</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>0.4</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>0</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>33</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>23300</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>8</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>170000</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>1.21</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>0</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>23</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>146000</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>9</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>90000</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>0.83</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>0</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>36</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>222000</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>8</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>122900</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>1.94</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>0</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>4</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>212000</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>6</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; ">
<p>325000</p>
</td><td style="border-right: 0.5pt solid ; ">
<p>2.29</p>
</td><td style="border-right: 0.5pt solid ; ">
<p>0</p>
</td><td style="border-right: 0.5pt solid ; ">
<p>123</p>
</td><td style="border-right: 0.5pt solid ; ">
<p>126000</p>
</td><td style="">
<p>12</p>
</td></tr></tbody></table></div><p>Table 3: Sample data from the "Saratoga NY Homes" dataset</p><p>We further took only the first two features (that is, Price and Lot Size) using the Spark feature learning algorithm presented in the previous chapter for simplicity. Our target is to show an exploratory analysis based on these two features for possible neighborhoods of the house located in the same area. First, look at the basic scatter plot diagram based on the value in the dataset:</p><div class="mediaobject"><img src="graphics/image_05_009.jpg" /><div class="caption"><p>Figure 8: Cluster of the neighborhoods</p></div></div><p>It's clearly seen that there are four cluster on the plot marked as circles in <span class="emphasis"><em>Figure 8</em></span>. However, finding a number of clusters is a tricky task. Here, we have the advantage of visual inspection, which is not available for data on hyperplanes or multidimensional data. Now we need to find the same result using Spark. For simplicity, we will use the K-means clustering API of Spark. The use of raw data and finding feature vectors is shown in <span class="emphasis"><em>Figure 9</em></span>:</p><div class="mediaobject"><img src="graphics/image_05_010.jpg" /><div class="caption"><p>Figure 9: Unsupervised learning using Spark</p></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl3sec68"></a>K-means clustering of the neighborhood</h4></div></div></div><p>Before performing the feature extraction, we need to load and parse the Saratoga NY Homes dataset. This step also includes: loading packages and related dependencies, reading the dataset as RDD, model training and prediction, collecting the local parsed data, and comparing clustering.</p><p><span class="strong"><strong>Step 1: Import statistics and related classes</strong></span></p><p>Here is the code to import statistics and related classes:</p><pre class="programlisting">import java.io.Serializable; 
import java.util.List; 
import org.apache.spark.api.java.JavaRDD; 
import org.apache.spark.api.java.function.Function; 
import org.apache.spark.mllib.clustering.KMeans; 
import org.apache.spark.mllib.clustering.KMeansModel; 
import org.apache.spark.mllib.linalg.Vector; 
import org.apache.spark.mllib.linalg.Vectors; 
import org.apache.spark.rdd.RDD; 
import org.apache.spark.sql.SparkSession;  
</pre><p><span class="strong"><strong>Step 2: Create the Spark session</strong></span></p><p>Here is the code to create a Spark session:</p><pre class="programlisting">  static SparkSession spark = SparkSession 
      .builder().appName("JavaLDAExample") 
      .master("local[*]") 
      .config("spark.sql.warehouse.dir", "E:/Exp/") 
      .getOrCreate(); 
</pre><p><span class="strong"><strong>Step 3: Load the Saratoga NY Homes.txt</strong></span></p><p>Read, parse, and create RDDs from the dataset:</p><pre class="programlisting">RDD&lt;String&gt; data = spark.sparkContext().textFile("input/Saratoga_ NY_Homes.txt", 2); 
</pre><p><span class="strong"><strong>Step 4:Transform the data into an RDD of dense vectors</strong></span></p><p>If you follow the preceding step carefully, actually we have created the normal RDD. Therefore, that RDD has to be converted into the corresponding <code class="literal">JavaRDD</code> before mapping into a dense vector using Vector:</p><pre class="programlisting">JavaRDD&lt;Vector&gt; parsedData = data.toJavaRDD().map(new Function&lt;String, Vector&gt;() { 
      @Override 
      public Vector call(String s) throws Exception { 
        String[] sarray = s.split(","); 
        double[] values = new double[sarray.length]; 
        for (int i = 0; i &lt; sarray.length; i++) 
          values[i] = Double.parseDouble(sarray[i]); 
        return Vectors.dense(values); 
      } 
    });  
</pre><p><span class="strong"><strong>Step 5: Train the model</strong></span></p><p>Train the model by specifying four clusters and five iterations. Just refer the following code for doing that:</p><pre class="programlisting">int numClusters = 4; 
int numIterations = 10; 
int runs = 2; 
KMeansModel clusters = KMeans.train(parsedData.rdd(), numClusters, numIterations, runs , KMeans.K_MEANS_PARALLEL());  
Now estimate the cost to compute the clsuters as follows: 
double cost = clusters.computeCost(parsedData.rdd()); 
System.out.println("Cost: " + cost);  
</pre><p>You should receive the results as follows:</p><pre class="programlisting">Cost: 3.60148995801542E12   
</pre><p><span class="strong"><strong>Step 6: Show the cluster centers</strong></span></p><pre class="programlisting">Vector[] centers = clusters.clusterCenters(); 
System.out.println("Cluster Centers: "); 
for (Vector center : centers)  
{ 
  System.out.println(center); 
} 
</pre><p>The preceding code should produce the center of the clusters as follows:</p><pre class="programlisting">[545360.4081632652,0.9008163265306122,0.1020408163265306,21.73469387755102,111630.61224489794,0.061224489795918366,0.7551020408163265,2.3061224489795915,2.1632653061224487,2.714285714285714,2860.755102040816,59.346938775510196,3.510204081632653,1.1020408163265305,2.714285714285714,10.061224489795917] 
[134073.06845637583,0.3820000000000002,0.0026845637583892616,33.72617449664429,19230.76510067114,0.012080536912751677,0.22818791946308722,2.621476510067114,2.7234899328859057,2.6630872483221477,1332.9234899328858,52.86040268456375,2.7395973154362414,0.38120805369127514,1.4946308724832214,5.806711409395973] 
[218726.0625,0.5419711538461538,0.0,25.495192307692307,32579.647435897434,0.041666666666666664,0.3830128205128205,2.3205128205128203,2.4615384615384617,2.692307692307692,1862.3076923076922,57.4599358974359,3.3894230769230766,0.7019230769230769,2.032852564102564,7.44551282051282] 
[332859.0580645161,0.6369354838709671,0.025806451612903226,19.803225806451614,63188.06451612903,0.13870967741935483,0.6096774193548387,2.2225806451612904,2.2483870967741937,2.774193548387097,2378.4290322580646,57.66774193548387,3.6225806451612903,0.8516129032258064,2.479032258064516,8.719354838709677] 
</pre><p><span class="strong"><strong>Step 7: Evaluate the model error rate</strong></span></p><pre class="programlisting">double WSSSE = clusters.computeCost(parsedData.rdd()); 
System.out.println("Within Set Sum of Squared Errors = " + WSSSE); 
</pre><p>This should produce the result as follows:</p><pre class="programlisting">Within Set Sum of Squared Errors = 3.60148995801542E12 
</pre><p><span class="strong"><strong>Step 8: Predict the cluster for the second element</strong></span></p><pre class="programlisting">List&lt;Vector&gt; houses = parsedData.collect(); 
int prediction  = clusters.predict(houses.get(18)); 
System.out.println("Prediction: "+prediction);  
</pre><p>Output prediction: 0</p><p><span class="strong"><strong>Step 9: Stop the Spark session</strong></span></p><p>Stop the Spark session using the <code class="literal">stop()</code> method as follows:</p><pre class="programlisting">spark.stop(); 
</pre><p><span class="strong"><strong>Step 10: Cluster comparing</strong></span></p><p>Now let's compare the cluster assignments by k-means versus the ones we have done individually. The k-means algorithm gives the cluster IDs starting from 0. Once you inspect the data, you find out the following mapping between the A to D cluster IDs we gave versus K-means in Table 4:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Cluster name</strong></span></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Cluster number</strong></span></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Cluster assignment</strong></span></p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>A</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>3</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>A=&gt;3</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>B</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>1</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>B=&gt;1</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>C</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>0</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>C=&gt;0</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; ">
<p>D</p>
</td><td style="border-right: 0.5pt solid ; ">
<p>2</p>
</td><td style="">
<p>D=&gt;2</p>
</td></tr></tbody></table></div><p>Table 4: Cluster assignment for the neighbourhood K-means clustering example</p><p>Now, let's pick some of the data from different parts of the chart and predict which cluster it belongs to. Let's look at the house (say 1 as an example) data, which has a lot size of 876 square feet and is priced at $665K:</p><pre class="programlisting">int prediction  = clusters.predict(houses.get(18)); 
    System.out.println("Prediction: "+prediction); 
</pre><p>[Output] Prediction: 2</p><p>That means the house with the preceding properties falls in the cluster 2. You can test the prediction capability with more data of course. Let's do some neighborhood analysis to see what meaning these clusters carry. We can assume that most of the houses in cluster 3 are near downtown. The cluster 2 houses are on hilly terrain for example.</p><p>In this example, we dealt with a very small set of features; common sense and visual inspection would also lead us to the same conclusions. However, if you want to acquire more accuracy, of course, you should construct more meaningful features by not only considering only the lot size and the house price but other features like the number of rooms, house age, land value, heating type, and so on.</p><p>However, it would not be wise to include the <span class="emphasis"><em>Waterfront</em></span> as a meaningful feature since no house has a water garden in front of the house in this example. We will provide a detailed analysis towards the better accuracy of meaningful a prediction in next chapter, where we will demonstrate these considerations.</p><p>The beauty of the k-means algorithm is that it does the clustering on the data with an unlimited number of features. It is a great tool to use when you have a raw data and would like to know the patterns in that data. However, deciding pn the number of clusters prior to doing the experiment might not be successful but sometimes may lead to an overfitting problem or an underfitting problem.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip26"></a>Tip</h3><p>To overcome the aformentioned limitation of the K-means, we have some more robust algorithms like <span class="strong"><strong>Markov Chain Monte Carlo </strong></span>(<span class="strong"><strong>MCMC</strong></span> , see also <a class="ulink" href="https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo" target="_blank">https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo</a>) presented in Tribble, Seth D., <span class="emphasis"><em>Markov chain Monte Carlo algorithms using completely uniformly distributed driving sequences</em></span>, Diss. Stanford University, 2007. Moreover, a more technical discussion can be found at the URL <a class="ulink" href="http://www.autonlab.org/tutorials/kmeans11.pdf" target="_blank">http://www.autonlab.org/tutorials/kmeans11.pdf</a> too.</p></div></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec39"></a>Recommender system</h2></div></div><hr /></div><p>A recommender system is an original killer application which is a subclass of an information filtering system that looks to predict the rating or preference from the users that they usually provide to an item. The concept of recommender systems has become very common in recent years and has been subsequently applied in different applications. The most popular ones are probably products (for example, movies, music, books, research articles), news, search queries, social tags, and so on). Recommender systems can be typed into four categories as stated in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Machine Learning Best Practices.</em></span> These are shown in <span class="emphasis"><em>Figure 10</em></span>:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>The collaborative filtering system</strong></span>: This is the accumulation of a consumer's preferences and recommendations to other users based on likeness in behavioral patterns <span class="strong"><strong>Content-based systems</strong></span>: Here the supervised machine learning is used to persuade a classifier to distinguish between interesting and uninteresting items for the users</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Hybrid recommender systems</strong></span>: This is a recent research and hybrid approach (that is, combining collaborative filtering and content-based filtering)</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Knowledge-based systems</strong></span>: Here knowledge about users and products are used to understand what fulfils a user's requirements, using a perception tree, decision support systems, and case-based reasoning:</p><div class="mediaobject"><img src="graphics/image_05_011.jpg" /><div class="caption"><p>Figure 10: Hierarchy of the recommendation systems</p></div></div></li></ul></div><p>From the technical viewpoint, we can further categorize them as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The item <span class="strong"><strong>hierarchy</strong></span> is the weakest one where it is naively assuming that one item is correlated to another, for example, if you buy a printer, it is more likely that you will buy the ink. Previously this approach was used by <span class="strong"><strong>BestBuy</strong></span>
</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Attribute-based recommendation</strong></span>: Assumes that you like action movies starring Sylvester Stallone, therefore, you might be like the Rambo series. <span class="strong"><strong>Netflix</strong></span> used to use this approach</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Collaborative filtering </strong></span>(U<span class="strong"><strong>ser-user similarity):</strong></span> This assumes and exemplifies those people like you who brought baby milk also bought diapers. Target use this approach</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Collaborative filtering</strong></span> (<span class="strong"><strong>Item-item similarity)</strong></span>: This assumes and exemplifies that people who like Godfather series also like Scarface. Netflix currently uses this approach</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Social, interest and graph-based approach</strong></span>: This assumes for example that your friend who likes Michel Jackson will also like <span class="emphasis"><em>Just Beat It</em></span>. The tech giant like <span class="strong"><strong>LinkedIn</strong></span> and <span class="strong"><strong>Facebook</strong></span> use this approach</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Model-based approach</strong></span>: This uses an advanced algorithm such as <span class="strong"><strong>SVM</strong></span>, <span class="strong"><strong>LDA</strong></span>, and <span class="strong"><strong>SVD</strong></span> based on the implicit features</p></li></ul></div><p>As shown in <span class="emphasis"><em>Figure 11</em></span>, the model-based recommender system that widely used advanced algorithms such as SVM, LDA, or SVD is the most robust approach in the recommender system class:</p><div class="mediaobject"><img src="graphics/B05243_05_12.jpg" /><div class="caption"><p>Figure 11: The recommender system from the technical point of view</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec66"></a>Collaborative filtering in Spark</h3></div></div></div><p>As already mentioned, the collaborative filtering techniques are commonly used for recommender systems. However, Spark MLlib currently supports model-based collaborative filtering only. Here, users and products are described by a small set of latent factors. The latent factors are later used for making the prediction of the missing entries. According to the Spark API reference for the collaborative filtering on <a class="ulink" href="http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html" target="_blank">http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html</a>: the <span class="strong"><strong>Alternating Least Squares </strong></span>(<span class="strong"><strong>ALS</strong></span>) (also known as non-linear least square,that is, NLS; see more at <a class="ulink" href="https://en.wikipedia.org/wiki/Non-linear_least_squares" target="_blank">https://en.wikipedia.org/wiki/Non-linear_least_squares</a>) algorithm is used to learn these latent factors by considering the following parameters:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><code class="literal">numBlocks</code> is the number of blocks used for the parallelized computation using the native LAPACK</p></li><li style="list-style-type: disc"><p><code class="literal">rank</code> is the number of latent factors during the machine learning model building</p></li><li style="list-style-type: disc"><p><code class="literal">iterations</code> are the number of iterations needed to gain more accurate predictions</p></li><li style="list-style-type: disc"><p><code class="literal">lambda</code> signifies the regularization parameter for the ALS algorithm</p></li><li style="list-style-type: disc"><p><code class="literal">implicitPrefs</code> specifies which feedback to be used (explicit feedback ALS variant or one adapted for implicit feedback data)</p></li><li style="list-style-type: disc"><p><code class="literal">alpha</code> specifies the baseline confidence in preference observations for the ALS algorithm</p></li></ul></div><p>At first, the ALS, which is an iterative algorithm, is used to model the rating matrix as the multiplication of low-ranked users and product factors. After that, the learning task is done by using these factors by minimizing the reconstruction error of the observed ratings.</p><p>However, the unknown ratings can successively be calculated by multiplying these factors together. The approach for the move recommendation or any other recommendation based on the collaborative filtering technique used in the Spark MLlib has been proven a high performer with high prediction accuracy and is scalable for the billions of ratings on commodity clusters used by companies such as Netflix. In following this way, a company such as Netflix can recommend movies to its subscribers based on the predicted ratings. The ultimate target is to increase the sales and of course the customer satisfaction.</p><p>For brevity and page limitation, we will not show the movie recommendations using the collaborative filtering approach in this chapter. However, a step-by-step example using Spark will be shown in <a class="link" href="#" linkend="ch09">Chapter 9</a>, <span class="emphasis"><em>Advanced</em></span>
<span class="emphasis"><em> Machine Learning with Streaming and Graph Data</em></span>.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip27"></a>Tip</h3><p>For the time being, interested readers are advised to visit the Spark website for the latest API and codes for the same at this URL: <a class="ulink" href="http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html" target="_blank">http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html</a>, where an example has been presented to show the sample movie recommendations using the ALS algorithm.</p></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec40"></a>Advanced learning and generalizations</h2></div></div><hr /></div><p>In this section, we will discuss some advanced aspects of learning, for example how we can generalize the supervised learning techniques for semi-supervised learning, active learning, structured prediction, and reinforcement learning. Moreover, reinforcement and semi-supervised learning will be discussed in brief.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec67"></a>Generalizations of supervised learning</h3></div></div></div><p>There are several ways in which the standard supervised learning problem can be generalized:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Semi-supervised learning</strong></span>: In this generalization technique, only the required output values for selected features are provided for a subset of the training data to build and evaluate the machine learning model. On the other hand, the remaining data is kept unchanged or unlabeled.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Active learning</strong></span>: In contrast, in active learning, algorithms typically interactively collect new features by making queries to a human user instead of assuming all the training features are given. Consequently, the queries used here are based on unlabeled data. Interestingly, it is also an example that combines semi-supervised learning with active learning.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Structured prediction</strong></span>: Sometimes the desired features need to be extracted or selected from complex objects like a parse tree or a labelled graph, and then the standard supervised or unsupervised methods must be improved towards adaptability for making it generalized. To be more precise, for example when a supervised machine learning technique tries to predict structured or unstructured texts such as translating NLP sentences into syntactic representation, structured prediction evolves that need to handle a large-scale parse tree. To make this task easier, often the structured SVMs or Markov logic networks or constrained conditional models are used that technically extend and update the classical supervised learning algorithms.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Learning to rank</strong></span>: Machine-learned ranking is required when the input itself is a subset of objects and the desired output is a ranking of those objects, then the standard methods must be extended or improved similarly to the structure prediction technique.</p></li></ul></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip28"></a>Tip</h3><p>Interested readers can refer to these two URLs: <a class="ulink" href="https://en.wikipedia.org/wiki/Learning_to_rank" target="_blank">https://en.wikipedia.org/wiki/Learning_to_rank</a> and <a class="ulink" href="https://en.wikipedia.org/wiki/Structured_prediction" target="_blank">https://en.wikipedia.org/wiki/Structured_prediction</a>, where a more details discussion can be found.</p></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec41"></a>Summary</h2></div></div><hr /></div><p>We have discussed some supervised, unsupervised, and recommender systems from a theoretical and Spark's perspective. However, there are numerous examples for the supervised, unsupervised, reinforcement or recommendation systems too. Nevertheless, we have tried to present some simple examples for the sake of simplicity.</p><p>We will provide more insights on these examples in <a class="link" href="#" linkend="ch06">Chapter 6</a>, <span class="emphasis"><em>Building Scalable Machine Learning Pipelines</em></span>. More feature incorporation, extraction, selection using Spark ML and Spark MLlib pipelines, model scaling, and tuning will be discussed too. We also intend to provide some examples including data collection to model building and prediction.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch06"></a>Chapter 6.   Building Scalable Machine Learning Pipelines  </h2></div></div></div><p>The ultimate goal of machine learning is to make a machine that can automatically build models from data without requiring tedious and time-consuming human involvement and interaction. Therefore, this chapter guides the readers through creating some practical and widely used machine learning pipelines and applications using Spark MLlib and Spark ML. Both APIs will be described in detail, and a baseline use case will also be covered for both. Then we will focus on scaling up the ML application so that it can cope with increasing data loads. After reading all the sections in this chapter, readers will be able to differentiate between both APIs and select the one which best fits their requirements. In a nutshell, the following topics will be covered throughout this chapter:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Spark machine learning pipeline APIs</p></li><li style="list-style-type: disc"><p>Cancer-diagnosis pipeline with Spark</p></li><li style="list-style-type: disc"><p>Cancer-prognosis pipeline with Spark</p></li><li style="list-style-type: disc"><p>Market basket analysis with Spark Core</p></li><li style="list-style-type: disc"><p>OCR pipeline with Spark</p></li><li style="list-style-type: disc"><p>Topic modeling using Spark MLlib and ML</p></li><li style="list-style-type: disc"><p>Credit-risk-analysis pipeline with Spark</p></li><li style="list-style-type: disc"><p>Scaling the ML pipelines</p></li><li style="list-style-type: disc"><p>Tips and performance considerations</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec42"></a>Spark machine learning pipeline APIs</h2></div></div><hr /></div><p>MLlib's goal is to make practical machine learning (ML) scalable and easy. Spark introduces the pipeline API for the easy creation and tuning of practical ML pipelines. As discussed in <a class="link" href="#" linkend="ch04">Chapter 4</a>, <span class="emphasis"><em>Extracting Knowledge through Feature Engineering</em></span>, a practical ML pipeline involves a sequence of data collection, pre-processing, feature extraction, feature selection, model fitting, validation, and model evaluation stages. For example, classifying the text documents might involve text segmentation and cleaning, extracting features, and training a classification model with cross-validation toward tuning. Most ML libraries are not designed for distributed computation, or they do not provide native support for pipeline creation and tuning.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec68"></a>Dataset abstraction</h3></div></div></div><p>As described in <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Introduction to Data Analytics with Spark</em></span>, when running SQL from within another programming language, the results return as a DataFrame. A DataFrame is a distributed collection of data organized into named columns. A Dataset, on the other hand, is an interface that tries to provide the benefits of RDDs out of the Spark SQL.</p><p>A Dataset can be constructed from JVM objects, which can be used both in Scala and Java. In the Spark pipeline design, a dataset is represented by Spark SQL's Dataset. An ML pipeline involves a number of the sequence of Dataset transformations and models. Each transformation takes an input dataset and outputs the transformed dataset, which becomes the input to the next stage.</p><p>Consequently, the data import and export are the start and end point of an ML pipeline. To make these easier, Spark MLlib and Spark ML provide import and export utilities of a Dataset, DataFrame, RDD, and model, for several application-specific types, including:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>LabeledPoint for classification and regression</p></li><li style="list-style-type: disc"><p>LabeledDocument for cross-validation and <span class="strong"><strong>Latent Dirichlet Allocation</strong></span> (<span class="strong"><strong>LDA</strong></span>)</p></li><li style="list-style-type: disc"><p>Rating and ranking for collaborative filtering</p></li></ul></div><p>However, real datasets usually contain numerous types, such as user ID, item IDs, labels, timestamps, and raw records.</p><p>Unfortunately, the current utilities of Spark implementation cannot easily handle datasets consisting of these types, especially time-series datasets. If you recall the section <span class="emphasis"><em>Machine learning pipeline - an overview</em></span>, in <a class="link" href="#" linkend="ch04">Chapter 4</a>, <span class="emphasis"><em>Extracting Feature through Feature Engineering</em></span>, feature transformation usually forms the majority of a practical ML pipeline. A feature transformation can be viewed as appending or dropping a new column created from existing columns.</p><p>In <span class="emphasis"><em>Figure 1</em></span>, <span class="emphasis"><em>Text processing for machine learning model</em></span>, you will see that the text tokenizer breaks a document into a bag of words. After that, the TF-IDF algorithm converts a bag of words into a feature vector. During the transformations, the labels need to be preserved for the model-fitting stage:</p><div class="mediaobject"><img src="graphics/image_06_001.jpg" /><div class="caption"><p>Figure 1: Text processing for machine learning model (DS indicates data sources)</p></div></div><p>If you recall <span class="emphasis"><em>Figure 5</em></span> and <span class="emphasis"><em>Figure 6</em></span> from <a class="link" href="#" linkend="ch04">Chapter 4</a>, <span class="emphasis"><em>Extracting Feature through Feature Engineering</em></span>, ID, text, and words are conceded during the transformations steps. They are useful in making predictions and model inspection. However, they are actually unnecessary for model fitting to state. According to a Databricks blog on ML Pipeline at <a class="ulink" href="https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html" target="_blank">https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html</a>, it doesn't provide much information if the prediction dataset only contains the predicted labels.</p><p>Consequently, if you want to inspect the prediction metrics, such as the accuracy, precision, recall, weighted true positives, and weighted false positives, it is quite useful to look at the predicted labels along with the raw input text and tokenized words. The same recommendation also applies to other machine learning applications using Spark ML and Spark MLlib, too.</p><p>Therefore, an easy conversion between RDDs, Dataset, and DataFrames has been made possible for in-memory, disk, or external data sources such as Hive and Avro. Although creating new columns from existing columns is easy with user-defined functions, the manifestation of Dataset is a lazy operation.</p><p>In contrast, the Dataset supports only some standard data types. However, to increase the usability and for making a better fit for the machine learning model, Spark has also added the support for the Vector type as a user-defined type that supports both dense and sparse feature vectors under the <code class="literal">mllib.linalg.DenseVector</code> and <code class="literal">mllib.linalg.Vector</code>.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip29"></a>Tip</h3><p>Complete DataFrame, Dataset, and RDD examples in Java, Scala, and Python can be found under the <code class="literal">examples/src/main/</code> folder under the Spark distribution. Interested readers can refer to Spark SQL's user guide at <a class="ulink" href="http://spark.apache.org/docs/latest/sql-programming-guide.html" target="_blank">http://spark.apache.org/docs/latest/sql-programming-guide.html</a> to learn more about DataFrame, Dataset, and the operations they support.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec69"></a>Pipeline</h3></div></div></div><p>Spark provides the pipeline API under Spark ML. As previously stated, a pipeline is comprised of a sequence of stages consisting of transformers and estimators. There are two basic types of pipeline stages, called Transformer and Estimator.</p><p>A transformer takes a dataset as an input and produces an augmented dataset as the output so that the output can be fed to the next step. For example, <span class="strong"><strong>Tokenizer</strong></span> and <span class="strong"><strong>H<span class="strong"><strong>ashingTF</strong></span>
</strong></span> are two transformers. Tokenizer transforms a dataset with text into a dataset with tokenized words. A HashingTF, on the other hand, produces the term frequencies. The concept of tokenization and HashingTF is commonly used in text mining and text analytics.</p><p>On the contrary, an estimator must be the first on the input dataset to produce a model. In this case, the model itself will be used as the transformer for transforming the input dataset into the augmented output dataset. For example, a <span class="strong"><strong>Logistic Regression</strong></span> or linear regression can be used as an estimator after fitting the training dataset with corresponding labels and features.</p><p>After that, it produces a logistic or linear regression model. It implies that developing a pipeline is easy and simple. Well, all you need is to declare the required stages, then configure the related stage's parameters; finally, chain them in a pipeline object, as shown in <span class="emphasis"><em>Figure 2</em></span>:</p><div class="mediaobject"><img src="graphics/image_06_002.jpg" /><div class="caption"><p>Figure 2: Spark ML pipeline model using logistic regression estimator (DS indicates data store and the steps inside the dashed line only happen during pipeline fitting)</p></div></div><p>If you look at <span class="emphasis"><em>Figure 2</em></span>, the fitted model consists of a tokenizer, a hashingTF feature extractor, and a fitted logistic regression model. The fitted pipeline model acts as a transformer that can be used for prediction, model validation, model inspection, and finally, model deployment. However, increasing the performance in terms of prediction accuracy, the model itself needs to be tuned. We will discuss more about how to tune a machine learning model in <a class="link" href="#" linkend="ch07">Chapter 7</a>, <span class="emphasis"><em>Tuning Machine Learning Model</em></span>.</p><p>To show the pipelining technique more practically, the following section shows how to create a practical pipeline for cancer diagnosis using Spark ML and MLlib.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec43"></a>Cancer-diagnosis pipeline with Spark</h2></div></div><hr /></div><p>In this section, we will look at how to develop a cancer-diagnosis pipeline with Spark ML and MLlib. A real dataset will be used to predict the probability of breast cancer, which is almost curable since the culprit genes for this cancer type have already been identified successfully. However, we would like to argue about this cancer type since in third world countries in Africa and Asia it is still a lethal disease.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip30"></a>Tip</h3><p>We suggest the readers keep an open mind about the outcome or the status of this disease, as we will just show how the Spark ML API can be used to predict cancer by integrating and combining datasets from the Wisconsin Breast Cancer (original), <span class="strong"><strong>Wisconsin Diagnosis Breast Cancer</strong></span> (<span class="strong"><strong>WDBC</strong></span>), and <span class="strong"><strong>Wisconsin Prognosis Breast Cancer</strong></span> (<span class="strong"><strong>WPBC</strong></span>) datasets from the following website: <a class="ulink" href="http://archive.ics.uci.edu/ml" target="_blank">http://archive.ics.uci.edu/ml</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec70"></a>Breast-cancer-diagnosis pipeline with Spark</h3></div></div></div><p>In this subsection, we will develop a step-by-step cancer diagnosis pipeline. The steps include a background study of breast cancer, dataset collection, data exploration, problem formalization, and Spark-based implementation.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec69"></a>Background study</h4></div></div></div><p>According to Salama et al. (<span class="emphasis"><em>Breast Cancer Diagnosis on Three Different Datasets Using Multi-Classifiers, International Journal of Computer and Information Technology</em></span> (<span class="emphasis"><em>2277 - 0764</em></span>) <span class="emphasis"><em>Volume 01- Issue 01, September 2012</em></span>), breast cancer comes in fourth position after thyroid cancer, melanoma, and lymphoma, in women between 20 and 29 years.</p><p>Breast cancer develops from breast tissue that mutates due to several factors including sex, obesity, alcohol, family history, lack of physical exercise, and so on. Furthermore, according to statistics by <span class="strong"><strong>The Centre for Diseases Control and Prevention</strong></span> (<span class="strong"><strong>TCDCP</strong></span>) (<a class="ulink" href="https://www.cdc.gov/cancer/breast/statistics/" target="_blank">https://www.cdc.gov/cancer/breast/statistics/</a>), in 2013, a total of 230,815 women and 2,109 men were diagnosed with breast cancer across the USA. Unfortunately, 40,860 women and 464 men died from it.</p><p>Research has found that about 5-10% cases are due to some genetic inheritance from parents, including BRCA1 and BRCA2 gene mutations and so on. An early diagnosis could help to save thousands of breast cancer sufferers around the globe. Although the culprit genes have been identified, chemotherapy has not proven very effective. Gene silencing is becoming popular, but more research is required.</p><p>As mentioned previously, the learning tasks in machine learning depend heavily on classification, regression, and clustering techniques. Moreover, traditional data-mining techniques are being applied along with these machine learning techniques, which are the most essential and important task. Therefore, by integrating with Spark, these applied techniques are gaining wide acceptance and adoption in the area of biomedical data analytics. Furthermore, numerous experiments are being performed on biomedical datasets using multiclass and multilevel classifiers and feature-selection techniques toward cancer diagnosis and prognosis.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec70"></a>Dataset collection</h4></div></div></div><p><span class="strong"><strong>The Cancer Genome Atlas</strong></span> (<span class="strong"><strong>TCGA</strong></span>), <span class="strong"><strong>Catalogue of Somatic Mutations in Cancer</strong></span> (<span class="strong"><strong>COSMIC</strong></span>), <span class="strong"><strong>International Cancer Genome Consortium</strong></span> (<span class="strong"><strong>ICGC</strong></span>) is the most widely used cancer and tumor-related dataset for research purposes. These data sources have been curated from world-renowned institutes such as MIT, Harvard, Oxford, and others. However, the datasets that are available are unstructured, complex, and multidimensional. Therefore, we cannot use them directly to show how to apply large-scale machine learning techniques to them. The reason is that these datasets require lots of pre-processing and cleaning, which requires lots of pages.</p><p>After practising this application, we believe readers will be able to apply the same technique for any kind of biomedical dataset for cancer diagnosis. Due to the page limitation, we should use simpler datasets that are structured and manually curated for machine learning application development and of course, many of them show good classification accuracy.</p><p>For example, the Wisconsin Breast Cancer datasets from the UCI Machine Learning Repository available at <a class="ulink" href="http://archive.ics.uci.edu/ml" target="_blank">http://archive.ics.uci.edu/ml</a> contains data that was donated by researchers at the University of Wisconsin and includes measurements from digitized images of a fine-needle aspiration of a breast mass. The values represent characteristics of the cell nuclei present in the digital image described in the following subsection.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip31"></a>Tip</h3><p>To read more about the Wisconsin breast cancer data, refer to the authors' publication: <span class="emphasis"><em>Nuclear feature extraction for breast tumor diagnosis. IS&amp;T/SPIE 1993 International Symposium on Electronic Imaging: Science and Technology, volume 1905, pp 861-870 by W.N. Street, W.H. Wolberg, and O.L. Mangasarian, 1993</em></span>.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec71"></a>Dataset description and preparation</h4></div></div></div><p>As shown in the <span class="strong"><strong>Wisconsin Breast Cancer Dataset</strong></span> (<span class="strong"><strong>WDBC</strong></span>) manual available at <a class="ulink" href="https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.names" target="_blank">https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.names</a>, the Clump thickness benign cells tend to be grouped in monolayers, while cancerous cells are often grouped in multilayers. Therefore, all the features and fields mentioned in the manual are important and before applying the machine learning technique since these features will help to identify if a particular cell is cancerous or not.</p><p>The breast cancer data includes 569 samples of cancer biopsies, each with 32 features. One feature is the identification number of the patient, another is the cancer diagnosis, labeled as benign or malignant, and the remainder are numeric-valued is called bio-assay that was identified in the molecular laboratory works. The diagnosis is coded as either M to indicate malignant or B to indicate benign with regard to the cancer diagnosis.</p><p>The Class distribution is as follows: Benign: 357 (62.74%) and Malignant: 212 (37.25%). The training and test dataset will be prepared following the dataset description given here. The 30 numeric measurements include the mean, standard error, and worst, which is the mean of the three largest values. Field 3 is the mean radius, 13 is the Radius SE, and 23 is the Worst Radius. The 10 real-valued features are computed for each cell nucleus by means of different characteristics of the digitized cell nuclei described in <span class="emphasis"><em>Table 1, 10 real-valued features and their descriptions</em></span>:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>No.</strong></span></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Value</strong></span></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Explanation</strong></span></p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>1</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Radius</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Mean of distances from center to points on the perimeter</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>2</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Texture</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Standard deviation of gray-scale values</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>3</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Perimeter</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>The perimeter of the cell nucleus</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>4</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Area</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Area of the cell nucleus covering the perimeter</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>5</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Smoothness</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Local variation in radius lengths</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>6</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Compactness</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Calculated as follows: (Perimeter)^2 / area - 1.0</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>7</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Concavity</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Severity of concave portions of the contour</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>8</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Concave points</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Number of concave portions of the contour</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>9</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Symmetry</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Indicates if the cell structure is symmetrical</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; ">
<p>10</p>
</td><td style="border-right: 0.5pt solid ; ">
<p>Fractal dimension</p>
</td><td style="">
<p>Calculated as: coastline approximation - 1</p>
</td></tr></tbody></table></div><p>Table 1: 10 real-valued features and their descriptions</p><p>All feature values are recorded with four significant digits and there are no missing or NULL values. Therefore, we don't need to perform any data cleaning. However, from the previous description, it's really difficult for someone to get any good knowledge of the data. For example, you are unlikely to know how each field relates to benign or malignant masses unless you are an oncologist. These patterns will be revealed as we continue the machine learning process. A sample snapshot of the dataset is shown in <span class="emphasis"><em>Figure 3</em></span>:</p><div class="mediaobject"><img src="graphics/B05243_06_03.jpg" /><div class="caption"><p>Figure 3: Snapshot of the data (partial)</p></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec72"></a>Problem formalization</h4></div></div></div><p><span class="emphasis"><em>Figure 4</em></span>, <span class="emphasis"><em>The breast cancer diagnosis and prognosis pipeline model</em></span>, describes the proposed breast cancer diagnosis model. The model consists of two phases, namely, the training and testing phases:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The training phase includes four steps: data collection, pre-processing, feature extraction, and feature selection</p></li><li style="list-style-type: disc"><p>The testing phase includes the same four steps as the training phase with the addition of the classification step</p></li></ul></div><p>In the data-collection step, first the pre-processing is done to check if there is an unwanted value or any values are missing. We have already mentioned that there are no missing values. However, it is always good practice to check, since even the unwanted value of a special character could halt the whole training process. After that, the feature engineering step is done through the feature extraction and selection process for determining the correct input vector for the subsequent logistic or linear regression classifier:</p><div class="mediaobject"><img src="graphics/image_06_004.jpg" /><div class="caption"><p>Figure 4: The breast cancer diagnosis and prognosis pipeline model</p></div></div><p>This helps to make a decision regarding the class associated to the pattern vectors. Based on either feature selection or feature extraction, the dimensionality reduction technique is accomplished. However, please note that we will not use any formal dimensionality reduction algorithms to develop this application. For more on dimensionality reduction, you can refer to the <span class="emphasis"><em>Dimensionality reduction</em></span> section in <a class="link" href="#" linkend="ch04">Chapter 4</a>, <span class="emphasis"><em>Extracting Knowledge through Feature Engineering</em></span>.</p><p>In the classification step, a logistic regression classifier is applied to get the best result for the diagnosis and prognosis of the tumor.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec73"></a>Developing a cancer-diagnosis pipeline with Spark ML</h4></div></div></div><p>As mentioned previously, the details of the attributes found in the WDBC dataset at <a class="ulink" href="https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.names" target="_blank">https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.names</a> include patient ID, diagnosis (M = malignant, B = benign), and 10 real-valued features are computed for each cell nucleus, as described in <span class="emphasis"><em>Table 1</em></span>, <span class="emphasis"><em>10 real-valued features and their description</em></span>.</p><p>These features are computed from a digitized image of a <span class="strong"><strong>fine needle aspiration</strong></span> (<span class="strong"><strong>FNA</strong></span>) of a breast mass, since we have enough knowing about the dataset. In this subsection, we will look at how to develop a breast cancer diagnosis machine learning pipeline step-by-step including taking the input of the dataset to prediction in the 10 steps described in <span class="emphasis"><em>Figure 4</em></span>, as a data workflow.</p><p><span class="strong"><strong>Step 1: Import the necessary packages/libraries/APIs</strong></span></p><p>Here is the code to import the packages:</p><pre class="programlisting">import org.apache.spark.api.java.JavaRDD; 
import org.apache.spark.api.java.function.Function; 
import org.apache.spark.ml.Pipeline; 
import org.apache.spark.ml.PipelineModel; 
import org.apache.spark.ml.PipelineStage; 
import org.apache.spark.ml.classification.LogisticRegression; 
import org.apache.spark.ml.feature.LabeledPoint; 
import org.apache.spark.ml.linalg.DenseVector; 
import org.apache.spark.ml.linalg.Vector; 
import org.apache.spark.sql.Dataset; 
import org.apache.spark.sql.Row; 
import org.apache.spark.sql.SparkSession; 
</pre><p><span class="strong"><strong>Step 2: Initialize Spark session</strong></span></p><p>A Spark session can be initialized with the help of the following code:</p><pre class="programlisting">static SparkSession spark = SparkSession 
        .builder() 
        .appName("BreastCancerDetectionDiagnosis") 
       .master("local[*]") 
       .config("spark.sql.warehouse.dir", "E:/Exp/") 
       .getOrCreate();</pre><p>Here we set the application name as <code class="literal">BreastCancerDetectionDiagnosis</code>, and the master URL as <code class="literal">local</code><code class="literal">.</code> The Spark Context is the entry point of the program. Please set these parameters accordingly.</p><p><span class="strong"><strong>Step 3: Take the breast cancer data as input and prepare JavaRDD out of the data</strong></span></p><p>Here is the code to prepare <code class="literal">JavaRDD:</code></p><pre class="programlisting">  String path = "input/wdbc.data"; 
  JavaRDD&lt;String&gt; lines = spark.sparkContext().textFile(path, 3).toJavaRDD();</pre><p>To learn more about the data, please refer to <span class="emphasis"><em>Figure 3</em></span>: <span class="emphasis"><em>Snapshot of the data (partial</em></span>.</p><p><span class="strong"><strong>Step 4: Create LabeledPoint RDDs for regression</strong></span></p><p>Create <code class="literal">LabeledPoint</code> RDDs for diagnosis (B = benign and M= Malignant):</p><pre class="programlisting">JavaRDD&lt;LabeledPoint&gt; linesRDD = lines 
        .map(new Function&lt;String, LabeledPoint&gt;() { 
          public LabeledPoint call(String lines) { 
            String[] tokens = lines.split(","); 
            double[] features = new double[30]; 
            for (int i = 2; i &lt; features.length; i++) { 
              features[i - 2] = Double.parseDouble(tokens[i]); 
            } 
            Vector v = new DenseVector(features); 
            if (tokens[1].equals("B")) { 
              return new LabeledPoint(1.0, v); // benign 
            } else { 
              return new LabeledPoint(0.0, v); // malignant 
            } 
          } 
        }); 
</pre><p><span class="strong"><strong>Step 5: Create the Dataset of Row from the linesRDD and show the top features</strong></span></p><p>Here is the code illustrated:</p><pre class="programlisting">Dataset&lt;Row&gt; data = spark.createDataFrame(linesRDD,LabeledPoint.class); 
data.show(); 
</pre><p>The following figure shows the top features and their corresponding labels:</p><div class="mediaobject"><img src="graphics/B05243_06_05.jpg" /><div class="caption"><p>Figure 5: Top features and their corresponding labels</p></div></div><p><span class="strong"><strong>Step 6: Split the Dataset to prepare the training and test sets</strong></span></p><p>Here we split the original data frame into training and test set as 60% and 40%, respectively. Here, <code class="literal">12345L</code> is the seed value. This value signifies that the split will be the same every time, so that the ML model produces the same result in each iteration. We follow the same conversion in each chapter for preparing the test and training set:</p><pre class="programlisting">Dataset&lt;Row&gt;[] splits = data.randomSplit(new double[] { 0.6, 0.4 }, 12345L); 
Dataset&lt;Row&gt; trainingData = splits[0]; 
Dataset&lt;Row&gt; testData = splits[1]; 
</pre><p>To see a quick snapshot of these two sets just write <code class="literal">trainingData.show()</code> and <code class="literal">testData.show()</code> for training and test sets, respectively.</p><p><span class="strong"><strong>Step 7: Create a Logistic Regression classifier</strong></span></p><p>Create a logistic regression classifier by specifying the max iteration and regression parameter:</p><pre class="programlisting">LogisticRegression logisticRegression = new LogisticRegression() 
                          .setMaxIter(100) 
                             .setRegParam(0.01) 
                             .setElasticNetParam(0.4); 
</pre><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip32"></a>Tip</h3><p>Logistic regression typically takes three parameters: the number of max iteration, the regression parameter, and the elastic-net regularization. See the following lines to get a clearer idea:</p></div><pre class="programlisting">      LogisticRegression lr = new 
      LogisticRegression().setMaxIter(100)
      .setRegParam(0.01).setElasticNetParam(0.4); 
</pre><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip33"></a>Tip</h3><p>The preceding statements create a logistic regression model <code class="literal">lr</code> with max iteration <code class="literal">100</code>, regression parameter <code class="literal">0.01</code>, and elastic net parameter <code class="literal">0.4</code>.</p></div><p><span class="strong"><strong>Step 8: Create and train the pipeline model</strong></span></p><p>Here is the code illustrated:</p><pre class="programlisting">Pipeline pipeline = new Pipeline().setStages(new PipelineStage[] {logisticRegression}); 
PipelineModel model = pipeline.fit(trainingData); 
</pre><p>Here we have created a pipeline whose stages are defined by the logistic regression stage, which is also an estimator we have just created. Note that you could try creating the Tokenizer and HashingTF stages if you are dealing with a text dataset.</p><p>However, in this cancer dataset, all of our values are numeric. Therefore, we don't create such stages to be chained to the pipeline.</p><p><span class="strong"><strong>Step 9: Create a Dataset, transform the model and prediction</strong></span></p><p>Create a Dataset of type Row and transform the model to do the prediction based on the test dataset:</p><pre class="programlisting">Dataset&lt;Row&gt; predictions=model.transform(testData); 
</pre><p><span class="strong"><strong>Step 10: Show the prediction with prediction precision</strong></span></p><pre class="programlisting">predictions.show(); 
long count = 0; 
for (Row r : predictions.select("features", "label", "prediction").collectAsList()) { 
    System.out.println("(" + r.get(0) + ", " + r.get(1) + r.get(2) + ", prediction=" + r.get(2)); 
      count++; 
    } 
</pre><div class="mediaobject"><img src="graphics/image_06_006.jpg" /><div class="caption"><p>Figure 6: Prediction with prediction precision</p></div></div><p><span class="emphasis"><em>Figure 7</em></span> shows the prediction Dataset for the test set. The print method shown essentially generates output, much like the following:</p><div class="mediaobject"><img src="graphics/B05243_06_07.jpg" /><div class="caption"><p>Figure 7: Sample output toward the prediction. The first value is the feature, the second is the label, and the final value is the prediction value</p></div></div><p>Now let's calculate the precision score. We do this by multiplying the counter by 100 and then dividing the value against how many predictions were done, as follows:</p><pre class="programlisting">System.out.println("precision: " + (double) (count * 100) / predictions.count()); 
Precision - 100.0 
</pre><p>Therefore, the precision is 100%, which is fantastic. However, if you are still unsatisfied or have any confusion, the following chapter will demonstrate how you can still tune several parameters so that the prediction accuracy increases, as there might have been many false-negative predictions.</p><p>Furthermore, the result might vary on your platform due to the random-split nature and dataset processing on your side.</p></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec44"></a>Cancer-prognosis pipeline with Spark</h2></div></div><hr /></div><p>In the previous section, we showed how to develop a cancer diagnosis pipeline for predicting cancer based on two labels (Benign and Malignant). In this section, we will look at how to develop a cancer prognosis pipeline with Spark ML and MLlib APIs. The <span class="strong"><strong>Wisconsin Prognosis Breast Cancer</strong></span> (<span class="strong"><strong>WPBC</strong></span>) datasets will be used to predict the probability of breast cancer toward the prognosis for recurrent and non-recurrent tumor cells. Again, the dataset was downloaded from <a class="ulink" href="https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Prognostic)" target="_blank">https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Prognostic)</a>. To understand the problem formalization, please refer to <span class="emphasis"><em>Figure 1</em></span> once again as we will follow almost the same stages during the cancer-prognosis pipeline development.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec71"></a>Dataset exploration</h3></div></div></div><p>The details of the attributes found in the WPBC dataset in <a class="ulink" href="https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wpbc.names" target="_blank">https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wpbc.names</a> are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>ID number</p></li><li style="list-style-type: disc"><p>Outcome (R = recurrent, N = non-recurrent)</p></li><li style="list-style-type: disc"><p>Time (recurrence time if field 2 =&gt; R, disease-free time if field 2 =&gt; N)</p></li><li style="list-style-type: disc"><p>3 to 33: Ten real-valued features are computed for each cell nucleus: Radius, Texture, Perimeter, Area, Smoothness, Compactness, Concavity, Concave points, Symmetry, and Fractal dimension. Thirty-four is Tumor size and thirty-five is the Lymph node status, as follows:
</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Tumor size: Diameter of the excised tumor in centimeters</p></li><li style="list-style-type: disc"><p>Lymph node status: The number of positive axillary lymph nodes</p></li></ul></div><p>
</p></li></ul></div><p>If you compare <span class="emphasis"><em>Figure 3</em></span> and <span class="emphasis"><em>Figure 9</em></span>, you will see that the diagnosis and prognosis have the same features, yet the prognosis has two additional features (mentioned previously as 34 and 35). Note that these are observed at the time of surgery from the year 1988 to 1995 and out of the 198 instances, 151 are non-recurring (N) and 47 are recurring (R), as shown in <span class="emphasis"><em>Figure 8</em></span>.</p><p>Of course, a real cancer diagnosis and prognosis dataset today contains many other features and fields in a structured or unstructured way:</p><div class="mediaobject"><img src="graphics/B05243_06_08.jpg" /><div class="caption"><p>Figure 8: Snapshot of the data (partial)</p></div></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip34"></a>Tip</h3><p>For more detailed discussion and meaningful insights, interested readers can refer to the following research paper: <span class="emphasis"><em>The Wisconsin Breast Cancer Problem: Diagnosis and DFS time prognosis using probabilistic and generalized regression neural classifiers Oncology Reports, special issue Computational Analysis and Decision Support Systems in Oncology, last quarter 2005 by Ioannis A. et al. found in the following link: </em></span><a class="ulink" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.65.2463&amp;rep=rep1&amp;type=pdf" target="_blank">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.65.2463&amp;rep=rep1&amp;type=pdf</a>.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec72"></a>Breast-cancer-prognosis pipeline with Spark ML/MLlib</h3></div></div></div><p>In this subsection, we will look at how to develop a breast cancer prognosis machine learning pipeline step-by-step, including taking the input of the dataset to prediction in 10 different steps that are described in <span class="emphasis"><em>Figure 1</em></span>, as a data workflow.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip35"></a>Tip</h3><p>Readers are advised to download the dataset and the project files, along with the <code class="literal">pom.xml</code> file for the Maven project configuration, from the Packt materials. We have advised how to make the code work in previous chapters, for example, <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Introduction to Data Analytics with Spark</em></span>.</p></div><p><span class="strong"><strong>Step 1: Import necessary packages/libraries/APIs</strong></span></p><pre class="programlisting">import org.apache.spark.api.java.JavaRDD; 
import org.apache.spark.api.java.function.Function; 
import org.apache.spark.ml.Pipeline; 
import org.apache.spark.ml.PipelineModel; 
import org.apache.spark.ml.PipelineStage; 
import org.apache.spark.ml.classification.LogisticRegression; 
import org.apache.spark.ml.feature.LabeledPoint; 
import org.apache.spark.ml.linalg.DenseVector; 
import org.apache.spark.ml.linalg.Vector; 
import org.apache.spark.sql.Dataset; 
import org.apache.spark.sql.Row; 
import org.apache.spark.sql.SparkSession; 
</pre><p><span class="strong"><strong>Step 2: Initialize the necessary Spark environment</strong></span></p><pre class="programlisting">static SparkSession spark = SparkSession 
        .builder() 
        .appName("BreastCancerDetectionPrognosis") 
       .master("local[*]") 
       .config("spark.sql.warehouse.dir", "E:/Exp/") 
       .getOrCreate(); 
</pre><p>Here we set the application name as <code class="literal">BreastCancerDetectionPrognosis</code>, the master URL as <code class="literal">local[*]</code>. The Spark Context is the entry point of the program. Please set these parameters accordingly.</p><p><span class="strong"><strong>Step 3: Take the breast cancer data as input and prepare JavaRDD out of the data</strong></span></p><pre class="programlisting">String path = "input/wpbc.data"; 
JavaRDD&lt;String&gt; lines = spark.sparkContext().textFile(path, 3).toJavaRDD(); 
</pre><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip36"></a>Tip</h3><p>To learn more about the data, please refer to <span class="emphasis"><em>Figure 5</em></span> and its description and the Dataset exploration subsection.</p></div><p><span class="strong"><strong>Step 4: Create LabeledPoint RDDs</strong></span></p><p>Create <code class="literal">LabeledPoint</code> RDDs for the prognosis for N = recurrent and R= non-recurrent, respectively, using the following code segments:</p><pre class="programlisting">JavaRDD&lt;LabeledPoint&gt; linesRDD = lines.map(new Function&lt;String, LabeledPoint&gt;() { 
      public LabeledPoint call(String lines) { 
        String[] tokens = lines.split(","); 
        double[] features = new double[30]; 
        for (int i = 2; i &lt; features.length; i++) { 
          features[i - 2] = Double.parseDouble(tokens[i]); 
        } 
        Vector v = new DenseVector(features); 
        if (tokens[1].equals("N")) { 
          return new LabeledPoint(1.0, v); // recurrent 
        } else { 
          return new LabeledPoint(0.0, v); // non-recurrent 
        } 
      } 
    });  
</pre><p><span class="strong"><strong>Step 5: Create the Dataset from the lines RDD and show the top features</strong></span></p><pre class="programlisting">Dataset&lt;Row&gt; data = spark.createDataFrame(linesRDD,LabeledPoint.class); 
data.show(); 
</pre><p>The top features and their corresponding labels are shown in <span class="emphasis"><em>Figure 9</em></span>:</p><div class="mediaobject"><img src="graphics/B05243_06_09.jpg" /><div class="caption"><p>Figure 9: Top features and their corresponding labels</p></div></div><p><span class="strong"><strong>Step 6: Split the Dataset to prepare the training and test sets</strong></span></p><p>Here we split the dataset to test and the training set as 60% and 40%, respectively. Please adjust these based on your requirements:</p><pre class="programlisting">Dataset&lt;Row&gt;[] splits = data.randomSplit(new double[] { 0.6, 0.4 }, 12345L); 
Dataset&lt;Row&gt; trainingData = splits[0];   
Dataset&lt;Row&gt; testData = splits[1]; 
</pre><p>To see a quick snapshot of these two sets, just write <code class="literal">trainingData.show()</code> and <code class="literal">testData.show()</code>, for training and test sets, respectively.</p><p><span class="strong"><strong>Step 7: Create a Logistic Regression classifier</strong></span></p><p>Create a logistic regression classifier by specifying the max iteration and regression parameter:</p><pre class="programlisting">LogisticRegression logisticRegression = new LogisticRegression() 
.setMaxIter(100) 
.setRegParam(0.01) 
.setElasticNetParam(0.4); 
</pre><p><span class="strong"><strong> Step 8: Create a pipeline and train the pipeline model</strong></span></p><pre class="programlisting">Pipeline pipeline = new Pipeline().setStages(new PipelineStage[]{logisticRegression}); 
PipelineModel model=pipeline.fit(trainingData); 
</pre><p>Here, similarly to the diagnosis pipeline, we have created the prognosis pipeline whose stages are defined by only the logistic regression, which is again an estimator, and of course a stage.</p><p><span class="strong"><strong>Step 9: Create a Dataset and transform the model</strong></span></p><p>Create a Dataset and do the transformation to make a prediction based on the test dataset:</p><pre class="programlisting">Dataset&lt;Row&gt; predictions=model.transform(testData); 
</pre><p><span class="strong"><strong>Step 10: Show the prediction with prediction precision</strong></span></p><pre class="programlisting">predictions.show(); 
</pre><div class="mediaobject"><img src="graphics/image_06_010.jpg" /><div class="caption"><p>Figure 10: Prediction with prediction precision</p></div></div><pre class="programlisting">long count = 0; 
for (Row r : predictions.select("features", "label", "prediction").collectAsList()) { 
      System.out.println("(" + r.get(0) + ", " + r.get(1) + r.get(2) + ", prediction=" + r.get(2)); 
      count++; 
    } 
</pre><p>This code segment will produce an output similar to that shown in <span class="emphasis"><em>Figure 7</em></span>, with different features, labels, and predictions:</p><pre class="programlisting">System.out.println("precision: " + (double) (count * 100) / predictions.count());  
Precision: 100.0  
</pre><p>Therefore, the precision is almost 100%, which is fantastic. However, depending upon the data preparation, you might receive different results.</p><p>If you have any confusion, the following chapter demonstrates how to tune parameters so that the prediction accuracy increases, as they might have many false-negative predictions.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip37"></a>Tip</h3><p>In their book titled <span class="emphasis"><em>Machine Learning with R, Packt Publishing, 2015</em></span>, Brett Lantz at el. argue that it's possible to eliminate the false negatives completely by classifying every mass as malignant, benign, recurrent, or non-recurrent. Obviously, this is not a realistic strategy. Still, it illustrates the fact that prediction involves striking a balance between the false-positive rate and the false-negative rate.</p></div><p>If you are still unsatisfied, we will be tuning several parameters in <a class="link" href="#" linkend="ch07">Chapter 7</a>, <span class="emphasis"><em>Tuning Machine Learning Models</em></span>, so that the prediction accuracy increases toward more sophisticated methods for measuring predictive accuracy that can be used to identify places where the error rate can be optimized depending on the costs of each type of error.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec45"></a>Market basket analysis with Spark Core</h2></div></div><hr /></div><p>In this section, we will look at how to develop a large-scale machine learning pipeline in terms of market basket analysis. Other than using the Spark ML and MLlib, we will demonstrate how to use Spark Core to develop such an application.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec73"></a>Background</h3></div></div></div><p>In an early paper, <span class="emphasis"><em>An Efficient Market Basket Analysis Technique with Improved MapReduce Framework on Hadoop: An E-commerce Perspective</em></span> (available at <a class="ulink" href="http://onlinepresent.org/proceedings/vol6_2012/8.pdf" target="_blank">http://onlinepresent.org/proceedings/vol6_2012/8.pdf</a>), the authors have argued that the <span class="strong"><strong>market basket analysis</strong></span> (<span class="strong"><strong>MBA</strong></span>) technique is of substantial importance to everyday business decision, since customers' purchase rules can be extracted from the association rules by discovering what items they are buying frequently and together. Consequently, purchase rules can be revealed for frequent shoppers based on these association rules.</p><p>You might still be wondering why we need market basket analysis, why it is important, and why it is computationally expensive. Well, if you could identify highly specific association rules like, for example, if a customer prefers mango or orange jam along with their milk or butter, you need to have large-scale transactional data to be analyzed and processed. Moreover, some massive chain retailers or supermarkets, for example, E-mart (UK), HomePlus (Korea), Aldi (Germany), or Dunnes Stores (Ireland) use databases of many millions, or even billions, of transactions in order to find the associations among particular items with regard to brand, color, origin, or even flavor, to increase the probability of sales and profit.</p><p>In this section, we will look at an efficient approach for large-scale market basket analysis with Spark libraries. After reading and practising this, you will be able to show how the Spark framework lifts the existing single-node pipeline to a pipeline usable on a multi-node data-mining cluster. The result is that our proposed association-rules mining algorithm can be reused in parallel with the same benefits.</p><p>We use the acronym SAMBA, for Spark-based Market Basket Analysis, <span class="emphasis"><em>min_sup</em></span> for minimum support, and <span class="emphasis"><em>min_conf</em></span> for minimum confidence. We also use the terms frequent patterns and frequent itemset interchangeably.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec74"></a>Motivations</h3></div></div></div><p>Traditional main memory or disk-based computing and RDBMS are not capable of handling ever-increasing large transactional data. Furthermore, as discussed in <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Introduction to Data Analytics with Spark</em></span>, MapReduce has several issues with the I/O operation, algorithmic complexity, low-latency, and fully disk-based operation. Therefore, finding the null transactions and later eliminating them from the future scheme is the initial part of this approach.</p><p>It is quite possible to find all the null transactions by identifying those transactions that do not appear against at least one frequent 1-itemset. As already mentioned, Spark caches the intermediate data into memory and provides an abstraction of <span class="strong"><strong>Resilient Distributed Datasets</strong></span> (<span class="strong"><strong>RDDs</strong></span>), which can be used to overcome these issues by making a huge difference, achieving tremendous success in the last three years for handling large-scale data in distributed computing systems. These successes are promising and motivating examples to explore this research work to applying Spark in market basket analysis.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec75"></a>Exploring the dataset</h3></div></div></div><p>Please download the grocery dataset for the market basket analysis from <a class="ulink" href="https://github.com/stedy/Machine-Learning-with-R-datasets/blob/master/groceries.csv" target="_blank">https://github.com/stedy/Machine-Learning-with-R-datasets/blob/master/groceries.csv</a>. The first five rows of the raw <code class="literal">grocery.csv </code>data are as follows in <span class="emphasis"><em>Figure 11</em></span>. These lines indicate 10 separate grocery-store transactions. The first transaction includes four items: citrus fruit, semi-finished bread, margarine, and ready soups. In comparison, the third transaction includes only one item, whole milk:</p><div class="mediaobject"><img src="graphics/image_06_011.jpg" /><div class="caption"><p>Figure 11: A snapshot of the groceries dataset</p></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec76"></a>Problem statements</h3></div></div></div><p>We believe we have enough motivations and reasons for why we need to analyze the market basket using transactional or retail datasets. Now, let us discuss some background studies, which are needed to apply our Spark-based market basket analysis technique.</p><p>Suppose you have a set of distinct items <span class="emphasis"><em>I = {i1, i2...in}</em></span> and <span class="emphasis"><em>n</em></span> is the number of distinct items. A transactional database <span class="emphasis"><em>T = {t1, t2...tN}</em></span> is a set of <span class="emphasis"><em>N</em></span> transactions and <span class="emphasis"><em>|N|</em></span> is the number of total transactions. A set <span class="emphasis"><em>X</em></span>
<span class="inlinemediaobject"><img src="graphics/B05243_06_48.jpg" /></span>
 is called a pattern or itemset. We assume that input is given as a sequence of transactions, where items are separated by a comma, as shown in <span class="emphasis"><em>Table 1</em></span>.</p><p>For the sake of simplicity to describe the background study, the same transactions are presented with a single character in <span class="emphasis"><em>Table 2</em></span>:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; ">
<p>Transaction 1</p><p>
</p><p>Transaction 2</p><p>
</p><p>Transaction 3</p><p>
</p><p>Transaction 4</p><p>
</p><p>...</p>
</td><td style="">
<p>crackers, ice-cream, coke, orange,</p><p>
</p><p>beef, pizza, coke, bread</p><p>
</p><p>baguette, soda, shampoo, crackers, pepsi</p><p>
</p><p>burger, cream cheese, diapers, milk</p><p>
</p><p>...</p>
</td></tr></tbody></table></div><p>Table 1. Sample transactions made by a customer</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>TID</strong></span></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Itemset (Sequence of items)</strong></span></p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>10</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>A, B, C, F</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>20</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>C, D, E</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>30</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>A, C, E, D</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>40</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>A</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>50</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>D, E, G</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>60</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>B, D</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>70</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>B</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>80</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>A, E, C</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>90</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>A, C, D</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; ">
<p>100</p>
</td><td style="">
<p>B, E, D</p>
</td></tr></tbody></table></div><p>Table 2. A transactional database</p><p>If <span class="inlinemediaobject"><img src="graphics/Inline1.jpg" /></span>
, it is said that <span class="emphasis"><em>X</em></span> occurs in <span class="emphasis"><em>t</em></span> or <span class="emphasis"><em>t</em></span> contains <span class="emphasis"><em>X</em></span>. The support count is the frequency of occurrence of an itemset in all transactions, which can be described as follows:</p><div class="mediaobject"><img src="graphics/B05243_06_43.jpg" /></div><p>In other words, if the <span class="emphasis"><em>support</em></span><span class="inlinemediaobject"><img src="graphics/B05243_06_50.jpg" /></span>
, we say that <span class="emphasis"><em>X</em></span> is a frequent itemset. For example, in <span class="emphasis"><em>Table 2</em></span>, the occurrences of itemsets <span class="emphasis"><em>CD</em></span>, <span class="emphasis"><em>DE,</em></span> and <span class="emphasis"><em>CDE</em></span> are <span class="emphasis"><em>3</em></span>, <span class="emphasis"><em>3</em></span>, and <span class="emphasis"><em>2</em></span>, respectively, and if the <span class="emphasis"><em>min_sup</em></span> is <span class="emphasis"><em>2</em></span>, all of these are frequent itemsets.</p><p>On the other hand, association rules are statements of form <span class="inlinemediaobject"><img src="graphics/B05243_06_49.jpg" /></span>
 or more formally:</p><div class="mediaobject"><img src="graphics/B05243_06_44.jpg" /></div><p>Therefore, we can say that an association rule is a pattern that states when <span class="emphasis"><em>X</em></span> occurs, then <span class="emphasis"><em>Y</em></span> occurs with a certain probability. Confidence for the association rule defined in equation 1 can be expressed as how often items in <span class="emphasis"><em>Y</em></span> appear in transactions that also contain <span class="emphasis"><em>X</em></span>, as follows:</p><div class="mediaobject"><img src="graphics/B05243_06_45.jpg" /></div><p>Now we need to introduce a new parameter, called <code class="literal">lift</code>, which as a metric is a measure of how much more likely one item is to be purchased relative to its typical purchase rate, given that you know another item has been purchased. This is defined by the following equation:</p><div class="mediaobject"><img src="graphics/B05243_06_46.jpg" /></div><p>In a nutshell, given a transactional database, now the problem of market basket analysis is to find the complete set of a customer's purchase rules by means of association rules from the frequent itemsets whose support and confidence are no less than the <span class="emphasis"><em>min_sup</em></span> and <span class="emphasis"><em>min_conf</em></span> threshold, respectively.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec77"></a>Large-scale market basket analysis using Spark</h3></div></div></div><p>As shown in <span class="emphasis"><em>Figure 12</em></span>, we assume that transactional databases are stored in a distributed way in a cluster of DB servers. A DB server is a computing node with large storage and main memory. Therefore, it can store large datasets, so it can compute any task assigned to it. The Driver PC is also a computing node, which mainly works as a client and controls the overall process.</p><p>Obviously, it needs to have a large memory for processing and holding the Spark codes to send across the computing nodes. The codes consist of a DB server ID, minimum support, minimum confidence, and mining algorithm:</p><div class="mediaobject"><img src="graphics/B05243_06_12-918x1024.jpg" /><div class="caption"><p>Figure 12: Workflow of the SAMBA algorithm using Spark</p></div></div><p>From the patterns, frequent patterns are generated using reduce phase 1, which satisfies the constraints <span class="emphasis"><em>min_sup</em></span>. The map phase is applied on the computed frequent patterns to generate the sub-patterns that eventually help to generate the association rules. From the sub-patterns, reduce phase 2 is applied to generate the association rules that satisfy the constraints <span class="emphasis"><em>min_conf</em></span>.</p><p>The incorporation of two Map and Reduce phases is possible because of the Spark ecosystem toward the Spark core and associated APIs. The final results are the complete set of association rules with their respective support count and confidence.</p><p>These store keepers have the full form to place their items, based on the association between items, to increase sales to frequent and non-frequent shoppers. Due to space constraints, we cannot show a step-by-step example for the sample transactional database presented in <span class="emphasis"><em>Table 2</em></span>.</p><p>However, we believe that the workflow and the pseudo codes will suffice to understand the total scenario. A DB server takes the input of codes sent from the Driver PC and starts the computation. From an environment variable Spark session, we create some initial data reference or RDD objects. Then, the initial RDD objects are transformed to create more and brand new RDD objects in the DB server. At first, it reads the dataset as a plain text (or other supported format) and null transactions using narrow/wide transformations (that is, <code class="literal">flatMap</code>, <code class="literal">mapToPair</code>, and <code class="literal">reduceByKey</code>).</p><p>Thereby, the filter join RDD operation provides a data segment without null transactions. Then the RDD objects are materialized to dump the RDD into the DB server's storage as filtered datasets. Spark's inter RDD join operation allows for the combining of the contents of multiple RDDs within a single data node. In summary, we follow the steps given here before getting the filtered dataset:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Set the system property of the distributed processing model and cluster manager (that is, Mesos) as true. This value can be saved on your application development as standard Spark code.</p></li><li><p>Set SparkConf, AppName, Master URL, Spark local IP, Spark driver host IP, Spark executor memory, and Spark driver memory.</p></li><li><p>Create <code class="literal">JavaSparkContext</code> using the <code class="literal">SparkConf</code>.</p></li><li><p>Create <code class="literal">JavaRDD</code> and read the dataset as plain text, as transactions, and perform necessary partitioning.</p></li><li><p>Perform a <code class="literal">flatMap</code> operation over the RDD to split the transactions as items.</p></li><li><p>Perform the <code class="literal">mapToPair</code> operation to ease finding the key/value pairs of the items.</p></li><li><p>Perform the filter operation to remove all the null transactions.</p></li></ol></div><p>When we have the filtered databases, we materialize an action inter-RDD join operation to save the dataset on a DB server or partition if it does not have enough storage for a single machine, or cache if there's not enough memory.</p><p><span class="emphasis"><em>Figure 12</em></span> shows the complete workflow of getting association rules as the final results using Spark's APIs. On the other hand, Figure 13 shows the pseudo-code of the algorithm, namely, <span class="strong"><strong>Spark-Based Market Basket Analysis</strong></span> (<span class="strong"><strong>SAMBA</strong></span>). There are actually two Map and Reduce operations associated, as outlined here:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Map/Reduce phase 1</strong></span>: Mappers read the transactions from the HDFS servers and convert the transactions to patterns. Reducers, on the other hand, find the frequent patterns.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Map/Reduce phase 2</strong></span>: Mappers convert the frequent patterns into sub-patterns. On the other hand, a reducer generates the association rules based on the given constraints (<code class="literal">min_conf</code> and <code class="literal">lift</code>):</p><div class="mediaobject"><img src="graphics/image_06_016.jpg" /><div class="caption"><p>Figure 13: The SAMBA algorithm</p></div></div></li></ul></div><p>After that, the SAMBA algorithm reads the <span class="strong"><strong>filtered database</strong></span> (<span class="strong"><strong>FTDB</strong></span>) and applies map phase 1 to generate all the possible combinations of the patterns. Then the <code class="literal">mapToPair()</code> methods them as patterns with their respective supports.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec78"></a>The algorithm solution using Spark Core</h3></div></div></div><p>Here we will look at how to do the market basket analysis using Spark Core. Please note, we will not use the Spark ML or MLlib since although MLlib provides a technique of calculating the association rules, however, it does not show how to calculate some other parameters such as calculating confidence, support, and lift that are very needed for a complete analysis of groceries dataset. Therefore, we will show a complete example, step-by-step, from data exploration to association rules generation.</p><p><span class="strong"><strong>Step 1: Import the necessary packages and APIs</strong></span></p><p>Here is the code to import packages and APIs:</p><pre class="programlisting">import java.util.ArrayList; 
import java.util.Iterator; 
import java.util.List; 
import org.apache.spark.api.java.JavaPairRDD; 
import org.apache.spark.api.java.JavaRDD; 
import org.apache.spark.api.java.function.Function; 
import org.apache.spark.api.java.function.Function2; 
import org.apache.spark.api.java.function.PairFlatMapFunction; 
import org.apache.spark.rdd.RDD; 
import org.apache.spark.sql.SparkSession; 
import scala.Tuple2;  
import scala.Tuple4; 
</pre><p><span class="strong"><strong>Step 2: Create the entry point by specifying the Spark session</strong></span></p><p>The entry point can be created with the help of the following code:</p><pre class="programlisting">SparkSession spark = SparkSession 
.builder() 
.appName("MarketBasketAnalysis") 
.master("local[*]") 
.config("spark.sql.warehouse.dir", "E:/Exp/") 
.getOrCreate(); 
</pre><p><span class="strong"><strong>Step 3: Create the Java RDD for the transactions</strong></span></p><p>Java RDD for the transactions can be created with the help of the following code:</p><pre class="programlisting">String transactionsFileName = "Input/groceries.data"; 
RDD&lt;String&gt; transactions = spark.sparkContext().textFile(transactionsFileName, 1); 
transactions.saveAsTextFile("output/transactions"); 
</pre><p><span class="strong"><strong>Step 4: Create a method for creating the list</strong></span></p><p>Create a method named <code class="literal">toList</code> from the created transactions RDDs, which will add all the items in the transactions:</p><pre class="programlisting">  static List&lt;String&gt; toList(String transaction) { 
    String[] items = transaction.trim().split(","); 
    List&lt;String&gt;list = new ArrayList&lt;String&gt;(); 
    for (String item :items) { 
      list.add(item); 
    } 
    returnlist; 
  } 
</pre><p><span class="strong"><strong>Step 5: Remove infrequent items and null transactions</strong></span></p><p>Create a method named <code class="literal">removeOneItemAndNullTransactions</code> to remove infrequent items and null transactions:</p><pre class="programlisting">static List&lt;String&gt; removeOneItemAndNullTransactions(List&lt;String&gt;list, int i) { 
    if ((list == null) || (list.isEmpty())) { 
      returnlist; 
    } 
    if ((i&lt; 0) || (i&gt; (list.size() - 1))) { 
      returnlist; 
    } 
    List&lt;String&gt;cloned = new ArrayList&lt;String&gt;(list); 
    cloned.remove(i); 
    return cloned; 
  } 
</pre><p><span class="strong"><strong>Step 6: Flat mapping and 1-itemsets creation (map phase 1)</strong></span></p><p>Do the <code class="literal">flatmap</code> and create 1-itemsets. Finally, save the patterns:</p><pre class="programlisting">JavaPairRDD&lt;List&lt;String&gt;, Integer&gt; patterns = transactions.toJavaRDD() 
        .flatMapToPair(new PairFlatMapFunction&lt;String, List&lt;String&gt;, Integer&gt;() { 
          @Override 
  public Iterator&lt;Tuple2&lt;List&lt;String&gt;, Integer&gt;&gt; call(String transaction) { 
  List&lt;String&gt; list = toList(transaction); 
  List&lt;List&lt;String&gt;&gt; combinations = Combination.findSortedCombinations(list); 
  List&lt;Tuple2&lt;List&lt;String&gt;, Integer&gt;&gt; result = new ArrayList&lt;Tuple2&lt;List&lt;String&gt;, Integer&gt;&gt;(); 
for (List&lt;String&gt; combList : combinations) { 
  if (combList.size() &gt; 0) { 
  result.add(new Tuple2&lt;List&lt;String&gt;, Integer&gt;(combList, 1)); 
              } 
            } 
    return result.iterator(); 
          } 
        }); 
    patterns.saveAsTextFile("output/1itemsets"); 
</pre><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note38"></a>Note</h3><p>Note that the last saving of the patterns RDD is for optional reference purposes and so that you can see the contents of the RDDs.</p></div><p>The following is a screenshot of the 1-itemsets:</p><div class="mediaobject"><img src="graphics/B05243_06_14.jpg" /><div class="caption"><p>Figure 14: 1-itemsets</p></div></div><p><span class="strong"><strong>Step 7: Combine and reduce frequent patterns (reduce phase 1)</strong></span></p><p>Combine and reduce all the frequent patterns, and save them:</p><pre class="programlisting">JavaPairRDD&lt;List&lt;String&gt;, Integer&gt; combined = patterns.reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() { 
      public Integer call(Integer i1, Integer i2) { 
        int support = 0; 
        if (i1 + i2 &gt;= 2) { 
          support = i1 + i2; 
        } 
        // if(support &gt;= 2) 
        return support; 
      } 
    }); 
  combined.saveAsTextFile("output/frequent_patterns"); 
</pre><p>The following is a snapshot of the frequent patterns with their respective support (frequency in <span class="emphasis"><em>Figure 15)</em></span>:</p><div class="mediaobject"><img src="graphics/B05243_06_15.jpg" /><div class="caption"><p>Figure 15: Frequent patterns with their respective support (frequency)</p></div></div><p><span class="strong"><strong>Step 8: Generate all the candidate frequent patterns (map phase 2)</strong></span></p><p>Generate all the candidate frequent patterns or sub-patterns by removing the 1-itemsets from the frequent patterns, and finally, save the candidate patterns:</p><pre class="programlisting">JavaPairRDD&lt;List&lt;String&gt;, Tuple2&lt;List&lt;String&gt;, Integer&gt;&gt; candidate-patterns = combined.flatMapToPair( 
new PairFlatMapFunction&lt;Tuple2&lt;List&lt;String&gt;, Integer&gt;, List&lt;String&gt;, Tuple2&lt;List&lt;String&gt;, Integer&gt;&gt;() { 
          @Override 
public Iterator&lt;Tuple2&lt;List&lt;String&gt;, Tuple2&lt;List&lt;String&gt;, Integer&gt;&gt;&gt; call( 
Tuple2&lt;List&lt;String&gt;, Integer&gt; pattern) { 
List&lt;Tuple2&lt;List&lt;String&gt;, Tuple2&lt;List&lt;String&gt;, Integer&gt;&gt;&gt; result = new ArrayList&lt;Tuple2&lt;List&lt;String&gt;, Tuple2&lt;List&lt;String&gt;, Integer&gt;&gt;&gt;(); 
  List&lt;String&gt; list = pattern._1; 
  frequency = pattern._2; 
  result.add(new Tuple2(list, new Tuple2(null, frequency))); 
            if (list.size() == 1) { 
              return result.iterator(); 
            } 
 
  // pattern has more than one item 
  // result.add(new Tuple2(list, new Tuple2(null,size))); 
    for (int i = 0; i &lt; list.size(); i++) { 
    List&lt;String&gt; sublist = removeOneItem(list, i); 
              result.add(new Tuple2&lt;List&lt;String&gt;, Tuple2&lt;List&lt;String&gt;, Integer&gt;&gt;(sublist, 
                  new Tuple2(list, frequency))); 
            } 
            return result.iterator(); 
          } 
        }); 
candidate-patterns.saveAsTextFile("output/sub_patterns"); 
</pre><p>The following is a snapshot of the sub-patterns:</p><div class="mediaobject"><img src="graphics/B05243_06_16.jpg" /><div class="caption"><p>Figure 16: Sub-patterns for the items</p></div></div><p><span class="strong"><strong>Step 9: Combine all the sub-patterns</strong></span></p><p>Combine all the sub-patterns and save them on disk or persist on memory:</p><pre class="programlisting">JavaPairRDD&lt;List&lt;String&gt;, Iterable&lt;Tuple2&lt;List&lt;String&gt;, Integer&gt;&gt;&gt;rules = candidate_patterns.groupByKey(); 
rules.saveAsTextFile("Output/combined_subpatterns"); 
</pre><p>The following is a screenshot of the candidate patterns (sub-patterns) in combined form:</p><div class="mediaobject"><img src="graphics/B05243_06_17.jpg" /><div class="caption"><p>Figure 17: Candidate patterns (sub-patterns) in combined form</p></div></div><p><span class="strong"><strong>Step 10: Generate association rules</strong></span></p><p>Generate all the association rules from the sub-patterns (reduce phase 2) by specifying the <code class="literal">confidence</code> and <code class="literal">lift</code>:</p><pre class="programlisting">JavaRDD&lt;List&lt;Tuple4&lt;List&lt;String&gt;, List&lt;String&gt;, Double, Double&gt;&gt;&gt; assocRules = rules.map( 
        new Function&lt;Tuple2&lt;List&lt;String&gt;, Iterable&lt;Tuple2&lt;List&lt;String&gt;, Integer&gt;&gt;&gt;, List&lt;Tuple4&lt;List&lt;String&gt;, List&lt;String&gt;, Double, Double&gt;&gt;&gt;() { 
          @Override 
public List&lt;Tuple4&lt;List&lt;String&gt;, List&lt;String&gt;, Double, Double&gt;&gt; call( 
Tuple2&lt;List&lt;String&gt;, Iterable&lt;Tuple2&lt;List&lt;String&gt;, Integer&gt;&gt;&gt; in) throws Exception { 
 
List&lt;Tuple4&lt;List&lt;String&gt;, List&lt;String&gt;, Double, Double&gt;&gt; result = new ArrayList&lt;Tuple4&lt;List&lt;String&gt;, List&lt;String&gt;, Double, Double&gt;&gt;(); 
  List&lt;String&gt; fromList = in._1; 
  Iterable&lt;Tuple2&lt;List&lt;String&gt;, Integer&gt;&gt; to = in._2; 
  List&lt;Tuple2&lt;List&lt;String&gt;, Integer&gt;&gt; toList = new ArrayList&lt;Tuple2&lt;List&lt;String&gt;, Integer&gt;&gt;(); 
Tuple2&lt;List&lt;String&gt;, Integer&gt; fromCount = null; 
      for (Tuple2&lt;List&lt;String&gt;, Integer&gt; t2 : to) { 
        // find the "count" object 
      if (t2._1 == null) { 
                fromCount = t2; 
              } else { 
                toList.add(t2); 
              } 
            } 
            if (toList.isEmpty()) { 
              return result; 
            } 
for (Tuple2&lt;List&lt;String&gt;, Integer&gt; t2 : toList) { 
  double confidence = (double) t2._2 / (double) fromCount._2; 
double lift = confidence / (double) t2._2; 
double support = (double) fromCount._2; 
List&lt;String&gt; t2List = new ArrayList&lt;String&gt;(t2._1); 
t2List.removeAll(fromList); 
if (support &gt;= 2.0 &amp;&amp; fromList != null &amp;&amp; t2List != null) { 
  result.add(new Tuple4(fromList, t2List, support, confidence)); 
System.out.println(fromList + "=&gt;" + t2List + "," + support + "," + confidence + "," + lift); 
              } 
            } 
            return result; 
          } 
        }); 
assocRules.saveAsTextFile("output/association_rules_with_conf_lift"); 
</pre><p>The following is the output of the association rules including their confidence and lift. For more details on support, confidence, and lift, refer to the problem statement section.</p><p>[Antecedent=&gt;Consequent], Support, Confidence, Lift:</p><div class="mediaobject"><img src="graphics/B05243_06_18.jpg" /><div class="caption"><p>Figure 18: Association rules including their confidence and lift</p></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec79"></a>Tuning and setting the correct parameters in SAMBA</h3></div></div></div><p>Note that if you attempt to use the default parameter settings as support = 0.1 and confidence = 0.6, you might end up with null rules, or technically, no rules, to be generated. You might be wondering why. Actually, the default support of 0.1 means that in order to generate an association rule, an item must have appeared in at least <span class="emphasis"><em>0.1 * 9385 = 938.5</em></span> transactions or 938.5 times (for the dataset we are using, |N| = 9385).</p><p>However, in this regard, in their book titled <span class="emphasis"><em>Machine Learning with R, Packt Publishing, 2015</em></span>, Brett Lantz at el. argue that there is one way to tackle this issue while setting support. They suggest considering the minimum number of transactions needed before you would consider a pattern's interestingness. Moreover, for example, you could also argue that if an item is purchased twice a day (which is approximately 60 times a month), then it may be non-trivial to consider that transaction.</p><p>From this perspective, it is possible to estimate how to set the value of support needed to find only rules matching at least that many transactions. Consequently, you can set the value of minimum support as 0.006, because 60 out of 9,835 equals 0.006; we'll try setting the support there first.</p><p>On the other hand, setting the minimum confidence also requires a tricky balance and in this regard, again, we would like to refer you to the book by Brett Lantz et al. titled <span class="emphasis"><em>Machine Learning with R, Packt Publishing, 2015</em></span>. If confidence is too low, obviously we might be incredulous with a pretty large of unreliable rules false positive results.</p><p>As a result, the optimum value of the minimum confidence threshold depends heavily on the goals of your analysis. Consequently, if you start with conservative values, you can always reduce them to broaden the search if you aren't finding actionable intelligence. If you set the minimum confidence threshold at 0.25, it means that in order to be included in the results, the rule has to be correct at least 25 percent of the time. This will eliminate the most unreliable rules while allowing some room for us to modify behavior with targeted promotions of the product.</p><p>Now, let's talk about the third parameter, the <code class="literal">lift</code>. Before suggesting how to set the value of the <code class="literal">lift</code>, let's see a practical example of how it might affect the generation of the association rules in the first place. For the third time, we refer to the book by Brett Lantz et al. titled <span class="emphasis"><em>Machine Learning with R, Packt Publishing, 2015</em></span>.</p><p>For example, suppose at a supermarket store, many people often purchase milk and bread together. Therefore, naturally, you would expect to find many transactions that contain both milk and bread. However, if <code class="literal">lift</code> (milk =&gt; bread) is greater than 1, this implies that the two items are found together more often than one would expect by chance. As a consequence, a large <code class="literal">lift</code> value is, therefore, a strong indicator that a rule is important, and reflects a true connection between the items in the transactions.</p><p>In summary, we need to set the values of these parameters carefully, by considering the preceding examples. However, as a standalone model the algorithms might take hours to finish. So, run the application with enough time. Alternatively, reduce the long transaction to reduce the time overhead.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec46"></a>OCR pipeline with Spark</h2></div></div><hr /></div><p>Image processing and computer vision are two classical but still-emerging research areas that often make proper utilization of many types of machine learning algorithms. There are several use cases where the relationships of linking the patterns of image pixels to higher concepts are extremely complex and hard to define, and of course, computationally extensive, too.</p><p>From a practical point of view, it's relatively easier for a human being to recognize if an object is a face, a dog, or letters or characters. However, defining these patterns under certain circumstances is difficult. Additionally, image-related datasets are often noisy.</p><p>In this section, we will develop a model similar to those used at the core of the <span class="strong"><strong>Optical Character Recognition</strong></span> (<span class="strong"><strong>OCR</strong></span>) used as document scanners. This kind of software helps to process paper-based documents by converting printed or handwritten text into an electronic form to be saved in a database.</p><p>When OCR software first processes a document, it divides the paper, or any object, into a matrix such that each cell in the grid contains a single glyph (also known as different graphical shapes), which is just an elaborate way of referring to a letter, symbol, or number, or any contextual information from the paper or the object.</p><p>To demonstrate the OCR pipeline, we will assume that the document contains only alpha characters in English that match glyphs to one of the 26 letters, A to Z. We will use the OCR letter dataset from the UCI Machine Learning Data Repository (<a class="ulink" href="http://archive.ics.uci.edu/ml" target="_blank">http://archive.ics.uci.edu/ml</a> ). The dataset was donated by W. Frey and D. J. Slate et al. To explore the dataset, we have found that the dataset contains 20,000 examples of 26 English alphabet capital letters printed using 20 different randomly reshaped and distorted black-and-white fonts as glyphs of different shapes.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip39"></a>Tip</h3><p>For more information about these data, refer to <span class="emphasis"><em>Letter recognition using Holland-style adaptive classifiers, Machine Learning, Vol. 6, pp. 161-182, by W. Frey and D.J. Slate (1991)</em></span>.</p></div><p>The image shown in <span class="emphasis"><em>Figure 19</em></span> was published by Frey and Slate and provides an example of some of the printed glyphs. Distorted in this way, the letters are challenging for a computer to identify, yet are easily recognized by a human being. The statistical attributes for the top 20 rows are shown in <span class="emphasis"><em>Figure 20</em></span>:</p><div class="mediaobject"><img src="graphics/Capture.jpg" /><div class="caption"><p>Figure 19: Some of the printed glyphs [courtesy of the article titled Letter recognition using Holland-style adaptive classifiers, Machine Learning, Vol. 6, pp. 161-182, by W. Frey and D.J. Slate (1991)]</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec80"></a>Exploring and preparing the data</h3></div></div></div><p>According to the documentation provided by Frey and Slate, when the glyphs are scanned using an OCR reader to the computer they are automatically converted into pixels. Consequently, the 16 statistical attributes mentioned are recorded to the computer too.</p><p>Note that the concentration of black pixels across the various areas of the box where the character is indicated should provide a way to differentiate among the 26 letters of the alphabet using an OCR or a machine learning algorithm to be trained.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip40"></a>Tip</h3><p>To follow along with this example, download the <code class="literal">letterdata.data</code> file from the Packt Publishing website and save it to your project directory working on one or the other directory.</p></div><p>Before reading the data from the Spark working directory, we confirm that we have received the data with the 16 features that define each example of the letter class. As expected, the letter has 26 levels, as shown in <span class="emphasis"><em>Figure 20</em></span>:</p><div class="mediaobject"><img src="graphics/B05243_06_20.jpg" /><div class="caption"><p>Figure 20: A snapshot of the dataset shown as Data Frame</p></div></div><p>Recall that SVM, Naive Baseyan-based classifier, or any other classifier algorithms, along with their associated learners, require all the features to be numeric. Moreover, each feature is scaled to a fairly small interval.</p><p>Also, SVM works well on dense vectorized features and consequently, will perform poorly against the sparse vectorized features. In our case, every feature is an integer. Therefore, we do not need to convert any values into numbers. On the other hand, some of the ranges for these integer variables appear fairly wide.</p><p>In practical cases, it might require that we normalize the data against all few features points.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec81"></a>OCR pipeline with Spark ML and Spark MLlib</h3></div></div></div><p>Because of its accuracy and robustness, let's see whether the SVM is up to the task. As you can see in <span class="emphasis"><em>Figure 17</em></span>, we have a multiclass OCR dataset (with 26 classes, to be more precise); therefore, we need to have a multiclass classification algorithm, for example, the logistic regression model, since the current implementation of liner SVM in Spark does not support the multi-class classification.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip41"></a>Tip</h3><p>Please refer to the following URL for more details: <a class="ulink" href="http://spark.apache.org/docs/latest/mllib-linear-methods.html#linear-support-vector-machines-svms" target="_blank">http://spark.apache.org/docs/latest/mllib-linear-methods.html#linear-support-vector-machines-svms</a>.</p></div><p><span class="strong"><strong>Step 1: Import necessary packages/libraries/APIs</strong></span></p><p>The following is the code to import the necessary packages:</p><pre class="programlisting">import java.util.HashMap; 
import java.util.Map; 
import org.apache.spark.api.java.JavaRDD; 
import org.apache.spark.api.java.function.Function; 
import org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS; 
import org.apache.spark.mllib.evaluation.MulticlassMetrics; 
import org.apache.spark.mllib.evaluation.MultilabelMetrics; 
import org.apache.spark.mllib.linalg.DenseVector; 
import org.apache.spark.mllib.linalg.Vector; 
import org.apache.spark.mllib.regression.LabeledPoint; 
import org.apache.spark.sql.Dataset; 
import org.apache.spark.sql.Row; 
import org.apache.spark.sql.SparkSession; 
import scala.Tuple2; 
</pre><p><span class="strong"><strong>Step 2: Initialize necessary Spark environment</strong></span></p><p>The following is the code to initialize a Spark environment:</p><pre class="programlisting">  static SparkSession spark = SparkSession 
        .builder() 
        .appName("OCRPrediction") 
            .master("local[*]") 
            .config("spark.sql.warehouse.dir", "E:/Exp/"). 
            getOrCreate(); 
</pre><p>Here we set the application name as <code class="literal">OCRPrediction</code>, and the master URL as <code class="literal">local</code>. The Spark session is the entry point of the program. Please set these parameters accordingly.</p><p><span class="strong"><strong>Step 3: Read the data file and create a corresponding Dataset and show the first 20 rows</strong></span></p><p>The following is the code to read the data file:</p><pre class="programlisting">String input = "input/letterdata.data"; 
Dataset&lt;Row&gt; df = spark.read().format("com.databricks.spark.csv").option("header", "true").load(input);  
  df.show();  
</pre><p>For the first 20 rows, please refer to <span class="emphasis"><em>Figure 5</em></span>. As we can see, there are 26 characters presented as single characters that need to be predicted; therefore, we need to assign each character a random double value to align the value to the other features. Therefore, in the next step, that is what we'll do.</p><p><span class="strong"><strong>Step 4: Create a dictionary for assigning each character a double value randomly</strong></span></p><p>The following code is to create a dictionary for assigning each character a double value randomly:</p><pre class="programlisting">final Map&lt;String, Integer&gt;alpha = newHashMap(); 
    intcount = 0; 
    for(chari = 'A'; i&lt;= 'Z'; i++){ 
      alpha.put(i + "", count++); 
      System.out.println(alpha); 
    } 
</pre><p>And here is the mapping output generated from the preceding code segment:</p><div class="mediaobject"><img src="graphics/B05243_06_21.jpg" /><div class="caption"><p>Figure 21: Mapping assignments</p></div></div><p><span class="strong"><strong>Step 5: Creating the labeled point and the feature vector</strong></span></p><p>Create the labeled points and feature vectors for the features combined from the 16 features (that is, 16 columns). Also, save them as Java RDD and dump or cache on disk or memory, and show the sample output:</p><pre class="programlisting">JavaRDD&lt;LabeledPoint&gt; dataRDD = df.toJavaRDD().map(new Function&lt;Row, LabeledPoint&gt;() { 
      @Override 
      public LabeledPoint call(Row row) throws Exception { 
       
        String letter = row.getString(0); 
        double label = alpha.get(letter); 
        double[] features= new double [row.size()]; 
        for(int i = 1; i &lt; row.size(); i++){ 
          features[i-1] = Double.parseDouble(row.getString(i)); 
        } 
        Vector v = new DenseVector(features);         
        return new LabeledPoint(label, v); 
      } 
    }); 
     
dataRDD.saveAsTextFile("Output/dataRDD"); 
System.out.println(dataRDD.collect()); 
</pre><p>If you look carefully at the preceding code segments, we have created an array named features for 16 features altogether and created a dense vector representation, since the dense vector representation is a more compact representation where the contents can be shown as in the following screenshot:</p><div class="mediaobject"><img src="graphics/image_06_025.jpg" /><div class="caption"><p>Figure 22: The Java RDD for the corresponding label and features as vectors</p></div></div><p><span class="strong"><strong>Step 6: Generating the training and test set</strong></span></p><p>Here is the code for generating the test set:</p><pre class="programlisting">JavaRDD&lt;LabeledPoint&gt;[] splits = dataRDD.randomSplit(new double[] {0.7, 0.3}, 12345L); 
JavaRDD&lt;LabeledPoint&gt; training = splits[0]; 
JavaRDD&lt;LabeledPoint&gt; test = splits[1];  
</pre><p>If you wish to see the snaps of the training or test datasets, you should dump or cache them. The following is a sample code for that:</p><pre class="programlisting">training.saveAsTextFile("Output/training"); 
test.saveAsTextFile("Output/test"); 
</pre><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip42"></a>Tip</h3><p>We have randomly generated the training and the test set for the model to be trained and tested. In our case, it was 70% and 30%, respectively, and 11L as the long seed. Readjust the values based on your dataset. Note that if you add a seed to a Random, you get the same results each time you run your code and these come as primes up to 1062348.</p></div><p><span class="strong"><strong>Step 7: Train the model</strong></span></p><p>As you can see, we have a multiclass dataset with 26 classes; therefore, we need to have a multiclass classification algorithm, for example, the logistic regression model:</p><pre class="programlisting">Boolean useFeatureScaling= true; 
final LogisticRegressionModel model = new LogisticRegressionWithLBFGS() 
  .setNumClasses(26).setFeatureScaling(useFeatureScaling) 
  .run(training.rdd()); 
</pre><p>The preceding code segment builds a model using the training datasets by specifying the number of classes (that is, <code class="literal">26</code>) and the feature scaling as <code class="literal">Boolean true</code>. As you can see, we have used the RDD version of the training datasets using <code class="literal">training.rdd()</code>, since the training datasets are in normal vector format.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip43"></a>Tip</h3><p>Spark has the support of the multiclass logistic regression algorithm that supports the <span class="strong"><strong>Limited-Memory-Broyden-Fletcher-Goldfarb-Shanno</strong></span> (<span class="strong"><strong>LBFGS</strong></span>) algorithm. In numerical optimization, the <span class="strong"><strong>Broyden-Fletcher-Goldfarb-Shanno </strong></span>(<span class="strong"><strong>BFGS</strong></span>) algorithm is an iterative method for solving unconstrained nonlinear optimization problems.</p></div><p><span class="strong"><strong>Step 8: Compute the raw scores on the test dataset</strong></span></p><p>Here is the code to compute the raw scores:</p><pre class="programlisting">JavaRDD&lt;Tuple2&lt;Object, Object&gt;&gt; predictionAndLabels = test.map( 
    new Function&lt;LabeledPoint, Tuple2&lt;Object, Object&gt;&gt;() { 
    public Tuple2&lt;Object, Object&gt; call(LabeledPoint p) { 
    Double prediction = model.predict(p.features()); 
    return new Tuple2&lt;Object, Object&gt;(prediction, p.label()); 
          } 
        } 
      );  
predictionAndLabels.saveAsTextFile("output/prd2");  
</pre><p>If you look at the preceding code carefully, you will see that we are actually calculating the predicted features out of the model we created in <span class="emphasis"><em>Step 7</em></span> by making them Java RDD.</p><p><span class="strong"><strong>Step 9: Predict the outcome for label 8.0 (that is, I) and get the evaluation metrics</strong></span></p><p>The following code illustrates how to predict the outcome:</p><pre class="programlisting">MulticlassMetrics metrics = new MulticlassMetrics(predictionAndLabels.rdd()); 
MultilabelMetrics(predictionAndLabels.rdd()); 
System.out.println(metrics.confusionMatrix()); 
double precision = metrics.precision(metrics.labels()[0]); 
double recall = metrics.recall(metrics.labels()[0]); 
double tp = 8.0; 
double TP = metrics.truePositiveRate(tp); 
double FP = metrics.falsePositiveRate(tp); 
double WTP = metrics.weightedTruePositiveRate(); 
double WFP =  metrics.weightedFalsePositiveRate(); 
System.out.println("Precision = " + precision); 
System.out.println("Recall = " + recall); 
System.out.println("True Positive Rate = " + TP); 
System.out.println("False Positive Rate = " + FP); 
System.out.println("Weighted True Positive Rate = " + WTP); 
System.out.println("Weighted False Positive Rate = " + WFP); 
</pre><div class="mediaobject"><img src="graphics/B05243_06_23.jpg" /><div class="caption"><p>Figure 23: Performance metrics for precision and recall</p></div></div><p>Therefore, the precision is 75%, which is obviously not satisfactory. However, if you are still unsatisfied, the following chapter will look at how to tune parameters so that the prediction accuracy increases.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip44"></a>Tip</h3><p>To get an idea about how to calculate the precision, recall, true positive rate, and true negative rate, please refer to the Wikipedia page at <a class="ulink" href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity" target="_blank">https://en.wikipedia.org/wiki/Sensitivity_and_specificity</a>, which discusses the sensitivity and the specificity elaborately. You can also refer to <span class="emphasis"><em>Powers, David M W (2011). Evaluation: From Precision, Recall and F-Measure to ROC, Informedness, Markedness &amp; Correlation(PDF). Journal of Machine Learning Technologies 2 (1): 37-63</em></span>.</p></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec47"></a>Topic modeling using Spark MLlib and ML</h2></div></div><hr /></div><p>The topic modeling technique is widely used in the task of mining text from a large collection of documents. These topics can then be used to summarize and organize documents that include the topic terms and their relative weights. Since the release of Spark 1.3, MLlib supports the LDA, which is one of the most successfully used topic-modeling techniques in the area of text mining and <span class="strong"><strong>Natural Language Processing</strong></span> (<span class="strong"><strong>NLP</strong></span>). Moreover, LDA is also the first MLlib algorithm to adopt Spark GraphX.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip45"></a>Tip</h3><p>To get more information about how the theory behind the LDA works, please refer to <span class="emphasis"><em>David M. Blei, Andrew Y. Ng and Michael I. Jordan, Latent Dirichlet Allocation, Journal of Machine Learning Research 3 (2003) 993-1022</em></span>.</p></div><p><span class="emphasis"><em>Figure 24</em></span> shows the output of the topic distribution from randomly generated tweet text, to be discussed further in <a class="link" href="#" linkend="ch09">Chapter 9</a>, <span class="emphasis"><em>Advanced Machine Learning with Streaming and Graph Data</em></span>. Moreover, we will provide more justification for why we used LDA other than other topic-modeling algorithms in <a class="link" href="#" linkend="ch09">Chapter 9</a>, <span class="emphasis"><em>Advanced Machine Learning with Streaming and Graph Data</em></span>:</p><div class="mediaobject"><img src="graphics/image_06_027.jpg" /><div class="caption"><p>Figure 24: The topic distribution and how it looks like</p></div></div><p>In this section, we will look at an example of topic modeling using the LDA algorithm of Spark MLlib with unstructured raw tweets datasets.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec82"></a>Topic modeling with Spark MLlib</h3></div></div></div><p>In this subsection, we represent a semi-automated technique of topic modeling using Spark. The following steps show the topic modeling from data reading to printing the topics, along with their term-weights. Using other options as defaults, we train LDA on the dataset downloaded from the GitHub URL at <a class="ulink" href="https://github.com/minghui/Twitter-LDA/tree/master/data/Data4Model/test" target="_blank">https://github.com/minghui/Twitter-LDA/tree/master/data/Data4Model/test</a>.</p><p><span class="strong"><strong>Step 1: Load required packages and APIs</strong></span></p><p>Here is the code to load the required packages:</p><pre class="programlisting">import java.io.File; 
import java.io.FileNotFoundException; 
import java.io.Serializable; 
import java.util.ArrayList; 
import java.util.List; 
import java.util.Scanner; 
import org.apache.spark.ml.clustering.LDA; 
import org.apache.spark.ml.clustering.LDAModel; 
import org.apache.spark.ml.feature.ChiSqSelector; 
import org.apache.spark.ml.feature.HashingTF; 
import org.apache.spark.ml.feature.IDF; 
import org.apache.spark.ml.feature.IDFModel; 
import org.apache.spark.ml.feature.RegexTokenizer; 
import org.apache.spark.ml.feature.StopWordsRemover; 
import org.apache.spark.ml.feature.StringIndexer; 
import org.apache.spark.sql.Dataset; 
import org.apache.spark.sql.Row; 
import org.apache.spark.sql.SparkSession; 
import org.apache.spark.sql.types.DataTypes; 
</pre><p><span class="strong"><strong>Step 2: Create a Spark session</strong></span></p><p>Here is the code to create a Spark session:</p><pre class="programlisting">static SparkSession spark = SparkSession 
        .builder() 
        .appName("JavaLDAExample") 
        .master("local[*]") 
        .config("spark.sql.warehouse.dir", "E:/Exp/") 
        .getOrCreate(); 
</pre><p><span class="strong"><strong>Step 3: Reading and seeing the content of the datasets</strong></span></p><p>The following code illustrates how to read and see the content of the datasets:</p><pre class="programlisting">Dataset&lt;Row&gt; df = spark.read().text("input/test/*.txt"); 
</pre><p>Please note that the use of the character <code class="literal">
<span class="emphasis"><em>*</em></span>
</code> indicates to read all the text files in the input/text directory in the project path. If we print the first 20 rows, just use the following code and you will see the following text:</p><pre class="programlisting">df.show(); 
</pre><div class="mediaobject"><img src="graphics/B05243_06_25.jpg" /><div class="caption"><p>Figure 25: First 20 rows of the texts</p></div></div><p>From the preceding screenshot, it is clear that the text files we are using are nothing but very unstructured texts containing the column name label. They therefore need to be pre-processed using the feature transformation using the regular expression tokenizer before we can use them for our purposes.</p><p><span class="strong"><strong>Step 4: Feature Transformers using RegexTokenizer</strong></span></p><p>Here is the code for <code class="literal">RegexTokenizer</code>:</p><pre class="programlisting">RegexTokenizer regexTokenizer1 = new RegexTokenizer().setInputCol("value").setOutputCol("labelText").setPattern("\\t.*$"); 
</pre><p>If you look at the preceding code segment carefully, you will see that we have specified our input column name as <code class="literal">value</code> and our output column name as <code class="literal">labelText</code> and the pattern. Now create another data frame using the regular expression tokenizer we just tokenized using the following code segment:</p><pre class="programlisting">Dataset&lt;Row&gt; labelTextDataFrame = regexTokenizer1.transform(df); 
</pre><p>Now, let's see what the new data frame <code class="literal">labelTextDataFrame</code> contains using the following statement:</p><pre class="programlisting">labelTextDataFrame.show(); 
</pre><div class="mediaobject"><img src="graphics/image_06_029.jpg" /><div class="caption"><p>Figure 26: A new column with characters converted into the corresponding lowercase characters</p></div></div><p>The preceding screenshot (<span class="emphasis"><em>Figure 26</em></span>) shows that the tokenizer created a new column and that mostly, the uppercase words or characters have been converted into the corresponding lowercase characters. Since topic modeling cares about the term-weight and frequency of each input word, we need to separate the words from the label texts, which is done by using the following code segments:</p><pre class="programlisting">RegexTokenizer regexTokenizer2 = new RegexTokenizer().setInputCol("value").setOutputCol("text").setPattern("\\W"); 
</pre><p>Now let's create another data frame and see the results of the transformation using the following code:</p><pre class="programlisting">Dataset&lt;Row&gt; labelFeatureDataFrame = regexTokenizer2.transform(labelTextDataFrame); 
labelFeaturedDataFrame.show(); 
</pre><div class="mediaobject"><img src="graphics/B05243_06_27.jpg" /><div class="caption"><p>Figure 27: Label texts as separate words separated by a comma</p></div></div><p>From the preceding screenshot (<span class="emphasis"><em>Figure 27</em></span>), we can see that a new column, <code class="literal">label</code> has been added, which shows the label texts as separate words, separated by a comma.</p><p>Now, since we have a bunch of texts available, to make the prediction and topic modeling easier, we need to make the indexing for the words we split. But before that, we need to swap the <code class="literal">labelText</code> and <code class="literal">text</code> in a new data frame, as shown in <span class="emphasis"><em>Figure 28</em></span>. To check if it has really happened, just print the newly created data frame:</p><pre class="programlisting">Dataset&lt;Row&gt; newDF = labelFeatureDataFrame 
        .withColumn("labelTextTemp",          labelFeatureDataFrame.col("labelText") 
          .cast(DataTypes.StringType))        .drop(labelFeatureDataFrame.col("labelText")).withColumnRenamed("labelTextTemp", "labelText"); 
newDF.show(); 
</pre><div class="mediaobject"><img src="graphics/B05243_06_28.jpg" /><div class="caption"><p>Figure 28: Swapping the labelText and text in a new data frame</p></div></div><p><span class="strong"><strong>Step 5: Feature transformation by means of string indexer</strong></span></p><p>Here is the code for feature transformation:</p><pre class="programlisting">StringIndexer indexer = new StringIndexer().setInputCol("labelText").setOutputCol("label"); 
</pre><p>Now create a new data frame for the data frame <code class="literal">newDF</code> we created in <span class="emphasis"><em>Step 2</em></span>, and look at the contents of the data frame. Note that we have selected the old column <code class="literal">labelText</code> and set the new column simply as <code class="literal">label</code>:</p><pre class="programlisting">Dataset&lt;Row&gt; indexed = indexer.fit(newDF).transform(newDF); 
indexed.select(indexed.col("labelText"), indexed.col("label"), indexed.col("text")).show(); 
Indexed.show(); 
</pre><div class="mediaobject"><img src="graphics/B05243_06_29.jpg" /><div class="caption"><p>Figure 29: Corresponding labels against the labelText column</p></div></div><p>So, as shown in <span class="emphasis"><em>Figure 29</em></span>, we got a new column, <code class="literal">label</code>, which contains the corresponding labels against the <code class="literal">labelText</code> column. The very next step is about removing the stop words.</p><p><span class="strong"><strong>Step 6: Feature transformation (removing the stop words)</strong></span></p><p>Here is the code for feature transformation for removing the stop words:</p><pre class="programlisting">StopWordsRemover remover = new StopWordsRemover(); 
String[] stopwords = remover.getStopWords(); 
remover.setStopWords(stopwords).setInputCol("text").setOutputCol("filteredWords"); 
</pre><p>The current implementation of the <code class="literal">StopWordsRemover</code> class of Spark contains the following words as stop words. Since we don't have any precondition, we have used those words directly:</p><div class="mediaobject"><img src="graphics/B05243_06_30.jpg" /><div class="caption"><p>Figure 30: A bunch of stop words provided by Spark for text analytics</p></div></div><p><span class="strong"><strong>Step 7: Create a filtered dataset by removing stop words</strong></span></p><p>Here is the code to create a filtered dataset by removing stop words:</p><pre class="programlisting">Dataset&lt;Row&gt; filteredDF = remover.transform(indexed); 
filteredDF.show(); 
filteredDF.select(filteredDF.col("label"), filteredDF.col("filteredWords")).show(); 
</pre><p>Now create a new data frame for the filtered words (that is, excluding the stop words). Let's look at the contents of the filtered dataset:</p><pre class="programlisting">Dataset&lt;Row&gt; featurizedData = hashingTF.transform(filteredDF); 
featurizedData.show(); 
</pre><div class="mediaobject"><img src="graphics/B05243_06_31.jpg" /><div class="caption"><p>Figure 31: Filtered words excluding the stop words</p></div></div><p><span class="strong"><strong>Step 8: Feature extraction using HashingTF</strong></span></p><p>Here is the code for feature extraction using HashingTF:</p><pre class="programlisting">int numFeatures = 5; 
HashingTF hashingTF = new HashingTF().setInputCol("filteredWords").setOutputCol("rawFeatures").setNumFeatures(numFeatures); 
</pre><p>In the previous code, we have done the HashingTF for only five features, for simplicity. Now create another data frame out of the extracted features on the old data frame (that is, <code class="literal">filteredDF</code>) and show the same output:</p><pre class="programlisting">Dataset&lt;Row&gt; featurizedData = hashingTF.transform(filteredDF); 
       featurizedData.show();   
</pre><div class="mediaobject"><img src="graphics/B05243_06_32.jpg" /><div class="caption"><p>Figure 32: Data frame out of the extracted features on the old data frame, filteredDF</p></div></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip46"></a>Tip</h3><p>For more information and API documentation details for feature transformation, estimator, and hashing, please refer to the Spark website at <a class="ulink" href="https://spark.apache.org/docs/latest/ml-features.html" target="_blank">https://spark.apache.org/docs/latest/ml-features.html</a>.</p></div><p><span class="strong"><strong>Step 9: Feature extraction using IDF estimator</strong></span></p><pre class="programlisting">IDF idf = new IDF().setInputCol("rawFeatures").setOutputCol("features"); 
IDFModel idfModel = idf.fit(featurizedData); 
</pre><p>The preceding code creates new features from the raw features by fitting the <code class="literal">idfModel</code> that takes the featured data frame (that is, <code class="literal">featurizedData</code>) in step 5. Now let's create and show a new data frame for the rescaled data using the estimator we just created (that is, <code class="literal">idfModel</code>), which consumes the old data frame for the featurized data (that is, <code class="literal">featurizedData</code>):</p><pre class="programlisting">Dataset&lt;Row&gt; rescaledData = idfModel.transform(featurizedData); 
rescaledData.show(). 
</pre><div class="mediaobject"><img src="graphics/B05243_06_33.jpg" /><div class="caption"><p>Figure 33: The rescaled data using the estimator</p></div></div><p><span class="strong"><strong>Step 10: Chi-Squared feature selection</strong></span></p><p>The Chi-Squared feature selection selects categorical features to use for predicting a categorical label. The following code segment does this selection:</p><pre class="programlisting">ChiSqSelector selector = new org.apache.spark.ml.feature.ChiSqSelector(); 
selector.setNumTopFeatures(5).setFeaturesCol("features").setLabelCol("label").setOutputCol("selectedFeatures"); 
</pre><p>Now create another data frame for the selected features, as follows:</p><pre class="programlisting">Dataset&lt;Row&gt; result = selector.fit(rescaledData).transform(rescaledData); 
result.show(); 
</pre><div class="mediaobject"><img src="graphics/B05243_06_34.jpg" /><div class="caption"><p>Figure 34: Chi-Squared feature selection</p></div></div><p>You can see from the preceding output/screenshot that our data is ready for the LDA model to be trained and do the topic modeling.</p><p><span class="strong"><strong>Step 11: Create and train an LDA model</strong></span></p><p>Create and train the LDA model using the training datasets (that is, the data frame result) by specifying the <span class="emphasis"><em>K</em></span> (which is the number of clusters the topic modeling must have to be &gt;1, where the default value is 10) and max iteration:</p><pre class="programlisting">long value = 5;     
LDA lda = new LDA().setK(10).setMaxIter(10).setSeed(value); 
LDAModel model = lda.fit(result); 
</pre><p>Now that we have the model trained, fitted, and ready for our purposes, let's look at our output. However, before doing that, we need to have a data frame that could capture the topic-related metrics. Use the following code:</p><pre class="programlisting">System.out.println(model.vocabSize()); 
Dataset&lt;Row&gt; topics = model.describeTopics(5); 
org.apache.spark.ml.linalg.Matrix metric = model.topicsMatrix(); 
</pre><p>Now let's see the topics distribution. Look at the previous Dataset:</p><pre class="programlisting">System.out.println(metric); 
topics.show(false); 
</pre><div class="mediaobject"><img src="graphics/B05243_06_35.jpg" /><div class="caption"><p>Figure 35: Corresponding term weight, topic name, and term indices</p></div></div><p>If you look at the preceding output carefully, we have found the corresponding term weight, topic name, and term indices. The preceding terms and their corresponding weights will be used in <a class="link" href="#" linkend="ch09">Chapter 9</a>, <span class="emphasis"><em>Advanced Machine Learning with Streaming and Graph Data</em></span>, for finding connected components using GraphX with Scala.</p><p>However, we also need to have the actual terms. We will show the detailed technique of retrieving the terms in <a class="link" href="#" linkend="ch09">Chapter 9</a>, <span class="emphasis"><em>Advanced Machine Learning with Streaming and Graph Data</em></span> hat extensively depends on the vocabulary of the terms that need to be developed  or generated using the terms from the bag of words concept. </p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec83"></a>Scalability</h3></div></div></div><p>The previous example shows how to perform topic modeling using the LDA algorithm as a standalone application. However, according to a Databricks blog by Joseph B. at <a class="ulink" href="https://databricks.com/blog/2015/03/25/topic-modeling-with-lda-mllib-meets-graphx.html" target="_blank">https://databricks.com/blog/2015/03/25/topic-modeling-with-lda-mllib-meets-graphx.html</a>, the parallelization of LDA is not straightforward, and there have been many research papers proposing different strategies. The key obstacle in this regard is that all methods involve a large amount of communication. According to the blog on the Databricks website, here are the statistics of the dataset and related training and test sets that were used during the experimentation:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Training set size: 4.6 million documents</p></li><li style="list-style-type: disc"><p>Vocabulary size: 1.1 million terms</p></li><li style="list-style-type: disc"><p>Training set size: 1.1 billion tokens (~239 words/document)</p></li><li style="list-style-type: disc"><p>100 topics</p></li><li style="list-style-type: disc"><p>16-worker EC2 cluster, for example, M4.large or M3.medium depending upon budget and requirements</p></li><li style="list-style-type: disc"><p>Timing results: 176 secs/iteration on average over 10 iterations</p></li></ul></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec48"></a>Credit risk analysis pipeline with Spark</h2></div></div><hr /></div><p>In this section, we will develop a credit risk pipeline that is commonly used in financial institutions such as banks and credit unions. First we will discuss what credit risk analysis is and why it is important before developing a Spark ML-based pipeline using a Random-Forest-based classifier. Finally, we will provide some performance improvement suggestions.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec84"></a>What is credit risk analysis? Why is it important?</h3></div></div></div><p>When an applicant applies for a loan and a bank receives that application, based on the applicant's profile, the bank has to make a decision whether to approve the loan application or not.</p><p>In this regard, there are two types of risk associated with the bank's decision on the loan application:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Applicant is a good credit risk</strong></span>: That means the client or applicant is more likely to repay the loan. Then, if the loan is not approved, the bank can potentially suffer loss of business.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Applicant is a bad credit risk</strong></span>: That means that the client or applicant is most likely not to repay the loan. In that case, approving the loan to the client will result in financial loss to the bank.</p></li></ul></div><p>Our common sense says that the second risk is the greater risk, as the bank has a higher chance of not being reimbursed the borrowed amount.</p><p>Therefore, most banks or credit unions evaluate the risks associated with lending money to a client, applicant, or customer. In business analytics, minimizing the risk tends to maximize the profit to the bank itself. In other words, maximizing the profit and minimizing the loss from a financial perspective is important.</p><p>Often, the bank makes a decision about approving a loan application based on different factor and parameters of an applicant. For example, the demographic and socio-economic conditions regarding their loan application.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec85"></a>Developing a credit risk analysis pipeline with Spark ML</h3></div></div></div><p>In this section, we will first discuss the credit risk dataset in detail in order to gain some insight. After that, we will look at how to develop a large-scale credit risk pipeline. Finally, we will provide some performance improvement suggestions toward better prediction accuracy.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec74"></a>The dataset exploration</h4></div></div></div><p>The German Credit dataset was downloaded from the UCI Machine Learning Repository at <a class="ulink" href="https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/" target="_blank">https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/</a>. Although a detailed description of the dataset is available in the link, we provide some brief insights here in <span class="emphasis"><em>Table 3</em></span>. The data contains credit-related data on 21 variables and the classification of whether an applicant is considered a good or a bad credit risk for 1000 loan applicants. <span class="emphasis"><em>Table 3</em></span> shows details about each variable that was considered before making the dataset available online:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Entry</strong></span></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Variable</strong></span></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Explanation</strong></span></p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>1</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">creditability</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Capable of repaying</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>2</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">balance</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Current balance</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>3</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">duration</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Duration of the loan being applied for</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>4</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">history</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Is there any bad loan history?</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>5</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">purpose</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Purpose of the loan</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>6</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">amount</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Amount being applied for</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>7</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">savings</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Monthly saving</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>8</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">employment</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Employment status</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>9</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">instPercent</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Interest percent</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>10</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">sexMarried</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Sex and marriage status</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>11</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">guarantors</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Are there any guarantors?</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>12</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">residenceDuration</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Duration of residence at the current address</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>13</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">assets</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Net assets</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>14</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">age</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Age of the applicant</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>15</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">concCredit</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Concurrent credit</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>16</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">apartment</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Residential status</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>17</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">credits</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Current credits</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>18</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">occupation</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Occupation</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>19</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">dependents</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Number of dependents</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>20</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><code class="literal">hasPhone</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>If the applicant uses a phone</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; ">
<p>21</p>
</td><td style="border-right: 0.5pt solid ; ">
<p><code class="literal">foreign</code></p>
</td><td style="">
<p>If the applicant is a foreigner</p>
</td></tr></tbody></table></div><p>Table 3: German credit dataset properties</p><p>Note that, although <span class="emphasis"><em>Table 3</em></span> describes the variables in the dataset, there is no associated header. In <span class="emphasis"><em>Table 3</em></span>, we have shown the variable, position, and associated significance of each variable.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec86"></a>Credit risk pipeline with Spark ML</h3></div></div></div><p>There will be several steps involved, from data loading, parsing, data preparation, training testing set preparation, model training, model evaluation, and result interpretation. Let's go through the steps one by one.</p><p><span class="strong"><strong>Step 1: Load required APIs and libraries</strong></span></p><p>The following is the code for loading the required APIs and libraries:</p><pre class="programlisting">import org.apache.spark.api.java.JavaRDD; 
import org.apache.spark.api.java.function.Function; 
import org.apache.spark.ml.classification.RandomForestClassificationModel; 
import org.apache.spark.ml.classification.RandomForestClassifier; 
import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator; 
import org.apache.spark.ml.feature.StringIndexer; 
import org.apache.spark.ml.feature.VectorAssembler; 
import org.apache.spark.mllib.evaluation.RegressionMetrics; 
import org.apache.spark.sql.Dataset; 
import org.apache.spark.sql.Row; 
import org.apache.spark.sql.SparkSession; 
</pre><p><span class="strong"><strong>Step 2: Create a Spark session</strong></span></p><p>The following is another code for creating a Spark session:</p><pre class="programlisting">  static SparkSession spark = SparkSession.builder() 
      .appName("CreditRiskAnalysis") 
      .master("local[*]") 
      .config("spark.sql.warehouse.dir", "E:/Exp/") 
      .getOrCreate();  
</pre><p><span class="strong"><strong>Step 3: Load and parse the credit risk dataset</strong></span></p><p>Note that the dataset is in <span class="strong"><strong>Comma-Separated Value</strong></span> (<span class="strong"><strong>CSV</strong></span>) format. Now load and parse the dataset using the Databricks-provided CSV readers and prepare a Dataset of Row, as follows:</p><pre class="programlisting">String csvFile = "input/german_credit.data"; 
Dataset&lt;Row&gt; df = spark.read().format("com.databricks.spark.csv").option("header", "false").load(csvFile); 
</pre><p>Now, show the Dataset to get to know the exact structure, as follows:</p><pre class="programlisting">df.show(); 
</pre><div class="mediaobject"><img src="graphics/image_06_039.jpg" /><div class="caption"><p>Figure 36: A snapshot of the credit risk dataset</p></div></div><p><span class="strong"><strong>Step 4: Create an RDD of type Credit</strong></span></p><p>Create an RDD of typed class <code class="literal">Credit</code>, as follows:</p><pre class="programlisting">JavaRDD&lt;Credit&gt; creditRDD = df.toJavaRDD().map(new Function&lt;Row, Credit&gt;() { 
      @Override 
      public Credit call(Row r) throws Exception { 
        return new Credit(parseDouble(r.getString(0)), parseDouble(r.getString(1)) - 1, 
            parseDouble(r.getString(2)), parseDouble(r.getString(3)), parseDouble(r.getString(4)), 
            parseDouble(r.getString(5)), parseDouble(r.getString(6)) - 1, parseDouble(r.getString(7)) - 1, 
            parseDouble(r.getString(8)), parseDouble(r.getString(9)) - 1, parseDouble(r.getString(10)) - 1, 
            parseDouble(r.getString(11)) - 1, parseDouble(r.getString(12)) - 1, 
            parseDouble(r.getString(13)), parseDouble(r.getString(14)) - 1, 
            parseDouble(r.getString(15)) - 1, parseDouble(r.getString(16)) - 1, 
            parseDouble(r.getString(17)) - 1, parseDouble(r.getString(18)) - 1, 
            parseDouble(r.getString(19)) - 1, parseDouble(r.getString(20)) - 1); 
      } 
    }); 
</pre><p>The preceding code segments creates an RDD of type <code class="literal">Credit</code> after taking the variable as double values by using the <code class="literal">parseDouble()</code> method, which takes a string and returns the corresponding value in <code class="literal">Double</code> format. The <code class="literal">parseDouble()</code> method goes as follows:</p><pre class="programlisting">  public static double parseDouble(String str) { 
    return Double.parseDouble(str); 
  } 
</pre><p>Now we need to know the structure of the <code class="literal">Credit</code> class so that the structure itself helps to create the RDDs using the typed class.  </p><p>Well, the <code class="literal">Credit</code> class is basically a singleton class that initializes all the setter and getter methods for the 21 variables from the dataset through the constructor. Here is the class:</p><pre class="programlisting">public class Credit { 
  private double creditability; 
  private double balance; 
  private double duration; 
  private double history; 
  private double purpose; 
  private double amount; 
  private double savings; 
  private double employment; 
  private double instPercent; 
  private double sexMarried; 
  private double guarantors; 
  private double residenceDuration; 
  private double assets; 
  private double age; 
  private double concCredit; 
  private double apartment; 
  private double credits; 
  private double occupation; 
  private double dependents; 
  private double hasPhone; 
  private double foreign; 
 
  public Credit(double creditability, double balance, double duration, 
  double history, double purpose, double amount, 
      double savings, double employment, double instPercent, 
      double sexMarried, double guarantors, 
      double residenceDuration, double assets, double age, 
      double concCredit, double apartment, double credits, 
      double occupation, double dependents, double hasPhone, double foreign) { 
    super(); 
    this.creditability = creditability; 
    this.balance = balance; 
    this.duration = duration; 
    this.history = history; 
    this.purpose = purpose; 
    this.amount = amount; 
    this.savings = savings; 
    this.employment = employment; 
    this.instPercent = instPercent; 
    this.sexMarried = sexMarried; 
    this.guarantors = guarantors; 
    this.residenceDuration = residenceDuration; 
    this.assets = assets; 
    this.age = age; 
    this.concCredit = concCredit; 
    this.apartment = apartment; 
    this.credits = credits; 
    this.occupation = occupation; 
    this.dependents = dependents; 
    this.hasPhone = hasPhone; 
    this.foreign = foreign; 
  } 
 
  public double getCreditability() { 
    return creditability; 
  } 
 
  public void setCreditability(double creditability) { 
    this.creditability = creditability; 
  } 
 
  public double getBalance() { 
    return balance; 
  } 
 
  public void setBalance(double balance) { 
    this.balance = balance; 
  } 
 
  public double getDuration() { 
    return duration; 
  } 
 
  public void setDuration(double duration) { 
    this.duration = duration; 
  } 
 
  public double getHistory() { 
    return history; 
  } 
 
  public void setHistory(double history) { 
    this.history = history; 
  } 
 
  public double getPurpose() { 
    return purpose; 
  } 
 
  public void setPurpose(double purpose) { 
    this.purpose = purpose; 
  } 
 
  public double getAmount() { 
    return amount; 
  } 
 
  public void setAmount(double amount) { 
    this.amount = amount; 
  } 
 
  public double getSavings() { 
    return savings; 
  } 
 
  public void setSavings(double savings) { 
    this.savings = savings; 
  } 
 
  public double getEmployment() { 
    return employment; 
  } 
 
  public void setEmployment(double employment) { 
    this.employment = employment; 
  } 
 
  public double getInstPercent() { 
    return instPercent; 
  } 
 
  public void setInstPercent(double instPercent) { 
    this.instPercent = instPercent; 
  } 
 
  public double getSexMarried() { 
    return sexMarried; 
  } 
 
  public void setSexMarried(double sexMarried) { 
    this.sexMarried = sexMarried; 
  } 
 
  public double getGuarantors() { 
    return guarantors; 
  } 
 
  public void setGuarantors(double guarantors) { 
    this.guarantors = guarantors; 
  } 
 
  public double getResidenceDuration() { 
    return residenceDuration; 
  } 
 
  public void setResidenceDuration(double residenceDuration) { 
    this.residenceDuration = residenceDuration; 
  } 
 
  public double getAssets() { 
    return assets; 
  } 
 
  public void setAssets(double assets) { 
    this.assets = assets; 
  } 
 
  public double getAge() { 
    return age; 
  } 
 
  public void setAge(double age) { 
    this.age = age; 
  } 
 
  public double getConcCredit() { 
    return concCredit; 
  } 
 
  public void setConcCredit(double concCredit) { 
    this.concCredit = concCredit; 
  } 
 
  public double getApartment() { 
    return apartment; 
  } 
 
  public void setApartment(double apartment) { 
    this.apartment = apartment; 
  } 
 
  public double getCredits() { 
    return credits; 
  } 
 
  public void setCredits(double credits) { 
    this.credits = credits; 
  } 
 
  public double getOccupation() { 
    return occupation; 
  } 
 
  public void setOccupation(double occupation) { 
    this.occupation = occupation; 
  } 
 
  public double getDependents() { 
    return dependents; 
  } 
 
  public void setDependents(double dependents) { 
    this.dependents = dependents; 
  } 
 
  public double getHasPhone() { 
    return hasPhone; 
  } 
 
  public void setHasPhone(double hasPhone) { 
    this.hasPhone = hasPhone; 
  } 
 
  public double getForeign() { 
    return foreign; 
  } 
 
  public void setForeign(double foreign) { 
    this.foreign = foreign; 
  } 
} 
</pre><p>If you look at the flow of the class, at first it declares 21 variables for the 21 features in the dataset. Then it initializes them using the constructor. The rest are simple setter and getter methods.</p><p><span class="strong"><strong>Step 5: Create a Dataset of type Row from the RDD of type Credit</strong></span></p><p>The following code shows how to create a Dataset of type Row:</p><pre class="programlisting">Dataset&lt;Row&gt; creditData = spark.sqlContext().createDataFrame(creditRDD, Credit.class); 
</pre><p>Now save the Dataset as a temporary view, or more formally, a table in-memory for query purposes, as follows:</p><pre class="programlisting">creditData.createOrReplaceTempView("credit"); 
</pre><p>Now let's get to know the schema of the table as follows:</p><pre class="programlisting">creditData.printSchema(); 
</pre><div class="mediaobject"><img src="graphics/image_06_040.jpg" /><div class="caption"><p>Figure 37: The schema of the Dataset</p></div></div><p><span class="strong"><strong>Step 6: Create the feature vector using the VectorAssembler</strong></span></p><p>Create a new feature vector for the 21 variables using the <code class="literal">VectorAssembler</code> class of Spark, as follows:</p><pre class="programlisting">VectorAssembler assembler = new VectorAssembler() 
        .setInputCols(new String[] { "balance", "duration", "history", "purpose", "amount", "savings", 
            "employment", "instPercent", "sexMarried", "guarantors", "residenceDuration", "assets", "age", 
            "concCredit", "apartment", "credits", "occupation", "dependents", "hasPhone", "foreign" }) 
        .setOutputCol("features"); 
</pre><p><span class="strong"><strong>Step 7: Create a Dataset by combining and transforming the assembler</strong></span></p><p>Create a Dataset by transforming the assembler using the <code class="literal">creditData</code> Dataset previously created, and print the first top 20 rows of the Dataset, as follows:</p><pre class="programlisting">Dataset&lt;Row&gt; assembledFeatures = assembler.transform(creditData); 
assembledFeatures.show(); 
</pre><div class="mediaobject"><img src="graphics/B05243_06_38-1024x240.jpg" /><div class="caption"><p>Figure 38: Newly created featured Credit Dataset</p></div></div><p><span class="strong"><strong>Step 8: Create label for making predictions</strong></span></p><p>Create a label column out of the creditability column of the preceding Dataset (<span class="emphasis"><em>Figure 38</em></span>), as follows:</p><pre class="programlisting">StringIndexer creditabilityIndexer = new StringIndexer().setInputCol("creditability").setOutputCol("label"); 
Dataset&lt;Row&gt; creditabilityIndexed = creditabilityIndexer.fit(assembledFeatures).transform(assembledFeatures); 
</pre><p>Now let's explore the new Dataset using the <code class="literal">show()</code> method as follows:</p><pre class="programlisting">creditabilityIndexed.show(); 
</pre><div class="mediaobject"><img src="graphics/B05243_06_39-1024x239.jpg" /><div class="caption"><p>Figure 39: Dataset with a new label column</p></div></div><p>From the preceding figure, we can understand that there are only two labels associated with the Dataset, which are 1.0 and 0.0. That signifies the problem as a binary classification problem.</p><p><span class="strong"><strong>Step 9: Prepare the training and test set</strong></span></p><p>Prepare the training and test set as follows:</p><pre class="programlisting">long splitSeed = 12345L; 
Dataset&lt;Row&gt;[] splits = creditabilityIndexed.randomSplit(new double[] { 0.7, 0.3 }, splitSeed); 
Dataset&lt;Row&gt; trainingData = splits[0]; 
Dataset&lt;Row&gt; testData = splits[1]; 
</pre><p>Here, the ratio is 70% and 30% for the training and testing set, respectively, with a long seed value to disallow the random result generation in each iteration.</p><p><span class="strong"><strong>Step 10: Train the Random Forest model</strong></span></p><p>To train the Random Forest model, use the following code:</p><pre class="programlisting">RandomForestClassifier classifier = new RandomForestClassifier() 
        .setImpurity("gini") 
        .setMaxDepth(3) 
        .setNumTrees(20) 
        .setFeatureSubsetStrategy("auto") 
        .setSeed(splitSeed); 
</pre><p>As previously mentioned, the problem is a binary classification problem. Therefore, we will evaluate the Random Forest model using a binary evaluator for the <code class="literal">label</code> column, as follows:</p><pre class="programlisting">RandomForestClassificationModel model = classifier.fit(trainingData); 
BinaryClassificationEvaluator evaluator = new BinaryClassificationEvaluator().setLabelCol("label"); 
</pre><p>Now we need to collect the model performance metric on the test set that goes as follows:</p><pre class="programlisting">Dataset&lt;Row&gt; predictions = model.transform(testData); 
model.toDebugString(); 
</pre><p><span class="strong"><strong>Step 11: Print the performance parameters</strong></span></p><p>We will observe several performance parameters of the binary evaluator, for example, accuracy after fitting the model, <span class="strong"><strong>Mean Square Error</strong></span> (<span class="strong"><strong>MSE</strong></span>), <span class="strong"><strong>Mean Absolutize Error</strong></span> (<span class="strong"><strong>MAE</strong></span>), <span class="strong"><strong>Root Mean Squared Error</strong></span> (<span class="strong"><strong>RMSE</strong></span>), R Squared and explained variable, and so on. Let's do it as follows:</p><pre class="programlisting">double accuracy = evaluator.evaluate(predictions); 
System.out.println("Accuracy after pipeline fitting: " + accuracy); 
RegressionMetrics rm = new RegressionMetrics(predictions); 
System.out.println("MSE: " + rm.meanSquaredError()); 
System.out.println("MAE: " + rm.meanAbsoluteError()); 
System.out.println("RMSE Squared: " + rm.rootMeanSquaredError()); 
System.out.println("R Squared: " + rm.r2()); 
System.out.println("Explained Variance: " + rm.explainedVariance() + "\n"); 
</pre><p>The preceding code segment generates the following output:</p><pre class="programlisting">Accuracy after pipeline fitting: 0.7622000403307129 
MSE: 1.926235109206349E7 
MAE: 3338.3492063492063 
RMSE Squared: 4388.8895055655585 
R Squared: -1.372326447615067 
Explained Variance: 1.1144695981899707E7 
</pre><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec75"></a>Performance tuning and suggestions</h4></div></div></div><p>If you look at the performance metrics in Step 11, it is obvious that the credit risk predictions are not satisfactory, especially in terms of accuracy, which is only 76.22%. That means that for the given test data, our model can predict if there is a credit risk with 76.22% precision. Since we need to be more careful about such sensitive financial sectors, therefore, more accuracy is desired no doubt.</p><p>Now, if you want to increase the prediction performance, you should try training your model using a model other than the Random-Forest-based classifier. For example, a Logistic Regression or NaÃ¯ve Baseyan-based classifier.</p><p>Moreover, you can use the SVM-based classifier or neural-network-based Multilayer Perceptron classifier. In <a class="link" href="#" linkend="ch07">Chapter 7</a>, <span class="emphasis"><em>Tuning Machine Learning Models</em></span>, we will look at how to tune the hyper parameters in order to select the best model.</p></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec49"></a>Scaling the ML pipelines</h2></div></div><hr /></div><p>Data mining and machine learning algorithms impose outstanding challenges on parallel and distributed computing platforms. Furthermore, parallelizing the machine learning algorithms is highly task-specific and often depends on the preceding questions. In <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Introduction to Data Analytics with Spark</em></span>, we discussed and showed how to deploy the same machine learning application on top of a cluster or cloud computing infrastructure (that is, Amazon AWS/EC2).</p><p>Following that method, we can handle datasets with enormous batch sizes or in real time. In addition to this, scaling up the machine learning applications evolves another trade-off such as cost, complexity, run-time, and technical requirements. Furthermore, making task-appropriate algorithm and platform choices for large-scale machine learning requires an understanding of the benefits, trade-offs, and constraints of the available options.</p><p>To handle these issues, in this section, we will provide some theoretical aspects of handling big Datasets for deploying large-scale machine learning applications. However, before going any further, we need to know the answer to some questions. For example:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>How do we collect the big dataset to fulfil our needs?</p></li><li style="list-style-type: disc"><p>How large are the big datasets and how do we handle them?</p></li><li style="list-style-type: disc"><p>How much training data is enough to scale-up the ML application on a big dataset?</p></li><li style="list-style-type: disc"><p>What is an alternative approach if we don't have enough training data?</p></li><li style="list-style-type: disc"><p>What sorts of machine learning algorithms should be used for fulfilling our needs?</p></li><li style="list-style-type: disc"><p>What platform should be chosen for parallel learning?</p></li></ul></div><p>Here we discuss some important aspects of deploying and scaling up a machine learning application that handles the preceding big data challenges, including size, data skewness, cost, and infrastructure.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec87"></a>Size matters</h3></div></div></div><p>Big data is data in volume, variety, veracity, velocity, and value that is too great to process by traditional in-memory computer systems. Scaling up machine learning applications by handling big data involves tasks such as classification, clustering, regression, feature selection, boosted decision trees, and SVMs. How do we handle 1 billion or 1 trillion data instances? Moreover, 5 billion cell phones, social networks such as Twitter produce big datasets in an unprecedented way. On the other hand, crowdsourcing is the reality, which is labeling of 100,000+ data instances within a week.</p><p>In terms of sparsity, Big datasets cannot be too sparse but dense from a content perspective. From the machine learning perspective, to justify this claim, let's think of an example of data labeling. For instance, 1M data instances cannot belong to 1M classes, simply because it's not practical to have 1M classes but more than once data instances belong to a particular class. Therefore, based on the sparsity and size of such a large-scale dataset, making predictive analytics is another challenge, which needs to be considered and handled while scaling up.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec88"></a>Size versus skewness considerations</h3></div></div></div><p>Machine learning also depends on the availability of labeled data, and its trustworthiness is based on the learning task, such as supervised, unsupervised, or semi-supervised. You might have a structured dataset, but with extreme skewness. More specifically, suppose you have 1K labeled and 1M unlabeled data points, so the labeled and unlabeled ratio is 0.1%.</p><p>Therefore, do you think that only the 1K label points are enough to train a supervised model? As another example, suppose, for instance, that you have 1M labeled and 1B unlabeled data points, where the labeled and unlabeled ratio is also 0.1%. Again, the same question arises, which is, is it enough to have only the 1M labels to train a supervised model?</p><p>Now the concern is what can be done or approach using existing labels as only guidance rather than a directive for the semi-supervised clustering, classification, or regression. Alternatively, label more data, either manually, or with a little help from the crowd. For example, suppose someone wants to cluster or classify analysis on a disease. More specifically, suppose we want to classify tweets, if particular tweets indicate an Ebola- or flu-related disease. In this case, we should use the semi-supervised approach for labeling the tweets.</p><p>However, in this case, a dataset might be very skewed, or labeling might be biased. Usually, the training data comes from different users, where an explicit user feedback might often be misleading.</p><p>Therefore, learning from the implicit feedback is a better idea; for example, collecting data by clicking on web search results. In these types of large-scale datasets, the skewness of the training data is hard to detect, as discussed in <a class="link" href="#" linkend="ch04">Chapter 4</a>, <span class="emphasis"><em>Extracting Knowledge through Feature Engineering</em></span>. Therefore, be wary of this skewness in big Datasets.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec89"></a>Cost and infrastructure</h3></div></div></div><p>To scale-up your machine learning application, you will need better infrastructure and computing power to handle such big datasets. Initially, you might want to utilize a local cluster. However, sometimes, the cluster might not be enough to scale-up your ML application if the dataset increases exponentially.</p><p>As discussed in the chapter about deploying the ML pipeline on powerful infrastructure such as Amazon AWS cloud computing like EC2, you will have to go for pay-as-you-go to enjoy the cloud as Platform as a Service and Infrastructure as a Service, even though you use your own ML application as the Software as a Service.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec50"></a>Tips and performance considerations</h2></div></div><hr /></div><p>Spark also supports cross-validation for hyperparameter tuning that will be discussed broadly in the following chapter. Spark's view toward the cross-validation as a meta-algorithm, which fits the underlying estimator with user-specified combinations of parameters, cross-evaluates the fitted models and outputs the best one.</p><p>However, there is no specific requirement of the underlying estimator, which could be a pipeline, as long as it can be paired with an evaluator that outputs a scalar metric, such as precision and recall, from the predictions.</p><p>Let's recall the OCR prediction, where we found that the precision is 75%, which is obviously not satisfactory, once again. To further investigate the reason now, let's prints the confusion matrix for the label 8.0 or "I". If you look at the following matrix in <span class="emphasis"><em>Figure 40</em></span>, you will find the number of correctly predicted instances is low:</p><div class="mediaobject"><img src="graphics/image_06_043.jpg" /><div class="caption"><p>Figure 40: The confusion matrix for the label 8.0 or "I"</p></div></div><p>Now let's try to use the Random Forest model to do the prediction. But before going into the model training step, let's do some initialization of the parameters that are needed for the Random Forest classifier, which also supports multiclass classification such as the logistic regression model with LBFGS:</p><pre class="programlisting">Integer numClasses = 26; 
HashMap&lt;Integer, Integer&gt;categoricalFeaturesInfo = new HashMap&lt;Integer, Integer&gt;(); 
Integer numTrees = 5; // Use more in practice. 
String featureSubsetStrategy = "auto"; // Let the algorithm choose. 
String impurity = "gini"; 
Integer maxDepth = 20; 
Integer maxBins = 40; 
Integer seed = 12345; 
</pre><p>Now train the model by specifying the previous parameters, as follows:</p><pre class="programlisting">final RandomForestModelmodel = RandomForest.trainClassifier(training, numClasses, categoricalFeaturesInfo, numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins, seed); 
</pre><p>Now let's see how it performs. We'll reuse the same code segments that we used in step 9. Refer to the following screenshot:</p><div class="mediaobject"><img src="graphics/B05243_06_41-1024x199.jpg" /><div class="caption"><p>Figure 41: Performance metrics for the precision and recall</p></div></div><div class="mediaobject"><img src="graphics/image_06_045.jpg" /><div class="caption"><p>Figure 42: The improved confusion matrix for the label 8.0 or "I"</p></div></div><p>If you look at <span class="emphasis"><em>Figure 42</em></span>, we have a significant improvement in terms of all the parameters printed, and the precision has been increased from 75.30% to 89.20%. The reason behind this is the improved interpretation of the Random Forest model toward the global maxima calculation for prediction accuracy and the confusion matrix, as shown in <span class="emphasis"><em>Figure 38</em></span>; you will find significant improvements in the number of predicted instances marked by a diagonal arrow.</p><p>Through a process of trial and error, you can settle on a short list of algorithms that show promise, but how do you know which is the best? Moreover, as previously mentioned, it is difficult to find a well-performing machine learning algorithm for your dataset. Therefore, if you are still unsatisfied with the accuracy of 89.20%, I suggest you tune the parametric values and look at the precision and the recall.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec51"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we have shown several machine learning applications and tried to differentiate between Spark MLlib and Spark ML. We also showed that it's really difficult to develop a complete machine learning application using only Spark ML or Spark MLlib.</p><p>However, we would like to argue that a combined approach, or interoperability, between these two APIs would be best for these purposes. In addition, we learned how to build an ML pipeline by using both Spark ML libraries and how to scale-up the basic model by considering some performance considerations.</p><p>Tuning an algorithm or machine learning application can simply be thought of as a process by which one goes through to optimize the parameters that impact the model in order to enable the algorithm to perform at its best (in terms of runtime and memory usage).</p><p>In <a class="link" href="#" linkend="ch07">Chapter 7</a>, <span class="emphasis"><em>Tuning Machine Learning Models</em></span>, we will discuss more about tuning machine learning models. We will try to reuse some of the applications from this chapter and <a class="link" href="#" linkend="ch05">Chapter 5</a>, <span class="emphasis"><em>Supervised and Unsupervised Learning by Examples</em></span>, to tune up the performance by tuning several parameters.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch07"></a>Chapter 7. Tuning Machine Learning Models</h2></div></div></div><p>Tuning an algorithm or a machine learning application is simply a process that one goes through in order to enable the algorithm to perform optimally (in terms of runtime and memory usage) when optimizing the parameters that impact the model. This chapter aims to guide the reader through model tuning. It will cover the main techniques used to optimize an ML algorithm's performance. Techniques will be explained both from the MLlib and Spark ML perspective. In <a class="link" href="#" linkend="ch05">Chapter 5</a>, <span class="emphasis"><em>Supervised and Unsupervised Learning by Examples</em></span> and <a class="link" href="#" linkend="ch06">Chapter 6</a>, <span class="emphasis"><em>Building Scalable Machine Learning Pipelines</em></span>, we described how to develop some complete machine learning applications and pipelines from data collection to model evaluation. In this chapter, we will try to reuse some of those applications to improve the performance by tuning several parameters such as Hyperparameter tuning, Grid search parameter tuning with MLlib and Spark ML, random search parameter tuning and cross-validation. The hypothesis ,which is also an important statistical test, will be discussed. In summary, the following topics will be covered in this chapter:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Details about machine learning model tuning</p></li><li style="list-style-type: disc"><p>Typical challenges in model tuning</p></li><li style="list-style-type: disc"><p>Evaluating machine learning models</p></li><li style="list-style-type: disc"><p>Validation and evaluation techniques</p></li><li style="list-style-type: disc"><p>Parameter tuning for machine learning</p></li><li style="list-style-type: disc"><p>Hypothesis testing</p></li><li style="list-style-type: disc"><p>Machine learning model selection</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec52"></a>Details about machine learning model tuning</h2></div></div><hr /></div><p>Fundamentally, one can argue that the ultimate goal of <span class="strong"><strong>machine learning</strong></span> (<span class="strong"><strong>ML</strong></span>) is to make a machine that can automatically build models from data without requiring tedious and time-consuming human involvement. You will see that one of the difficulties in ML is that learning algorithms are like decision trees, random forests, and clustering techniques that require you to set parameters before you use the models for practical purposes. Alternatively, you need to set some constraints on those parameters.</p><p>How you set those parameters depends on a set of factors and the specification. Your goal in this regard is usually to set those parameters to the optimal values that enable you to complete a learning task in the best possible way. Thus, tuning an algorithm or ML technique can be simply thought of as a process where one goes through a series of steps in which they optimize the parameters that impact the model's performance in order to enable the algorithm to perform in the best way.</p><p>In <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Understanding the Problem by Understanding the Data</em></span>, and <a class="link" href="#" linkend="ch05">Chapter 5</a>, <span class="emphasis"><em>Supervised and Unsupervised Learning by Examples</em></span>, we discussed some techniques to choose the best algorithm based on your data and discuss the most widely used algorithms. To get the best result out of your model, you have to first define what the <span class="emphasis"><em>best</em></span> actually is. We will discuss tuning in both abstract and concrete ways.</p><p>In the abstract sense of machine learning, tuning involves working with variables or based on parameters that have been identified to affect system performance as evaluated by some appropriate metric. Hence, the improved performance reveals which parameter settings are more favorable (that is, tuned) or less favorable (that is, un-tuned). In common sense terms, tuning is essentially selecting the best parameters for an algorithm to optimize its performance, given the working environment of hardware, specific workloads, and so on And tuning in machine learning is an automated process for doing this.</p><p>Well, let's get to the point and make the discussion more concrete through some examples. If you take an ML algorithm for clustering like KNN or K-Means, as a developer/data scientist/data engineer you need to specify the number of K in your model or centroids.</p><p>So the question is 'how can you do this?' Technically, there is no shortcut around the need to tune the model. Computationally a naïve approach would be to try with different values of K as a model and of course observing how it goes to inter and intra-group error as you vary the number of K in your model.</p><p>The second example could be by using the <span class="strong"><strong>Support Vector Machine</strong></span> (<span class="strong"><strong>SVM</strong></span>) for classification tasks. As you know, an SVM classification requires an initial learning phase in which the training data are used to adjust the classification parameters. This really denotes an initial parameter-tuning phase where you might try to tune the models in order to achieve high-quality results.</p><p>The third practical example suggest that there is no such thing as a perfect set of optimizations for all deployments of an Apache web server. A sysadmin learns from the data on the job, so to speak, and optimizes it's own Apache web server configuration as appropriate for its specific environment. Now imagine an automated process for doing those three things, that is, a system that can learn from data on its own; the definition of machine learning. A system that tunes its own parameters in such a data-based fashion would be an instance of tuning in machine learning. Now, let us summarize the main points of why we evaluate the predictive performance of a model:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>We want to estimate the generalization error, the predictive performance of our model on future (unseen) data.</p></li><li style="list-style-type: disc"><p>We want to increase the predictive performance by tweaking the learning algorithm and selecting the best-performing model from a given hypothesis space.</p></li><li style="list-style-type: disc"><p>We want to identify the machine learning algorithm that is best-suited for the problem at hand; thus, we want to compare different algorithms, selecting the best-performing one as well as the best-performing model from the algorithm's hypothesis space.</p></li></ul></div><p>In a nutshell, there are four steps in the process of finding the best parameter set:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Define the parametric space</strong></span>: First, we need to decide the exact parameter values we would like to consider for the algorithm.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Define the cross-validation settings</strong></span>: Secondly we need to decide how to choose the optimal cross-validation folds for the data (to be discussed later in this chapter).</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Define the metric</strong></span>: Thirdly, we need to decide which metric to use for determining the best combination of parameters. For example, accuracy, root means squared error, precision, recall, or f-score, and so on.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Train, evaluate and compare</strong></span>: Fourthly, for each unique combination of the parameter values, cross-validation is carried out and based on the error metric defined by the user in the third step, the best-performing model can be chosen.</p></li></ul></div><p>There are many techniques and algorithms available for model tunings like hyperparameter optimization or model selection, hyperparameter tuning, grid search parameter tuning, random search parameter tuning and <span class="strong"><strong>Cross Validation</strong></span> (<span class="strong"><strong>CV</strong></span>). Unfortunately, the current implementation of Spark has developed only a few of them including Cross Validator and TrainValidationSplit.</p><p>Therefore, we will try to use these two hyperparameters to tune different models including Random Forest, Liner Regression, and Logistic Regression. Some applications from <a class="link" href="#" linkend="ch05">Chapter 5</a>, <span class="emphasis"><em>Supervised and Unsupervised Learning by Examples</em></span> and <a class="link" href="#" linkend="ch06">Chapter 6</a>, <span class="emphasis"><em>Building Scalable Machine Learning Pipelines</em></span> will be re-used without providing many details again to make the model tuning easier.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec53"></a>Typical challenges in model tuning</h2></div></div><hr /></div><p>After the following discussion, you might be thinking that this process is difficult, and you'd be right. In fact, because of the difficulty in determining what optimal model parameters are, often some more complex learning algorithms are used before experimenting effectively with simpler options with better-tuned parameters. As we've already discussed, machine learning involves a lot of experimentation. And the tuning of the internal knobs of a learning algorithm, commonly referred to as hyperparameters, are equally important from model building to prediction and before deployment.</p><p>Technically, running a learning algorithm over a training dataset with different hyperparameter settings will result in different models, and of course different performance parameters. According to Oracle developers, it is not recommended to begin tuning without having first established clear objectives, since you cannot succeed if there is any definition of success.</p><p>Subsequently, we are typically interested in selecting the best-performing model from the training data set; we need to find a way to estimate their respective performances in order to rank them against each other. Going one step beyond mere algorithm fine-tuning, we are usually not only experimenting with the one single algorithm that we think would be the best solution under the given circumstances. More often than not, we want to compare different algorithms to each other, often in terms of predictive and computational performance.</p><p>Often there is a very basic question regarding parameter tuning using grid search and random search. Typically, some machine learning methods have parameters that need to be tuned using either of them. For example, according to Wei Ch. et al., (<span class="emphasis"><em>A General Formulation for Support Vector Machine, proceedings of the 9th International Conference on Neural Information Processing (ICONIP, 02), V-5, 18-22 Nov. 2002</em></span>) the standard formulation of SVMs is as follows:</p><div class="mediaobject"><img src="graphics/B05243_07_01.jpg" /><div class="caption"><p>Figure 1: The standard formula for SVM</p></div></div><p>Now, suppose we need to tune model parameter C, and we need to do it with ease. It's clearly seen from the equation that, tuning C also involves other parameters like <span class="emphasis"><em>xi</em></span>, <span class="emphasis"><em>i</em></span> and <span class="emphasis"><em>w</em></span>; where, the regularization parameter C &gt; 0, is the norm of w. In the RHS it is the stabilizer and <span class="inlinemediaobject"><img src="graphics/B05243_07_15.jpg" /></span> is the empirical loss term depending upon the target function f(xi). In the standard SVMs (either linear SVM or other variants), the regularized functionalities can be minimized further by solving the convex quadratic optimization problem. Once the problem is solved, it guarantees the unique global minimum solution to gain the best predictive performance. Therefore, the whole process is more or less an optimization problem.</p><p>In summary, <span class="emphasis"><em>Figure 2, The model tuning process, consideration, and workflow</em></span> shows the tuning process and its consideration as a workflow:</p><div class="mediaobject"><img src="graphics/B05243_07_02-1024x451.jpg" /><div class="caption"><p>Figure 2: The model tuning process, consideration, and workflow</p></div></div><p>Now, if you are given the raw dataset, you will most probably do the pre-processing and split the dataset into training and test sets. Therefore, to tune hyperparameter C, you need to first split the training set into a validation training set and a validation test set.</p><p>After that, you might try tuning the parameters using the validation training set and validation test set. Then use the best parameters you've got and retrain the model on the complete training set. Now you can perform the testing on the test set as the final step.</p><p>Up to this point your approach seems to be okay, but which of the following two options do you think is better, on average?</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Would it be better to use the final model from validation, which was trained on a validation training set for the final testing?</p></li><li style="list-style-type: disc"><p>Or would it better to use the entire training set and retrain the model with the best parameters from the grid or random search? Although the parameters were not optimised for this set, we have final training data in this case.</p></li></ul></div><p>Are you intending to go for option 1, because the parameters were already optimized on this training (that is, the validation training set) set? Or are you intending to go for option 2 because, although the parameters were not optimized for the training set, you have the final training data in this case? We suggest you go for option 2, but only if you trust your validation setup in option 2. The reason is that you have performed the <span class="strong"><strong>Cross Validation</strong></span> (<span class="strong"><strong>CV</strong></span>) to identify the most general parameters setup or else model selection or whatever you're trying to optimize. These findings should be applied to the entire training set and tested once on the test set.</p><p>Well, suppose you went for option 2; now the second challenge is evolving. How do we estimate the performance of a machine learning model? Technically, you might argue that we should nourish the training data to our learning algorithm to learn the optimal model. Then we could predict the labels based on the test labels. Thirdly, we count the number of wrong predictions on the test dataset to compute the model's error rate.</p><p>That's it? Not so fast my friend! Be contingent on our goal, unfortunately guesstimating the performance of that model is not that insignificant. Maybe we should address the previous question from another angle: why do we care about performance estimation at all? Well, ideally, the estimated performance of a model tells how well it performs on unobserved data - making predictions on future data is often the main problem we want to solve in applications of machine learning or the development of novel algorithms.</p><p>Finally, there are several other challenges depending upon the data structure, problem type, problem domain and appropriate use cases that need to be addressed that you will come across when you start practicing a lot.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec54"></a>Evaluating machine learning models</h2></div></div><hr /></div><p>In this section, we will discuss how to evaluate a machine learning model because you should always evaluate a model to determine if it is ready to perform well consistently, predicting the target for new and future data. Obviously future data might have many unknown target values. Therefore, you need to check performance-related metrics such as the accuracy metric of the ML model on the data. In this regard, you need to provide a dataset containing scores generated from a trained model and then evaluate the model to compute a set of industry-standard evaluation metrics.</p><p>To evaluate a model appropriately, you need to present a sample of data that has been labeled with the target and this data will be used as the ground truth or facts dataset from the training data source. As we have already discussed, evaluating the predictive accuracy of an ML model with the same training dataset might not be useful.</p><p>The reason is that the model itself can remember the training data based on the rewards it receives instead of generalizing from it. Thus, when the ML model training has finished, you can get to know the target values to be predicted from the presented observations in the model. After that you can compare the predicted values that are returned by the ML model you have trained against the known target value.</p><p>Finally, you might be interested in computing the summary metric that shows the performance metrics to indicate how well the predicted and true values match accuracy parameters such as precision, recall, weighted true positive, weighted true negative, lift, and so on. In this section, however, we will particularly discuss how we can evaluate regression, classification (that is, binary classification, multiclass classification) and clustering model in the first place.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec90"></a>Evaluating a regression model</h3></div></div></div><p>Assume you that you are an online currency trader and you work on Forex or Fortrade. Right now you have two currency pairs in mind to buy or sell, for example, GBP/USD and USD/JPY pairs. If you look at these two pairs carefully, you'll see that USD is common in both pairs. Now, if you observe the historical prices of USD, GBP or JPY you can predict the future outcome of whether you should open the trade in buy or sell.</p><p>This kind of problem can be treated as the typical regression problem. Here, the target variable (price in this case) is a continuous numeric value changing over time, based on the market opening time. Therefore, for making predictions on the prices, based on the given feature values of a certain currency (that is, USD, GBP or JPY in this example), we can fit a simple linear regression or logistic regression model.</p><p>In this case, the feature values could be the historical prices and some external factors that drift the value of a certain currency or currency pair. In this way, the trained build model can predict the price of a certain currency.</p><p>The regression models (that is, linear, logistic or generalized liner regression models) can be used for finding or calculating the score of the same dataset we trained, now that the predicted prices of all of the currencies or the historical prices of these three currencies are available. We can further evaluate the performance of the model by analyzing, on average, how much the predicted prices deviate compared to the actual prices. In this way, people can guess whether the predicted price will go up or down and can earn money from online currency websites such as Forex or Fortrade and so on.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec91"></a>Evaluating a binary classification model</h3></div></div></div><p>As we have already discussed in the binary classification scenario, there are only two possible outcomes for the target variable. For example: {0, 1}, {spam, hap}, {B, N}, {false, true} and {negative, positive} and so on. Now assume that you are given a dataset comprising researchers around the world with demographic, socio-economic and employment variables and you would like to predict the Ph.D. scholarship amount (that is, salary level) as a binary variable with the values of, say, {&lt;=1.5K$, &gt;=4.5K$}.</p><p>In this particular example, the negative class would represent the researcher whose salary or scholarship is less than or equal to $1,500 monthly. Consequently, the positive class, on the other hand, represents all other researchers whose salary is more than or equal to $4,500.</p><p>Now, from the problem scenario, it is clear that it is also a regression problem. As a result, you would train a model, score the data, and evaluate the results and see how much it deviates with actual labels. Therefore, in this type of problem, you would perform an experiment to evaluate the performance of a two-class (that is, binary class) logistic regression model, which is one of the most commonly used binary classifier in the area of ML.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec92"></a>Evaluating a multiclass classification model</h3></div></div></div><p>In <a class="link" href="#" linkend="ch06">Chapter 6</a>, <span class="emphasis"><em>Building Scalable Machine Learning Pipelines</em></span>, we developed several applications and pipelines, and you might remember that we also developed a multiclass classification problem for the OCR dataset using logistic regression and showed the result using Multiclass Metrics. In that example, there were 26 classes for 26 characters (that is, from A to Z). We had to predict the class or label of a certain character and whether it really fell under the correct class or labels. This kind of regression problem can be resolved using the multiclass classification method.</p><p>Therefore, in this type of problem, you would perform an experiment to evaluate the performance of a multiclass (that is, more than two class) logistic regression model too.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec93"></a>Evaluating a clustering model</h3></div></div></div><p>Since the clustering models differ expressively from classification and regression models in many different aspects, if you evaluate a clustering model you will find a different set of statistics and performance related metrics for clustering models. The performance metrics that were returned in the clustering model evaluation technique, describe how many data points were assigned to each cluster, the amount of separation between clusters, and how tightly the data points are bunched within each cluster.</p><p>For example, if you recall, in <a class="link" href="#" linkend="ch05">Chapter 5</a>, <span class="emphasis"><em>Supervised and Unsupervised Learning by Examples</em></span>, you found a clustering problem that we discussed and resolved using Spark ML and MLlib in the <span class="emphasis"><em>Unsupervised Learning with Spark: An example section</em></span>. In that particular example, we showed K-Means clustering of the neighborhood using the Saratoga NY Homes dataset, and showed an exploratory analysis based on price and lot size features for possible neighborhoods of houses located in the same area. This kind of problem can be resolved and evaluated using a clustering model. However, the current implementation of Spark does not yet provide any developed algorithm for model evaluation.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec55"></a>Validation and evaluation techniques</h2></div></div><hr /></div><p>There are several widely used terms in machine learning application development that can be a bit tricky and confusing, so let's talk through them and sort them out. These terms include model, target function, hypothesis, confusion matrix, model deployment, induction algorithm, classifier, learning algorithms, cross-validation, and parameters:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Target function</strong></span>: In reinforcement learning or predictive modeling, let's say we focus on modeling an object. The ultimate target is to learn or approximate a specific and unknown but targeted function. The target function is denoted as <span class="emphasis"><em>f(x) = y</em></span>; where <span class="emphasis"><em>x</em></span> and <span class="emphasis"><em>y</em></span> both are variable and <span class="emphasis"><em>f(x)</em></span> is the true function that we want to model and it also signifies that we are trying to maximize or achieve the target value <span class="emphasis"><em>y</em></span>.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Hypothesis</strong></span>: A statistical hypothesis (don't confuse this with the research hypothesis proposed by the researcher) is a generalized function that is testable on the basis of observing a process. The process is similar to the true function that is modeled through a set of random variables from the training dataset. The objective behind the hypothesis testing is that a hypothesis is proposed for measuring the statistical relationship between the two data sets for the example training set and test set, where, both the datasets have to be statistically significant from an idealized (remember not a randomized or normalized model) model.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Learning algorithm</strong></span>: As already stated, the ultimate goal of a machine learning application is to find or approximate the target function. In this continuous process, the learning algorithm is a set of instructions that models the target function using the training dataset. Technically, a learning algorithm often comes with a hypothesis space and formulates the final hypothesis.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Model</strong></span>: A statistical model is a mathematical model that exemplifies a set of assumptions while generating sample and similar data from a larger population. Finally, the model often represents a significantly idealized form for the data-generating process. Also, in the machine learning area, the terms hypothesis and model are often used interchangeably.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Induction algorithm</strong></span>: An induction algorithm takes input specific instances to produce a generalized model beyond these input instances.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Model deployment</strong></span>: Model deployment usually denotes applying an already built and developed model to the real data in order to make a prediction for an example.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Cross-validation</strong></span>: this is a method for estimating accuracy in terms of an error out of a machine learning model by dividing the data into K mutually exclusive subsets or folds of approximately equal size. The model then is trained and tested K times in iteration, each time on the available dataset excluding a fold and then tested on that fold.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Classifier</strong></span>: A classifier is a special case of hypothesis or discrete-valued function that is used to assign the most categorical class labels to particular data points such as the label point.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Regressor</strong></span>: A regressor is also a special case of hypothesis that does the mapping from unlabeled features to a value within a predefined metric space.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Hyperparameters</strong></span>: Hyperparameters are the tuning parameters of a machine learning algorithm to which a learning algorithm fits the training data.</p></li></ul></div><p>According to Pedro D. et al., <span class="emphasis"><em>A Few Useful Things to Know about Machine Learning</em></span> at <a class="ulink" href="http://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf" target="_blank">http://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf</a>, we also need a few more things outlined, as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Representation</strong></span>: a classifier or regressor must be represented in some formal language that a computer can process by creating proper hypothesis spaces.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Evaluation</strong></span>: An evaluation function (that is, objective function or scoring function) is needed to distinguish a good vs bad classifier r regressor, which is used internally by the algorithm by which the model has been build or trained.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Optimization</strong></span>: We also need to have a method for searching among the classifiers or regressor, aiming for the highest scoring one. Thus the optimization is the key to the efficiency of the learner that helps to determine the optimal parameters.</p></li></ul></div><p>In a nutshell, the key formula in learning in an ML algorithm is:</p><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>Learning = Representation + Evaluation + Optimization</em></span></p></blockquote></div><p>Consequently, to validate and evaluate the trained model you need to understand the above terms very clearly so that you can conceptualize the ML problem and the proper uses of the Spark ML and MLlib APIs.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec56"></a>Parameter tuning for machine learning models</h2></div></div><hr /></div><p>In this section, we will discuss tuning parameters and technique for machine learning models such as hyperparameter tuning, random search parameter tuning, and grid search parameter tuning and cross-validation.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec94"></a>Hyperparameter tuning</h3></div></div></div><p>Hyperparameter tuning is a technique for choosing the right combination of parameters based on the performance of presented data. It is one of the fundamental requirements to obtain meaningful and accurate results from machine learning algorithms in practice. For example, suppose we have two hyperparameters to tune for a pipeline presented in <span class="emphasis"><em>Figure 3</em></span>, a Spark ML pipeline model using a logistic regression estimator (dash lines only happen during pipeline fitting).</p><p>We can see that we have put three candidate values for each. Therefore, there would be nine combinations in total. However, only four are shown in the diagram, namely <span class="strong"><strong>Tokenizer</strong></span>, <span class="strong"><strong>HashingTF</strong></span>, <span class="strong"><strong>Transformer</strong></span> and <span class="strong"><strong>Logistic Regression</strong></span> (<span class="strong"><strong>LR</strong></span>). Now we want to find the one that will lead to the model with the best evaluation result eventually. As we have already discussed in <a class="link" href="#" linkend="ch06">Chapter 6</a>, <span class="emphasis"><em>Building Scalable Machine Learning Pipelines,</em></span> the fitted model consists of the tokenizer, the hashing TF feature extractor, and the fitted logistic regression model:</p><div class="mediaobject"><img src="graphics/B05243_07_03-1024x336.jpg" /><div class="caption"><p>Figure 3: Spark ML pipeline model using logistic regression estimator (dash lines only happen during pipeline fitting)</p></div></div><p><span class="emphasis"><em>Figure 3</em></span> shows the typical workflow of the previously mentioned pipeline. The dash line however happens only during the pipeline fitting.</p><p>As mentioned earlier, the fitted pipeline model is a transformer. The transformer can be used for prediction, model validation, and model inspection. In addition, we also argued that one ill-fated distinguishing characteristic of the ML algorithms is that typically they have many hyperparameters that need to be tuned for better performance.</p><p>For example, the degree of regularizations in these hyperparameters is distinctive from the model parameters optimized by the Spark MLlib. As a consequence, it is really hard to guess or measure the best combination of hyperparameters without expert knowledge of the data and the algorithm to use. Since the complex dataset is based on the ML problem type, the size of the pipeline and the number of hyperparameters may grow exponentially (or linearly), the hyperparameter tuning becomes cumbersome even for an ML expert, not to mention that the result of the tuning parameters may become unreliable</p><p>According to Spark API documentation provided at <a class="ulink" href="http://spark.apache.org/docs/latest/ml-guide.html" target="_blank">http://spark.apache.org/docs/latest/ml-guide.html</a>, a unique and uniform API is used for specifying Spark ML Estimators and Transformers. A <code class="literal">ParamMap</code> is a set of (parameter, value) pairs with a <code class="literal">Param</code> as a named parameter with self-contained documentation provided by Spark. Technically, there are two ways for passing the parameters to an algorithm as specified in the following options:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Setting parameters. For example, if an LR is an instance of <code class="literal">LogisticRegression</code> (that is, Estimator), you can call the <code class="literal">setMaxIter()</code> method as follows: <code class="literal">LR.setMaxIter(5)</code>. It essentially fits the model pointing the regression instance as follows: <code class="literal">LR.fit()</code>. In this particular example, there would be at most five iterations.</p></li><li style="list-style-type: disc"><p>The second option involves passing a <code class="literal">ParamMaps</code> to <code class="literal">fit()</code> or <code class="literal">transform()</code> (refer <span class="emphasis"><em>Figure 1</em></span> for details). In this circumstance, any parameters will be overridden by the <code class="literal">ParamMaps</code> previously specified via setter methods in the ML application-specific codes or algorithms.</p></li></ul></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip47"></a>Tip</h3><p>An excellent way to create a shortlist of well-performing algorithms for your dataset is to use the Caret package in R since tuning in Spark is not that robust. Caret is a package in R created and maintained by Max Kuhn from Pfizer. Development started in 2005 and was later made open source and uploaded to CRAN which is actually an acronym which stands for <span class="strong"><strong>Classification And Regression Training </strong></span>(<span class="strong"><strong>CARET</strong></span>). It was initially developed out of the need to run multiple different algorithms for a given problem. Interested readers can have a look at that package for theoretical and practical consideration by visiting: <a class="ulink" href="http://topepo.github.io/caret/index.html" target="_blank">http://topepo.github.io/caret/index.html</a>.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec95"></a>Grid search parameter tuning</h3></div></div></div><p>Suppose you have selected your hyperparameters, and by applying tuning, you now also need to find the features. In this regard, a full grid search of the space of hyperparameters and features is computationally too intensive. Therefore, you need to perform a fold of the K-fold cross-validation instead of a full grid search:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Tune the required hyperparameters using cross-validation on the training set of the fold, using all the available features</p></li><li style="list-style-type: disc"><p>Select the required features using those hyperparameters</p></li><li style="list-style-type: disc"><p>Repeat the computation for each fold in K</p></li><li style="list-style-type: disc"><p>The final model is constructed on all the data using the N most prevalent features that were selected from each fold of CV</p></li></ul></div><p>The interesting thing is that the hyperparameters would also be tuned again using all the data in a cross-validation loop. Would there be a large downside from this method as compared to a full grid search? In essence, I am doing a line search in each dimension of free parameters (finding the best value in one dimension, holding that constant then finding the best in the next dimension), rather than every single combination of parameter settings.</p><p>The most important downside for searching along single parameters instead of optimizing them altogether is that you ignore interactions. It is quite common that, for instance, more than one parameter influences model complexity. In that case, you need to look at their interaction in order to successfully optimize the hyperparameters. Depending on how large your data set is and how many models you compare, optimization strategies that return the maximum observed performance may run into trouble (this is true for both grid search and your strategy).</p><p>The reason is that searching through a large number of performance estimates for the maximum skims the variance of the performance estimate: you may just end up with a model and training/test split combination that accidentally happens to look good. Even worse, you may get several perfect-looking combinations, and the optimization then cannot know which model to choose and thus becomes unstable.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec96"></a>Random search parameter tuning</h3></div></div></div><p>The default method for optimizing tuning parameters in the train is to use a grid search. This approach is usually effective but, in cases when there are many tuning parameters, it can be inefficient. There are a number of models where this can be beneficial in finding reasonable values of the tuning parameters in a relatively short time. However, there are some models where the efficiency in a small search field can cancel out other optimizations. Unfortunately, the current implementation in Spark for hyperparameter tuning does not provide any technique for random search tuning.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip48"></a>Tip</h3><p>In contrast, for example, a number of models in CARET utilize the <span class="emphasis"><em>sub-model trick</em></span> where M tuning parameter combinations are evaluated; potentially far fewer than M model fits are required. This approach is best leveraged when a simple grid search is used. For this reason, it may be inefficient to use a random search. Finally, many of the models wrapped by train have a small number of parameters. The average number of parameters is 1.7. To use a random search, another option is available in <span class="strong"><strong>trainControl</strong></span> called search. Possible values of this argument are <code class="literal">grid</code> and <code class="literal">random</code>. The built-in models contained in CARET contain code to generate random tuning parameter combinations. The total number of unique combinations is specified by the <span class="strong"><strong>tuneLength</strong></span> option to train.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec97"></a>Cross-validation</h3></div></div></div><p>Cross-validation (also called the <span class="strong"><strong>Rotation Estimation</strong></span> (<span class="strong"><strong>RE</strong></span>)) is a model validation technique for assessing the quality of the statistical analysis and results. The target is to make the model generalize towards an independent test set.</p><p>One of the perfect uses of the cross-validation technique is making a prediction from a machine learning model. Technically, it will help if you want to estimate how a predictive model will perform accurately in practice when you deploy it as an ML application.</p><p>During the cross-validation process, a model is usually trained with a dataset of a known type. Conversely, it is tested using a dataset of unknown type. In this regard, cross-validation help describes a dataset to test the model in the training phase using the validation set.</p><p>However, in order to minimize the flaws in the machine learning model such as overfitting and underfitting, the cross-validation technique provides insights into how the model will generalize to an independent set.</p><p>There are two types of cross-validation that can be typed as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Exhaustive cross-validation</strong></span>: This includes leave-p-out cross-validation and leave-one-out cross-validation</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Non-exhaustive cross-validation</strong></span>: This includes the K-fold cross-validation and repeated random sub-sampling validation cross-validation</p></li></ul></div><p>Detailed discussion of these will not be conducted in this book due to page limitation. Moreover, using Spark ML and Spark MLlib, readers will be able to perform the cross-validation following our examples in the next section.</p><p>Except for the time series data, in most of the cases, the researcher/data scientist/data engineer uses 10-fold cross-validation instead of testing on a validation set (where K = 10). This is the most widely used cross-validation technique across the use cases and problem type. Moreover, to reduce the variability, multiple iterations of cross-validation are performed using different partitions; finally, the validation results are averaged over the rounds across.</p><p>Using cross-validation instead of conventional validation has two main advantages outlined as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Firstly, if there is not enough data available to partition across the separate training and test sets, there's the chance of losing significant modelling or testing capability.</p></li><li style="list-style-type: disc"><p>Secondly, the K-fold cross-validation Estimator has a lower variance than a single hold-out set Estimator. This low variance limits the variability and is again very important if the amount of available data is limited.</p></li></ul></div><p>In these circumstances, a fair way to properly estimate the model prediction and related performance is to use cross-validation as a powerful general technique for model selection and validation.</p><p>A more technical example will be shown in the <span class="emphasis"><em>Machine learning model selection</em></span> section. Let's draw a concrete example to illustrate this. Suppose, we need to perform manual features and a parameter selection for the model tuning and, after that, perform a model evaluation with a 10-fold cross-validation on the entire dataset. What would be the best strategy? We would suggest you go for the strategy that provides an optimistic score as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Divide the dataset into training, say 80%, and testing 20% or whatever you chose</p></li><li style="list-style-type: disc"><p>Use the K-fold cross-validation on the training set to tune your model</p></li><li style="list-style-type: disc"><p>Repeat the CV until you find your model optimized and therefore tuned</p></li><li style="list-style-type: disc"><p>Now use your model to predict on the testing set to get an estimate of out-of-model errors</p></li></ul></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec57"></a>Hypothesis testing</h2></div></div><hr /></div><p>Hypothesis testing is a statistical tool used to determine whether a result is statistically significant. Additionally, it can also be used to justify whether the result you received occurred by chance or whether it is an actual result.</p><p>In this regard, moreover, according to Oracle developers at <a class="ulink" href="https://docs.oracle.com/cd/A57673_01/DOC/server/doc/A48506/method.htm" target="_blank">https://docs.oracle.com/cd/A57673_01/DOC/server/doc/A48506/method.htm</a>, a certain workflow would provide better performance tuning. The typical steps they have suggested are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Set clear goals for tuning</p></li><li style="list-style-type: disc"><p>Create minimum repeatable tests</p></li><li style="list-style-type: disc"><p>Test the hypothesis</p></li><li style="list-style-type: disc"><p>Keep all records</p></li><li style="list-style-type: disc"><p>Avoid common errors</p></li><li style="list-style-type: disc"><p>Stop tuning when the objectives are achieved</p></li></ul></div><p>Usually, the observed value tobs of the test statistic T is first computed. After that, the probability, also called p-value, is calculated under the null hypothesis. Finally, if and only if the p-value is less than the significance level (the selected probability) threshold, the null hypothesis is rejected in favor of the alternative hypothesis.</p><p>To find out more, refer to the following publication: <span class="emphasis"><em>R.A. Fisher et al., Statistical Tables for Biological Agricultural and Medical Research, 6th ed., Table IV, Oliver &amp; Boyd, Ltd., Edinburgh</em></span>.</p><p>Here are two rules of thumb (although these may vary for your case depending on data quality and types):</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>If the p-value is p &gt; 0.05, accept your hypothesis. Note that if a deviation is small enough that chance could be the acceptance level. A p-value of 0.6, for example, means that there is a 60% probability of any deviation from the expected result. However, this is within the range of an acceptable deviation.</p></li><li style="list-style-type: disc"><p>If the p-value is p &lt; 0.05, reject your hypothesis by concluding that some factors other than chance are operating for the deviation to be perfect. Similarly, a p-value of 0.01 means that there is only a 1% chance that this deviation is due to chance alone, which means that other factors must be involved, and these need to be addressed.</p></li></ul></div><p>However, these two rules might not be applicable in every hypothesis testing. In the next subsection, we will show an example of hypothesis testing using Spark MLlib.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec98"></a>Hypothesis testing using ChiSqTestResult of Spark MLlib</h3></div></div></div><p>According to the API documentation provided by Apache at <a class="ulink" href="http://spark.apache.org/docs/latest/mllib-statistics.html#hypothesis-testing" target="_blank">http://spark.apache.org/docs/latest/mllib-statistics.html#hypothesis-testing</a>, the current implementation of the Spark MLlib supports Pearson's chi-squared (<span class="emphasis"><em>χ2</em></span>) tests for goodness of fit and independence:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The goodness of the fit, or if the independence test is being conducted, is determined by the input data types</p></li><li style="list-style-type: disc"><p>The goodness of the fit test requires an input of type vector (mostly dense vectors although it works for sparse vectors). On the other hand, the independence test requires a Matrix as an input format</p></li></ul></div><p>In addition to these, Spark MLlib also supports input type RDD [LabeledPoint] to enable feature selection via chi-square independence tests, especially for the SVM or regression based test where the statistical class provides the necessary methods to run Pearson's chi-squared tests.</p><p>Additionally, Spark MLlib provides a 1-sample, 2-sided implementation of the <span class="strong"><strong>Kolmogorov-Smirnov</strong></span> (<span class="strong"><strong>KS</strong></span>) test for equality of probability distributions. Spark MLlib provides online implementations of some tests to support use cases like A/B testing. These tests may be performed on a Spark Streaming DStream [(Boolean, Double)] where the first element of each tuple indicates the control group (false) or treatment group (true) and the second element is the value of an observation.</p><p>However, due to brevity and page limitation, this two testing technique will not be discussed. The following example demonstrates how to run and interpret hypothesis tests through the <code class="literal">ChiSqTestResult</code>. In the example, we will show three tests: goodness of fit of the result on the dense vector created from the breast cancer diagnosis dataset, an independence test for a randomly created matrix and finally, an independence test on a contingency table from the cancer dataset itself.</p><p><span class="strong"><strong> Step 1: Load required packages</strong></span></p><p>Here is the code to load the required packages:</p><pre class="programlisting">import java.io.BufferedReader;
import java.io.FileReader;
import java.io.IOException;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.mllib.linalg.DenseVector;
import org.apache.spark.mllib.linalg.Matrices;
import org.apache.spark.mllib.linalg.Matrix;
import org.apache.spark.mllib.linalg.Vector;
import org.apache.spark.mllib.regression.LabeledPoint;
import org.apache.spark.mllib.stat.Statistics;
import org.apache.spark.mllib.stat.test.ChiSqTestResult;
import org.apache.spark.rdd.RDD;
import org.apache.spark.sql.SparkSession;
import com.example.SparkSession.UtilityForSparkSession;
</pre><p><span class="strong"><strong>Step 2: Create a Spark session</strong></span></p><p>The following code helps us to create a Spark Session:</p><pre class="programlisting">static SparkSession spark = UtilityForSparkSession.mySession();
</pre><p>The implementation of the <code class="literal">UtilityForSparkSession</code> class is as follows:</p><pre class="programlisting">public class UtilityForSparkSession {
  public static SparkSession mySession() {
  SparkSession spark = SparkSession
               .builder()
               .appName("JavaHypothesisTestingOnBreastCancerData ")
               .master("local[*]")
              .config("spark.sql.warehouse.dir", "E:/Exp/")
              .getOrCreate();
    return spark;
  }
}
</pre><p><span class="strong"><strong>Step 3: Perform the goodness of fit test</strong></span></p><p>First we need to prepare a dense vector from a categorical dataset such as the Wisconsin Breast Cancer Diagnosis dataset. As we have already provided many examples on this dataset, we will not discuss the data exploration anymore in this section. The following line of code collects the vectors that we created using the <code class="literal">myVector()</code> method:</p><pre class="programlisting">Vector v = myVector();
</pre><p>Where, the implementation of the <code class="literal">myVector()</code> method goes as follows:</p><pre class="programlisting">public static Vector myVector() throws NumberFormatException, IOException {
BufferedReader br = new BufferedReader(new FileReader(path));
    String line = nulNow let's compute the goodness of the fit. Note, if a second vector tol;
    Vector v = null;
    while ((line = br.readLine()) != null) {
      String[] tokens = line.split(",");
      double[] features = new double[30];
      for (int i = 2; i &lt; features.length; i++) {
        features[i-2] =
                       Double.parseDouble(tokens[i]);
      }
      v = new DenseVector(features);
    }
    return v;
  }
</pre><p>Now let's compute the goodness of the fit. Note, if a second vector to test is not supplied as a parameter, the test runs occur against a uniform distribution automatically:</p><pre class="programlisting">ChiSqTestResult goodnessOfFitTestResult = Statistics.chiSqTest(v);
</pre><p>Now let's print the result of the goodness using the following:</p><pre class="programlisting">System.out.println(goodnessOfFitTestResult + "\n");
</pre><p>Note the summary of the test includes the p-value, degrees of freedom, test statistic, the method used, and the null hypothesis. We got the following output:</p><pre class="programlisting">Chi squared test summary:
method: pearson
degrees of freedom = 29
statistic = 4528.611649568829
pValue = 0.0
</pre><p>There is a very strong presumption against a null hypothesis: the observed follows the same distribution as expected.</p><p>Since the p-value is low enough to be insignificant, consequently we cannot accept the hypothesis based on the data.</p><p><span class="strong"><strong>Step 4: An independence test on the contingency matrix</strong></span></p><p>First let's create a contingency 4x3 matrix randomly. Here, the matrix appears as follows:</p><pre class="programlisting"> ((1.0, 3.0, 5.0, 2.0), (4.0, 6.0, 1.0, 3.5), (6.9, 8.9, 10.5, 12.6))
Matrix mat = Matrices.dense(4, 3, new double[] { 1.0, 3.0, 5.0, 2.0, 4.0, 6.0, 1.0, 3.5, 6.9, 8.9, 10.5, 12.6});
</pre><p>Now let's conduct Pearson's independence test on the input contingency matrix:</p><pre class="programlisting">ChiSqTestResult independenceTestResult = Statistics.chiSqTest(mat);
</pre><p>Now let's evaluate the test result and give the summary of the test including the p-value and the degrees of freedom:</p><pre class="programlisting">System.out.println(independenceTestResult + "\n");
</pre><p>We got the following statistic summarized as follows:</p><pre class="programlisting">Chi squared test summary:
method: pearson
degrees of freedom = 6
statistic = 6.911459343085576
pValue = 0.3291131185252161
No presumption against null hypothesis: the occurrence of the outcomes is statistically independent.
</pre><p><span class="strong"><strong>Step 5: An independence test on the contingency table</strong></span></p><p>First, let's create a contingency table by means of RDDs from the cancer dataset as follows:</p><pre class="programlisting">static String path = "breastcancer/input/wdbc.data";
RDD&lt;String&gt; lines = spark.sparkContext().textFile(path, 2);
JavaRDD&lt;LabeledPoint&gt; linesRDD = lines.toJavaRDD().map(new Function&lt;String, LabeledPoint&gt;() {
    public LabeledPoint call(String lines) {
    String[] tokens = lines.split(",");
    double[] features = new double[30];
    for (int i = 2; i &lt; features.length; i++) {
    features[i - 2] = Double.parseDouble(tokens[i]);
            }
    Vector v = new DenseVector(features);
    if (tokens[1].equals("B")) {
    return new LabeledPoint(1.0, v); // benign
      } else {
    return new LabeledPoint(0.0, v); // malignant
        }
      }
    });
</pre><p>We have constructed a contingency table from the raw (feature, label) pairs and used it to conduct the independence test. Now let's conduct the test as <code class="literal">ChiSquaredTestResult</code> for every feature against the label:</p><pre class="programlisting">ChiSqTestResult[] featureTestResults = Statistics.chiSqTest(linesRDD.rdd());
</pre><p>Now let's observe the test result against each column (that is, for each 30 feature point) using the following code segment:</p><pre class="programlisting">int i = 1;
for (ChiSqTestResult result : featureTestResults) {
System.out.println("Column " + i + ":");
System.out.println(result + "\n");
i++;
}

Column 1:
Chi-squared test summary:
method: Pearson
degrees of freedom = 455
statistic = 513.7450859274513
pValue = 0.02929608473276224
Strong presumption against null hypothesis: the occurrence of the outcomes is statistically independent.

Column 2:
Chi-squared test summary:
method: Pearson
degrees of freedom = 478
statistic = 498.41630331377735
pValue = 0.2505929829141742
No presumption against null hypothesis: the occurrence of the outcomes is statistically independent.

Column 3:
Chi-squared test summary:
method: Pearson
degrees of freedom = 521
statistic = 553.3147340697276
pValue = 0.1582572931194156
No presumption against null hypothesis: the occurrence of the outcomes is statistically independent.
.
.
Column 30:
Chi-squared test summary:
method: Pearson
degrees of freedom = 0
statistic = 0.0
pValue = 1.0
No presumption against null hypothesis: the occurrence of the outcomes is statistically independent.
</pre><p>From this result, we can see that for some feature points (that is, column) we have a large p-value compared to the others. Readers are, therefore, advised to select the proper dataset and do the hypothesis test prior to applying the hyperparameter tuning. There is no concrete example in this regard since the result may vary against the datasets you have.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec99"></a>Hypothesis testing using the Kolmogorov–Smirnov test from Spark MLlib</h3></div></div></div><p>Since Spark release 1.1.0, Spark also provides the facility of doing the hypothesis testing using for the real-time streaming data through the Kolmogorov-Smirnov test. Where, the probability of obtaining a test statistic result (at least as extreme as the one) that was actually observed. It actually assumes that the null hypothesis is always true. </p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip49"></a>Tip</h3><p>For more details, interested readers should refer to the Java class (<code class="literal">JavaHypothesisTestingKolmogorovSmirnovTestExample.java</code>) in the Spark distribution under the following directory: <code class="literal">spark-2.0.0-bin-hadoop2.7\examples\src\main\java\org\apache\spark\examples\mllib</code>.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec100"></a>Streaming significance testing of Spark MLlib</h3></div></div></div><p>Other than the Kolmogorov-Smirnov test, Spark also supports Streaming significance testing, which is an online implementation of the hypothesis testing like the A/B testing? These tests can be performed on a Spark streaming using DStream (more technical discussion on this topic will be carried out in <a class="link" href="#" linkend="ch09">Chapter 9</a>, <span class="emphasis"><em>Advanced Machine Learning with Streaming and Graph Data</em></span>).</p><p>The MLlib based streaming significance testing supports the following two parameters:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>peacePeriod</strong></span>: This is the number of initial data points from the stream to ignore. This is actually used to mitigate novelty effects and the quality of the streaming you will be receiving.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>windowSize</strong></span>: This is the number of past batches over which to perform hypothesis testing. If you set its value to 0, it will perform cumulative processing using all the prior batches received and processed.</p></li></ul></div><p>Interested readers should refer to the Spark API documentation at <a class="ulink" href="http://spark.apache.org/docs/latest/mllib-statistics.html#hypothesis-testing" target="_blank">http://spark.apache.org/docs/latest/mllib-statistics.html#hypothesis-testing</a>.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec58"></a>Machine learning model selection</h2></div></div><hr /></div><p>Most machine learning algorithms are dependent on various parameters. When we train a model, we need to provide values for those parameters. The efficacy of the trained model is dependent on the model parameters that we choose. The process of finding out the optimal set of parameters is known as model selection.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec101"></a>Model selection via the cross-validation technique</h3></div></div></div><p>When performing machine learning using Python's scikit-learn library or R, you can often get a reasonably predictive performance by using out-of-the-box settings for your models. However, the payoff can be huge if you invest some time in tuning models to your specific problem and data set.</p><p>However, we also need to consider other issues like overfitting, cross-validation, and bias-variance trade-off. These ideas are central to doing a good job at optimizing the hyperparameters of algorithms.</p><p>In this section, we will explore the concepts behind Hyperparameter optimization and demonstrate the process of tuning and training a logistic regression classifier for the famous Spam Filtering dataset. The goal is to tune and apply a logistic regression to these features in order to predict whether a given email/SMS is spam or not-spam:</p><div class="mediaobject"><img src="graphics/B05243_07_04-1024x428.jpg" /><div class="caption"><p>Figure 4: Model selection via cross-validation</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl3sec76"></a>Cross-validation and Spark</h4></div></div></div><p>Pipelines enable model selection by tuning an entire Pipeline at once, rather than tuning each element in the Pipeline unconnectedly. See the API documentation at <a class="ulink" href="http://spark.apache.org/docs/latest/ml-guide.html" target="_blank">http://spark.apache.org/docs/latest/ml-guide.html</a>.</p><p>The current implementation of Spark ML supports model selection using the <code class="literal">CrossValidator</code> class. It takes an Estimator, a set of <code class="literal">ParamMaps</code>, and an Evaluator. The model selection task begins with splitting the dataset (that is, splitting it into a set of folds) the folds of which are then used as separate training and test datasets.</p><p>For example, with K=10 folds, the <code class="literal">CrossValidator</code> will generate 10 (training, test) dataset pairs. Each of these uses two thirds (2/3) of the data for the training, and the other third (1/3) for the testing. After that, the <code class="literal">CrossValidator</code> iterates through the set of <code class="literal">ParamMaps</code>. For each <code class="literal">ParamMap</code>, it trains the given Estimator and evaluates it using the available Evaluator. The Evaluator can be a related ML task for example:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><code class="literal">RegressionEvaluator</code> for regression related problems</p></li><li style="list-style-type: disc"><p><code class="literal">BinaryClassificationEvaluator</code> for binary data and its related problems</p></li><li style="list-style-type: disc"><p><code class="literal">MultiClassClassificationEvaluator</code> for a multiclass problem</p></li></ul></div><p>When it comes to the best <code class="literal">ParamMap</code> selection, a default metric is used. Note that the <code class="literal">ParamMap</code> can be also be overridden by the <code class="literal">setMetric()</code> method in each of these evaluators. In contrast, when it comes to the best model selection:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The <code class="literal">ParamMap</code> produces the best evaluation metric</p></li><li style="list-style-type: disc"><p>The evaluations metrics are then averaged over the K folds</p></li><li style="list-style-type: disc"><p>Finally, the best model is selected</p></li></ul></div><p>Once the best <code class="literal">ParamMap</code> and model are selected, the <code class="literal">CrossValidator</code> fits the Estimator using them for the entire dataset.</p><p>To get a clearer insight into the <code class="literal">CrossValidator</code> and to select from a grid of parameters, Spark uses the <code class="literal">ParamGridBuilder</code> utility to construct the parameter grid. For example, suppose the parameter grid has a value of 4 as the <code class="literal">hashingTF.numFeatures</code> and a value of 3 for the LR.<code class="literal">regParam</code>. Also, lets say that the <code class="literal">CrossValidator</code> uses 10 folds.</p><p>Once, these values are multiplying results to 120 (that is, 4*3 *10 = 120), this signifies that a significant amount of different models (that is, 120) are being trained. Therefore, using the <code class="literal">CrossValidator</code> sometimes can be very expensive. Nevertheless, it is also a well-established method for choosing associated performance and hyperparameters that are sounder statistically compared to the heuristic-based hand tuning.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip50"></a>Tip</h3><p>Interested readers may refer to the following three books for more insight:</p><p>Evan R. Sparks et al., <span class="emphasis"><em>Automating Model Search for Large-Scale Machine Learning</em></span>, ACM, 978-1-4503-3651-2/15/08, <a class="ulink" href="http://dx.doi.org/10.1145/2806777.2806945" target="_blank">http://dx.doi.org/10.1145/2806777.2806945</a>.</p><p>Cawley, G. C. &amp; Talbot, N. L on over-fitting in model selection and subsequent selection bias in performance evaluation, <span class="emphasis"><em>The Journal of Machine Learning Research</em></span>, JMLR. org, 2010, 11, 2079-2107.</p><p>N. Japkowicz and M. Shah, <span class="emphasis"><em>Evaluating learning algorithms: a classification perspective</em></span>, Cambridge University Press, 2011.</p></div><p>In the next sub-section, we will show how to perform cross-validation on a dataset for model selection using Spark ML API.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl3sec77"></a>Cross-validation using Spark ML for SPAM filtering a dataset</h4></div></div></div><p>In this sub-section, we will show you how to perform cross-validation on the e-mail spam dataset for model selection. We will use logistic regression in the first place then we will move forward for other models. Finally, we will recommend the most suitable model for e-mail spam classification.</p><p><span class="strong"><strong>Step 1: Import necessary packages/libraries/APIs</strong></span></p><p>Here is the code to import necessary packages/libraries/APIs:</p><pre class="programlisting">import java.io.Serializable;
import java.util.Arrays;
import java.util.logging.Level;
import java.util.logging.Logger;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.ml.Pipeline;
import org.apache.spark.ml.PipelineStage;
import org.apache.spark.ml.classification.LogisticRegression;
import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator;
import org.apache.spark.ml.feature.HashingTF;
import org.apache.spark.ml.feature.Tokenizer;
import org.apache.spark.ml.param.ParamMap;
import org.apache.spark.ml.tuning.CrossValidator;
import org.apache.spark.ml.tuning.CrossValidatorModel;
import org.apache.spark.ml.tuning.ParamGridBuilder;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import scala.Tuple2;
</pre><p><span class="strong"><strong>Step 2: Initialize the necessary Spark environment</strong></span></p><p>The following code helps us to initialize the necessary Spark environment:</p><pre class="programlisting">  static SparkSession spark = SparkSession
        .builder()
        .appName("CrossValidationforSpamFiltering")
        .master("local[*]")
        .config("spark.sql.warehouse.dir", "E:/Exp/")
        .getOrCreate();
</pre><p>Here we set the application name as cross-validation, the master URL as <code class="literal">local[*]</code>, and the Spark session as the entry point of the program. Please set these parameters accordingly. Most importantly, set the warehouse directory as <code class="literal">E:/Exp/</code> and replace it with the appropriate path.</p><p><span class="strong"><strong>Step 3: Prepare a Dataset from the SMS spam dataset</strong></span></p><p>Take the e-mail spam data as your input, prepare a dataset out of the data, using it as raw text, and check if the data was read properly by calling the <code class="literal">show()</code> method:</p><pre class="programlisting">Dataset&lt;Row&gt; df = spark.read().text("input/SMSSpamCollection.txt");
df.show();
</pre><div class="mediaobject"><img src="graphics/B05243_07_05.jpg" /><div class="caption"><p>Figure 5: The top 20 rows</p></div></div><p>To learn more about the data, please refer to the section <span class="emphasis"><em>Pipeline - an example with Spark ML</em></span>, in <a class="link" href="#" linkend="ch04">Chapter 4</a>, <span class="emphasis"><em>Extracting Knowledge through Feature Engineering</em></span>, and the description for the dataset exploration. As you can see in <span class="emphasis"><em>Figure 5</em></span>, The top 20 rows, there are only two labels - spam or ham (that is, it is a binary classification problem) associated along with the text values (that is, each row).</p><p>However, there is no numeric label or ID. Therefore, we need to prepare the dataset (training set) such that the data frame also contains the ID, and labels along with text (that is, value) so that we can prepare a test set and predict the corresponding labels using any classification algorithm (that is, logistic regression) and can decide if our model selection is appropriate.</p><p>However, for doing so we need to prepare the training dataset first. As you can see, the data frame shown above has only one column and, as mentioned previously, we do need to have three columns. If we prepare an RDD from the previous Dataset (that is, <code class="literal">df</code>), it would be easier for us to make that transformation.</p><p><span class="strong"><strong>Step 4: Create Java pairs of RDDs to store the rows and indices</strong></span></p><p>Create a Java pair of RDDs by converting (transforming) the DataFrame (that is, <code class="literal">df</code>) to Java RDD and by zipping the index:</p><pre class="programlisting">JavaPairRDD&lt;Row, Long&gt; rowRDD = df.toJavaRDD().zipWithIndex();
</pre><p><span class="strong"><strong>Step 5: Create LabeledDocument RDDs</strong></span></p><p>Create <code class="literal">LabeledDocument</code> Java RDDs by splitting the dataset based on two labels and converting the text label to a numeric label (that is, <code class="literal">1.0</code> if ham otherwise 0.0). Note that the <code class="literal">LabeledDocument</code> is a user-defined class that is discussed in <span class="emphasis"><em>step 6</em></span>:</p><pre class="programlisting">JavaRDD&lt;LabeledDocument&gt; splitedRDD = rowRDD.map(new Function&lt;Tuple2&lt;Row, Long&gt;, LabeledDocument&gt;() {
@Override
public LabeledDocument call(Tuple2&lt;Row, Long&gt; v1) throws Exception {
  Row r = v1._1;
  long index = v1._2;
  String[] split = r.getString(0).split("\t");
  if(split[0].equals("ham"))
    return new LabeledDocument(index,split[1], 1.0);
  else
    return new LabeledDocument(index,split[1], 0.0);
      }
    });
</pre><p><span class="strong"><strong>Step 6: Prepare the training dataset</strong></span></p><p>Prepare the training dataset from the <code class="literal">LabeledDocument</code> RDDs using the <code class="literal">createDataFrame()</code> method and by specifying the class. Finally, see the data frame structure using the <code class="literal">show()</code> method as follows:</p><pre class="programlisting">Dataset&lt;Row&gt; training = spark.createDataFrame(splitedRDD, LabeledDocument.class);
training.show(false);
</pre><div class="mediaobject"><img src="graphics/B05243_07_06.jpg" /><div class="caption"><p>Figure 6: The newly created label and ID from the dataset; where ID is the number of row.</p></div></div><p>From <span class="emphasis"><em>Figure 6</em></span>, <span class="emphasis"><em>The newly created label and ID from the dataset; where ID is the number of rows</em></span>, we can see that the new training dataset has three columns: ID, test, and label. It can actually be done by adding a new ID against each row (that is, each line) of the original document. Let's create a class, called <code class="literal">Document</code> for our purpose, that should set a unique ID against each line of text. The structure of the class can be something like the following:</p><pre class="programlisting">public class Document implements Serializable {
  private long id;
  private String text;
  //Initialise the constructor that should take two parameters: id and text//
  Set the id
  Get the id
  Set the text
  Get the text
  }
</pre><p>Now let's look at the structure of the class constructor:</p><pre class="programlisting">  public LabeledDocument (long id, String text) {
      this.id = id;
           this.text = text;
  }
</pre><p>The setter and getter for the ID could be something like this:</p><pre class="programlisting">  public void setId(long id) {
    this.id = id;
  }
  public long getId() {
    return this.id;
  }
</pre><p>Similarly the setter and getter methods for the text could be something like this:</p><pre class="programlisting">  public String getText() {
    return this.text;
  }

  public void setText(String text) {
    this.text = text;
  }
</pre><p>Therefore, if we summarize, the <code class="literal">Document</code> class could be something like this:</p><pre class="programlisting">import java.io.Serializable;
public class Document implements Serializable {
  private long id;
  private String text;
  public Document(long id, String text) {
    this.id = id;
    this.text = text;
  }
  public long getId() {
    return this.id;
  }
  public void setId(long id) {
    this.id = id;
  }
  public String getText() {
    return this.text;
  }
  public void setText(String text) {
    this.text = text;
  }
}
</pre><p>The structure of the <code class="literal">LabeledDocument</code> class, on the other hand, can be as follows and that can be extended from the Document class (to be discussed later):</p><p>Now let's look at the structure of the class constructor:</p><pre class="programlisting">  public LabeledDocument(long id, String text, double label) {
    this.label = label;
  }
</pre><p>However, we're not done yet since we will be extending the <code class="literal">Document</code> class we need to inherit the constructor from the <code class="literal">Document</code> class using the <code class="literal">super()</code> method as follows:</p><pre class="programlisting">  public LabeledDocument(long id, String text, double label) {
    super(id, text);
    this.label = label;
  }
</pre><p>Now the setter method can be something like this:</p><pre class="programlisting">  public void setLabel(double label) {
    this.label = label;
  }
</pre><p>And of course the getter method for the label could be something like this:</p><pre class="programlisting">  public double getLabel() {
    return this.label;
  }
</pre><p>Therefore, in a nutshell, the <code class="literal">LabelDocument</code> class is as follows:</p><pre class="programlisting">import java.io.Serializable;
public class LabeledDocument extends Document implements Serializable {
  private double label;
  public LabeledDocument(long id, String text, double label) {
    super(id, text);
    this.label = label;
  }
  public double getLabel() {
    return this.label;
  }
  public void setLabel(double label) {
    this.label = label;
  }
}
</pre><p><span class="strong"><strong>Step 7: Configure an ML pipeline</strong></span></p><p>Configure an ML pipeline, which consists of three stages: <code class="literal">tokenizer</code>, <code class="literal">hashingTF</code>, and <code class="literal">lr</code>:</p><pre class="programlisting">Tokenizer tokenizer = new Tokenizer().setInputCol("text").setOutputCol("words");
HashingTF hashingTF = new HashingTF().setNumFeatures(1000).setInputCol(tokenizer.getOutputCol()).setOutputCol("features");
LogisticRegression lr = new LogisticRegression().setMaxIter(10).setRegParam(0.01);
Pipeline pipeline = new Pipeline().setStages(new PipelineStage[] { tokenizer, hashingTF, lr });
</pre><p><span class="strong"><strong>Step 8: Construct a grid of parameters to search over</strong></span></p><p>Currently, Spark uses a <code class="literal">ParamGridBuilder</code> to construct a grid of parameters to search over. In this regard, suppose we have three values for <code class="literal">hashingTF.numFeatures</code> and two values for <code class="literal">lr.regParam</code>, this grid will have 3 x 2 = 6 parameter settings for the <code class="literal">CrossValidator</code> to choose from:</p><pre class="programlisting">ParamMap[] paramGrid = new ParamGridBuilder()
.addGrid(hashingTF.numFeatures(), new int[] { 10, 100, 1000 }).addGrid(lr.regParam(), new double[] { 0.1, 0.01 })
 .build();
</pre><p>We now treat the pipeline as an estimator, wrapping it in a <code class="literal">CrossValidator</code> instance. This will allow us to jointly choose parameters for all Pipeline stages. A <code class="literal">CrossValidator</code> requires an Estimator, a set of Estimator <code class="literal">ParamMaps</code>, and an Evaluator. Note that the evaluator here is a <code class="literal">BinaryClassificationEvaluator</code> and its default metric is <code class="literal">areaUnderROC</code>.</p><p><span class="strong"><strong>Step 9: Create a CrossValidator instance</strong></span></p><p>Here is the code to create a <code class="literal">CrossValidator</code> instance:</p><pre class="programlisting">    CrossValidator cv = new CrossValidator()
        .setEstimator(pipeline)
        .setEvaluator(new BinaryClassificationEvaluator())
        .setEstimatorParamMaps(paramGrid)
        .setNumFolds(5); // 5-fold cross validation
</pre><p><span class="strong"><strong>Step 10: Run cross-validation</strong></span></p><p>Run the cross-validation and choose the best set of parameters. Just use the following code segments:</p><pre class="programlisting">CrossValidatorModel cvModel = cv.fit(training);
</pre><p>Now your <code class="literal">CrossValidator</code> model is ready to perform the prediction. However, before that, we need a test set or validation set. Now let's prepare a sample test set. Just create a dataset using the following code segment:</p><pre class="programlisting">    Dataset&lt;Row&gt; test = spark.createDataFrame(Arrays.asList(
      new Document(4L, "FreeMsg CALL j k"),
      new Document(5L, "Siva  hostel"),
      new Document(6L, "darren now"),
     new Document(7L, "Sunshine Quiz! Win a super Sony")),Document.class);
</pre><p>Now let's see the structure of the test set by calling the <code class="literal">show()</code> method in <span class="emphasis"><em>Figure 7</em></span>:</p><div class="mediaobject"><img src="graphics/B05243_07_07.jpg" /><div class="caption"><p>Figure 7: The test set</p></div></div><p><span class="strong"><strong>Step 11: Create a dataset to collect the prediction parameters</strong></span></p><p>The following code illustrates how to create a dataset:</p><pre class="programlisting">Dataset&lt;Row&gt; predictions = cvModel.transform(test);
</pre><p><span class="strong"><strong>Step 12: Display the prediction parameters for each text in the test set</strong></span></p><p>With the help of the following code, we can display the prediction parameters:</p><pre class="programlisting">for (Row r : predictions.select("id", "text", "probability", "prediction").collect())
    {
System.out.println("(" + r.get(0) + ", " + r.get(1) + ") --&gt; prob=" + r.get(2) + ", prediction=" + r.get(3));
    }
</pre><div class="mediaobject"><img src="graphics/B05243_07_08.jpg" /><div class="caption"><p>Figure 8: Prediction against each text and ID</p></div></div><p>Therefore, if you compare the results shown in <span class="emphasis"><em>Figure 6</em></span>, <span class="emphasis"><em>The newly created label and ID from the dataset; where ID is the number of rows,</em></span> you'll find that the prediction accuracy increases from more sophisticated methods for measuring predictive accuracy that can be used to identify places where the error rate can be optimized depending on the costs of each type of error.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec102"></a>Model selection via training validation split</h3></div></div></div><p>According to the API documentation provided by Spark at <a class="ulink" href="http://spark.apache.org/docs/latest/ml-guide.html" target="_blank">http://spark.apache.org/docs/latest/ml-guide.html</a>, Spark also offers a <code class="literal">TrainValidationSplit</code> for hyperparameter tuning along with the <code class="literal">CrossValidator</code>. The idea of the <code class="literal">TrainValidationSplit</code> is it only evaluates each combination of the parameters compared to cross-validation that iterates to k times. It is, therefore, computationally less expensive and produces the result more quickly. The results, however, will not be as reliable as the <code class="literal">CrossValidator</code>. There is an exception: if the training dataset is sufficiently large then it can also produce reliable results.</p><p>The theory behind the <code class="literal">TrainValidationSplit</code> is that it takes the following three as inputs:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>An Estimator</p></li><li style="list-style-type: disc"><p>A set of <code class="literal">ParamMap</code><code class="literal">s</code> provided in the <code class="literal">estimatorParamMaps</code> parameter</p></li><li style="list-style-type: disc"><p>An Evaluator</p></li></ul></div><p>Consequently, it begins the model selection by splitting the dataset into two parts using the <code class="literal">trainRatio</code> parameter. The <code class="literal">trainRatio</code> parameter, on the other hand, is used for separate training and test datasets.</p><p>For example, with <span class="emphasis"><em>trainRatio = 0.75</em></span> (the default value is also 0.75), the <code class="literal">TrainValidationSplit</code> algorithm generates a training and testing pair. In that case, 75% of the total data is used for training the model. Consequently, the rest of the 25% is used as the validation set.</p><p>Similar to the <code class="literal">CrossValidator</code>, the <code class="literal">TrainValidationSplit</code> also iterates through a set of ParamMaps as mentioned earlier. For each combination of the parameters, it trains the given Estimator in each iteration.</p><p>Consequently, the model is evaluated using the given Evaluator. After that, the best model is selected as the best option, since the <code class="literal">ParamMap</code> produces the best evaluation metric and thereby eases the model selection. The <code class="literal">TrainValidationSplit</code> finally fits the Estimator using the best available <code class="literal">ParamMap</code> and for the entire dataset.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl3sec78"></a>Linear regression–based model selection for an OCR dataset</h4></div></div></div><p>In this sub-section, we will show how to perform train validation split tuning for OCR data. The logistic regression will be used in the first place; then we will move forward for other models. Finally, we will recommend the most suitable parameters for the OCR data classification. </p><p><span class="strong"><strong>Step 1: Import necessary packages/libraries/APIs:</strong></span></p><pre class="programlisting">import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator;
import org.apache.spark.ml.param.ParamMap;
import org.apache.spark.ml.regression.LinearRegression;
import org.apache.spark.ml.tuning.ParamGridBuilder;
import org.apache.spark.ml.tuning.TrainValidationSplit;
import org.apache.spark.ml.tuning.TrainValidationSplitModel;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;</pre><p><span class="strong"><strong>Step 2: Initialize necessary Spark environment</strong></span></p><pre class="programlisting">  SparkSession spark = SparkSession
          .builder()
          .appName("TrainSplitOCR")
              .master("local[*]")
               .config("spark.sql.warehouse.dir",
                 "E:/Exp/")
              .getOrCreate();
</pre><p>Here we set the application name as <code class="literal">TrainValidationSplit</code>, the master URL as <code class="literal">local[*]</code>, and the Spark Context is the entry point of the program. Please set these parameters accordingly.</p><p><span class="strong"><strong>Step 3: Prepare the OCR data as a libsvm format</strong></span></p><p>If you recall <span class="emphasis"><em>Figure 19</em></span> in <a class="link" href="#" linkend="ch06">Chapter 6</a>, <span class="emphasis"><em>Building Scalable Machine Learning Pipelines,</em></span> you will remember the data as follows in <span class="emphasis"><em>Figure 9</em></span>, <span class="emphasis"><em>A snapshot of the original OCR dataset as a Data Frame</em></span>:</p><div class="mediaobject"><img src="graphics/B05243_07_09.jpg" /><div class="caption"><p>Figure 9: A snapshot of the original OCR dataset as a Data Frame</p></div></div><p>However, the current implementation of the <code class="literal">TrainValidationSplitModel</code> API only works on datasets that are already in <code class="literal">libsvm</code> format.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip51"></a>Tip</h3><p>Interested readers should refer to the following research article for more in depth knowledge: Chih-Chung Chang and Chih-Jen Lin, <span class="emphasis"><em>LIBSVM - A Library for Support Vector Machines</em></span>. ACM Transactions on Intelligent Systems and Technology, 2:27:1--27:27, 2011. The software is available at <a class="ulink" href="http://www.csie.ntu.edu.tw/~cjlin/libsvm" target="_blank">http://www.csie.ntu.edu.tw/~cjlin/libsvm</a>.</p></div><p>Therefore, we do need to convert the dataset from the current tab separated OCR data to a <code class="literal">libsvm</code> format.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip52"></a>Tip</h3><p>Readers should use the dataset provided with Packt packages or can convert the CSV/CSV file to the corresponding <code class="literal">libsvm</code> format. Interested readers can refer to our public script provided on GitHub at <a class="ulink" href="https://github.com/rezacsedu/CSVtoLibSVMConverterinR" target="_blank">https://github.com/rezacsedu/CSVtoLibSVMConverterinR</a> that directly converts a CSV file to <code class="literal">libsvm</code> format. Just properly show the input and output file path and run the script on your RStudio.</p></div><p><span class="strong"><strong>Step 4: Prepare the OCR data set and also prepare the training and test set</strong></span></p><p>We are assuming that readers already have downloaded the data or have converted the OCR data using our GitHub script or using their own script. Now, take the OCR <code class="literal">libsvm</code> format data as input and prepare the Dataset out of the data as raw texts and check if the data was read properly by calling the <code class="literal">show()</code> method as follows:</p><pre class="programlisting">Dataset&lt;Row&gt; data = spark.read().format("libsvm").load("input/Letterdata_libsvm.data");
data.show(false);
</pre><div class="mediaobject"><img src="graphics/B05243_07_10.jpg" /><div class="caption"><p>Figure 10: The top 20 rows</p></div></div><pre class="programlisting">// Prepare training and test data.
Dataset&lt;Row&gt;[] splits = data.randomSplit(new double[] {0.9, 0.1}, 12345);
Dataset&lt;Row&gt; training = splits[0];
Dataset&lt;Row&gt; test = splits[1];
</pre><p>To learn more about the data, please refer to the section <span class="emphasis"><em>Pipeline - An Example with Spark ML</em></span>, in <a class="link" href="#" linkend="ch04">Chapter 4</a>, <span class="emphasis"><em>Extracting Knowledge through Feature Engineering</em></span>, and the description of dataset exploration. As you can see in <span class="emphasis"><em>Figure 2</em></span>, <span class="emphasis"><em>Spark ML pipeline model using logistic regression estimator (dash lines only happen during pipeline fitting)</em></span>, there are only two labels (spam or ham) associated along with the text values (that is, each row). However, there is no numeric label or ID.</p><p>Therefore, we need to prepare the dataset (training set) so that the Dataset also contains the ID, and labels along with text (that is, value) so that we can prepare a test set and predict their corresponding labels for any classification algorithm (that is, logistic regression) and can decide if our model selection is appropriate.</p><p>However, in order to do so we first need to prepare the training dataset. As you can see, the data frame shown above has only one column and as previously mentioned we do need to have three columns. If we prepare an RDD from the above Dataset (that is, <code class="literal">df</code>) it would be easier for us to make that transformation.</p><p><span class="strong"><strong>Step 5: Configure an ML pipeline using linear regression</strong></span></p><pre class="programlisting">LinearRegression lr = new LinearRegression();
</pre><p><span class="strong"><strong>Step 6: Construct a grid of parameters to search over</strong></span></p><p>Currently, Spark uses a <code class="literal">ParamGridBuilder</code> to construct a grid of parameters to search over. In this regard, then, with three values for <code class="literal">hashingTF.numFeatures</code> and two values for <code class="literal">lr.regParam</code>, this grid will have 3 x 2 = 6 parameter settings for the <code class="literal">CrossValidator</code> to choose from:</p><pre class="programlisting">ParamMap[] paramGrid = new ParamGridBuilder()
.addGrid(hashingTF.numFeatures(), new int[] { 10, 100, 1000 }).addGrid(lr.regParam(), new double[] { 0.1, 0.01 })
.build();
</pre><p>We now treat the pipeline as an Estimator, wrapping it in a <code class="literal">CrossValidator</code> instance. This will allow us to jointly choose parameters for all pipeline stages. As already discussed, a <code class="literal">CrossValidator</code> requires an Estimator, a set of Estimator <code class="literal">ParamMaps</code>, and an Evaluator.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note53"></a>Note</h3><p>Note that the evaluator here is a <code class="literal">BinaryClassificationEvaluator</code> and its default metric is <code class="literal">areaUnderROC</code>.</p></div><p><span class="strong"><strong>Step 7: Create a TrainValidationSplit instance:</strong></span></p><pre class="programlisting">TrainValidationSplit trainValidationSplit = new TrainValidationSplit()
    .setEstimator(lr)
    .setEvaluator(new MulticlassClassificationEvaluator())
    .setEstimatorParamMaps(paramGrid)
    .setTrainRatio(0.7);
</pre><p>In this case the estimator is simply the linear regression that we created in <span class="emphasis"><em>Step 4</em></span>. A <code class="literal">TrainValidationSplit</code> requires an Estimator, a set of Estimator <code class="literal">ParamMaps</code>, and an Evaluator. In this case, 70% of the data will be used as training and the remaining 30% for the validation.</p><p><span class="strong"><strong>Step 8: Run TrainValidationSplit, and chooses parameters</strong></span></p><p>Run the <code class="literal">TrainValidationSplit</code> and choose the best set of parameters for your problem using
the training set. Just use the following code segment:</p><pre class="programlisting">TrainValidationSplitModel model = trainValidationSplit.fit(training);
</pre><p><span class="strong"><strong>Step 9: Making a prediction on the test set</strong></span></p><p>Make predictions on the test data where model is the model with the combination of parameters that performed best. Finally, to show the predictions, use the following code segment:</p><pre class="programlisting">Dataset&lt;Row&gt; per_param = model.transform(test);
per_param.show(false);
</pre><div class="mediaobject"><img src="graphics/B05243_07_11.jpg" /><div class="caption"><p>Figure 11: Prediction against each feature and label</p></div></div><p>In <span class="emphasis"><em>Figure 11</em></span>, we showed the row prediction against the actual label. The first column is the actual label, the second column signifies the feature vector, and the third column shows the raw prediction based on the feature vectors the <code class="literal">TrainValidationSplitModel</code> created.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl3sec79"></a>Logistic regression-based model selection for the cancer dataset</h4></div></div></div><p>In this sub-section, we will show how to perform train validation split tuning for OCR data. We will use the logistic regression in the first place; then we will move forward for other models. Finally, we will recommend the most suitable parameters for the OCR data classification.</p><p><span class="strong"><strong>Step 1: Import the necessary packages/libraries/APIs</strong></span></p><pre class="programlisting">import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.ml.classification.LogisticRegression;
import org.apache.spark.ml.evaluation.RegressionEvaluator;
import org.apache.spark.ml.feature.LabeledPoint;
import org.apache.spark.ml.linalg.DenseVector;
import org.apache.spark.ml.linalg.Vector;
import org.apache.spark.ml.param.ParamMap;
import org.apache.spark.ml.tuning.ParamGridBuilder;
import org.apache.spark.ml.tuning.TrainValidationSplit;
import org.apache.spark.ml.tuning.TrainValidationSplitModel;
import org.apache.spark.rdd.RDD;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
</pre><p><span class="strong"><strong>Step 2: Initialize the necessary Spark environment</strong></span></p><pre class="programlisting">static SparkSession spark = SparkSession
  .builder()
  .appName("CrossValidationforSpamFiltering")
  .master("local[*]")
  .config("spark.sql.warehouse.dir", "C:/Exp/").
  getOrCreate();
</pre><p>Here we set the application name as <code class="literal">CancerDiagnosis</code>, the master URL as <code class="literal">local[*]</code> and the Spark Context as the entry point of the program. Please set these parameters accordingly.</p><p><span class="strong"><strong>Step 3: Create the Java RDD</strong></span></p><p>Parse the cancer diagnosis data and prepare the Java RDDs for strings:</p><pre class="programlisting">String path = "breastcancer/input/wdbc.data";
RDD&lt;String&gt; lines = spark.sparkContext().textFile(path, 3);
</pre><p><span class="strong"><strong>Step 4: Prepare the cancer diagnosis LabeledPoint RDDs</strong></span></p><p>As already discussed in <a class="link" href="#" linkend="ch06">Chapter 6</a>, <span class="emphasis"><em>Building Scalable Machine Learning Pipelines</em></span>, the cancer diagnosis dataset contains two labels <span class="emphasis"><em>B</em></span> and <span class="emphasis"><em>M</em></span> for Benign and Malignant. However, we need to convert them into a numeric label. Just use the following code to convert all of them from label transforming to <code class="literal">LabeledPoint</code> RDDs preparation:</p><pre class="programlisting">JavaRDD&lt;LabeledPoint&gt; linesRDD = lines.toJavaRDD().map(new Function&lt;String, LabeledPoint&gt;() {
      public LabeledPoint call(String lines) {
      String[] tokens = lines.split(",");
      double[] features = new double[30];
      for (int i = 2; i &lt; features.length; i++) {
          features[i - 2] =
                 Double.parseDouble(tokens[i]);
        }
           Vector v = new DenseVector(features);
           if (tokens[1].equals("B")) {
      return new LabeledPoint(1.0, v); // benign
    } else {
    return new LabeledPoint(0.0, v); // malignant
    }
      }
    });
</pre><div class="mediaobject"><img src="graphics/B05243_07_12.jpg" /><div class="caption"><p>Figure 12: The Label Point RDDs snapshot</p></div></div><p>As you can see in <span class="emphasis"><em>Figure 9</em></span>, <span class="emphasis"><em>The top 20 rows</em></span>, the labels B and M have been converted into 1.0 and 0.0. Now we need to create a data frame out of the label point RDDs.</p><p><span class="strong"><strong>Step 5: Create a Dataset and also prepare the training and test set</strong></span></p><p>Create a Dataset from the previous RDDs (that is, <code class="literal">linesRDD</code>) by specifying the Label Point class:</p><pre class="programlisting">Dataset&lt;Row&gt; data = spark.sqlContext().createDataFrame(linesRDD, LabeledPoint.class);
data.show();
</pre><div class="mediaobject"><img src="graphics/B05243_07_13.jpg" /><div class="caption"><p>Figure 13: The created Dataset showing the top 20 rows.</p></div></div><pre class="programlisting">Dataset&lt;Row&gt;[] splits=data.randomSplit(new double[] {0.8, 0.2});
Dataset&lt;Row&gt; training = splits[0];
Dataset&lt;Row&gt; test = splits[1];</pre><p>Note, that you will have to set the ratio of the random split based on your data and problem type accordingly. </p><p><span class="strong"><strong>Step 6: Configure an ML pipeline using logistic regression:</strong></span></p><pre class="programlisting">LogisticRegression lr = new LogisticRegression();
</pre><p><span class="strong"><strong>Step 7: Construct a grid of parameters to search over</strong></span></p><p>Currently, Spark uses a <code class="literal">ParamGridBuilder</code> to construct a grid of parameters to search over. In this regard, suppose we have three values for <code class="literal">hashingTF.numFeatures</code> and two values for <code class="literal">lr.regParam</code>, this grid will have 3 x 2 = 6 parameter settings for <code class="literal">CrossValidator</code> to choose from:</p><pre class="programlisting">ParamMap[] paramGrid = new ParamGridBuilder()
.addGrid(lr.regParam(), new double[] {0.1, 0.01})
.addGrid(lr.fitIntercept())
.addGrid(lr.elasticNetParam(), new double[] {0.0, 0.5, 1.0})
.build();</pre><p>Note, that you will have to set the values of above parameters based on your data and problem type accordingly.</p><p><span class="strong"><strong>Step 8: Create a TrainValidationSplit instance:</strong></span></p><pre class="programlisting">TrainValidationSplit trainValidationSplit = new TrainValidationSplit()
.setEstimator(lr)
.setEvaluator(new RegressionEvaluator())
.setEstimatorParamMaps(paramGrid)
.setTrainRatio(0.8);
</pre><p>In this case, the estimator is simply the linear regression that we created in <span class="emphasis"><em>Step 4</em></span>. Prepare the cancer diagnosis <code class="literal">LabeledPoint</code> RDDs. A <code class="literal">TrainValidationSplit</code> requires an Estimator, a set of Estimator <code class="literal">ParamMaps</code>, and an Evaluator that supports binary classification since our dataset has only two classes where 80% is used for the purpose of training and the remaining 20% for the validation.</p><p><span class="strong"><strong>Step 9: Run the TrainValidationSplit and choose the parameters</strong></span></p><p>Run the <code class="literal">TrainValidationSplit</code>, and choose the best set of parameters for your problem using the training set. Just use the following code segments:</p><pre class="programlisting">TrainValidationSplitModel model = trainValidationSplit.fit(training);
</pre><p><span class="strong"><strong>Step 10: Make predictions on the test set</strong></span></p><p>Make predictions on the test data where the model is the model with a combination of parameters that performed the best. Finally, show the predictions. Just use the following code segment for doing so:</p><pre class="programlisting">Dataset&lt;Row&gt; per_param = model.transform(test);
per_param.show();
</pre><div class="mediaobject"><img src="graphics/B05243_07_14.jpg" /><div class="caption"><p>Figure 14: Prediction against each feature and label</p></div></div><p>Therefore, if you compare these results with those shown in <span class="emphasis"><em>Figure 6, The newly created label and ID from the dataset; where ID is the number of rows</em></span>, you find that the prediction accuracy increases for more sophisticated methods of measuring predictive accuracy that can be used to identify places where the error rate can be optimized depending on the costs of each type of error.</p></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec59"></a>Summary</h2></div></div><hr /></div><p>Tuning an algorithm or machine learning application can be thought of as simply a process through which one goes when they optimise the parameters that impact the model in order to enable the algorithm to perform the best (in terms of run-time and memory usages). In this chapter, we have shown how to perform ML model tuning using train-validation split and cross-validation techniques of Spark ML.</p><p>We also want to mention that the tuning related support and algorithms are still not well enriched until the date of (14th October 2016) the current release of Spark. Interested readers are encouraged to visit the Spark tuning page at <a class="ulink" href="http://spark.apache.org/docs/latest/ml-tuning.html" target="_blank">http://spark.apache.org/docs/latest/ml-tuning.html</a> for more updates since we believe that more features will be added to the Spark website and they will certainly provide enough documentation.</p><p>In the next chapter, we will discuss how to make your machine learning algorithm or models adaptable for new datasets. This chapter covers advanced machine learning techniques to be able to make algorithms adaptable to new data. It will mainly focus on batch/streaming architectures and on online learning algorithms by using Spark streaming.</p><p>The ultimate target is to bring dynamism to the static machine learning models. Readers will also see how machine learning algorithms learn incrementally over the data; that is to say the models are updated each time they see a new training instance.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch08"></a>Chapter 8.  Adapting Your Machine Learning Models </h2></div></div></div><p>This chapter covers advanced <span class="strong"><strong>machine learning</strong></span> (<span class="strong"><strong>ML</strong></span>) techniques in order to be able to make algorithms adaptable to new data. Readers will also see how machine learning algorithms learn incrementally over the data, that is to say that the models are updated each time they see a new training instance. Learning in dynamic environments by conceding different constraints will also be discussed. In summary, the following topics will be covered in this chapter:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Adapting machine learning models</p></li><li style="list-style-type: disc"><p>Generalization of ML models</p></li><li style="list-style-type: disc"><p>Adapting through incremental algorithms</p></li><li style="list-style-type: disc"><p>Adapting through reusing ML models</p></li><li style="list-style-type: disc"><p>Machine learning in dynamic environments</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec60"></a>Adapting machine learning models</h2></div></div><hr /></div><p>As we discussed earlier, as a part of the ML training process, a model is trained using a set of data (that is, training, test, and validation set). Machine learning models that can adapt to their environments and learn from their experience have attracted consumers and researchers from diverse areas, including computer science, engineering, mathematics, physics, neuroscience, and cognitive science. In this section, we will provide a technical overview of how to adopt machine learning models for the new data and requirements.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec103"></a>Technical overview</h3></div></div></div><p>Technically, the same models might need to be retrained at a later stage if required for the betterment. This is really dependent on several factors, for example, when the new data becomes available, or when the consumer of the API has their own data to train the model or when the data needs to be filtered and the model trained with the subset of data. In these scenarios, ML algorithms should provide enough APIs to provide a convenient way to allow its consumers to produce a client that can be used on a one-time or regular basis, so that they can retrain the model using their own data.</p><p>As a result, the client will be able to evaluate the results of retraining and updating the web service API accordingly. Alternatively, they will be able to use the newly trained model. In this regard, there are several contexts of domain adaptation. However, they differ in the information considered for application type and requirements:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Unsupervised domain adaptation</strong></span>: The learning sample contains a set of labeled source examples, a set of unlabeled source examples, and an unlabeled set of target examples</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Semi-supervised domain adaptation</strong></span>: In this situation, we also consider a small set of labeled target examples</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Supervised domain adaptation</strong></span>: All the examples considered are supposed to be labeled:</p></li></ul></div><div class="mediaobject"><img src="graphics/B05243_08_01.jpg" /><div class="caption"><p>Figure 1: The retraining process overview (dashed line presents the retrain steps)</p></div></div><p>Technically, there should be three alternatives for making the ML models adaptable:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The most widely used machine learning techniques and algorithms include decision trees, decision rules, neural networks, statistical classifiers, and probabilistic graphical models, and they all need to be developed so that they can be adaptable for the new requirements</p></li><li style="list-style-type: disc"><p>Secondly, previous mentioned algorithms or techniques should be generalized so that they can be used with minimum effort</p></li><li style="list-style-type: disc"><p>Moreover, more robust theoretical frameworks and algorithms such as Bayesian learning theory, classical statistical theory, minimum description length theory, and statistical mechanics approaches need to be developed in order to understand computational learning theory</p></li></ul></div><p>The benefits from these three adaptation properties and techniques will provide insight into experimental results that will also guide machine learning communities to contribute towards the different learning algorithms.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec61"></a>The generalization of ML models</h2></div></div><hr /></div><p>In <a class="link" href="#" linkend="ch05">Chapter 5</a>, <span class="emphasis"><em>Supervised and Unsupervised Learning by Example</em></span>, we discussed how and why to generalize the learning algorithm to fit semi-supervised learning, active learning, structured prediction, and reinforcement learning. In this section, we will discuss how to generalize the linear regression algorithm on the <span class="strong"><strong>Optical Character Recognition</strong></span> (<span class="strong"><strong>OCR</strong></span>) dataset to show an example of generalization of the linear regression model.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec104"></a>Generalized linear regression</h3></div></div></div><p>As discussed in <a class="link" href="#" linkend="ch05">Chapter 5</a>, <span class="emphasis"><em>Supervised and Unsupervised Learning by Example</em></span>, the linear regression and logistic regression techniques assume that the output follows a Gaussian distribution. The <span class="strong"><strong>generalized linear models</strong></span> (<span class="strong"><strong>GLMs</strong></span>) on the other hand are specifications of the linear models where the response variable, that is, Yi, follows the linear distribution from an exponential family of distribution.</p><p>Spark's <code class="literal">GeneralizedLinearRegression</code> API allows us a flexible specification of the GLMs. The current implementation of the generalized linear regression in Spark can be used for numerous types of prediction problems, for example, linear regression, Poisson regression, logistic regression, and others.</p><p>However, the limitation is that only a subset of the exponential family distributions is supported in the current implementation of the Spark-based GLM algorithm. In addition, there is another scalability issue that only 4096 features are supported through its <code class="literal">GeneralizedLinearRegression</code> API. Consequently, if the number of features are beyond 4096, the algorithm will throw an exception.</p><p>Fortunately, your models with an increased number of features can be trained using the LinearRegression and LogisticRegression estimators, as shown in several examples in <a class="link" href="#" linkend="ch06">Chapter 6</a>, <span class="emphasis"><em>Building Scalable Machine Learning Pipelines</em></span>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec105"></a>Generalized linear regression with Spark</h3></div></div></div><p>In this sub-section, we discuss a step-by-step example that shows how to apply the generalized linear regression on the <code class="literal">libsvm</code> version of the <span class="strong"><strong>Optical Character Recognition</strong></span> (<span class="strong"><strong>OCR</strong></span>) data that we discussed in <a class="link" href="#" linkend="ch07">Chapter 7</a>, <span class="emphasis"><em>Tuning Machine Learning Models</em></span>. Since the same dataset will be re-used here, we've decided not to describe them further.</p><p><span class="strong"><strong>Step 1: Load the necessary API and packages</strong></span></p><p>Here is the code to load the necessary API and packages:</p><pre class="programlisting">import java.util.Arrays; 
import org.apache.spark.ml.regression.GeneralizedLinearRegression; 
import org.apache.spark.ml.regression.GeneralizedLinearRegressionModel; 
import org.apache.spark.ml.regression.GeneralizedLinearRegressionTrainingSummary; 
import org.apache.spark.sql.Dataset; 
import org.apache.spark.sql.Row; 
import org.apache.spark.sql.SparkSession; 
</pre><p><span class="strong"><strong>Step 2: Create the Spark session</strong></span></p><p>The following code shows how to create the Spark session:</p><pre class="programlisting">SparkSession spark = SparkSession 
    .builder() 
    .appName("JavaGeneralizedLinearRegressionExample") 
    .master("local[*]") 
    .config("spark.sql.warehouse.dir", "E:/Exp/") 
    .getOrCreate();  
</pre><p><span class="strong"><strong>Step 3: Load and create the Dataset</strong></span></p><p>Load and create the dataset from the OCR dataset. Here we have specified the dataset format as <code class="literal">libsvm</code>:</p><pre class="programlisting">Dataset&lt;Row&gt;dataset = spark.read().format("libsvm").load("input/Letterdata_libsvm.data"); 
</pre><p><span class="strong"><strong>Step 4: Prepare the training and test sets</strong></span></p><p>The following code illustrates how to prepare the training and test sets:</p><pre class="programlisting">double[] weights = {0.8, 0.2}; 
long seed = 12345L; 
Dataset&lt;Row&gt;[] split = dataset.randomSplit(weights, seed); 
Dataset&lt;Row&gt; training = split[0]; 
Dataset&lt;Row&gt; test = split[1]; 
</pre><p><span class="strong"><strong>Step 5: Create a generalized linear regression estimator</strong></span></p><p>Create the generalized linear regression estimator by specifying the family, link, and max iteration and regression parameters. Here we have selected the family as <code class="literal">"gaussian"</code> and link as <code class="literal">"identity"</code>:</p><pre class="programlisting">GeneralizedLinearRegression glr = new GeneralizedLinearRegression() 
.setFamily("gaussian") 
.setLink("identity") 
.setMaxIter(10) 
.setRegParam(0.3); 
</pre><p>Note that according to the API documentation at <a class="ulink" href="http://spark.apache.org/docs/latest/ml-classification-regression.html#generalized-linear-regression" target="_blank">http://spark.apache.org/docs/latest/ml-classification-regression.html#generalized-linear-regression</a>, the following options are supported with this algorithm implementation with Spark:</p><div class="mediaobject"><img src="graphics/B05243_08_02.jpg" /><div class="caption"><p>Figure 2: Available supported families with the current implementation of Generalized Linear Regression</p></div></div><p><span class="strong"><strong>Step 6: Fit the model</strong></span></p><p>Here is the code to fit the model:</p><pre class="programlisting">GeneralizedLinearRegressionModel model = glr.fit(training); 
</pre><p><span class="strong"><strong>Step 7: Check the coefficients and intercept</strong></span></p><p>Print the coefficients and intercept for the linear regression model that we created in Step 6:</p><pre class="programlisting">System.out.println("Coefficients: " + model.coefficients()); 
System.out.println("Intercept: " + model.intercept()); 
</pre><p>Output for these two parameters will be similar to the following:</p><pre class="programlisting">Coefficients: [-0.0022864381796305487,-0.002728958263362158,0.001582003618682323,-0.0027708788253722914,0.0021962329827476565,-0.014769839282003813,0.027752802299957722,0.005757124632688538,0.013869444611365267,-0.010555326094498824,-0.006062727351948948,-0.01618167221020619,0.02894330366681715,-0.006180003317929849,-0.0025768386348180294,0.015161831324693125,0.8125261496082304] 
Intercept: 1.2140016821111255  
</pre><p>It is to be noted that the <code class="literal">System.out.println</code> method will not work in cluster mode. This will work only in standalone or Pseudo mode. It is only for the verification of the result.</p><p><span class="strong"><strong>Step 8: Summarize the model</strong></span></p><p>Summarize the model over the training set and print out some metrics:</p><pre class="programlisting">GeneralizedLinearRegressionTrainingSummary summary = model.summary(); 
</pre><p><span class="strong"><strong>Step 9: Verify some generalized metrics</strong></span></p><p>Let's print some generalized metrics such as <span class="strong"><strong>Coefficient Standard Errors</strong></span> (<span class="strong"><strong>CSE</strong></span>), T values, P values, Dispersions, Null deviance, the Residual degree of freedom null, AIC and Deviance residuals. Owing to the page limitations, we have not shown how these values are significant or the calculations procedure:</p><pre class="programlisting">System.out.println("Coefficient Standard Errors: " 
      + Arrays.toString(summary.coefficientStandardErrors())); 
System.out.println("T Values: " + Arrays.toString(summary.tValues())); 
System.out.println("P Values: " + Arrays.toString(summary.pValues())); 
System.out.println("Dispersion: " + summary.dispersion()); 
System.out.println("Null Deviance: " + summary.nullDeviance()); 
System.out.println("Residual Degree Of Freedom Null: " + summary.residualDegreeOfFreedomNull()); 
System.out.println("Deviance: " + summary.deviance()); 
System.out.println("Residual Degree Of Freedom: " + summary.residualDegreeOfFreedom()); 
    System.out.println("AIC: " + summary.aic()); 
</pre><p>Let's see the values for the training set we created previously:</p><pre class="programlisting">Coefficient Standard Errors:[2.877963555951775E-4, 0.0016618949921257992, 9.147115254397696E-4, 0.001633197607413805, 0.0013194682048354774, 0.001427648472211677, 0.0010797461071614422, 0.001092731825368789, 7.922778963434026E-4, 9.413717346009722E-4, 8.746375698587989E-4, 9.768068714323967E-4, 0.0010276211138097238, 0.0011457739746946476, 0.0015025626835648176, 9.048329671989396E-4, 0.0013145697411570455, 0.02274018067510297] 
T Values:[-7.944639100457261, -1.6420762300218703, 1.729510971146599, -1.6965974067032972, 1.6644834446931607, -10.345571455081481, 25.703081600282317, 5.2685613240426585, 17.50578259898057, -11.212707697212734, -6.931702411237277, -16.56588695621814, 28.165345454527458, -5.3937368577226055, -1.714962485760994, 16.756497468951743, 618.0928437414578, 53.385753589911985] 
P Values:[1.9984014443252818E-15, 0.10059394323065063, 0.08373705354670546, 0.0897923347927514, 0.09603552109755675, 0.0, 0.0, 1.3928712139232857E-7, 0.0, 0.0, 4.317657342767234E-12, 0.0, 0.0, 6.999167956323049E-8, 0.08637155105770145, 0.0, 0.0, 0.0] 
Dispersion: 0.07102433332236015  
Null Deviance: 41357.85510971454 
Residual Degree Of Freedom Null: 15949 
Deviance: 1131.5596784918419 
Residual Degree Of Freedom: 15932 
AIC: 3100.6418768238423  
</pre><p><span class="strong"><strong>Step 10: Show the deviance residuals</strong></span></p><p>The following code is used to show the deviance residuals:</p><pre class="programlisting">summary.residuals().show(); 
</pre><div class="mediaobject"><img src="graphics/image_08_003.jpg" /><div class="caption"><p>Figure 3: Summary of the deviance residuals for the OCR dataset</p></div></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip54"></a>Tip</h3><p>Interested readers should refer to the following web page to get more information on and insight into this algorithm and its implementation details: <a class="ulink" href="http://spark.apache.org/docs/latest/ml-classification-regression.html" target="_blank">
http://spark.apache.org/docs/latest/ml-classification-regression.html
</a></p></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec62"></a>Adapting through incremental algorithms</h2></div></div><hr /></div><p>According to Robi Polikar et al., (<span class="emphasis"><em>Learn++: An Incremental Learning Algorithm for Supervised Neural Networks, IEEE Transactions on Systems, Man, And Cybernetics, V-21, No-4, November 2001</em></span>), various algorithms have been suggested for incremental learning. The incremental learning is therefore implied for solving different problems. In some literature, the term incremental learning has been used to refer to either the growing of or pruning of a classifier. Alternatively, it may refer to the selection of most informative training samples for solving a problem in an incremental way.</p><p>In other cases, making a regular ML algorithm incremental means performing some form of controlled modification of weights in the classifier, by retraining with misclassified signals. Some algorithms are capable of learning new information; however, they do not synchronously satisfy all of the previously mentioned criteria. Moreover, they either require access to the old data or need to forget the prior knowledge along the way, and as they are unable to accommodate new classes they are not adaptable for new datasets.</p><p>Considering the previously mentioned issues, in this section, we will discuss how to adopt ML models using an incremental version of the original algorithms. Incremental SVM, Bayesian Network, and Neural networks will be discussed in brief. Moreover, when applicable, we will provide regular Spark implementation of these algorithms.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec106"></a>Incremental support vector machine</h3></div></div></div><p>It's pretty difficult to make a regular ML algorithm incremental. In short, it's possible but not altogether easy. If you want to do it you have to change the underlying source codes in the Spark library you are using or implement the training algorithm yourself.</p><p>Unfortunately, Spark does not have an incremental version of SVM implemented. However, before making the linear SVM incremental, you need to first understand the linear SVM itself. Therefore, we provide some concepts of linear SVMs in the next sub-section using Spark for the new dataset.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip55"></a>Tip</h3><p>According to our knowledge, we have found only two possible solutions called SVMHeavy (<a class="ulink" href="http://people.eng.unimelb.edu.au/shiltona/svm/" target="_blank">http://people.eng.unimelb.edu.au/shiltona/svm/</a>) and LaSVM (<a class="ulink" href="http://leon.bottou.org/projects/lasvm" target="_blank">http://leon.bottou.org/projects/lasvm</a>), which support incremental training. But we haven't used either. Interested readers should follow these two papers on incremental SVMs to get some insight. These two papers are straightforward and show good research if you're just getting started:</p><p><a class="ulink" href="http://cbcl.mit.edu/cbcl/publications/ps/cauwenberghs-nips00.pdf" target="_blank">http://cbcl.mit.edu/cbcl/publications/ps/cauwenberghs-nips00.pdf</a>.</p><p><a class="ulink" href="http://www.jmlr.org/papers/volume7/laskov06a/laskov06a.pdf" target="_blank">http://www.jmlr.org/papers/volume7/laskov06a/laskov06a.pdf</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl3sec80"></a>Adapting SVMs for new data with Spark</h4></div></div></div><p>In this section, we will first discuss how to perform binary classification using linear SVMs of Spark implementation. Then we will show how to adopt the same algorithm for the new data type.</p><p><span class="strong"><strong>Step 1: Data collection and exploration</strong></span></p><p>We have collected a colon cancer dataset from <a class="ulink" href="https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html" target="_blank">https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html</a>. Originally, the dataset was labeled as -1.0 and 1.0 as follows:</p><div class="mediaobject"><img src="graphics/image_08_004.jpg" /><div class="caption"><p>Figure 4: Original colon cancer data snapshot</p></div></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip56"></a>Tip</h3><p>The dataset was used in the following publication: <span class="emphasis"><em>U. Alon, N. Barkai, D. A. Notterman, K. Gish, S.Ybarra, D.Mack, and A. J. Levine. Broad patterns of gene expression revealed by clustering analysis of tumour and normal colon tissues probed by oligonucleotide arrays</em></span>.<span class="emphasis"><em> Cell Biology, 96:6745-6750, 1999</em></span>. Interested readers should refer to the publication to get more insights into the dataset.</p></div><p>After that, instance-wise normalization is carried out to mean zero and variance one. Then feature wise normalization is carried out to get zero and variance one as a pre-processing step. However, for simplicity, we have considered -1.0 as 0.1, since SVM does not recognize symbols (that is, + or -). Therefore, the dataset now contains two labels 1 and 0 (that is, to say it's a binary classification problem). After pre-processing and scaling, there are two classes and 2000 features. Here is a sample of the dataset in <span class="emphasis"><em>Figure 5</em></span>:</p><div class="mediaobject"><img src="graphics/image_08_005.jpg" /><div class="caption"><p>Figure 5: Pre-processed colon cancer data</p></div></div><p><span class="strong"><strong>Step 2: Load the necessary packages and APIs</strong></span></p><p>Here is the code to load the necessary packages:</p><pre class="programlisting">import org.apache.spark.api.java.JavaRDD; 
import org.apache.spark.api.java.function.Function; 
import org.apache.spark.mllib.classification.SVMModel; 
import org.apache.spark.mllib.classification.SVMWithSGD; 
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics; 
import org.apache.spark.mllib.evaluation.MulticlassMetrics; 
import org.apache.spark.mllib.optimization.L1Updater; 
import org.apache.spark.mllib.regression.LabeledPoint; 
import org.apache.spark.mllib.util.MLUtils; 
import org.apache.spark.sql.SparkSession; 
</pre><p><span class="strong"><strong>Step 3: Configure the Spark session</strong></span></p><p>The following code helps us to create the Spark session:</p><pre class="programlisting">SparkSession spark = SparkSession 
    .builder() 
    .appName("JavaLDAExample") 
    .master("local[*]") 
    .config("spark.sql.warehouse.dir", "E:/Exp/") 
    .getOrCreate(); 
</pre><p><span class="strong"><strong>Step 4: Create a Dataset out of the data</strong></span></p><p>Here is the code to create a Dataset:</p><pre class="programlisting">String path = "input/colon-cancer.data"; 
JavaRDD&lt;LabeledPoint&gt;data = MLUtils.loadLibSVMFile(spark.sparkContext(), path).toJavaRDD(); 
</pre><p><span class="strong"><strong>Step 5: Prepare the training and test sets</strong></span></p><p>Here is the code to prepare the training and test sets:</p><pre class="programlisting">    JavaRDD&lt;LabeledPoint&gt;training = data.sample(false, 0.8, 11L); 
training.cache(); 
    JavaRDD&lt;LabeledPoint&gt;test = data.subtract(training); 
</pre><p><span class="strong"><strong>Step 6: Build and train the SVM model</strong></span></p><p>The following code illustrates how to build and train the SVM model: </p><pre class="programlisting">intnumIterations = 500; 
final SVMModel model = SVMWithSGD.train(training.rdd(), numIterations); 
</pre><p><span class="strong"><strong>Step 7: Compute the raw prediction score on the test set</strong></span></p><p>Here is the code to compute the raw prediction:</p><pre class="programlisting">JavaRDD&lt;Tuple2&lt;Object, Object&gt;&gt;scoreAndLabels = test.map( 
newFunction&lt;LabeledPoint, Tuple2&lt;Object, Object&gt;&gt;() { 
public Tuple2&lt;Object, Object&gt; call(LabeledPoint p) { 
          Double score = model.predict(p.features()); 
returnnew Tuple2&lt;Object, Object&gt;(score, p.label()); 
        }}); 
</pre><p><span class="strong"><strong>Step 8: Evaluate the model</strong></span></p><p>Here is the code to evaluate the model:</p><pre class="programlisting">BinaryClassificationMetrics metrics = new BinaryClassificationMetrics(JavaRDD.toRDD(scoreAndLabels)); 
System.out.println("Area Under PR = " + metrics.areaUnderPR()); 
System.out.println("Area Under ROC = " + metrics.areaUnderROC()); 
Area Under PR = 0.6266666666666666 
Area Under ROC = 0.875  
</pre><p>However, the value of ROC is between 0.5 and 1.0. Where the value is more than 0.8 this indicates a good classifier and if the value of ROC is less than 0.8, this signals a bad classifier. The <code class="literal">SVMWithSGD.train()</code> method by default performs Level Two (L2) regularization with the regularization parameter set to 1.0.</p><p>If you want to configure this algorithm, you should customize the <code class="literal">SVMWithSGD</code> further by creating a new object directly. After that, you can further the setter methods to set the value of the object.</p><p>Interestingly, all the other Spark MLlib algorithms can be customized this way. However, after the customization has been completed, you need to build the source code to make changes up to API level. Interested readers can add themselves to the Apache Spark mailing list if they want to contribute to the open source.</p><p>Note the source code of Spark is available on GitHub at the URL <a class="ulink" href="https://github.com/apache/spark" target="_blank">https://github.com/apache/spark</a> as an open source and it sends pull requests to enrich Spark. More technical discussion can be found at the Spark website at <a class="ulink" href="http://spark.apache.org/" target="_blank">http://spark.apache.org/</a>.</p><p>For example, the following code produces a level one (<code class="literal">L1</code>) regularized variant of SVMs with the regularization parameter set to 0.1, and runs the training algorithm for 500 iterations as follows:</p><pre class="programlisting">SVMWithSGD svmAlg = new SVMWithSGD(); 
svmAlg.optimizer() 
      .setNumIterations(500) 
      .setRegParam(0.1) 
      .setUpdater(new L1Updater()); 
final SVMModel model = svmAlg.run(training.rdd()); 
</pre><p>Your model is now trained. Now if you perform <span class="emphasis"><em>step 7</em></span> and <span class="emphasis"><em>step 8</em></span>, the following metrics will be generated:</p><pre class="programlisting">Area Under PR = 0.9380952380952381 
Area Under ROC = 0.95 
</pre><p>If you compare this result with the result produced in <span class="emphasis"><em>step 8</em></span>, it's much better now, isn't it? However, depending on the data preparation, you might experience different results.</p><p>It indicates a better classification (please see also at <a class="ulink" href="https://www.researchgate.net/post/What_is_the_value_of_the_area_under_the_roc_curve_AUC_to_conclude_that_a_classifier_is_excellent" target="_blank">https://www.researchgate.net/post/What_is_the_value_of_the_area_under_the_roc_curve_AUC_to_conclude_that_a_classifier_is_excellent</a>). In this way the SVM can be optimized or adaptive for the new data type.</p><p>However, the parameters (that is, number of iterations, regression params, and updater) should be set accordingly.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec107"></a>Incremental neural networks</h3></div></div></div><p>The incremental version of the neural network in R or Mat lab provides adaptability using the adapt function. Does this update instead of overwriting iteratively? To verify this statement, readers can try using the R or Mat lab version of the incremental neural network-based classifier that may need to select a subset of your first data chunk as the second chunk in training. If it is overwriting, when you use the trained net with the subset to test your first data chunk, it will likely poorly predict the data that does not belong to the subset.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl3sec81"></a>Multilayer perceptron classification with Spark</h4></div></div></div><p>To date, there is no implementation of the incremental version of the neural network in Spark yet. According to the API documentation provided at <a class="ulink" href="https://spark.apache.org/docs/latest/ml-classification-regression.html#multilayer-perceptron-classifier" target="_blank">https://spark.apache.org/docs/latest/ml-classification-regression.html#multilayer-perceptron-classifier</a>, Spark's <span class="strong"><strong>Multilayer Perceptron Classifier</strong></span> (<span class="strong"><strong>MLPC</strong></span>) is a classifier based on the <span class="strong"><strong>Feedforward Artificial Neural Network </strong></span>(<span class="strong"><strong>FANN</strong></span>). The MLPC consists of multiple layers of nodes including hidden layers. Each layer is fully connected to the next layer and so on in a network. A node in the input layer represents the input data. All other nodes map inputs to outputs by a linear combination of the inputs with the node's weights <span class="emphasis"><em>w</em></span> and bias <span class="emphasis"><em>b</em></span> and by applying the activation function. The number of nodes <span class="emphasis"><em>N</em></span> in the output layer corresponds to the number of classes.</p><p>MLPC also performs backpropagation for learning the model. Spark uses the logistic loss function for optimization and <span class="strong"><strong>Limited-memory Broyden-Fletcher-Goldfarb-Shanno</strong></span> (<span class="strong"><strong>L-BFGS</strong></span>) as an optimization routine. Note that the L-BFGS is an optimization algorithm in the family of <span class="strong"><strong>Quasi-Newton Method</strong></span> (<span class="strong"><strong>QNM</strong></span>) that approximates the Broyden-Fletcher-Goldfarb-Shanno algorithm using a limited main memory. To train the multilayer perceptron classifier, the following parameters need to be set:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Layer</p></li><li style="list-style-type: disc"><p>Tolerance of iteration</p></li><li style="list-style-type: disc"><p>The block size of the learning</p></li><li style="list-style-type: disc"><p>Seed size</p></li><li style="list-style-type: disc"><p>Max iteration number</p></li></ul></div><p>Note the layers consist of the input, hidden, and output layers. Moreover, a smaller value of convergence tolerance will lead to higher accuracy with the cost of more iterations. The default block size parameter is 128 and the maximum number of iteration is set to be 100 as a default value. We suggest you set these values accordingly and carefully.</p><p>In this sub-section, we will show how Spark has implemented the neural network learning algorithms through the multilayer perception classifier on the Iris dataset.</p><p><span class="strong"><strong>Step 1: Dataset collection, processing, and exploration</strong></span></p><p>The original Iris plant dataset was collected from the UCI machine learning repositories (<a class="ulink" href="http://www.ics.uci.edu/~mlearn/MLRepository.html" target="_blank">http://www.ics.uci.edu/~mlearn/MLRepository.html</a>) and then pre-processed, scaled to libsvm format by Chang et al., and placed as the libsvm a comprehensive library for support vector machine at (<a class="ulink" href="https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html" target="_blank">https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html</a>) for the binary, multi-class, and multi-label classification task. The Iris dataset contains three classes and four features, where the sepal and petal lengths are scaled according to the libsvm format. More specifically, here is the attribute information:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Class: Iris Setosa, Iris Versicolour, Iris Virginica (column 1)</p></li><li style="list-style-type: disc"><p>Sepal length in cm (column 2)</p></li><li style="list-style-type: disc"><p>Sepal width in cm (column 3)</p></li><li style="list-style-type: disc"><p>Petal length in cm (column 4)</p></li><li style="list-style-type: disc"><p>Petal width in cm (column 5)</p></li></ul></div><p>A snapshot of the dataset is shown in <span class="emphasis"><em>Figure 6</em></span>:</p><div class="mediaobject"><img src="graphics/image_08_006.jpg" /><div class="caption"><p>Figure 6: Irish dataset snapshot</p></div></div><p><span class="strong"><strong>Step 2: Load the required packages and APIs</strong></span></p><p>Here is the code to load the required packages and APIs:</p><pre class="programlisting">import org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel; 
import org.apache.spark.ml.classification.MultilayerPerceptronClassifier; 
import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator; 
import org.apache.spark.sql.Dataset; 
import org.apache.spark.sql.Row; 
import org.apache.spark.sql.SparkSession; 
import com.example.SparkSession.UtilityForSparkSession; 
</pre><p><span class="strong"><strong>Step 3: Create a Spark session</strong></span></p><p>The following code helps us to create the Spark session:</p><pre class="programlisting">SparkSession spark = UtilityForSparkSession.mySession(); 
</pre><p>Note, the <code class="literal">mySession()</code> method that creates and returns a Spark session object is as follows:</p><pre class="programlisting">public static SparkSession mySession() { 
SparkSession spark = SparkSession.builder() 
.appName("MultilayerPerceptronClassificationModel") 
.master("local[*]") 
.config("spark.sql.warehouse.dir", "E:/Exp/") 
.getOrCreate(); 
    return spark; 
  } 
</pre><p><span class="strong"><strong>Step 4: Parse and prepare the dataset</strong></span></p><p>Load the input data as <code class="literal">libsvm</code> format:</p><pre class="programlisting">String path = "input/iris.data"; 
Dataset&lt;Row&gt; dataFrame = spark.read().format("libsvm").load(path); 
</pre><p><span class="strong"><strong>Step 5: Prepare the training and test set</strong></span></p><p>Prepare the training and test set: training = 70%, test = 30%, and seed = 12345L:</p><pre class="programlisting">Dataset&lt;Row&gt;[] splits = dataFrame.randomSplit(new double[] { 0.7, 0.3 }, 12345L); 
Dataset&lt;Row&gt; train = splits[0]; 
Dataset&lt;Row&gt; test = splits[1]; 
</pre><p><span class="strong"><strong>Step 6: Specify the layers for the neural network</strong></span></p><p>Specify the layers for the neural network. Here, input layer size 4 (features), two intermediate layers (that is, hidden layers) of size 4 and 3, and output size 3 (classes):</p><pre class="programlisting">int[] layers = newint[] { 4, 4, 3, 3 }; 
</pre><p><span class="strong"><strong>Step 7: Create the multilayer perceptron estimator</strong></span></p><p>Create the <code class="literal">MultilayerPerceptronClassifier</code> trainer and set its parameters. Here, set the value of <code class="literal">param [[layers]]</code> using the <code class="literal">setLayers()</code> method from <span class="emphasis"><em>Step 6</em></span>. Set the convergence tolerance of iterations using the <code class="literal">setTol()</code> method, since, a smaller value will lead to higher accuracy with the cost of more iterations.</p><p>Note the default is <code class="literal">1E-4</code>. Set the value of Param <code class="literal">[[blockSize]]</code> using the <code class="literal">setBlockSize()</code> method, where the default is 128KB. Set the seed for weight initialization if the weights using the <code class="literal">setInitialWeights()</code> are not set. Finally, set the maximum number of iterations using the <code class="literal">setMaxIter()</code> method, where the default is 100:</p><pre class="programlisting">MultilayerPerceptronClassifier trainer = new MultilayerPerceptronClassifier() 
        .setLayers(layers)        
        .setTol(1E-4)         
        .setBlockSize(128)         
        .setSeed(12345L)  
        .setMaxIter(100); 
</pre><p><span class="strong"><strong>Step 8: Train the model</strong></span></p><p>Train the <code class="literal">MultilayerPerceptronClassificationModel</code> using the preceding estimator from <span class="emphasis"><em>step 7</em></span>:</p><pre class="programlisting">MultilayerPerceptronClassificationModel model = trainer.fit(train); 
</pre><p><span class="strong"><strong>Step 9: Compute the accuracy on the test set</strong></span></p><p>Here is the code to compute the accuracy on the test set:</p><pre class="programlisting">Dataset&lt;Row&gt; result = model.transform(test); 
Dataset&lt;Row&gt; predictionAndLabels = result.select("prediction", "label"); 
</pre><p><span class="strong"><strong>Step 10: Evaluate the model</strong></span></p><p>Evaluate the model, calculate the metrics`, and print the accuracy, weighted precision and weighted recall:</p><pre class="programlisting">MulticlassClassificationEvaluator evaluator = new MulticlassClassificationEvaluator().setMetricName("accuracy"); 
MulticlassClassificationEvaluator evaluator2 = new MulticlassClassificationEvaluator().setMetricName("weightedPrecision"); 
MulticlassClassificationEvaluator evaluator3 = new MulticlassClassificationEvaluator().setMetricName("weightedRecall"); 
System.out.println("Accuracy = " + evaluator.evaluate(predictionAndLabels)); 
System.out.println("Precision = " + evaluator2.evaluate(predictionAndLabels)); 
System.out.println("Recall = " + evaluator3.evaluate(predictionAndLabels)); 
</pre><p>The output should appear as follows:</p><pre class="programlisting">Accuracy = 0.9545454545454546  
Precision = 0.9595959595959596 
Recall = 0.9545454545454546  
</pre><p><span class="strong"><strong>Step 11: Stop the Spark session</strong></span></p><p>The following code is used to stop the Spark session:</p><pre class="programlisting">spark.stop(); 
</pre><p>From the preceding prediction metrics, it is clear that the classification task is quite impressive. Now it's your turn to make your model adaptable. Now try training and testing with the new dataset and make your ML model adaptable.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec108"></a>Incremental Bayesian networks</h3></div></div></div><p>As we discussed earlier, Naive Bayes is a simple multiclass classification algorithm with the assumption of independence between each pair of features. The Naive Bayes based model can be trained very efficiently. The model can compute the conditional probability distribution of each feature, given the label, since a pass to the training data. After that, it applies the Bayes theorem to compute the conditional probability distribution of the labels for making the prediction.</p><p>However, there is still no implementation of the incremental version of the Bayesian network into Spark yet. According to the API documentation provided at <a class="ulink" href="http://spark.apache.org/docs/latest/mllib-naive-bayes.html" target="_blank">http://spark.apache.org/docs/latest/mllib-naive-bayes.html</a>, each observation is a document and each feature represents a term. The value of an observation is the frequency of the term or a zero or one. This value indicates if the term has been found in the document for the multinomial Naive Bayes and Bernoulli Naive Bayes respectively for the document classification.</p><p>Note that as with linear SVM-based learning, here the feature values must be non-negative too. The type of the model is selected with an optional parameter, multinomial or Bernoulli. The default model type is multinomial. Furthermore, additive smoothing (that is, lambda) can be used by setting the parameter Î». Note the default of lambda is 1.0.</p><p>More technical details on the big data approach of Bayesian network based learning can be found in the paper: <span class="emphasis"><em>A Scalable Data Science Workflow Approach for Big Data Bayesian Network Learning b</em></span>
<span class="emphasis"><em>y Jianwu W., et al.</em></span>, (<a class="ulink" href="http://users.sdsc.edu/~jianwu/JianwuWang_files/A_Scalable_Data_Science_Workflow_Approach_for_Big_Data_Bayesian_Network_Learning.pdf" target="_blank">http://users.sdsc.edu/~jianwu/JianwuWang_files/A_Scalable_Data_Science_Workflow_Approach_for_Big_Data_Bayesian_Network_Learning.pdf</a>).</p><p>Interested readers also should refer to the following publications for more insight into the incremental Bayesian networks:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><a class="ulink" href="http://www.jmlr.org/papers/volume11/henderson10a/henderson10a.pdf" target="_blank">http://www.jmlr.org/papers/volume11/henderson10a/henderson10a.pdf</a>
</p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://www.machinelearning.org/proceedings/icml2007/papers/351.pdf" target="_blank">http://www.machinelearning.org/proceedings/icml2007/papers/351.pdf</a>
</p></li><li style="list-style-type: disc"><p><a class="ulink" href="https://tel.archives-ouvertes.fr/tel-01284332/document" target="_blank">https://tel.archives-ouvertes.fr/tel-01284332/document</a>
</p></li></ul></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl3sec82"></a>Classification using Naive Bayes with Spark</h4></div></div></div><p>The current implementation in Spark MLlib supports both the multinomial Naive Bayes and Bernoulli Naive Bayes. However, the incremental version has not been implemented yet. Therefore, in this section, we will show you how to perform the classification using the Spark MLlib version of NaÃ¯ve Bayes on the Vehicle Scale dataset to provide you with some concepts of the NaÃ¯ve Bayes based learning.</p><p>Note, due to the low accuracy and precision using Spark ML, we did not provide the Pipeline version but implemented the same using only Spark MLlib. Moreover, if you have suitable and better data, you can try to implement the Spark ML version with ease.</p><p><span class="strong"><strong>Step 1: Data collection, pre-processing, and exploration</strong></span></p><p>The dataset was downloaded from <a class="ulink" href="https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html#aloi" target="_blank">https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html#aloi</a> and provided by David D. Lewis, Yiming Yang, Tony G. Rose, and Fan Li. RCV1: A new benchmark collection for text categorization research. <span class="emphasis"><em>Journal of Machine Learning Research</em></span>, 5:361-397, 2004.</p><p><span class="strong"><strong>Pre-processing:</strong></span> For the pre-processing, two steps were considered as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The label hierarchy is reorganized by mapping the data set to the second level of RCV1 (that is, revision) topic hierarchy. The documents, having the third or fourth level, are mapped to their parent category of the second level only. Consequently, documents having the first level are not considered for creating the mapping.</p></li><li style="list-style-type: disc"><p>Multi-labeled instances were removed since the current implementation of multi-level classifier in Spark is not robust enough.</p></li></ul></div><p>After performing these two steps, there are finally 53 classes and 47,236 features collected. Here is a snapshot of the dataset shown in <span class="emphasis"><em>Figure 7:</em></span></p><div class="mediaobject"><img src="graphics/image_08_007.jpg" /><div class="caption"><p>Figure 7: RCV1 topic hierarchy dataset</p></div></div><p><span class="strong"><strong>Step 2: Load the required library and packages</strong></span></p><p>Here is the code to load the library and packages:</p><pre class="programlisting">import org.apache.spark.api.java.JavaPairRDD; 
import org.apache.spark.api.java.JavaRDD; 
import org.apache.spark.api.java.function.Function; 
import org.apache.spark.api.java.function.PairFunction; 
import org.apache.spark.mllib.classification.NaiveBayes; 
import org.apache.spark.mllib.classification.NaiveBayesModel; 
import org.apache.spark.mllib.regression.LabeledPoint; 
import org.apache.spark.mllib.util.MLUtils; 
import org.apache.spark.sql.SparkSession; 
importscala.Tuple2; 
</pre><p><span class="strong"><strong>Step 3: Initiate a Spark session</strong></span></p><p>The following code helps us to create the Spark session:</p><pre class="programlisting">static SparkSession spark = SparkSession 
      .builder() 
      .appName("JavaLDAExample").master("local[*]") 
      .config("spark.sql.warehouse.dir", "E:/Exp/") 
      .getOrCreate();  
</pre><p><span class="strong"><strong>Step 4: Prepare LabeledPoint RDDs</strong></span></p><p>Parse the dataset in the libsvm format and prepare <code class="literal">LabeledPoint</code> RDDs:</p><pre class="programlisting">static String path = "input/rcv1_train.multiclass.data"; 
JavaRDD&lt;LabeledPoint&gt; inputData = MLUtils.loadLibSVMFile(spark.sparkContext(), path).toJavaRDD();  
</pre><p>For document classification, the input feature vectors are usually sparse, and sparse vectors should be supplied as input to take advantage of sparsity. Since the training data is only used once, it is not necessary to cache it.</p><p><span class="strong"><strong>Step 5: Prepare the training and test set</strong></span></p><p>Here is the code to prepare the training and test set:</p><pre class="programlisting">JavaRDD&lt;LabeledPoint&gt;[] split = inputData.randomSplit(new double[]{0.8, 0.2}, 12345L); 
JavaRDD&lt;LabeledPoint&gt; training = split[0];  
JavaRDD&lt;LabeledPoint&gt; test = split[1]; 
</pre><p><span class="strong"><strong>Step 6: Train the Naive Bayes model</strong></span></p><p>Train a Naive Bayes model by specifying the model type as multinomial and lambda = 1.0, which is the default and suitable for the multiclass classification of any features. However, note that Bernoulli naive Bayes requires 0 or 1 feature values:</p><pre class="programlisting">final NaiveBayesModel model = NaiveBayes.train(training.rdd(), 1.0, "multinomial"); 
</pre><p><span class="strong"><strong>Step 7: Calculate the prediction on the test dataset</strong></span></p><p>Here is the code to calculate the prediction:</p><pre class="programlisting">JavaPairRDD&lt;Double,Double&gt; predictionAndLabel = 
test.mapToPair(new PairFunction&lt;LabeledPoint, Double, Double&gt;() { 
@Override 
public Tuple2&lt;Double, Double&gt; call(LabeledPoint p) { 
return new Tuple2&lt;&gt;(model.predict(p.features()), p.label()); 
          } 
        }); 
</pre><p><span class="strong"><strong>Step 8: Calculate the prediction accuracy</strong></span></p><p>Here is the code to calculate the prediction accuracy:</p><pre class="programlisting">double accuracy = predictionAndLabel.filter(new Function&lt;Tuple2&lt;Double, Double&gt;, Boolean&gt;() { 
@Override 
public Boolean call(Tuple2&lt;Double, Double&gt;pl) { 
returnpl._1().equals(pl._2()); 
        } 
      }).count() / (double) test.count(); 
</pre><p><span class="strong"><strong>Step 9: Print the accuracy</strong></span></p><p>Here is the code to print the accuracy:</p><pre class="programlisting">System.out.println("Accuracy of the classification: "+accuracy); 
</pre><p>This provides the following output:</p><pre class="programlisting">Accuracy of the classification: 0.5941753719531497  
</pre><p>This is pretty low, right? This is as we discussed when we tuned the ML models in <a class="link" href="#" linkend="ch07">Chapter 7</a>, <span class="emphasis"><em>Tuning Machine Learning Models</em></span>. There are further opportunities to improve the prediction accuracy by selecting appropriate algorithms (that is, classifier or regressor) via cross-validation and train split.</p></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec63"></a>Adapting through reusing ML models</h2></div></div><hr /></div><p>In this section, we will describe how to make a machine learning model adaptable for new datasets. An example will be shown for the prediction of heart disease. At first we will describe the problem statement, and then we will explore the heart diseases dataset. Following the dataset exploration, we will train and save the model to local storage. After that the model will be evaluated to see how it performs. Finally, we will reuse/reload the same model trained to work for the new data type.</p><p>More specifically, we will show how to predict the possibility of future heart disease by using the Spark machine learning APIs including Spark MLlib, Spark ML, and Spark SQL.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec109"></a>Problem statements and objectives</h3></div></div></div><p>Machine learning and big data together are a radical combination that has created some great impacts in the field of research to academia and the industry as well as in the biomedical sector. In the area of biomedical data analytics, this carries a better impact on a real dataset for diagnosis and prognosis for better healthcare. Moreover, life science research is also entering into big data since datasets are being generated and produced in an unprecedented way. This imposes great challenges to machine learning and bioinformatics tools and algorithms to find the VALUE from big data criteria such as volume, velocity, variety, veracity, visibility, and value.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec110"></a>Data exploration</h3></div></div></div><p>In recent times, biomedical research has advanced enormously and more and more life sciences datasets are being generated making many of them open source. However, for simplicity and ease, we have decided to use the Cleveland database. To date, most of the researchers who have applied the machine learning technique to biomedical data analytics have used this dataset. According to the dataset description at <a class="ulink" href="https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/heart-disease.names" target="_blank">https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/heart-disease.names</a>, this heart disease dataset is one of the most used and well-studied datasets by researchers from biomedical data analytics and machine learning fields, respectively.</p><p>The dataset is freely available at the UCI machine learning dataset repository at <a class="ulink" href="https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/" target="_blank">https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/</a>. This data contains a total of 76 attributes, however, most of the published research papers refer to using a subset of only 14 features in the field. The "goal" field is used to refer to if the heart diseases are present or absent. It has five possible values ranging from 0 to 4. The value 0 signifies no presence of heart diseases. The values 1 and 2 signify that the disease is present but in the primary stage. The values 3 and 4, on the other hand, indicate the strong possibility of heart disease. Biomedical laboratory experiments with the Cleveland dataset have simply attempted to distinguish presence (values 1, 2, 3, 4) from absence (value 0). In short, the higher the value the more the disease is possible and the more evidence of the presence there is. Another thing is that privacy is an important concern in the area of biomedical data analytics as well as all kinds of diagnosis and prognosis. Therefore, the names and social security numbers of the patients were recently removed from the dataset to avoid the privacy issue. Consequently, those values have been replaced with dummy values instead.</p><p>It is to be noted that three files have been processed, containing the Cleveland, Hungarian, and Switzerland datasets altogether. All four unprocessed files also exist in this directory. To demonstrate the example, we will use the Cleveland dataset for training and evaluating the models. However, the Hungarian dataset will be used to re-use the saved model. As we have said already, although the number of attributes is 76 (including the predicted attribute), like other ML/Biomedical researchers, we will also use only 14 attributes with the following attribute information:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>No.</strong></span></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Attribute name</strong></span></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Explanation</strong></span></p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>1</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>age</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Age in years</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>2</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>sex</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Either male or female: sex (1 = male; 0 = female)</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>3</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>cp</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Chest pain type:</p><p>
</p><p>— Value 1: typical angina</p><p>
</p><p>— Value 2: atypical angina</p><p>
</p><p>— Value 3: non-angina pain</p><p>
</p><p>— Value 4: asymptomatic</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>4</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>trestbps</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Resting blood pressure (in mm Hg on admission to the hospital)</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>5</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>chol</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Serum cholesterol in mg/dl</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>6</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>fbs</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Fasting blood sugar. If &gt; 120 mg/dl)(1 = true; 0 = false)</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>7</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>restecg</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Resting electrocardiographic results:</p><p>
</p><p>— Value 0: normal</p><p>
</p><p>— Value 1: having ST-T wave abnormality</p><p>
</p><p>— Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>8</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>thalach</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Maximum heart rate achieved</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>9</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>exang</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Exercise induced angina (1 = yes; 0 = no)</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>10</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>oldpeak</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>ST depression induced by exercise relative to rest</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>11</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>slope</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>The slope of the peak exercise ST segment</p><p>
</p><p>— Value 1: upsloping</p><p>
</p><p>— Value 2: flat</p><p>
</p><p>— Value 3: down-sloping</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>12</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>ca</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Number of major vessels (0-3) colored by fluoroscopy</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>13</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>thal</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Heart rate:</p><p>
</p><p>—Value 3 = normal;</p><p>
</p><p>—Value 6 = fixed defect</p><p>
</p><p>—Value 7 = reversible defect</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; ">
<p>14</p>
</td><td style="border-right: 0.5pt solid ; ">
<p>num</p>
</td><td style="">
<p>Diagnosis of heart disease (angiographic disease status)</p><p>
</p><p>— Value 0: &lt; 50% diameter narrowing</p><p>
</p><p>— Value 1: &gt; 50% diameter narrowing</p>
</td></tr></tbody></table></div><p>Table 1: Dataset characteristics</p><p>A sample snapshot of the dataset is given as follows:</p><div class="mediaobject"><img src="graphics/image_08_008.jpg" /><div class="caption"><p>Figure 8: A sample snapshot of the heart diseases dataset</p></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec111"></a>Developing a heart diseases predictive model</h3></div></div></div><p><span class="strong"><strong>Step 1: Loading the required packages and APIs</strong></span></p><p>The following packages and APIs need to be imported for our purpose. We believe the packages are self-explanatory if you have the minimum working experience with Spark 2.0.0:</p><pre class="programlisting">import java.util.HashMap; 
import java.util.List; 
import org.apache.spark.api.java.JavaPairRDD; 
import org.apache.spark.api.java.JavaRDD; 
import org.apache.spark.api.java.function.Function; 
import org.apache.spark.api.java.function.PairFunction; 
import org.apache.spark.ml.classification.LogisticRegression; 
import org.apache.spark.mllib.classification.LogisticRegressionModel; 
import org.apache.spark.mllib.classification.NaiveBayes; 
import org.apache.spark.mllib.classification.NaiveBayesModel; 
import org.apache.spark.mllib.linalg.DenseVector; 
import org.apache.spark.mllib.linalg.Vector; 
import org.apache.spark.mllib.regression.LabeledPoint; 
import org.apache.spark.mllib.regression.LinearRegressionModel; 
import org.apache.spark.mllib.regression.LinearRegressionWithSGD; 
import org.apache.spark.mllib.tree.DecisionTree; 
import org.apache.spark.mllib.tree.RandomForest; 
import org.apache.spark.mllib.tree.model.DecisionTreeModel; 
import org.apache.spark.mllib.tree.model.RandomForestModel; 
import org.apache.spark.rdd.RDD; 
import org.apache.spark.sql.Dataset; 
import org.apache.spark.sql.Row; 
import org.apache.spark.sql.SparkSession; 
import com.example.SparkSession.UtilityForSparkSession; 
import javassist.bytecode.Descriptor.Iterator; 
import scala.Tuple2; 
</pre><p><span class="strong"><strong>Step 2: Create an active Spark session</strong></span></p><p>The following code helps us to create the Spark session:</p><pre class="programlisting">SparkSession spark = UtilityForSparkSession.mySession(); 
</pre><p>Here is the <code class="literal">UtilityForSparkSession</code> class that creates and returns an active Spark session:</p><pre class="programlisting">import org.apache.spark.sql.SparkSession; 
public class UtilityForSparkSession { 
  public static SparkSession mySession() { 
    SparkSession spark = SparkSession 
                          .builder() 
                          .appName("UtilityForSparkSession") 
                          .master("local[*]") 
                          .config("spark.sql.warehouse.dir", "E:/Exp/") 
                          .getOrCreate(); 
    return spark; 
  } 
} 
</pre><p>Note that here in the Windows 7 platform, we have set the Spark SQL warehouse as <code class="literal">E:/Exp/</code>, but set your path accordingly, based on your operating system.</p><p><span class="strong"><strong>Step 3: Data parsing and RDD of Labelpoint creation</strong></span></p><p>Take the input as a simple text file, parse them as a text file, and create an RDD of the label point that will be used for the classification and regression analysis. Also specify the input source and number of partition. Adjust the number of partition based on your dataset size. Here the number of partition has been set to 2:</p><pre class="programlisting">String input = "heart_diseases/processed_cleveland.data"; 
Dataset&lt;Row&gt; my_data = spark.read().format("com.databricks.spark.csv").load(input); 
my_data.show(false); 
RDD&lt;String&gt; linesRDD = spark.sparkContext().textFile(input, 2); 
</pre><p>Since <code class="literal">JavaRDD</code> cannot be created directly from the text files, we have created a simple RDDs so that we can convert them to <code class="literal">JavaRDD</code> when necessary. Now let's create the <code class="literal">JavaRDD</code> with a Label Point. However, we first need to convert the RDD to <code class="literal">JavaRDD</code> to serve our purpose as follows:</p><pre class="programlisting">JavaRDD&lt;LabeledPoint&gt; data = linesRDD.toJavaRDD().map(new Function&lt;String, LabeledPoint&gt;() { 
      @Override 
  public LabeledPoint call(String row) throws Exception { 
      String line = row.replaceAll("\\?", "999999.0"); 
      String[] tokens = line.split(","); 
      Integer last = Integer.parseInt(tokens[13]); 
      double[] features = new double[13]; 
      for (int i = 0; i &lt; 13; i++) { 
      features[i] = Double.parseDouble(tokens[i]); 
      } 
      Vector v = new DenseVector(features); 
      Double value = 0.0; 
      if (last.intValue() &gt; 0) 
        value = 1.0; 
      LabeledPoint lp = new LabeledPoint(value, v); 
    return lp; 
      } 
    }); 
</pre><p>Using the <code class="literal">replaceAll()</code> method, we have handled the invalid values such as the missing values that are specified in the original file using the <span class="emphasis"><em>?</em></span> character. To get rid of the missing or invalid values we have replaced them with a very large value that has no side effect to the original classification or predictive results. The reason for this is that missing or sparse data can lead you to highly misleading results.</p><p><span class="strong"><strong>Step 4: Splitting the RDD of the label point into training and test sets </strong></span></p><p>In the previous step, we created RDD label point data that can be used for the regression or classification task. Now we need to split the data into training and test sets as follows:</p><pre class="programlisting">double[] weights = {0.7, 0.3}; 
long split_seed = 12345L; 
JavaRDD&lt;LabeledPoint&gt;[] split = data.randomSplit(weights, split_seed); 
JavaRDD&lt;LabeledPoint&gt; training = split[0]; 
JavaRDD&lt;LabeledPoint&gt; test = split[1]; 
</pre><p>If you look at the preceding code segments, you will find that we have split the RDD label point as 70% for the training and 30% for the test set. The <code class="literal">randomSplit()</code> method performs this split. Note that we have set this RDD's storage level to persist its values across operations after the first time it is computed. This can only be used to assign a new storage level if the RDD does not have a storage level set yet. The split seed value is a long integer that signifies that the split would be random, but the result would not be a change in each run or iteration during the model building or training.</p><p><span class="strong"><strong>Step 5: Train the model</strong></span></p><p>First we will train the linear regression model, which is the simplest regression classifier:</p><pre class="programlisting">final double stepSize = 0.0000000009; 
final int numberOfIterations = 40;  
LinearRegressionModel model = LinearRegressionWithSGD.train(JavaRDD.toRDD(training), numberOfIterations, stepSize); 
</pre><p>As you can see, the preceding code trains a linear regression model with no regularization using the Stochastic Gradient Descent. This solves the least squares regression formulation f <span class="emphasis"><em>(weights) = 1/n ||A weights-y||^2^</em></span>, which is the mean squared error. Here the data matrix has <span class="emphasis"><em>n</em></span> rows, and the input RDD holds the set of rows of A, each with its corresponding right-hand side label y. Also, to train the model, it takes the training set, number of iterations, and the step size. We provide some random values for the last two parameters here.</p><p><span class="strong"><strong>Step 6: Model saving for future use</strong></span></p><p>Now let's save the model that we just created for future use. It's pretty simple - just use the following code by specifying the storage location as follows:</p><pre class="programlisting">String model_storage_loc = "models/heartModel";   
model.save(spark.sparkContext(), model_storage_loc); 
</pre><p>Once the model is saved in your desired location, you will see the following output in your Eclipse console:</p><div class="mediaobject"><img src="graphics/image_08_009.jpg" /><div class="caption"><p>Figure 9: The log after the model is saved to the storage</p></div></div><p><span class="strong"><strong>Step 7: Evaluate the model with a test set</strong></span></p><p>Now let's calculate the prediction score on the test dataset:</p><pre class="programlisting">JavaPairRDD&lt;Double,Double&gt; predictionAndLabel = 
  test.mapToPair(new PairFunction&lt;LabeledPoint, Double, Double&gt;() { 
            @Override 
    public Tuple2&lt;Double, Double&gt; call(LabeledPoint p) { 
       return new Tuple2&lt;&gt;(model.predict(p.features()), p.label()); 
            } 
          });   
</pre><p>Predict the accuracy of the prediction:</p><pre class="programlisting">double accuracy = predictionAndLabel.filter(new Function&lt;Tuple2&lt;Double, Double&gt;, Boolean&gt;() { 
          @Override 
          public Boolean call(Tuple2&lt;Double, Double&gt; pl) { 
            return pl._1().equals(pl._2()); 
          } 
        }).count() / (double) test.count(); 
System.out.println("Accuracy of the classification: "+accuracy);   
</pre><p>The output appears as follows:</p><pre class="programlisting">Accuracy of the classification: 0.0 
</pre><p><span class="strong"><strong>Step 8: Predictive analytics using a different classifier</strong></span></p><p>Unfortunately, there is no prediction accuracy at all, right? There might be several reasons for that, including the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The dataset characteristic</p></li><li style="list-style-type: disc"><p>Model selection</p></li><li style="list-style-type: disc"><p>Parameters selection - also called hyperparameter tuning</p></li></ul></div><p>For simplicity, we assume the dataset is okay since, as we have already said, it is a widely used dataset used for machine learning research used by many researchers around the globe. Now, what next? Let's consider another classifier algorithm, for example, a Random forest or decision tree classifier. What about the Random forest? Let's go for the random forest classifier at second place. Just use the following code to train the model using the training set:</p><pre class="programlisting">Integer numClasses = 26; //Number of classes 
</pre><p>Now use the <code class="literal">HashMap</code> to restrict the delicacy in the tree construction:</p><pre class="programlisting">HashMap&lt;Integer, Integer&gt; categoricalFeaturesInfo = new HashMap&lt;Integer, Integer&gt;(); 
</pre><p>Now declare the other parameters needed to train the Random Forest classifier:</p><pre class="programlisting">Integer numTrees = 5; // Use more in practice 
String featureSubsetStrategy = "auto"; // Let algorithm choose the best 
String impurity = "gini"; // info. gain &amp; variance also available 
Integer maxDepth = 20; // set the value of maximum depth accordingly 
Integer maxBins = 40; // set the value of bin accordingly 
Integer seed = 12345; //Setting a long seed value is recommended       
final RandomForestModel model = RandomForest.trainClassifier(training, numClasses,categoricalFeaturesInfo, numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins, seed); 
</pre><p>We believe the parameters used by the <code class="literal">trainClassifier()</code> method are self-explanatory and so we'll leave it to the readers to get to know the significance of each parameter. Fantastic! We have trained the model using the Random forest classifier and managed the cloud to save the model for future use. Now if you reuse the same code that we described in the <span class="emphasis"><em>Evaluate the model with test set</em></span> step, you should have the following output:</p><pre class="programlisting">Accuracy of the classification: 0.7843137254901961  
</pre><p>Now the predictive accuracy should be much better. If you are still not satisfied, you can try with another classifier model such as the Naive Bayes classifier and carry out the hyperparameter tuning discussed in <a class="link" href="#" linkend="ch07">Chapter 7</a>, <span class="emphasis"><em>Tuning Machine Learning Models</em></span>.</p><p><span class="strong"><strong>Step 9: Making the model adaptable for a new dataset</strong></span></p><p>We already mentioned that we have saved the model for future use, now we should take the opportunity to use the same model for new datasets. The reason, if you recall the steps, is that we have trained the model using the training set and evaluated it using the test set. Now, if you have more data or new data available to be used, what will you do? Will you go for re-training the model? Of course not, since you will have to iterate several steps and you will have to sacrifice valuable time and cost too.</p><p>Therefore, it would be wise to use the already trained model and predict the performance on a new dataset. Well, now let's reuse the stored model. Note that you will have to reuse the same model that is to be trained for the same model. For example, if you have done the model training using the Random forest classifier and saved the model while reusing it, you will have to use the same classifier model to load the saved model. Therefore, we will use the Random forest to load the model while using the new dataset. Use the following code to do that. Now create an RDD label point from the new dataset (that is, the Hungarian database with the same 14 attributes):</p><pre class="programlisting">String new_data = "heart_diseases/processed_hungarian.data"; 
RDD&lt;String&gt; linesRDD = spark.sparkContext().textFile(new_data, 2); 
JavaRDD&lt;LabeledPoint&gt; data = linesRDD.toJavaRDD().map(new Function&lt;String, LabeledPoint&gt;() { 
      @Override 
  public LabeledPoint call(String row) throws Exception { 
  String line = row.replaceAll("\\?", "999999.0"); 
  String[] tokens = line.split(","); 
  Integer last = Integer.parseInt(tokens[13]); 
    double[] features = new double[13]; 
             for (int i = 0; i &lt; 13; i++) { 
          features[i] = Double.parseDouble(tokens[i]); 
                } 
      Vector v = new DenseVector(features); 
      Double value = 0.0; 
      if (last.intValue() &gt; 0) 
        value = 1.0; 
      LabeledPoint p = new LabeledPoint(value, v); 
      return p; 
      } }); 
</pre><p>Now let's load the saved model using the Random forest model algorithm as follows:</p><pre class="programlisting">RandomForestModel model2 =  
RandomForestModel.load(spark.sparkContext(), model_storage_loc); 
</pre><p>Now let's calculate the prediction on the test set:</p><pre class="programlisting">JavaPairRDD&lt;Double, Double&gt; predictionAndLabel = 
  data.mapToPair(new PairFunction&lt;LabeledPoint, Double, Double&gt;() { 
          @Override 
          public Tuple2&lt;Double, Double&gt; call(LabeledPoint p) { 
      return new Tuple2&lt;&gt;(model2.predict(p.features()), p.label()); 
            } 
          }); 
</pre><p>Now calculate the accuracy of the prediction as follows:</p><pre class="programlisting">double accuracy = predictionAndLabel.filter(new Function&lt;Tuple2&lt;Double, Double&gt;, Boolean&gt;() { 
          @Override 
          public Boolean call(Tuple2&lt;Double, Double&gt; pl) { 
            return pl._1().equals(pl._2()); 
          } 
        }).count() / (double) data.count(); 
System.out.println("Accuracy of the classification: "+accuracy);   
</pre><p>We should have the following output:</p><pre class="programlisting">Accuracy of the classification: 0.9108910891089109 
</pre><p>Now train the NaÃ¯ve Bayesian classifier and see the predictive performance. Just download the source code for the Naive Bayesian classifier and run the code as a Maven-friendly project using the <code class="literal">pom.xml</code> file that includes all the dependencies of the required JARs and APIs.</p><p>The following table shows a comparison of the predictive accuracies among three classifiers (that is, Linear Regression, Random Forest, and the Naive Bayesian classifier). Note that, depending upon the training, the model you get might have different output since we randomly split the dataset into training and testing:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Classifier</strong></span></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Model building time</strong></span></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Model saving time</strong></span></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Accuracy</strong></span></p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Linear regression</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>1199 ms</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>2563 ms</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>0.0%</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>NaÃ¯ve Bayes</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>873 ms</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>2514 ms</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>45%</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; ">
<p>Random forest</p>
</td><td style="border-right: 0.5pt solid ; ">
<p>2120 ms</p>
</td><td style="border-right: 0.5pt solid ; ">
<p>2538 ms</p>
</td><td style="">
<p>91%</p>
</td></tr></tbody></table></div><p>Table 2: Comparison between three classifiers</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note57"></a>Note</h3><p>We avail the preceding output in a machine with a Windows 7(64-bit), Core i7 (2.90GHz) processor, and 32GB of main memory. Therefore, depending upon your OS type and hardware configuration, you might receive different results.</p></div><p>This way the ML model can be made adaptable for the new data type. However, make sure that you use the same classifier or regressor to train and reuse the model to make the ML application adaptable.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec64"></a>Machine learning in dynamic environments</h2></div></div><hr /></div><p>Making a prediction in dynamic environments does not always succeed in producing desired outcomes, particularly in complex and unstructured data.</p><p>There are several reasons for that. For example, how do you infer a realistic outcome from a bit of data or deal with unstructured and high dimensional data that has been found too tedious? Moreover, model revision with efficient strategies to control the realistic environments is also costly.</p><p>Furthermore, sometimes the dimensionality of the input dataset is high. Consequently, data might be too dense or very sparse. In that case, how you deal with very large settings and how to apply the static models in emerging application areas such as robotics, image processing, deep learning, computer vision, or web mining is challenging. On the other hand, ensemble methods are becoming more popular for selecting and combining models from existing models to make the ML model more adaptable. A hierarchical and dynamic environment-based learning is shown in <span class="emphasis"><em>Figure 10:</em></span></p><div class="mediaobject"><img src="graphics/image_08_010.jpg" /><div class="caption"><p>Figure 10: The hierarchy of machine learning in a dynamic environment</p></div></div><p>In this case, ML techniques such as neural networks and statistically-based learning are also becoming popular for their success with numerous applications in industry and research such as biological systems. In particular, classical learning algorithms such as neural networks, decision trees, or vector quantizes are often restricted to purely feedforward settings, and simple vectorial data, instead of dynamic environments. The feature of vectorization often provides a better prediction because of the rich structure. In summary, there are three challenges in developing ML applications in a dynamic environment:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>How does the data structure emerge to shape in an autonomous environment?</p></li><li style="list-style-type: disc"><p>How do we deal with input data that is statistically sparse and high dimensional? More specifically, what about making predictive analysis using online algorithms for large-scale datasets, applying the dimensionality reduction and so on?</p></li></ul></div><p>With only limited reinforcement signals, ill-posed domains, or partially underspecified settings, how do we develop controlled and effective strategies in dynamic environments? Considering these issues and promising advancement in the research, in this section, we will provide some insights into online learning techniques through a statistical and adversarial model. Since learning in a dynamic environment such as streaming will be discussed in <a class="link" href="#" linkend="ch09">Chapter 9</a>, <span class="emphasis"><em>Advanced Machine Learning with Streaming and Graph Data</em></span>, we will not discuss streaming-based learning in this chapter.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec112"></a>Online learning</h3></div></div></div><p>Batch learning techniques generate the best predictor by learning on the entire training dataset at once and are often called static learning. Static learning algorithms take batches of training data to train a model, then a prediction is made using the test sample and the found relationship, whereas online learning algorithms take an initial guess model and then pick up a one-one observation from the training population and recalibrate the weights on each input parameter. Data usually becomes available in a sequential order as batches. The sequential data is used to update the best predictor of the outcome at each step as outlined in <span class="emphasis"><em>Figure 11</em></span>. There are three use cases of online-based learning:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Firstly, where it is computationally infeasible to train an ML model over the entire dataset, an online learning is commonly used</p></li><li style="list-style-type: disc"><p>Secondly, it is also used in a situation where it is necessary for the algorithm to dynamically adapt to new patterns in the data</p></li><li style="list-style-type: disc"><p>Thirdly, it is used when the data itself is generated as a function of time, for example, the stock price prediction</p></li></ul></div><p>Online learning, therefore, requires out-of-core algorithms, that is, algorithms that can perform considering the constraints of networks. There are two general modeling strategies that exist for online learning models:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Statistical learning models</strong></span>: For example, stochastic gradient descent and perceptron</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Adversarial models</strong></span>: For example, spam filtering falls into this category, as the adversary will dynamically generate new spam based on the current behavior of the spam detector</p></li></ul></div><p>Although online and incremental learning techniques are similar, they also differ slightly. In online, it's generally a single pass (epoch=1) or a number of epochs that could be configured, whereas, incremental would mean that you already have a model. No matter how it is built, the model can be mutable by new examples. Also, a combination of online and incremental is often what is required.</p><p>Data is being generated in an unprecedented way everywhere, every day. This huge data imposes an enormous challenge to building ML tools that can handle data with high volume, velocity, and veracity. In short, data generated online is also big data. Therefore, we need to know the technique by which to learn about the online learning algorithms that are meant to handle data with such high volume and velocity with limited performance machines.</p><div class="mediaobject"><img src="graphics/image_08_011.jpg" /><div class="caption"><p>Figure 11: Batch (static) versus online learning, an overview</p></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec113"></a>Statistical learning model</h3></div></div></div><p>As already outlined, in statistically-based learning models such as <span class="strong"><strong>stochastic gradient descents</strong></span>
<span class="strong"><strong>(SGD)</strong></span> and artificial neural networks or perceptron, data samples are assumed to be independent of each other. In addition to this, it is also assumed that the dataset is identically distributed as random variables. In other words, they don't adapt with time. Therefore, an ML algorithm has a limited access to the data.</p><p>In the field of the statistical learning model there are two interpretations that are considered significant:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>First interpretation</strong></span>: This considers the stochastic gradient descent method as applied to the problem of minimizing the expected risks. In an infinite stream of data, the predictive analytics is assumed to be drawn from the normal distribution. Therefore only the stochastic gradient descent method is used to bind the deviation. This interpretation is also valid for the finite training set.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Second interpretation:</strong></span> This applies to the case of a finite training set and considers the SGD algorithm as an instance of incremental gradient descent method. In this case, one instead looks at the empirical risk: Since the gradients of in the incremental gradient descent, iterations are also stochastic estimates of the gradient of, this interpretation but applied to minimize the empirical risk as opposed to the expected risk. For why multiple passes through the data are readily allowed and actually lead to tighter bounds on the deviations.</p></li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec114"></a>Adversarial model</h3></div></div></div><p>Classical machine learning, which is especially taught in classes, emphasizes a static environment where usually unchanging data is used to make predictions. It is, therefore, formally easier compared to a statistical or causal inference or dynamic environment. On the other hand, finding and solving the learning problem as a game between two players, for example, learner versus data generator in a dynamic environment, is an example of an adversarial model. This kind of modeling and making predictive analytics is critically tedious since the world does not know that you are trying to model it formally.</p><p>Furthermore, your model does not have any positive or negative effect on the world. Therefore, the ultimate goal of this kind of model is to minimize losses prevailing from circumstances generated by the move made and played by the other player. The opponent can adapt the data generated based on the output of the learning algorithm in run-time or dynamically. Since no distributional assumptions are made about the data, performing well for the entire sequence that could be viewed ahead of time becomes the ultimate goal. Additionally, regret is to be minimized on the hypothesis at the last pace. According to Cathy O. et al (<span class="emphasis"><em>Weapons of Math Destruction</em></span>, Cathy O'Neil, and Crown, September 6, 2016) t adversarial-based machine learning can be defined as follows:</p><p>Adversarial machine learning is the formal name for studying what happens when conceding even a slightly more realistic alternative to assumptions of these types (harmlessly called <span class="strong"><strong>relaxing assumptions</strong></span>).</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip58"></a>Tip</h3><p>Up to the Spark 2.0.0 release, there was no formal algorithm implemented in the release. Therefore, we were unable to provide any concrete examples that could be further explained elaborately. Interested readers should check the latest Spark release to understand the updates.</p></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec65"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we tried to cover some advanced machine learning techniques to make machine learning models and applications adaptable for new problem and data types.</p><p>We have shown several examples of machine learning algorithms that learn from batch or static-based learning over the data of models that are updated each time they see a new training instance.</p><p>We have also discussed how to make the models adaptable through generalization, through incremental learning, through model reusing, and in dynamic environments.</p><p>In <a class="link" href="#" linkend="ch09">Chapter 9</a>, 
<span class="emphasis"><em>Advanced Machine Learning with Streaming and Graph Data</em></span>, we will guide you on how to apply machine learning techniques with the help of Spark MLlib and Spark ML on streaming and graph data, for example, topic modeling.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch09"></a>Chapter 9.    Advanced Machine Learning with Streaming and Graph Data   </h2></div></div></div><p>This chapter guides the reader on how to apply machine learning techniques with the help
of Spark MLlib, Spark ML and Spark Streaming to streaming and graph data using GraphX. For example, topic modeling from the real-time tweets data from Twitter. The readers will be able to use available APIs to build real-time and predictive applications from streaming data sources such as Twitter. Through the Twitter data analysis, we will show how to perform large scale social sentiment analysis. We will also show how to develop a large-scale movie recommendation using Spark MLlib, which is an implicit part of social network analysis. In a nutshell, the following topics will be covered throughout this chapter:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Developing real-time ML pipelines</p></li><li style="list-style-type: disc"><p>Time series and social network analysis</p></li><li style="list-style-type: disc"><p>Movie recommendation using Spark</p></li><li style="list-style-type: disc"><p>Developing a real-time ML pipeline from streaming</p></li><li style="list-style-type: disc"><p>ML pipeline on graph data and semi-supervised graph-based learning</p></li></ul></div><p>However, what it really needs in order to be an effective and emerging ML application is a continuous flow of labeled data. Consequently, preprocessing the large-scale unstructured data and accurate labeling to that data essentially introduces many unwanted latencies.</p><p>Nowadays, we hear and read a lot about real-time machine learning. More or less, people usually provide this appealing business scenario when discussing sentiment analysis from <span class="strong"><strong>Social Network Services</strong></span> (<span class="strong"><strong>SNS</strong></span>), credit card fraud detection systems, or mining purchase rules related to the customer from business oriented transactional data.</p><p>According to many ML experts, it is possible to continuously update the credit card fraud detection model in real time. It's fantastic, but not realistic to me, for several reasons. Firstly, ensuring the continuous flow of this kind of data is not needed for model retraining. Secondly, creating labeled data would probably be the slowest and the most expensive step in most of the machine learning systems.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec66"></a>Developing real-time ML pipelines</h2></div></div><hr /></div><p>In order to develop a real-time machine learning application, we need to have access to a continuous flow of data. The data might include transactional data, simple texts, tweets from Twitter, messaging or streaming from Flume or Kafka, and so on, as this is mostly unstructured data.</p><p>To deploy these kinds of ML applications, we need to go through a series of steps. The most unreliable source of the data that would serve our purpose is the real-time data from several sources. Often networks are a performance bottleneck.</p><p>For example, it's not guaranteed that you will always receive a bunch of tweets from Twitter. Moreover, labeling this data towards building an ML model on the fly is not a realistic idea. Nevertheless, here we provide a real insight on how we could develop and deploy an ML pipeline from real-time streaming data. <span class="emphasis"><em>Figure 1</em></span> shows the workflow of a real-time ML application development.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec115"></a>Streaming data collection as unstructured text data</h3></div></div></div><p>We would like to stress here that real time stream data collection depends on:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The purpose of the data collection. If the purpose is to develop a credit card fraud detection online, then the data should be collected from your own network through the web API. If the purpose is to collect social media sentiment analysis then data could be collected from Twitter, LinkedIn, Facebook, or newspaper sites, and if the purpose is to network anomaly detection, data could be collected from the network data.</p></li><li style="list-style-type: disc"><p>Data availability is an issue since not all social media platforms provide public APIs for collecting data. The network condition is important since stream data is huge and needs very fast network connectivity.</p></li><li style="list-style-type: disc"><p>Storage capability is an important consideration since a collection of a few minutes of tweets data, for example, could contribute to several GB of data.</p></li></ul></div><p>Moreover, we should wait at least a couple of days before marking the transactions as <span class="emphasis"><em>Fraud</em></span> or <span class="emphasis"><em>Not Fraud</em></span>, for example. In contrast, if somebody reported a fraud transaction, we can immediately label this transaction as <span class="emphasis"><em>Fraud</em></span> for the sake of simplicity.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl3sec83"></a>Labeling the data towards making the supervised machine learning</h4></div></div></div><p>The labeled data set plays a central role in the whole process. It ensures that it is very easy to change the parameters of an algorithm such as the feature normalization or loss function. In that case, we would have several options of choosing the algorithm itself from logistic regression, to <span class="strong"><strong>Support Vector Machine</strong></span> (<span class="strong"><strong>SVM</strong></span>), or random forest, for example.</p><p>However, we cannot change the labeled data set since this information is predefined and your model should predict the labels that you already have. In previous chapters, we have shown that labeling the structured data takes a considerable amount of time.</p><p>Now think about the fully unstructured stream data that we would be receiving from streaming or real-time sources. In that case, labeling the data would take a considerable amount of time. Nevertheless, we will also have to do the pre-processing, such as tokenization, cleaning, indexing, removing stop words, and removing special characters from the unstructured data.</p><p>Now, essentially, there would be a question of<span class="emphasis"><em> how long does the data labeling process take?</em></span> The final thing about the labeled dataset is that we should understand that the labeled dataset might be biased sometimes if we don't do the labeling carefully, which might lead to a lot of issues with the model's performance.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch09lvl4sec2"></a>Creating and building the model</h5></div></div></div><p>For training the sentiment analytics, the credit card fraud detection model, and the association rule mining model, we need to have a lot of examples of transaction data that is as accurate as possible. Once we have the labeled dataset, we are ready to train and build the model:</p><div class="mediaobject"><img src="graphics/B05243_09_01-1024x255.jpg" /><div class="caption"><p>Figure 1: Real-time machine learning workflow.</p></div></div><p>In <a class="link" href="#" linkend="ch07">Chapter 7</a>, <span class="emphasis"><em>Tuning Machine Learning Models</em></span>, we discussed how to choose appropriate models and ML algorithms in order to produce better predictive analytics. The model can be presented as a binary or multiclass classifier with several classes. Alternatively, use an LDA model for the sentiment analysis using the topic modeling concept. In a nutshell, <span class="emphasis"><em>Figure 1</em></span> shows a real-time machine learning workflow.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl3sec84"></a>Real-time predictive analytics</h4></div></div></div><p>When your ML model is properly trained and built, your model is ready for doing the real-time predictive analytics. If you get a good prediction from the model it would be fantastic. However, as we previously mentioned when discussing some accuracy issues such as true positives and false positives, if the number of false positives is high then that means the performance of the model is not satisfactory.</p><p>This essentially means three things: we have not properly labeled the stream dataset, in that case iterate step two (labeling the data in <span class="emphasis"><em>Figure 1</em></span>), or have not selected the proper ML algorithm to train the model, and finally we have not tuned what would eventually help us to find the appropriate hyperparameters or model selection; in that case, go straight to step seven (model deployment in <span class="emphasis"><em>Figure 1</em></span>).</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl3sec85"></a>Tuning the ML model for improvement and model evaluation</h4></div></div></div><p>As mentioned in step four (model evaluation in <span class="emphasis"><em>Figure 1</em></span>), if the performance of the model is not satisfactory or convincing enough, then we need to tune the model. As discussed in <a class="link" href="#" linkend="ch07">Chapter 7</a>, <span class="emphasis"><em>Tuning Machine Learning Models</em></span>, we learned about how to choose the appropriate model and ML algorithms in order to produce better predictive analytics. There are several techniques for tuning the models, performance and we can go for them based on requirements and the situation. When we have done the tuning, finally we should do the model evaluation.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl3sec86"></a>Model adaptability and deployment</h4></div></div></div><p>When we have the tuned and found best model, the machine learning model has to be prepared in order to learn incrementally over the new data types when the model is updated each time it sees a new training instance. When we have our model ready for making the accurate and reliable prediction for the large-scale streaming data, we can deploy it in real life.</p></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec67"></a>Time series and social network analysis</h2></div></div><hr /></div><p>In this section, we will try to provide some insights and challenges of dealing and developing a large-scale ML pipeline from the time series and social network data.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec116"></a>Time series analysis</h3></div></div></div><p>Time series data often arises, when, for example, monitoring industrial processes or tracking corporate business metrics. One of the fundamental differences between modeling data via time series methods is that time series analysis accounts for the fact that data points taken over time may have an internal structure.</p><p>This might include autocorrelation, trends, or seasonal variation that should be taken into account. Regression analysis in this regard is mostly used to test the theories. The target is to test to make sure that the current values of one or more independent times series parameters are correlated to the current properties of other time series data.</p><p>To develop large scale predictive analytics applications, time series analysis techniques can be applied to real-valued, categorical variables, continuous data, discrete numeric data, or even discrete symbolic data. A time series is a sequence of floating-point values, each linked to a timestamp. In particular, we try as hard as possible to stick with <span class="emphasis"><em>time series</em></span> as meaning a univariate time series, although in other contexts, it sometimes refers to a series of multiple values at the same timestamp.</p><p>An instant in the time series data is the vector of values in a collection of time series corresponding to a single point in time. An observation is a tuple (timestamp, key, value), that is, a single value in a time series or instant. In a nutshell, a time series has mainly four characteristics:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Series with trends</strong></span>: Since observations increase or decrease over time, although the trends are persistent and have long term movement.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Series data with seasonality</strong></span>: Since observations stay high then drop off and some patterns repeat from one period to the next, and contain regular periodic fluctuations, say within a 12 month period.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Series data with cyclic component</strong></span>: Since the business model changes periodically, that is, recessions in the business occur in a cyclic order sometimes. It also might contain the repeating swings or movements over more than one year.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Random variation</strong></span>: An unpredictable component that gives time series graphs an irregular or zigzag appearance. It also contains erratic or residual fluctuations.</p></li></ul></div><p>Because of these kinds of challenging characteristics, it really becomes difficult to develop practical machine learning applications for practical purposes. Hence, until now, there is only one package available for time series data analysis, developed by Cloudera; it is called the Spark-TS library. Here each time series is typically labeled with a key that enables identifying it among a collection of time series.</p><p>However, the current implementation of Spark does not provide any implemented algorithm for the time series data analysis. However, since it is an emerging and trending topic of interest, hopefully, we will have at least some algorithms implemented in Spark in coming releases. In <a class="link" href="#" linkend="ch10">Chapter 10</a>, <span class="emphasis"><em>Configuring and Working with External Libraries</em></span>, we will provide more insight into how to use these kinds of third-party packages with Spark.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec117"></a>Social network analysis</h3></div></div></div><p>A social network is made up of <span class="strong"><strong>nodes</strong></span> (points) and associated <span class="strong"><strong>links</strong></span>,where nodes, links, or edges are then identifiable categories of analysis. These nodes might include the information about the people, groups, and organizations. Typically, this information is usually the main priority and concern for any type of social experimentation and analysis. The links in this kind of analysis focus on the collective way to include social contacts and exchangeable information to expand social interaction, such as Facebook, LinkedIn, Twitter, and so on. Therefore, it is obvious that organizations that are embedded in networks of larger social processes, links, and nodes, influence the others.</p><p>On the other hand, according to Otte E.et al. (<span class="emphasis"><em>Social network analysis: A powerful strategy, also for the information sciences</em></span>, Journal of Information Science, 28: 441-453), <span class="strong"><strong>Social Network Analysis</strong></span> (<span class="strong"><strong>SNA</strong></span>) is the study of finding the mapping and measuring the relationships between the connected people or groups. It is also used to find the flows between people, groups, organizations, and information processing entities.</p><p>A proper SNA analysis could be used to show the distinction between the three most popular individual centrality measures: degree centrality, betweenness centrality, and closeness centrality. <span class="strong"><strong>Degree centrality</strong></span> signifies how many links or incidents a node has or how many ties a node has.</p><p>The <span class="strong"><strong>betweenness centrality</strong></span> is a centrality measure of a vertex within a graph. This also considers the edge of betweenness. Moreover, the betweenness centrality signifies the number of times a node acts as a bridge by considering the shortest paths between other nodes.</p><p>On the other hand, the <span class="strong"><strong>closeness centrality</strong></span> of a node is the average length of the shortest path between a particular node and all other nodes in a connected graph, such as a social network.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip59"></a>Tip</h3><p>Interested readers are recommended to read more about the eigenvector centrality, Katz centrality, PageRank centrality, Percolation centrality, Cross-clique centrality, and alpha centrality for the proper understanding of statistical as well as social network centrality.</p></div><p>The social network is often represented as connected graphs (directed or undirected). As a result, it also involves graph data analysis, where people act as nodes and the connections or links act as the edges. Moreover, collecting and analyzing large-scale data and later on developing predictive and descriptive analytics applications from the social network, such as Facebook, Twitter, and LinkedIn also involve social network data analysis, including: link prediction such as predicting relations or friendship, determining communities in social networks such as clustering on graphs, and determining opinion leaders in networks, which is essentially a PageRank problem if the proper structure is done on a graph data.</p><p>Spark has its dedicated API for the analysis of graphs, which is called GraphX. This API can be used, for instance, to search for spam, rank search results, determine communities in social networks, or search for opinion leaders, and it's not a complete list of applying methods for analyzing graphs. We will discuss using the GraphX later in this chapter in more detail.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec68"></a>Movie recommendation using Spark</h2></div></div><hr /></div><p>Model-based collaborative filtering is commonly being used by many companies, such as Netflix, as a recommender system for a real-time movie recommendation. In this section, we will see a complete example of how it works towards recommending movies for new users.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec118"></a>Model-based movie recommendation using Spark MLlib</h3></div></div></div><p>The implementation in Spark MLlib supports the model-based collaborative filtering. In the model based collaborative filtering technique, users and products are described by a small set of factors, also called the <span class="strong"><strong>latent factors </strong></span>(<span class="strong"><strong>LFs</strong></span>).The LFs are then used for predicting the missing entries. Spark API provides the implementation of the <span class="strong"><strong>Alternating Least Squares </strong></span>(also known as the <span class="strong"><strong>ALS</strong></span> widely) algorithm, which is used to learn these latent factors by considering six parameters, including:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><code class="literal">numBlocks</code>
</p></li><li style="list-style-type: disc"><p><code class="literal">rank</code>
</p></li><li style="list-style-type: disc"><p><code class="literal">iterations</code>
</p></li><li style="list-style-type: disc"><p><code class="literal">lambda</code>
</p></li><li style="list-style-type: disc"><p><code class="literal">implicitPrefs</code>
</p></li><li style="list-style-type: disc"><p><code class="literal">alpha</code>
</p></li></ul></div><p>To learn more about these parameters, refer to the recommendation system section in <a class="link" href="#" linkend="ch05">Chapter 5</a>, <span class="emphasis"><em>Supervised and Unsupervised Learning by Examples</em></span>. Note that to construct an ALS instance with default parameters, you can set the value based on your requirements. The default values are as follows: <code class="literal">numBlocks</code>: -1, <code class="literal">rank</code>: 10, <code class="literal">iterations</code>: 10, <code class="literal">lambda</code>: 0.01, <code class="literal">implicitPrefs</code>: false, <code class="literal">alpha</code>: 1.0.</p><p>The construction of an ALS instance, in short, is as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>At first, the ALS, which is an iterative algorithm, is used to model the rating matrix as the multiplication of low-ranked users and product factors</p></li><li style="list-style-type: disc"><p>After that, the learning task is done by using these factors by minimizing the reconstruction error of the observed ratings</p></li></ul></div><p>However, the unknown ratings can successively be calculated by multiplying these factors together.</p><p>The approach for a movie recommendation or any other recommendation based on the collaborative filtering technique used in the Spark MLlib has been proven to be a high performer with high prediction accuracy and scalable for the billions of ratings on commodity clusters used by companies such as Netflix. By following this approach, a company such as Netflix can recommend movies to its subscriber based on the predicted ratings. The ultimate target is to increase the sales, and of course, the customer satisfaction.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl3sec87"></a>Data exploration</h4></div></div></div><p>The movie and the corresponding rating dataset were downloaded from the MovieLens website (<a class="ulink" href="https://movielens.org/movies/1" target="_blank">https://movielens.org</a>). According to the data description on the MovieLens website, all the ratings are described in the <code class="literal">ratings.csv</code> file. Each row of this file followed by the header represents one rating for one movie by one user.</p><p>The CSV dataset has the following columns: <code class="literal">userId</code>, <code class="literal">movieId</code>, <code class="literal">rating</code>, and <code class="literal">timestamp</code> as shown in <span class="emphasis"><em>Figure 2</em></span>. The rows are ordered first by the <code class="literal">userId</code> within the user, by <code class="literal">movieId</code>. Ratings are made on a five-star scale; with half-star increments (0.5 stars up to 5.0 stars). The timestamps represent the seconds since midnight <span class="strong"><strong>Coordinated Universal Time</strong></span> (<span class="strong"><strong>UTC</strong></span>) of January 1, 1970, where we have 105,339 ratings from the 668 users on 10,325 movies:</p><div class="mediaobject"><img src="graphics/B05243_09_02.jpg" /><div class="caption"><p>Figure 2: Sample ratings for the top 20 movies.</p></div></div><p>On the other hand, the movie information is contained in the <code class="literal">movies.csv</code> file. Each row apart from the header information represents one movie containing the columns: <code class="literal">movieId</code>, <code class="literal">title</code>, and <code class="literal">genres</code>.</p><p>Movie titles are either created or inserted manually or imported from the website of the movie database at <a class="ulink" href="https://www.themoviedb.org/" target="_blank">https://www.themoviedb.org/</a>. The release year, however, is shown in the bracket.</p><p>Since movie titles are inserted manually, some errors or inconsistencies may exist in these titles. Readers are therefore recommended to check the IMDb database (<a class="ulink" href="http://www.ibdb.com/" target="_blank">http://www.ibdb.com/</a>) to make sure if there are no inconsistencies or incorrect titles with their corresponding release year.</p><p>Genres are a separated list, and are selected from the following genre categories:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Action, Adventure, Animation, Children's, Comedy, Crime</p></li><li style="list-style-type: disc"><p>Documentary, Drama, Fantasy, Film-Noir, Horror, Musical</p></li><li style="list-style-type: disc"><p>Mystery, Romance, Sci-Fi, Thriller, Western, War</p></li></ul></div><div class="mediaobject"><img src="graphics/B05243_09_03.jpg" /><div class="caption"><p>Figure 3: The title and genres for the top 20 movies.</p></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl3sec88"></a>Movie recommendation using Spark MLlib</h4></div></div></div><p>In this subsection, we will show you how to recommend the movie for other users through a step-by-step example from data collection to movie recommendation. Download the <code class="literal">movies.csv</code> and <code class="literal">ratings.csv</code> files from Packt supplementary documents and place them in your project directory.</p><p><span class="strong"><strong>Step 1: Configure your Spark environment</strong></span></p><p>Here is the code to configure your Spark environment:</p><pre class="programlisting">static SparkSession spark = SparkSession 
      .builder() 
      .appName("JavaLDAExample") 
         .master("local[*]") 
         .config("spark.sql.warehouse.dir", "E:/Exp/"). 
          getOrCreate(); 
</pre><p><span class="strong"><strong>Step 2: Load, parse, and explore the movie and rating Dataset</strong></span></p><p>Here is the code illustrated:</p><pre class="programlisting">String ratigsFile = "input/ratings.csv"; 
Dataset&lt;Row&gt; df1 = spark.read().format("com.databricks.spark.csv").option("header", "true").load(ratigsFile);     
Dataset&lt;Row&gt; ratingsDF = df1.select(df1.col("userId"), df1.col("movieId"), df1.col("rating"),  df1.col("timestamp")); 
ratingsDF.show(false); 
</pre><p>This code segment should return you the <code class="literal">Dataset&lt;Row&gt;</code> of the ratings same as in <span class="emphasis"><em>Figure 2</em></span>. On the other hand, the following code segment shows you the <code class="literal">Dataset&lt;Row&gt;</code> of movies, same as in <span class="emphasis"><em>Figure 3</em></span>:</p><pre class="programlisting">String moviesFile = "input/movies.csv"; 
Dataset&lt;Row&gt; df2 = spark.read().format("com.databricks.spark.csv").option("header", "true").load(moviesFile); 
Dataset&lt;Row&gt; moviesDF = df2.select(df2.col("movieId"), df2.col("title"), df2.col("genres"));  
moviesDF.show(false); 
</pre><p><span class="strong"><strong>Step 3: Register both Datasets as temp tables</strong></span></p><p>To register both Datasets, we can use the following code:</p><pre class="programlisting">ratingsDF.createOrReplaceTempView("ratings"); 
moviesDF.createOrReplaceTempView("movies"); 
</pre><p>This will help to make the in-memory querying faster by creating a temporary view as a table in min-memory. The lifetime of the temporary table using the <code class="literal">createOrReplaceTempView()</code> method is tied to the <code class="literal">[[SparkSession]]</code> that was used to create this Dataset.</p><p><span class="strong"><strong>Step 4: Explore and query for related statistics</strong></span></p><p>Let's check the ratings related statistics. Just use the following code lines:</p><pre class="programlisting">long numRatings = ratingsDF.count(); 
long numUsers = ratingsDF.select(ratingsDF.col("userId")).distinct().count(); 
long numMovies = ratingsDF.select(ratingsDF.col("movieId")).distinct().count(); 
System.out.println("Got " + numRatings + " ratings from " + numUsers + " users on " + numMovies + " movies."); 
</pre><p>You should find 105,339 ratings from 668 users on 10,325 movies. Now, let's get the maximum and minimum ratings along with the count of users who have rated a movie.</p><p>However, you need to perform a SQL query on the rating table we just created in-memory in the previous step. Making a query here is simple, and it is similar to making a query from a MySQL database or RDBMS.</p><p>However, if you are not familiar with SQL-based queries, you are suggested to look at the SQL query specification to find out how to perform a selection using <code class="literal">SELECT</code> from a particular table, how to perform the ordering using <code class="literal">ORDER</code>, and how to perform a joining operation using the <code class="literal">JOIN</code> keyword.</p><p>Well, if you know the SQL query, you should get a new Dataset by using a complex SQL query as follows:</p><pre class="programlisting">Dataset&lt;Row&gt; results = spark.sql("select movies.title, movierates.maxr, movierates.minr, movierates.cntu " + "from(SELECT ratings.movieId,max(ratings.rating) as maxr,"  + "min(ratings.rating) as minr,count(distinct userId) as cntu "  + "FROM ratings group by ratings.movieId) movierates " + "join movies on movierates.movieId=movies.movieId " + "order by movierates.cntu desc"); 
results.show(false); 
</pre><div class="mediaobject"><img src="graphics/B05243_09_04.jpg" /><div class="caption"><p>Figure 4: Maximum and minimum ratings along with the count of users who have rated a movie.</p></div></div><p>To get an insight, we need to know more about the users and their ratings. Now let's find the top most active users and how many times they rated a movie:</p><pre class="programlisting">Dataset&lt;Row&gt; mostActiveUsersSchemaRDD = spark.sql("SELECT ratings.userId, count(*) as ct from ratings " + "group by ratings.userId order by ct desc limit 10"); 
mostActiveUsersSchemaRDD.show(false); 
</pre><div class="mediaobject"><img src="graphics/image_09_006.jpg" /><div class="caption"><p>Figure 5: Number of ratings provided by an individual user.</p></div></div><p>Now let's have a look at a particular user, and find the movies that, say user 668, rated higher than 4:</p><pre class="programlisting">Dataset&lt;Row&gt; results2 = spark.sql("SELECT ratings.userId, ratings.movieId," + "ratings.rating, movies.title FROM ratings JOIN movies "+ "ON movies.movieId=ratings.movieId " + "where ratings.userId=668 and ratings.rating &gt; 4"); 
results2.show(false); 
</pre><div class="mediaobject"><img src="graphics/B05243_09_06.jpg" /><div class="caption"><p>Figure 6: The related rating for user 668.</p></div></div><p><span class="strong"><strong>Step 5: Prepare training and test rating data and see the counts</strong></span></p><p>Here is the code illustrated:</p><pre class="programlisting">Dataset&lt;Row&gt; [] splits = ratingsDF.randomSplit(new double[] { 0.8, 0.2 }); 
Dataset&lt;Row&gt; trainingData = splits[0]; 
Dataset&lt;Row&gt; testData = splits[1]; 
long numTraining = trainingData.count(); 
long numTest = testData.count(); 
System.out.println("Training: " + numTraining + " test: " + numTest); 
</pre><p>You should find that there are 84,011 ratings in the training and 21,328 ratings in the test Dataset.</p><p><span class="strong"><strong>Step 6: Prepare the data for building the recommendation model using ALS</strong></span></p><p>The following code illustrates for building the recommendation model using APIs:</p><pre class="programlisting">JavaRDD&lt;Rating&gt; ratingsRDD = trainingData.toJavaRDD().map(new Function&lt;Row, Rating&gt;() { 
      @Override 
      public Rating call(Row r) throws Exception { 
        // TODO Auto-generated method stub 
        int userId = Integer.parseInt(r.getString(0)); 
        int movieId = Integer.parseInt(r.getString(1)); 
        double ratings = Double.parseDouble(r.getString(2)); 
        return new Rating(userId, movieId, ratings); 
      } 
    }); 
</pre><p>The <code class="literal">ratingsRDD</code> RDD will contain the <code class="literal">userId</code>, <code class="literal">movieId</code>, and corresponding ratings from the training dataset that we prepared in the previous step. On the other hand, the following <code class="literal">testRDD</code> also contains the same information coming from the test Dataset we prepared in the previous step:</p><pre class="programlisting">JavaRDD&lt;Rating&gt; testRDD = testData.toJavaRDD().map(new Function&lt;Row, Rating&gt;() { 
      @Override 
      public Rating call(Row r) throws Exception { 
        int userId = Integer.parseInt(r.getString(0)); 
        int movieId = Integer.parseInt(r.getString(1)); 
        double ratings = Double.parseDouble(r.getString(2)); 
        return new Rating(userId, movieId, ratings); 
      } 
    }); 
</pre><p><span class="strong"><strong>Step 7: Build an ALS user product matrix</strong></span></p><p>Build an ALS user matrix model based on the <code class="literal">ratingsRDD</code> by specifying the rank, iterations, and lambda:</p><pre class="programlisting">int rank = 20; 
int numIterations = 10; 
double lambda = 0.01; 
MatrixFactorizationModel model = ALS.train(JavaRDD.toRDD(ratingsRDD), rank, numIterations, 0.01); 
</pre><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Note that we have randomly selected the value of rank as <code class="literal">20</code> and have iterated the model for learning for 10 times and the lambda as <code class="literal">0.01</code>. With this setting, we got a good prediction accuracy. Readers are suggested to apply the hyper-parameter tuning to get to know the most optimum values for these parameters.</p></li><li style="list-style-type: disc"><p>However, readers are suggested to change the value of these two parameters based on their dataset. Moreover, as mentioned earlier, they also can use and specify other parameters such as <code class="literal">numberblock</code>, <code class="literal">implicitPrefs</code>, and <code class="literal">alpha</code> if the prediction performance is not satisfactory. Furthermore, set the number of blocks for both user blocks and product blocks to parallelize the computation into <code class="literal">pass -1</code> for an auto-configured number of blocks. The value is <code class="literal">-1</code>.</p></li></ul></div><p><span class="strong"><strong>Step 8: Making predictions</strong></span></p><p>Let's get the top six movie predictions for user 668:</p><pre class="programlisting">System.out.println("Rating:(UserID, MovieId, Rating)"); 
Rating[] topRecsForUser = model.recommendProducts(668, 6); 
for (Rating rating : topRecsForUser) 
System.out.println(rating.toString()); 
</pre><div class="mediaobject"><img src="graphics/B05243_09_07.jpg" /><div class="caption"><p>Figure 7: Top six movies rated by the user 668.</p></div></div><p><span class="strong"><strong>Step 9: Get the predicted ratings to compare with the test ratings</strong></span></p><p>Here is the code illustrated:</p><pre class="programlisting">JavaRDD&lt;Tuple2&lt;Object, Object&gt;&gt; testUserProductRDD = testData.toJavaRDD() 
        .map(new Function&lt;Row, Tuple2&lt;Object, Object&gt;&gt;() { 
          @Override 
          public Tuple2&lt;Object, Object&gt; call(Row r) throws Exception { 
 
            int userId = Integer.parseInt(r.getString(0)); 
            int movieId = Integer.parseInt(r.getString(1)); 
            double ratings = Double.parseDouble(r.getString(2)); 
            return new Tuple2&lt;Object, Object&gt;(userId, movieId); 
          } 
        }); 
JavaRDD&lt;Rating&gt; predictionsForTestRDD = model.predict(JavaRDD.toRDD(testUserProductRDD)).toJavaRDD(); 
</pre><p>Now let's check the top 10 prediction for 10 users:</p><pre class="programlisting">System.out.println(predictionsForTestRDD.take(10).toString()); 
</pre><p><span class="strong"><strong>Step 10: Prepare predictions</strong></span></p><p>Here we will prepare the predictions related to RDDs in two steps. The first step includes preparing predictions for comparison from the <code class="literal">predictionsForTestRDD</code> RDD structure. It goes as follows:</p><pre class="programlisting">JavaPairRDD&lt;Tuple2&lt;Integer, Integer&gt;, Double&gt; predictionsKeyedByUserProductRDD = JavaPairRDD.fromJavaRDD( 
        predictionsForTestRDD.map(new Function&lt;Rating, Tuple2&lt;Tuple2&lt;Integer, Integer&gt;, Double&gt;&gt;() { 
          @Override 
          public Tuple2&lt;Tuple2&lt;Integer, Integer&gt;, Double&gt; call(Rating r) throws Exception { 
            return new Tuple2&lt;Tuple2&lt;Integer, Integer&gt;, Double&gt;( 
                new Tuple2&lt;Integer, Integer&gt;(r.user(), r.product()), r.rating()); 
          } 
        })); 
</pre><p>The second step includes preparing the test for comparison:</p><pre class="programlisting">JavaPairRDD&lt;Tuple2&lt;Integer, Integer&gt;, Double&gt; testKeyedByUserProductRDD = JavaPairRDD  .fromJavaRDD(testRDD.map(new Function&lt;Rating, Tuple2&lt;Tuple2&lt;Integer, Integer&gt;, Double&gt;&gt;() { 
          @Override 
          public Tuple2&lt;Tuple2&lt;Integer, Integer&gt;, Double&gt; call(Rating r) throws Exception { 
            return new Tuple2&lt;Tuple2&lt;Integer, Integer&gt;, Double&gt;( 
                new Tuple2&lt;Integer, Integer&gt;(r.user(), r.product()), r.rating()); 
          } 
        })); 
</pre><p><span class="strong"><strong>Step 11: Join the test with predictions and see the combined ratings</strong></span></p><p>Join <code class="literal">testKeyedByUserProductRDD</code> and <code class="literal">predictionsKeyedByUserProductRDD</code> RDDs to get the combined test as well as predicted ratings against each user and <code class="literal">movieId</code>:</p><pre class="programlisting">JavaPairRDD&lt;Tuple2&lt;Integer, Integer&gt;, Tuple2&lt;Double, Double&gt;&gt; testAndPredictionsJoinedRDD = testKeyedByUserProductRDD 
        .join(predictionsKeyedByUserProductRDD); 
System.out.println("(UserID, MovieId) =&gt; (Test rating, Predicted rating)"); 
System.out.println("----------------------------------"); 
for (Tuple2&lt;Tuple2&lt;Integer, Integer&gt;, Tuple2&lt;Double, Double&gt;&gt; t : testAndPredictionsJoinedRDD.take(6)) { 
      Tuple2&lt;Integer, Integer&gt; userProduct = t._1; 
      Tuple2&lt;Double, Double&gt; testAndPredictedRating = t._2; 
      System.out.println("(" + userProduct._1() + "," + userProduct._2() + ") =&gt; (" + testAndPredictedRating._1() 
          + "," + testAndPredictedRating._2() + ")"); 
    } 
</pre><div class="mediaobject"><img src="graphics/B05243_09_08.jpg" /><div class="caption"><p>Figure 8: Combined test as well as predicted ratings against each user and movieId.</p></div></div><p><span class="strong"><strong>Step 12: Evaluating the model against prediction performance</strong></span></p><p>Let's check the performance of the ALS model by checking the number of true positives and false positives:</p><pre class="programlisting">JavaPairRDD&lt;Tuple2&lt;Integer, Integer&gt;, Tuple2&lt;Double, Double&gt;&gt; truePositives = testAndPredictionsJoinedRDD 
        .filter(new Function&lt;Tuple2&lt;Tuple2&lt;Integer, Integer&gt;, Tuple2&lt;Double, Double&gt;&gt;, Boolean&gt;() { 
          @Override 
          public Boolean call(Tuple2&lt;Tuple2&lt;Integer, Integer&gt;, Tuple2&lt;Double, Double&gt;&gt; r) throws Exception { 
            return (r._2._1() &lt;= 1 &amp;&amp; r._2._2() &lt; 5); 
          } 
        }); 
</pre><p>Now print the number of true positive predictions. We have considered that a prediction is a true prediction when the predicted rating is less than the highest rating (that is, <code class="literal">5</code>). Consequently, if the predicted rating is more or equal to <code class="literal">5</code>, consider that prediction as a false positive:</p><pre class="programlisting">for (Tuple2&lt;Tuple2&lt;Integer, Integer&gt;, Tuple2&lt;Double, Double&gt;&gt; t : truePositives.take(2)) { 
      Tuple2&lt;Integer, Integer&gt; userProduct = t._1; 
      Tuple2&lt;Double, Double&gt; testAndPredictedRating = t._2; 
    } 
System.out.println("Number of true positive prediction is: "+ truePositives.count()); 
</pre><p>You should find the value as follows.</p><p>The number of true positive prediction is 798. Now it's time to print the statistics for the false positives:</p><pre class="programlisting">JavaPairRDD&lt;Tuple2&lt;Integer, Integer&gt;, Tuple2&lt;Double, Double&gt;&gt; falsePositives = testAndPredictionsJoinedRDD 
        .filter(new Function&lt;Tuple2&lt;Tuple2&lt;Integer, Integer&gt;, Tuple2&lt;Double, Double&gt;&gt;, Boolean&gt;() { 
          @Override 
          public Boolean call(Tuple2&lt;Tuple2&lt;Integer, Integer&gt;, Tuple2&lt;Double, Double&gt;&gt; r) throws Exception { 
            return (r._2._1() &lt;= 1 &amp;&amp; r._2._2() &gt;= 5); 
          } 
        }); 
for (Tuple2&lt;Tuple2&lt;Integer, Integer&gt;, Tuple2&lt;Double, Double&gt;&gt; t : falsePositives.take(2)) { 
      Tuple2&lt;Integer, Integer&gt; userProduct = t._1; 
      Tuple2&lt;Double, Double&gt; testAndPredictedRating = t._2; 
    } 
System.out.println("Number of false positive prediction is: "+ falsePositives.count()); 
</pre><p>In this particular example, we have got only 14 false positive predictions, which is outstanding. Now let's check the performance of the prediction in terms of mean absolute error calculation between the test and predictions:</p><pre class="programlisting">double meanAbsoluteError = JavaDoubleRDD        .fromRDD(testAndPredictionsJoinedRDD.values().map(new Function&lt;Tuple2&lt;Double, Double&gt;, Object&gt;() { 
          public Object call(Tuple2&lt;Double, Double&gt; pair) { 
            Double err = pair._1() - pair._2(); 
            return err * err; 
          } 
        }).rdd()).mean(); 
    System.out.printing("Mean Absolute Error: "+meanAbsoluteError); 
</pre><p>Which returns the value as follows:</p><pre class="programlisting">Mean Absolute Error: 1.5800601618477566 
</pre></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec69"></a>Developing a real-time ML pipeline from streaming</h2></div></div><hr /></div><p>According to the API guidelines provided by Spark at <a class="ulink" href="http://spark.apache.org/docs/latest/streaming-programming-guide.html" target="_blank">http://spark.apache.org/docs/latest/streaming-programming-guide.html</a>, technically, Spark Streaming receives live input data streams as objects (objects could be <code class="literal">Java/Python/R</code> objects). Later on, the streams are divided into batches, which are then processed by the Spark engine to generate the final input stream in batches. To make this process even easier, Spark Streaming provides a high-level abstraction, which is also called a discretized stream or DStream.</p><p>The <span class="strong"><strong>DStream</strong></span> represents a continuous stream of data coming from real-time streaming sources such as Twitter, Kafka, Fume, Kinesis, Sensors, or any other sources. Discretized streams can be created from these sources, alternatively, high-level operations on other DStreams can also be applied for doing that. Internally, a DStream is represented as a sequence of RDDs, that means the RDD abstraction has been reused for processing the stream of RDDs.</p><p>As already discussed in <a class="link" href="#" linkend="ch06">Chapter 6</a>, <span class="emphasis"><em>Building Large Scale Machine Learning Pipelines</em></span>, a topic modeling technique automatically infers the topics discussed and inherently places them in a collection of documents as hidden resources. This is commonly used in the <span class="strong"><strong>Natural Language Processing</strong></span> (<span class="strong"><strong>NLP</strong></span>) and text mining tasks. These topics can be used to analyze, summarize, and organize those documents. Alternatively, those topics can be used for featurization and dimensionality reduction in later stages of a <span class="strong"><strong>machine learning</strong></span> (<span class="strong"><strong>ML</strong></span>) pipeline development. The most popular topic modeling algorithms are <span class="strong"><strong>Latent Dirichlet Allocation </strong></span>(<span class="strong"><strong>LDA</strong></span>) and <span class="strong"><strong>Probabilistic Latent Semantic Analysis</strong></span> (<span class="strong"><strong>pLSA</strong></span>). Previously we discussed how to apply the LDA algorithm for the static dataset that is already available. However, if the topic modeling is prepared from the real-time streaming data, that would be great and would be more live in knowing the trends in social media such as Twitter, LinkedIn, or Facebook.</p><p>However, due to the limited API facility by Facebook or LinkedIn, it would be difficult to collect the real-time data from those social media platforms. Spark also provides the API for accessing data from Twitter, Kafka, Flume, and Kinesis. The workflow of a near real-time ML application development from streaming data should follow the workflows as presented in <span class="emphasis"><em>Figure 9</em></span>:</p><div class="mediaobject"><img src="graphics/B05243_09_09-1024x322.jpg" /><div class="caption"><p>Figure 9: Real-time predictive ML model development from streaming data using Spark.</p></div></div><p>In this section, we will show you how to develop a real-time ML pipeline that handles streaming data. More specifically, we will show a step-by-step topic modeling from Twitter streaming data. The topic modeling here has two steps: Twitter data collection and topic modeling using LDA.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec119"></a>Real-time tweet data collection from Twitter</h3></div></div></div><p>Spark provides APIs to access real-time tweets from the Twitter timeline. The tweets can be further made by using keywords or hashtags. Alternatively, tweets can also be downloaded from someone's Twitter timeline. However, before accessing the tweets data, you will have to create a sample Twitter application on Twitter and generate four keys: <code class="literal">consumerKey</code>, <code class="literal">consumerSecret</code>, <code class="literal">accessToken</code>, and <code class="literal">accessTokenSecret</code>.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip60"></a>Tip</h3><p>Note that Twitter (and some other) driver support has been removed during Spark upgrade from 1.6.2 to 2.0.0.  This means streaming data collection support using Spark from less used streaming connectors, including Twitter, Akka, MQTT, and ZeroMQ has been removed in Spark 2.0.0. Therefore, it is not possible to develop an application for Twitter data collection using Spark 2.0.0. Consequently, Spark 1.6.1 will be used for the demonstration for Twitter data collection in this section. Readers are suggested to create a Maven project on Eclipse using the provided Maven friendly <code class="literal">pom.xml</code> file. </p></div><p>After authenticating your Spark ML application to collect data from Twitter, you will have to define the <code class="literal">JavaStreamingContext</code> by specifying the <code class="literal">SparkConf</code> and duration for collecting tweets. After that, tweets data can be downloaded as DStream or discrete stream through the <code class="literal">TwitterUtils</code> API of Spark once you revoke the <code class="literal">start()</code> method using the <code class="literal">JavaStreamingContext</code> object.</p><p>Upon starting to receive the tweets, you can save the tweets data on your local machine or HDFS or any other filesystem where applicable. However, the streaming will be continued until you terminate the streaming using <code class="literal">awaitTermination()</code>.</p><p>The received tweets, however, can be also be pre-processed or cleaned by using the <code class="literal">foreachRDD</code> design pattern and then can be saved to your desired location. Due to page limitation, we have limited our discussion here.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip61"></a>Tip</h3><p>Moreover, interested readers should follow the API guidelines for Spark Streaming in the following web page of Spark:<a class="ulink" href="http://spark.apache.org/docs/latest/streaming-programming-guide.html" target="_blank">http://spark.apache.org/docs/latest/streaming-programming-guide.html</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl3sec89"></a>Tweet collection using TwitterUtils API of Spark</h4></div></div></div><p>In this sub-section, at first, we will show you how to collect real-time tweets data from Twitter using the <code class="literal">TwitterUtils</code> API. Then the same tweets data will be used for topic modeling in the next sub-section.</p><p><span class="strong"><strong>Step 1: Load required packages and APIs</strong></span></p><p>Here is the code to load the required packages:</p><pre class="programlisting">import org.apache.log4j.Level; 
import org.apache.log4j.Logger; 
import org.apache.spark.SparkConf; 
import org.apache.spark.api.java.JavaRDD; 
import org.apache.spark.api.java.function.Function; 
import org.apache.spark.streaming.Duration; 
import org.apache.spark.streaming.api.java.JavaDStream; 
import org.apache.spark.streaming.api.java.JavaStreamingContext; 
import org.apache.spark.streaming.twitter.TwitterUtils; 
import twitter4j.Status;  
</pre><p><span class="strong"><strong>Step 2: Setting the Logger level</strong></span></p><p>For setting the Logger level, we use the following code:</p><pre class="programlisting">Logger.getLogger("org").setLevel(Level.OFF); 
Logger.getLogger("akka").setLevel(Level.OFF); 
Logger.getLogger("org.apache.spark").setLevel(Level.WARN); 
Logger.getLogger("org.eclipse.jetty.server").setLevel(Level.OFF);  
</pre><p><span class="strong"><strong>Step 3: Spark streaming environment setting</strong></span></p><p>Here is the codes for Spark streaming illustrated:</p><pre class="programlisting">SparkConf conf = new SparkConf().setMaster("local[*]").setAppName("TwitterExample"); 
JavaStreamingContext jssc = new JavaStreamingContext(conf, new Duration(1000)); 
</pre><p><span class="strong"><strong>Step 4: Setting the authentication for accessing Twitter data</strong></span></p><p>Get the authentications values from the sample Twitter application by visiting the following URL: <a class="ulink" href="https://apps.twitter.com/" target="_blank">https://apps.twitter.com/</a>:</p><pre class="programlisting">String consumerKey = "VQINrM6ZcNqaCAawA6IN4xRTP"; 
String consumerSecret = "F2OsVEuJypOZSAoNDFrWgoCHyNJNXbTr8T3yEbp9cWEYjTctye"; 
String accessToken = "475468363-IfRcZnbkEVPRw6bwXovMnw1FsbxuetvEF2JvbAvD"; 
String accessTokenSecret = "vU7VtzZVyugUHO7ddeTvucu1wRrCZqFTPJUW8VAe6xgyf"; 
</pre><p>Note that here we have provided the same values for these four secret keys. Replace these values with your own keys accordingly. Well, now we need to set the system property using <code class="literal">twitter4j.oauth</code> for the previous four keys:</p><pre class="programlisting">System.setProperty("twitter4j.oauth.consumerKey", consumerKey); 
System.setProperty("twitter4j.oauth.consumerSecret", consumerSecret); 
System.setProperty("twitter4j.oauth.accessToken", accessToken); 
System.setProperty("twitter4j.oauth.accessTokenSecret", accessTokenSecret); 
</pre><p><span class="strong"><strong>Step 5: Enable the check pointing</strong></span></p><p>The following code shows how to enable the check pointing:</p><pre class="programlisting">jssc.checkpoint("src/main/resources/twitterdata/"); 
</pre><p>The metadata check pointing is primarily needed for recovery from driver failures, whereas data or RDD check pointing is necessary even for basic functioning if stateful transformations are used. For more details, please visit the following web page of Spark: <a class="ulink" href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#checkpointing" target="_blank">http://spark.apache.org/docs/latest/streaming-programming-guide.html#checkpointing</a>.</p><p><span class="strong"><strong>Step 6: Start accepting stream of tweets as a discrete stream</strong></span></p><p>Let's collect only 100 tweets for simplicity and but can collect as much as you want:</p><pre class="programlisting">JavaDStream&lt;Status&gt; tweets = TwitterUtils.createStream(jssc); 
final String outputDirectory="src/main/resources/twitterdata/"; 
final long numTweetsToCollect = 100; 
</pre><p><span class="strong"><strong>Step 7: Filter tweets and save as regular text files</strong></span></p><p>The code for filter tweets is shown here:</p><pre class="programlisting">tweets.foreachRDD(new Function&lt;JavaRDD&lt;Status&gt;, Void&gt;() { 
      public long numTweetsCollected = 0; 
      @Override 
      public Void call(JavaRDD&lt;Status&gt; status) throws Exception {         
        long count = status.count(); 
        if (count &gt; 0) { 
          status.saveAsTextFile(outputDirectory + "/tweets_" + System.currentTimeMillis()); 
          numTweetsCollected += count; 
          if (numTweetsCollected &gt;= numTweetsToCollect) { 
               System.exit(0); 
          } 
        } 
        return null; 
      } 
    }); 
</pre><p>Here we are pre-processing the tweets using the singleton method, <code class="literal">foreachRDD</code>, which accepts only filtered tweets, that is, if the status count is at least 1. When the number of collected tweets is equal or more than the number of tweets to be collected, then we exit the collection. Finally, we save the tweets as texts in the output directory.</p><p><span class="strong"><strong>Step 8: Controlling the streaming switch</strong></span></p><p>The code for controlling the streaming switch is shown here:</p><pre class="programlisting">jssc.start(); 
jssc.awaitTermination();  
</pre><p>Eventually, we will use these texts of tweets for the topic modeling in the next step. If you recall the topic modeling in <a class="link" href="#" linkend="ch06">Chapter 6</a>, <span class="emphasis"><em>Building Scalable Machine Learning Pipelines</em></span>, we saw the corresponding term weight, topic name, and term indices. However, we also need to have the actual terms. In the next step, we will show the detailed technique of retrieving the terms extensively dependent on the vocabulary that needs to be created for that.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec120"></a>Topic modeling using Spark</h3></div></div></div><p>In this sub-section, we represented a semi-automated technique of topic modeling using Spark. The following steps show the topic modeling from data reading to printing the topics along with their term-weights.</p><p><span class="strong"><strong>Step 1: Load necessary packages and APIs</strong></span></p><p>Here is the code to load the necessary packages:</p><pre class="programlisting">import java.io.Serializable; 
import java.util.ArrayList; 
import java.util.HashMap; 
import java.util.Map; 
import org.apache.log4j.Level; 
import org.apache.log4j.Logger; 
import org.apache.spark.SparkConf; 
import org.apache.spark.api.java.JavaPairRDD; 
import org.apache.spark.api.java.JavaRDD; 
import org.apache.spark.api.java.JavaSparkContext; 
import org.apache.spark.api.java.function.Function; 
import org.apache.spark.api.java.function.Function2; 
import org.apache.spark.api.java.function.PairFlatMapFunction; 
import org.apache.spark.api.java.function.PairFunction; 
import org.apache.spark.ml.feature.StopWordsRemover; 
import org.apache.spark.mllib.clustering.LDA; 
import org.apache.spark.mllib.clustering.LDAModel; 
import org.apache.spark.mllib.linalg.Vector; 
import org.apache.spark.mllib.linalg.Vectors; 
import org.apache.spark.sql.SQLContext; 
import scala.Tuple2; 
</pre><p><span class="strong"><strong>Step 2: Configure the Spark environment</strong></span></p><p>Here is the code to configure the Spark:</p><pre class="programlisting">private transient static SparkConf sparkConf = new SparkConf().setMaster("local[*]").setAppName("TopicModelingLDA"); 
private transient static JavaSparkContext jsc = new JavaSparkContext(sparkConf); 
private transient static SQLContext sqlContext = new org.apache.spark.sql.SQLContext(jsc); 
</pre><p><span class="strong"><strong>Step 3: Setting the logging level</strong></span></p><p>Here is the code to set the logging level:</p><pre class="programlisting">Logger.getLogger("org").setLevel(Level.OFF); 
Logger.getLogger("akka").setLevel(Level.OFF); 
Logger.getLogger("org.apache.spark").setLevel(Level.WARN); 
Logger.getLogger("org.eclipse.jetty.server").setLevel(Level.OFF); 
</pre><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note62"></a>Note</h3><p>Note that setting the logging level that was just previously shown is optional.</p></div><p><span class="strong"><strong>Step 4: Create Java RDD and cache them in-memory</strong></span></p><p>Create Java RDD and cache them for the tweets data from the previous step:</p><pre class="programlisting">JavaRDD&lt;String&gt; data = jsc.wholeTextFiles("src/main/resources/test/*.txt") 
        .map(new Function&lt;Tuple2&lt;String, String&gt;, String&gt;() { 
          @Override 
          public String call(Tuple2&lt;String, String&gt; v1) throws Exception {     
            return v1._2; 
          } 
        }).cache(); 
</pre><p><span class="strong"><strong>Step 5: Tokenize the terms</strong></span></p><p>Load the list of stop-words provided by Spark and tokenize the terms by filtering them by applying three constraints: text length at least 4, not a stop word, and making them each lower case. Note that we discussed stop-words in <a class="link" href="#" linkend="ch06">Chapter 6</a>, <span class="emphasis"><em>Building Scalable Machine Learning Pipelines</em></span>:</p><pre class="programlisting">public static String[] stopwords = new StopWordsRemover().getStopWords(); 
JavaRDD&lt;String[]&gt; tokenized = data.map(new Function&lt;String, String[]&gt;() { 
list.toArray(new String[0]); 
      }      @Override 
      public String[] call(String v1) throws Exception { 
        ArrayList&lt;String&gt; list = new ArrayList&lt;&gt;(); 
        for (String s : v1.split("\\s")) { 
          if (s.length() &gt; 3 &amp;&amp; !isStopWord(s) &amp;&amp; isOnlyLetter(s)) 
            list.add(s.toLowerCase()); 
        } 
        return 
    }); 
</pre><p><span class="strong"><strong>Step 6: Prepare the term counts</strong></span></p><p>Prepare the term counts by filtering them by applying four constraints: text length at least 4, not stop words, selecting only characters, and making them all lower case:</p><pre class="programlisting">JavaPairRDD&lt;String, Integer&gt; termCounts = data 
        .flatMapToPair(new PairFlatMapFunction&lt;String, String, Integer&gt;() { 
          @Override 
          public Iterable&lt;Tuple2&lt;String, Integer&gt;&gt; call(String t) throws Exception {     
            ArrayList&lt;Tuple2&lt;String, Integer&gt;&gt; tc = new ArrayList&lt;&gt;();   
            for (String s : t.split("\\s")) { 
 
              if (s.length() &gt; 3 &amp;&amp; !isStopWord(s) &amp;&amp; isOnlyLetter(s)) 
                tc.add(new Tuple2&lt;String, Integer&gt;(s.toLowerCase(), 1)); 
            } 
            return tc; 
          } 
        }).reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() { 
          @Override 
          public Integer call(Integer v1, Integer v2) throws Exception {     
            return v1 + v2; 
          } 
        }); 
</pre><p>Note that here <code class="literal">isStopWords()</code> and <code class="literal">isOnlyLetters()</code> are two user defined methods that will be discussed at the end of this step.</p><p><span class="strong"><strong>Step 7: Sort the term counts</strong></span></p><p>Sort the terms counts by applying two transformations, <code class="literal">sortByKey()</code> and <code class="literal">mapToPair()</code>:</p><pre class="programlisting">JavaPairRDD&lt;String, Integer&gt; termCountsSorted = termCounts 
        .mapToPair(new PairFunction&lt;Tuple2&lt;String, Integer&gt;, Integer, String&gt;() { 
          @Override 
          public Tuple2&lt;Integer, String&gt; call(Tuple2&lt;String, Integer&gt; t) throws Exception { 
            return t.swap(); 
          } 
        }).sortByKey().mapToPair(new PairFunction&lt;Tuple2&lt;Integer, String&gt;, String, Integer&gt;() { 
          @Override 
          public Tuple2&lt;String, Integer&gt; call(Tuple2&lt;Integer, String&gt; t) throws Exception { 
            return t.swap(); 
          } 
        }); 
</pre><p><span class="strong"><strong>Step 8: Create the vocabulary</strong></span></p><p>Create a vocabulary RDD by mapping the sorted term counts. Finally, print the key value pairs:</p><pre class="programlisting">JavaRDD&lt;String&gt; vocabArray = termCountsSorted.map(new Function&lt;Tuple2&lt;String, Integer&gt;, String&gt;() { 
      @Override 
      public String call(Tuple2&lt;String, Integer&gt; v1) throws Exception { 
        return v1._1; 
      } 
    }); 
final Map&lt;String, Long&gt; vocab = vocabArray.zipWithIndex().collectAsMap(); 
    for (Map.Entry&lt;String, Long&gt; entry : vocab.entrySet()) { 
      System.out.println(entry.getKey() + "/" + entry.getValue()); 
    } 
</pre><p>Let's see a screenshot of the vocabulary terms and their indices shown in <span class="emphasis"><em>Figure 10</em></span>:</p><div class="mediaobject"><img src="graphics/B05243_09_10.jpg" /><div class="caption"><p>Figure 10: Vocabulary terms and their indices.</p></div></div><p><span class="strong"><strong>Step 9: Create the document matrix from the tokenized words/terms</strong></span></p><p>Create the document matrix as <code class="literal">JavaPairRDD</code> from the tokenized terms by mapping the vocabulary we created in the previous step. After that, cache the RDD in-memory for faster processing:</p><pre class="programlisting">JavaPairRDD&lt;Long, Vector&gt; documents = JavaPairRDD 
        .fromJavaRDD(tokenized.zipWithIndex().map(new Function&lt;Tuple2&lt;String[], Long&gt;, Tuple2&lt;Long, Vector&gt;&gt;() { 
          @Override 
          public Tuple2&lt;Long, Vector&gt; call(Tuple2&lt;String[], Long&gt; v1) throws Exception { 
            String[] tokens = v1._1; 
            Map&lt;Integer, Double&gt; counts = new HashMap(); 
 
            for (String s : tokens) { 
              if (vocab.containsKey(s)) { 
                long idx = vocab.get(s); 
                int a = (int) idx; 
                if (counts.containsKey(a)) { 
                  counts.put(a, counts.get(a) + 1.0); 
                } else 
                  counts.put(a, 0.0); 
              } 
            } 
            ArrayList&lt;Tuple2&lt;Integer, Double&gt;&gt; ll = new ArrayList&lt;&gt;(); 
            ArrayList&lt;Double&gt; dd = new ArrayList&lt;&gt;(); 
 
            for (Map.Entry&lt;Integer, Double&gt; entry : counts.entrySet()) { 
              ll.add(new Tuple2&lt;Integer, Double&gt;(entry.getKey(), entry.getValue())); 
              dd.add(entry.getValue()); 
            } 
 
            return new Tuple2&lt;Long, Vector&gt;(v1._2, Vectors.sparse(vocab.size(), ll)); 
          } 
        })).cache(); 
</pre><p><span class="strong"><strong>Step 10: Train the LDA model</strong></span></p><p>Train the LDA model using the documents matrix from step 9 and describe 10 topic terms against four topics for simplicity.</p><p>Note that here we have used <span class="strong"><strong>Latent Dirichilet Allocation </strong></span>(<span class="strong"><strong>LDA</strong></span>), which is one of the most popular topic modeling algorithms commonly used for text mining. We could use more robust topic modeling algorithms such as <span class="strong"><strong>Probabilistic Latent Sentiment Analysis</strong></span> (<span class="strong"><strong>pLSA</strong></span>), <span class="strong"><strong>Pachinko Allocation Model</strong></span> (<span class="strong"><strong>PAM</strong></span>), or <span class="strong"><strong>Hierarchical Dirichilet Process</strong></span> (<span class="strong"><strong>HDP</strong></span>) algorithms. However, pLSA has the overfitting problem.</p><p>On the other hand, both HDP and PAM are more complex topic modeling algorithms used for complex text mining such as mining topics from high dimensional text data or documents of unstructured text. Moreover, to this date, Spark has implemented only one topic modeling algorithm, that is LDA. Therefore, we have to use LDA reasonably:</p><pre class="programlisting">LDAModel ldaModel = new LDA().setK(4).setMaxIterations(10).run(documents); 
Tuple2&lt;int[], double[]&gt;[] topicDesces = ldaModel.describeTopics(10); 
int topicCount = topicDesces.length; 
</pre><p>Note that to keep the topic generation simple, we have set the number of the topic as 4 and have iterated the LDA 10 times. Another reason is that in the next section we want to show how to connect these four topics through their common terms. Readers are recommended to change the value based on their requirements.</p><p><span class="strong"><strong>Step 11: Get the topic terms, index, term weights, and total sum across each topic</strong></span></p><p>Get these statistics from the vocabulary and topic description described in step 10 and step 8:</p><pre class="programlisting">for (int t = 0; t &lt; topicCount; t++) { 
      Tuple2&lt;int[], double[]&gt; topic = topicDesces[t]; 
      System.out.println("      Topic: " + t); 
      int[] indices = topic._1(); 
      double[] values = topic._2(); 
      double sum = 0.0d; 
      int wordCount = indices.length; 
      System.out.println("Terms |\tIndex |\tWeight"); 
      System.out.println("------------------------"); 
      for (int w = 0; w &lt; wordCount; w++) { 
        double prob = values[w]; 
        int vocabIndex = indices[w]; 
        String vocabKey = ""; 
        for (Map.Entry&lt;String, Long&gt; entry : vocab.entrySet()) { 
          if (entry.getValue() == vocabIndex) { 
            vocabKey = entry.getKey(); 
            break; 
          } } 
System.out.format("%s \t %d \t %f \n", vocabKey, vocabIndex, prob); 
        sum += prob; 
      } 
      System.out.println("--------------------"); 
      System.out.println("Sum:= " + sum); 
      System.out.println(); 
    }  } 
</pre><p>If you look at the preceding code segment carefully, the <code class="literal">vocabKey</code> indicates the corresponding topic term, <code class="literal">vocabIndex</code> is the index, and <code class="literal">prob</code> indicates the term weight for each term in a topic. The print statements have been used to format the outputs. Now let's see the output that describes four topics for simplicity in <span class="emphasis"><em>Figure 11</em></span>:</p><div class="mediaobject"><img src="graphics/B05243_09_11.jpg" /><div class="caption"><p>Figure 11: Describing four topics.</p></div></div><p>As we mentioned in <span class="emphasis"><em>step 6</em></span>, here we will show how we develop the <code class="literal">isStopWord()</code> method. Just use the following code:</p><pre class="programlisting">public static boolean isStopWord(String word) { 
    for (String s : stopwords) { 
      if (word.equals(s))   
        return true; 
    } 
    return false; 
  } 
</pre><p>And the <code class="literal">isOnlyLetters()</code> method goes as follows:</p><pre class="programlisting">public static boolean isOnlyLetter(String word) { 
    for (Character ch : word.toCharArray()) { 
      if (!Character.isLetter(ch)) 
        return false; 
    } 
    return true; 
  } 
</pre><p>In the next section, we will introduce how to parse and handle large-scale graph data using GraphX API of Spark to find the connected components from the topics data that we got from <span class="emphasis"><em>Figure 10</em></span>.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec70"></a>ML pipeline on graph data and semi-supervised graph-based learning</h2></div></div><hr /></div><p>Due to the big data deluge, there has been a large amount of unlabeled data, and very small amounts of labeled data. As already discussed, labeling and annotating this data is computationally expensive and an obstacle in finding real insight from the data. Also, the increasing growth of the social network and media producers graph data at scale. These data thrive to develop real-time and large-scale supervised learning methods that can use information in the input distribution.</p><p>The idea behind the graph-based semi-supervised learning is to construct a graph connecting similar data points or components. That lets the hidden and unobserved labels be random variables on the nodes of this graph. In this type of learning, similar data points can have similar labels and the information <span class="emphasis"><em>propagates</em></span> from labeled data points to other data points. A similar restriction also limits the ability to express many important steps in typical processing and analytical pipelines. However, the graph-based learning is not optimized for an iterative diffusion technique such as PageRank since lots of computational aspects are underlying and involved.</p><p>Moreover, due to API restriction and unavailability, we will not discuss this graph-based machine learning in detail with suitable examples.</p><p>However, in this section, we will provide a graph-based semi-supervised application development, which is basically a continuation of the topic modeling that we presented in the previous section.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec121"></a>Introduction to GraphX</h3></div></div></div><p>GraphX is a comparatively new component in Spark for graphs processing, graph analytics, graph visualization, and graph-parallel computation. Actually, the original computational aspect of the Spark RDD was extended by introducing a new graph abstraction layer as a resilient distributed graph computation that provides resilient properties in the graph processing and storage.</p><p>To provide the graph related computation, a set of basic operators such as subgraph, <code class="literal">jointVertices</code>, and <code class="literal">aggregateMessages</code> are exposed by the GraphX. In addition to this, it also inherited the optimized variant of the Pregel API in the GraphX implementation.</p><p>Moreover, to simplify graph analytics tasks, GraphX is being enriched and growing with a collection of graph algorithms and builders.</p><p>In the next section, we will introduce how to parse and handle large-scale graph data using the GraphX API of Spark to find the connected components from the topics data that we got from <span class="emphasis"><em>Figure 11</em></span>.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl3sec90"></a>Getting and parsing graph data using the GraphX API</h4></div></div></div><p>In this subsection, we will show you how to parse graph data using the GraphX API and then the connected components from the graph will be described in the next subsection. Due to API limitation in GraphX, we were unable to provide the same implementation in Java, but we did so for Scala implementation.</p><p>To run the following source code, go to your Spark distribution and start the Spark shell by providing the following commands:</p><pre class="programlisting">
<span class="strong"><strong>$ cd home/spark-2.0.0-bin-hadoop2.7/bin</strong></span>
<span class="strong"><strong>$./spark-shell</strong></span>
</pre><p>Then the Spark shell will be available with the Spark session. We assume your Spark distribution is saved in the <code class="literal">home/spark-2.0.0-bin-hadoop2.7</code> path. Please change the path accordingly in order to run the Spark shell. Also, please save the topic terms shown in <span class="emphasis"><em>Figure 11</em></span> to separate text files so that you will be able to use those terms for analyzing as graph data before you proceed to follow the next steps:</p><p><span class="strong"><strong>Step 1: Loading required packages and APIs</strong></span></p><p>Here is the code to load the required packages:</p><pre class="programlisting">package com.examples.graphs 
import org.apache.spark._ 
import org.apache.spark.graphx._ 
import org.apache.spark.rdd.RDD  
</pre><p><span class="strong"><strong>Step 2: Prepare the Spark environment</strong></span></p><p>Here is the code to prepare the Spark environment:</p><pre class="programlisting">val conf = new SparkConf().setAppName("GraphXDemo").setMaster("local[*]") 
val sc = new SparkContext(conf) 
</pre><p><span class="strong"><strong>Step 3: Parse the topic's terms and tokenize them</strong></span></p><p>The following code illustrates to parse he topic's terms:</p><pre class="programlisting">val corpus: RDD[String] = sc.wholeTextFiles("home/ /topics/*.txt").map(_._2) 
val tokenized: RDD[Seq[String]] = corpus.map(_.toLowerCase.split("\\s")) 
tokenized.foreach { x =&gt; println(x) } 
</pre><p><span class="strong"><strong>Step 4: Create RDDs of documents</strong></span></p><p>Create the RDDs of documents as a RDD[(<code class="literal">DocumentID</code>, (<code class="literal">nodeName</code>, <code class="literal">wordCount</code>))] notation. For example, RDD[(1L, (Topic_0, 4))]:</p><pre class="programlisting">val nodes: RDD[(VertexId, (String, Long))] = tokenized.zipWithIndex().map{  
      case (tokens, id) =&gt; 
        val nodeName="Topic_"+id; 
        (id, (nodeName, tokens.size)) 
         
    } 
    nodes.collect().foreach{ 
      x =&gt;  
        println(x._1+": ("+x._2._1+","+x._2._2+")")        
    } 
</pre><p>The preceding print method generates the output as shown in Figure 12:</p><div class="mediaobject"><img src="graphics/B05243_09_12.jpg" /><div class="caption"><p>Figure 12: The nodes.</p></div></div><p><span class="strong"><strong>Step 5: Make a word document pair</strong></span></p><p>Here is the code to make a word document pair:</p><pre class="programlisting">val wordPairs: RDD[(String, Long)] = 
      tokenized.zipWithIndex.flatMap { 
        case (tokens, id) =&gt; 
          val list = new Array[(String, Long)](tokens.size) 
 
          for (i &lt;- 0 to tokens.length - 1) { 
            //tokens.foreach { term =&gt; 
            list(i) = (tokens(i), id) 
          } 
          list.toSeq 
      } 
    wordPairs.collect().foreach(x =&gt; println(x)) 
    println(wordPairs.count()) 
</pre><p><span class="strong"><strong>Step 6: Create the graph relationships between nodes</strong></span></p><p>The following code shows to create a graph relationships between nodes:</p><pre class="programlisting">val relationships: RDD[Edge[String]] = wordPairs.groupByKey().flatMap{ 
      case(edge, nodes)=&gt; 
        val nodesList = nodes.toArray 
        val list = new Array[Edge[String]](nodesList.length * nodesList.length) 
        if (nodesList.length&gt;1){                   
          var count:Int=0; 
          for (i &lt;- 0 to nodesList.length-2) { 
            for(j&lt;-i+1 to nodesList.length-1){ 
         list(count) = new Edge(nodesList(i), nodesList(j), edge)  
         //list(count+1) = new Edge(nodesList(j), nodesList(i), edge) 
              count += 1; 
              //count += 2; 
            } 
          } 
        } 
        list.toSeq 
    }.filter { x =&gt; x!=null } 
    relationships.collect().foreach { x =&gt; println(x) } 
</pre><p>Note; if you want to make the graph connected, but not undirected, then just enable the following line:</p><pre class="programlisting">list(count+1) = new Edge(nodesList(j), nodesList(i), edge) 
</pre><p>Immediately after the following line:</p><pre class="programlisting">list(count) = new Edge(nodesList(i), nodesList(j), edge)  
</pre><p>Increment the count by 2, that is, <code class="literal">count += 2</code> to make the changes consistent.</p><p><span class="strong"><strong>Step 7: Initialize the graph</strong></span></p><p>The code illustrated here shows how to illustrate the graph:</p><pre class="programlisting">val graph = Graph(nodes, relationships) 
println(graph.edges.count) 
</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl3sec91"></a>Finding the connected components</h4></div></div></div><p>According to the API documentation at <a class="ulink" href="http://spark.apache.org/docs/latest/graphx-programming-guide.html" target="_blank">http://spark.apache.org/docs/latest/graphx-programming-guide.html</a>, each connected components of the graph are labeled by the connected components algorithm using the lowest-numbered vertex IDs. For example, in a social network analysis, the clusters are approximated by the connected components. To make this even easier and faster, the GraphX API contains an implementation of the algorithm as the <code class="literal">ConnectedComponents</code> object.</p><p>However, there's no Java, Python, or R-based implementation for finding the connected components. Therefore, it allows us to compute the connected components in the topics we have calculated using the LDA algorithm through one or more term, as follows:</p><pre class="programlisting">val facts: RDD[String] = 
      graph.triplets.map(triplet =&gt; 
        triplet.srcAttr._1 + " contains the terms "" + triplet.attr + "" like as " + triplet.dstAttr._1) 
    facts.collect.foreach(println(_)) 
</pre><p>This should produce the output as shown in <span class="emphasis"><em>Figure 13</em></span>:</p><div class="mediaobject"><img src="graphics/image_09_014.jpg" /><div class="caption"><p>Figure 13: Connection between the topics using GraphX.</p></div></div><p>If you look the output in <span class="emphasis"><em>Figure 13</em></span> carefully, we have printed the relationships with a triplet. It is to be noted that in addition to the support of the vertices and edges, Spark GraphX also has the notion of a triplet. More technically, a triplet is an object that broadens the Edge object. From the graph perspective, it stores the information about an edge and the related vertices adjacent to it in a graph.</p></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec71"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we have shown how to develop large-scale machine learning applications from real-time Twitter stream data and graph data. We have discussed the social network and time-series data analysis. In addition, we also developed an emerging recommendation application by using the content-based collaborative filtering algorithms of Spark MLlib to make movie recommendations for users. These applications, however, can be extended and deployed for other use cases.</p><p>It is worth noting that the current implementation of Spark contains a few implemented algorithms for the streaming or network data analysis. However, we can hope that, for example, GraphX will be improved in the future and extended for not only Scala, but for Java, R, and Python too. In the next chapter, we will focus on how to interact with external data sources to make the Spark working environment more diverse.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch10"></a>Chapter 10.  Configuring and Working with External Libraries </h2></div></div></div><p>This chapter guides you on using external libraries to expand your data analysis to make the Spark more versatile. Examples will be given for deploying third-party-developed packages or libraries for machine learning applications with Spark core and ML/MLlib. We will also discuss how to compile and use external libraries with the core libraries of Spark for time series. As promised, we will also discuss how to configure SparkR to increase exploratory data manipulation and operations. In a nutshell, the following topics will be covered throughout this chapter:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Third-party ML libraries with Spark</p></li><li style="list-style-type: disc"><p>Using external libraries when deploying Spark ML on a cluster</p></li><li style="list-style-type: disc"><p>Time series analysis using the Spark-TS package of Cloudera</p></li><li style="list-style-type: disc"><p>Configuring SparkR with RStudio</p></li><li style="list-style-type: disc"><p>Configuring Hadoop run-time on Windows</p></li></ul></div><p>In order to provide a user-friendly environment for the developer, it is also possible to incorporate third-party APIs and libraries with Spark Core and other APIs such as Spark MLlib/ML, Spark Streaming, GraphX, and so on. Interested readers should refer to the following website that is listed on the Spark website as the <span class="strong"><strong>Third-Party Packages</strong></span>: <a class="ulink" href="https://spark-packages.org/" target="_blank">https://spark-packages.org/</a>.</p><p>This website is a community index of third-party packages for Apache Spark. To date, there are total 252 packages registered on this site, as shown in <span class="emphasis"><em>Table 1</em></span>:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /></colgroup><tbody><tr><td style="border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Domain</strong></span></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>No. of packages</strong></span></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>URL</strong></span></p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Spark core</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>9</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p><a class="ulink" href="https://spark-packages.org/?q=tags%3A%22Core%22" target="_blank">https://spark-packages.org/?q=tags%3A%22Core%22</a></p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Data sources</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>39</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p><a class="ulink" href="https://spark-packages.org/?q=tags%3A%22Data%20Sources%22" target="_blank">https://spark-packages.org/?q=tags%3A%22Data%20Sources%22</a></p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Machine learning</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>55</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p><a class="ulink" href="https://spark-packages.org/?q=tags%3A%22Machine%20Learning%22" target="_blank">https://spark-packages.org/?q=tags%3A%22Machine%20Learning%22</a></p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Streaming</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>36</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p><a class="ulink" href="https://spark-packages.org/?q=tags%3A%22Streaming%22" target="_blank">https://spark-packages.org/?q=tags%3A%22Streaming%22</a></p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Graph processing</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>13</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p><a class="ulink" href="https://spark-packages.org/?q=tags%3A%22Graph%22" target="_blank">https://spark-packages.org/?q=tags%3A%22Graph%22</a></p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Spark with Python</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>5</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p><a class="ulink" href="https://spark-packages.org/?q=tags%3A%22PySpark%22" target="_blank">https://spark-packages.org/?q=tags%3A%22PySpark%22</a></p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Cluster deployment</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>10</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p><a class="ulink" href="https://spark-packages.org/?q=tags%3A%22Deployment%22" target="_blank">https://spark-packages.org/?q=tags%3A%22Deployment%22</a></p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Data processing example</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>18</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p><a class="ulink" href="https://spark-packages.org/?q=tags%3A%22Examples%22" target="_blank">https://spark-packages.org/?q=tags%3A%22Examples%22</a></p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Applications</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>10</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p><a class="ulink" href="https://spark-packages.org/?q=tags%3A%22Applications%22" target="_blank">https://spark-packages.org/?q=tags%3A%22Applications%22</a></p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Tools</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>24</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p><a class="ulink" href="https://spark-packages.org/?q=tags%3A%22Tools%22" target="_blank">https://spark-packages.org/?q=tags%3A%22Tools%22</a></p>
</td></tr><tr><td style="border-right: 0.5pt solid ; ">
<p>Total Packages: 252</p>
</td><td class="auto-generated" style=""> </td><td class="auto-generated" style=""> </td></tr></tbody></table></div><p>Table 1: Third-party libraries for Spark based on application domain</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec72"></a>Third-party ML libraries with Spark</h2></div></div><hr /></div><p>The 55 third-party machine learning libraries include libraries for neural data analysis, generalized clustering, streaming, topic modelling, feature selection, matrix factorization, distributed DataFrame for distributed ML, model matrix, Stanford Core NLP wrapper for Spark, social network analysis, deep learning module running, assembly of fundamental statistics, binary classifier calibration, and tokenizer for DataFrame.</p><p><span class="emphasis"><em>Table 2</em></span> provides a summary of the most useful packages based on use cases and application areas of machine learning. Interested readers should visit the respective websites for more insights:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Third party ML library for Spark</strong></span></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Use cases</strong></span></p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>thunder</p>
<p>ScalaNetwork</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Neural network</p>
<p>Large-scale neural data analysis with Spark where the neural network implementation is done with Scala.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>generalized-kmeans-clustering</p>
<p>patchwork</p>
<p>bisecting-kmeans</p>
<p>spark-knn</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Clustering</p>
<p>This project generalizes the Spark MLLIB K-means cluster to support arbitrary distance functions.Highly scalable grid-density clustering algorithm for Spark MLlib.</p>
<p>This is a prototype implementation of Bisecting K-Means Clustering on Spark.</p>
<p>k-nearest neighbors algorithm on Spark.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>spark-ml-streaming</p>
<p>streaming-matrix-factorization</p>
<p>twitter-stream-ml</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Streaming</p>
<p>Visualize streaming machine learning in Spark.</p>
<p>Streaming Recommendation engine using matrix factorization with user and product bias.</p>
<p>Machine learning over Twitter's stream. Using Apache Spark, Web Server and Lightning Graph server.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>pipeline</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Docker-based pipelining</p>
<p>End-to-End, real-time, advanced analytics big data reference pipeline using Spark, Spark SQL, Spark Streaming, ML, MLlib, GraphX, Kafka, Cassandra, Redis, Apache Zeppelin, Spark-Notebook, iPython/Jupyter Notebook, Tableau, H2O Flow, and Tachyon.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>dllib</p>
<p>CaffeOnSpark<code class="literal">dl4j-spark-ml</code></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Deep learning</p>
<p>dllib is a deep learning tool running on Apache Spark. Users need to download the tools as .jar and then can integrate with Spark and  develop deep-learning-based applications.</p>
<p>CaffeOnSpark is a scalable deep learning running with the Spark executors. It is based on peer-to-peer (P2P) communication. <code class="literal">dl4j-spark-ml</code> can be used to develop deep-learning-based ML applications by integrating with Spark ML.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>kNN_IS</p>
<p>sparkboostspark-calibration</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Classification</p>
<p>kNN-IS: An Iterative Spark-based design of the k-Nearest Neighbours classifier for big data.A distributed implementation of AdaBoost.MH and MP-Boost using Apache Spark.</p>
<p>Assesses binary classifier calibration (that is, how well classifier outputs match observed class proportions) in Spark.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Zen</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Regression</p>
<p>Zen provides a platform for large-scale and efficient machine learning on top of Spark. For example, logistic regression, linear regression, Latent Dirichlet Allocation (LDA), factorization machines and Deep Neural Network (DNN) are implemented in the current release.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>modelmatrix</p>
<p>spark-infotheoretic-feature-selection</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Feature engineering</p>
<p>spark-infotheoretic-feature-selection tools provide an alternative to Spark for developing large-scale machine learning applications. They provides robust feature engineering through the pipelining including the feature extractors, feature selectors. It is focused on building sparse feature-vector-based pipelines.</p>
<p>On the other hand, it can be used as a feature selection framework based on Information Theory. Algorithms based on Information Theory include mRMR, InfoGain, JMI, and other commonly used FS filters.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>spark-knn-graphs</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Graph processing</p>
<p>Spark algorithms for building and processing k-nn graphs</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>TopicModeling</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Topic modelling</p>
<p>Distributed Topic Modelling on Apache Spark</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; ">
<p>Spark.statistics</p>
</td><td style="">
<p>Statistics</p>
<p>Apart from SparkR, Spark.statistics works as an assembler of basic statistics implementation based on the Spark core</p>
</td></tr></tbody></table></div><p>Table 2: Summary of the most useful third-party packages based on use cases and application areas of machine learning with Spark</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec73"></a>Using external libraries with Spark Core</h2></div></div><hr /></div><p>In order to work with these external libraries, instead of placing the jars in any specific folder, a simple fix would be to start the <code class="literal">pyspark</code> shell or spark-shell with the following arguments:</p><pre class="programlisting">
<span class="strong"><strong>bin/pyspark --packages com.databricks:spark-csv_2.10:1.0.3</strong></span>
<span class="strong"><strong>bin/spark-shell --packages com.databricks:spark-csv_2.10:1.0.3</strong></span>
</pre><p>This will automatically load the required <code class="literal">spark-csv</code> jars. However, these two jar files have to be downloaded to the Spark distribution using the following command in Ubuntu:</p><pre class="programlisting">
<span class="strong"><strong>wget http://search.maven.org/remotecontent?filepath=org/apache/commons/commons-csv/1.1/commons-csv-1.1.jar</strong></span>
<span class="strong"><strong>wget http://search.maven.org/remotecontent?filepath=com/databricks/spark-csv_2.10/1.0.0/spark-csv_2.10-1.0.0.jar</strong></span>
</pre><p>Then, to create an active Spark session, use the following line of codes:</p><pre class="programlisting">static SparkSession spark = SparkSession 
        .builder() 
        .appName("JavaLDAExample") 
          .master("local[*]") 
          .config("spark.sql.warehouse.dir", "C:/Exp/")               
          .getOrCreate(); 
</pre><p>Once you have instantiated an active Spark session, use the following lines of code to read the csv input file: </p><pre class="programlisting">String input = "input/letterdata.data"; 
Dataset&lt;Row&gt; df = spark.read().format("com.databricks.spark.csv").option("header", "true").load(input);  
df.show();   
</pre><p>Note that here we define the <code class="literal">com.databricks.spark.csv</code> input format by using the <code class="literal">format()</code> method, dedicatedly developed by Databricks for faster CSV file reading and parsing, and by setting the auxiliary option for the header as true using the <code class="literal">option()</code> method. Finally, the <code class="literal">load()</code> method loads the input data from the <code class="literal">input/letterdata.data</code> location, for example.</p><p>As a continuation, in the next section, we will discuss configuring the Spark-TS library for time series data analysis.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip63"></a>Tip</h3><p>Interested readers should visit the third-party ML packages web page for Spark at <a class="ulink" href="https://spark-packages.org/?q=tags%3A%22Machine%20Learning%22" target="_blank">https://spark-packages.org/?q=tags%3A%22Machine%20Learning%22</a> for the package-specific discussion, updates, and configuration procedures.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec74"></a>Time series analysis using the Cloudera Spark-TS package</h2></div></div><hr /></div><p>As discussed in <span class="emphasis"><em><a class="link" href="#" linkend="ch09">Chapter 9</a></em></span>, <span class="emphasis"><em>Advanced Machine Learning with Streaming and Graph Data</em></span>, we will see how to configure the Spark-TS package developed by Cloudera. Mainly, we will talk about the
TimeSeriesRDD in this section.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec122"></a>Time series data</h3></div></div></div><p>Time series data consists of sequences of measurements, each occurring at a point in time. A variety of terms are used to describe time series data, and many of them apply to conflicting or overlapping concepts. In the interest of clarity, in Spark-TS, Cloudera sticks to a particular vocabulary. Three objects are important in time series data analysis: time series, instant, and observation:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>A time series is a sequence of real (that is, floating-point) values, each linked to a specific timestamp. Particularly, this sticks with time series as meaning a univariate time series. In Scala, a time series is usually represented by a Breeze presented at <a class="ulink" href="https://github.com/scalanlp/breeze" target="_blank">https://github.com/scalanlp/breeze</a> vector, and in Python, a 1-D NumPy array (refer to <a class="ulink" href="http://www.numpy.org/" target="_blank">http://www.numpy.org/</a> for more), and has a <code class="literal">DateTimeIndex</code> as shown at <a class="ulink" href="https://github.com/sryza/spark-timeseries/blob/master/src/main/scala/com/cloudera/sparkts/DateTimeIndex.scala" target="_blank">https://github.com/sryza/spark-timeseries/blob/master/src/main/scala/com/cloudera/sparkts/DateTimeIndex.scala</a>.</p></li><li style="list-style-type: disc"><p>On the other hand, an instant is the vector of values in a collection of time series corresponding to a single point in time. In the Spark-TS library, each time series is typically labeled with a key that enables it to be identified among a collection of time series.</p></li><li style="list-style-type: disc"><p>Finally, an observation is a tuple of (timestamp, key, value), that is, a single value in a time series or instant.</p></li></ul></div><p>However, not all data with timestamps are time series data. For example, logs don't fit directly into time series since they consist of discrete events, not scalar measurements taken at intervals. However, measurements of log messages per hour would constitute a time series.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec123"></a>Configuring Spark-TS</h3></div></div></div><p>The most straightforward way to access Spark-TS from Scala is to depend on it in a Maven project. Do this by including the following repo in <code class="literal">pom.xml</code>:</p><pre class="programlisting">&lt;repositories&gt; 
    &lt;repository&gt; 
      &lt;id&gt;cloudera-repos&lt;/id&gt; 
      &lt;name&gt;Cloudera Repos&lt;/name&gt; 
      &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt; 
    &lt;/repository&gt; 
&lt;/repositories&gt; 
 And including the following dependency in the pom.xml:  
&lt;dependency&gt; 
      &lt;groupId&gt;com.cloudera.sparkts&lt;/groupId&gt; 
      &lt;artifactId&gt;sparkts&lt;/artifactId&gt; 
      &lt;version&gt;0.1.0&lt;/version&gt; 
&lt;/dependency&gt; 
</pre><p>To get the raw <code class="literal">pom.xml</code> file, interested readers should go to the following URL:</p><p><a class="ulink" href="https://github.com/sryza/spark-timeseries/blob/master/pom.xml" target="_blank">https://github.com/sryza/spark-timeseries/blob/master/pom.xml</a></p><p>Alternatively, to access it in a spark-shell, download the JAR from <a class="ulink" href="https://repository.cloudera.com/cloudera/libs-release-local/com/cloudera/sparkts/sparkts/0.1.0/sparkts-0.1.0-jar-with-dependencies.jar" target="_blank">https://repository.cloudera.com/cloudera/libs-release-local/com/cloudera/sparkts/sparkts/0.1.0/sparkts-0.1.0-jar-with-dependencies.jar</a>, and then launch the shell with the following command as discussed in the <span class="emphasis"><em>Using external libraries with Spark Core</em></span> section in this chapter:</p><pre class="programlisting">
<span class="strong"><strong>spark-shell \</strong></span>
<span class="strong"><strong>      --jars sparkts-0.1.0-jar-with-dependencies.jar \</strong></span>
<span class="strong"><strong>      --driver-class-path sparkts-0.1.0-jar-with-dependencies.jar</strong></span>
</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec124"></a>TimeSeriesRDD</h3></div></div></div><p>According to the Spark-TS engineering blog written on the Cloudera website at <a class="ulink" href="http://blog.cloudera.com/blog/2015/12/spark-ts-a-new-library-for-analyzing-time-series-data-with-apache-spark/" target="_blank">http://blog.cloudera.com/blog/2015/12/spark-ts-a-new-library-for-analyzing-time-series-data-with-apache-spark/</a>, TimeSeriesRDD is central to Spark-TS, where each object in the RDD stores a full univariate series. Operations that tend to apply exclusively to time series are much more efficient. For example, if you want to generate a set of lagged time series from your original collection of time series, each lagged series can be computed just by looking at a single record in the input RDD.</p><p>Similarly, with imputing missing values based on surrounding values, or fitting time series models to each series, all of the data needed is present in a single array. Therefore, the central abstraction of the Spark-TS library is TimeSeriesRDD, which is simply a collection of time series on which you can operate in a distributed fashion. This approach allows you to avoid storing timestamps for each series and instead store a single <code class="literal">DateTimeIndex</code> to which all the series vectors conform. <code class="literal">TimeSeriesRDD[K]</code> extends <code class="literal">RDD[(K, Vector[Double])]</code>, where K is the key type (usually a string), and the second element in the tuple is a Breeze vector representing the time series.</p><p>A more technical discussion can be found in the GitHub URL: <a class="ulink" href="https://github.com/sryza/spark-timeseries" target="_blank">https://github.com/sryza/spark-timeseries</a>. Since this is a Third Party Package, a detailed discussion is out of the scope of this book, we believe.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec75"></a>Configuring SparkR with RStudio</h2></div></div><hr /></div><p>Let's assume you have RStudio installed on your machine. Follow the steps mentioned here:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Now open RStudio and create a new R script; then write the following code:</p><pre class="programlisting">      SPARK_HOME = "/home/spark-2.0.0-bin-hadoop2.7/R/lib" 
      Sys.setenv(SPARK_MEM="8g") 
      Sys.setenv(SPARK_HOME = "/home/spark-2.0.0-bin-hadoop2.7") 
      .libPaths(c(file.path(Sys.getenv("SPARK_HOME"), "R",
      "lib"),.libPaths())) 
</pre></li><li><p>Load the necessary package for SparkR by using this code:</p><pre class="programlisting">      library(SparkR, lib.loc = SPARK_HOME)
      ibrary(SparkR) 
</pre></li><li><p>Configure the SparkR environment as follows:</p><pre class="programlisting">      sc &lt;- sparkR.init(appName = "SparkR-DataFrame-example", master =
      "local")
      sqlContext &lt;- sparkRSQL.init(sc) 
</pre></li><li><p>Now let's create the first DataFrame and print the first few rows, as follows:</p><pre class="programlisting">      df &lt;- createDataFrame(sqlContext, faithful) 
      head(df) 
</pre></li><li><p>You might need to install the following packages in order to make the <code class="literal">devtools</code> package work:</p><pre class="programlisting">      install.packages("xml2", dependencies = TRUE) 
      install.packages("Rcpp", dependencies = TRUE) 
      install.packages("plyr", dependencies = TRUE) 
      install.packages("devtools", dependencies = TRUE) 
      install.packages("MatrixModels", dependencies = TRUE) 
      install.packages("quantreg", dependencies = TRUE)  
      install.packages("moments", dependencies = TRUE) 
      install.packages("xml2") 
      install.packages(c("digest", "gtable", "scales", "rversions",
      "lintr")) 
</pre></li><li><p>Morever, you might need to install <code class="literal">libcurl</code> for RCurl, which devtools depends on. To do this, just run this command:</p><pre class="programlisting">
<span class="strong"><strong>      sudo apt-get -y build-dep libcurl4-gnutls-dev 
      sudo apt-get install libcurl4-gnutls-dev 
      sudo apt-get install r-cran-plyr 
      sudo apt-get install r-cran-reshape2</strong></span>
</pre></li><li><p>Now configure the <code class="literal">ggplot2.SparkR</code> package from GitHub using the following code:</p><pre class="programlisting">      library(devtools) 
      devtools::install_github("SKKU-SKT/ggplot2.SparkR") 
</pre></li><li><p>Now let's compute the skewness and kurtosis for the sample DataFrame that we have just created. Before that, load the necessary packages:</p><pre class="programlisting">      library(moments) 
      library(ggplot2) 
</pre></li><li><p>Let's create the DataFrame for the daily exercise example shown in the <span class="emphasis"><em>Feature engineering and data exploration</em></span> section in <span class="emphasis"><em><a class="link" href="#" linkend="ch04">Chapter 4</a></em></span>, <span class="emphasis"><em>Extracting Knowledge through Feature Engineering</em></span>, and show the first few rows using <code class="literal">head</code> command:</p><pre class="programlisting">      time_taken &lt;- c (15, 16, 18, 17.16, 16.5, 18.6, 19.0, 20.4, 20.6, 
      25.15, 27.27, 25.24, 21.05, 21.65, 20.92, 22.61, 23.71, 35, 39, 50) 
      df_new &lt;- data.frame(time_taken)  
      head(df_new)  
      df&lt;- createDataFrame(sqlContext, data = df_new)  
      head(df) 
</pre></li><li><p>Now calculate the skewness and kurtosis, as follows:</p><pre class="programlisting">      skewness(df) 
      kurtosis(df_new) 
</pre><p>You are probably aware that we used the two terms <code class="literal">skewness</code> and <code class="literal">kurtosis</code> in <a class="link" href="#" linkend="ch04">Chapter 4</a>, <span class="emphasis"><em>Extracting Knowledge through Feature Engineering</em></span>. If you are not familiar with these two terms, here is a bit of definition of them. Well, from the statistical perspective, <code class="literal">skewness</code> is a measure of symmetry. Alternatively and more precisely, it signifies the lack of symmetry in a distribution of the dataset.</p><p>Now you might be wondering what symmetric is. Well, a distribution of the dataset is symmetric if it looks the same to the left and right of the center point.</p><p>Kurtosis, on the other hand, is a measure of whether the data are heavy-tailed or light-tailed relative to a normal distribution:</p></li><li><p>Finally, let's plot the density plot graph by calling the <code class="literal">ggplot()</code> method of the <code class="literal">ggplot2.SparkR</code> package:</p><pre class="programlisting">      ggplot(df, aes(x = time_taken)) + stat_density(geom="line",
      col= "green", size = 1, bw = 4) + theme_bw() 
</pre></li></ol></div><p>If you are not familiar with the <code class="literal">ggplot2</code> R package, note that <code class="literal">ggplot2</code> is a plotting system for R based on the grammar of graphics of base and lattice graphics. It provides many fiddly details of the graphics that make plotting a hassle, for example, placing or drawing legends in a graph, as well as providing a powerful model of graphics. This will make your life easier in order to produce simple as well as complex multi-layered graphics.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip64"></a>Tip</h3><p>More info about <code class="literal">ggplot2</code> and its documentation can be found at the following website: <a class="ulink" href="http://docs.ggplot2.org/current/" target="_blank">http://docs.ggplot2.org/current/</a>.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec76"></a>Configuring Hadoop run-time on Windows</h2></div></div><hr /></div><p>If you are developing your machine learning application on windows using Eclipse (as Maven project of course), probably you will face a problem since Spark expects that there is a runtime environment for Hadoop on Windows too.</p><p>More specifically, suppose you are running a Spark project written in Java with main class as <code class="literal">JavaNaiveBayes_ML.java</code>, then you will experience an IO exception saying that:</p><pre class="programlisting">16/10/04 11:59:52 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.</pre><div class="mediaobject"><img src="graphics/image_10_001.jpg" /><div class="caption"><p>Figure 1: IO exception due to the missing Hadoop runtime</p></div></div><p>The reason is that by default Hadoop is developed for the Linux environment and if you are developing your Spark applications on windows platform, a bridge is required that will provide the Hadoop environment for the Hadoop runtime for Spark to be properly executed.</p><p>Now, how to get rid of this problem then? The solution is straight forward. As the error message says, we need to have an executable namely <code class="literal">winutils.exe</code>. Now download the <code class="literal">winutils.exe</code> file from the code directory of Packt for this chapter and copy and paste it in the Spark distribution directory and configure Eclipse.</p><p>More specifically, suppose your Spark distribution containing Hadoop is located at <code class="literal">C:/Users/spark-2.0.0-bin-hadoop2.7</code>. Inside the Spark distribution there is a directory named <code class="literal">bin.</code> Now, paste the executable there (that is, <code class="literal">path = C:/Users/spark-2.0.0-binhadoop2.7/</code><code class="literal">bin/</code>).</p><p>The second phase of the solution is going to Eclipse, select the main class (that is, <code class="literal">JavaNaiveBayes_ML.java</code> in this case), and then go to the <span class="strong"><strong>Run</strong></span> menu. From the <span class="strong"><strong>Run</strong></span> menu go to the <span class="strong"><strong>Run Configurations</strong></span> option and from this option select the <span class="strong"><strong>Environment</strong></span> tab. If you select the tab, you a will have the option to create a new environmental variable for Eclipse suing the JVM.</p><p>Now create a new environmental variable and put the value as <code class="literal">C:/Users/spark-2.0.0-bin-hadoop2.7/</code>. Now press on <span class="strong"><strong>apply</strong></span> and re-run your application and your problem should be resolved.</p><p>More technically, the details of the IO exception can be described as follows in Figure 1:</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec77"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we showed how to use external libraries with Spark to expand data analyses.</p><p>More and more Spark as well as third-party packages are being developed by open source contributors. Readers should be updated with the latest news and release on the Spark website. They also should be notified of about the latest machine learning APIs, since the development of Spark is continuous and innovative and, of course, sometimes after a certain package becomes obsolete or deprecated.</p><p>Throughout this book, we have tried to guide you on how to use the most popular and widely used machine learning algorithms that have been developed by Spark. However, there are other algorithms too that we could not discuss, and more and more algorithms will be added to the Spark ML and MLlib packages.</p><p>This is more or less the end of our little journey with Spark. Now a general suggestion from our side to you as readers, or if you are relatively new to machine learning, Java, or Spark at first try to understand whether a problem is really a machine learning problem. If it is a machine learning problem, try to guess what type of learning algorithms should be the best fit, that is, classification, clustering, regression, recommendation, or frequent pattern mining.</p><p>Then define and formulate the problem. After that you should generate or download the appropriate data based on the feature engineering concept of Spark that we have discussed. Then you can select an ML model that will provide better results in terms of accuracy. However, as discussed earlier, the model selection really depends on your data and problem type.</p><p>Now that you have your data ready to train the model, go straight to train the model towards making predictive analytics.</p><p>When your model is trained, evaluate it to see how it goes and fulfills your prediction expectations. Well, if you are not happy with the performance, try changing to other ML algorithms towards model selection. As discussed in <a class="link" href="#" linkend="ch07">Chapter 7</a>, <span class="emphasis"><em>Tuning Machine Learning Models</em></span>, even proper model selection cannot provide the best result sometimes because of the nature of the data you have.</p><p>So what is to be done? It's simple. Tune your ML model using the available tuning algorithms to properly set the hyperparameters. You might also need to make your model adaptable for new data types, especially if you are developing an ML application for a dynamic environment such as time series analysis or streaming analytics.</p><p>Finally, deploy your model and you have it as a robust ML application.</p><p>Our final recommendation to the readers is to browse the Spark website (at <a class="ulink" href="http://spark.apache.org/" target="_blank">http://spark.apache.org/</a>) regularly to get updates and also try to incorporate the regular Spark provided APIs with other third-party applications to get the best result of the collaboration.</p></div></div></div></div>
</div></div></div></body></html>
