<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="">

    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0"> <!--320-->
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no, user-scalable=no">

    <link rel="icon" href="../../mapt/images/favicon.ico">

    <link rel="stylesheet" href="../../mapt/css/font-awesome.css">
    <link rel="stylesheet" href="../../mapt/css/google-fonts.css">
    <link rel="stylesheet" href="../../mapt/css/devicon.css">

    <link rel="stylesheet" href="../../mapt/css/bootstrap.css">
    <link rel="stylesheet" href="../../mapt/css/bootstrap-xl.css">
    <link rel="stylesheet" href="../../mapt/css/magnific-popup.css">
    <link rel="stylesheet" href="../../mapt/css/prism.css">
    <link rel="stylesheet" href="../../mapt/css/hljs-github.css">

    <link rel="stylesheet" href="../../mapt/css/mapt.css">
    <link rel="stylesheet" href="../../mapt/css/custom.css">

    <script src="../../mapt/js/jquery.js"></script>
    <script src="../../mapt/js/bootstrap.js"></script>
    <script src="../../mapt/js/jquery.magnific-popup.js"></script>
    <script src="../../mapt/js/highlight.min.js"></script>

    <script src="../../mapt/js/custom.js"></script>
    
    <title>Learning Concurrent Programming in Scala - Second Edition</title>
</head>

<body class="home-body">
    <div id="wrapper">
        <div id="sidebar-wrapper">    
            <ul class="sidebar-nav">
                <div class="list-group" id="sidebar-nav" role="tablist">
                    <li>
                        <a href="../../index.html" class="sidenav-menu-holder back-btn" id="back-link">
                            <span class="sidenav-menu">Book List</span>
                            <span class="pull-left mr5"><i class="fa fa-chevron-left"></i></span>
                        </a>
                    </li>
                    
                    <li class="book-info copyright">
                        <span class="info text-nowrap"><span class="copyleft">&copy;</span><span><strong>RuTracker</strong>.org</span></span>
                    </li>          
                    <li class="book-info copyright">
                        <span class="info text-nowrap">Pub date: <strong>22 Feb 2017</strong></span>
                    </li>         
                    <li class="book-info">
                        <span class="info text-nowrap">Price: â‚¬<strong>34.99</strong></span>
                        <span class="info text-nowrap">ISBN: <strong>9781786466891</strong></span>
                    </li>     
            
                    <li>
                        <a href="graphics/cover.jpg" class="sidenav-menu-holder cover-img">
                            <img src="graphics/cover.jpg" class="cover-image">
                        </a>
                    </li>        
            
                    <div class="book_navigation">
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse1">
                                <div class="section-name">1: Introduction</div>
                            </a>
                        </li>
                        <div id="collapse1" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="1" class="sub-nav">
                                <a href="#ch01">
                                    <div class="section-name">Chapter 1: Introduction</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec7" class="sub-nav">
                                <a href="#ch01lvl1sec7">                    
                                    <div class="section-name">Concurrent programming</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec8" class="sub-nav">
                                <a href="#ch01lvl1sec8">                    
                                    <div class="section-name">The advantages of Scala</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec9" class="sub-nav">
                                <a href="#ch01lvl1sec9">                    
                                    <div class="section-name">Preliminaries</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec10" class="sub-nav">
                                <a href="#ch01lvl1sec10">                    
                                    <div class="section-name">Overview of new features in Scala 2.12</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec11" class="sub-nav">
                                <a href="#ch01lvl1sec11">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec12" class="sub-nav">
                                <a href="#ch01lvl1sec12">                    
                                    <div class="section-name">Exercises</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse2">
                                <div class="section-name">2: Concurrency on the JVM and the Java Memory Model</div>
                            </a>
                        </li>
                        <div id="collapse2" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="2" class="sub-nav">
                                <a href="#ch02">
                                    <div class="section-name">Chapter 2: Concurrency on the JVM and the Java Memory Model</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec13" class="sub-nav">
                                <a href="#ch02lvl1sec13">                    
                                    <div class="section-name">Processes and threads</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec14" class="sub-nav">
                                <a href="#ch02lvl1sec14">                    
                                    <div class="section-name">Monitors and synchronization</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec15" class="sub-nav">
                                <a href="#ch02lvl1sec15">                    
                                    <div class="section-name">Volatile variables</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec16" class="sub-nav">
                                <a href="#ch02lvl1sec16">                    
                                    <div class="section-name">The Java Memory Model</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec17" class="sub-nav">
                                <a href="#ch02lvl1sec17">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec18" class="sub-nav">
                                <a href="#ch02lvl1sec18">                    
                                    <div class="section-name">Exercises</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse3">
                                <div class="section-name">3: Traditional Building Blocks of Concurrency</div>
                            </a>
                        </li>
                        <div id="collapse3" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="3" class="sub-nav">
                                <a href="#ch03">
                                    <div class="section-name">Chapter 3: Traditional Building Blocks of Concurrency</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec19" class="sub-nav">
                                <a href="#ch03lvl1sec19">                    
                                    <div class="section-name">The Executor and ExecutionContext objects</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec20" class="sub-nav">
                                <a href="#ch03lvl1sec20">                    
                                    <div class="section-name">Atomic primitives</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec21" class="sub-nav">
                                <a href="#ch03lvl1sec21">                    
                                    <div class="section-name">Lazy values</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec22" class="sub-nav">
                                <a href="#ch03lvl1sec22">                    
                                    <div class="section-name">Concurrent collections</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec23" class="sub-nav">
                                <a href="#ch03lvl1sec23">                    
                                    <div class="section-name">Custom concurrent data structures</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec24" class="sub-nav">
                                <a href="#ch03lvl1sec24">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec25" class="sub-nav">
                                <a href="#ch03lvl1sec25">                    
                                    <div class="section-name">Exercises</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse4">
                                <div class="section-name">4: Asynchronous Programming with Futures and Promises</div>
                            </a>
                        </li>
                        <div id="collapse4" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="4" class="sub-nav">
                                <a href="#ch04">
                                    <div class="section-name">Chapter 4: Asynchronous Programming with Futures and Promises</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec26" class="sub-nav">
                                <a href="#ch04lvl1sec26">                    
                                    <div class="section-name">Futures</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec27" class="sub-nav">
                                <a href="#ch04lvl1sec27">                    
                                    <div class="section-name">Promises</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec28" class="sub-nav">
                                <a href="#ch04lvl1sec28">                    
                                    <div class="section-name">Futures and blocking</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec29" class="sub-nav">
                                <a href="#ch04lvl1sec29">                    
                                    <div class="section-name">The Scala Async library</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec30" class="sub-nav">
                                <a href="#ch04lvl1sec30">                    
                                    <div class="section-name">Alternative future frameworks</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec31" class="sub-nav">
                                <a href="#ch04lvl1sec31">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec32" class="sub-nav">
                                <a href="#ch04lvl1sec32">                    
                                    <div class="section-name">Exercises</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse5">
                                <div class="section-name">5: Data-Parallel Collections</div>
                            </a>
                        </li>
                        <div id="collapse5" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="5" class="sub-nav">
                                <a href="#ch05">
                                    <div class="section-name">Chapter 5: Data-Parallel Collections</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec33" class="sub-nav">
                                <a href="#ch05lvl1sec33">                    
                                    <div class="section-name">Scala collections in a nutshell</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec34" class="sub-nav">
                                <a href="#ch05lvl1sec34">                    
                                    <div class="section-name">Using parallel collections</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec35" class="sub-nav">
                                <a href="#ch05lvl1sec35">                    
                                    <div class="section-name">Caveats with parallel collections</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec36" class="sub-nav">
                                <a href="#ch05lvl1sec36">                    
                                    <div class="section-name">Using parallel and concurrent collections together</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec37" class="sub-nav">
                                <a href="#ch05lvl1sec37">                    
                                    <div class="section-name">Implementing custom parallel collections</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec38" class="sub-nav">
                                <a href="#ch05lvl1sec38">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec39" class="sub-nav">
                                <a href="#ch05lvl1sec39">                    
                                    <div class="section-name">Exercises</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse6">
                                <div class="section-name">6: Concurrent Programming with Reactive Extensions</div>
                            </a>
                        </li>
                        <div id="collapse6" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="6" class="sub-nav">
                                <a href="#ch06">
                                    <div class="section-name">Chapter 6: Concurrent Programming with Reactive Extensions</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec40" class="sub-nav">
                                <a href="#ch06lvl1sec40">                    
                                    <div class="section-name">Creating Observable objects</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec41" class="sub-nav">
                                <a href="#ch06lvl1sec41">                    
                                    <div class="section-name">Composing Observable objects</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec42" class="sub-nav">
                                <a href="#ch06lvl1sec42">                    
                                    <div class="section-name">Rx schedulers</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec43" class="sub-nav">
                                <a href="#ch06lvl1sec43">                    
                                    <div class="section-name">Subjects and top-down reactive programming</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec44" class="sub-nav">
                                <a href="#ch06lvl1sec44">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec45" class="sub-nav">
                                <a href="#ch06lvl1sec45">                    
                                    <div class="section-name">Exercises</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse7">
                                <div class="section-name">7: Software Transactional Memory</div>
                            </a>
                        </li>
                        <div id="collapse7" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="7" class="sub-nav">
                                <a href="#ch07">
                                    <div class="section-name">Chapter 7: Software Transactional Memory</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec46" class="sub-nav">
                                <a href="#ch07lvl1sec46">                    
                                    <div class="section-name">The trouble with atomic variables</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec47" class="sub-nav">
                                <a href="#ch07lvl1sec47">                    
                                    <div class="section-name">Using Software Transactional Memory</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec48" class="sub-nav">
                                <a href="#ch07lvl1sec48">                    
                                    <div class="section-name">Composing transactions</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec49" class="sub-nav">
                                <a href="#ch07lvl1sec49">                    
                                    <div class="section-name">Retrying transactions</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec50" class="sub-nav">
                                <a href="#ch07lvl1sec50">                    
                                    <div class="section-name">Transactional collections</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec51" class="sub-nav">
                                <a href="#ch07lvl1sec51">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec52" class="sub-nav">
                                <a href="#ch07lvl1sec52">                    
                                    <div class="section-name">Exercises</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse8">
                                <div class="section-name">8: Actors</div>
                            </a>
                        </li>
                        <div id="collapse8" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="8" class="sub-nav">
                                <a href="#ch08">
                                    <div class="section-name">Chapter 8: Actors</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec53" class="sub-nav">
                                <a href="#ch08lvl1sec53">                    
                                    <div class="section-name">Working with actors</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec54" class="sub-nav">
                                <a href="#ch08lvl1sec54">                    
                                    <div class="section-name">Communication between actors</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec55" class="sub-nav">
                                <a href="#ch08lvl1sec55">                    
                                    <div class="section-name">Actor supervision</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec56" class="sub-nav">
                                <a href="#ch08lvl1sec56">                    
                                    <div class="section-name">Remote actors</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec57" class="sub-nav">
                                <a href="#ch08lvl1sec57">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec58" class="sub-nav">
                                <a href="#ch08lvl1sec58">                    
                                    <div class="section-name">Exercises</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse9">
                                <div class="section-name">9: Concurrency in Practice</div>
                            </a>
                        </li>
                        <div id="collapse9" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="9" class="sub-nav">
                                <a href="#ch09">
                                    <div class="section-name">Chapter 9: Concurrency in Practice</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec59" class="sub-nav">
                                <a href="#ch09lvl1sec59">                    
                                    <div class="section-name">Choosing the right tools for the job</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec60" class="sub-nav">
                                <a href="#ch09lvl1sec60">                    
                                    <div class="section-name">Putting it all together - a remote file browser</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec61" class="sub-nav">
                                <a href="#ch09lvl1sec61">                    
                                    <div class="section-name">Debugging concurrent programs</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec62" class="sub-nav">
                                <a href="#ch09lvl1sec62">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec63" class="sub-nav">
                                <a href="#ch09lvl1sec63">                    
                                    <div class="section-name">Exercises</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse10">
                                <div class="section-name">10: Reactors</div>
                            </a>
                        </li>
                        <div id="collapse10" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="10" class="sub-nav">
                                <a href="#ch10">
                                    <div class="section-name">Chapter 10: Reactors</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec64" class="sub-nav">
                                <a href="#ch10lvl1sec64">                    
                                    <div class="section-name">The need for reactors</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec65" class="sub-nav">
                                <a href="#ch10lvl1sec65">                    
                                    <div class="section-name">Getting started with Reactors</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec66" class="sub-nav">
                                <a href="#ch10lvl1sec66">                    
                                    <div class="section-name">The &amp;quot;Hello World&amp;quot; program</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec67" class="sub-nav">
                                <a href="#ch10lvl1sec67">                    
                                    <div class="section-name">Event streams</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec68" class="sub-nav">
                                <a href="#ch10lvl1sec68">                    
                                    <div class="section-name">Reactors</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec69" class="sub-nav">
                                <a href="#ch10lvl1sec69">                    
                                    <div class="section-name">Schedulers</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec70" class="sub-nav">
                                <a href="#ch10lvl1sec70">                    
                                    <div class="section-name">Reactor lifecycle</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec71" class="sub-nav">
                                <a href="#ch10lvl1sec71">                    
                                    <div class="section-name">Reactor system services</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec72" class="sub-nav">
                                <a href="#ch10lvl1sec72">                    
                                    <div class="section-name">Protocols</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec73" class="sub-nav">
                                <a href="#ch10lvl1sec73">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec74" class="sub-nav">
                                <a href="#ch10lvl1sec74">                    
                                    <div class="section-name">Exercises</div>
                                </a>
                            </li>
                        </div>
                    </div>
                </div>
            </ul>
        </div>
        
        <div id="page-content-wrapper" class="book-page">
            <a href="#" id="menu-toggle" class="toggle-nav"><i class="fa fa-bars fa-2x mr5"></i></a>
            
            <a href="#" id="back_to_top" class="back-to-top"><img src="../../mapt/images/kopimi.svg"></a>
            
            <div class="col-sm-12 col-xl-offset-2 col-xl-8 col-lg-offset-1 col-lg-10">
                <div class="btn-group pull-right mt15 mb30" role="group">
                    <a href="#home" class="btn btn-default">
                        <i class="fa fa-share fa-lg no-text-padding"></i>
                        <span class="hidden-xs ml5">Book Home</span>
                    </a>
                    <button class="btn btn-default" data-nid="27038" id="code-download">
                        <i class="fa fa-file fa-lg"></i>
                        <span class="hidden-xs ml5">Download Code Files</span>
                    </button>
                </div>
            </div>
            <div class="clearfix"></div>
            
            <div id="book-wrapper" class="container-fluid">
                <div class="col-sm-12 col-xl-offset-2 col-xl-8 col-lg-offset-1 col-lg-10" id="home">
                    <h2 class="product-title">Learning Concurrent Programming in Scala - Second Edition</h2>
                    <hr>
                    <div class="row">
                        <div class="col-sm-12">
                            <h5 class="mt10">By Aleksandar Prokopec</h5>
                            <div>
                                <p class="mb20"><b>Learn the art of building intricate, modern, scalable, and concurrent applications using Scala</b></p>
                                <a href="#ch01" class="btn btn-info btn-lg pull-right hidden-xs">
                                    Start Reading <i class="fa fa-chevron-right ml5"></i>
                                </a>
                                <a href="#ch01" class="btn btn-info btn-lg btn-block mt20 mb20 visible-xs">
                                    Start Reading <i class="fa fa-chevron-right ml5"></i>
                                </a>
                                <div class="clearfix"></div>
                                <div class="col-sm-12">
                                    <ul id="myTabs" class="nav nav-tabs nav-justified hidden-xs mt20" role="tablist">
                                        <li class="active">
                                            <a href="#info" role="tab" id="info-tab" data-toggle="tab">
                                                <h5>Info</h5>
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#content" role="tab" id="content-tab" data-toggle="tab">
                                                <h5>Contents</h5>
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#author" role="tab" id="author-tab" data-toggle="tab">
                                                <h5>Author</h5>
                                            </a>
                                        </li>
                                    </ul>
                
                                    <ul id="myTabsMobile" class="nav nav-pills text-center nav-stacked visible-xs mb60" role="tablist">
                                        <li class="active">
                                            <a href="#info" role="tab" id="info-tab-responsive" data-toggle="tab">
                                                Info
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#content" role="tab" id="content-tab-responsive" data-toggle="tab">
                                                Contents
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#author" role="tab" id="author-tab-responsive" data-toggle="tab">
                                                Author
                                            </a>
                                        </li>
                                    </ul>
                
                                    <div id="myTabContent" class="tab-content pt30">
                                    
                                        <div role="tabpanel" class="tab-pane active fade in" id="info">
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">Features</h5>
                                            </div>
                                            <div class="hidden-xs">
                                                <h5>Features</h5>
                                            </div>
                                            <hr>
                                            <div>
                                                <ul>
                <li>Make the most of Scala by understanding its philosophy and harnessing the power of multicores</li>
                <li>Get acquainted with cutting-edge technologies in the field of concurrency, through practical, real-world applications</li>
                <li>Get this step-by-step guide packed with pragmatic examples</li>
                </ul>
                                            </div>
                                            <br>
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">Learning</h5>
                                            </div>
                                            <div class="hidden-xs">
                                                <h5>Learning</h5>
                                            </div>
                                            <hr>
                                            <div>
                                                <ul>
                <li>Get to grips with the fundamentals of concurrent programming on modern multiprocessor systems.</li>
                <li><span id="what_you_will_learn_c" class="sugar_field">Build high-performance concurrent systems from simple, low-level concurrency primitives</span></li>
                <li><span id="what_you_will_learn_c" class="sugar_field">Express asynchrony in concurrent computations with futures and promises</span></li>
                <li><span id="what_you_will_learn_c" class="sugar_field">Seamlessly accelerate sequential programs by using data-parallel collections</span></li>
                <li><span id="what_you_will_learn_c" class="sugar_field"><span id="what_you_will_learn_c" class="sugar_field">Design safe, scalable, and easy-to-comprehend in-memory transactional data models</span></span></li>
                <li><span id="what_you_will_learn_c" class="sugar_field">Transparently create distributed applications that scale across multiple machines</span></li>
                <li><span id="what_you_will_learn_c" class="sugar_field">Integrate different concurrency frameworks together in large applications</span></li>
                <li><span id="what_you_will_learn_c" class="sugar_field">Develop and implement scalable and easy-to-understand concurrent applications in Scala 2.12</span></li>
                </ul>
                                            </div>
                                            <br>
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">About</h5>
                                            </div>
                                            <div class="hidden-xs">
                                                <h5>About</h5>
                                            </div>
                                            <hr>
                                            <div>
                                                <p><span id="description" class="sugar_field">Scala is a modern, multiparadigm programming language designed to express common programming patterns in a concise, elegant, and type-safe way. Scala smoothly integrates the features of object-oriented and functional languages.</span></p>
                <p><span id="cover_text_c" class="sugar_field">In this second edition, you will find updated coverage of the Scala 2.12 platform. The Scala 2.12 series targets Java 8 and requires it for execution. The book starts by introducing you to the foundations of concurrent programming on the JVM, outlining the basics of the Java Memory Model, and then shows some of the classic building blocks of concurrency, such as the atomic variables, thread pools, and concurrent data structures, along with the caveats of traditional concurrency.</span></p>
                <p><span id="description" class="sugar_field">The book then walks you through different high-level concurrency abstractions, each tailored toward a specific class of programming tasks, while touching on the latest advancements of async programming capabilities of Scala. It also covers some useful patterns and idioms to use with the techniques described. Finally, the book presents an overview of when to use which concurrency library and demonstrates how they all work together, and then presents new exciting approaches to building concurrent and distributed systems.</span></p>
                                            </div>
                                        </div>
                                        
                                        <div role="tabpanel" class="tab-pane fade in" id="content">
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">Contents</h5>
                                                <hr>
                                            </div>
                                            <ul>
                                                <div>
                                                    <li data-chapter="1">
                                                        <div class="section-name">1: Introduction</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="1" class="chapter-section">
                                                                    <a href="#ch01">        
                                                                        <div class="section-name">Chapter 1: Introduction</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec7" class="chapter-section">
                                                                    <a href="#ch01lvl1sec7">                    
                                                                        <div class="section-name">Concurrent programming</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec8" class="chapter-section">
                                                                    <a href="#ch01lvl1sec8">                    
                                                                        <div class="section-name">The advantages of Scala</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec9" class="chapter-section">
                                                                    <a href="#ch01lvl1sec9">                    
                                                                        <div class="section-name">Preliminaries</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec10" class="chapter-section">
                                                                    <a href="#ch01lvl1sec10">                    
                                                                        <div class="section-name">Overview of new features in Scala 2.12</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec11" class="chapter-section">
                                                                    <a href="#ch01lvl1sec11">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec12" class="chapter-section">
                                                                    <a href="#ch01lvl1sec12">                    
                                                                        <div class="section-name">Exercises</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="2">
                                                        <div class="section-name">2: Concurrency on the JVM and the Java Memory Model</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="2" class="chapter-section">
                                                                    <a href="#ch02">        
                                                                        <div class="section-name">Chapter 2: Concurrency on the JVM and the Java Memory Model</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec13" class="chapter-section">
                                                                    <a href="#ch02lvl1sec13">                    
                                                                        <div class="section-name">Processes and threads</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec14" class="chapter-section">
                                                                    <a href="#ch02lvl1sec14">                    
                                                                        <div class="section-name">Monitors and synchronization</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec15" class="chapter-section">
                                                                    <a href="#ch02lvl1sec15">                    
                                                                        <div class="section-name">Volatile variables</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec16" class="chapter-section">
                                                                    <a href="#ch02lvl1sec16">                    
                                                                        <div class="section-name">The Java Memory Model</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec17" class="chapter-section">
                                                                    <a href="#ch02lvl1sec17">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec18" class="chapter-section">
                                                                    <a href="#ch02lvl1sec18">                    
                                                                        <div class="section-name">Exercises</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="3">
                                                        <div class="section-name">3: Traditional Building Blocks of Concurrency</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="3" class="chapter-section">
                                                                    <a href="#ch03">        
                                                                        <div class="section-name">Chapter 3: Traditional Building Blocks of Concurrency</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec19" class="chapter-section">
                                                                    <a href="#ch03lvl1sec19">                    
                                                                        <div class="section-name">The Executor and ExecutionContext objects</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec20" class="chapter-section">
                                                                    <a href="#ch03lvl1sec20">                    
                                                                        <div class="section-name">Atomic primitives</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec21" class="chapter-section">
                                                                    <a href="#ch03lvl1sec21">                    
                                                                        <div class="section-name">Lazy values</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec22" class="chapter-section">
                                                                    <a href="#ch03lvl1sec22">                    
                                                                        <div class="section-name">Concurrent collections</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec23" class="chapter-section">
                                                                    <a href="#ch03lvl1sec23">                    
                                                                        <div class="section-name">Custom concurrent data structures</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec24" class="chapter-section">
                                                                    <a href="#ch03lvl1sec24">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec25" class="chapter-section">
                                                                    <a href="#ch03lvl1sec25">                    
                                                                        <div class="section-name">Exercises</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="4">
                                                        <div class="section-name">4: Asynchronous Programming with Futures and Promises</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="4" class="chapter-section">
                                                                    <a href="#ch04">        
                                                                        <div class="section-name">Chapter 4: Asynchronous Programming with Futures and Promises</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec26" class="chapter-section">
                                                                    <a href="#ch04lvl1sec26">                    
                                                                        <div class="section-name">Futures</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec27" class="chapter-section">
                                                                    <a href="#ch04lvl1sec27">                    
                                                                        <div class="section-name">Promises</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec28" class="chapter-section">
                                                                    <a href="#ch04lvl1sec28">                    
                                                                        <div class="section-name">Futures and blocking</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec29" class="chapter-section">
                                                                    <a href="#ch04lvl1sec29">                    
                                                                        <div class="section-name">The Scala Async library</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec30" class="chapter-section">
                                                                    <a href="#ch04lvl1sec30">                    
                                                                        <div class="section-name">Alternative future frameworks</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec31" class="chapter-section">
                                                                    <a href="#ch04lvl1sec31">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec32" class="chapter-section">
                                                                    <a href="#ch04lvl1sec32">                    
                                                                        <div class="section-name">Exercises</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="5">
                                                        <div class="section-name">5: Data-Parallel Collections</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="5" class="chapter-section">
                                                                    <a href="#ch05">        
                                                                        <div class="section-name">Chapter 5: Data-Parallel Collections</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec33" class="chapter-section">
                                                                    <a href="#ch05lvl1sec33">                    
                                                                        <div class="section-name">Scala collections in a nutshell</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec34" class="chapter-section">
                                                                    <a href="#ch05lvl1sec34">                    
                                                                        <div class="section-name">Using parallel collections</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec35" class="chapter-section">
                                                                    <a href="#ch05lvl1sec35">                    
                                                                        <div class="section-name">Caveats with parallel collections</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec36" class="chapter-section">
                                                                    <a href="#ch05lvl1sec36">                    
                                                                        <div class="section-name">Using parallel and concurrent collections together</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec37" class="chapter-section">
                                                                    <a href="#ch05lvl1sec37">                    
                                                                        <div class="section-name">Implementing custom parallel collections</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec38" class="chapter-section">
                                                                    <a href="#ch05lvl1sec38">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec39" class="chapter-section">
                                                                    <a href="#ch05lvl1sec39">                    
                                                                        <div class="section-name">Exercises</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="6">
                                                        <div class="section-name">6: Concurrent Programming with Reactive Extensions</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="6" class="chapter-section">
                                                                    <a href="#ch06">        
                                                                        <div class="section-name">Chapter 6: Concurrent Programming with Reactive Extensions</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec40" class="chapter-section">
                                                                    <a href="#ch06lvl1sec40">                    
                                                                        <div class="section-name">Creating Observable objects</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec41" class="chapter-section">
                                                                    <a href="#ch06lvl1sec41">                    
                                                                        <div class="section-name">Composing Observable objects</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec42" class="chapter-section">
                                                                    <a href="#ch06lvl1sec42">                    
                                                                        <div class="section-name">Rx schedulers</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec43" class="chapter-section">
                                                                    <a href="#ch06lvl1sec43">                    
                                                                        <div class="section-name">Subjects and top-down reactive programming</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec44" class="chapter-section">
                                                                    <a href="#ch06lvl1sec44">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec45" class="chapter-section">
                                                                    <a href="#ch06lvl1sec45">                    
                                                                        <div class="section-name">Exercises</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="7">
                                                        <div class="section-name">7: Software Transactional Memory</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="7" class="chapter-section">
                                                                    <a href="#ch07">        
                                                                        <div class="section-name">Chapter 7: Software Transactional Memory</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec46" class="chapter-section">
                                                                    <a href="#ch07lvl1sec46">                    
                                                                        <div class="section-name">The trouble with atomic variables</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec47" class="chapter-section">
                                                                    <a href="#ch07lvl1sec47">                    
                                                                        <div class="section-name">Using Software Transactional Memory</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec48" class="chapter-section">
                                                                    <a href="#ch07lvl1sec48">                    
                                                                        <div class="section-name">Composing transactions</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec49" class="chapter-section">
                                                                    <a href="#ch07lvl1sec49">                    
                                                                        <div class="section-name">Retrying transactions</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec50" class="chapter-section">
                                                                    <a href="#ch07lvl1sec50">                    
                                                                        <div class="section-name">Transactional collections</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec51" class="chapter-section">
                                                                    <a href="#ch07lvl1sec51">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec52" class="chapter-section">
                                                                    <a href="#ch07lvl1sec52">                    
                                                                        <div class="section-name">Exercises</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="8">
                                                        <div class="section-name">8: Actors</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="8" class="chapter-section">
                                                                    <a href="#ch08">        
                                                                        <div class="section-name">Chapter 8: Actors</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec53" class="chapter-section">
                                                                    <a href="#ch08lvl1sec53">                    
                                                                        <div class="section-name">Working with actors</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec54" class="chapter-section">
                                                                    <a href="#ch08lvl1sec54">                    
                                                                        <div class="section-name">Communication between actors</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec55" class="chapter-section">
                                                                    <a href="#ch08lvl1sec55">                    
                                                                        <div class="section-name">Actor supervision</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec56" class="chapter-section">
                                                                    <a href="#ch08lvl1sec56">                    
                                                                        <div class="section-name">Remote actors</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec57" class="chapter-section">
                                                                    <a href="#ch08lvl1sec57">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec58" class="chapter-section">
                                                                    <a href="#ch08lvl1sec58">                    
                                                                        <div class="section-name">Exercises</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="9">
                                                        <div class="section-name">9: Concurrency in Practice</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="9" class="chapter-section">
                                                                    <a href="#ch09">        
                                                                        <div class="section-name">Chapter 9: Concurrency in Practice</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec59" class="chapter-section">
                                                                    <a href="#ch09lvl1sec59">                    
                                                                        <div class="section-name">Choosing the right tools for the job</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec60" class="chapter-section">
                                                                    <a href="#ch09lvl1sec60">                    
                                                                        <div class="section-name">Putting it all together - a remote file browser</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec61" class="chapter-section">
                                                                    <a href="#ch09lvl1sec61">                    
                                                                        <div class="section-name">Debugging concurrent programs</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec62" class="chapter-section">
                                                                    <a href="#ch09lvl1sec62">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec63" class="chapter-section">
                                                                    <a href="#ch09lvl1sec63">                    
                                                                        <div class="section-name">Exercises</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="10">
                                                        <div class="section-name">10: Reactors</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="10" class="chapter-section">
                                                                    <a href="#ch10">        
                                                                        <div class="section-name">Chapter 10: Reactors</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec64" class="chapter-section">
                                                                    <a href="#ch10lvl1sec64">                    
                                                                        <div class="section-name">The need for reactors</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec65" class="chapter-section">
                                                                    <a href="#ch10lvl1sec65">                    
                                                                        <div class="section-name">Getting started with Reactors</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec66" class="chapter-section">
                                                                    <a href="#ch10lvl1sec66">                    
                                                                        <div class="section-name">The &amp;quot;Hello World&amp;quot; program</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec67" class="chapter-section">
                                                                    <a href="#ch10lvl1sec67">                    
                                                                        <div class="section-name">Event streams</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec68" class="chapter-section">
                                                                    <a href="#ch10lvl1sec68">                    
                                                                        <div class="section-name">Reactors</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec69" class="chapter-section">
                                                                    <a href="#ch10lvl1sec69">                    
                                                                        <div class="section-name">Schedulers</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec70" class="chapter-section">
                                                                    <a href="#ch10lvl1sec70">                    
                                                                        <div class="section-name">Reactor lifecycle</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec71" class="chapter-section">
                                                                    <a href="#ch10lvl1sec71">                    
                                                                        <div class="section-name">Reactor system services</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec72" class="chapter-section">
                                                                    <a href="#ch10lvl1sec72">                    
                                                                        <div class="section-name">Protocols</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec73" class="chapter-section">
                                                                    <a href="#ch10lvl1sec73">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec74" class="chapter-section">
                                                                    <a href="#ch10lvl1sec74">                    
                                                                        <div class="section-name">Exercises</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                            </ul>
                                        </div>
                                        
                                        <div role="tabpanel" class="tab-pane fade" id="author">
                                            <div class="visible-xs">
                                                <h4 class="mobile-title">About the Author</h4>
                                                <hr>
                                            </div>
                                            <p><strong>Aleksandar Prokopec</strong></p>
                                            <div>
                                                <p><span id="author_bio_c" class="sugar_field">Aleksandar Prokopec, who also authored the first edition of this book, is a concurrent and distributed programming researcher. He holds a PhD in computer science from the Ã‰cole Polytechnique FÃ©dÃ©rale de Lausanne, Switzerland. He has worked at Google and is currently a principal researcher at Oracle Labs.</span></p>
                <p><span id="author_bio_c" class="sugar_field">As a member of the Scala team at EPFL, Aleksandar actively contributed to the Scala programming language, and he has worked on programming abstractions for concurrency, data-parallel programming support, and concurrent data structures for Scala. He created the Scala Parallel Collections framework, which is a library for high-level data-parallel programming in Scala, and participated in working groups for Scala concurrency libraries, such as Futures, Promises, and ScalaSTM. Aleksandar is the primary author of the reactor programming model for distributed computing.</span></p>
                                            </div>
                                        </div>
                                        
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    
                    <div class="next-wrapper">
                        <div class="row ns">
                            <hr />
                            <span class="hidden-xs">
                                <h4 class="pull-left">
                                    <strong>Up Next: </strong><span class="section-title"></span>
                                </h4>
                                <a href="#" class="btn btn-primary pull-right btn-lg">
                                    Next Section
                                </a>
                            </span>
                            <span class="visible-xs">
                                <a href="#" class="btn btn-primary btn-block btn-lg">
                                    Next Section
                                </a>
                            </span>
                        </div>
                        <div class="row ns">
                            <hr>
                        </div>
                    </div>
                </div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch01"></a>ChapterÂ 1.Â Introduction</h2></div></div></div><div class="blockquote"><table border="0" width="100%" cellspacing="0" cellpadding="0" class="blockquote" summary="Block quote"><tr><td width="10%" valign="top">Â </td><td width="80%" valign="top"><p><span class="emphasis"><em>"For over a decade prophets have voiced the contention that the organization of a single computer has reached its limits and that truly significant advances can be made only by interconnection of a multiplicity of computers."</em></span></p></td><td width="10%" valign="top">Â </td></tr><tr><td width="10%" valign="top">Â </td><td colspan="2" align="right" valign="top">--<span class="attribution"><span class="emphasis"><em>Gene Amdahl, 1967</em></span></span></td></tr></table></div><p>Although the discipline of concurrent programming has a long history, it has gained a lot of traction in recent years with the arrival of multi core processors. The recent development in computer hardware not only revived some classical concurrency techniquesÂ but also started a major paradigm shift in concurrent programming. At a time when concurrency is becoming so important, an understanding of concurrent programming is an essential skill for every software developer.</p><p>This chapter explains the basics of concurrent computing and presents some Scala preliminaries required for this book. Specifically, it does the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Shows a brief overview of concurrent programming</p></li><li style="list-style-type: disc"><p>Studies the advantages of using Scala when it comes to concurrency</p></li><li style="list-style-type: disc"><p>Covers the Scala preliminaries required for reading this book</p></li></ul></div><p>We will start by examining what concurrent programming is and why it is important.</p></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec7"></a>Concurrent programming</h2></div></div><hr /></div><p>In <span class="strong"><strong>concurrent programming</strong></span>, we express a program as a set of concurrent computations that execute during overlapping time intervals and coordinate in some way. Implementing a concurrent program that functions correctly is usually much harder than implementing a sequential one. All the pitfalls present in sequential programming lurk in every concurrent program, but there are many other things that can go wrong, as we will learn in this book. A natural question arises: why bother? Can't we just keep writing sequential programs?</p><p>Concurrent programming has multiple advantages. First, increased concurrency can improve <span class="strong"><strong>program performance</strong></span>. Instead of executing the entire program on a single processor, different subcomputations can be performed on separate processors, making the program run faster. With the spread of multicore processors, this is the primary reason why concurrent programming is nowadays getting so much attention.</p><p>A concurrent programming model can result in faster I/O operations. A purely sequential program must periodically poll I/O to check if there is any data input available from the keyboard, the network interface, or some other device. A concurrent program, on the other hand, can react to I/O requests immediately. For I/O-intensive operations, this results in improved throughput, and is one of the reasons why concurrent programming support existed in programming languages even before the appearance of multiprocessors. Thus, concurrency can ensure the improved <span class="strong"><strong>responsiveness</strong></span> of a program that interacts with the environment.</p><p>Finally, concurrency can simplify the <span class="strong"><strong>implementation</strong></span> and <span class="strong"><strong>maintainability</strong></span> of computer programs. Some programs can be represented more concisely using concurrency. It can be more convenient to divide the program into smaller, independent computations than to incorporate everything into one large program. User interfaces, web servers, and game engines are typical examples of such systems.</p><p>In this book, we adopt the convention that concurrent programs communicate through the use of shared memory, and execute on a single computer. By contrast, a computer program that executes on multiple computers, each with its own memory, is called a <span class="strong"><strong>distributed program</strong></span>, and the discipline of writing such programs is called <span class="strong"><strong>distributed programming</strong></span>. Typically, a distributed program must assume that each of the computers can fail at any point, and provide some safety guarantees if this happens. We will mostly focus on concurrent programs, but we will also look at examples of distributed programs.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec11"></a>A brief overview of traditional concurrency</h3></div></div></div><p>In a computer system, concurrency can manifest itself in the computer hardware, at the operating system level, or at the programming language level. We will focus mainly on programming-language-level concurrency.</p><p>The coordination of multiple executions in a concurrent system is called <span class="strong"><strong>synchronization</strong></span>, and it is a key part in successfully implementing concurrency. Synchronization includes mechanisms used to order concurrent executions in time. Furthermore, synchronization specifies how concurrent executions communicate, that is, how they exchange information. In concurrent programs, different executions interact by modifying the shared memory subsystem of the computer. This type of synchronization is called <span class="strong"><strong>shared memory communication</strong></span>. In distributed programs, executions interact by exchanging messages, so this type of synchronization is called <span class="strong"><strong>message-passing communication</strong></span>.</p><p>At the lowest level, concurrent executions are represented by entities called processes and threads, covered in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Concurrency on the JVM and the Java Memory Model</em></span>. Processes and threads traditionally use entities such as locks and monitors to order parts of their execution. Establishing an order between the threads ensures that the memory modifications done by one thread are visible to a thread that executes later.</p><p>Often, expressing concurrent programs using threads and locks is cumbersome. More complex concurrent facilities have been developed to address this, such as communication channels, concurrent collections, barriers, countdown latches, and thread pools. These facilities are designed to more easily express specific concurrent programming patterns, and some of them are covered in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Traditional Building Blocks of Concurrency</em></span>.</p><p>Traditional concurrency is relatively low-level and prone to various kinds of errors, such as deadlocks, starvations, data races, and race conditions. You will usually not use low-level concurrency primitives when writing concurrent Scala programs. Still, a basic knowledge of low-level concurrent programming will prove invaluable in understanding high-level concurrency concepts later.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec12"></a>Modern concurrency paradigms</h3></div></div></div><p>Modern concurrency paradigms are more advanced than traditional approaches to concurrency. Here, the crucial difference lies in the fact that a high-level concurrency framework expresses <span class="emphasis"><em>which</em></span> goal to achieve, rather than <span class="emphasis"><em>how to achieve</em></span> that goal.</p><p>In practice, the difference between low-level and high-levelÂ concurrency is less clear, and different concurrency frameworks form a continuum rather than two distinct groups. Still, recent developments in concurrent programming show a bias towards declarative and functional programming styles.</p><p>As we will see in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Concurrency on the JVM and the Java Memory Model</em></span>, computing a value concurrently requires creating a thread with a custom <code class="literal">run</code> method, invoking the <code class="literal">start</code> method, waiting until the thread completes, and then inspecting specific memory locations to read the result. Here, what we really want to say is <span class="emphasis"><em>compute some value concurrently, and inform me when you are done</em></span>. Furthermore, we would prefer to use a programming model that abstracts over the coordination details of the concurrent computation, to treat the result of the computation as if we already have it, rather than having to wait for it and then reading it from the memory. <span class="strong"><strong>Asynchronous programming using futures</strong></span> is a paradigm designed to specifically support these kinds of statements, as we will learn in <a class="link" href="#" linkend="ch04">Chapter 4</a>, <span class="emphasis"><em>Asynchronous Programming with Futures and Promises</em></span>. Similarly, <span class="strong"><strong>reactive programming using event streams</strong></span> aims to declaratively express concurrent computations that produce many values, as we will see in <a class="link" href="#" linkend="ch06">Chapter 6</a>, <span class="emphasis"><em>Concurrent Programming with Reactive Extensions</em></span>.</p><p>The declarative programming style is increasingly common in sequential programming too. Languages such as Python, Haskell, Ruby, and Scala express operations on their collections in terms of functional operatorsÂ and allow statements such as <span class="emphasis"><em>filter all negative integers from this collection</em></span>. This statement expresses a goal rather than the underlying implementation, allowing to it easy to parallelize such an operation behind the scene. <a class="link" href="#" linkend="ch05">Chapter 5</a>, <span class="emphasis"><em>Data-Parallel Collections</em></span>, describes the <span class="strong"><strong>data-parallel</strong></span> collections framework available in Scala, which is designed to accelerate collection operations using multicores.</p><p>Another trend seen in high-level concurrency frameworks is specialization towards specific tasks. Software transactional memory technology is specifically designed to express memory transactionsÂ and does not deal with how to start concurrent executions at all. A <span class="strong"><strong>memory transaction</strong></span> is a sequence of memory operations that appear as if they either execute all at once or do not execute at all. This is similar to the concept of database transactions. The advantage of using memory transactions is that this avoids a lot of errors typically associated with low-level concurrency. <a class="link" href="#" linkend="ch07">Chapter 7</a>, <span class="emphasis"><em>Software Transactional Memory</em></span>, explains software transactional memory in detail.</p><p>Finally, some high-level concurrency frameworks aim to transparently provide distributed programming support as well. This is especially true for data-parallel frameworks and message-passing concurrency frameworks, such as the <span class="strong"><strong>actors</strong></span> described in Chapter 8, <span class="emphasis"><em>Actors</em></span>.</p></div></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec8"></a>The advantages of Scala</h2></div></div><hr /></div><p>Although Scala is still a language on the rise, it has yet to receive the wide-scale adoption of a language such as Java; nonetheless its support for concurrent programming is rich and powerful. Concurrency frameworks for nearly all the different styles of concurrent programming exist in the Scala ecosystemÂ and are being actively developed. Throughout its development, Scala has pushed the boundaries when it comes to providing modern, high-level application programming interfaces or APIs for concurrent programming. There are many reasons for this.</p><p>The primary reason that so many modern concurrency frameworks have found their way into Scala is its inherent syntactic flexibility. Thanks to features such as first-class functions, byname parameters, type inference, and pattern matching explained in the following sections, it is possible to define APIs that look as if they are built-in language features.</p><p>Such APIs emulate various programming models as embedded domain-specific languages, with Scala serving as a host language: actors, software transactional memory, and futures are examples of APIs that look like they are basic language featuresÂ when they are in fact implemented as libraries. On one hand, Scala avoids the need for developing a new language for each new concurrent programming modelÂ and serves as a rich nesting ground for modern concurrency frameworks. On the other hand, lifting the syntactic burden present in many other languages attracts more users.</p><p>The second reason Scala has pushed ahead lies in the fact that it is a safe language. Automatic garbage collection, automatic bound checks, and the lack of pointer arithmetic helps to avoid problems such as memory leaks, buffer overflows, and other memory errors. Similarly, static type safety eliminates a lot of programming errors at an early stage. When it comes to concurrent programming, which is in itself prone to various kinds of concurrency errors, having one less thing to worry about can make a world of difference.</p><p>The third important reason is interoperability. Scala programs are compiled into Java bytecode, so the resulting executable code runs on top of the <span class="strong"><strong>Java Virtual Machine</strong></span> (<span class="strong"><strong>JVM</strong></span>). This means that Scala programs can seamlessly use existing Java libraries, and interact with Java's rich ecosystem. Often, transitioning to a different language is a painful process. In the case of Scala, a transition from a language such as Java can proceed gradually and is much easier. This is one of the reasons for its growing adoption, and also a reason why some Java-compatible frameworks choose Scala as their implementation language.</p><p>Importantly, the fact that Scala runs on the JVM implies that Scala programs are portable across a range of different platforms. Not only that, but the JVM has well-defined threading and memory models, which are guaranteed to work in the same way on different computers. While portability is important for the consistent semantics of sequential programs, it is even more important when it comes to concurrent computing.</p><p>Having seen some of Scala's advantages for concurrent programming, we are now ready to study the language features relevant for this book.</p></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec9"></a>Preliminaries</h2></div></div><hr /></div><p>This book assumes a basic familiarity with sequential programming. While we advise readers to get acquainted with the Scala programming language, an understanding of a similar language, such as Java or C#, should be sufficient for this book. A basic familiarity with concepts in object-oriented programming, such as classes, objects, and interfaces, is helpful. Similarly, a basic understanding of functional programming principles, such as first-class functions, purity, and type-polymorphism are beneficial in understanding this bookÂ but are not a strict prerequisite.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec13"></a>Execution of a Scala program</h3></div></div></div><p>To better understand the execution model of Scala programs, let's consider a simple program that uses the <code class="literal">square</code> method to compute the square value of the number <code class="literal">5</code>, and then prints the result to the standard output:</p><pre class="programlisting">object SquareOf5 extends App {
  def square(x: Int): Int = x * x
  val s = square(5)
  println(s"Result: $s")
}
</pre><p>We can run this program using the <span class="strong"><strong>Simple Build Tool</strong></span> (<span class="strong"><strong>SBT</strong></span>), as described in the <span class="emphasis"><em>Preface</em></span>. When a Scala program runs, the JVM runtime allocates the memory required for the program. Here, we consider two important memory regions--the <span class="strong"><strong>call stack</strong></span> and the <span class="strong"><strong>object heap</strong></span>. The call stack is a region of memory in which the program stores information about the local variables and parameters of the currently executed methods. The object heap is a region of memory in which objects are allocated by the program. To understand the difference between the two regions, we consider a simplified scenario of this program's execution.</p><p>First, in figure <span class="strong"><strong>1</strong></span>, the program allocates an entry to the call stack for the local variable <code class="literal">s</code>. Then, it calls the <code class="literal">square</code> method in figure <span class="strong"><strong>2</strong></span> to compute the value for the local variable <code class="literal">s</code>. The program places the value <code class="literal">5</code> on the call stack, which serves as the value for the <code class="literal">x</code> parameter. It also reserves a stack entry for the return value of the method. At this point, the program can execute the <code class="literal">square</code> method, so it multiplies the <code class="literal">x</code> parameter by itself, and places the return value <code class="literal">25</code> on the stack in figure <span class="strong"><strong>3</strong></span>. This is shown in the first row in the following illustration:</p><div class="mediaobject"><img src="graphics/B05779_01_01.jpg" /></div><p>After the <code class="literal">square</code> method returns the result, the result <code class="literal">25</code> is copied into the stack entry for the local variable <code class="literal">s</code>, as shown in figure <span class="strong"><strong>4</strong></span>. Now, the program must create the string for the <code class="literal">println</code> statement. In Scala, strings are represented as object instances of the <code class="literal">String</code> class, so the program allocates a new <code class="literal">String</code> object to the object heap, as illustrated in figure <span class="strong"><strong>5</strong></span>. Finally, in figure <span class="strong"><strong>6</strong></span>, the program stores the reference to the newly allocated object into the stack entry <code class="literal">x</code>, and calls the <code class="literal">println</code> method.</p><p>Although this demonstration is greatly simplified, it shows the basic execution model for Scala programs. In <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Concurrency on the JVM and the Java Memory Model</em></span>, we will learn that each thread of execution maintains a separate call stack, and that threads mainly communicate by modifying the object heap. We will learn that the disparity between the state of the heap and the local call stack is frequently responsible for certain kinds of error in concurrent programs.</p><p>Having seen an example of how Scala programs are typically executed, we now proceed to an overview of Scala features that are essential to understand the contents of this book.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec14"></a>A Scala primer</h3></div></div></div><p>In this section, we present a short overview of the Scala programming language features that are used in the examples in this book. This is a quick and cursory glance through the basics of Scala. Note that this section is not meant to be a complete introduction to Scala. This is to remind you about some of the language's features, and contrast them with similar languages that might be familiar to you. If you would like to learn more about Scala, refer to some of the books referred to in the <span class="emphasis"><em>Summary</em></span> of this chapter.</p><p>A <code class="literal">Printer</code> class, which takes a <code class="literal">greeting</code> parameter and has two methods named <code class="literal">printMessage</code> and <code class="literal">printNumber</code>, is declared as follows:</p><pre class="programlisting">class Printer(val greeting: String) {
  def printMessage(): Unit = println(greeting + "!")
  def printNumber(x: Int): Unit = {
    println("Number: " + x)
  }
}
</pre><p>In the preceding code, the <code class="literal">printMessage</code> method does not take any argumentsÂ and contains a single <code class="literal">println</code> statement. The <code class="literal">printNumber</code> method takes a single argument <code class="literal">x</code> of the <code class="literal">Int</code> type. Neither method returns a value, which is denoted by the <code class="literal">Unit</code> type.</p><p>We instantiate the class and call its methods as follows:</p><pre class="programlisting">val printy = new Printer("Hi")
printy.printMessage()
printy.printNumber(5)
</pre><p>Scala allows the declaration of <span class="strong"><strong>singleton objects</strong></span>. This is like declaring a class and instantiating its single instance at the same time. We saw the <code class="literal">SquareOf5</code> singleton object earlier, which was used to declare a simple Scala program. The following singleton object, named <code class="literal">Test</code>, declares a single <code class="literal">Pi</code> field and initializes it with the value <code class="literal">3.14</code>:</p><pre class="programlisting">object Test {
  val Pi = 3.14
}
</pre><p>While classes in similar languages extend entities that are called interfaces, Scala classes can extend <span class="strong"><strong>traits</strong></span>. Scala's traits allow declaring both concrete fields and method implementations. In the following example, we declare the <code class="literal">Logging</code> trait, which outputs a custom error and warning messages using the abstract <code class="literal">log</code> method, and then mix the trait into the <code class="literal">PrintLogging</code> class:</p><pre class="programlisting">trait Logging {
  def log(s: String): Unit
  def warn(s: String) = log("WARN: " + s)
  def error(s: String) = log("ERROR: " + s)
}
class PrintLogging extends Logging {
  def log(s: String) = println(s)
}
</pre><p>Classes can have <span class="strong"><strong>type parameters</strong></span>. The following generic <code class="literal">Pair</code> class takes two type parameters, <code class="literal">P</code> and <code class="literal">Q</code>, which determines the types of its arguments, named <code class="literal">first</code> and <code class="literal">second</code>:</p><pre class="programlisting">class Pair[P, Q](val first: P, val second: Q)
</pre><p>Scala has support for first-classÂ function objects, also called <span class="strong"><strong>lambdas</strong></span>. In the followingÂ code snippet, we declare a <code class="literal">twice</code> lambda, which multiplies its argument by two:</p><pre class="programlisting">val twice: Int =&gt; Int = (x: Int) =&gt; x * 2
</pre><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip3"></a>Tip</h3><p><span class="strong"><strong>Downloading the example code:</strong></span></p><p>You can download the example code files for all Packt books you have purchased from your account at <a class="ulink" href="http://www.packtpub.com" target="_blank">http://www.packtpub.com</a>. If you purchased this book elsewhere, you can visit <a class="ulink" href="http://www.packtpub.com/support" target="_blank">http://www.packtpub.com/support</a> and register to have the files e-mailed directly to you.</p></div><p>In the preceding code, the <code class="literal">(x: Int)</code> part is the argument to the lambda, and <code class="literal">x * 2</code> is its body. The <code class="literal">=&gt;</code> symbol must be placed between the arguments and the body of the lambda. The same <code class="literal">=&gt;</code> symbol is also used to express the type of the lambda, which is <code class="literal">Int =&gt; Int</code>, pronounced as <code class="literal">Int</code> to <code class="literal">Int</code>. In the preceding example, we can omit the type annotation <code class="literal">Int =&gt; Int</code>, and the compiler will infer the type of the <code class="literal">twice</code> lambda automatically, as shown in the following code:</p><pre class="programlisting">val twice = (x: Int) =&gt; x * 2
</pre><p>Alternatively, we can omit the type annotation in the lambda declaration and arrive at a more convenient syntax, as follows:</p><pre class="programlisting">val twice: Int =&gt; Int = x =&gt; x * 2
</pre><p>Finally, whenever the argument to the lambda appears only once in the body of the lambda, Scala allows a more convenient syntax, as follows:</p><pre class="programlisting">val twice: Int =&gt; Int = _ * 2
</pre><p>First-class functions allow manipulating blocks of code as if they were first-class values. They allow a more lightweight and concise syntax. In the following example, we use <span class="strong"><strong>byname parameters</strong></span> to declare a <code class="literal">runTwice</code> method, which runs the specified block of code <code class="literal">body</code> twice:</p><pre class="programlisting">def runTwice(body: =&gt;Unit) = {
  body
  body
}
</pre><p>A byname parameter is formed by putting the <code class="literal">=&gt;</code> annotation before the type. Whenever the <code class="literal">runTwice</code> method references the <code class="literal">body</code> argument, the expression is re-evaluated, as shown in the following snippet:</p><pre class="programlisting">runTwice { // this will print Hello twice
  println("Hello")
}
</pre><p>Scala <code class="literal">for</code> expressions are a convenient way to traverse and transform collections. The following <code class="literal">for</code> loop prints the numbers in the range from <code class="literal">0Â until 10</code>;Â where <code class="literal">10</code> is not included in the range:</p><pre class="programlisting">for (i &lt;- 0 until 10) println(i)
</pre><p>In the preceding code, the range is created with the expression <code class="literal">0 until 10</code>; this is equivalent to the expression <code class="literal">0.until(10)</code>, which calls the method <code class="literal">until</code> on the value <code class="literal">0</code>. In Scala, the dot notation can sometimes be dropped when invoking methods on objects.</p><p>Every <code class="literal">for</code> loop is equivalent to a <code class="literal">foreach</code> call. The preceding <code class="literal">for</code> loop is translated by the Scala compiler to the following expression:</p><pre class="programlisting">(0 until 10).foreach(i =&gt; println(i))
</pre><p>Â 
 For-comprehensions areÂ usedÂ to transform data. The following for-comprehension transforms all the numbers from <code class="literal">0Â until 10</code> by multiplying them by <code class="literal">-1</code>:</p><pre class="programlisting">val negatives = for (i &lt;- 0 until 10) yield -i
</pre><p>The <code class="literal">negatives</code> value contains negative numbers from <code class="literal">0</code> until <code class="literal">-10</code>. This for-comprehension is equivalent to the following <code class="literal">map</code> call:</p><pre class="programlisting">val negatives = (0 until 10).map(i =&gt; -1 * i)
</pre><p>It is also possible to transform data from multiple inputs. The following for-comprehension creates all pairs of integers between <code class="literal">0</code> and <code class="literal">4</code>:</p><pre class="programlisting">val pairs = for (x &lt;- 0 until 4; y &lt;- 0 until 4) yield (x, y)
</pre><p>The preceding for-comprehension is equivalent to the following expression:</p><pre class="programlisting">val pairs = (0 until 4).flatMap(x =&gt; (0 until 4).map(y =&gt; (x, y)))
</pre><p>We can nest an arbitrary number of generator expressions in a for-comprehension. The Scala compiler will transform them into a sequence of nested <code class="literal">flatMap</code> calls, followed by a <code class="literal">map</code> call at the deepest level.</p><p>Commonly used Scala collections include sequences, denoted by the <code class="literal">Seq[T]</code> type; maps, denoted by the <code class="literal">Map[K, V]</code> type; and sets, denoted by the <code class="literal">Set[T]</code> type. In the following code, we create a sequence of strings:</p><pre class="programlisting">val messages: Seq[String] = Seq("Hello", "World.", "!")
</pre><p>Throughout this book, we rely heavily on the <span class="strong"><strong>string interpolation</strong></span> feature. Normally, Scala strings are formed with double quotation marks. Interpolated strings are preceded with an <code class="literal">s</code> character, and can contain <code class="literal">$</code> symbols with arbitrary identifiers resolved from the enclosing scope, as shown in the following example:</p><pre class="programlisting">val magic = 7
val myMagicNumber = s"My magic number is $magic"
</pre><p><span class="strong"><strong>Pattern matching</strong></span> is another important Scala feature. For readers with Java, C#, or C background, a good way to describe it is to say that Scala's <code class="literal">match</code> statement is like the <code class="literal">switch</code> statement on steroids. The <code class="literal">match</code> statement can decompose arbitrary datatypesÂ and allows you to express different cases in the program concisely.</p><p>In the following example, we declare a <code class="literal">Map</code> collection, named <code class="literal">successors</code>, used to map integers to their immediate successors. We then call the <code class="literal">get</code> method to obtain the successor of the number 5. The <code class="literal">get</code> method returns an object with the <code class="literal">Option[Int]</code> type, which may be implemented either with the <code class="literal">Some</code> class, indicating that the number 5 exists in the map, or the <code class="literal">None</code> class, indicating that the number 5 is not a key in the map. Pattern matching on the <code class="literal">Option</code> object allows proceeding casewise, as shown in the following code snippet:</p><pre class="programlisting">val successors = Map(1 -&gt; 2, 2 -&gt; 3, 3 -&gt; 4)
successors.get(5) match {
  case Some(n) =&gt; println(s"Successor is: $n")
  case None    =&gt; println("Could not find successor.")
}
</pre><p>In Scala, most operators can be overloaded. <span class="strong"><strong>Operator overloading</strong></span> is no different from declaring a method. In the following code snippet, we declare a <code class="literal">Position</code> class with a <code class="literal">+</code> operator:</p><pre class="programlisting">class Position(val x: Int, val y: Int) {
  def +(that: Position) = new Position(x + that.x, y + that.y)
}
</pre><p>Finally, Scala allows defining <span class="strong"><strong>package objects</strong></span> to store top-level method and value definitions for a given package. In the following code snippet, we declare theÂ package object for the <code class="literal">org.learningconcurrency</code> package. We implement the top level <code class="literal">log</code> method, which outputs a given string and the current thread name:</p><pre class="programlisting">package org
package object learningconcurrency {
  def log(msg: String): Unit =
    println(s"${Thread.currentThread.getName}: $msg")
}
</pre><p>We will use the <code class="literal">log</code> method in the examples throughout this book to trace how concurrent programs are executed.</p><p>This concludes our quick overview of important Scala features. If you would like to obtain a deeper knowledge about any of these language constructs, we suggest that you check out one of the introductory books on sequential programming in Scala.</p></div></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec10"></a>Overview of new features in Scala 2.12</h2></div></div><hr /></div><p>At the time of writing, the next planned release of the language is Scala 2.12. From the user and API perspective, Scala 2.12 does not introduce new ground-breaking features. The goal of the 2.12 release is to improve code optimization and make Scala compliant with the Java 8 runtime. Since Scala's primary target is the Java runtime, making Scala compliant with Java 8 runtime will reduce the size of compiled programs and JAR files, better performance and faster compilation. From the user perspective, the major change is that you will have to install the JDK 8 framework instead of JDK 7.</p><p>The particular changes in Scala 2.12 worth mentioning are the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>In previous versions, traits compiled to a single interface if all of their methods were abstract. If the trait had a concrete method implementation, the compiler generated two class files--one containing the JVM interface, and another class file containing the implementations of the concrete methods. In Scala 2.12, the compiler will generate a single interface file containing the Java 8 <span class="strong"><strong>default methods</strong></span>. The net effect is reduced code size.</p></li><li style="list-style-type: disc"><p>Previously, each Scala closure was compiled into a separate class. Starting with 2.12, Scala closures are compiled into Java 8-style lambdas. The consequence is reduced code sizeÂ and potentially better optimizations by the Java 8 runtime.</p></li><li style="list-style-type: disc"><p>Scala compiles into Java bytecodes, which are then interpreted on the Java Virtual Machine. In Scala 2.12, the old compiler backend is replaced with a new implementation that generates bytecode more quicklyÂ with a positive impact on compilation speed.</p></li><li style="list-style-type: disc"><p>Scala 2.12 comes with a new optimizer, which is enabled with the <code class="literal">-opt</code> compiler flag. The new optimizer is more aggressive at inlining final methods, does better escape analysis for objects and functions that are created and used in a single method, and does dead code elimination. All this has a positive impact on theÂ performance of Scala programs.</p></li><li style="list-style-type: disc"><p>Scala 2.12 allows using lambdas for Single Abstract Method (SAM) types. SAM types are classes or traits that have exactly one abstract method, which is normally implemented by extending the class. Assume that we have a method invocation with an argument whose expected type is a SAM type. If the user passes a lambda, that is, a function literal, instead of a SAM type instance, the 2.12 compiler will automatically convert the function object into an instance of the SAM type.</p></li></ul></div></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec11"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we studied what concurrent programming is and why Scala is a good language for concurrency. We gave a brief overview of what you will learn in this book, and how the book is organized. Finally, we stated some Scala preliminaries necessary for understanding the various concurrency topics in the subsequent chapters. If you would like to learn more about sequential Scala programming, we suggest that you read the book, <span class="emphasis"><em>Programming in Scala</em></span>, <span class="emphasis"><em>Martin Odersky, Lex Spoon, and Bill Venners</em></span>, <span class="emphasis"><em>Artima Inc</em></span>.</p><p>In the next chapter, we will start with the fundamentals of concurrent programming on the JVM. We will introduce the basic concepts in concurrent programming, present the low-level concurrency utilities available on the JVM, and learn about the Java Memory Model.</p></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec12"></a>Exercises</h2></div></div><hr /></div><p>The following exercises are designed to test your knowledge of the Scala programming language. They cover the content presented in this chapter, along with some additional Scala features. The last two exercises contrast the difference between concurrent and distributed programming, as defined in this chapter. You should solve them by sketching out a pseudocode solution, rather than a complete Scala program.</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Implement a <code class="literal">compose </code>Â method with the following signature:
</p><pre class="programlisting">            def compose[A, B, C]
            (g: B =&gt; C, f: A =&gt; B): A =&gt; C = ???
</pre><p>
</p><p>This method must return a function <code class="literal">h</code>, which is the composition of the functions <code class="literal">f</code> and <code class="literal">g</code></p></li><li><p>Implement a <code class="literal">fuse </code>method with the following signature:
</p><pre class="programlisting">            def fuse[A, B]
            (a: Option[A], b: Option[B]): Option[(A, B)] = ???
</pre><p>
</p><p>The resulting <code class="literal">Option</code> object should contain a tuple of values from the <code class="literal">Option</code> objects <code class="literal">a</code> and <code class="literal">b</code>, given that both <code class="literal">a</code> and <code class="literal">b</code> are non-empty. Use for-comprehensions</p></li><li><p>Implement a <code class="literal">check</code> method, which takes a set of values of type <code class="literal">T</code> and a function of type <code class="literal">T =&gt; Boolean</code>:
</p><pre class="programlisting">            def check[T](xs: Seq[T])(pred: T =&gt; Boolean): Boolean = ???
</pre><p>
</p><p>The method must return <code class="literal">true</code> if and only if the <code class="literal">pred</code> function returns <code class="literal">true</code> for all the values in <code class="literal">xs</code> without throwing an exception. Use the <code class="literal">check</code> method as follows:</p><p>
</p><pre class="programlisting">            check(0 until 10)(40 / _ &gt; 0)
</pre><p>
</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip4"></a>Tip</h3><p>The <code class="literal">check</code> method has a curried definition: instead of just one parameter list, it has two of them. Curried definitions allow a nicer syntax when calling the function, but are otherwise semantically equivalent to single-parameter list definitions.</p></div></li><li><p>Modify the <code class="literal">Pair</code> class from this chapter so that it can be used in a pattern match.
</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip5"></a>Tip</h3><p>If you haven't already done so, familiarize yourself with pattern matching in Scala.</p></div></li><li><p>Implement a <code class="literal">permutations</code> function, which, given a string, returns a sequence of strings that are lexicographic permutations of the input string:
</p><pre class="programlisting">            def permutations(x: String): Seq[String]
</pre></li><li><p>Implement a <code class="literal">combinations</code> function that, given a sequence of elements, produces an iterator over all possible combinations of length <code class="literal">n</code>. A combination is a way of selecting elements from the collectionÂ so that every element is selected once, and the order of elements does not matter. For example, given a collection <code class="literal">Seq(1, 4, 9, 16)</code>, combinations of length 2 are <code class="literal">Seq(1, 4)</code>, <code class="literal">Seq(1, 9)</code>, <code class="literal">Seq(1, 16)</code>, <code class="literal">Seq(4, 9)</code>, <code class="literal">Seq(4, 16)</code>, and <code class="literal">Seq(9, 16)</code>. The combinations function has the following signature:
</p><pre class="programlisting">            def combinations(n: Int, xs: Seq[Int]): Iterator[Seq[Int]]
</pre><p>
</p><p>See the <code class="literal">Iterator</code> API in the standard library documentation</p></li><li><p>Implement a method that takes a regular expression, and returns a partial function from a string to lists of matches within that string:</p><pre class="programlisting">            def matcher   (regex: String): PartialFunction[String,
            List[String]]
</pre><p>
</p><p>The partial function should not be defined if there are no matches within the argument strings. Otherwise, it should use the regular expression to output the list of matches.</p></li><li><p>Consider that you and three of your colleagues working in an office divided into cubicles. You cannot see each other, and you are not allowed to verbally communicate, as that might disturb other workers. Instead, you can throw pieces of paper with short messages at each other. Since you are confined in a cubicle, neither of you can tell if the message has reached its destination. At any point, you or one of your colleagues may be called to the boss's office and kept there indefinitely. Design an algorithm in which you and your colleagues can decide when to meet at the local bar. With the exception of the one among you who was called to the boss's office, all of you have to decide on the same time. What if some of the paper pieces can arbitrarily miss the target cubicle?</p></li><li><p>Imagine that, in the previous exercise, you and your colleagues also have a whiteboard in the hall next to the office. Each one of you can occasionally pass through the hall and write something on the whiteboard, but there is no guarantee that either of you will be in the hall at the same time.</p></li></ol></div><p>Solve the problem from the previous exercise, this time using the whiteboard.</p></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch02"></a>ChapterÂ 2.Â Concurrency on the JVM and the Java Memory Model</h2></div></div></div><div class="blockquote"><table border="0" width="100%" cellspacing="0" cellpadding="0" class="blockquote" summary="Block quote"><tr><td width="10%" valign="top">Â </td><td width="80%" valign="top"><p><span class="emphasis"><em>"All non-trivial abstractions, to some degree, are leaky."</em></span></p></td><td width="10%" valign="top">Â </td></tr><tr><td width="10%" valign="top">Â </td><td colspan="2" align="right" valign="top">--<span class="attribution"><span class="emphasis"><em>Jeff Atwood</em></span></span></td></tr></table></div><p>Since its inception, Scala has run primarily on top of JVM, and this fact has driven the design of many of its concurrency libraries. The memory model in Scala, its multithreading capabilities, and its inter-thread synchronization are all inherited from the JVM. Most, if not all, higher-level Scala concurrency constructs are implemented in terms of the low-level primitives presented in this chapter. These primitives are the basic way to deal with concurrency-in a way, the APIs and synchronization primitives in this chapter constitute the assembly of concurrent programming on the JVM.</p><p>In most cases, you should avoid low-level concurrency in place of higher-level constructs introduced later, but we felt it was important for you to understand what a thread is, that a guarded block is better than busy-waiting, or why a memory model is useful. We are convinced that this is essential for a better understanding of high-level concurrency abstractions. Despite the popular notion that an abstraction that requires knowledge about its implementation is broken, understanding the basics often proves very handy- in practice, all abstractions are to some extent leaky.</p><p>In what follows, we not only explain the cornerstones of concurrency on JVM, but also discuss how they interact with some Scala-specific features. In particular, we will cover the following topics in this chapter:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Creating and starting threads and waiting for their completion</p></li><li style="list-style-type: disc"><p>Communication between threads using object monitors and the <code class="literal">synchronized</code> statement</p></li><li style="list-style-type: disc"><p>How to avoid busy-waiting using guarded blocks</p></li><li style="list-style-type: disc"><p>The semantics of volatile variables</p></li><li style="list-style-type: disc"><p>The specifics of theÂ <span class="strong"><strong>Java Memory Model</strong></span> (<span class="strong"><strong>JMM</strong></span>), and why the JMM is important</p></li></ul></div><p>In the following section, we will study how to use threads--the basic way to express concurrent computations.</p></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec13"></a>Processes and threads</h2></div></div><hr /></div><p>In modern, pre-emptive, multitasking operating systems, the programmer has little or no control over the choice of processor on which the program will be executed. In fact, the same program might run on many different processors during its execution and sometimes even simultaneously on several processors. It is usually the task of the <span class="strong"><strong>Operating System</strong></span> (<span class="strong"><strong>OS</strong></span>) to assign executable parts of the program to specific processors--this mechanism is called <span class="strong"><strong>multitasking</strong></span>, and it happens transparently for the computer user.</p><p>Historically, multitasking was introduced to operating systems to improve the user experience by allowing multiple users or programs to use resources of the same computer simultaneously. In cooperative multitasking, programs were able to decide when to stop using the processor and yield control to other programs. However, this required a lot of discipline on the programmer's part and programs could easily give the impression of being unresponsive. For example, a download manager that starts downloading a file must take care in order to yield control to other programs. Blocking the execution until a download finishes will completely ruin the user experience. Most operating systems today rely on pre-emptive multitasking, in which each program is repetitively assigned slices of execution time at a specific processor. These slices are called <span class="strong"><strong>time slices</strong></span>. Thus, multitasking happens transparently for the application programmer as well as the user.</p><p>The same computer program can be started more than once, or even simultaneously within the same OS. A <span class="strong"><strong>process</strong></span> is anÂ instance of a computer program that is being executed. When a process starts, the OS reserves a part of the memory and other computational resourcesÂ and associates them with a specific computer program. The OS then associates the processor with the process, and the process executes during one-time slice. Eventually, the OS gives other processes control over the processor. Importantly, the memory and other computational resources of one process are isolated from the other processes: two processes cannot read each other's memory directly or simultaneously use most of the resources.</p><p>Most programs are comprised of a single process, but some programs run in multiple processes. In this case, different tasks within the program are expressed as separate processes. Since separate processes cannot access the same memory areas directly, it can be cumbersome to express multitasking using multiple processes.</p><p>Multitasking was important long before recent yearsÂ when multicore computers became mainstream. Large programs such as web browsers are divided into many logical modules. A browser's download manager downloads files independent of rendering the web page or updating the HTML <span class="strong"><strong>Document Object Model</strong></span> (<span class="strong"><strong>DOM</strong></span>). While the user is browsing a social networking website, the file download proceeds in the background; but both independent computations occur as part of the same process. These independent computations occurring in the same process are called <span class="strong"><strong>threads</strong></span>. In a typical operating system, there are many more threads than processors.</p><p>Every thread describes the current state of the program<span class="strong"><strong> stack</strong></span> and the program<span class="strong"><strong> counter</strong></span> during program execution. The program stack contains a sequence of method invocations that are currently being executed, along with the local variables and method parameters of each method. The program counter describes the position of the current instruction in the current method. A processor can advance the computation in some thread by manipulating the state of its stack or the state of the program objects and executing the instruction at the current program counter. When we say that a thread performs an action such as writing to a memory location, we mean that the processor executing that thread performs that action. In pre-emptive multitasking, thread execution is scheduled by the operating system. A programmer must assume that the processor time assigned to their thread is unbiased towards other threads in the system.</p><p><span class="strong"><strong>OS threads</strong></span> are a programming facility provided by the OS, usually exposed through an OS-specific programming interface. Unlike separate processes, separate OS threads within the same process share a region of memory, and communicate by writing to and reading parts of that memory. Another way to define a process is to define it as a set of OS threads along with the memory and resources shared by these threads.</p><p>Â 
 Based on the preceding discussion about the relationships between processes and threads, a summary of a typical OS is depicted in the following simplified diagram:</p><div class="mediaobject"><img src="graphics/image_02_001.jpg" /></div><p>The preceding diagram shows an OS in which multiple processes are executing simultaneously. Only the first three processes are shown in the illustration. Each process is assigned a fixed region of computer memory. In practice, the memory system of the OS is much more complex, but this approximation serves as a simple mental model.</p><p>Each of the processes contains multiple OS threads, two of which are shown for each process. Currently, <span class="strong"><strong>Thread 1</strong></span> of <span class="strong"><strong>Process 2</strong></span> is executing on <span class="strong"><strong>CPU Core 1</strong></span>, and <span class="strong"><strong>Thread 2</strong></span> of <span class="strong"><strong>Process 3</strong></span> is executing on <span class="strong"><strong>CPU Core 2</strong></span>. The OS periodically assigns different OS threads to each of the CPU cores to allow the computation to progress in all the processes.</p><p>Having shown the relationship between the OS threads and processes, we turn our attention to see how these concepts relate to the <span class="strong"><strong>Java Virtual Machine</strong></span> (<span class="strong"><strong>JVM</strong></span>), the runtime on top of which Scala programs execute.</p><p>Starting a new JVM instance always creates only one process. Within the JVM process, multiple threads can run simultaneously. The JVM represents its threads with the <code class="literal">java.lang.Thread</code> class. Unlike runtimes for languages such as Python, the JVM does not implement its custom threads. Instead, each Java thread is directly mapped to an OS thread. This means that Java threads behave in a very similar way to the OS threads, and the JVM depends on the OS and its restrictions.</p><p>Scala is a programming language that is by default compiled to the JVM bytecode, and the Scala compiler output is largely equivalent to that of Java from the JVM's perspective. This allows Scala programs to transparently call Java libraries, and in some cases, even vice versa. Scala reuses the threading API from Java for several reasons. First, Scala can transparently interact with the existing Java thread model, which is already sufficiently comprehensive. Second, it is useful to retain the same threading API for compatibility reasons, and there is nothing fundamentally new that Scala can introduce with respect to the Java thread API.</p><p>The rest of this chapter shows how to create JVM threads using Scala, how they can be executed, and how they can communicate. We will show and discuss several concrete examples. Java aficionados, already well-versed in this subject, might choose to skip the rest of this chapter.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec15"></a>Creating and starting threads</h3></div></div></div><p>Every time a new JVM process starts, it creates several threads by default. The most important thread among them is the <span class="strong"><strong>main thread</strong></span>, which executes the <code class="literal">main</code> method of the Scala program. We will show this in the following program, which gets the name of the current thread and prints it to the standard output:</p><pre class="programlisting">object ThreadsMain extends App {
  val t: Thread = Thread.currentThread
  val name = t.getName
  println(s"I am the thread $name")
}
</pre><p>On the JVM, thread objects are represented with the <code class="literal">Thread</code> class. The preceding program uses the static <code class="literal">currentThread</code> method to obtain a reference to the current thread object, and stores it to a local variable named <code class="literal">t</code>. It then calls the <code class="literal">getName</code> method to obtain the thread's name. If you are running this program from <span class="strong"><strong>Simple Build Tool</strong></span> (SBT) with the <code class="literal">run</code> command, as explained in <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Introduction</em></span>, you should see the following output:</p><pre class="programlisting">
<span class="strong"><strong>[info] I am the thread run-main-0</strong></span>
</pre><p>Normally, the name of the main thread is just the <code class="literal">main</code> method. The reason we see a different name is because SBT started our program on a separate thread inside the SBT process. To ensure that the program runs inside a separate JVM process, we need to set SBT's <code class="literal">fork</code> setting to <code class="literal">true</code>:</p><pre class="programlisting">
<span class="strong"><strong>&gt; set fork := true</strong></span>
</pre><p>Invoking the SBT <code class="literal">run</code> command again should give the following output:</p><pre class="programlisting">
<span class="strong"><strong>[info] I am the thread main</strong></span>
</pre><p>Every thread goes through several <span class="strong"><strong>thread states</strong></span> during its existence. When a <code class="literal">Thread</code> object is created, it is initially in the <span class="strong"><strong>new</strong></span> state. After the newly created thread object starts executing, it goes into the <span class="strong"><strong>runnable</strong></span> state. After the thread is done executing, the thread object goes into the <span class="strong"><strong>terminated state</strong></span>, and cannot execute anymore.</p><p>Starting an independent thread of computation consists of two steps. First, we need to create a <code class="literal">Thread</code> object to allocate the memory for the stack and thread state. To start the computation, we need to call the <code class="literal">start</code> method on this object. We show how to do this in the following example application called <code class="literal">ThreadsCreation</code>:</p><pre class="programlisting">object ThreadsCreation extends App {
  class MyThread extends Thread {
    override def run(): Unit = {
      println("New thread running.")
    }
  }
  val t = new MyThread
  t.start()
  t.join()
  println("New thread joined.")
}
</pre><p>When a JVM application starts, it creates a special thread called the <span class="strong"><strong>main thread</strong></span> that executes the method called <code class="literal">main</code> in the specified class, in this case, the <code class="literal">ThreadsCreation</code> object. When the <code class="literal">App</code> class is extended, the <code class="literal">main</code> method is automatically synthesized from the object body. In this example, the main thread first creates another thread of the <code class="literal">MyThread</code> type and assigns it to <code class="literal">t</code>.</p><p>Next, the main thread starts <code class="literal">t</code> by calling the <code class="literal">start</code> method. Calling the <code class="literal">start</code> method eventually results in executing the <code class="literal">run</code> method from the new thread. First, the OS is notified that <code class="literal">t</code> must start executing. When the OS decides to assign the new thread to some processor, this is largely out of the programmer's control, but the OS must ensure that this eventually happens. After the main thread starts the new thread <code class="literal">t</code>, it calls its <code class="literal">join</code> method. This method halts the execution of theÂ main thread until <code class="literal">t</code> completes its execution. We say that the <code class="literal">join</code> operation puts the main thread into the <span class="strong"><strong>waiting state</strong></span> until <code class="literal">t</code> terminates. Importantly, the waiting thread relinquishes its control over the processor, and the OS can assign that processor to some other thread.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note6"></a>Note</h3><p>Waiting threads notify the OS that they are waiting for some condition and cease spending CPU cycles, instead of repetitively checking that condition.</p></div><p>In the meantime, the OS finds an available processor and instructs it to run the child thread. The instructions that a thread must execute are specified by overriding its <code class="literal">run</code> method. The <code class="literal">t</code> instance of the <code class="literal">MyThread</code> class starts by printing the "<code class="literal">New thread running."</code> text to the standard output and then terminates. At this point, the operating system is notified that <code class="literal">t</code> is terminated and eventually lets the main thread continue the execution. The OS then puts the main thread back into the running state, and the main thread prints <code class="literal">"New thread joined."</code>. This is shown in the following diagram:</p><div class="mediaobject"><img src="graphics/image_02_002.jpg" /></div><p>It is important to note that the two outputs <code class="literal">"New thread running."</code> and <code class="literal">"New thread joined."</code> are always printed in this order. This is because the <code class="literal">join</code> call ensures that the termination of the <code class="literal">t</code> thread occurs before the instructions following the <code class="literal">join</code> call.</p><p>When running the program, it is executed so fast that the two <code class="literal">println</code> statements occur almost simultaneously. Could it be that the ordering of the <code class="literal">println</code> statements is just an artifact in how the OS chooses to execute these threads? To verify the hypothesis that the main thread really waits for <code class="literal">t</code> and that the output is not just because the OS is biased to execute <code class="literal">t</code> first in this particular example, we can experiment by tweaking the execution schedule. Before we do that, we will introduce a shorthand to create and start a new thread; the current syntax is too verbose! The new <code class="literal">thread</code> method simply runs a block of code in a newly started thread. This time, we will create the new thread using an anonymous thread class declared inline at the instantiation site:</p><pre class="programlisting">def thread(body: =&gt;Unit): Thread = {
  val t = new Thread {
    override def run() = body
  }
  t.start()
  t
}
</pre><p>The <code class="literal">thread</code> method takes a block of code body, creates a new thread that executes this block of code in its <code class="literal">run</code> method, starts the thread, and returns a reference to the new thread so that the clients can call <code class="literal">join</code> on it.</p><p>Creating and starting threads using the <code class="literal">thread</code> statement is much less verbose. To make the examples in this chapter more concise, we will use the <code class="literal">thread</code> statement from now on. However, you should think twice before using the <code class="literal">thread</code> statement in production projects. It is prudent to correlate the syntactic burden with the computational cost; lightweight syntax can be mistaken for a cheap operation and creating a new thread is relatively expensive.</p><p>We can now experiment with the OS by making sure that all the processors are available. To do this, we will use the static <code class="literal">sleep</code> method on the <code class="literal">Thread</code> class, which postpones the execution of the thread that is being currently executed for the specified number of milliseconds. This method puts theÂ thread into the <span class="strong"><strong>timed waiting</strong></span> state. The OS can reuse the processor for other threads when <code class="literal">sleep</code> is called. Still, we will require a sleep time much larger than the time slice on a typical OS, which ranges from 10 to 100 milliseconds. The following code depicts this:</p><pre class="programlisting">object ThreadsSleep extends App {
  val t = thread {
    Thread.sleep(1000)
    log("New thread running.")
    Thread.sleep(1000)
    log("Still running.")
    Thread.sleep(1000)
    log("Completed.")
  }
  t.join()
  log("New thread joined.")
}
</pre><p>The main thread of the <code class="literal">ThreadSleep</code> application creates and starts a new <code class="literal">t</code> thread that sleeps for one second, then outputs some text, and repeats this two or more times before terminating. The main thread calls <code class="literal">join</code> as before and then prints <code class="literal">"New thread joined."</code>.</p><p>Note that we are now using the <code class="literal">log</code> method described in <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Introduction</em></span>. The <code class="literal">log</code> method prints the specified string value along with the name of the thread that calls the <code class="literal">log</code> method.</p><p>Regardless of how many times you run the preceding application, the last output will always be <code class="literal">"New thread joined."</code>. This program is <span class="strong"><strong>deterministic</strong></span>: given a particular input, it will always produce the same output, regardless of the execution schedule chosen by the OS.</p><p>However, not all the applications using threads will always yield the same output if given the same input. The following code is an example of a <span class="strong"><strong>nondeterministic</strong></span> application:</p><pre class="programlisting">object ThreadsNondeterminism extends App {
  val t = thread { log("New thread running.") }
  log("...")
  log("...")
  t.join()
  log("New thread joined.")
}
</pre><p>There is no guarantee that the <code class="literal">log("...")</code> statements in the main thread occur before or after the <code class="literal">log</code> call in the <code class="literal">t</code> thread. Running the application several times on a multicore processor prints <code class="literal">"..."</code> before, after, or interleaved with the output by the <code class="literal">t</code> thread. By running the program, we get the following output:</p><pre class="programlisting">
<span class="strong"><strong>run-main-46: ...</strong></span>
<span class="strong"><strong>Thread-80: New thread running.</strong></span>
<span class="strong"><strong>run-main-46: ...</strong></span>
<span class="strong"><strong>run-main-46: New thread joined.</strong></span>
</pre><p>Running the program again results in a different order between these outputs:</p><pre class="programlisting">
<span class="strong"><strong>Thread-81: New thread running.</strong></span>
<span class="strong"><strong>run-main-47: ...</strong></span>
<span class="strong"><strong>run-main-47: ...</strong></span>
<span class="strong"><strong>run-main-47: New thread joined.</strong></span>
</pre><p>Most multithreaded programs are nondeterministic, and this is what makes multithreaded programming so hard. There are multiple possible reasons for this. First, the program might be too big for the programmer to reason about its determinism properties, and interactions between threads could simply be too complex. But some programs are inherently non-deterministic. A web server has no idea which client will be the first to send a request for a web page. It must allow these requests to arrive in any possible order and respond to them as soon as they arrive. Depending on the order in which the clients prepare inputs for the web server, they can behave differently even though the requests might be the same.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec16"></a>Atomic execution</h3></div></div></div><p>We have already seen one basic way in which threads can communicate: by waiting for each other to terminate. The information that the joined thread delivers is that it has terminated. In practice, however, this information is not necessarily useful; for example, a thread that renders one page in a web browser must inform the other threads that a specific URL has been visited, so as to render such a visited URL in a different color.</p><p>It turns out that the <code class="literal">join</code> method onÂ threads has an additional property. All the writes to memory performed by the thread being joined occur before the <code class="literal">join</code> call returnsÂ and are visible to threads that call the <code class="literal">join</code> method. This is illustrated by the following example:</p><pre class="programlisting">object ThreadsCommunicate extends App {
  var result: String = null
  val t = thread { result = "\nTitle\n" + "=" * 5 }
  t.join()
  log(result)
}
</pre><p>The main thread will never print <code class="literal">null</code>, as the call to <code class="literal">join</code> always occurs before the <code class="literal">log</code> call, and the assignment to <code class="literal">result</code> occurs before the termination of <code class="literal">t</code>. This pattern is a very basic way in which the threads can use their results to communicate with each other.</p><p>However, this pattern only allows very restricted one-way communication, and it does not allow threads to mutually communicate during their execution. There are many use cases for an unrestricted two-way communication. One example is assigning unique identifiers, in which a set of threads concurrently choose numbers such that no two threads produce the same number. We might be tempted to proceed as in the following incorrect example. We start by showing the first half of the program:</p><pre class="programlisting">object ThreadsUnprotectedUid extends App {
  var uidCount = 0L
  def getUniqueId() = {
    val freshUid = uidCount + 1
    uidCount = freshUid
    freshUid
  }
</pre><p>In the preceding code snippet, we first declare a <code class="literal">uidCount</code> variable that will hold the last unique identifier picked by any thread. The threads will call the <code class="literal">getUniqueId</code> method to compute the first unused identifierÂ and then update the <code class="literal">uidCount</code> variable. In this example, reading <code class="literal">uidCount</code> to initialize <code class="literal">freshUid</code> and assigning <code class="literal">freshUid</code> back to <code class="literal">uniqueUid</code> do not necessarily happen together. We say that the two statements do not happen <span class="strong"><strong>atomically</strong></span>Â since the statements from the other threads can interleave arbitrarily. We next define a <code class="literal">printUniqueIds</code> method such that, given a number <code class="literal">n</code>, the method calls <code class="literal">getUniqueId</code> to produce <code class="literal">n</code> unique identifiers and then prints them. We use Scala for-comprehensions to map the range <code class="literal">0 until n</code> to unique identifiers. Finally, the main thread starts a new <code class="literal">t</code> thread that calls the <code class="literal">printUniqueIds</code> method, and then calls <code class="literal">printUniqueIds</code> concurrently with the <code class="literal">t</code> thread as follows:</p><pre class="programlisting">  def printUniqueIds(n: Int): Unit = {
    val uids = for (i&lt;- 0 until n) yield getUniqueId()
    log(s"Generated uids: $uids")
  }
  val t = thread { printUniqueIds(5) }
  printUniqueIds(5)
  t.join()
}
</pre><p>Running this application several times reveals that the identifiers generated by the two threads are not necessarily unique; the application prints <code class="literal">Vector(1, 2, 3, 4, 5)</code> and <code class="literal">Vector(1, 6, 7, 8, 9)</code> in some runs, but not in the others! The outputs of the program depend on the timing at which the statements in separate threads get executed.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note7"></a>Note</h3><p>A <span class="strong"><strong>race condition</strong></span> is a phenomenon in which the output of a concurrent program depends on the execution schedule of the statements in the program.</p></div><p>A race condition is not necessarily an incorrect program behavior. However, if some execution schedule causes an undesired program output, the race condition is considered to be a program error. The race condition from the previous example is a program error, because the <code class="literal">getUniqueId</code> method is not atomic. The <code class="literal">t</code> thread and the main thread sometimes concurrently calls <code class="literal">getUniqueId</code>. In the first line, they concurrently read the value of <code class="literal">uidCount</code>, which is initially <code class="literal">0</code>, and conclude that their own <code class="literal">freshUid</code> variable should be <code class="literal">1</code>. The <code class="literal">freshUid</code> variable is a local variable, so it is allocated on the thread stack; each thread sees a separate instance of that variable. At this point, the threads decide to write the value <code class="literal">1</code> back to <code class="literal">uidCount</code> in any order, and both return a non-unique identifier <code class="literal">1</code>. This is illustrated in the following figure:</p><div class="mediaobject"><img src="graphics/image_02_003.jpg" /></div><p>There is a mismatch between the mental model that most programmers inherit from sequential programming and the execution of the <code class="literal">getUniqueId</code> method when it is run concurrently. This mismatch is grounded in the assumption that <code class="literal">getUniqueId</code> executes atomically. Atomic execution of a block of code means that the individual statements in that block of code executed by one thread cannot interleave with those statements executed by another thread. In atomic execution, the statements can only be executed all at once, which is exactly how the <code class="literal">uidCount</code> field should be updated. The code inside the <code class="literal">getUniqueId</code> function reads, modifies, and writes a value, which is not atomic on the JVM. An additional language construct is necessary to guarantee atomicity. The fundamental Scala construct that allows this sort of atomic execution is called the <code class="literal">synchronized</code> statement, and it can be called on any object. This allows us to define <code class="literal">getUniqueId</code> as follows:</p><pre class="programlisting">def getUniqueId() = this.synchronized {
  val freshUid = uidCount + 1
  uidCount = freshUid
  freshUid
}
</pre><p>The <code class="literal">synchronized</code> call ensures that the subsequent block of code can only execute if there is no other thread simultaneously executing this synchronized block of code, or any other synchronized block of code called on the same <code class="literal">this</code> object. In our case, the <code class="literal">this</code> object is the enclosing singleton object, <code class="literal">ThreadsUnprotectedUid</code>, but in general, this can be an instance of the enclosing class or trait.</p><p>Two concurrent invocations of the <code class="literal">getUniqueId</code> method are shown in the following figure:</p><div class="mediaobject"><img src="graphics/image_02_004.jpg" /></div><p>We can also call <code class="literal">synchronized</code> and omit the <code class="literal">this</code> part, in which case the compiler will infer what the surrounding object is, but we strongly discourage you from doing so. Synchronizing on incorrect objects results in concurrency errors that are not easily identified.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip8"></a>Tip</h3><p>Always explicitly declare the receiver for the <code class="literal">synchronized</code> statement doing so protects you from subtle and hard to spot program errors.</p></div><p>The JVM ensures that the thread executing a <code class="literal">synchronized</code> statement invoked on some <code class="literal">x</code> object is the only thread executing any <code class="literal">synchronized</code> statement on that particular <code class="literal">x</code> object. If a <code class="literal">T</code> thread calls <code class="literal">synchronized</code> on <code class="literal">x</code>, and there is another <code class="literal">S</code> thread calling <code class="literal">synchronized</code> on <code class="literal">x</code>, then the <code class="literal">T</code> thread is put into the <span class="strong"><strong>blocked</strong></span> state. Once the <code class="literal">S</code> thread completes its <code class="literal">synchronized</code> statement, the JVM can choose the <code class="literal">T</code> thread to execute its own <code class="literal">synchronized</code> statement.</p><p>Every object created inside the JVM has a special entity called an <span class="strong"><strong>intrinsic lock</strong></span> or a <span class="strong"><strong>monitor</strong></span>, which is used to ensure that only one thread is executing some <code class="literal">synchronized</code> block on that object. When a thread starts executing the <code class="literal">synchronized</code> block, we say that the thread <span class="strong"><strong>gains ownership</strong></span> of the <code class="literal">x</code> monitor, or alternatively, <span class="strong"><strong>acquires</strong></span> it. When a thread completes the <code class="literal">synchronized</code> block, we say that it <span class="strong"><strong>releases</strong></span> the monitor.</p><p>The <code class="literal">synchronized</code> statement is one of the fundamental mechanisms for inter-thread communication in Scala and on the JVM. Whenever there is a possibility that multiple threads access and modify a field in some object, you should use the <code class="literal">synchronized</code> statement.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec17"></a>Reordering</h3></div></div></div><p>The <code class="literal">synchronized</code> statement is not without a price: writes to fields such as <code class="literal">uidCount</code>, which are protected by the <code class="literal">synchronized</code> statement are usually more expensive than regular unprotected writes. The performance penalty of the <code class="literal">synchronized</code> statement depends on the JVM implementation, but it is usually not large. You might be tempted to avoid using <code class="literal">synchronized</code> when you think that there is no bad interleaving of program statements, like the one we saw previously in the unique identifier example. Never do this! We will now show you a minimal example in which this leads to serious errors.</p><p>Let's consider the following program, in which two threads, <code class="literal">t1</code> and <code class="literal">t2</code>, access a pair of Boolean variables, <code class="literal">a</code> and <code class="literal">b</code>, and a pair of Integer variables, <code class="literal">x</code> and <code class="literal">y</code>. The <code class="literal">t1</code> thread sets the variable <code class="literal">a</code> to <code class="literal">true</code>, and then reads the value of <code class="literal">b</code>. If the value of <code class="literal">b</code> is <code class="literal">true</code>, the <code class="literal">t1</code> thread assigns <code class="literal">0</code> to <code class="literal">y</code>, and otherwise it assigns <code class="literal">1</code>. The <code class="literal">t2</code> thread does the opposite: it first assigns <code class="literal">true</code> to the variable <code class="literal">b</code>, and then assigns <code class="literal">0</code> to <code class="literal">x</code> if <code class="literal">a</code> is <code class="literal">true</code>, and <code class="literal">1</code> otherwise. This is repeated in a loop <code class="literal">100000</code> times, as shown in the following snippet:</p><pre class="programlisting">object ThreadSharedStateAccessReordering extends App {
  for (i &lt;- 0 until 100000) {
    var a = false
    var b = false
    var x = -1
    var y = -1
    val t1 = thread {
      a = true
      y = if (b) 0 else 1
    }
    val t2 = thread {
      b = true
      x = if (a) 0 else 1
    }
    t1.join()
    t2.join()
    assert(!(x == 1 &amp;&amp; y == 1), s"x = $x, y = $y")
  }
}
</pre><p>This program is somewhat subtle, so we need to carefully consider several possible execution scenarios. By analyzing the possible interleaving of the instructions of the <code class="literal">t1</code> and <code class="literal">t2</code> threads,Â we can conclude that if both threads simultaneously assign to <code class="literal">a</code> and <code class="literal">b</code>, then they will both assign <code class="literal">0</code> to <code class="literal">x</code> and <code class="literal">y</code>.</p><p>This outcome indicates that both the threads started at almost the same time, and is shown on the left in the following figure:</p><div class="mediaobject"><img src="graphics/image_02_005.jpg" /></div><p>Alternatively, let's assume that the <code class="literal">t2</code> thread executes faster. In this case, the <code class="literal">t2</code> thread sets the variable <code class="literal">b</code> to <code class="literal">true</code>, and proceeds to read the value of <code class="literal">a</code>. This happens before the assignment to <code class="literal">a</code> by the <code class="literal">t1</code> thread, so the <code class="literal">t2</code> thread reads the value <code class="literal">false</code>, and assigns <code class="literal">1</code> to <code class="literal">x</code>. When the <code class="literal">t1</code> thread executes, it sees that the value of <code class="literal">b</code> is <code class="literal">true</code>, and assigns <code class="literal">0</code> to <code class="literal">y</code>. This sequence of events is shown on the right in the preceding figure. Note that the case where the <code class="literal">t1</code> thread starts first results in a similar assignment where <code class="literal">x = 0</code> and <code class="literal">y = 1</code>, so it is not shown in the figure.</p><p>The conclusion is that regardless of how we reorder the execution of the statements in the <code class="literal">t1</code> and <code class="literal">t2</code> threads, the output of the program should never be such that <code class="literal">x = 1</code> and <code class="literal">y = 1</code> simultaneously. Thus, the assertion at the end of the loop never throws an exception.</p><p>However, after running the program several times, we get the following output, which indicates that both <code class="literal">x</code> and <code class="literal">y</code> can be assigned the value <code class="literal">1</code> simultaneously:</p><pre class="programlisting">
<span class="strong"><strong>[error] Exception in thread "main": assertion failed: x = 1, y = 1</strong></span>
</pre><p>This result is scary and seems to defy common sense. Why can't we reason about the execution of the program the way we did? The answer is that by the JMM specification, the JVM is allowed to reorder certain program statements executed by one thread as long as it does not change the serial semantics of the program for that particular thread. This is because some processors do not always execute instructions in the program order. Additionally, the threads do not need to write all their updates to the main memory immediately, but can temporarily keep them cached in registers inside the processor. This maximizes the efficiency of the program and allows better compiler optimizations.</p><p>How then should we reason about multithreaded programs? The error we made when analyzing the example is that we assumed that the writes by one thread are immediately visible to all the other threads. We always need to apply proper synchronization to ensure that the writes by one thread are visible to another thread.</p><p>The <code class="literal">synchronized</code> statement is one of the fundamental ways to achieve proper synchronization. Writes by any thread executing the <code class="literal">synchronized</code> statement on an <code class="literal">x</code> object are not only atomic but also visible to threads that execute <code class="literal">synchronized</code> on <code class="literal">x</code>. Enclosing each assignment in the <code class="literal">t1</code> and <code class="literal">t2</code> threads in a <code class="literal">synchronized</code> statement makes the program behave as expected.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip9"></a>Tip</h3><p>Use the <code class="literal">synchronized</code> statement on some object <code class="literal">x</code> when accessing (reading or modifying) a state shared between multiple threads. This ensures that at most, a single thread is at any time executing a <code class="literal">synchronized</code> statement on <code class="literal">x</code>. It also ensures that all the writes to the memory by the <code class="literal">T</code> thread are visible to all the other threads that subsequently execute <code class="literal">synchronized</code> on the same object <code class="literal">x</code>.</p></div><p>In the rest of this chapter and in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Traditional Building Blocks of Concurrency</em></span>, we will see additional synchronization mechanisms, such as volatile and atomic variables. In the next section, we will take a look at other use cases of the <code class="literal">synchronized</code> statement and learn about object monitors.</p></div></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec14"></a>Monitors and synchronization</h2></div></div><hr /></div><p>In this section, we will study inter-thread communication using the <code class="literal">synchronized</code> statement in more detail. As we saw in the previous sections, the <code class="literal">synchronized</code> statement serves both to ensure the visibility of writes performed by different threads, and to limit concurrent access to a shared region of memory. Generally speaking, a synchronization mechanism that enforces access limits on a shared resource is called a <span class="strong"><strong>lock</strong></span>. Locks are also used to ensure that no two threads execute the same code simultaneously; that is, they implement <span class="strong"><strong>mutual exclusion</strong></span>.</p><p>As mentioned previously, each object on the JVM has aÂ specialÂ built-in <span class="strong"><strong>monitor lock</strong></span>, alsoÂ called the <span class="strong"><strong>intrinsic lock</strong></span>. When a thread calls the <code class="literal">synchronized</code> statement on an <code class="literal">x</code> object, it gains ownership of the monitor lock of the <code class="literal">x</code> object, given that no other thread owns the monitor. Otherwise, the thread is blocked until the monitor is released. Upon gaining ownership of the monitor, the thread can witness the memory writes of all the threads that previously released that monitor.</p><p>A natural consequence is that <code class="literal">synchronized</code> statements can be nested. A thread can own monitors belonging to several different objects simultaneously. This is useful when composing larger systems from simpler components. We do not know which sets ofÂ monitors independent software components use in advance. Let's assume that we are designing an online banking system in which we want to log money transfers. We can maintain the transfers list of all the money transfers in a mutable <code class="literal">ArrayBuffer</code> growable array implementation. The banking application does not manipulate transfers directly, but instead appends new messages with a <code class="literal">logTransfer</code> method that calls <code class="literal">synchronized</code> on <code class="literal">transfers</code>. The <code class="literal">ArrayBuffer</code> implementation is a collection designed for single-threaded use, so we need to protect it from concurrent writes. We will start by defining the <code class="literal">logTransfer</code> method:</p><pre class="programlisting">object SynchronizedNesting extends App {
  import scala.collection._
  private val transfers = mutable.ArrayBuffer[String]()
  def logTransfer(name: String, n: Int) = transfers.synchronized {
    transfers += s"transfer to account '$name' = $n"
  }
</pre><p>Apart from the logging modules of the banking system, the accounts are represented with the <code class="literal">Account</code> class. The <code class="literal">Account</code> objects hold information about their owner and the amount of money with them. To add some money to an account, the system uses an <code class="literal">add</code> method that obtains the monitor of the <code class="literal">Account</code> object and modifies its <code class="literal">money</code> field. The bank's business process requires treating large transfers specially: if a money transfer is bigger than 10 currency units, we need to log it. In the following code, we will define the <code class="literal">Account</code> class and the <code class="literal">add</code> method, which adds an amount <code class="literal">n</code> to the <code class="literal">Account</code> object:</p><pre class="programlisting">  class Account(val name: String, var money: Int)
  def add(account: Account, n: Int) = account.synchronized {
    account.money += n
    if (n &gt; 10) logTransfer(account.name, n)
  }
</pre><p>The <code class="literal">add</code> method calls <code class="literal">logTransfer</code> from inside the <code class="literal">synchronized</code> statement, and <code class="literal">logTransfer</code> first obtains the <code class="literal">transfers</code> monitor. Importantly, this happens without releasing the <code class="literal">account</code> monitor. If the <code class="literal">transfers</code> monitor is currently acquired by some other thread, the current thread goes into the blocked state without releasing any of the monitors that it previously acquired.</p><p>In the following example, the main application creates two separate accounts and three threads that execute transfers. Once all the threads complete their transfers, the main thread outputs all the transfers that were logged:</p><pre class="programlisting">  // Continuation of the bank account example.
  val jane = new Account("Jane", 100)
  val john = new Account("John", 200)
  val t1 = thread { add(jane, 5) }
  val t2 = thread { add(john, 50) }
  val t3 = thread { add(jane, 70) }
  t1.join(); t2.join(); t3.join()
  log(s"--- transfers ---\n$transfers")
}
</pre><p>The use of the <code class="literal">synchronized</code> statement in this example prevents threads <code class="literal">t1</code> and <code class="literal">t3</code> from corrupting Jane's account by concurrently modifying it. The <code class="literal">t2</code> and <code class="literal">t3</code> threads also access the <code class="literal">transfers</code> log. This simple example shows why nesting is useful: we do not know which other components in our banking system potentially use the <code class="literal">transfers</code> log. To preserve encapsulation and prevent code duplication, independent software components should not explicitly synchronize to log a money transfer; synchronization is instead hidden in the <code class="literal">logTransfer</code> method.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec18"></a>Deadlocks</h3></div></div></div><p>A factor that worked to our advantage in the banking system example is that the <code class="literal">logTransfer</code> method never attempts to acquire any monitors other than the <code class="literal">transfers</code> monitor. Once the monitor is obtained, a thread will eventually modify the <code class="literal">transfers</code> buffer and release the monitor; in a stack of nested monitor acquisitions, <code class="literal">transfers</code> always comes last. Given that <code class="literal">logTransfer</code> is the only method synchronizing on <code class="literal">transfers</code>, it cannot indefinitely delay other threads that synchronize on <code class="literal">transfers</code>.</p><p>A <span class="strong"><strong>deadlock</strong></span> is aÂ general situation in which two or more executions wait for each other to complete an action before proceeding with their own action. The reason for waiting is that each of the executions obtains an exclusive access to a resource that the other execution needs to proceed. As an example from our daily life, assume that you are sitting in a cafeteria with your colleague and just about to start your lunch. However, there is only a single fork and a single knife at the table, and you need both the utensils to eat. You grab the fork, but your colleague grabs a knife. Both of you wait for the other to finish the meal, but do not let go of your own utensil. You are now in a state of deadlock, and you will never finish your lunch. Well, at least not until your boss arrives to see what's going on.</p><p>In concurrent programming, when two threads obtain two separate monitors at the same time and then attempt to acquire the other thread's monitor, a deadlock occurs. Both the threads go into a blocked state until the other monitor is released, but do not release the monitors they own.</p><p>The <code class="literal">logTransfer</code> method can never cause a deadlock, because it only attempts to acquire a single monitor that is released eventually. Let's now extend our banking example to allow money transfers between specific accounts, as follows:</p><pre class="programlisting">object SynchronizedDeadlock extends App {
  import SynchronizedNesting.Account
  def send(a: Account, b: Account, n: Int) = a.synchronized {
    b.synchronized {
      a.money -= n
      b.money += n
    }
  }
</pre><p>We import the <code class="literal">Account</code> class from the previous example. The <code class="literal">send</code> method atomically transfers a given amount of money <code class="literal">n</code> from an account <code class="literal">a</code> to another account <code class="literal">b</code>. To do so, it invokes the <code class="literal">synchronized</code> statement on both the accounts to ensure that no other thread is modifying either account concurrently, as shown in the following snippet:</p><pre class="programlisting">  val a = new Account("Jack", 1000)
  val b = new Account("Jill", 2000)
  val t1 = thread { for (i&lt;- 0 until 100) send(a, b, 1) }
  val t2 = thread { for (i&lt;- 0 until 100) send(b, a, 1) }
  t1.join(); t2.join()
  log(s"a = ${a.money}, b = ${b.money}")
}
</pre><p>Now, assume that two of our bank's new clients, Jack and Jill, just opened their accounts and are amazed with the new e-banking platform. They log in and start sending each other small amounts of money to test it, frantically hitting the send button a 100 times. Soon, something very bad happens. The <code class="literal">t1</code> and <code class="literal">t2</code> threads, which execute Jack's and Jill's requests, invoke <code class="literal">send</code> simultaneously with the order of accounts <code class="literal">a</code> and <code class="literal">b</code> reversed. Thread <code class="literal">t1</code> locks <code class="literal">a</code> and <code class="literal">t2</code> locks <code class="literal">b</code>, but are then both unable to lock the other account. To Jack's and Jill's surprise, the new transfer system is not as shiny as it seems. If you are running this example, you'll want to close the terminal session at this point and restart SBT.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note10"></a>Note</h3><p>A deadlock occurs when a set of two or more threads acquire resources and then cyclically try to acquire other thread's resources without releasing their own.</p></div><p>How do we prevent deadlocks from occurring? Recall that, in the initial banking system example, the order in which the monitors were acquired was well defined. A single account monitor was acquired first and the <code class="literal">transfers</code> monitor was possibly acquired afterward. You should convince yourself that whenever resources are acquired in the same order, there is no danger of a deadlock. When a thread <code class="literal">T</code> waits for a resource <code class="literal">X</code> acquired by some other thread S, the thread S will never try to acquire any resource <code class="literal">Y</code> already held by <code class="literal">T</code>, because <code class="literal">Y</code> &lt; <code class="literal">X</code> and <code class="literal">S</code> might only attempt to acquire resources <code class="literal">Y</code> &gt; <code class="literal">X</code>. The ordering breaks the cycle, which is one of the necessary preconditions for a deadlock.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip11"></a>Tip</h3><p>Establish a total order between resources when acquiring them; this ensures that no set of threads cyclically wait on the resources they previously acquired.</p></div><p>In our example, we need to establish an order between different accounts. One way of doing so is to use the <code class="literal">getUniqueId</code> method introduced in an earlier section:</p><pre class="programlisting">import SynchronizedProtectedUid.getUniqueId
class Account(val name: String, var money: Int) {
  val uid = getUniqueId()
}
</pre><p>The new <code class="literal">Account</code> class ensures that no two accounts share the same <code class="literal">uid</code> value, regardless of the thread they were created on. The deadlock-free <code class="literal">send</code> method then needs to acquire the accounts in the order of their <code class="literal">uid</code> values, as follows:</p><pre class="programlisting">def send(a1: Account, a2: Account, n: Int) {
  def adjust() {
    a1.money -= n
    a2.money += n
  }
  if (a1.uid &lt; a2.uid)
    a1.synchronized { a2.synchronized { adjust() } }
  else a2.synchronized { a1.synchronized { adjust() } }
}
</pre><p>After a quick response from the bank's software engineers, Jack and Jill happily send each other money again. A cyclic chain of blocked threads can no longer happen.</p><p>Deadlocks are inherent to any concurrent system in which the threads wait indefinitely for a resource without releasing the resources they previously acquired. However, while they should be avoided, deadlocks are often not as deadly as they sound. A nice thing about deadlocks is that by their definition, a deadlocked system does not progress. The developer that resolved Jack and Jill's issue was able to act quickly by doing a heap dump of the running JVM instance and analyzing the thread stacks; deadlocks can at least be easily identified, even when they occur in a production system. This is unlike the errors due to race conditions, which only become apparent long after the system transitions into an invalid state.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec19"></a>Guarded blocks</h3></div></div></div><p>Creating a new thread is much more expensive than creating a new lightweight object such as <code class="literal">Account</code>. A high-performance banking system should be quick and responsive, and creating a new thread on each request can be too slow when there are thousands of requests per second. The same thread should be reused for many requests; a set of such reusable threads is usually called a <span class="strong"><strong>thread pool</strong></span>.</p><p>In the following example, we will define a special thread called <code class="literal">worker</code> that will execute a block of code when some other thread requests it. We will use the mutable <code class="literal">Queue</code> class from the Scala standard library collections package to store the scheduled blocks of code:</p><pre class="programlisting">import scala.collection._
object SynchronizedBadPool extends App {
  private val tasks = mutable.Queue[() =&gt; Unit]()
</pre><p>We represent the blocks of code with the <code class="literal">() =&gt; Unit</code> function type. The <code class="literal">worker</code> thread will repetitively call the <code class="literal">poll</code> method that synchronizes on <code class="literal">tasks</code> to check whether the queue is non-empty. The <code class="literal">poll</code> method shows that the <code class="literal">synchronized</code> statement can return a value. In this case, it returns an optional <code class="literal">Some</code> value if there are tasks to do, or <code class="literal">None</code> otherwise. The <code class="literal">Some</code> object contains the block of code to execute:</p><pre class="programlisting">  val worker = new Thread {
    def poll(): Option[() =&gt; Unit] = tasks.synchronized {
      if (tasks.nonEmpty) Some(tasks.dequeue()) else None
    }
    override def run() = while (true) poll() match {
      case Some(task) =&gt; task()
      case None =&gt;
    }
  }
  worker.setName("Worker")
  worker.setDaemon(true)
  worker.start()
</pre><p>We set the <code class="literal">worker</code> thread to be a <span class="strong"><strong>daemon</strong></span> thread before starting it. Generally, a JVM process does not stop when the main thread terminates. The JVM process terminates when all non-daemon threads terminate. We want <code class="literal">worker</code> to be a daemon thread because we send work to it using the <code class="literal">asynchronous</code> method, which schedules a given block of code to eventually execute the <code class="literal">worker</code> thread:</p><pre class="programlisting">  def asynchronous(body: =&gt;Unit) = tasks.synchronized {
    tasks.enqueue(() =&gt; body)
  }
  asynchronous { log("Hello") }
  asynchronous { log(" world!")}
  Thread.sleep(5000)
}
</pre><p>Run the preceding example and witness the <code class="literal">worker</code> thread print <code class="literal">Hello</code> and then<code class="literal"> world!</code>. Now listen to your laptop. The fan should start humming by now. Turn on your <span class="strong"><strong>Task Manager</strong></span> or simply type <code class="literal">top</code> into your terminal if you are running this on a Unix system. One of your CPUs is completely used up by a process called <code class="literal">java</code>. You can guess the reason. After <code class="literal">worker</code> completes its work, it isÂ constantlyÂ checking if there are any tasks on the queue. We say that the <code class="literal">worker</code> thread is <span class="strong"><strong>busy-waiting</strong></span>. Busy-waiting is undesired, because it needlessly uses processor power. Still, shouldn't a daemon thread be stopped once the main thread terminates? In general, yes, but we are running this example from SBT in the same JVM process that SBT itself is running. SBT has non-daemon threads of its own, so our <code class="literal">worker</code> thread is not stopped. To tell SBT that it should execute the <code class="literal">run</code> command in a new process, enter the following directive:</p><pre class="programlisting">set fork := true
</pre><p>Running the preceding example again should stop the <code class="literal">worker</code> thread as soon as the main thread completes its execution. Still, our busy-waiting <code class="literal">worker</code> thread can be a part of a larger application that does not terminate so quickly. Creating new threads all the time might be expensive, but a busy-waiting thread is even more expensive. Several such threads can quickly compromise system performance. There are only a handful of applications in which busy-waiting makes sense. If you still have doubts that this is dangerous, start this example on your laptop while running on battery power and go grab a snack. Make sure that you save any open files before you do this; you might lose data once the CPU drains all the battery power.</p><p>What we would really like the <code class="literal">worker</code> thread to do is to go to the waiting state, similar to what a thread does when we call <code class="literal">join</code>. It should only wake up after we ensure that there are additional function objects to execute on the <code class="literal">tasks</code> queue.</p><p>Scala objects (and JVM objects in general) support a pair of special methods called <code class="literal">wait</code> and <code class="literal">notify</code>, which allow waiting and awakening the waiting threads, respectively. It is only legal to call these methods on an <code class="literal">x</code> object if the current thread owns the monitor of the object <code class="literal">x</code>. In other words, <code class="literal">wait</code> and <code class="literal">notify</code> can only be called from a thread that owns the monitor of that object. When a thread <code class="literal">T</code> calls <code class="literal">wait</code> on an object, it releases the monitor and goes into the waiting state until some other thread <code class="literal">S</code> calls <code class="literal">notify</code> on the same object. The thread <code class="literal">S</code> usually prepares some data for <code class="literal">T</code>, as in the following example in which the main thread sets the <code class="literal">Some</code> message for the <code class="literal">greeter</code> thread to print:</p><pre class="programlisting">object SynchronizedGuardedBlocks extends App {
  val lock = new AnyRef
  var message: Option[String] = None
  val greeter = thread {
    lock.synchronized {
      while (message == None) lock.wait()
      log(message.get)
    }
  }
  lock.synchronized {
    message = Some("Hello!")
    lock.notify()
  }
  greeter.join()
}
</pre><p>The threads use the monitor from a fresh <code class="literal">lock</code> object of an <code class="literal">AnyRef</code> type that maps into the <code class="literal">java.lang.Object</code> class. The <code class="literal">greeter</code> thread starts by acquiring the lock's monitor and checks whether the message is set to <code class="literal">None</code>. If it is, there is nothing to output as yet and the <code class="literal">greeter</code> thread calls <code class="literal">wait</code> on <code class="literal">lock</code>. Upon calling <code class="literal">wait</code>, the <code class="literal">lock</code> monitor is released and the main thread, which was previously blocked at its <code class="literal">synchronized</code> statement, now obtains the ownership of the <code class="literal">lock</code> monitor, sets the message, and calls <code class="literal">notify</code>. When the main thread leaves the <code class="literal">synchronized</code> block, it releases <code class="literal">lock</code>. This causes <code class="literal">greeter</code> to wake up, acquire <code class="literal">lock</code>, check whether there is a message again, and then output it. Since <code class="literal">greeter</code> acquires the same monitor that the main thread previously released, the write to <code class="literal">message</code> by the main thread occurs before the check by the <code class="literal">greeter</code> thread. We now know that the <code class="literal">greeter</code> thread will see the message. In this example, the <code class="literal">greeter</code> thread will output <code class="literal">Hello!</code> regardless of which thread runs <code class="literal">synchronized</code> first.</p><p>An important property of the <code class="literal">wait</code> method isÂ that it can cause <span class="strong"><strong>spurious wakeups</strong></span>. Occasionally, the JVM is allowed to wake up a thread that called <code class="literal">wait</code> even though there is no corresponding <code class="literal">notify</code> call. To guard against this, we must always use <code class="literal">wait</code> in conjunction with a <code class="literal">while</code> loop that checks the condition, as in the previous example. Using an <code class="literal">if</code> statement would be incorrect, as a spurious wakeup could allow the thread to execute <code class="literal">message.get</code>, even though <code class="literal">message</code> was not set to a value different than <code class="literal">None</code>.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note12"></a>Note</h3><p>After the thread that checks the condition wakes up, the monitor becomes owned by that thread, so we are guaranteed that the check is performed atomically.
Note that a thread that checks the condition must acquire the monitor to wake up. If it cannot acquire the monitor immediately, it goes into the blocked state.</p></div><p>A <code class="literal">synchronized</code> statement in which some condition is repetitively checked before calling <code class="literal">wait</code> is called a <span class="strong"><strong>guarded block</strong></span>. We can now use our insight on guarded blocks to avoid the busy-wait in our <code class="literal">worker</code> thread in advance. We will now show the complete <code class="literal">worker</code> implementation using monitors:</p><pre class="programlisting">object SynchronizedPool extends App {
  private val tasks = mutable.Queue[() =&gt; Unit]()
  object Worker extends Thread {
    setDaemon(true)
    def poll() = tasks.synchronized {
      while (tasks.isEmpty) tasks.wait()
      tasks.dequeue()
    }
    override def run() = while (true) {
      val task = poll()
      task()
    }
  }
  Worker.start()
  def asynchronous(body: =&gt;Unit) = tasks.synchronized {
    tasks.enqueue(() =&gt; body)
    tasks.notify()
  }
  asynchronous { log("Hello ") }
  asynchronous { log("World!") }
  Thread.sleep(500)
}
</pre><p>In this example, we declared the <code class="literal">Worker</code> thread as a singleton object within our application to be more concise. This time, the <code class="literal">poll</code> method calls <code class="literal">wait</code> on the <code class="literal">tasks</code> object and waits until the main thread adds a code block to <code class="literal">tasks</code> and calls <code class="literal">notify</code> in the <code class="literal">asynchronous</code> method. Start the example and inspect your CPU usage again. If you restarted SBT (and still have battery power) since running the busy-wait example, you will see that the CPU usage by the <code class="literal">java</code> process is zero.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec20"></a>Interrupting threads and the graceful shutdown</h3></div></div></div><p>In the previous example, the <code class="literal">Worker</code> thread loops forever in its <code class="literal">run</code> method and never terminates. You might be satisfied with this; <code class="literal">Worker</code> does not use the CPU if it has no work to do, and since <code class="literal">Worker</code> is a daemon thread, it is destroyed when the application exits. However, its stack space is not reclaimed until the application terminates. If we have a lot of dormant workers lying around, we might run out of memory. One way to stop a dormant thread from executing is to interrupt it, as follows:</p><pre class="programlisting">Worker.interrupt()
</pre><p>Calling the <code class="literal">interrupt</code> method on a thread that is in the waiting or timed waiting state causes it to throw an <code class="literal">InterruptedException</code>. This exception can be caught and handled, but in our case, it will terminate the <code class="literal">Worker</code> thread. However, if we call this method while the thread is running, the exception is not thrown and the thread's <code class="literal">interrupt</code> flag is set. A thread that does not block must periodically query the interrupt flag with the <code class="literal">isInterrupted</code> method.</p><p>An alternative is to implement an idiom known as the <span class="strong"><strong>graceful shutdown</strong></span>. In the graceful shutdown, one thread sets the condition for the termination and then calls <code class="literal">notify</code> to wake up a worker thread. The worker thread then releases all its resources and terminates willingly. We first introduce a variable called <code class="literal">terminated</code> that is <code class="literal">true</code> if the thread should be stopped. The <code class="literal">poll</code> method additionally checks this variable before waiting on <code class="literal">tasks</code> and optionally returns a task only if the <code class="literal">Worker</code> thread should continue to run, as shown in the following code:</p><pre class="programlisting">object Worker extends Thread {
  var terminated = false
  def poll(): Option[() =&gt; Unit] = tasks.synchronized {
    while (tasks.isEmpty &amp;&amp; !terminated) tasks.wait()
    if (!terminated) Some(tasks.dequeue()) else None
  }
</pre><p>We change the <code class="literal">run</code> method to check if <code class="literal">poll</code> returns <code class="literal">Some(task)</code> in a pattern match. We no longer use a <code class="literal">while</code> loop in the <code class="literal">run</code> method. Instead, we call <code class="literal">run</code> tail-recursively if <code class="literal">poll</code> returned <code class="literal">Some(task)</code>:</p><pre class="programlisting">  import scala.annotation.tailrec
  @tailrec override def run() = poll() match {
    case Some(task) =&gt; task(); run()
    case None =&gt;
  }
  def shutdown() = tasks.synchronized {
    terminated = true
    tasks.notify()
  }
}
</pre><p>The main thread can now call the synchronized <code class="literal">shutdown</code> method on the <code class="literal">Worker</code> thread to communicate with the termination request. There is no need to make the <code class="literal">Worker</code> thread a daemon thread anymore. Eventually, the <code class="literal">Worker</code> thread will terminate on its own.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip13"></a>Tip</h3><p>To ensure that various utility threads terminate correctly without race conditions, use the graceful shutdown idiom.</p></div><p>The situation where calling <code class="literal">interrupt</code> is preferred to a graceful shutdown is when we cannot wake the thread using <code class="literal">notify</code>. One example is when the thread does blocking I/O on an <code class="literal">InterruptibleChannel</code> object, in which case the object the thread is calling the <code class="literal">wait</code> method on is hidden.</p><p>The <code class="literal">Thread</code> class also defines a deprecated <code class="literal">stop</code> method that immediately terminates a thread by throwing a <code class="literal">ThreadDeath</code> exception. You should avoid it as it stops the thread's execution at an arbitrary point, possibly leaving the program data in an inconsistent state.</p></div></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec15"></a>Volatile variables</h2></div></div><hr /></div><p>The JVM offers a more lightweight form of synchronization than the <code class="literal">synchronized</code> block, called <span class="strong"><strong>volatile variables</strong></span>. Volatile variables can be atomically read and modified, and are mostly used as status flags; for example, to signal that a computation is completed or canceled. They have two advantages. First, writes to and reads from volatile variables cannot be reordered in a single thread. Second, writing to a volatile variable is immediately visible to all the other threads.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note14"></a>Note</h3><p>Reads and writes to variables marked as volatile are never reordered. If a write <code class="literal">W</code> to a volatile <code class="literal">v</code> variable is observed on another thread through a read <code class="literal">R</code> of the same variable, then all the writes that preceded the write <code class="literal">W</code> are guaranteed to be observed after the read <code class="literal">R</code>.</p></div><p>In the following example, we search for at least one <code class="literal">!</code> character in several pages of the text. Separate threads start scanning separate pages <code class="literal">p</code> of the text written by a person that is particularly fond of a popular fictional hero. As soon as one thread finds the exclamation, we want to stop searching in other threads:</p><pre class="programlisting">class Page(val txt: String, var position: Int)
object Volatile extends App {
  val pages = for (i&lt;- 1 to 5) yield
    new Page("Na" * (100 - 20 * i) + " Batman!", -1)
  @volatile var found = false
  for (p &lt;- pages) yield thread {
    var i = 0
    while (i &lt; p.txt.length &amp;&amp; !found)
      if (p.txt(i) == '!') {
        p.position = i
        found = true
      } else i += 1
  }
  while (!found) {}
  log(s"results: ${pages.map(_.position)}")
}
</pre><p>Separate pages of text are represented by the <code class="literal">Page</code> class, which has a special <code class="literal">position</code> field for storing the result of the exclamation mark search. The <code class="literal">found</code> flag denotes that some thread has found an exclamation. We add the <code class="literal">@volatile</code> annotation to the <code class="literal">found</code> flag to declare it volatile. When some thread finds an exclamation character in some page, the <code class="literal">position</code> value is stored and the <code class="literal">found</code> flag is set so that the other threads can stop their search early. It is entirely possible that all the threads end up scanning the entire text, but more likely that they see that <code class="literal">found</code> is <code class="literal">true</code> before that. Thus, at least one thread stores the exclamation position.</p><p>For the purposes of this example, the main thread busy-waits until it reads <code class="literal">found</code>, which is <code class="literal">true</code>. It then prints the positions. Note that a write to <code class="literal">position</code> occurs before the write to <code class="literal">found</code> in the spawned threads, which in turn occurs before reading <code class="literal">found</code> in the main thread. This means that the main thread always sees the write of the thread that set <code class="literal">found</code>, and hence prints at least one position other than <code class="literal">-1</code>.</p><p>The <code class="literal">ThreadSharedStateAccessReordering</code> example from an earlier section can be fixed by declaring all the variables as volatile. As we will learn in the next section, this ensures a correct order between reads from and writes to <code class="literal">a</code> and <code class="literal">b</code>. Unlike Java, Scala allows you to declare local fields volatile (in this case, local to the closure of the enclosing <code class="literal">for</code> loop). A heap object with a volatile field is created for each local volatile variable used in some closure or a nested class. We say the variable is <span class="strong"><strong>lifted</strong></span> into an object.</p><p>A volatile read is usually extremely cheap. In most cases, however, you should resort to the <code class="literal">synchronized</code> statements; volatile semantics are subtle and easy to get wrong. In particular, multiple volatile reads and writes are not atomic without additional synchronization; volatiles alone cannot help us to implement <code class="literal">getUniqueId</code> correctly.</p></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec16"></a>The Java Memory Model</h2></div></div><hr /></div><p>While we were never explicit about it throughout this chapter, we have actually defined most of the JMM. What is a memory model in the first place?</p><p>A language memory model is a specification that describes the circumstances under which a write to a variable becomes visible to other threads. You might think that a write to a variable <code class="literal">v</code> changes the corresponding memory location immediately after the processor executes it, and that other processors see the new value of <code class="literal">v</code> instantaneously. This memory consistency model is called <span class="strong"><strong>sequential consistency</strong></span>.</p><p>As we already saw in the <code class="literal">ThreadSharedStateAccessReordering</code> example, sequential consistency has little to do with how processors and compilers really work. Writes rarely end up in the main memory immediately; instead, processors have hierarchies of caches that ensure a better performanceÂ and guarantee that the data is only eventually written to the main memory. Compilers are allowed to use registers to postpone or avoid memory writes, and reorder statements to achieve optimal performance, as long as it does not change the serial semantics. It makes sense to do so; while the short examples in this book are interspersed with synchronization primitives, in actual programs, different threads communicate relatively rarely compared to the amount of time spent doing useful work.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note15"></a>Note</h3><p>A memory model is a trade-off between the predictable behavior of a concurrent program and a compiler's ability to perform optimizations. Not every language or platform has a memory model. A typical purely functional programming language, which doesn't support mutations, does not need a memory model at all.</p></div><p>Differences between processor architectures result in different memory models; it would be very difficult, if not impossible, to correctly write a Scala program that works in the same way on every computer without the precise semantics of the <code class="literal">synchronized</code> statement or volatile reads and writes. Scala inherits its memory model from the JVM, which precisely specifies a set of <span class="strong"><strong>happens-before</strong></span> relationships between different actions in a program.</p><p>In the JMM, the different actions are (volatile) variable reads and writes, acquiring and releasing object monitors, starting threads, and waiting for their termination. If an action A happens before an action B, then the action B sees A's memory writes. The same set of happens-before relationships is valid for the same program irrespective of the machine it runs on; it is the JVM's task to ensure this. We already summarized most of these rules, but we will now present a complete overview:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><span class="strong"><strong>Program order</strong></span>: Each action in a thread happens-before every other subsequent action in the program order of that thread.</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Monitor locking</strong></span>: Unlocking a monitor happens-before every subsequent locking of that monitor.</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Volatile fields</strong></span>: A write to a volatile field happens-before every subsequent read of that volatile field.</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Thread start</strong></span>: A call to <code class="literal">start()</code> on a thread happens-before any actions in the started thread.</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Thread termination</strong></span>: Any action in a thread happens-before another thread completes a <code class="literal">join()</code> call on that thread.</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Transitivity</strong></span>: If action A happens-before action B, and action B happens-before action C, then action A happens-before action C.</p></li></ul></div><p>Despite its somewhat misleading name, the happens-before relationship exists to ensure that threads see each other's memory writes. It does not exist to establish a temporal ordering between different statements in the program. When we say that a write A happens before a read B, it is guaranteed that the effects of the write A are visible to that particular read B. Whether or not the write A occurs earlier than the read B depends on the execution of the program.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note16"></a>Note</h3><p>The happens-before relationship describes the visibility of the writes performed by a different thread.</p></div><p>Additionally, the JMM guarantees that volatile reads and writes as well as monitor locks and unlocks are never reordered. The happens-before relationship ensures that nonvolatile reads and writes also cannot be reordered arbitrarily. In particular, this relationship ensures the following things:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>A non-volatile read cannot be reordered to appear before a volatile read (or monitor lock) that precedes it in the program order</p></li><li style="list-style-type: disc"><p>A non-volatile write cannot be reordered to appear after a volatile write (or monitor unlock) that follows it in the program order</p></li></ul></div><p>Higher-level constructs often establish a happens-before relationship on top of these rules. For example, an <code class="literal">interrupt</code> call happens before the interrupted thread detects it; this is because the <code class="literal">interrupt</code> call uses a monitor to wake the thread in a typical implementation. Scala concurrency APIs described in the later chapters also establish happens-before relationships between various method calls, as we will see. In all these cases, it is the task of the programmer to ensure that every write of a variable is in a happens-before relationship with every read of that variable that should read the written value. A program in which this is not true is said to contain <span class="strong"><strong>data races</strong></span>.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec21"></a>Immutable objects and final fields</h3></div></div></div><p>We have said that programs must establish happens-before relationships to avoid data races, but there is an exception to this rule. If an object contains only <span class="strong"><strong>final fields</strong></span> and the reference to the enclosing object does not become visible to another thread before the constructor completes, then the object is considered immutable and can be shared between the threads without any synchronization.</p><p>In Java, a final field is marked with the <code class="literal">final</code> keyword. In Scala, declaring an object field as <code class="literal">final</code> means that the getter for that field cannot be overridden in a subclass. The field itself is always final provided that it is a value declaration, that is, a <code class="literal">val</code> declaration. The following class depicts this:</p><pre class="programlisting">class Foo(final val a: Int, val b: Int)
</pre><p>The preceding class corresponds to the following Java class after the Scala compiler translates it:</p><pre class="programlisting">class Foo { // Java code below
  final private int a$;
  final private int b$;
  final public int a() { return a$; }
  public int b() { return b$; }
  public Foo(int a, int b) {
    a$ = a;
    b$ = b;
  }
}
</pre><p>Note that both the fields become final at the JVM level and can be shared without synchronization. The difference is that the getter for <code class="literal">a</code> cannot be overridden in a <code class="literal">Foo</code> subclass. We have to disambiguate finality in the reassignment and overriding sense.</p><p>Since Scala is a hybrid between functional and object-oriented paradigms, many of its language features map to immutable objects. A lambda value can capture a reference to the enclosing class or a lifted variable, as in the following example:</p><pre class="programlisting">var inc: () =&gt; Unit = null
val t = thread { if (inc != null) inc() }
private var number = 1
inc = () =&gt; { number += 1 }
</pre><p>The local <code class="literal">number</code> variable is captured by the lambda, so it needs to be lifted. The statement in the last line translates to an anonymous <code class="literal">Function0</code> class instantiation:</p><pre class="programlisting">number = new IntRef(1) // captured local variables become objects
inc = new Function0 {
  val $number = number // recall - vals are final!
  def apply() = $number.elem += 1
}
</pre><p>There is no happens-before relationship between the assignment to <code class="literal">inc</code> and the read of <code class="literal">inc</code> by the thread <code class="literal">t</code>. However, if the <code class="literal">t</code> thread sees that <code class="literal">inc</code> is not <code class="literal">null</code>, invoking <code class="literal">inc</code> still works correctly, because the <code class="literal">$number</code> field is appropriately initialized since it is stored as a field in the immutable lambda object. The Scala compiler ensures that lambda values contain only final, properly initialized fields. Anonymous classes, auto-boxed primitives, and value classes share the same philosophy.</p><p>In current versions of Scala, however, certain collections that are deemed immutable, such as <code class="literal">List</code> and <code class="literal">Vector</code>, cannot be shared without synchronization. Although their external API does not allow you to modify them, they contain non-final fields.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip17"></a>Tip</h3><p>Even if an object seems immutable, always use proper synchronization to share any object between the threads.</p></div></div></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec17"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we showed how to create and start threads, and wait for their termination. We have shown how to achieve inter-thread communication by modifying the shared memory and by using the <code class="literal">synchronized</code> statement, and what it means for a thread to be in a blocked state. We have studied approaches to prevent deadlocks by imposing theÂ ordering on the locks and avoided busy-waits in place of guarded blocks. We have seen how to implement a graceful shutdown for thread termination and when to communicate using volatiles. We witnessed how the correctness of a program can be compromised by undesired interactions known as race conditions as well as data races due to the lack of synchronization. And, most importantly, we have learned that the only way to correctly reason about the semantics of a multithreaded program is in terms of happens-before relationships defined by the JMM.</p><p>The language primitives and APIs presented in this section are low-level; they are the basic building blocks for concurrency on the JVM and in Scala, and there are only a handful of situations where you should use them directly. One of them is designing a new concurrency library yourself; another one is dealing with a legacy API built directly from these primitives. Although you should strive to build concurrent Scala applications in terms of concurrency frameworks introduced in the later chapters, the insights from this chapter will be helpful in understanding how higher-level constructs work. You should now have a valuable insight of what's going on under the hood.</p><p>If you would like to learn more about concurrency on the JVM and the JMM, we recommend that you read the book <span class="emphasis"><em>Java Concurrency in Practice</em></span>, <span class="emphasis"><em>Brian Goetz, Tim Peierls, Joshua Bloch, Joseph Bowbeer, David Holmes, and Doug Lea</em></span>, <span class="emphasis"><em>Addison-Wesley</em></span>. For an in-depth understanding of processes, threads, and the internals of operating systems, we recommend the book <span class="emphasis"><em>Operating System Concepts</em></span>, <span class="emphasis"><em>Abraham Silberschatz, Peter B. Galvin, and Greg Gagne</em></span>, <span class="emphasis"><em>Wiley</em></span>.</p><p>In the next chapter, we will cover more advanced building blocks of concurrent programs. We will learn how to use executors to avoid creating threads directly, concurrent collections for thread-safe data access, and atomic variables for deadlock-free synchronization. These high-level abstractions will alleviate many of the problems inherent to the fundamental concurrency primitives presented in this chapter.</p></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec18"></a>Exercises</h2></div></div><hr /></div><p>In the following set of exercises, you are required to implement higher-level concurrency abstractions in terms of basic JVM concurrency primitives. Some of these exercises introduce concurrent counterparts of sequential programming abstractions, and, in doing so, highlight important differences between sequential and concurrent programming. The exercises are not ordered in any particular order, but some of them rely on specific content from earlier exercises or this chapter:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Implement a <code class="literal">parallel</code> method, which takes two computation blocks, <code class="literal">a</code> and <code class="literal">b</code>, and starts each of them in a new thread. The method must return a tuple with the result values of both the computations. It should have the following signature:
</p><pre class="programlisting">            def parallel[A, B](a: =&gt;A, b: =&gt;B): (A, B)
</pre></li><li><p>Implement a <code class="literal">periodically</code> method, which takes a time interval <code class="literal">duration</code> specified in milliseconds, and a computation block <code class="literal">b</code>. The method starts a thread that executes the computation block <code class="literal">b</code> every <code class="literal">duration</code> milliseconds. It should have the following signature:
</p><pre class="programlisting">            def periodically(duration: Long)(b: =&gt;Unit): Unit
</pre></li><li><p>Implement a <code class="literal">SyncVar</code> class with the following interface:
</p><pre class="programlisting">                class SyncVar[T] {
                  def get(): T = ???
                  def put(x: T): Unit = ???
                }
</pre><p>
</p><p>A <code class="literal">SyncVar</code> object is used to exchange values between two or more threads. When created, the <code class="literal">SyncVar</code> object is empty:</p><p>
</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Calling <code class="literal">get</code> throws an exception</p></li><li style="list-style-type: disc"><p>Calling <code class="literal">put</code> adds a value to the <code class="literal">SyncVar</code> object</p></li></ul></div><p>
</p><p>After a value is added to a <code class="literal">SyncVar</code> object, we say that it is non-empty:</p><p>
</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Calling <code class="literal">get</code> returns the current value, and changes the state to empty</p></li><li style="list-style-type: disc"><p>Calling <code class="literal">put</code> throws an exception</p></li></ul></div></li><li><p>The <code class="literal">SyncVar</code> object from the previous exercise can be cumbersome to use, due to exceptions when the <code class="literal">SyncVar</code> object is in an invalid state. Implement a pair of methods, <code class="literal">isEmpty</code> and <code class="literal">nonEmpty</code>, on the <code class="literal">SyncVar</code> object. Then, implement a producer thread that transfers a range of numbers <code class="literal">0 until 15</code> to the consumer thread that prints them.</p></li><li><p>Using the <code class="literal">isEmpty</code> and <code class="literal">nonEmpty</code> pair of methods from the previous exercise requires busy-waiting. Add the following methods to the <code class="literal">SyncVar</code> class:
</p><pre class="programlisting">            def getWait(): T
            def putWait(x: T): Unit
</pre><p>
</p><p>These methods have similar semantics as before, but go into the waiting state instead of throwing an exception, and return once the <code class="literal">SyncVar</code> object is empty or non-empty, respectively.</p></li><li><p>A <code class="literal">SyncVar</code> object can hold at most one value at a time. Implement a <code class="literal">SyncQueue</code> class, which has the same interface as the <code class="literal">SyncVar</code> class, but can hold at most <code class="literal">n</code> values. The <code class="literal">n</code> parameter is specified in the constructor of the <code class="literal">SyncQueue</code> class.</p></li><li><p>The <code class="literal">send</code> method in the <span class="emphasis"><em>Deadlocks</em></span> section was used to transfer money between the two accounts. The <code class="literal">sendAll</code> method takes a set <code class="literal">accounts</code> of bank accounts and a <code class="literal">target</code> bank account, and transfers all the money from every account in <code class="literal">accounts</code> to the <code class="literal">target</code> bank account. The <code class="literal">sendAll</code> method has the following signature:
</p><pre class="programlisting">            def sendAll(accounts: Set[Account], target: Account): Unit
</pre><p>
</p><p>Implement the <code class="literal">sendAll</code> method and ensure that a deadlock cannot occur.</p></li><li><p>Recall the <code class="literal">asynchronous</code> method from the <span class="emphasis"><em>Guarded blocks</em></span> section. This method stores the tasks in a <span class="strong"><strong>First In First Out</strong></span> (<span class="strong"><strong>FIFO</strong></span>) queue; before a submitted task is executed, all the previously submitted tasks need to be executed. In some cases, we want to assign priorities to tasks so that a high-priority task can execute as soon as it is submitted to the task pool. Implement a <code class="literal">PriorityTaskPool</code> class that has the <code class="literal">asynchronous</code> method with the following signature:
</p><pre class="programlisting">            def asynchronous(priority: Int)(task: =&gt;Unit): Unit
</pre><p>
</p><p>A single worker thread picks tasks submitted to the pool and executes them. Whenever the worker thread picks a new task from the pool for execution, that task must have the highest priority in the pool.</p></li><li><p>Extend the <code class="literal">PriorityTaskPool</code> class from the previous exercise so that it supports any number of worker threads <code class="literal">p</code>. The parameter <code class="literal">p</code> is specified in the constructor of the <code class="literal">PriorityTaskPool</code> class.</p></li><li><p>Extend the <code class="literal">PriorityTaskPool</code> class from the previous exercise so that it supports the <code class="literal">shutdown</code> method:
</p><pre class="programlisting">            def shutdown(): Unit
</pre><p>
</p><p>When the <code class="literal">shutdown</code> method is called, all the tasks with the priorities greater than <code class="literal">important</code> must be completed, and the rest of the tasks must be discarded. The <code class="literal">important</code> integer parameter is specified in the constructor of the <code class="literal">PriorityTaskPool</code> class.</p></li><li><p>Implement a <code class="literal">ConcurrentBiMap</code> collection, which is a concurrent bidirectional map. The invariant is that every key is mapped to exactly one value, and vice versa. Operations must be atomic. The concurrent bidirectional map has the following interface:
</p><pre class="programlisting">            class ConcurrentBiMap[K, V] {
              def put(k: K, v: V): Option[(K, V)
              def removeKey(k: K): Option[V]
              def removeValue(v: V): Option[K]
              def getValue(k: K): Option[V]
              def getKey(v: V): Option[K]
              def size: Int
              def iterator: Iterator[(K, V)]
            }
</pre><p>
</p><p>Make sure that your implementation prevents deadlocks from occurring in the map.</p></li><li><p>Add a <code class="literal">replace</code> method to the concurrent bidirectional map from the previous exercise. The method should atomically replace a key-value pair with another key-value pair:
</p><pre class="programlisting">            def replace(k1: K, v1: V, k2: K, v2: V): Unit
</pre></li><li><p>Test the implementation of the concurrent bidirectional map from the earlier exercise by creating a test in which several threads concurrently insert millions of key-value pairs into the map. When all of them complete, another batch of threads must concurrently invert the entries in the map - for any key-value pair <code class="literal">(k1, k2)</code>, the thread should replace it with a key-value pair <code class="literal">(k2, k1)</code>.</p></li><li><p>Implement a <code class="literal">cache</code> method, which converts any function into a memoized version of itself. The first time that the resulting function is called for any argument, it is called in the same way as the original function. However, the result is memoized, and subsequently invoking the resulting function with the same arguments must return the previously returned value:
</p><pre class="programlisting">            def cache[K, V](f: K =&gt; V): K =&gt; V
</pre></li></ol></div><p>Make sure that your implementation works correctly when the resulting function is called simultaneously from multiple threads.</p></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch03"></a>ChapterÂ 3.Â Traditional Building Blocks of Concurrency</h2></div></div></div><div class="blockquote"><table border="0" width="100%" cellspacing="0" cellpadding="0" class="blockquote" summary="Block quote"><tr><td width="10%" valign="top">Â </td><td width="80%" valign="top"><p><span class="emphasis"><em>"There's an old story about the person who wished his computer were as easy to use as his telephone. That wish has come true, since I no longer know how to use my telephone."</em></span></p></td><td width="10%" valign="top">Â </td></tr><tr><td width="10%" valign="top">Â </td><td colspan="2" align="right" valign="top">--<span class="attribution"><span class="emphasis"><em>Bjarne Stroustrup</em></span></span></td></tr></table></div><p>The concurrency primitives shown in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Concurrency on the JVM and the Java Memory Model</em></span>, are the basics of concurrent programming on JVM. Nevertheless, we usually avoid using them directly, as their low-level nature makes them delicate and prone to errors. As we saw, low-level concurrency is susceptible to effects such as data races, reordering, visibility, deadlocks, and non-determinism. Fortunately, people have come up with more advanced building blocks of concurrency, that capture common patterns in concurrent programs and are a lot safer to use. Although these building blocks do not solve all the issues of concurrent programming, they simplify the reasoning about concurrent programs and can be found across concurrency frameworks and libraries in many languages, including Scala. This chapter extends the fundamental concurrent programming model from <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Concurrency on the JVM and the Java Memory Model</em></span>, with traditional building blocks of concurrency and shows how to use them in practice.</p><p>In general, there are two aspects of a concurrent programming model. The first deals with expressing concurrency in a program. Given a program, which of its parts can execute concurrently and under which conditions? In the previous chapter, we saw that JVM allows declaring and starting separate threads of control. In this chapter, we will visit a more lightweight mechanism for starting concurrent executions. The second important aspect of concurrency is data access. Given a set of concurrent executions, how can these executions correctly access and modify the program data? Having seen a low-level answer to these questions in the previous chapter, such as the <code class="literal">synchronized</code> statement and volatile variables, we will now dive into more complex abstractions. We will study the following topics:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Using the <code class="literal">Executor</code> and <code class="literal">ExecutionContext</code> objects</p></li><li style="list-style-type: disc"><p>Atomic primitives for non-blocking synchronization</p></li><li style="list-style-type: disc"><p>The interaction of lazy values and concurrency</p></li><li style="list-style-type: disc"><p>Using concurrent queues, sets, and maps</p></li><li style="list-style-type: disc"><p>How to create processes and communicate with them</p></li></ul></div><p>The ultimate goal of this chapter will be to implement a safe API for concurrent file handling. We will use the abstractions in this chapter to implement a simple, reusable file-handling API for applications such as filesystem managers or FTP servers. We will thus see how the traditional building blocks of concurrency work separately and how they all fit together in a larger use case.</p></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec19"></a>The Executor and ExecutionContext objects</h2></div></div><hr /></div><p>As discussed in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Concurrency on the JVM and the Java Memory Model</em></span>, although creating a new thread in a Scala program takes orders of magnitude less computational time compared to creating a new JVM process, thread creation is still much more expensive than allocating a single object, acquiring a monitor lock, or updating an entry in a collection. If an application performs a large number of small concurrent tasks and requires high throughput, we cannot afford to create a fresh thread for each of these tasks. Starting a thread requires us to allocate a memory region for its call stack and a context switch from one thread to another, which can be much more time-consuming than the amount of work in the concurrent task. For this reason, most concurrency frameworks have facilities that maintain a set of threads in a waiting state and start running when concurrently executable work tasks become available. Generally, we call such facilities <span class="strong"><strong>thread pools</strong></span>.</p><p>To allow programmers to encapsulate the decision of how to run concurrently executable work tasks, JDK comes with an abstraction called <code class="literal">Executor</code>. The <code class="literal">Executor</code> interface is a simple interface that defines a single <code class="literal">execute</code> method. This method takes a <code class="literal">Runnable</code> object and eventually calls the <code class="literal">Runnable</code> object's <code class="literal">run</code> method. The <code class="literal">Executor</code> object decides on which thread and when to call the <code class="literal">run</code> method. An <code class="literal">Executor</code> object can start a new thread specifically for this invocation of <code class="literal">execute</code> or even execute the <code class="literal">Runnable</code> object directly on the caller thread. Usually, the <code class="literal">Executor</code> executes the <code class="literal">Runnable</code> object concurrently to the execution of the thread that called the <code class="literal">execute</code> method, and it is implemented as a thread pool.</p><p>One <code class="literal">Executor</code> implementation, introduced in JDK 7, is <code class="literal">ForkJoinPool</code> and it is available in the <code class="literal">java.util.concurrent</code> package. Scala programs can use it in JDK 6 as well by importing the contents of the <code class="literal">scala.concurrent.forkjoin</code> package. In the following code snippet, we show you how to instantiate a <code class="literal">ForkJoinPool</code> class implementation and submit a task that can be asynchronously executed:</p><pre class="programlisting">import scala.concurrent._
import java.util.concurrent.ForkJoinPool
object ExecutorsCreate extends App {
Â  val executor = new ForkJoinPool
Â  executor.execute(new Runnable {
Â  Â  def run() = log("This task is run asynchronously.")
Â  })
Â  Thread.sleep(500)
}</pre><p>We start by importing the <code class="literal">scala.concurrent</code> package. In later examples, we implicitly assume that this package is imported. We then call the <code class="literal">ForkJoinPool</code> class and assign it to a value called the <code class="literal">executor</code> method. Once instantiated, the <code class="literal">executor</code> value is sent a task in the form of a <code class="literal">Runnable</code> object that prints to the standard output. Finally, we invoke the <code class="literal">sleep</code> statement in order to prevent the daemon threads in the <code class="literal">ForkJoinPool</code> instance from being terminated before they call the <code class="literal">run</code> method on the <code class="literal">Runnable</code> object. Note that the <code class="literal">sleep</code> statement is not required if you are running the example from SBT with the <code class="literal">fork</code> setting set to <code class="literal">false</code>.</p><p>Why do we need <code class="literal">Executor</code> objects in the first place? In the previous example, we can easily change the <code class="literal">Executor</code> implementation without affecting the code in the <code class="literal">Runnable</code> object. The <code class="literal">Executor</code> objects serve to decouple the logic in the concurrent computations from how these computations are executed. The programmer can focus on specifying parts of the code that potentially execute concurrently, separately from where and when to execute those parts of the code.</p><p>The more elaborate subtype of the <code class="literal">Executor</code> interface, also implemented by the <code class="literal">ForkJoinPool</code> class, is called <code class="literal">ExecutorService</code>. This extended <code class="literal">Executor</code> interface defines several convenience methods, the most prominent being the <code class="literal">shutdown</code> method. The <code class="literal">shutdown</code> method makes sure that the <code class="literal">Executor</code> object gracefully terminates by executing all the submitted tasks and then stopping all the worker threads. Fortunately, our <code class="literal">ForkJoinPool</code> implementation is benign with respect to termination. Its threads are daemons by default, so there is no need to shut it down explicitly at the end of the program. In general, however, programmers should call the <code class="literal">shutdown</code> method on the <code class="literal">ExecutorService</code> objects they created, typically before the program terminates.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip18"></a>Tip</h3><p>When your program no longer needs the <code class="literal">ExecutorService</code> object you created, you should ensure that the <code class="literal">shutdown</code> method is called.</p></div><p>To ensure that all the tasks submitted to the <code class="literal">ForkJoinPool</code> object are complete, we need to additionally call the <code class="literal">awaitTermination</code> method, specifying the maximum amount of time to wait for their completion. Instead of calling the <code class="literal">sleep</code> statement, we can do the following:</p><pre class="programlisting">import java.util.concurrent.TimeUnit
executor.shutdown()
executor.awaitTermination(60, TimeUnit.SECONDS)
</pre><p>The <code class="literal">scala.concurrent</code> package defines the <code class="literal">ExecutionContext</code> trait that offers a similar functionality to that of <code class="literal">Executor</code> objects but is more specific to Scala. We will later learn that many Scala methods take <code class="literal">ExecutionContext</code> objects as implicit parameters. Execution contexts implement the abstract <code class="literal">execute</code> method, which exactly corresponds to the <code class="literal">execute</code> method on the <code class="literal">Executor</code> interface, and the <code class="literal">reportFailure</code> method, which takes a <code class="literal">Throwable</code> object and is called whenever some task throws an exception. The <code class="literal">ExecutionContext</code> companion object contains the default execution context called <code class="literal">global</code>, which internally uses a <code class="literal">ForkJoinPool</code> instance:</p><pre class="programlisting">object ExecutionContextGlobal extends App {
  val ectx = ExecutionContext.global
  ectx.execute(new Runnable {
    def run() = log("Running on the execution context.")
  })
  Thread.sleep(500)
}
</pre><p>The <code class="literal">ExecutionContext</code> companion object defines a pair of methods, <code class="literal">fromExecutor</code> and <code class="literal">fromExecutorService</code>, which create an <code class="literal">ExecutionContext</code> object from an <code class="literal">Executor</code> or <code class="literal">ExecutorService</code> interface, respectively:</p><pre class="programlisting">object ExecutionContextCreate extends App {
  val pool = new forkjoin.ForkJoinPool(2)
  val ectx = ExecutionContext.fromExecutorService(pool)
  ectx.execute(new Runnable {
    def run() = log("Running on the execution context again.")
  })
  Thread.sleep(500)
}
</pre><p>In the preceding example, we will create an <code class="literal">ExecutionContext</code> object from a <code class="literal">ForkJoinPool</code> instance with a parallelism level ofÂ <code class="literal">2</code>. This means that the <code class="literal">ForkJoinPool</code> instance will usually keep two worker threads in its pool.</p><p>In the examples that follow, we will rely on the global <code class="literal">ExecutionContext</code> object. To make the code more concise, we will introduce the <code class="literal">execute</code> convenience method in the package object of this chapter, which executes a block of code on the global <code class="literal">ExecutionContext</code> object:</p><pre class="programlisting">def execute(body: =&gt;Unit) = ExecutionContext.global.execute(
  new Runnable { def run() = body }
)
</pre><p>The <code class="literal">Executor</code> and <code class="literal">ExecutionContext</code> objects are a nifty concurrent programming abstraction, but they are not a silver bullets. They can improve throughput by reusing the same set of threads for different tasks, but they are unable to execute tasks if those threads become unavailable, because all the threads are busy with running other tasks. In the following example, we declare <code class="literal">32</code> independent executions, each of which lasts two seconds, and wait <code class="literal">10</code> seconds for their completion:</p><pre class="programlisting">object ExecutionContextSleep extends App {
  for (i&lt;- 0 until 32) execute {
    Thread.sleep(2000)
    log(s"Task $i completed.")
  }
  Thread.sleep(10000)
}
</pre><p>You would expect that all the executions terminate after two seconds, but this is not the case. Instead, on our quad-core CPU with hyper threading, the global <code class="literal">ExecutionContext</code> object has eight threads in the thread pool, so it executes work tasks in batches of eight. After two seconds, a batch of eight tasks print that they are completed, after two more seconds another batch prints, and so on. This is because the global <code class="literal">ExecutionContext</code> object internally maintains a pool of eight worker threads, and calling <code class="literal">sleep</code> puts all of them into a timed waiting state. Only once the <code class="literal">sleep</code> method call in these worker threads is completed can another batch of eight tasks be executed. Things can be much worse. We could start eight tasks that execute the guarded block idiom seen in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Concurrency on the JVM and the Java Memory Model</em></span>, and another task that calls theÂ <code class="literal">notify</code> method to wake them up. As the <code class="literal">ExecutionContext</code> object can execute only eight tasks concurrently, the worker threads would, in this case, be blocked forever. We say that executing blocking operations on <code class="literal">ExecutionContext</code> objects can cause starvation.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip19"></a>Tip</h3><p>Avoid executing operations that might block indefinitely on <code class="literal">ExecutionContext</code> and <code class="literal">Executor</code> objects.</p></div><p>Having seen how to declare concurrent executions, we turn our attention to how these concurrent executions interact by manipulating program data.</p></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec20"></a>Atomic primitives</h2></div></div><hr /></div><p>In <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Concurrency on the JVM and the Java Memory Model</em></span>, we learned that memory writes do not happen immediately unless proper synchronization is applied. A set of memory writes is not executed at once, that is, atomically. We saw that visibility is ensured by the happens-before relationship, and we relied on the <code class="literal">synchronized</code> statement to achieve it. Volatile fields were a more lightweight way of ensuring happens-before relationships, but a less powerful synchronization construct. Recall how volatile fields alone could not implement the <code class="literal">getUniqueId</code> method correctly.</p><p>In this section, we study atomic variables that provide basic support for executing multiple memory reads and writes at once. Atomic variables are close cousins of volatile variables, but are more expressive than them; they are used to build complex concurrent operations without relying on the <code class="literal">synchronized</code> statement.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec22"></a>Atomic variables</h3></div></div></div><p>An atomic variable is a memory location that supports complex <span class="emphasis"><em>linearizable</em></span> operations. A linearizable operation is any operation that appears to occur instantaneously to the rest of the system. For example, a volatile write is a linearizable operation. A complex linearizable operation is a linearizable operation equivalent to at least two reads and/or writes. We will use the term <span class="emphasis"><em>atomically</em></span> to refer to complex linearizable operations.</p><p>Various atomic variables defined in the <code class="literal">java.util.concurrent.atomic</code> package support some complex linearizable operations on the Boolean, integer, long, and reference types with the <code class="literal">AtomicBoolean</code>, <code class="literal">AtomicInteger</code>, <code class="literal">AtomicLong</code>, and <code class="literal">AtomicReference</code> classes, respectively. Recall that the <code class="literal">getUniqueId</code> method from <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Concurrency on the JVM and the Java Memory Model</em></span>, needs to return a unique numeric identifier each time a thread invokes it. We previously implemented this method using the <code class="literal">synchronized</code> statement, and we now reimplement it using atomic long variables:</p><pre class="programlisting">import java.util.concurrent.atomic._
object AtomicUid extends App {
  private val uid = new AtomicLong(0L)
  def getUniqueId(): Long = uid.incrementAndGet()
  execute { log(s"Uid asynchronously: ${getUniqueId()}") }
  log(s"Got a unique id: ${getUniqueId()}")
}
</pre><p>Here, we declare an atomic long variable, which is <code class="literal">uid</code>, with an initial value <code class="literal">0</code> and call its <code class="literal">incrementAndGet</code> method from <code class="literal">getUniqueId</code>. The <code class="literal">incrementAndGet</code> method is a complex linearizable operation. It simultaneously reads the current value <code class="literal">x</code> of <code class="literal">uid</code>, computes <code class="literal">x + 1</code>, writes <code class="literal">x + 1</code> back to <code class="literal">uid</code>, and returns <code class="literal">x + 1</code>. These steps cannot be interleaved with steps in other invocations of theÂ <code class="literal">incrementAndGet</code> method, so each invocation of the <code class="literal">getUniqueId</code> method returns a unique number.</p><p>Atomic variables define other methods such as the <code class="literal">getAndSet</code> method, which atomically reads the value of the variable, sets the new value, and returns its previous value. Numeric atomic variables additionally have methods such as <code class="literal">decrementAndGet</code> and <code class="literal">addAndGet</code>. It turns out that all these atomic operations are implemented in terms of a fundamental atomic operation, which is <code class="literal">compareAndSet</code>. The compare-and-set operation, sometimes called <span class="strong"><strong>compare-and-swap</strong></span> (<span class="strong"><strong>CAS</strong></span>), takes the expected previous value and the new value for the atomic variable and atomically replaces the current value with the new value only if the current value is equal to the expected value.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note20"></a>Note</h3><p>The CAS operation is a fundamental building block for lock-free programming.</p></div><p>The CAS operation is conceptually equivalent to the following <code class="literal">synchronized</code> block, but is more efficient and does not get blocked on most JVMs, as it is implemented in terms of a processor instruction:</p><pre class="programlisting">def compareAndSet(ov: Long, nv: Long): Boolean =
  this.synchronized {
    if (this.get == ov) false else {
      this.set(nv)
      true
    }
  }
</pre><p>The CAS operation is available on all types of atomic variables; <code class="literal">compareAndSet</code> also exists in the generic <code class="literal">AtomicReference[T]</code> class used to store object references of an arbitrary object of type <code class="literal">T</code>, and is equivalent to the following:</p><pre class="programlisting">def compareAndSet(ov: T, nv: T): Boolean = this.synchronized {
  if (this.get eq ov) false else {
    this.set(nv)
    true
  }
}
</pre><p>If CAS does replace the old value with the new value, it returns the value <code class="literal">true</code>. Otherwise, CAS returns <code class="literal">false</code>. When using CAS, we usually start by calling the <code class="literal">get</code> method on the atomic variable to read its value. We then compute a new value based on the value we read. Finally, we invoke the CAS operation to change the value we previously read with the new value. If the CAS operation returns <code class="literal">true</code>, we are done. If the CAS operation returns <code class="literal">false</code>, then some other thread must have changed the atomic variable since we last read it using the <code class="literal">get</code> variable.</p><p>Let's see how CAS works in a concrete example. We will re-implement the <code class="literal">getUniqueId</code> method using the <code class="literal">get</code> and <code class="literal">compareAndSet</code> methods:</p><pre class="programlisting">@tailrec def getUniqueId(): Long = {
  val oldUid = uid.get
  val newUid = oldUid + 1
  if (uid.compareAndSet(oldUid, newUid)) newUid
  else getUniqueId()
}
</pre><p>This time, the thread<code class="literal">T</code> calls the <code class="literal">get</code> method to read the value of <code class="literal">uid</code> into a local variable <code class="literal">oldUid</code>. Note that local variables such as <code class="literal">oldUid</code> are only used by a single thread that initialized them, so no other thread can see the version of the <code class="literal">oldUid</code> variable in thread T. The thread <code class="literal">T</code> then computes the new value <code class="literal">newUid</code>. This does not happen atomically, and at this point, another thread S might concurrently change the value of the <code class="literal">uid</code> variable. The <code class="literal">compareAndSet</code> call by T changes <code class="literal">uid</code> successfully only if no other thread S modified the value of the <code class="literal">uid</code> variable since thread <code class="literal">T</code> called the <code class="literal">get</code> method in the first line. If the <code class="literal">compareAndSet</code> method is not successful, the method is called again tail-recursively. Hence, we use the <code class="literal">@tailrec</code> annotation to force the compiler to generate a tail-recursive call. We say that thread <code class="literal">T</code> needs to retry the operation. This is illustrated in the following figure:</p><div class="mediaobject"><img src="graphics/image_03_001.jpg" /></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip21"></a>Tip</h3><p>Always use the <code class="literal">@tailrec</code> annotation for these functions, which are intended to be tail-recursive. The compiler will check all the annotated functions to see whether or not they are tail-recursive.</p></div><p>Retrying is a common pattern when programming with CAS operations. This retry can happen infinitely many times. The good news is that a CAS in thread <span class="strong"><strong>T</strong></span> can fail only when another thread <span class="strong"><strong>S</strong></span> completes the operation successfully; if our part of the system does not progress, at least some other part of the system does. In fact, the <code class="literal">getUniqueId</code> method is fair to all the threads in practice, and most JDKs implement the <code class="literal">incrementAndGet</code> method in a very similar manner to our CAS-based implementation of theÂ <code class="literal">getUniqueId</code> method.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec23"></a>Lock-free programming</h3></div></div></div><p>A <span class="strong"><strong>lock</strong></span> is a synchronization mechanismÂ used to limit access to a resource that can be used by multiple threads. In <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Concurrency on the JVM and the Java Memory Model</em></span>, we learned that every JVM object has an intrinsic lock that is used when invoking the <code class="literal">synchronized</code> statement on the object. Recall that an intrinsic lock makes sure that at most one thread executes the <code class="literal">synchronized</code> statement on the object. The intrinsic lock accomplishes this by blocking all the threads that try to acquire it when it is unavailable. We will study other examples of locks in this section.</p><p>As we already learned, programming using locks is susceptible to deadlocks. Also, if the OS pre-empts a thread that is holding a lock, it might arbitrarily delay the execution of other threads. In lock-free programs, these effects are less likely to compromise the program's performance.</p><p>Why do we need atomic variables? Atomic variables allow us to implement <span class="emphasis"><em>lock-free operations</em></span>. As the name implies, a thread that executes a lock-free operation does not acquire any locks. Consequently, many lock-free algorithms have an improved throughput. A thread executing a lock-free algorithm does not hold any locks when it gets pre-empted by the OS, so it cannot temporarily block other threads. Furthermore, lock-free operations are impervious to deadlocks, because threads cannot get blocked indefinitely without locks.</p><p>Our CAS-based implementation of the <code class="literal">getUniqueId</code> method is an example of a lock-free operation. It acquires no locks that can permanently suspend other threads. If one thread fails due to concurrent CAS operations, it immediately restarts and tries to execute the <code class="literal">getUniqueId</code> method again.</p><p>However, not all operations composed from atomic primitives are lock-free. Using atomic variables is a necessary precondition for lock-freedom, but it is not sufficient. To show this, we will implement our own simple <code class="literal">synchronized</code> statement, which will use atomic variables:</p><pre class="programlisting">object AtomicLock extends App {
  private val lock = new AtomicBoolean(false)
  def mySynchronized(body: =&gt;Unit): Unit = {
    while (!lock.compareAndSet(false, true)) {}
    try body finally lock.set(false)
  }
  var count = 0
  for (i&lt;- 0 until 10) execute { mySynchronized { count += 1 } }
  Thread.sleep(1000)
  log(s"Count is: $count")
}
</pre><p>The <code class="literal">mySynchronized</code> statement executes a block of code <code class="literal">body</code> in isolation. It uses the atomic <code class="literal">lock</code> Boolean variable to decide whether some thread is currently calling theÂ <code class="literal">mySynchronized</code> method or not. The first thread that changes the <code class="literal">lock</code> variable from <code class="literal">false</code> to <code class="literal">true</code> using the <code class="literal">compareAndSet</code> method can proceed with executing the body. While the thread is executing the body, other threads calling the <code class="literal">mySynchronized</code> method repetitively invoke the <code class="literal">compareAndSet</code> method on the <code class="literal">lock</code> variable but fail. Once <code class="literal">body</code> completes executing, the thread unconditionally sets the <code class="literal">lock</code> variable back to <code class="literal">false</code> in the <code class="literal">finally</code> block. A <code class="literal">compareAndSet</code> method in some other thread can then succeed, and the process is repeated again. After all the tasks are completed, the value of the <code class="literal">count</code> variable is always <code class="literal">10</code>. The main difference with respect to the <code class="literal">synchronized</code> statement is that threads calling <code class="literal">mySynchronized</code> busy-wait in the <code class="literal">while</code> loop until the lock becomes available. Such locks are dangerous and much worse than the <code class="literal">synchronized</code> statement. This example shows you that we need to define lock-freedom more carefully, because a lock can implicitly exist in the program without the programmer being aware of it.</p><p>In <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Concurrency on the JVM and the Java Memory Model</em></span>, we learned that most modern operating systems use pre-emptive multitasking, where a thread <code class="literal">T</code> can be temporarily suspended by the operating system at any point in time. If this happens while thread <code class="literal">T</code> is holding a lock, other threads waiting for the same lock cannot proceed until the lock is released. These other threads have to wait until the operating system continues executing the thread <code class="literal">T</code> and the thread <code class="literal">T</code> releases the lock. This is unfortunate, as these threads could be doing useful work while the thread <code class="literal">T</code> is suspended. We say that a slow thread <code class="literal">T</code> blocked the execution of other threads. In a lock-free operation, a slow thread cannot block the execution of other threads. If multiple threads execute an operation concurrently, then at least one of these threads must complete in a finite amount of time.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note22"></a>Note</h3><p>Given a set of threads executing an operation, an operation is lock-free if at least one thread always completes the operation after a finite number of steps, regardless of the speed at which different threads progress.</p></div><p>With this more formal definition of lock-freedom, you can get a feel for why lock-free programming is hard. It is not easy to prove that an operation is lock-free, and implementing more complex lock-free operations is notoriously difficult. The CAS-based <code class="literal">getUniqueId</code> implementation is indeed lock-free. Threads only loop if the CAS fails, and the CAS can only fail if some thread successfully computed the unique identifier: this means that some other thread executed <code class="literal">getUniqueId</code> method successfully in a finite number of steps between the <code class="literal">get</code> and <code class="literal">compareAndSet</code> method calls. This fact proves lock-freedom.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec24"></a>Implementing locks explicitly</h3></div></div></div><p>In some cases, we really do want locks, and atomic variables allow us to implement locks that do not have to block the caller. The trouble with intrinsic object locks from <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Concurrency on the JVM and the Java Memory Model</em></span>, is that a thread cannot inspect whether the object's intrinsic lock is currently acquired. Instead, a thread that calls <code class="literal">synchronized</code> is immediately blocked until the monitor becomes available. Sometimes, we would like our threads to execute a different action when a lock is unavailable.</p><p>We now turn to the concurrent filesystem API mentioned at the beginning of this chapter. Inspecting the state of a lock is something we need to do in an application such as a file manager. In the good old days of DOS and Norton Commander, starting a file copy blocked the entire user interface, so you could sit back, relax, and grab your Game Boy until the file transfer completes. Times change; modern file managers need to start multiple file transfers simultaneously, cancel existing transfers, or delete different files simultaneously. Our filesystem API must ensure that:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>If a thread is creating a new file, then that file cannot be copied or deleted</p></li><li style="list-style-type: disc"><p>If one or more threads are copying a file, then the file cannot be deleted</p></li><li style="list-style-type: disc"><p>If a thread is deleting a file, then the file cannot be copied</p></li><li style="list-style-type: disc"><p>Only a single thread in the file manager is deleting a file at a time</p></li></ul></div><p>The filesystem API will allow the concurrent copying and deleting of files. In this section, we will start by ensuring that only a single thread gets to delete a file. We model a single file or directory with the <code class="literal">Entry</code> class:</p><pre class="programlisting">class Entry(val isDir: Boolean) {
  val state = new AtomicReference[State](new Idle)
}
</pre><p>The <code class="literal">isDir</code> field of the <code class="literal">Entry</code> class denotes whether the respective path is a file or a directory. The <code class="literal">state</code> field describes the file state: whether the file is idle, currently being created, copied, or is scheduled for deletion. We model these states with a sealed trait called <code class="literal">State</code>:</p><pre class="programlisting">sealed trait State
class Idle extends State
class Creating extends State
class Copying(val n: Int) extends State
class Deleting extends State
</pre><p>Note that, in the case of the <code class="literal">Copying</code> state, the <code class="literal">n</code> field also tracks how many concurrent copies are in progress. When using atomic variables, it is often useful to draw a diagram of the different states that an atomic variable can be in. As illustrated in the following figure, <code class="literal">state</code> is set to <code class="literal">Creating</code> immediately after an <code class="literal">Entry</code> class is created and then becomes the <code class="literal">Idle</code> state. After that, an <code class="literal">Entry</code> object can jump between the <code class="literal">Copying</code> and <code class="literal">Idle</code> states indefinitely and, eventually, getÂ from <code class="literal">Idle</code> to <code class="literal">Deleting</code>. After getting into the <code class="literal">Deleting</code> state, the <code class="literal">Entry</code> class can no longer be modified; this indicates that we are about to delete the file.</p><div class="mediaobject"><img src="graphics/image_03_002.jpg" /></div><p>Let's assume that we want to delete a file. There might be many threads running inside our file manager, and we want to avoid having two threads delete the same file. We will require the file being deleted to be in the <code class="literal">Idle</code> state and atomically change it to the <code class="literal">Deleting</code> state. If the file is not in the <code class="literal">Idle</code> state, we report an error. We will use the <code class="literal">logMessage</code> method, which is defined later; for now, we can assume that this method just calls our <code class="literal">log</code> statement:</p><pre class="programlisting">@tailrec private def prepareForDelete(entry: Entry): Boolean = {
  val  s0 = entry.state.get
  s0 match {
    case i: Idle =&gt;
      if (entry.state.compareAndSet(s0, new Deleting)) true
      else prepareForDelete(entry)
    case c: Creating =&gt;
      logMessage("File currently created, cannot delete."); false
    case c: Copying =&gt;
      logMessage("File currently copied, cannot delete."); false
    case d: Deleting =&gt;
      false
  }
}
</pre><p>The <code class="literal">prepareForDelete</code> method starts by reading the <code class="literal">state</code> atomic reference variable and stores its value into a local variable, <code class="literal">s0</code>. It then checks whether theÂ <code class="literal">s0</code> variable is theÂ <code class="literal">Idle</code> state and attempts to atomically change the state to theÂ <code class="literal">Deleting</code> state. Just like in the <code class="literal">getUniqueId</code> method example, a failed CAS indicates that another thread changed the <code class="literal">state</code> variable and the operation needs to be repeated. The file cannot be deleted if another thread is creating or copying it, so we report an error and return <code class="literal">false</code>. If another thread is already deleting the file, we only return <code class="literal">false</code>.</p><p>The <code class="literal">state</code> atomic variable implicitly acts like a lock in this example, although it neither blocks the other threads nor busy-waits. If the <code class="literal">prepareForDelete</code> method returns <code class="literal">true</code>, we know that our thread can safely delete the file, as it is the only thread that changed the <code class="literal">state</code> variable value to <code class="literal">Deleting</code>. However, if the method returns <code class="literal">false</code>, we report an error in the file manager UI instead of blocking it.</p><p>An important thing to note about the <code class="literal">AtomicReference</code> class is that it always uses reference equality when comparing the old object and the new object assigned to <code class="literal">state</code>.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note23"></a>Note</h3><p>The CAS instructions on atomic reference variables always use reference equality and never call the <code class="literal">equals</code> method, even when the <code class="literal">equals</code> method is overridden.</p></div><p>As an expert in sequential Scala programming, you might be tempted to implement <code class="literal">State</code> subtypes as case classes in order to get the <code class="literal">equals</code> method for free, but this does not affect the <code class="literal">compareAndSet</code> method operation.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec25"></a>The ABA problem</h3></div></div></div><p>The <span class="strong"><strong>ABA problem</strong></span> is a situation in concurrent programming where two reads of the same memory location yield the same value A, which is used to indicate that the value of the memory location did not change between the two reads. This conclusion can be violated if other threads concurrently write some value B to the memory location, followed by the write of value A again. The ABA problem is usually a type of a race condition. In some cases, it leads to program errors.</p><p>Suppose that we implemented <code class="literal">Copying</code> as a class with a mutable field <code class="literal">n</code>. We might then be tempted to reuse the same <code class="literal">Copying</code> object for subsequent calls to <code class="literal">release</code> and <code class="literal">acquire</code>. This is almost certainly not a good idea!</p><p>Let's assume that we have a hypothetical pair of methods called <code class="literal">releaseCopy</code> and <code class="literal">acquireCopy</code>. The <code class="literal">releaseCopy</code> method assumes that the <code class="literal">Entry</code> class is in the <code class="literal">Copying</code> state and changes the state from <code class="literal">Copying</code> to another <code class="literal">Copying</code> or <code class="literal">Idle</code> state. It then returns the old <code class="literal">Copying</code> object associated with the previous state:</p><pre class="programlisting">def releaseCopy(e: Entry): Copying = e.state.get match {
  case c: Copying =&gt;
    val nstate = if (c.n == 1) new Idle else new Copying(c.n - 1)
    if (e.state.compareAndSet(c, nstate)) c
    else releaseCopy(e)
}
</pre><p>The <code class="literal">acquireCopy</code> method takes a currently unused <code class="literal">Copying</code> object and attempts to replace the old state with the previously used <code class="literal">Copying</code> object:</p><pre class="programlisting">def acquireCopy(e: Entry, c: Copying) = e.state.get match {
  case i: Idle =&gt;
    c.n = 1
    if (!e.state.compareAndSet(i, c)) acquire(e, c)
  case oc: Copying =&gt;
    c.n = oc.n + 1
    if (!e.state.compareAndSet(oc, c)) acquire(e, c)
}
</pre><p>Upon calling the <code class="literal">releaseCopy</code> method, a thread might store the old <code class="literal">Copying</code> object. Later, the same thread can reuse the old <code class="literal">Copying</code> object in the call to the <code class="literal">acquireCopy</code> method. Here, the programmer's intent could be to reduce the pressure on the garbage collector by allocating fewerÂ <code class="literal">Copying</code> objects. However, this leads to the ABA problem, as we will describe further.</p><p>We consider two threads <span class="strong"><strong>T1</strong></span> and <span class="strong"><strong>T2</strong></span>, which call the <code class="literal">releaseCopy</code> method. They both read the state of the <code class="literal">Entry</code> object and create a new state object <code class="literal">nstate</code>, which is <code class="literal">Idle</code>. Let's assume that the thread <span class="strong"><strong>T1</strong></span> executes the <code class="literal">compareAndSet</code> operation first and returns the old <code class="literal">Copying</code> object <code class="literal">c</code> from the <code class="literal">releaseCopy</code> method. Next, let's assume that a third thread <span class="strong"><strong>T3</strong></span> calls the <code class="literal">acquireCopy</code> method and changes the state of the <code class="literal">Entry</code> object to <code class="literal">Copying(1)</code>. If the thread <span class="strong"><strong>T1</strong></span> now calls the <code class="literal">acquireCopy</code> method with the old <code class="literal">Copying</code> object <code class="literal">c</code>, the state of the <code class="literal">Entry</code> object becomes <code class="literal">Copying(2)</code>. Note that, at this point, the old <code class="literal">Copying</code> object <code class="literal">c</code> is once again stored inside the atomic variable <code class="literal">state</code>. If the thread <span class="strong"><strong>T1</strong></span> now attempts to call <code class="literal">compareAndSet</code>, it will succeed and set the state of the <code class="literal">Entry</code> object to <code class="literal">Idle</code>. Effectively, the last <code class="literal">compareAndSet</code> operation changes the state from <code class="literal">Copying(2)</code> to <code class="literal">Idle</code>, so one acquire is lost.</p><p>This scenario is shown in the following figure:</p><div class="mediaobject"><img src="graphics/image_03_003.jpg" /></div><p>In the preceding example, the ABA problem manifests itself in the execution of thread <span class="strong"><strong>T2</strong></span>. Having first read the value of the <code class="literal">state</code> field in the <code class="literal">Entry</code> object with the <code class="literal">get</code> method and with the <code class="literal">compareAndSet</code> method later, thread <span class="strong"><strong>T2</strong></span> assumes that the value of the <code class="literal">state</code> field has not changed between these two writes. In this case, this leads to a program error.</p><p>There is no general technique to avoid the ABA problem, so we need to guard the program against it on a per-problem basis. Still, the following guidelines are useful when avoiding the ABA problem in a managed runtime, such as JVM:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Create new objects before assigning them to the <code class="literal">AtomicReference</code> objects</p></li><li style="list-style-type: disc"><p>Store immutable objects inside the <code class="literal">AtomicReference</code> objects</p></li><li style="list-style-type: disc"><p>Avoid assigning a value that was previously already assigned to an atomic variable</p></li><li style="list-style-type: disc"><p>If possible, make updates to numeric atomic variables monotonic, that is, either strictly decreasing or strictly increasing with respect to the previous value</p></li></ul></div><p>There are other techniques in order to avoid the ABA problem, such as pointer masking and hazard pointers, but these are not applicable to JVM.</p><p>In some cases, the ABA problem does not affect the correctness of the algorithm; for example, if we change the <code class="literal">Idle</code> class to a singleton object, the <code class="literal">prepareForDelete</code> method will continue to work correctly. Still, it is a good practice to follow the preceding guidelines, because they simplify the reasoning about lock-free algorithms.</p></div></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec21"></a>Lazy values</h2></div></div><hr /></div><p>You should be familiar with lazy values from sequential programming in Scala. Lazy values are the value declarations that are initialized with their right-hand side expression when the lazy value is read for the first time. This is unlike regular values, which are initialized the moment they are created. If a lazy value is never read inside the program, it is never initialized and it is not necessary to pay the cost of its initialization. Lazy values allow you to implement data structures such as lazy streams; they improve complexities of persistent data structures, can boost the program's performance, and help avoid initialization order problems in Scala's mix-in composition.</p><p>Lazy values are extremely useful in practice, and you will often deal with them in Scala. However, using them in concurrent programs can have some unexpected interactions, and this is the topic of this section. Note that lazy values must retain the same semantics in a multithreaded program; a lazy value is initialized only when a thread accesses it, and it is initialized at most once. Consider the following motivating example in which two threads access two lazy values, which are <code class="literal">obj</code> and <code class="literal">non</code>:</p><pre class="programlisting">object LazyValsCreate extends App {
  lazy val obj = new AnyRef
  lazy val non = s"made by ${Thread.currentThread.getName}"
  execute {
    log(s"EC sees obj = $obj")
    log(s"EC sees non = $non")
  }
  log(s"Main sees obj = $obj")
  log(s"Main sees non = $non")
  Thread.sleep(500)
}
</pre><p>You know from sequential Scala programming that it is a good practice to initialize the lazy value with an expression that does not depend on the current state of the program. The lazy value <code class="literal">obj</code> follows this practice, but the lazy value <code class="literal">non</code> does not. If you run this program once, you might notice that <code class="literal">non</code> lazy value is initialized with the name of the main thread:</p><pre class="programlisting">[info] main: Main sees non = made by main
[info] FJPool-1-worker-13: EC sees non = made by main
</pre><p>Running the program again shows you that <code class="literal">non</code> is initialized by the worker thread:</p><pre class="programlisting">[info] main: Main sees non = made by FJPool-1-worker-13
[info] FJPool-1-worker-13: EC sees non = made by FJPool-1-worker-13
</pre><p>As the previous example shows you, lazy values are affected by non-determinism. Non-deterministic lazy values are a recipe for trouble, but we cannot always avoid them. Lazy values are deeply tied into Scala, because singleton objects are implemented as lazy values under the hood:</p><pre class="programlisting">object LazyValsObject extends App {
  object Lazy { log("Running Lazy constructor.") }
  log("Main thread is about to reference Lazy.")
  Lazy
  log("Main thread completed.")
}
</pre><p>Running this program reveals that the <code class="literal">Lazy</code> initializer runs when the object is first referenced in the third line and not when it is declared. Getting rid of singleton objects in your Scala code would be too restrictive, and singleton objects are often large; they can contain all kinds of potentially non-deterministic code.</p><p>You might think that a little bit of non-determinism is something we can live with. However, this non-determinism can be dangerous. In the existing Scala versions, lazy values and singleton objects are implemented with the so-called <span class="emphasis"><em>double-checked locking idiom</em></span> under the hood. This concurrent programming pattern ensures that a lazy value is initialized by at most one thread when it is first accessed. Thanks to this pattern, upon initializing the lazy value, its subsequent reads are cheap and do not need to acquire any locks. Using this idiom, a single lazy value declaration, which is <code class="literal">obj</code> from the previous example, is translated by the Scala compiler as follows:</p><pre class="programlisting">object LazyValsUnderTheHood extends App {
  @volatile private var _bitmap = false
  private var _obj: AnyRef = _
  def obj = if (_bitmap) _obj else this.synchronized {
    if (!_bitmap) {
      _obj = new AnyRef
      _bitmap = true
    }
    _obj
  }
  log(s"$obj")
  log(s"$obj")
}
</pre><p>The Scala compiler introduces an additional volatile field,Â <code class="literal">_bitmap</code>, when a class contains lazy fields. The private <code class="literal">_obj</code> field that holds the value is uninitialized at first. After the <code class="literal">obj</code> getter assigns a value to the <code class="literal">_obj</code> field, it sets the <code class="literal">_bitmap</code> field to <code class="literal">true</code> to indicate that the lazy value was initialized. Other subsequent invocations of the getter know whether they can read the lazy value from theÂ <code class="literal">_obj</code> field by checking the <code class="literal">_bitmap</code> field.</p><p>The getter <code class="literal">obj</code> starts by checking whether the <code class="literal">_bitmap</code> field is <code class="literal">true</code>. If <code class="literal">_bitmap</code> field is <code class="literal">true</code>, then the lazy value was already initialized and the getter returns <code class="literal">_obj</code>. Otherwise, the getter <code class="literal">obj</code> attempts to obtain the intrinsic lock of the enclosing object, in this case, <code class="literal">LazyValsUnderTheHood</code>. If the <code class="literal">_bitmap</code> field is still not set from within the <code class="literal">synchronized</code> block, the getter evaluates the <code class="literal">new AnyRef</code> expression, assigns it to <code class="literal">_obj</code>, and sets <code class="literal">_bitmap</code> to <code class="literal">true</code>. After this point, the lazy value is considered initialized. Note that the <code class="literal">synchronized</code> statement, together with the check that the <code class="literal">_bitmap</code> field is <code class="literal">false</code>, ensure that a single thread at most initializes the lazy value.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note24"></a>Note</h3><p>The double-checked locking idiom ensures that every lazy value is initialized by at most a single thread.</p></div><p>This mechanism is robust and ensures that lazy values are both thread-safe and efficient. However, synchronization on the enclosing object can cause problems. Consider the following example in which two threads attempt to initialize lazy values <code class="literal">A.x</code> and <code class="literal">B.y</code> at the same time:</p><pre class="programlisting">object LazyValsDeadlock extends App {
  object A { lazy val x: Int = B.y }
  object B { lazy val y: Int = A.x }
  execute { B.y }
  A.x
}
</pre><p>In a sequential setting, accessing either <code class="literal">A.x</code> or <code class="literal">B.y</code> results in a stack overflow. Initializing <code class="literal">A.x</code> requires calling the getter for <code class="literal">B.y</code>, which is not initialized. Initialization of <code class="literal">B.y</code> calls the getter for <code class="literal">A.x</code> and continues in infinite recursion. However, this example was carefully tuned to access both <code class="literal">A.x</code> and <code class="literal">B.y</code> at the same time by both the main thread and the worker thread. Prepare to restart SBT. After both <code class="literal">A</code> and <code class="literal">B</code> are initialized, their monitors are acquired simultaneously by two different threads. Each of these threads needs to acquire a monitor owned by the other thread. Neither thread lets go of its own monitor, and this results in a deadlock.</p><p>Cyclic dependencies between lazy values are unsupported in both sequential and concurrent Scala programs. The difference is that they potentially manifest themselves as deadlocks instead of stack overflows in concurrent programming.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip25"></a>Tip</h3><p>Avoid cyclic dependencies between lazy values, as they can cause deadlocks.</p></div><p>The previous example is not something you are likely to do in your code, but cyclic dependencies between lazy values and singleton objects can be much more devious and harder to spot. In fact, there are other ways to create dependencies between lazy values besides directly accessing them. A lazy value initialization expression can block a thread until some other value becomes available. In the following example, the initialization expression uses the <code class="literal">thread</code> statement from <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Concurrency on the JVM and the Java Memory Model</em></span>, to start a new thread and join it:</p><pre class="programlisting">object LazyValsAndBlocking extends App {
  lazy val x: Int = {
    val t = ch2.thread { println(s"Initializing $x.") }
    t.join()
    1
  }
  x
}
</pre><p>Although there is only a single lazy value in this example, running it inevitably results in a deadlock. The new thread attempts to access <code class="literal">x</code>, which is not initialized, so it attempts to call theÂ <code class="literal">synchronized</code> statement on the <code class="literal">LazyValsAndBlocking</code> object and blocks, because the main thread already holds this lock. On the other hand, the main thread waits for the other thread to terminate, so neither thread can progress.</p><p>While the deadlock is relatively obvious in this example, in a larger code base, circular dependencies can easily sneak past your guard. In some cases, they might even be non-deterministic and occur only in particular system states. To guard against them, avoid blocking in the lazy value expression altogether.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip26"></a>Tip</h3><p>Never invoke blocking operations inside lazy value initialization expressions or singleton object constructors.</p></div><p>Lazy values cause deadlocks even when they do not block themselves. In the following example, the main thread calls theÂ <code class="literal">synchronized</code> statement on the enclosing object, starts a new thread, and waits for its termination. The new thread attempts to initialize the lazy value <code class="literal">x</code>, but it cannot acquire the monitor until the main thread releases it:</p><pre class="programlisting">object LazyValsAndMonitors extends App {
  lazy val x = 1
  this.synchronized {
    val t = ch2.thread { x }
    t.join()
  }
}
</pre><p>This kind of deadlock is not inherent to lazy values and can happen with arbitrary code that uses <code class="literal">synchronized</code> statements. The problem is that the <code class="literal">LazyValsAndMonitors</code> lock is used in two different contexts: as a lazy value initialization lock and as the lock for some custom logic in the main thread. To prevent two unrelated software components from using the same lock, always call <code class="literal">synchronized</code> on separate private objects that exist solely for this purpose.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip27"></a>Tip</h3><p>Never call <code class="literal">synchronized</code> on publicly available objects; always use a dedicated, private dummy object for synchronization.</p></div><p>Although we rarely use separate objects for synchronization in this book, to keep the examples concise, you should strongly consider doing this in your programs. This tip is useful outside the context of lazy values; keeping your locks private reduces the possibility of deadlocks.</p></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec22"></a>Concurrent collections</h2></div></div><hr /></div><p>As you can conclude from the discussion on the Java Memory Model in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Concurrency on the JVM and the Java Memory Model</em></span>, modifying the Scala standard library collections from different threads can result in arbitrary data corruption. Standard collection implementations do not use any synchronization. Data structures underlying mutable collections can be quite complex; predicting how multiple threads affect the collection state in the absence of synchronization is neither recommended nor possible. We will demonstrate this by letting two threads add numbers to the <code class="literal">mutable.ArrayBuffer</code> collection:</p><pre class="programlisting">import scala.collection._
object CollectionsBad extends App {
  val buffer = mutable.ArrayBuffer[Int]()
  def asyncAdd(numbers: Seq[Int]) = execute {
    buffer ++= numbers
    log(s"buffer = $buffer")
  }
  asyncAdd(0 until 10)
  asyncAdd(10 until 20)
  Thread.sleep(500)
}
</pre><p>Instead of printing an array buffer with 20 different elements, this example arbitrarily prints different results or throws exceptions each time it runs. The two threads modify the internal array buffer state simultaneously and cause data corruption.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip28"></a>Tip</h3><p>Never use mutable collections from several different threads without applying proper synchronization.</p></div><p>We can restore synchronization in several ways. First, we can use <span class="strong"><strong>immutable collections</strong></span> along with synchronization to share them between threads. For example, we can store immutable data structures inside atomic reference variables. In the following code snippet, we introduce an <code class="literal">AtomicBuffer</code> class that allows concurrent <code class="literal">+=</code> operations. Appending reads the current immutable <code class="literal">List</code> value from the atomic reference buffer and creates a new <code class="literal">List</code> object containing <code class="literal">x</code>. It then invokes a CAS operation to atomically update the buffer, retrying the operation if the CAS operation is not successful:</p><pre class="programlisting">class AtomicBuffer[T] {
  private val buffer = new AtomicReference[List[T]](Nil)
  @tailrec def +=(x: T): Unit = {
    val xs = buffer.get
    val nxs = x :: xs
    if (!buffer.compareAndSet(xs, nxs)) this += x
  }
}
</pre><p>While using atomic variables or the <code class="literal">synchronized</code> statements with immutable collections is simple, it can lead to scalability problems when many threads access an atomic variable at once.</p><p>If we intend to continue using mutable collections, we need to add <code class="literal">synchronized</code> statements around calls to collection operations:</p><pre class="programlisting">def asyncAdd(numbers: Seq[Int]) = execute {
  buffer.synchronized {
    buffer ++= numbers
    log(s"buffer = $buffer")
  }
}
</pre><p>This approach can be satisfactory, provided that collection operations do not block inside <code class="literal">synchronized</code>. In fact, this approach allows you to implement guarded blocks around collection operations, as we saw in the <code class="literal">SynchronizedPool</code> example in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Concurrency on the JVM and the Java Memory Model</em></span>. However, using theÂ <code class="literal">synchronized</code> statement can also lead to scalability problems when many threads attempt to acquire the lock at once.</p><p>Finally, concurrent collections are collection implementations with operations that can be safely invoked from different threads without synchronization. In addition to the thread-safe versions of basic collection operations, some concurrent collections provide more expressive operations. Conceptually, the same operations can be achieved using atomic primitives, <code class="literal">synchronized</code> statements, and guarded blocks, but concurrent collections ensure far better performance and scalability.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec26"></a>Concurrent queues</h3></div></div></div><p>A common pattern used in concurrent programming is the <span class="strong"><strong>producer-consumer pattern</strong></span>. In this pattern, the responsibility for different parts of the computational workload is divided across several threads. In an FTP server, one or more threads can be responsible for reading chunks of a large file from the disk. Such threads are called producers. Another dedicated set of threads can bear the responsibility of sending file chunks through the network. We call these threads consumers. In their relationship, consumers must react to work elements created by the producers. Often, the two are not perfectly synchronized, so work elements need to be buffered somewhere.</p><p>The concurrent collection that supports this kind of buffering is called a <span class="strong"><strong>concurrent queue</strong></span>. There are three main operations we expect from a concurrent queue. The enqueue operation allows producers to add work elements to the queue, and the dequeue operation allows consumers to remove them. Finally, sometimes we want to check whether the queue is empty or inspect the value of the next item without changing the queue's contents. Concurrent queues can be <span class="strong"><strong>bounded</strong></span>, which means that they can only contain a maximum number of elements, or they can be <span class="strong"><strong>unbounded</strong></span>, which means that they can grow indefinitely. When a bounded queue contains the maximum number of elements, we say it is full. The semantics of the various versions of enqueue and dequeue operations differ with respect to what happens when we try to enqueue to a full queue or dequeue from an empty queue. This special case needs to be handled differently by the concurrent queue. In single-threaded programming, sequential queues usually return a special value such as <code class="literal">null</code> or <code class="literal">false</code> when they are full or empty, or simply throw an exception. In concurrent programming, the absence of elements in the queue can indicate that the producer has not yet enqueued an element, although it might enqueue it in the future. Similarly, a full queue means that the consumer did not yet remove elements but will do so later. For this reason, some concurrent queues have <span class="emphasis"><em>blocking</em></span> enqueue and dequeue implementations, which block the caller until the queue is non-full or non-empty, respectively.</p><p>JDK represents multiple efficient concurrent queue implementations in the <code class="literal">java.util.concurrent</code> package with the <code class="literal">BlockingQueue</code> interface. Rather than reinventing the wheel with its own concurrent queue implementations, Scala adopts these concurrent queues as part of its concurrency utilities and it does not currently have a dedicated trait for blocking queues.</p><p>The <code class="literal">BlockingQueue</code> interface contains several versions of the basic concurrent queue operations, each with slightly different semantics. Different variants of their enqueue, dequeue, and inspect-next methods are summarized in the following table. The inspect, dequeue, and enqueue versions are called <code class="literal">element</code>, <code class="literal">remove</code>, and <code class="literal">add</code> in the first column; they throw an exception when the queue is empty or full. Methods such as <code class="literal">poll</code> and <code class="literal">offer</code> return special values such as <code class="literal">null</code> or <code class="literal">false</code>. Timed versions of these methods block the caller for a specified duration before returning an element or a special value, and blocking methods block the calling thread until the queue becomes non-empty or non-full.</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /><col /><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Operation</strong></span></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Exception</strong></span></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Special value</strong></span></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Timed</strong></span></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Blocking</strong></span></p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Dequeue</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>remove(): T</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>poll(): T</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>poll(t: Long, u:</p><p>
</p><p>Â  TimeUnit): T</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>take(): T</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Enqueue</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>add(x: T)</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>offer(x: T):</p><p>
</p><p>Â  Boolean</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>offer(x: T, t: Long,</p><p>
</p><p>Â  u: TimeUnit)</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>put(x: T)</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; ">
<p>Inspect</p>
</td><td style="border-right: 0.5pt solid ; ">
<p>element: T</p>
</td><td style="border-right: 0.5pt solid ; ">
<p>peek: T</p>
</td><td style="border-right: 0.5pt solid ; ">
<p>N/A</p>
</td><td style="">
<p>N/A</p>
</td></tr></tbody></table></div><p>The <code class="literal">ArrayBlockingQueue</code> class is a concrete implementation of a bounded blocking queue. When creating the <code class="literal">ArrayBlockingQueue</code> class, we need to specify its capacity, which is the number of elements in the queue when it is full. If producers can potentially create elements faster than the consumers can process them, we need to use bounded queues. Otherwise, the queue size can potentially grow to the point where it consumes all the available memory in the program.</p><p>Another concurrent queue implementation is calledÂ <code class="literal">LinkedBlockingQueue</code>. This queue is unbounded, and we can use it when we are sure that the consumers work much faster than the producers. This queue is an ideal candidate for the logging component of our filesystem's API. Logging must return feedback about the execution to the user. In a file manager, logging produces messages to the user inside the UI, while in an FTP server it sends feedback over the network. To keep the example simple, we just print the messages to the standard output.</p><p>We use theÂ <code class="literal">LinkedBlockingQueue</code> collection to buffer various messages from different components of the filesystem API. We declare the queue to a private variable called <code class="literal">messages</code>. A separate daemon thread, called <code class="literal">logger</code>, repetitively calls the <code class="literal">take</code> method on messages. Recall from the previous table that the <code class="literal">take</code> method is blocking; it blocks the <code class="literal">logger</code> thread until there is a message in the queue. The <code class="literal">logger</code> thread then calls <code class="literal">log</code> to print the message. The <code class="literal">logMessage</code> method, which we used in the <code class="literal">prepareForDelete</code> method earlier, simply calls the <code class="literal">offer</code> method on the <code class="literal">messages</code> queue. We could have alternatively called <code class="literal">add</code> or <code class="literal">put</code>. We know that the queue is unbounded, so these methods never throw or block:</p><pre class="programlisting">private val messages = new LinkedBlockingQueue[String]
val logger = new Thread {
  setDaemon(true)
  override def run() = while (true) log(messages.take())
}
logger.start()
def logMessage(msg: String): Unit = messages.offer(msg)
</pre><p>We place these methods and the previously defined <code class="literal">prepareForDelete</code> method into the <code class="literal">FileSystem</code> class. To test this, we can simply instantiate our <code class="literal">FileSystem</code> class and call the <code class="literal">logMessage</code> method. Once the main thread terminates, the <code class="literal">logger</code> thread automatically stops:</p><pre class="programlisting">val fileSystem = new FileSystem(".")
fileSystem.logMessage("Testing log!")
</pre><p>An important difference between sequential queues and concurrent queues is that concurrent queues have <span class="strong"><strong>weakly consistent iterators</strong></span>. An iterator created with the <code class="literal">iterator</code> method traverses the elements that were in the queue at the moment the <code class="literal">iterator</code> method was created. However, if there is an enqueue or dequeue operation before the traversal is over, all bets are off, and the iterator might or might not reflect any modifications. Consider the following example, in which one thread traverses the concurrent queue while another thread dequeues its elements:</p><pre class="programlisting">object CollectionsIterators extends App {
  val queue = new LinkedBlockingQueue[String]
  for (i &lt;- 1 to 5500) queue.offer(i.toString)
  execute {
    val it = queue.iterator
    while (it.hasNext) log(it.next())
  }
  for (i &lt;- 1 to 5500) queue.poll()
  Thread.sleep(1000)
}
</pre><p>The main thread creates a queue with 5,500 elements. It then starts a concurrent task that creates an iterator and prints the elements one by one. At the same time, the main thread starts removing all the elements from the queue in the same order. In one of our thread runs, the iterator returns <code class="literal">1</code>, <code class="literal">4,779</code>, and <code class="literal">5,442</code>. This does not make sense, because the queue never contained these three elements alone; we would expect to see a suffix that has the range of <code class="literal">1</code> to <code class="literal">5,500</code>. We say that the iterator is not consistent. It is never corrupt and does not throw exceptions, but it fails to return a consistent set of elements that were in the queue at some point. With a few notable exceptions, this effect can happen when using any concurrent data structure.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip29"></a>Tip</h3><p>Use iterators on concurrent data structures only when you can ensure that no other thread will modify the data structure from the point where the iterator was created until the point where the iterator's <code class="literal">hasNext</code> method returns <code class="literal">false</code>.</p></div><p>The <code class="literal">CopyOnWriteArrayList</code> and <code class="literal">CopyOnWriteArraySet</code> collections in JDK are exceptions to this rule, but they copy the underlying data whenever the collection is mutated and can be slow. Later in this section, we will see a concurrent collection from the <code class="literal">scala.collection.concurrent</code> package called <code class="literal">TrieMap</code>, which creates consistent iterators without copying the underlying dataset and allows arbitrary modifications during the traversal.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec27"></a>Concurrent sets and maps</h3></div></div></div><p>Concurrent API designers strive to provide programmers with interfaces that resemble those from sequential programming. We have seen that this is the case with concurrent queues. As the main use case for concurrent queues is the producer-consumer pattern, the <code class="literal">BlockingQueue</code> interface additionally provides blocking versions of methods that are already known from sequential queues. Concurrent maps and concurrent sets are map and set collections, respectively, that can be safely accessed and modified by multiple threads. Like concurrent queues, they retain the API from the corresponding sequential collections. Unlike concurrent queues, they do not have blocking operations. The reason is that their principal use case is not the producer-consumer pattern, but encoding the program state.</p><p>The <code class="literal">concurrent.Map</code> trait in the <code class="literal">scala.collection</code> package represents different concurrent map implementations. In our filesystem API, we use it to track the files that exist in the filesystem as follows:</p><pre class="programlisting">val files: concurrent.Map[String, Entry]
</pre><p>This concurrent map contains paths and their corresponding <code class="literal">Entry</code> objects. These are the same <code class="literal">Entry</code> objects that <code class="literal">prepareForDelete</code> used earlier. The concurrent <code class="literal">files</code> map is populated when the <code class="literal">FileSystem</code> object is created.</p><p>For the examples in this section, we add the following dependency to our <code class="literal">build.sbt</code> file. This will allow us to use the Apache <code class="literal">Commons IO</code> library in order to handle files:</p><pre class="programlisting">libraryDependencies += "commons-io" % "commons-io" % "2.4"
</pre><p>We will allow <code class="literal">FileSystem</code> objects to only track files in a certain directory called <code class="literal">root</code>. By instantiating the <code class="literal">FileSystem</code> object with the <code class="literal">"."</code> string, we set the <code class="literal">root</code> directory to the root of our project with the example code. This way, the worst thing that can happen is that you delete all your examples by accident and have to rewrite them once more. However, that's okay, as practice makes perfect! The <code class="literal">FileSystem</code> class is shown in the following snippet:</p><pre class="programlisting">import scala.collection.convert.decorateAsScala._
import java.io.File
import org.apache.commons.io.FileUtils
class FileSystem(val root: String) {
  val rootDir = new File(root)
  val files: concurrent.Map[String, Entry] =
    new ConcurrentHashMap().asScala
  for (f &lt;- FileUtils.iterateFiles(rootDir, null, false).asScala)
  files.put(f.getName, new Entry(false))
}
</pre><p>We first create a new <code class="literal">ConcurrentHashMap</code> method from the <code class="literal">java.util.concurrent</code> package and wrap it to a Scala <code class="literal">concurrent.Map</code> trait by calling <code class="literal">asScala</code>. This method can be called to wrap most Java collections, provided the contents of the <code class="literal">decorateAsScala</code> object are imported like they are in our example. The <code class="literal">asScala</code> method ensures that Java collections obtain the Scala collection API. The <code class="literal">iterateFiles</code> method in the <code class="literal">FileUtils</code> class returns a Java iterator that traverses the files in a specific folder; we can only use Scala iterators in <code class="literal">for</code> comprehensions, so we call <code class="literal">asScala</code> again. The first argument for the <code class="literal">iterateFiles</code> method specifies the <code class="literal">root</code> folder, and the second method specifies an optional filter for the files. The final <code class="literal">false</code> argument for the <code class="literal">iterateFiles</code> method denotes that we do not scan files recursively in the subdirectories of <code class="literal">root</code>. We play it safe and expose only files in our <code class="literal">root</code> project directory to the <code class="literal">FileSystem</code> class. We place each <code class="literal">f</code> file along with a fresh <code class="literal">Entry</code> object into <code class="literal">files</code> by calling <code class="literal">put</code> on the concurrent map. There is no need to use a <code class="literal">synchronized</code> statement around <code class="literal">put</code>, as the concurrent map takes care of synchronization and thread-safety. The <code class="literal">put</code> operation is atomic, and it establishes a happens-before relationship with subsequent <code class="literal">get</code> operations.</p><p>The same is true for the other methods such as <code class="literal">remove</code>, which removes key-value pairs from a concurrent map. We can now use the <code class="literal">prepareForDelete</code> method implemented earlier to atomically lock a file for deletion and then remove it from the <code class="literal">files</code> map. We implement the <code class="literal">deleteFile</code> method for this purpose:</p><pre class="programlisting">def deleteFile(filename: String): Unit = {
  files.get(filename) match {
    case None =&gt;
      logMessage(s"Path '$filename' does not exist!")
    case Some(entry) if entry.isDir =&gt;
      logMessage(s"Path '$filename' is a directory!")
    case Some(entry) =&gt; execute {
      if (prepareForDelete(entry))
        if (FileUtils.deleteQuietly(new File(filename)))
          files.remove(filename)
    }
  }
}
</pre><p>If the <code class="literal">deleteFile</code> method finds that the concurrent map contains the file with the given name, it calls the <code class="literal">execute</code> method to asynchronously delete it, as we prefer not to block the caller thread. The concurrent task, started by the <code class="literal">execute</code> invocation, calls the <code class="literal">prepareForDelete</code> method. If the <code class="literal">prepareForDelete</code> method returns <code class="literal">true</code>, then it is safe to call the <code class="literal">deleteQuietly</code> method from the <code class="literal">Commons IO</code> library. This method physically removes the file from the disk. If the deletion is successful, the file entry is removed from the <code class="literal">files</code> map. We create a new file called <code class="literal">test.txt</code> and use it to test the <code class="literal">deleteFile</code> method. We prefer not to experiment with the build definition file. The following code shows the deletion of the file:</p><pre class="programlisting">fileSystem.deleteFile("test.txt")
</pre><p>The second time we run this line, our logger thread from before complains that the file does not exist. A quick check in our file manager reveals that theÂ <code class="literal">test.txt</code> file is no longer there.</p><p>The <code class="literal">concurrent.Map</code> trait also defines several complex linearizable methods. Recall that complex linearizable operations involve multiple reads and writes. In the context of concurrent maps, methods are complex linearizable operations if they involve multiple instances of theÂ <code class="literal">get</code> and <code class="literal">put</code> methods, but appear to get executed at a single point in time. Such methods are a powerful tool in our concurrency arsenal. We have already seen that volatile reads and writes do not allow us to implement the <code class="literal">getUniqueId</code> method; we need the <code class="literal">compareAndSet</code> method for that. Similar methods on concurrent maps have comparable advantages. Different atomic methods on atomic maps are summarized in the following table. Note that, unlike the CAS instruction, these methods use structural equality to compare keys and values, and they call the <code class="literal">equals</code> method.</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Signature</p>
</th><th style="border-bottom: 0.5pt solid ; ">
<p>Description</p>
</th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>putIfAbsent (k: K, Â v: V): Â  Â  Â  Â Option[V]</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This atomically assigns the value <code class="literal">v</code> to the key <code class="literal">k</code> if <code class="literal">k</code> is not in the map. Otherwise, it returns the value associated with <code class="literal">k</code>.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>remove (k: K, v: V):</p><p>
</p><p>Â  Boolean</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This atomically removes the key <code class="literal">k</code> if it is associated to the value equal to <code class="literal">v</code> and returns <code class="literal">true</code> if successful.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>replace (k: K, v: V):</p><p>
</p><p>Â  Option[V]</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>This atomically assigns the value <code class="literal">v</code> to the key <code class="literal">k</code> and returns the value previously associated with <code class="literal">k</code>.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; ">
<p>replace (k: K, ov: V, nv: V):</p><p>
</p><p>Â  Boolean</p>
</td><td style="">
<p>This atomically assigns the key <code class="literal">k</code> to the value <code class="literal">nv</code> if <code class="literal">k</code> was previously associated with <code class="literal">ov</code> and returns <code class="literal">true</code> if successful.</p>
</td></tr></tbody></table></div><p>Coming back to our filesystem API, let's see how these methods work to our advantage. We will now implement the <code class="literal">copyFile</code> method in the <code class="literal">FileSystem</code> class. Recall the diagram from the section on atomic variables. A copy operation can start only if the file is either in the <code class="literal">Idle</code> state or already in the <code class="literal">Copying</code> state, so we need to atomically switch the file state from <code class="literal">Idle</code> to <code class="literal">Copying</code> or from the <code class="literal">Copying</code> state to another <code class="literal">Copying</code> state with the value <code class="literal">n</code> incremented. We do this with the <code class="literal">acquire</code> method:</p><pre class="programlisting">@tailrec private def acquire(entry: Entry): Boolean = {
  val s0 = entry.state.get
  s0 match {
    case _: Creating | _: Deleting =&gt;
      logMessage("File inaccessible, cannot copy."); false
    case i: Idle =&gt;
      if (entry.state.compareAndSet(s0, new Copying(1))) true
      else acquire(entry)
    case c: Copying =&gt;
      if (entry.state.compareAndSet(s0, new Copying(c.n+1))) true
      else acquire(entry)
  }
}
</pre><p>After a thread completes copying a file, it needs to release the <code class="literal">Copying</code> lock. This is done by a similar <code class="literal">release</code> method, which decreases the <code class="literal">Copying</code> count or changes the state to <code class="literal">Idle</code>. Importantly, this method must be called after files are newly created in order to switch from the <code class="literal">Creating</code> state to the <code class="literal">Idle</code> state. By now, the retry pattern following unsuccessful CAS operations should be child's play for you. The following code shows this:</p><pre class="programlisting">@tailrec private def release(entry: Entry): Unit = {
  Val s0 = entry.state.get
  s0 match {
    case c: Creating =&gt;
      if (!entry.state.compareAndSet(s0, new Idle)) release(entry)
    case c: Copying =&gt;
      val nstate = if (c.n == 1) new Idle else new Copying(c.n-1)
      if (!entry.state.compareAndSet(s0, nstate)) release(entry)
  }
}
</pre><p>We now have all the machinery required to implement the <code class="literal">copyFile</code> method. This method checks whether an <code class="literal">src</code> entry exists in the <code class="literal">files</code> map. If the entry exists, the <code class="literal">copyFile</code> method starts a concurrent task to copy the file. The concurrent task attempts to acquire the file for copying and creates a new <code class="literal">destEntry</code> file entry in the <code class="literal">Creating</code> state. It then calls the <code class="literal">putIfAbsent</code> method, which atomically checks whether the file path <code class="literal">dest</code> is a key in the map and adds the <code class="literal">dest</code> and <code class="literal">destEntry</code> pair if it is not. Both theÂ <code class="literal">srcEntry</code> and <code class="literal">destEntry</code> value pair are locked at this point, so the <code class="literal">FileUtils.copyFile</code> method from the <code class="literal">Commons IO</code> library is called to copy the file on the disk. Once the copying is complete, both theÂ <code class="literal">srcEntry</code> and <code class="literal">destEntry</code>Â value pair are released:</p><pre class="programlisting">def copyFile(src: String, dest: String): Unit = {
  files.get(src) match {
    case Some(srcEntry) if !srcEntry.isDir =&gt; execute {
      if (acquire(srcEntry)) try {
        val destEntry = new Entry(isDir = false)
        destEntry.state.set(new Creating)
        if (files.putIfAbsent(dest, destEntry) == None) try {
          FileUtils.copyFile(new File(src), new File(dest))
        } finally release(destEntry)
      } finally release(srcEntry)
    }
  }
}
</pre><p>You should convince yourself that the <code class="literal">copyFile</code> method would be incorrect if it first called <code class="literal">get</code> to check whether <code class="literal">dest</code> is in the map and then called <code class="literal">put</code> to place <code class="literal">dest</code> in the map. This would allow another thread's <code class="literal">get</code> and <code class="literal">put</code> steps to interleave and potentially overwrite an entry in the <code class="literal">files</code> map. This demonstrates the importance of the <code class="literal">putIfAbsent</code> method.</p><p>There are some methods that the <code class="literal">concurrent.Map</code> trait inherits from the <code class="literal">mutable.Map</code> trait and that are not atomic. An example is theÂ <code class="literal">getOrElseUpdate</code> method, which retrieves an element if it is present in the map and updates it with a different element otherwise. This method is not atomic, while its individual steps are atomic; they can be interleaved arbitrarily with concurrent calls to the <code class="literal">getOrElseUpdate</code> method. Another example is <code class="literal">clear</code>, which does not have to be atomic on concurrent collections in general and can behave like the concurrent data structure iterators we studied before.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note30"></a>Note</h3><p>The<code class="literal">+=</code>, <code class="literal">-=</code>, <code class="literal">put</code>, <code class="literal">update</code>, <code class="literal">get</code>, <code class="literal">apply</code>, and <code class="literal">remove</code> methods in the <code class="literal">concurrent.Map</code> trait are linearizable methods. The <code class="literal">putIfAbsent</code>, conditional <code class="literal">remove</code>, and <code class="literal">replace</code> methods in the <code class="literal">concurrent.Map</code> trait are the only complex methods guaranteed to be linearizable.</p></div><p>Just like the Java concurrency library, Scala currently does not have a dedicated trait for concurrent sets. A concurrent set of the <code class="literal">Set[T]</code> type can be emulated with a concurrent map with the <code class="literal">ConcurrentMap[T, Unit]</code> type, which ignores the values assigned to keys. This is the reason why concrete concurrent set implementations appear less often in concurrency frameworks. In rare situations, where a Java concurrent set, such as the <code class="literal">ConcurrentSkipListSet[T]</code> class, needs to be converted to a Scala concurrent set, we can use the <code class="literal">asScala</code> method, which converts it to a <code class="literal">mutable.Set[T]</code> class.</p><p>As a final note, you should never use <code class="literal">null</code> as a key or value in a concurrent map or a concurrent set. Many concurrent data structure implementations on JVM rely on using <code class="literal">null</code> as a special indicator of the absence of an element.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip31"></a>Tip</h3><p>Avoid using the <code class="literal">null</code> value as a key or a value in a concurrent data structure.</p></div><p>Some implementations are defensive and will throw an exception; for others, the results might be undefined. Even when a concurrent collection specifies that <code class="literal">null</code> is allowed, you should avoid coupling <code class="literal">null</code> with your program logic in order to make potential refactoring easier.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec28"></a>Concurrent traversals</h3></div></div></div><p>As you had a chance to witness, Scala directly inherits many of its basic concurrency utilities from the Java concurrency packages. After all, these facilities were implemented by JVM's concurrency experts. Apart from providing conversions that make Java's traditional concurrency utilities feel Scala-idiomatic, there is no need to reinvent what's already there. When it comes to concurrent collections, a particularly bothersome limitation is that you cannot safely traverse most concurrent collections and modify them in the same time. This is not so problematic for sequential collections where we control the thread that calls theÂ <code class="literal">foreach</code> loop or uses iterators. In a concurrent system where threads are not perfectly synchronized with each other, it is much harder to guarantee that there will be no modifications during the traversal.</p><p>Fortunately, Scala has an answer for concurrent collection traversals. The <code class="literal">TrieMap</code> collection from the <code class="literal">scala.collection.concurrent</code> package, which is based on the concurrent <span class="strong"><strong>Ctrie</strong></span> data structure, is a concurrent map implementation that produces consistent iterators. When its <code class="literal">iterator</code> method is called, the <code class="literal">TrieMap</code> collection atomically takes a snapshot of all the elements. A <span class="strong"><strong>snapshot</strong></span> is complete information about the state of a data structure. The iterator then uses this snapshot to traverse the elements. If the <code class="literal">TrieMap</code> collection is later modified during the traversal, the modifications are not visible in the snapshot and the iterator does not reflect them. You might conclude that taking a snapshot is expensive and requires copying all the elements, but this is not the case. The <code class="literal">snapshot</code> method of the <code class="literal">TrieMap</code> class incrementally rebuilds parts of the <code class="literal">TrieMap</code> collection when they are first accessed by some thread. The <code class="literal">readOnlySnapshot</code> method, internally used by the <code class="literal">iterator</code> method, is even more efficient. It ensures that only the modified parts of the <code class="literal">TrieMap</code> collection are lazily copied. If there are no subsequent concurrent modifications, then no part of the <code class="literal">TrieMap</code> collection is ever copied.</p><p>Let's study the difference between the Java <code class="literal">ConcurrentHashMap</code> and the Scala <code class="literal">concurrent.TrieMap</code> collections in an example. Assume that we have a concurrent map that maps names to numerals in these names. For example, <code class="literal">"Jane"</code> will be mapped to <code class="literal">0</code>, but <code class="literal">"John"</code> will be mapped to <code class="literal">4</code>, and so on. In one concurrent task, we add different names for John in the order of <code class="literal">0</code> to <code class="literal">10</code> to the <code class="literal">ConcurrentHashMap</code> collection. We concurrently traverse the map and output these names:</p><pre class="programlisting">object CollectionsConcurrentMapBulk extends App {
  val names = new ConcurrentHashMap[String, Int]().asScala
  names("Johnny") = 0; names("Jane") = 0; names("Jack") = 0
  execute {
    for (n &lt;- 0 until 10) names(s"John $n") = n }
  execute {
    for (n &lt;- names) log(s"name: $n") }
  Thread.sleep(1000)
}
</pre><p>If the iterator was consistent, we would expect to see the three names <code class="literal">Johnny</code>, <code class="literal">Jane</code>, and <code class="literal">Jack</code> that were initially in the map and the name <code class="literal">John</code> in the interval from <code class="literal">0</code> to an <code class="literal">n</code> value, depending on how many names the first task added; this could be <code class="literal">John 1</code>, <code class="literal">John 2</code>, or <code class="literal">John 3</code>. Instead, the output shows you random nonconsecutive names such as <code class="literal">John 8</code> and <code class="literal">John 5</code>, which does not make sense. <code class="literal">John 8</code> should never appear in the map without <code class="literal">John 7</code>, and other entries inserted earlier by the other task. This never happens in a concurrent <code class="literal">TrieMap</code> collection. We can run the same experiment with the <code class="literal">TrieMap</code> collection and sort the names lexicographically before outputting them. Running the following program always prints all the <code class="literal">John</code> names in the interval of <code class="literal">0</code> and some value <code class="literal">n</code>:</p><pre class="programlisting">object CollectionsTrieMapBulk extends App {
  val names = new concurrent.TrieMap[String, Int]
  names("Janice") = 0; names("Jackie") = 0; names("Jill") = 0
  execute {for (n &lt;- 10 until 100) names(s"John $n") = n}
  execute {
    log("snapshot time!")
    for (n &lt;- names.map(_._1).toSeq.sorted) log(s"name: $n")
  }
}
</pre><p>How is this useful in practice? Imagine that we need to return a consistent snapshot of the filesystem; all the files are as seen by the file manager or an FTP server at a point in time. A <code class="literal">TrieMap</code> collection ensures that other threads that delete or copy files cannot interfere with the thread that is extracting the files. We thus use theÂ <code class="literal">TrieMap</code> collection to store files in our filesystem API and define a simple <code class="literal">allFiles</code> method that returns all the files. At the point where we start using the <code class="literal">files</code> map in a <code class="literal">for</code> comprehension, a snapshot with the filesystem contents is created:</p><pre class="programlisting">val files: concurrent.Map[String, Entry] = new concurrent.TrieMap()
def allFiles(): Iterable[String] = for ((name, state) &lt;- files) yield name
</pre><p>We use the <code class="literal">allFiles</code> method to display all the files in the <code class="literal">root</code> directory:</p><pre class="programlisting">val rootFiles = fileSystem.allFiles()
log("All files in the root dir: " + rootFiles.mkString(", "))
</pre><p>After having seen both these concurrent maps, you might be wondering about which one to use. This mainly depends on the use case. If the application requires consistent iterators, then you should definitely use the <code class="literal">TrieMap</code> collections. On the other hand, if the application does not require consistent iterators and rarely modifies the concurrent map, you can consider using <code class="literal">ConcurrentHashMap</code> collections, as their lookup operations are slightly faster.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip32"></a>Tip</h3><p>Use <code class="literal">TrieMap</code> collections if you require consistent iterators and <code class="literal">ConcurrentHashMap</code> collections when the <code class="literal">get</code> and <code class="literal">apply</code> operations are the bottlenecks in your program.</p></div><p>From a performance point of view, this tip is only applicable if your application is exclusively accessing a concurrent map all the time and doing nothing else. In practice, this is rarely the case, and in most situations, you can use either of these collections.</p></div></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec23"></a>Custom concurrent data structures</h2></div></div><hr /></div><p>In this section, we will show how to design a concurrent data structure. The data structure we will use as a running example will be simple, but sufficient to demonstrate the general approach. You will be able to apply the same principles to more complex data structures.</p><p>Before we start, there is a disclaimer. Designing a concurrent data structure is hard, and, as a rule of the thumb, you should almost never do it. Even if you manage to implement a correct and efficient concurrent data structure, the cost of doing so is usually high.</p><p>There are several reasons why designing a concurrent data structure is hard. The first is achieving correctness: errors are much harder to notice, reproduce, or analyze due to inherent non-determinism. Then, operations must not slow down when more processors use the data structure. In other words, the data structure must be scalable. Finally, a concurrent data structure must be efficient in absolute terms, and it must not be much slower than its sequential counterpart when used with a single processor.</p><p>That said, we proceed to designing a concrete data structure: a concurrent pool.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec29"></a>Implementing a lock-free concurrent pool</h3></div></div></div><p>In this section, we will implement a concurrent lock-free pool as an example of how to design a concurrent data structure. A <span class="strong"><strong>pool</strong></span> is one of the simplest data structure abstractions, and only has two methods--the <code class="literal">add</code> and the <code class="literal">remove</code> operations. The <code class="literal">add</code> operation simply adds an element into the pool, but the <code class="literal">remove</code> operation is more limited than in a set or a map of elements. Instead of removing a specific element from the pool, the <code class="literal">remove</code> operation removes any element, as long as the pool is non-empty. A lock-free pool is a pool whose operations are lock-free.</p><p>Although simple, the pool abstraction is very useful, as it allows temporarily storing expensive objects (for example, worker threads or database connectors). For this use-case, we do not care about which exact element the <code class="literal">remove</code> operation returns, as long as it returns some element.</p><p>Determining its operations is the first step in designing a concurrent data structure. Knowing the operations and their exact semantics drives the rest of the design, and adding supplementary operations later is likely to break the invariants of the data structure. It is usually hard to correctly extend a concurrent data structure once it has already been implemented.</p><p>Having determined the operations that a concurrent data structure must support, the next step is to think about data representation. Since we decided that the operations must be lock-free, one seemingly reasonable choice is to encode the state as an <code class="literal">AtomicReference</code> object holding a pointer to an immutable list:</p><pre class="programlisting">val pool = new AtomicReference[List[T]]
</pre><p>Both, theÂ <code class="literal">add</code> and <code class="literal">remove</code> operations follow naturally from this choice. To add an element, we read the old list, use it to append the element at the head of the list, and then invoke a <code class="literal">compareAndSet</code> operation to replace the old list, retrying if necessary. Elements would be removed in a similar fashion.</p><p>However, such an implementation would not be very scalable. Multiple processors would need to access the same memory location, and retrying would occur frequently. The expected time to complete the operation would then be <span class="emphasis"><em>O(P)</em></span>, where <span class="emphasis"><em>P</em></span> is the number of processors that are concurrently executing <code class="literal">add</code> and <code class="literal">remove</code> operations.</p><p>To improve this, we will need to allow different processors to pick different memory locations in the data structure when updating it. The fact that we are implementing a pool mitigates this decision, since the <code class="literal">remove</code> operation will not have to search for specific elements, and just needs to return any element. Therefore, the <code class="literal">add</code> operation can append the element to any location in the data structure.</p><p>With this in mind, we choose an array of atomic references, each holding an immutable list, as our internal representation. Having many atomic references allows each processor to pick an arbitrary slot to perform the update. This is shown in the following snippet:</p><pre class="programlisting">class Pool[T] {
  val parallelism = Runtime.getRuntime.availableProcessors * 32
  val buckets =
    new Array[AtomicReference[(List[T], Long)]](parallelism)
  for (i &lt;- 0 until buckets.length)
    buckets(i) = new AtomicReference((Nil, 0L))
</pre><p>Note that each atomic reference holds not only a list of values in the respective bucket, but also a <code class="literal">Long</code> value. This unique numeric value will serve as a timestamp that must be incremented each time the bucket is modified. Before we see why having the timestamp is important, we will implement the <code class="literal">add</code> operation.</p><p>The <code class="literal">add</code> operation must pick one of the atomic references in the <code class="literal">buckets</code> array, create a new version of the list that contains the new element, and then invoke the CAS instruction until the respective atomic reference is updated. When picking a bucket, the processor must aim for a bucket that no other processor is currently using, to prevent contention and retries. There are many ways to achieve this, but we will settle for a relatively simple strategy--we compute the bucket from the thread ID, and the hash code of the element. Once the bucket is picked, the <code class="literal">add</code> operation follows the standard retry pattern that we saw earlier. This is shown in the following snippet:</p><pre class="programlisting">  def add(x: T): Unit = {
    val i =
      (Thread.currentThread.getId ^ x.## % buckets.length).toInt
    @tailrec def retry() {
      val bucket = buckets(i)
      val v = bucket.get
      val (lst, stamp) = v
      val nlst = x :: lst
      val nstamp = stamp + 1
      val nv = (nlst, nstamp)
      if (!bucket.compareAndSet(v, nv)) retry()
    }
    retry()
  }
</pre><p>The <code class="literal">remove</code> operation is more complex. Unlike the <code class="literal">add</code> operation, which can pick any bucket when inserting an element, the <code class="literal">remove</code> operation must pick a non-empty bucket. The current design of the data structure offers no apriori way of knowing which bucket is non-empty, so the best we can do is pick some bucket, and scan the other buckets linearly until finding a non-empty bucket. This has two consequences. First, if our concurrent pool is nearly empty, we will need to scan all the buckets in the worst case scenario. The <code class="literal">remove</code> operation is only scalable if the pool is relatively full. Second, when the pool is almost empty, it is impossible to atomically scan all the entries. It can happen that, during the scan, one thread inserts an element to a bucket we already traversed, and another thread removes an element from a non-traversed bucket. In this case, the <code class="literal">remove</code> operation could falsely conclude that the pool is empty, which was never the case.</p><p>To address the second issue, we use the timestamps associated with each bucket. Recall that each timestamp is incremented when the respective bucket is modified. Therefore, if the sum of the timestamps remains constant, then no operation was executed on the pool. We can use this fact as follows. If we scan the bucket array twice, and see that the timestamp sum did not change, we can conclude that there have been no updates to the pool. This is crucial for the <code class="literal">remove</code> operation, which will use this information to know when to terminate.</p><p>The <code class="literal">remove</code> operation starts by picking a bucket based on the current thread ID, and then starting a tail-recursive <code class="literal">scan</code> method. The <code class="literal">scan</code> method traverses the array, searching for non-empty buckets. When an empty bucket is observed, its timestamp is added to the <code class="literal">sum</code> local variable. When a non-empty bucket is found, the standard CAS pattern attempts to remove an element from the bucket in the <code class="literal">retry</code> method. If successful, the element is immediately removed from the <code class="literal">remove</code> operation. Otherwise, if upon traversing the array the previous timestamp sum is equal to the current sum, the <code class="literal">scan</code> method terminates. This is shown in the following snippet:</p><pre class="programlisting">  def remove(): Option[T] = {
    val start =
      (Thread.currentThread.getId % buckets.length).toInt
    @tailrec def scan(witness: Long): Option[T] = {
      var i = (start + 1) % buckets.length
      var sum = 0L
      while (i != start) {
        val bucket = buckets(i)

        @tailrec def retry(): Option[T] = {
          bucket.get match {
            case (Nil, stamp) =&gt;
              sum += stamp
              None
            case v @ (lst, stamp) =&gt;
              val nv = (lst.tail, stamp + 1)
              if (bucket.compareAndSet(v, nv)) Some(lst.head)
              else retry()
          }
        }
        retry() match {
          case Some(v) =&gt; return Some(v)
          case None =&gt;
        }

        i = (i + 1) % buckets.length
      }
      if (sum == witness) None
      else scan(sum)
    }
    scan(-1L)
  }
}
</pre><p>We test the concurrent pool as follows. First, we instantiate a concurrent hash map that will track the elements we removed. Then, we create a concurrent pool, and set the number of threads <code class="literal">p</code> and the number of elements <code class="literal">num</code>:</p><pre class="programlisting">val check = new ConcurrentHashMap[Int, Unit]()
val pool = new Pool[Int]
val p = 8
val num = 1000000
</pre><p>We first start <code class="literal">p</code> inserter threads, which insert non-overlapping ranges of integers into the pool. We then wait for the threads to complete:</p><pre class="programlisting">val inserters = for (i &lt;- 0 until p) yield ch2.thread {
  for (j &lt;- 0 until num) pool.add(i * num + j)
}
inserters.foreach(_.join())
</pre><p>We similarly start <code class="literal">p</code> remover threads, which remove the elements from the pool, and store the removed elements to the <code class="literal">check</code> hash map we created earlier. Each thread removes <code class="literal">num</code> elements, so the pool should never be empty until all the threads complete:</p><pre class="programlisting">val removers = for (i &lt;- 0 until p) yield ch2.thread {
  for (j &lt;- 0 until num) {
    pool.remove() match {
      case Some(v) =&gt; check.put(v, ())
      case None =&gt; sys.error("Should be non-empty.")
    }
  }
}
removers.foreach(_.join())
</pre><p>At the end, we sequentially traverse the elements we expect to see in the <code class="literal">check</code> hash map, and assert that they are contained, as shown in the following snippet:</p><pre class="programlisting">for (i &lt;- 0 until (num * p)) assert(check.containsKey(i))
</pre><p>And this is it! We have verified that our concurrent pool implementation works correctly. Although we will not prove this, we loosely claim that the <code class="literal">add</code> operation runs in the expected <span class="emphasis"><em>O(1)</em></span> time, the <code class="literal">remove</code> operation runs in the expected <span class="emphasis"><em>O(1)</em></span> time when the pool has enoughÂ elements, and in the expected <span class="emphasis"><em>O(P)</em></span> time when the queue is nearly empty. As an exercise, you can try to improve the <code class="literal">remove</code> operation, so that it always runs in the expected <span class="emphasis"><em>O(1)</em></span> time.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec30"></a>Creating and handling processes</h3></div></div></div><p>So far, we focused on concurrency within a Scala program running in a single JVM process. Whenever we wanted to allow multiple computations to proceed concurrently, we created new threads or sent <code class="literal">Runnable</code> objects to <code class="literal">Executor</code> threads. Another route to concurrency is to create separate processes. As explained in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Concurrency on the JVM and the Java Memory Model</em></span>, separate processes have separate memory spaces and cannot share the memory directly.</p><p>There are several reasons why we occasionally want to do this. First, while JVM has a very rich ecosystem with thousands of software libraries for all kinds of tasks, sometimes the only available implementation of a certain software component is a command-line utility or prepackaged program. Running it in a new process could be the only way to harvest its functionality. Second, sometimes we want to put Scala or Java code that we do not trust in a sandbox. A third-party plugin might have to run with a reduced set of permissions. Third, sometimes we just don't want to run in the same JVM process for performance reasons. Garbage collection or JIT compilation in a separate process should not affect the execution of our process, given that the machine has sufficient CPUs.</p><p>The <code class="literal">scala.sys.process</code> package contains a concise API for dealing with other processes. We can run the child process synchronously, in which case the thread from the parent process that runs it waits until the child process terminates, or asynchronously, in which case, the child process runs concurrently with the calling thread from the parent process. We will first show you how to run a new process synchronously:</p><pre class="programlisting">import scala.sys.process._
object ProcessRun extends App {
  val command = "ls"
  val exitcode = command.!
  log(s"command exited with status $exitcode")
}
</pre><p>Importing the contents of the <code class="literal">scala.sys.process</code> package allows us to call the <code class="literal">!</code> method on any string. The shell command represented by the string is then run from the working directory of the current process. The return value is the exit code of the new process--zero when the process exits successfully and a nonzero error code otherwise.</p><p>Sometimes, we are interested in the standard output of a process rather than its exit code. In this case, we start the process with the<code class="literal">!!</code> method. Let's assume that we want a <code class="literal">lineCount</code> method for text files in <code class="literal">FileSystem</code>, but are too lazy to implement it from scratch:</p><pre class="programlisting">def lineCount(filename: String): Int = {
  val output = s"wc $filename".!!
  output.trim.split(" ").head.toInt
}
</pre><p>After removing the white space from the output with the <code class="literal">trim</code> method on <code class="literal">String</code> type and converting the first part of the output to an integer, we obtain the word count of a file.</p><p>To start the process asynchronously, we call the <code class="literal">run</code> method on a string that represents the command. This method returns a <code class="literal">Process</code> object with the <code class="literal">exitValue</code> method, which is blocked until the process terminates, and the <code class="literal">destroy</code> method, which stops the process immediately. Assume that we have a potentially long-running process that lists all the files in our filesystem. After one second, we might wish to stop it by calling the <code class="literal">destroy</code> method on the <code class="literal">Process</code> object:</p><pre class="programlisting">object ProcessAsync extends App {
  val lsProcess = "ls -R /".run()
  Thread.sleep(1000)
  log("Timeout - killing ls!")
  lsProcess.destroy()
}
</pre><p>Overloads of the <code class="literal">run</code> method allow you to communicate with the process by hooking the custom input and output streams or providing a custom <code class="literal">logger</code> object that is called each time the new process outputs a line.</p><p>The <code class="literal">scala.sys.process</code> API has additional features such as starting multiple processes and piping their outputs together, running a different process if the current process fails, or redirecting the output to a file. It strives to mimic much of the functionality provided by the Unix shells. For complete information, we refer the reader to the Scala standard library's documentation of the <code class="literal">scala.sys.process</code> package.</p></div></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec24"></a>Summary</h2></div></div><hr /></div><p>This chapter presented the traditional building blocks of concurrent programs in Scala. We saw how to use <code class="literal">Executor</code> objects to run concurrent computations. We learned how to use atomic primitives to atomically switch between different states in the program and implement locks and lock-free algorithms. We studied the implementation of lazy values and their impact on concurrent programs. We then showed you important classes of concurrent collections and learned how to apply them in practice, and we concluded by visiting the <code class="literal">scala.sys.process</code> package. These insights are not only specific to Scala; but most languages and platforms also have concurrency utilities that are similar to the ones presented in this chapter.</p><p>Many other Java concurrency APIs are thoroughly explained in the book <span class="emphasis"><em>Java Concurrency in Practice</em></span>, by Brian Goetz, Tim Peierls, Joshua Bloch, Joseph Bowbeer, David Holmes, and Doug Lea. To learn more about concepts such as lock-freedom, atomic variables, various types of locks, or concurrent data structures, we recommend the book <span class="emphasis"><em>The Art of Multiprocessor Programming</em></span> by Maurice Herlihy and Nir Shavit, Morgan Kaufmann.</p><p>Although the concurrency building blocks in this chapter are more high level than the basic concurrency primitives of <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Concurrency on the JVM and the Java Memory Model</em></span>, there are still culprits lurking at every corner. We had to be careful not to block when running on the execution context, to steer clear from the ABA problem, avoid synchronizing on objects that use lazy values, and ensure that concurrent collections are not modified while using their iterators. All this imposes quite a burden on the programmer. Couldn't concurrent programming be simpler? Fortunately, the answer is yes, as Scala supports styles of expressing concurrency that are more high level and declarative; less prone to effects such as deadlocks, starvation, or non-determinism; and generally easier to reason about. In the following chapters, we will dive into Scala-specific concurrency APIs that are safer and more intuitive to use. We will start by studying futures and promises in the next chapter, which allow you to compose asynchronous computations in a thread-safe and intuitive way.</p></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec25"></a>Exercises</h2></div></div><hr /></div><p>The following exercises cover the various topics from this chapter. Most of the exercises require implementing new concurrent data structures using atomic variables and the CAS instruction. These data structures can also be solved using the <code class="literal">synchronized</code> statement, so it is helpful to contrast the advantages of the two approaches:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Implement a custom <code class="literal">ExecutionContext</code> class called <code class="literal">PiggybackContext</code>, which executes <code class="literal">Runnable</code> objects on the same thread that calls theÂ <code class="literal">execute</code> method. Ensure that a <code class="literal">Runnable</code> object executing on the <code class="literal">PiggybackContext</code> can also call the <code class="literal">execute</code> method and that exceptions are properly reported.</p></li><li><p>Implement a <code class="literal">TreiberStack</code> class, which implements a concurrent stack abstraction:</p><pre class="programlisting">            class TreiberStack[T] {
              def push(x: T): Unit = ???
              def pop(): T = ???
            }
</pre><p>Use an atomic reference variable that points to a linked list of nodes that were previously pushed to the stack. Make sure that your implementation is lock-free and not susceptible to the ABA problem.</p></li><li><p>Implement a <code class="literal">ConcurrentSortedList</code> class, which implements a concurrent sorted list abstraction:</p><pre class="programlisting">            class ConcurrentSortedList[T](implicit val ord: Ordering[T]) {
               def add(x: T): Unit = ???
               def iterator: Iterator[T] = ???
            }
</pre><p>Under the hood, the <code class="literal">ConcurrentSortedList</code> class should use a linked list of atomic references. Ensure that your implementation is lock-free and avoids ABA problems.</p><p>The <code class="literal">Iterator</code> object returned by the <code class="literal">iterator</code> method must correctly traverse the elements of the list in ascending order under the assumption that there are no concurrent invocations of the <code class="literal">add</code> method.</p></li><li><p>If required, modify the <code class="literal">ConcurrentSortedList</code> class from the previous example so that calling the <code class="literal">add</code> method has the running time linear to the length of the list and creates a constant number of new objects when there are no retries due to concurrent <code class="literal">add</code> invocations.</p></li><li><p>Implement a <code class="literal">LazyCell</code> class with the following interface:</p><pre class="programlisting">            class LazyCell[T](initialization: =&gt;T) {
              def apply(): T = ???
            }
</pre><p>Creating a <code class="literal">LazyCell</code> object and calling the <code class="literal">apply</code> method must have the same semantics as declaring a lazy value and reading it, respectively.</p><p>You are not allowed to use lazy values in your implementation.</p></li><li><p>Implement a <code class="literal">PureLazyCell</code> class with the same interface and semantics as the <code class="literal">LazyCell</code> class from the previous exercise. The <code class="literal">PureLazyCell</code> class assumes that the initialization parameter does not cause side effects, so it can be evaluated more than once.</p><p>The <code class="literal">apply</code> method must be lock-free and should call the initialization as little as possible.</p></li><li><p>Implement a <code class="literal">SyncConcurrentMap</code> class that extends the <code class="literal">Map</code> interface from the <code class="literal">scala.collection.concurrent</code> package. Use the <code class="literal">synchronized</code> statement to protect the state of the concurrent map.</p></li><li><p>Implement a method <code class="literal">spawn</code> that, given a block of Scala code, starts a new JVM process and runs the specified block in the new process:</p><pre class="programlisting">            def spawn[T](block: =&gt;T): T = ???
</pre><p>Once the block returns a value, the <code class="literal">spawn</code> method should return the value from the child process. If the block throws an exception, the <code class="literal">spawn</code> method should throw the same exception.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip33"></a>Tip</h3><p>Use Java serialization to transfer the block of code, its return value, and the potential exceptions between the parent and the child JVM processes.</p></div></li><li><p>Augment the lock-free pool implementation from this chapter with a <code class="literal">foreach</code> operation, used to traverse all the elements in the pool. Then make another version of <code class="literal">foreach</code> that is both lock-free and linearizable.</p></li><li><p>Prove that the lock-free pool implementation from this chapter is correct.</p></li><li><p>Currently, the <code class="literal">remove</code> operation of the lock-free pool implementation from this chapter runs in <span class="emphasis"><em>O(P)</em></span> worst-case time, where <span class="emphasis"><em>P</em></span> is the number of processors on the machine. Improve the lock-free pool implementation so that the operations run in <span class="emphasis"><em>O(1)</em></span> expected time, both in terms of the number of stored elements and the number of processors.</p></li></ol></div></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch04"></a>ChapterÂ 4.Â    Asynchronous Programming with Futures and Promises   </h2></div></div></div><div class="blockquote"><table border="0" width="100%" cellspacing="0" cellpadding="0" class="blockquote" summary="Block quote"><tr><td width="10%" valign="top">Â </td><td width="80%" valign="top"><p><span class="emphasis"><em>Programming in a functional style makes the state presented to your code explicit, which makes it much easier to reason about, and, in a completely pure system, makes thread race conditions impossible.</em></span></p></td><td width="10%" valign="top">Â </td></tr><tr><td width="10%" valign="top">Â </td><td colspan="2" align="right" valign="top">--<span class="attribution"><span class="emphasis"><em>John Carmack</em></span></span></td></tr></table></div><p>In the examples of the previous chapters, we often dealt with blocking computations. We have seen that blocking synchronization can have negative effects: it can cause deadlocks, starve thread pools, or break lazy value initialization. While, in some cases, blocking is the right tool for the job, in many cases we can avoid it. Asynchronous programming refers to the programming style in which executions occur independently of the main program flow. Asynchronous programming helps you to eliminate blocking instead of suspending the thread whenever a resource is not available; a separate computation is scheduled to proceed once the resource becomes available.</p><p>In a way, many of the concurrency patterns seen so far support asynchronous programming; thread creation and scheduling execution context tasks can be used to start executing a computation concurrent to the main program flow. Still, usingÂ these facilities directly when avoiding blocking or composing asynchronous computations is not straightforward. In this chapter, we will focus on two abstractions in Scala that are specifically tailored for this task: futures and promises.</p><p>More specifically, we will study the following topics:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Starting asynchronous computations, and using <code class="literal">Future</code> objects</p></li><li style="list-style-type: disc"><p>Installing callbacks that handle the results of asynchronous computations</p></li><li style="list-style-type: disc"><p>Exception semantics of <code class="literal">Future</code> objects, and using the typeÂ <code class="literal">Try</code>Â </p></li><li style="list-style-type: disc"><p>Functional composition of <code class="literal">Future</code> objects</p></li><li style="list-style-type: disc"><p>Using <code class="literal">Promise</code> objects to interface with callback-based APIs, implement future combinators, and support cancellation</p></li><li style="list-style-type: disc"><p>Blocking threads inside asynchronous computations</p></li><li style="list-style-type: disc"><p>Using the Scala <code class="literal">Async</code> library</p></li></ul></div><p>In the next section, we will start by introducing the tryÂ <code class="literal">Future</code>Â , and show why it is useful.</p></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec26"></a>Futures</h2></div></div><hr /></div><p>In earlier chapters, we learned that parallel executions in a concurrent program proceed on entities called <span class="strong"><strong>threads</strong></span>. At any point, the execution of a thread can be temporarily suspended, until a specific condition is fulfilled. When this happens, we say that the thread is blocked. Why do we block threads in the first place in concurrent programming? One of the reasons is that we have a finite amount of resources; multiple computations that share these resources sometimes need to wait. In other situations, a computation needs specific data to proceed, and if that data is not yet available, threads responsible for producing the data could be slow or the source of the data could be external to the program. A classic example is waiting for the data to arrive over the network. Let's assume that we have a <code class="literal">getWebpage</code> method, that given a <code class="literal">url</code> string with the location of the webpage, returns that webpage's contents:</p><pre class="programlisting">def getWebpage(url: String): String 
</pre><p>The return type of the <code class="literal">getWebpage</code> method is <code class="literal">String</code>; the method must return a string with the webpage's contents. Upon sending an HTTP request, though, the webpage's contents are not available immediately. It takes some time for the request to travel over the network to the server and back before the program can access the document. The only way for the method to return the contents of the webpage as a string value is to wait for the HTTP response to arrive. However, this can take a relatively long time from the program's point of view; even with a high-speed Internet connection, the <code class="literal">getWebpage</code> method needs to wait. Since the thread that called the <code class="literal">getWebpage</code> method cannot proceed without the contents of the webpage, it needs to pause its execution; therefore, the only way to correctly implement the <code class="literal">getWebpage</code> method is to block.</p><p>We already know that blocking can have negative side-effects, so can we change the return value of the <code class="literal">getWebpage</code> method to some special value that can be returned immediately? The answer is yes. In Scala, this special value is called a <span class="strong"><strong>future</strong></span>. AÂ future is a placeholder, that is, a memory location for the value. This placeholder does not need to contain a value when the future is created; the value can be placed into the future eventually by the <code class="literal">getWebpage</code> method. We can change the signature of the <code class="literal">getWebpage</code> method to return a future as follows:</p><pre class="programlisting">def getWebpage(url: String): Future[String] 
</pre><p>Here, the <code class="literal">Future[String]</code> type means that the future object can eventually contain a <code class="literal">String</code> value. We can now implement the <code class="literal">getWebpage</code> method without blocking-we can start the HTTP request asynchronously and place the webpage's contents into the future when they become available. When this happens, we say that the <code class="literal">getWebpage</code> method completes the future. Importantly, after the future is completed with some value, that value can no longer change.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note34"></a>Note</h3><p>The <code class="literal">Future[T]</code> type encodes latency in the program; use it to encode values that will become available later during execution.</p></div><p>This removes blocking from the <code class="literal">getWebpage</code> method, but it is not clear how the calling thread can extract the content of the future. Polling is one non-blocking way of extracting the content. In the polling approach, the calling thread calls a special method to block until the value becomes available. While this approach does not eliminate blocking, it transfers the responsibility of blocking from the <code class="literal">getWebpage</code> method to the caller thread. Java defines its own <code class="literal">Future</code> type to encode values that will become available later. However, as a Scala developer, you should use Scala's futures instead; they allow additional ways of handling future values and avoid blocking, as we will soon see.</p><p>When programming with futures in Scala, we need to distinguish between <span class="strong"><strong>future values</strong></span> and <span class="strong"><strong>future computations</strong></span>. A future value of the typeÂ <code class="literal">Future[T]</code>Â  denotes some value of typeÂ <code class="literal">T</code>Â  in the program that might not be currently available, but could become available later. Usually, when we say a future, we really mean a future value. In the <code class="literal">scala.concurrent</code> package, futures are represented with the <code class="literal">Future[T]</code> trait:</p><pre class="programlisting">trait Future[T] 
</pre><p>By contrast, a future computation is an asynchronous computation that produces a future value. A future computation can be started by calling the <code class="literal">apply</code> method on the <code class="literal">Future</code> companion object. This method has the following signature in the <code class="literal">scala.concurrent</code> package:</p><pre class="programlisting">def apply[T](b: =&gt;T)(implicit e: ExecutionContext): Future[T] 
</pre><p>This method takes a by-name parameter of the typeÂ <code class="literal">T</code>Â . This is the body of the asynchronous computation that results in some value of type <code class="literal">T</code>. It also takes an implicit <code class="literal">ExecutionContext</code> parameter, which abstracts over where and when the thread gets executed, as we learned in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Traditional Building Blocks of Concurrency</em></span>. Recall that Scala's implicit parameters can either be specified when calling a method, in the same way as normal parameters, or they can be left out-in this case, the Scala compiler searches for a value of the <code class="literal">ExecutionContext</code> type in the surrounding scope. Most <code class="literal">Future</code> methods take an implicit execution context. Finally, the <code class="literal">Future.apply</code> method returns a future of the type <code class="literal">T</code>. This future is completed with the value resulting from the asynchronous computation, <code class="literal">b</code>.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec31"></a>Starting future computations</h3></div></div></div><p>Let's see how to start a future computation in an example. We first import the contents of the <code class="literal">scala.concurrent</code> package. We then import the <code class="literal">global</code> execution context from the <code class="literal">Implicits</code> object. This makes sure that future computations execute on <code class="literal">global</code>, the default execution context you can use in most cases:</p><pre class="programlisting">import scala.concurrent._ 
import ExecutionContext.Implicits.global 
object FuturesCreate extends App { 
  Future { log("the future is here") } 
  log("the future is coming") 
  Thread.sleep(1000) 
} 
</pre><p>The order in which the <code class="literal">log</code> method calls (in the future computation and the main thread) execute is nondeterministic. The <code class="literal">Future</code> singleton object followed by a block is syntactic sugar for calling the <code class="literal">Future.apply</code> method. The <code class="literal">Future.apply</code> method acts similarly to the <code class="literal">execute</code> statement from <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Traditional Building Blocks of Concurrency</em></span>. The difference is that the <code class="literal">Future.apply</code> method returns a future value. We can poll this future value until it is completed. In the following example, we can use the <code class="literal">scala.io.Source</code> object to read the contents of our <code class="literal">build.sbt</code> file in a future computation. The main thread calls the <code class="literal">isCompleted</code> method on the <code class="literal">buildFile</code> future value, returned from the future computation. Chances are that the build file was not read so rapid, so <code class="literal">isCompleted</code> returns <code class="literal">false</code>. After 250 milliseconds, the main thread calls <code class="literal">isCompleted</code> again, and this time <code class="literal">isCompleted</code> returns <code class="literal">true</code>. Finally, the main thread calls the <code class="literal">value</code> method, which returns the contents of the build file:</p><pre class="programlisting">import scala.io.Source 
object FuturesDataType extends App { 
  val buildFile: Future[String] = Future { 
    val f = Source.fromFile("build.sbt") 
    try f.getLines.mkString("\n") finally f.close() 
  } 
  log(s"started reading the build file asynchronously") 
  log(s"status: ${buildFile.isCompleted}") 
  Thread.sleep(250) 
  log(s"status: ${buildFile.isCompleted}") 
  log(s"value: ${buildFile.value}") 
} 
</pre><p>In this example, we used polling to obtain the value of the future. The <code class="literal">Future</code> singleton object's polling methods are non-blocking, but they are also nondeterministic; <code class="literal">isCompleted</code> will repeatedly return <code class="literal">false</code> until the future is completed. Importantly, completion of the future is in a happens-before relationship with the polling calls. If the future completes before the invocation of the polling method, then its effects are visible to the thread after polling completes.</p><p>Shown graphically, polling looks like the following figure:</p><div class="mediaobject"><img src="graphics/image_04_001.jpg" /></div><p>Polling diagram</p><p>Polling is like calling your potential employer every five minutes to ask if you're hired. What you really want to do is hand in a job application and then apply for other jobs, instead of busy-waiting for the employer's response. Once your employer decides to hire you, they will give you a call on the phone number you left them. We want futures to do the same; when they are completed, they should call a specific function we left for them. This is the topic of the next section.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec32"></a>Future callbacks</h3></div></div></div><p>A callback is a function that is called once its arguments become available. When a Scala future takes a callback, it eventually calls that callback. However, the future does not call the callback before this future is completed with some value.</p><p>Let's assume that we need to look up details of the URL specification from the W3 consortium. We are interested in all the occurrences of the <code class="literal">telnet</code> keyword. The URL specification is available as a text document atÂ <a class="ulink" href="https://www.w3.org/" target="_blank">https://www.w3.org/</a>. We can use the <code class="literal">scala.io.Source</code> object to fetch the contents of the specification, and use futures in the <code class="literal">getUrlSpec</code> method to asynchronously execute the HTTP request. The <code class="literal">getUrlSpec</code> method first calls the <code class="literal">fromURL</code> method to obtain a <code class="literal">Source</code> object with the text document. It then calls <code class="literal">getLines</code> to get a list of separate lines in the document:</p><pre class="programlisting">object FuturesCallbacks extends App { 
  def getUrlSpec(): Future[List[String]] = Future { 
    val url = "http://www.w3.org/Addressing/URL/url-spec.txt" 
    val f = Source.fromURL(url) 
    try f.getLines.toList finally f.close() 
  } 
  val urlSpec: Future[List[String]] = getUrlSpec() 
</pre><p>To find the lines in the <code class="literal">urlSpec</code> future that contains the <code class="literal">telnet</code> keyword, we use the <code class="literal">find</code> method whichÂ takes a list of lines and a keyword and returns a string containing the matches:</p><pre class="programlisting">  def find(lines: List[String], keyword: String): String = lines.zipWithIndex collect { 
      case (line, n) if line.contains(keyword) =&gt; (n, line) 
    } mkString("\n") 
</pre><p>The <code class="literal">find</code> method takes a <code class="literal">List[String]</code> parameter, but <code class="literal">urlSpec</code> is of the <code class="literal">Future[List[String]]</code> type. We cannot pass the <code class="literal">urlSpec</code> future directly to the <code class="literal">find</code> method; and for a good reason, the value might not be available at the time when we call the <code class="literal">find</code> method.</p><p>Instead, we install a callback to the future using the <code class="literal">foreach</code> method. Note that the equivalent of the <code class="literal">foreach</code> method is the <code class="literal">onSuccess</code> method, but it might be deprecated after Scala 2.11. This method takes a partial function that, given a value of the future, performs some action, as follows:</p><pre class="programlisting">  urlSpec foreach { 
    case lines =&gt; log(find(lines, "telnet")) 
  } 
  log("callback registered, continuing with other work") 
  Thread.sleep(2000) 
</pre><p>Importantly, installing a callback is a non-blocking operation. The <code class="literal">log</code> statement in the main thread immediately executes after the callback is registered, but the <code class="literal">log</code> statement in the callback can be called much later. This is illustrated in the following figure:</p><div class="mediaobject"><img src="graphics/image_04_002.jpg" /></div><p>Callback diagram</p><p>Note that the callback is not necessarily invoked immediately after the future is completed. Most execution contexts schedule a task to asynchronously process the callbacks. The same is true if the future is already completed when we try to install a callback.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note35"></a>Note</h3><p>After the future is completed, the callback is called <span class="emphasis"><em>eventually</em></span> and independently from other callbacks on the same future. The specified execution context decides when and on which thread the callback gets executed.</p><p>There is a happens-before relationship between completing the future and starting the callback.</p></div><p>We are not limited to installing a single callback to the future. If we additionally want to find all the occurrences of the <code class="literal">password</code> keyword, we can install another callback:</p><pre class="programlisting">  urlSpec foreach { 
    case lines =&gt; log(find(lines, "password")) 
  } 
  Thread.sleep(1000) 
} 
</pre><p>As an experienced Scala programmer, you might have heard about referential transparency. Roughly speaking, a function is referentially transparent if it does not execute any side effects such as variable assignment, modifying mutable collections, or writing to the standard output. Callbacks on futures have one very useful property. Programs using only the <code class="literal">Future.apply</code> and <code class="literal">foreach</code> calls with referentially transparent callbacks are deterministic. For the same inputs, such programs will always compute the same results.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note36"></a>Note</h3><p>Programs composed from referentially transparent future computations and callbacks are deterministic.</p></div><p>In the examples so far, we assumed that an asynchronous computation yielding a future always succeeds. However, computations occasionally fail and throw exceptions. We will study how to handle failures in asynchronous computations next.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec33"></a>Futures and exceptions</h3></div></div></div><p>If a future computation throws an exception, then its corresponding future object cannot be completed with a value. Ideally, we would like to be notified when this happens. If you apply for a job and the employer decides to hire someone else, you would still like to receive a phone call. Otherwise, you might spend days sitting idly in front of your phone, waiting for the call from the recruiter.</p><p>When a Scala future is completed, it can either be completed <span class="emphasis"><em>successfully</em></span> or with a failure. When a future is completed with a failure, we also say that a future has <span class="emphasis"><em>failed</em></span>. To summarize all the different states of a future, we show the following state diagram. A future is created without any associated callbacks. Then, any number of callbacks <code class="literal">f1</code>, <code class="literal">f2</code>, ..., <code class="literal">fn</code> can be assigned to it. When the future is completed, it has either completed successfully or has failed. After that, the future's state no longer changes, and registering a callback immediately schedules it for execution.</p><div class="mediaobject"><img src="graphics/image_04_003.jpg" /></div><p>We now take a closer look at handling the failure case. The <code class="literal">foreach</code> method only accepts callbacks that handle values from a successfully completed future, so we need another method to install failure callbacks. This method is called <code class="literal">failed</code>. It returns a <code class="literal">Future[Throwable]</code> object that contains the exception that the current object has failed with, and can be used with the <code class="literal">foreach</code> statement to access the exception:</p><pre class="programlisting">object FuturesFailure extends App { 
  val urlSpec: Future[String] = Future { 
    val invalidUrl = "http://www.w3.org/non-existent-url-spec.txt" 
    Source.fromURL(invalidUrl).mkString 
  } 
  urlSpec.failed foreach { 
    case t =&gt; log(s"exception occurred - $t") 
  } 
  Thread.sleep(1000) 
} 
</pre><p>In this example, our asynchronous computation sends an HTTP request to an invalid URL. As a result, the <code class="literal">fromURL</code> method throws an exception, and the <code class="literal">urlSpec</code> future fails. The program then prints the exception name and message with the <code class="literal">log</code> statement.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec34"></a>Using the Try type</h3></div></div></div><p>For conciseness, sometimes we want to subscribe to both successes and failures in the same callback. To do this, we need to use the typeÂ <code class="literal">Try[T]</code>. The typeÂ <code class="literal">Try[T]</code>Â is very similar to the typeÂ <code class="literal">Option[T]</code>Â . Recall from your experience with sequential Scala programming that the typeÂ <code class="literal">Option[T]</code>Â  is used to encode a value of the type <code class="literal">T</code> or its absence. A value of <code class="literal">Option[T]</code> type can either be an object of a typeÂ <code class="literal">Some[T]</code>Â , which holds some value, or <code class="literal">None</code>, which does not hold anything. We use pattern matching to determine whether an <code class="literal">Option[T]</code> type is <code class="literal">Some[T]</code> or <code class="literal">None</code>. Optional types are an alternative to using <code class="literal">null</code> values, which is what one typically does in Java. However, the <code class="literal">Option[T]</code> type does not allow encoding failures in its <code class="literal">None</code> subtype. The <code class="literal">None</code> subtype tells us nothing about the exception in the computation. For this, we use the <code class="literal">Try[T]</code> type.</p><p>The typeÂ <code class="literal">Try[T]</code>Â has two implementations--the typeÂ <code class="literal">Success[T]</code>, which encodes the results of the successful computations, and the <code class="literal">Failure[T]</code> type, which encodes the <code class="literal">Throwable</code> objects that failed the computation. We use pattern matching to determine which of the two a <code class="literal">Try[T]</code> object is:</p><pre class="programlisting">def handleMessage(t: Try[String]) = t match { 
  case Success(msg) =&gt; log(msg) 
  case Failure(error) =&gt; log(s"unexpected failure - $error") 
} 
</pre><p>The <code class="literal">Try[T]</code> objects are immutable objects used synchronously; unlike futures, they contain a value or an exception from the moment they are created. They are more akin to collections than to futures. We can even compose <code class="literal">Try[T]</code> values in for-comprehensions. In the following code snippet, we will compose the name of the current thread with some custom text:</p><pre class="programlisting">import scala.util.{Try, Success, Failure} 
object FuturesTry extends App { 
  val threadName: Try[String] = Try(Thread.currentThread.getName) 
  val someText: Try[String] = Try("Try objects are synchronous") 
  val message: Try[String] = for { 
    tn &lt;- threadName 
    st &lt;- someText 
  } yield s"Message $st was created on t = $tn" 
  handleMessage(message) 
} 
</pre><p>We will first create two <code class="literal">Try[String]</code> values, <code class="literal">threadName</code> and <code class="literal">someText</code>, using the <code class="literal">Try.apply</code> factory method. The <code class="literal">for</code> comprehension extracts the thread name, <code class="literal">tn</code>, from the <code class="literal">threadName</code> value, and then the <code class="literal">st</code> text from the <code class="literal">someText</code> value. These values are then used to yield another string. If any of the <code class="literal">Try</code> values in the <code class="literal">for</code> comprehension fail, then the resulting <code class="literal">Try</code> value fails with the <code class="literal">Throwable</code> object from the first failed <code class="literal">Try</code> value. However, if all the <code class="literal">Try</code> values are <code class="literal">Success</code>, then the resulting <code class="literal">Try</code> value is <code class="literal">Success</code> with the value of the expression after the <code class="literal">yield</code> keyword. If this expression throws an exception, the resulting <code class="literal">Try</code> value fails with that exception.</p><p>Note that the preceding example always prints the name of the main thread. Creating <code class="literal">Try</code> objects and using them in <code class="literal">for</code> comprehensions always occurs on the caller thread.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note37"></a>Note</h3><p>Unlike <code class="literal">Future[T]</code> values, <code class="literal">Try[T]</code> values are manipulated synchronously.</p></div><p>In most cases, we use the <code class="literal">Try</code> values in pattern matching. When calling the <code class="literal">onComplete</code> callback, we will provide a partial function that matches the <code class="literal">Success</code> and <code class="literal">Failure</code> values. Our example forÂ fetching theÂ URL specification is as follows:</p><pre class="programlisting">urlSpec onComplete { 
  case Success(txt) =&gt; log(find(txt)) 
  case Failure(err) =&gt; log(s"exception occurred - $err") 
} 
</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec35"></a>Fatal exceptions</h3></div></div></div><p>We have seen futures storing exceptions that caused them to fail. However, there are some <code class="literal">Throwable</code> objects that a future computation does not catch. In the following short program, the callback on the <code class="literal">f</code> future is never invoked. Instead, the stack trace of <code class="literal">InterruptedException</code> exception is printed on the standard error output:</p><pre class="programlisting">object FuturesNonFatal extends App { 
  val f = Future { throw new InterruptedException } 
  val g = Future { throw new IllegalArgumentException } 
  f.failed foreach { case t =&gt; log(s"error - $t") } 
  g.failed foreach { case t =&gt; log(s"error - $t") } 
} 
</pre><p>The <code class="literal">InterruptedException</code> exception and some severe program errors such as <code class="literal">LinkageError</code>, <code class="literal">VirtualMachineError</code>, <code class="literal">ThreadDeath</code>, and Scala's <code class="literal">ControlThrowable</code> error are forwarded to the execution context's <code class="literal">reportFailure</code> method introduced in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Traditional Building Blocks of Concurrency</em></span>. These types of <code class="literal">Throwable</code> object are called <span class="strong"><strong>fatal errors</strong></span>. To find out if a <code class="literal">Throwable</code> object will be stored in a <code class="literal">Future</code> instance, you can pattern match the <code class="literal">Throwable</code> object with the <code class="literal">NonFatal</code> extractor:</p><pre class="programlisting">f.failed foreach { 
  case NonFatal(t) =&gt; log(s"$t is non-fatal!") 
} 
</pre><p>Note that you never need to manually match in order to see whether errors in your futures are nonfatal. Fatal errors are automatically forwarded to the execution context.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note38"></a>Note</h3><p>Future computations do not catch fatal errors. Use the <code class="literal">NonFatal</code> extractor to pattern match against nonfatal errors.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec36"></a>Functional composition on futures</h3></div></div></div><p>Callbacks are useful, but they can make reasoning about control flow difficult when programs become larger. They also disallow certain patterns in asynchronous programming in particular, it is cumbersome to use a callback to subscribe to multiple futures at once. Luckily, Scala futures have an answer to these problems called <span class="strong"><strong>functional composition</strong></span>. Functional composition on futures allows using futures inside <code class="literal">for</code> comprehensions, and is often more intuitive to use than callbacks.</p><p>Introducing futures transfers the responsibility for blocking from the API to the caller. The <code class="literal">foreach</code> method helps you to avoid blocking altogether. It also eliminates non-determinism inherent to polling methods such as <code class="literal">isCompleted</code> and <code class="literal">value</code>. Still, there are some situations when theÂ <code class="literal">foreach</code> statement is not the best solution.</p><p>Let's say that we want to implement some of the functionality from the Git version control system; we want to use the <code class="literal">.gitignore</code> file to find files in our project tree that should not be versioned. We simplify our task by assuming that the <code class="literal">.gitignore</code> file only contains a list of prefixes for blacklisted file paths, and no regular expressions.</p><p>We perform two asynchronous actions. First, we fetch the contents of our <code class="literal">.gitignore</code> file in a future computation. Then, using its contents, we will asynchronously scan all the files in our project directory and match them. We will start by importing the packages necessary for file handling. In addition to the <code class="literal">scala.io.Source</code> object, we use the <code class="literal">java.io</code> package and the <code class="literal">apache.commons.io.FileUtils</code> class, and import them as follows:</p><pre class="programlisting">import java.io._ 
import org.apache.commons.io.FileUtils._ 
import scala.collection.convert.decorateAsScala._ 
</pre><p>If you haven't already added the dependency on Commons IO to your <code class="literal">build.sbt</code> file in the previous chapters, now is a good time to introduce the following line:</p><pre class="programlisting">libraryDependencies += "commons-io" % "commons-io" % "2.4" 
</pre><p>We will first create a future using the <code class="literal">blacklistFile</code> method, which reads the contents of the <code class="literal">.gitignore</code> file. Given the pace at which technology is evolving these days, we never know when a different version control system will become more popular; so we add the <code class="literal">name</code> parameter for the name of the blacklist file. We filter out the empty lines and all the comment lines starting with a <code class="literal">#</code> sign. We then convert them to a list, as shown in the following code snippet:</p><pre class="programlisting">object FuturesClumsyCallback extends App { 
  def blacklistFile(name: String): Future[List[String]] = Future { 
    val lines = Source.fromFile(name).getLines 
    lines.filter(x =&gt; !x.startsWith("#") &amp;&amp; !x.isEmpty).toList 
  } 
</pre><p>In our case, the future returned by the <code class="literal">blacklistFile</code> method eventually contains a list with a single string, the <code class="literal">target</code> directory is where SBT stores files created by the Scala compiler. Then, we implement another method named <code class="literal">findFiles</code> that, given a list of patterns, finds all the files in the current directory containing these patterns. The <code class="literal">iterateFiles</code> method from the Commons IO library returns a Java iterator over the project files, so we can convert it to a Scala iterator by calling <code class="literal">asScala</code>. We then yield all the matching file paths:</p><pre class="programlisting">  def findFiles(patterns: List[String]): List[String] = { 
    val root = new File(".") 
    for { 
      f &lt;- iterateFiles(root, null, true).asScala.toList 
      pat &lt;- patterns 
      abspat = root.getCanonicalPath + File.separator + pat 
      if f.getCanonicalPath.contains(abspat) 
    } yield f.getCanonicalPath 
  } 
</pre><p>If we now want to list blacklisted files, we first need to call <code class="literal">foreach</code> on the <code class="literal">blacklistFile</code> future, and call <code class="literal">findPatterns</code> from inside the callback, as follows:</p><pre class="programlisting">  blacklistFile(".gitignore") foreach { 
    case lines =&gt; 
      val files = findFiles(lines) 
      log(s"matches: ${files.mkString("\n")}") 
  } 
  Thread.sleep(1000) 
} 
</pre><p>Assume your fellow developer now asks you to implement another <code class="literal">blacklisted</code> methodÂ that takes the name of the blacklist file and returns a future with a list of blacklisted files. This allows us to specify the callback independently in the program; instead of printing the files to the standard output, another part of the program can, for example, create a safety backup of the blacklisted files using the following method:</p><pre class="programlisting">def blacklisted(name: String): Future[List[String]] 
</pre><p>Being an experienced object-oriented developer, you'd like to reuse the <code class="literal">blacklistFile</code> future and the <code class="literal">findFiles</code> method. After all, the functionality is already there. We challenge you to reuse the existing methods to implement the new <code class="literal">blacklisted</code> method. Try to use theÂ <code class="literal">foreach</code> statement. You will find this task extremely difficult.</p><p>So far, we haven't seen methods that produce new futures using the values in existing futures. The <code class="literal">Future</code> trait has a <code class="literal">map</code> method that maps the value in one future to a value in another future:</p><pre class="programlisting">def map[S](f: T =&gt; S)(implicit e: ExecutionContext): Future[S] 
</pre><p>This method is non blocking--it returns the <code class="literal">Future[S]</code> object immediately. After the original future completes with some value <code class="literal">x</code>, the returned <code class="literal">Future[S]</code> object is eventually completed with <code class="literal">f(x)</code>. With the <code class="literal">map</code> method, our task is trivial: we transform the patterns into a list of matching files by calling the <code class="literal">findFiles</code> method:</p><pre class="programlisting">def blacklisted(name: String): Future[List[String]] = 
  blacklistFile(name).map(patterns =&gt; findFiles(patterns)) 
</pre><p>As a Scala developer, you know that a <code class="literal">map</code> operation on a collection transforms many elements into a new collection. To more easily comprehend operations such as the <code class="literal">map</code> operation on futures, you can consider a future as a specific form of collectionÂ that contains at most one element.</p><p><span class="strong"><strong>Functional composition</strong></span> is a programming pattern in which simpler values are composed into more complex ones by means of higher-order functions called <span class="strong"><strong>combinators</strong></span>. Functional composition on Scala collections should be familiar to you from sequential Scala programming. For example, the <code class="literal">map</code> method on a collection produces a new collection containing elements from the original collection, mapped with a specified function.</p><p>Functional composition on futures is similar; we can produce new futures by transforming or merging existing futures, as in the preceding example. Callbacks are useful, but they do not directly allow functional composition in the way combinators such as <code class="literal">map</code> do. Just as with callbacks, a function passed to a combinator is never invoked before the corresponding future completes.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note39"></a>Note</h3><p>There is a happens-before relationship between completing the future and invoking the function in any of its combinators.</p></div><p>Choosing between alternative ways to handle futures can be confusing. When should we use functional composition in place of callbacks? A good rule of thumb is to use callbacks for side-effecting actions that depend on a single future. In all other situations, we can use functional composition.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip40"></a>Tip</h3><p>When an action in the program depends on the value of a single future, use callbacks on futures. When subsequent actions in the program depend on values of multiple futures or produce new futures, use functional composition on futures.</p></div><p>Let us consider several crucial combinators for functional composition. The <code class="literal">map</code> method on a <code class="literal">Future[T]</code> takes anÂ <code class="literal">f</code> function and returns a new <code class="literal">Future[S]</code> future. After the <code class="literal">Future[T]</code> is completed, the <code class="literal">Future[S]</code> is completed by applying <code class="literal">f</code> to the value in <code class="literal">Future[T]</code>. If <code class="literal">Future[T]</code> fails with anÂ exception <code class="literal">e</code>, or the mapping function <code class="literal">f</code> throws an exception <code class="literal">e</code>, then <code class="literal">Future[S]</code> also fails with that exception <code class="literal">e</code>.</p><p>Recall that Scala allows using for-comprehensions on objects that have a <code class="literal">map</code> method, so we can use futures in for-comprehensions. Let's assume that we want to get the future with the longest line from our <code class="literal">build.sbt</code> file. The computation proceeds in two steps. First, we read in the lines from the disk, and then we call the <code class="literal">maxBy</code> method to get the longest line:</p><pre class="programlisting">val buildFile = Future {  
  Source.fromFile("build.sbt").getLines  
} 
 
val longest = for (ls &lt;- buildFile) yield ls.maxBy(_.length) 
longest foreach { 
  case line =&gt; log(s"longest line: $line") 
} 
</pre><p>The <code class="literal">longest</code> declaration is desugared by the Scala compiler into the following line:</p><pre class="programlisting">val longest = buildFile.map(ls =&gt; ls.maxBy(_.length)) 
</pre><p>The real advantage of <code class="literal">for</code> comprehensions becomes apparent when we use the <code class="literal">flatMap</code> combinator, which has the following signature:</p><pre class="programlisting">def flatMap[S](f: T =&gt; Future[S])(implicit e: ExecutionContext): 
  Future[S] 
</pre><p>The <code class="literal">flatMap</code> combinator uses the current future with the <code class="literal">Future[T]</code> type to produce another future with the typeÂ <code class="literal">Future[S]</code>Â . The resulting <code class="literal">Future[S]</code> is completed by taking the value <code class="literal">x</code> of the type <code class="literal">T</code> from the current future, and mapping that value to another future <code class="literal">f(x)</code>. While the future resulting from a <code class="literal">map</code> method completes when the mapping function <code class="literal">f</code> completes, the future resulting from a <code class="literal">flatMap</code> method completes when both <code class="literal">f</code> and the future returned by <code class="literal">f</code> complete.</p><p>To understand how this combinator is useful, let's consider the following example. Assume that your job application went well and you got that new job you were hoping for. On the first day of work, you receive a chain e-mail from your secretary. The chain e-mail claims that you should never open URLs starting with <code class="literal">ftp://</code>, because all of them contain viruses. As a skillful techie with a lot of experience, you quickly recognize the chain letter for what it is--a scam. You, therefore, decide to enlighten your secretary by sending her instructions on how to communicate using e-mails, and an explanation of what FTP links are. You write a short program that replies asynchronously. You've got better things to do than to spend your day writing e-mails:</p><pre class="programlisting">val netiquetteUrl = "http://www.ietf.org/rfc/rfc1855.txt" 
val netiquette = Future { Source.fromURL(netiquetteUrl).mkString } 
val urlSpecUrl = "http://www.w3.org/Addressing/URL/url-spec.txt" 
val urlSpec = Future { Source.fromURL(urlSpecUrl).mkString } 
val answer = netiquette.flatMap { nettext =&gt; 
  urlSpec.map { urltext =&gt; 
    "Check this out: " + nettext + ". And check out: " + urltext 
  } 
} 
answer foreach { case contents =&gt; log(contents) } 
</pre><p>This program asynchronously fetches the good old <span class="strong"><strong>RFC 1855</strong></span>--the guidelines for e-mail communicationÂ or netiquette. It then asynchronously fetches the URL specification with information on the <code class="literal">ftp</code> schema. The program attempts to concatenate the two texts. It calls <code class="literal">flatMap</code> on the <code class="literal">netiquette</code> future. Based on the <code class="literal">nettext</code> value in the <code class="literal">netiquette</code> future, the <code class="literal">flatMap</code> future needs to return another future. It could return the <code class="literal">urlSpec</code> future directly, but the resulting future, <code class="literal">answer</code>, would then be completed with just the URL specification. Instead, we can call the <code class="literal">map</code> combinator on the <code class="literal">urlSpec</code> future; we map its value, <code class="literal">urltext</code>, into the concatenation of the <code class="literal">nettext</code> and <code class="literal">urltext</code> values. This results in another intermediate future holding the concatenation; once this future is completed, the <code class="literal">answer</code> future is completed as well. Graphically, this looks as follows:</p><div class="mediaobject"><img src="graphics/image_04_004.jpg" /></div><p>If you look at this execution diagram from a distance, you will notice that there is an inherent ordering between asynchronous computations. We can capture these relationships in a graph, as shown in the following figure:</p><div class="mediaobject"><img src="graphics/image_04_005.jpg" /></div><p>This graph is called the <span class="strong"><strong>dataflow graph</strong></span>, because it describes how the data flows from one future to another. Futures are represented with vertices and asynchronous computations are directed edges between them. An edge points from one vertex to another if the value of future in the first vertex is used to compute the value of future in the second vertex. In this graph, futures produced by <code class="literal">Future.apply</code> are source vertices-they have only outward edges. Various future combinators such as <code class="literal">map</code> and <code class="literal">flatMap</code> connect different vertices. Callback functions such as <code class="literal">foreach</code> lead to sink vertices-they have no outward edges. Some combinators, such as <code class="literal">flatMap</code>, can use values from multiple vertices.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip41"></a>Tip</h3><p>The <code class="literal">flatMap</code> combinator combines two futures into one: the one on which the <code class="literal">flatMap</code> combinator is invoked and the one that is returned by the argument function.</p></div><p>There are two issues with our e-mail example. First, we should be nicer to our new secretary; she's not a techie like we are. Second, using <code class="literal">flatMap</code> directly makes the program hard to understand. There are not many developers in the Scala community that use <code class="literal">flatMap</code> like this. Instead, <code class="literal">flatMap</code> should be used implicitly in <code class="literal">for</code> comprehensions:</p><pre class="programlisting">val answer = for { 
  nettext &lt;- netiquette 
  urltext &lt;- urlSpec 
} yield { 
  "First, read this: " + nettext + ". Now, try this: " + urltext 
} 
</pre><p>After desugaring, this <code class="literal">for</code> comprehension is identical to what we had before. This is much simpler; the program now almost reads itself. For the <code class="literal">nettext</code> value of the <code class="literal">netiquette</code> future and the <code class="literal">urltext</code> value of the <code class="literal">urlSpec</code> future, the <code class="literal">answer</code> future is a new future with the concatenation of <code class="literal">nettext</code> and <code class="literal">urltext</code>.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip42"></a>Tip</h3><p>You should prefer for-comprehensions to using <code class="literal">flatMap</code> directly to make programs more concise and understandable.</p></div><p>Note that the following for-comprehension looks very similar to what we had before, but it is not equivalent:</p><pre class="programlisting">val answer = for { 
  nettext &lt;- Future { Source.fromURL(netiquetteUrl).mkString } 
  urltext &lt;- Future { Source.fromURL(urlSpecUrl).mkString } 
} yield { 
  "First, read this: " + nettext + ". Now, try this: " + urltext 
} 
</pre><p>In the preceding code, the <code class="literal">nettext</code> value is extracted from the first future. Only after the first future is completed, the second future computation start. This is useful when the second asynchronous computation uses <code class="literal">nettext</code>, but in our case fetching the <code class="literal">netiquette</code> document and the URL specification can proceed concurrently.</p><p>So far, we have only considered future combinators that work with successful futures. When any of the input futures fail or the computation in the combinator throws an exception, the resulting future fails with the same exception. In some situations, we want to handle the exception in the future in the same way as we handle exceptions with a <code class="literal">try-catch</code> block in sequential programming. A combinator that is helpful in these situations is called <code class="literal">recover</code>. Its simplified signature is as follows:</p><pre class="programlisting">def recover(pf: PartialFunction[Throwable, T]) 
  (implicit e: ExecutionContext): Future[T] 
</pre><p>When this combinator is called on a future, which is successfully completed with some value <code class="literal">x</code> of the type <code class="literal">T</code>, the resulting future is completed with the same value <code class="literal">x</code>. On the other hand, if a future fails, then the <code class="literal">pf</code> partial function is applied to the <code class="literal">Throwable</code> object that failed it. If the <code class="literal">pf</code> partial function is not defined for the <code class="literal">Throwable</code> object, then the resulting future is failed with the same <code class="literal">Throwable</code> object. Otherwise, the resulting future is completed with the result of applying <code class="literal">pf</code> to the <code class="literal">Throwable</code> object. If the <code class="literal">pf</code> partial function itself throws an exception, the resulting future is completed with that exception.</p><p>Let's assume you're worried about misspelling the URL for the <code class="literal">netiquette</code> document. You can use the <code class="literal">recover</code> combinator on the <code class="literal">netiquette</code> future to provide a reasonable default message if anything fails, as follows:</p><pre class="programlisting">val netiquetteUrl = "http://www.ietf.org/rfc/rfc1855.doc" 
val netiquette = Future { Source.fromURL(netiquetteUrl).mkString } 
val answer = netiquette recover { 
  case e: java.io.FileNotFoundException =&gt; 
    "Dear secretary, thank you for your e-mail." + 
    "You might be interested to know that ftp links " + 
    "can also point to regular files we keep on our servers." 
} 
answer foreach { case contents =&gt; log(contents) } 
Thread.sleep(2000) 
</pre><p>Futures come with other combinators such as <code class="literal">filter</code>, <code class="literal">fallbackTo</code>, or <code class="literal">zip</code>, but we will not cover all of them here; an understanding of the basic combinators should be sufficient. You might wish to study the remaining combinators in the API documentation.</p></div></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec27"></a>Promises</h2></div></div><hr /></div><p>In <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Concurrency on the JVM and the Java Memory Model</em></span>, we implemented an <code class="literal">asynchronous</code> method that used a worker thread and a task queue to receive and execute asynchronous computations. That example should have left you with a basic intuition about how the <code class="literal">execute</code> method is implemented in execution contexts. You might be wondering how the <code class="literal">Future.apply</code> method can return and complete a <code class="literal">Future</code> object. We will study promises in this section to answer this question. <span class="strong"><strong>Promises</strong></span> are objects that can be assigned a value or an exception only once. This is why promises are sometimes also called single-assignment variables. A promise is represented with the <code class="literal">Promise[T]</code> type in Scala. To create a promise instance, we use the <code class="literal">Promise.apply</code> method on the <code class="literal">Promise</code> companion object:</p><pre class="programlisting">def apply[T](): Promise[T] 
</pre><p>This method returns a new promise instance. Like the <code class="literal">Future.apply</code> method, the <code class="literal">Promise.apply</code> method returns immediately; it is non-blocking. However, the <code class="literal">Promise.apply</code> method does not start an asynchronous computation; it just creates a fresh promise object. When the promise object is created, it does not contain a value or an exception. To assign a value or an exception to a promise, we use the <code class="literal">success</code> or <code class="literal">failure</code> method, respectively.</p><p>Perhaps you have noticed that promises are very similar to futures. Both futures and promises are initially empty and can be completed with either a value or an exception. This is intentional; every promise object corresponds to exactly one future object. To obtain the future associated with a promise, we can call the <code class="literal">future</code> method on the promise. Calling this method multiple times always returns the same future object.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note43"></a>Note</h3><p>A promise and a future represent two aspects of a single--assignment variable--the promise allows you to assign a value to the future object, whereas the future allows you to read that value.</p></div><p>In the following code snippet, we create two promises, <code class="literal">p</code> and <code class="literal">q</code>, that can hold string values. We then install a <code class="literal">foreach</code> callback on the future associated with the <code class="literal">p</code> promise and wait for one second. The callback is not invoked until the <code class="literal">p</code> promise is completed by calling the <code class="literal">success</code> method. We then fail the <code class="literal">q</code> promise in the same way and install a <code class="literal">failed.foreach</code> callback:</p><pre class="programlisting">object PromisesCreate extends App { 
  val p = Promise[String] 
  val q = Promise[String] 
  p.future foreach { case x =&gt; log(s"p succeeded with '$x'") } 
  Thread.sleep(1000) 
  p success "assigned" 
  q failure new Exception("not kept") 
  q.future.failed foreach { case t =&gt; log(s"q failed with $t") } 
  Thread.sleep(1000) 
} 
</pre><p>Alternatively, we can use the <code class="literal">complete</code> method and specify a <code class="literal">Try[T]</code> object to complete the promise. Depending on whether the <code class="literal">Try[T]</code> object is a success or a failure, the promise is successfully completed or failed. Importantly, after a promise is either successfully completed or failed, it cannot be assigned an exception or a value again in any way. Trying to do so results in an exception. Note that this is true even when there are multiple threads simultaneously calling <code class="literal">success</code> or <code class="literal">complete</code>. Only one thread completes the promise, and the rest throw an exception.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note44"></a>Note</h3><p>Assigning a value or an exception to an already completed promise is not allowed and throws an exception.</p></div><p>We can also use the <code class="literal">trySuccess</code>, <code class="literal">tryFailure</code>, and <code class="literal">tryComplete</code> methods that correspond to <code class="literal">success</code>, <code class="literal">failure</code>, and <code class="literal">complete</code>, respectively, but return a Boolean value to indicate whether the assignment was successful. Recall that using <code class="literal">Future.apply</code> and callback methods with referentially transparent functions results in deterministic concurrent programs. As long as we do not use the <code class="literal">trySuccess</code>, <code class="literal">tryFailure</code>, and <code class="literal">tryComplete</code> methods, and none of the <code class="literal">success</code>, <code class="literal">failure</code>, and <code class="literal">complete</code> methods ever throw an exception, we can use promises and retain determinism in our programs.</p><p>We now have everything we need to implement our custom <code class="literal">Future.apply</code> method. We call it <code class="literal">myFuture</code> in the following example. The <code class="literal">myFuture</code> method takes a <code class="literal">b</code> by-name parameter that is the asynchronous computation. First, it creates a <code class="literal">p</code> promise. Then, it starts an asynchronous computation on the <code class="literal">global</code> execution context. This computation tries to evaluate <code class="literal">b</code> and complete the promise. However, if the <code class="literal">b</code> body throws a nonfatal exception, the asynchronous computation fails the promise with that exception. In the meanwhile, the <code class="literal">myFuture</code> method returns the future immediately after starting the asynchronous computation:</p><pre class="programlisting">import scala.util.control.NonFatal 
object PromisesCustomAsync extends App { 
  def myFuture[T](b: =&gt;T): Future[T] = { 
    val p = Promise[T] 
    global.execute(new Runnable { 
      def run() = try { 
        p.success(b) 
      } catch { 
        case NonFatal(e) =&gt; p.failure(e) 
      } 
    }) 
    p.future 
  } 
  val f = myFuture { "naa" + "na" * 8 + " Katamari Damacy!" } 
  f foreach { case text =&gt; log(text) } 
} 
</pre><p>This is a common pattern when producing futures. We create a promise, let some other computation complete that promise, and return the corresponding future. However, promises were not invented just for our custom future computation method, <code class="literal">myFuture</code>. In the following sections, we will study use cases in which promises are useful.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec37"></a>Converting callback-based APIs</h3></div></div></div><p>Scala futures are great. We already saw how they can be used to avoid blocking. We have learned that callbacks help us to avoid polling and busy-waiting. We witnessed that futures compose well with functional combinators and <code class="literal">for</code> comprehensions. In some cases, futures and promises even guarantee deterministic programs. But, we have to face the truth-not all legacy APIs were created using Scala futures. Although futures are now the right way to do asynchronous computing, various third-party libraries have different approaches to encoding latency.</p><p>Legacy frameworks deal with latency in the program with raw callbacks. Methods that take an unbounded amount of time to complete do not return the result; instead, they take a callback argument, which is invoked with the result later. JavaScript libraries and frameworks are a good example for this--there is a single thread executing a JavaScript program and it is unacceptable to block that thread every time we call a blocking method.</p><p>Such legacy systems have issues in large-scale development. First, they do not nicely compose, as we already saw. Second, they are hard to understand and reason about; a bunch of unstructured callbacks feels almost like spaghetti code. The control flow of the program is not apparent from the code, but is dictated by the internals of the library. This is called <span class="strong"><strong>inversion of control</strong></span>. We would like to somehow create a bridge between legacy callback-based APIs and futures, and avoid this inversion of control. This is where promises come in handy.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip45"></a>Tip</h3><p>Use promises to bridge the gap between callback-based APIs and futures.</p></div><p>Let's consider the <code class="literal">org.apache.commons.io.monitor</code> package from the <span class="strong"><strong>Commons IO</strong></span> library. This package allows subscribing to filesystem events such as file and directory creation and deletion. Having become well versed in the use of futures, we do not want to deal with this API directly anymore. We, therefore, implement a <code class="literal">fileCreated</code> method that takes a directory name and returns a future with the name of the first file in that freshly created directory:</p><pre class="programlisting">import org.apache.commons.io.monitor._ 
</pre><p>To subscribe to a filesystem event using this package, we first need to instantiate a <code class="literal">FileAlterationMonitor</code> object. This object periodically scans the filesystem for changes. After that, we need to create a <code class="literal">FileAlterationObserver</code> object, which observes a specific directory for changes. Finally, we create a <code class="literal">FileAlterationListenerAdaptor</code> object, which represents the callback. Its <code class="literal">onFileCreate</code> method is called when a file is created in the filesystem; we use it to complete the promise with the name of the file that was changed:</p><pre class="programlisting">def fileCreated(directory: String): Future[String] = { 
  val p = Promise[String] 
  val fileMonitor = new FileAlterationMonitor(1000) 
  val observer = new FileAlterationObserver(directory) 
  val listener = new FileAlterationListenerAdaptor { 
    override def onFileCreate(file: File): Unit = 
      try p.trySuccess(file.getName) finally fileMonitor.stop() 
  } 
  observer.addListener(listener) 
  fileMonitor.addObserver(observer) 
  fileMonitor.start() 
  p.future 
} 
</pre><p>Notice that the structure of this method is the same as the structure of the <code class="literal">myFuture</code> method. We first create a promise and defer the completion of the promise to some other computation. Then, we return the future associated with the promise. This recurring pattern is called the future-callback bridge.</p><p>We can now use the future to subscribe to the first file change in the filesystem. We add a <code class="literal">foreach</code> call to the future returned by the <code class="literal">fileCreated</code> method, create a new file in the editor, and witness how the program detects a new file:</p><pre class="programlisting">fileCreated(".") foreach { 
  case filename =&gt; log(s"Detected new file '$filename'") 
} 
</pre><p>A useful utility that is not available on futures is the <code class="literal">timeout</code> method. We want to call a <code class="literal">timeout</code> method that takes some number of <code class="literal">t</code> millisecondsÂ and returns a future that is completed after at least <code class="literal">t</code> milliseconds. We apply the callback-future bridge to the <code class="literal">Timer</code> class from the <code class="literal">java.util</code> package. We use a single timer object for all the <code class="literal">timeout</code> calls:</p><pre class="programlisting">import java.util._ 
private val timer = new Timer(true) 
</pre><p>Again, we first create a promise <code class="literal">p</code>. This promise holds no useful information other than the fact that it is completed, so we give it the typeÂ <code class="literal">Promise[Unit]</code>Â . We then call the <code class="literal">Timer</code> class's <code class="literal">schedule</code> method with a <code class="literal">TimerTask</code> object that completes the <code class="literal">p</code> promise after <code class="literal">t</code> milliseconds:</p><pre class="programlisting">def timeout(t: Long): Future[Unit] = { 
  val p = Promise[Unit] 
  timer.schedule(new TimerTask { 
    def run() = { 
      p success () 
      timer.cancel() 
    } 
  }, t) 
  p.future 
} 
timeout(1000) foreach { case _ =&gt; log("Timed out!") } 
Thread.sleep(2000) 
</pre><p>The future returned by the <code class="literal">timeout</code> method can be used to install a callback, or it can be combined with other futures using combinators. In the next section, we will introduce new combinators for this purpose.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec38"></a>Extending the future API</h3></div></div></div><p>Usually, the existing future combinators are sufficient for most tasks, but occasionally we want to define new ones. This is another use case for promises. Assume that we want to add a combinator to futures, as follows:</p><pre class="programlisting">def or(that: Future[T]): Future[T] 
</pre><p>This method returns a new future of the same type that is assigned the value of the <code class="literal">this</code> future or the <code class="literal">that</code> future, whichever is completed first. We cannot add this method directly to the <code class="literal">Future</code> trait because futures are defined in the Scala standard library, but we can create an implicit conversion that adds this method. Recall that, if you call a nonexistent <code class="literal">xyz</code> method on an object of some typeÂ <code class="literal">A</code>, the Scala compiler will search for all implicit conversions from typeÂ <code class="literal">A</code>Â  to some other type that has the <code class="literal">xyz</code> method. One way to define such an implicit conversion is to use Scala's implicit classes:</p><pre class="programlisting">implicit class FutureOps[T](val self: Future[T]) { 
  def or(that: Future[T]): Future[T] = { 
    val p = Promise[T] 
    self onComplete { case x =&gt; p tryComplete x } 
    that onComplete { case y =&gt; p tryComplete y } 
    p.future 
  } 
} 
</pre><p>The implicit <code class="literal">FutureOps</code> class converts a future of typeÂ <code class="literal">Future[T]</code>Â  to an object with an additional <code class="literal">or</code> method. Inside the <code class="literal">FutureOps</code> object, we refer to the original future with the name <code class="literal">self</code>; we cannot use <code class="literal">this</code> word, because <code class="literal">this</code> is a reserved keyword that refers to the <code class="literal">FutureOps</code> object. The <code class="literal">or</code> method installs callbacks on <code class="literal">self</code> and <code class="literal">that</code> future. Each of these callbacks calls the <code class="literal">tryComplete</code> method on the <code class="literal">p</code> promise; the callback that executes first successfully completes the promise. The <code class="literal">tryComplete</code> method in the other callback returns <code class="literal">false</code> and does not change the state of the promise.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip46"></a>Tip</h3><p>Use promises to extend futures with additional functional combinators.</p></div><p>Note that we used the <code class="literal">tryComplete</code> method in this example, and the <code class="literal">or</code> combinator is nondeterministic as a result. The resulting future is completed with the value of one of the input futures depending on the execution schedule. In this particular case, this is exactly what we want.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec39"></a>Cancellation of asynchronous computations</h3></div></div></div><p>In some cases, we want to cancel a future computation. This might be because a future computation takes more than the allotted amount of time, or because the user clicks on the <span class="strong"><strong>Cancel</strong></span> button in the UI. In either case, we need to provide some alternative value for the canceled future.</p><p>Futures come without built-in support for cancellation. Once a future computation starts, it is not possible to cancel it directly. Recall from <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Concurrency on the JVM and the Java Memory Model</em></span>, that violently stopping concurrent computations can be harmful, and this is why the <code class="literal">Thread</code> methods such as <code class="literal">stop</code> were deprecated in the early JDK releases.</p><p>One approach to cancel a future is to compose it with another future called the <span class="strong"><strong>cancellation future</strong></span>. The <code class="literal">cancellation</code> future provides a default value when a future is canceled. We can use the <code class="literal">or</code> combinator, discussed in the previous section, along with the <code class="literal">timeout</code> method, to compose a future with its <code class="literal">cancellation</code> future:</p><pre class="programlisting">val f = timeout(1000).map(_ =&gt; "timeout!") or Future { 
  Thread.sleep(999) 
  "work completed!" 
} 
</pre><p>The nondeterminism of the <code class="literal">or</code> combinator is apparent when running this program. The <code class="literal">timeout</code> and <code class="literal">sleep</code> statements are precisely tuned to occur approximately at the same time. Another thing worth noting is that the computation started by the <code class="literal">Future.apply</code> method does not actually stop if a timeout occurs. The <code class="literal">f</code> future is completed with the value <code class="literal">"timeout!"</code>, but the future computation proceeds concurrently. Eventually, it fails to set the value of the promise when calling <code class="literal">tryComplete</code> in the <code class="literal">or</code> combinator. In many cases, this is not a problem. An HTTP request that needs to complete a future does not occupy any computational resources, and will eventually timeout anyway. A keyboard event that completes a future only consumes a small amount of CPU time when it triggers. Callback-based futures can usually be canceled, as in the preceding example. On the other hand, a future that performs an asynchronous computation can use a lot of CPU power or other resources. We might want to ensure that actions such as scanning the filesystem or downloading a huge file really terminate.</p><p>A future computation cannot be forcefully stopped. Instead, there should exist some form of cooperation between the future computation and the client of the future. In the examples seen so far, asynchronous computations always use futures to communicate a value to the client. In this case, the client also communicates in the opposite direction to let the asynchronous computation know that it should stop. Naturally, we use futures and promises to accomplish this two-way communication.</p><p>First, we define a typeÂ <code class="literal">Cancellable[T]</code>Â  as a pair of <code class="literal">Promise[Unit]</code> and <code class="literal">Future[T]</code> objects. The client will use the <code class="literal">Promise[Unit]</code> part to request a cancellation, and the <code class="literal">Future[T]</code> part to subscribe to the result of the computation:</p><pre class="programlisting">object PromisesCancellation extends App { 
  type Cancellable[T] = (Promise[Unit], Future[T]) 
</pre><p>The <code class="literal">cancellable</code> method takes the <code class="literal">b</code> body of the asynchronous computation. This time, the <code class="literal">b</code> body takes a single parameter, <code class="literal">Future[Unit]</code>, to check if the cancellation was requested. The <code class="literal">cancellable</code> method creates a <code class="literal">cancel</code> promise of the typeÂ <code class="literal">Promise[Unit]</code>Â  and forwards its corresponding future to the asynchronous computation. We call this promise the <span class="strong"><strong>cancellation promise</strong></span>. The <code class="literal">cancel</code> promise will be used to signal that the asynchronous computation <code class="literal">b</code> should end. After the asynchronous computation <code class="literal">b</code> returns some value <code class="literal">r</code>, the <code class="literal">cancel</code> promise needs to fail. This ensures that, if the typeÂ <code class="literal">Future[T]</code>Â  is completed, then the client cannot successfully cancel the computation using the <code class="literal">cancel</code> promise:</p><pre class="programlisting">  def cancellable[T](b: Future[Unit] =&gt; T): Cancellable[T] = { 
    val cancel = Promise[Unit] 
    val f = Future { 
      val r = b(cancel.future) 
      if (!cancel.tryFailure(new Exception)) 
        throw new CancellationException 
      r 
    } 
    (cancel, f) 
  } 
</pre><p>If calling <code class="literal">tryFailure</code> on the <code class="literal">cancel</code> promise returns <code class="literal">false</code>, then the client must have already completed the <code class="literal">cancel</code> promise. In this case, we cannot fail the client's attempt to cancel the computation, so we throw a <code class="literal">CancellationException</code>. Note that we cannot omit this check, as it exists to avoid the race in which the client successfully requests the cancellation, and the future computation simultaneously completes the future.</p><p>The asynchronous computation must occasionally check if the future was canceled using the <code class="literal">isCompleted</code> method on the <code class="literal">cancel</code> future. If it detects that it was canceled, it must cease execution by throwing a <code class="literal">CancellationException</code> value:</p><pre class="programlisting">  val (cancel, value) = cancellable { cancel =&gt; 
    var i = 0 
    while (i &lt; 5) { 
      if (cancel.isCompleted) throw new CancellationException 
      Thread.sleep(500) 
      log(s"$i: working") 
      i += 1 
    } 
    "resulting value" 
  } 
</pre><p>After the <code class="literal">cancellable</code> computation starts, the main thread waits for 1,500 milliseconds and then calls <code class="literal">trySuccess</code> to complete the cancellation promise. By this time, the cancellation promise could have already failed; in this case, calling <code class="literal">success</code> instead of the <code class="literal">trySuccess</code> method would result in an exception:</p><pre class="programlisting">  Thread.sleep(1500) 
  cancel trySuccess () 
  log("computation cancelled!") 
  Thread.sleep(2000) 
} 
</pre><p>We expect to see the final <code class="literal">working</code> message printed after the <code class="literal">"computation cancelled!"</code> message from the main thread. This is because the asynchronous computation uses polling and does not immediately detect that it was cancelled.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip47"></a>Tip</h3><p>Use promises to implement cancellationÂ or any other form of two-way communication between the client and the asynchronous computation.</p></div><p>Note that calling the <code class="literal">trySuccess</code> method on the <code class="literal">cancel</code> promise does not guarantee that the computation will really be canceled. It is entirely possible that the asynchronous computation fails the <code class="literal">cancel</code> promise before the client has a chance to cancel it. Thus, the client, such as the main thread in our example, should in general use the return value from theÂ <code class="literal">trySuccess</code>Â method to check if the cancellation succeeded.</p></div></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec28"></a>Futures and blocking</h2></div></div><hr /></div><p>Examples in this book should have shed the light into why blocking is sometimes considered an anti-pattern. Futures and asynchronous computations mainly exist to avoid blocking, but in some cases, we cannot live without it. It is, therefore, valid to ask how blocking interacts with futures.</p><p>There are two ways to block with futures. The first is waiting until a future is completed. The second is blocking from within an asynchronous computation. We will study both the topics in this section.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec40"></a>Awaiting futures</h3></div></div></div><p>In rare situations, we cannot use callbacks or future combinators to avoid blocking. For example, the main thread that starts multiple asynchronous computations has to wait for these computations to finish. If an execution context uses daemon threads, as is the case with the <code class="literal">global</code> execution context, the main thread needs to block to prevent the JVM process from terminating.</p><p>In these exceptional circumstances, we use the <code class="literal">ready</code> and <code class="literal">result</code> methods on the <code class="literal">Await</code> object from the <code class="literal">scala.concurrent</code> package. The <code class="literal">ready</code> method blocks the caller thread until the specified future is completed. The <code class="literal">result</code> method also blocks the caller thread, but returns the value of the future if it was completed successfully, or throws the exception in the future if the future was failed.</p><p>Both the methods require specifying a timeout parameter-the longest duration that the caller should wait for the completion of the future before a <code class="literal">TimeoutException</code> is thrown. To specify a timeout, we import the <code class="literal">scala.concurrent.duration</code> package. This allows us to write expressions such as <code class="literal">10.seconds</code>:</p><pre class="programlisting">import scala.concurrent.duration._ 
object BlockingAwait extends App { 
  val urlSpecSizeFuture = Future { 
    val specUrl = "http://www.w3.org/Addressing/URL/url-spec.txt" 
    Source.fromURL(specUrl).size 
  } 
  val urlSpecSize = Await.result(urlSpecSizeFuture, 10.seconds) 
  log(s"url spec contains $urlSpecSize characters") 
} 
</pre><p>In this example, the main thread starts a computation that retrieves the URL specification and then awaits. By this time, the World Wide Web Consortium is worried that a DOS attack is under way, so this is the last time we download the URL specification.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec41"></a>Blocking in asynchronous computations</h3></div></div></div><p>Waiting for the completion of a future is not the only way to block. Some legacy APIs do not use callbacks to asynchronously return results. Instead, such APIs expose the blocking methods. After we call a blocking method, we lose control over the thread; it is up to the blocking method to unblock the thread and return the control back.</p><p>Execution contexts are often implemented using thread pools. As we saw in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Traditional Building Blocks of Concurrency</em></span>, blocking worker threads can lead to thread starvation. Thus, by starting future computations that block, it is possible to reduce parallelism and even cause deadlocks. This is illustrated in the following example, in which 16 separate future computations call the <code class="literal">sleep</code> method, and the main thread waits until they complete for an unbounded amount of time:</p><pre class="programlisting">val startTime = System.nanoTime 
val futures = for (_ &lt;- 0 until 16) yield Future { 
  Thread.sleep(1000) 
} 
for (f &lt;- futures) Await.ready(f, Duration.Inf) 
val endTime = System.nanoTime 
log(s"Total time = ${(endTime - startTime) / 1000000} ms") 
log(s"Total CPUs = ${Runtime.getRuntime.availableProcessors}") 
</pre><p>Assume that you have eight cores in your processor. This program does not end in one second. Instead, theÂ first batch of eight futures started by the <code class="literal">Future.apply</code> method will block all the worker threads for one second, and then another batch of eight futures will block for another second. As a result, none of our eight processor cores can do any useful work for one second.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip48"></a>Tip</h3><p>Avoid blocking in asynchronous computations, as it can cause thread starvation.</p></div><p>If you absolutely must block, then the part of the code that blocks should be enclosed within the <code class="literal">blocking</code> call. This signals to the execution context that the worker thread is blocked and allows it to temporarily spawn additional worker threads if necessary:</p><pre class="programlisting">  val futures = for (_ &lt;- 0 until 16) yield Future { 
    blocking { 
      Thread.sleep(1000) 
    } 
  } 
</pre><p>With the <code class="literal">blocking</code> call around the <code class="literal">sleep</code> call, the <code class="literal">global</code> execution context spawns additional threads when it detects that there is more work than the worker threads. All 16 future computations can execute concurrently, and the program terminates after one second.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note49"></a>Note</h3><p>The <code class="literal">Await.ready</code> and <code class="literal">Await.result</code> statements block the caller thread until the future is completed, and are in most cases used outside theÂ asynchronous computations. They are blocking operations. The <code class="literal">blocking</code> statement is used inside asynchronous code to designate that the enclosed block of code contains a blocking call. It is not a blocking operation by itself.</p></div></div></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec29"></a>The Scala Async library</h2></div></div><hr /></div><p>In the final section of this chapter, we turn to the Scala <span class="strong"><strong>Async</strong></span> library. You should understand that the Scala Async library does not add anything conceptually new to futures and promises. If you got this far in this chapter, you already know everything that you need to know about asynchronous programming, callbacks, future composition, promises, and blocking. You can start building asynchronous applications right away.</p><p>Having said that, the Scala Async library is a convenient library for futures and promises that allowÂ expressing chains of asynchronous computations more conveniently. Every program that you express using the Scala Async library can also be expressed using futures and promises. Often, the Scala Async library allows writing shorter, more concise, and understandable programs.</p><p>The Scala Async library introduces two new method calls--the <code class="literal">async</code> and <code class="literal">await</code> methods. The <code class="literal">async</code> method is conceptually equivalent to the <code class="literal">Future.apply</code> method; it starts an asynchronous computation and returns a future object. The <code class="literal">await</code> method should not be confused with the <code class="literal">Await</code> object used to block on futures. The <code class="literal">await</code> method takes a future and returns that future's value. However, unlike the methods on the <code class="literal">Await</code> object, the <code class="literal">await</code> method does not block the underlying thread; we will soon see how this is possible.</p><p>The Scala Async library is currently not part of the Scala standard library. To use it, we need to add the following line to our build definition file:</p><pre class="programlisting">libraryDependencies += 
  "org.scala-lang.modules" %% "scala-async" % "0.9.1" 
</pre><p>As a simple example, consider the <code class="literal">delay</code> method, which returns a future that is completed after <code class="literal">n</code> seconds. We use the <code class="literal">async</code> method to start an asynchronous computation that calls the <code class="literal">sleep</code> method. When the <code class="literal">sleep</code> call returns, the future is completed:</p><pre class="programlisting">def delay(n: Int): Future[Unit] = async { 
  blocking { Thread.sleep(n * 1000) } 
} 
</pre><p>The <code class="literal">await</code> method must be statically enclosed within an <code class="literal">async</code> block in the same method; it is a compile-time error to invoke <code class="literal">await</code> outside of theÂ <code class="literal">async</code> block. Whenever the execution inside the <code class="literal">async</code> block reaches an <code class="literal">await</code> statement, it stops until the value from the future in the <code class="literal">await</code> statement becomes available. Consider the following example:</p><pre class="programlisting">async { 
  log("T-minus 1 second") 
  await { delay(1) } 
  log("done!") 
} 
</pre><p>Here, the asynchronous computation in the <code class="literal">async</code> block prints <code class="literal">"T-minus 1 second"</code>. It then calls <code class="literal">delay</code> to obtain a future that is completed after one second. The <code class="literal">await</code> call designates that the computation can proceed only after the future returned by <code class="literal">delay</code> completes. After that happens, the <code class="literal">async</code> block prints the text <code class="literal">done</code>.</p><p>The natural question is: How can the Scala Async library execute the preceding example without blocking? The answer is that the Scala Async library uses Scala Macros to transform the code inside the <code class="literal">async</code> statement. The code is transformed in such a way that the code after every <code class="literal">await</code> statement becomes a callback registered to the future inside <code class="literal">await</code>. Immensely simplifying how this transformation works under the hood, the preceding code is equivalent to the following computation:</p><pre class="programlisting">Future { 
  log("T-minus 1 second") 
  delay(1) foreach { 
    case x =&gt; log("done!") 
  } 
} 
</pre><p>As you can see, the equivalent code produced by the Scala Async library is completely non-blocking. The advantage of the <code class="literal">async</code>/<code class="literal">await</code> style code is that it is much more understandable. For example, it allows defining a custom <code class="literal">countdown</code> method that takes a number of seconds and an <code class="literal">n</code> and a <code class="literal">f</code> function to execute every second. We use a <code class="literal">while</code> loop for the <code class="literal">countdown</code> method inside the <code class="literal">async</code> block: each time an <code class="literal">await</code> instance is invoked, the execution is postponed for one second. The implementation using the Scala Async library feels like regular procedural code, but it does not incur the cost of blocking:</p><pre class="programlisting">def countdown(n: Int)(f: Int =&gt; Unit): Future[Unit] = async { 
  var i = n 
  while (i &gt; 0) { 
    f(i) 
    await { delay(1) } 
    i -= 1 
  } 
} 
</pre><p>The <code class="literal">countdown</code> method can be used from the main thread to print to the standard output every second. Since the <code class="literal">countdown</code> method returns a future, we can additionally install a <code class="literal">foreach</code> callback to execute after the <code class="literal">countdown</code> method is over:</p><pre class="programlisting">countdown(10) { n =&gt; log(s"T-minus $n seconds") } foreach { 
  case _ =&gt; log(s"This program is over!") 
} 
</pre><p>Having seen how expressive theÂ Async library is in practice, the question is: When to use it in place of callbacks, future combinators, and for-comprehensions? In most cases, whenever you can express a chain of asynchronous computations inside a single method, you are free to use Async. You should use your best judgment when applying it; always choose the programming style that results in concise, more understandable, and more maintainable programs.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip50"></a>Tip</h3><p>Use the Scala Async library when a chain of asynchronous computations can be expressed more intuitively as procedural code using the <code class="literal">async</code> and <code class="literal">await</code> statements.</p></div></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec30"></a>Alternative future frameworks</h2></div></div><hr /></div><p>Scala futures and promises API resulted from an attempt to consolidate several different APIs for asynchronous programming, among them, legacy Scala futures, Akka futures, Scalaz futures, and Twitter's Finagle futures. Legacy Scala futures and Akka futures have already converged to the futures and promises APIs that you've learned about so far in this chapter. Finagle's <code class="literal">com.twitter.util.Future</code> type is planned to eventually implement the same interface as <code class="literal">scala.concurrent.Future</code>, while the Scalaz <code class="literal">scalaz.concurrent.Future</code> type implements a slightly different interface. In this section, we give a brief description of Scalaz futures.</p><p>To use Scalaz, we add the following dependency to the <code class="literal">build.sbt</code> file:</p><pre class="programlisting">libraryDependencies += 
  "org.scalaz" %% "scalaz-concurrent" % "7.0.6" 
</pre><p>We now encode an asynchronous tombola program using Scalaz. The <code class="literal">Future</code> type in Scalaz does not have the <code class="literal">foreach</code> method. Instead, we use its <code class="literal">runAsync</code> method, which asynchronously runs the future computation to obtain its value, and then calls the specified callback:</p><pre class="programlisting">import scalaz.concurrent._ 
object Scalaz extends App { 
  val tombola = Future { 
    scala.util.Random.shuffle((0 until 10000).toVector) 
  } 
  tombola.runAsync { numbers =&gt; 
    log(s"And the winner is: ${numbers.head}") 
  } 
  tombola.runAsync { numbers =&gt; 
    log(s"... ahem, winner is: ${numbers.head}") 
  } 
} 
</pre><p>Unless you are terribly lucky and draw the same permutation twice, running this program reveals that the two <code class="literal">runAsync</code> calls print different numbers. Each <code class="literal">runAsync</code> call separately computes the permutation of the random numbers. This is not surprising, as Scalaz futures have the pull semantics, in which the value is computed each time some callback requests it, in contrast to the push semantics of Finagle and Scala futures, in which the callback is stored, and applied if and when the asynchronously computed value becomes available.</p><p>To achieve the same semantics as we would have with Scala futures, we need to use the <code class="literal">start</code> combinator that runs the asynchronous computation once, and caches its result:</p><pre class="programlisting">val tombola = Future { 
  scala.util.Random.shuffle((0 until 10000).toVector) 
} start 
</pre><p>With this change, the two <code class="literal">runAsync</code> calls use the same permutation of random numbers <code class="literal">tombola</code>, and print the same values.</p><p>We will not delve further into the internals of alternate frameworks. The fundamentals of futures and promises that you learned about in this chapter should be sufficient to easily familiarize yourself with other asynchronous programming libraries, should the need arise.</p></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec31"></a>Summary</h2></div></div><hr /></div><p>This chapter presented some powerful abstractions for asynchronous programming. We have seen how to encode latency with the <code class="literal">Future</code> type, how to avoid blocking with callbacks on futures, and how to compose values from multiple futures. We have learned that futures and promises are closely tied together and that promises allow interfacing with legacy callback-based systems. In cases where blocking was unavoidable, we learned how to use the <code class="literal">Await</code> object and the <code class="literal">blocking</code> statement. Finally, we learned that the Scala Async library is a powerful alternative for expressing future computations more concisely.</p><p>Futures and promises only allow dealing with a single value at a time. What if an asynchronous computation produces more than a single value before completing? Similarly, how do we efficiently execute thousands of asynchronous operations on different elements of large datasets? Should we use futures in such cases? In the next chapter, we will explore Scala's support for data-parallelism, a form of concurrency where similar asynchronous computations execute in parallel on different collection elements. We will see that using data-parallel collections is preferable to using futures when collections are large, as it results in a better performance.</p></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec32"></a>Exercises</h2></div></div><hr /></div><p>The following exercises summarize what we have learned about futures and promises in this chapter, and require implementing custom future factory methods and combinators. Several exercises also deal with several deterministic programming abstractions that were not covered in this chapter, such as single-assignment variables and maps:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Implement a command-line program that asks the user to input a URL of some website, and displays the HTML of that website. Between the time that the user hits <span class="strong"><strong>ENTER</strong></span> and the time that the HTML is retrieved, the program should repetitively print a <code class="literal">.</code> to the standard output every 50 milliseconds, with a 2 second timeout. Use only futures and promises, and avoid synchronization primitives from the previous chapters. You may reuse the <code class="literal">timeout</code> method defined in this chapter.</p></li><li><p>Implement an abstraction called a single-assignment variable, represented by the <code class="literal">IVar</code> class:
</p><pre class="programlisting">            class IVar[T] { 
              def apply(): T = ??? 
              def :=(x: T): Unit = ??? 
            } 
</pre><p>
</p><p>When created, the <code class="literal">IVar</code> class does not contain a value, and calling <code class="literal">apply</code> results in an exception. After a value is assigned using the <code class="literal">:=</code> method, subsequent calls to <code class="literal">:=</code> throw an exception, and the <code class="literal">apply</code> method returns the previously assigned value. Use only futures and promises, and avoid the synchronization primitives from the previous chapters.</p></li><li><p>Extend the <code class="literal">Future[T]</code> type with the <code class="literal">exists</code> method, which takes a predicate and returns a <code class="literal">Future[Boolean]</code> object:
</p><pre class="programlisting">            def exists(p: T =&gt; Boolean): Future[Boolean] 
</pre><p>
</p><p>The resulting future is completed with <code class="literal">true</code> if and only if the original future is completed and the predicate returns <code class="literal">true</code>, and <code class="literal">false</code> otherwise. You can use future combinators, but you are not allowed to create any <code class="literal">Promise</code> objects in the implementation.</p></li><li><p>Repeat the previous exercise, but use <code class="literal">Promise</code> objects instead of future combinators.</p></li><li><p>Repeat the previous exercise, but use the Scala Async framework.</p></li><li><p>Implement the <code class="literal">spawn</code> method, which takes a command-line string, asynchronously executes it as a child process, and returns a future with the exit code of the child process:
</p><pre class="programlisting">            def spawn(command: String): Future[Int] 
</pre><p>
</p><p>Make sure that your implementation does not cause thread starvation.</p></li><li><p>Implement the <code class="literal">IMap</code> class, which represents a single-assignment map:
</p><pre class="programlisting">            class IMap[K, V] { 
              def update(k: K, v: V): Unit 
              def apply(k: K): Future[V] 
            } 
</pre><p>
</p><p>Pairs of keys and values can be added to the <code class="literal">IMap</code> object, but they can never be removed or modified. A specific key can be assigned only once, and subsequent calls to <code class="literal">update</code> with that key result in an exception. Calling <code class="literal">apply</code> with a specific key returns a future, which is completed after that key is inserted into the map. In addition to futures and promises, you may use the <code class="literal">scala.collection.concurrent.Map</code> class.</p></li><li><p>Extend the typeÂ <code class="literal">Promise[T]</code>Â  with the <code class="literal">compose</code> method, which takes a function of the typeÂ <code class="literal">S =&gt; T</code>Â , and returns a <code class="literal">Promise[S]</code> object:
</p><pre class="programlisting">            def compose[S](f: S =&gt; T): Promise[S] 
</pre><p>
</p><p>Whenever the resulting promise is completed with some value <code class="literal">x</code> of the type <code class="literal">S</code> (or failed), the original promise must be completed with the value <code class="literal">f(x)</code> asynchronously (or failed), unless the original promise is already completed.</p></li><li><p>Implement the <code class="literal">scatterGather</code> method, which given a sequence of tasks, runs those tasks as parallel asynchronous computations, then combines the results, and returns a future that contains the sequence of results from different tasks. The <code class="literal">scatterGather</code> method has the following interface:
</p><pre class="programlisting">            def scatterGather[T](tasks: Seq[() =&gt; T]): Future[Seq[T]] 
</pre></li><li><p>Implement another version of the <code class="literal">timeout</code> method shown in this chapter, but without using the <code class="literal">blocking</code> construct or <code class="literal">Thread.sleep</code>. Instead use the <code class="literal">java.util.Timer</code> class from the JDK. What are the advantages of this new implementation?</p></li><li><p>A directed graph is a data structure composed from a finite set of nodes, where each node has a finite number of directed edges that connect it with other nodes in the graph. A directed acyclic graph, or shorter, DAG, is a directed graph data structure in which, starting from any node N and following any path along the directed edges, we cannot arrive back at N. In other words, directed edges of a DAG never form a cycle.
</p><p>One way to represent the nodes of the DAG data structure is as follows:</p><p>
</p><pre class="programlisting">            class DAG[T](val value: T) { 
              val edges = scala.collection.mutable.Set[DAG[T]] 
            } 
</pre><p>
</p><p>Here is an example of a DAG declaration:</p><p>
</p><pre class="programlisting">            val a = new DAG("a") 
            val b = new DAG("b") 
            val c = new DAG("c") 
            val d = new DAG("d") 
            val e = new DAG("e") 
            a.edges += b 
            b.edges += c 
            b.edges += d 
            c.edges += e 
            d.edges += e 
</pre><p>
</p><p>The preceding DAG declaration is shown graphically in the following figure:</p><p>
</p><div class="mediaobject"><img src="graphics/image_04_006.jpg" /></div><p>
</p><p>DAGs are often used to declare dependencies between different items, for example, build tasks in a project build tool or an IDE. Your task is to implement the <code class="literal">fold</code> method that takes a DAG node and a function that maps each item and its inputs into some value, and then returns the future with the resulting value of the input node:</p><p>
</p><pre class="programlisting">            def fold[T, S](g: DAG[T], f: (T, Seq[S]) =&gt; S): Future[S] 
</pre><p>
</p><p>The <code class="literal">fold</code> method runs an asynchronous task for each item in the DAG to map the item and its inputs to a new value. Dependencies between DAG items must be respected: an item can only run after all of its dependencies have been computed. For example, in the previous figure, task <code class="literal">b</code> can only run after both <code class="literal">c</code> and <code class="literal">d</code> have produced a result.</p></li></ol></div></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch05"></a>ChapterÂ 5.Â Data-Parallel Collections</h2></div></div></div><div class="blockquote"><table border="0" width="100%" cellspacing="0" cellpadding="0" class="blockquote" summary="Block quote"><tr><td width="10%" valign="top">Â </td><td width="80%" valign="top"><p><span class="emphasis"><em>"Premature optimization is the root of all evil."</em></span></p></td><td width="10%" valign="top">Â </td></tr><tr><td width="10%" valign="top">Â </td><td colspan="2" align="right" valign="top">--<span class="attribution"><span class="emphasis"><em>Donald Knuth</em></span></span></td></tr></table></div><p>So far, we have been composing multiple threads of computation into safe concurrent programs. In doing so, we focused on ensuring their correctness. We saw how to avoid blocking in concurrent programs, react to the completion of asynchronous computations, and how to use concurrent data structures to communicate information between threads. All these tools made organizing the structure of concurrent programs easier. In this chapter, we will focus mainly on achieving good performance. We require minimal or no changes in the organization of existing programs, but we will study how to reduce their running time using multiple processors. Futures from the previous chapter allowed doing this to a certain extent, but they are relatively heavyweight and inefficient when the asynchronous computation in each future is short.</p><p><span class="strong"><strong>Data parallelism</strong></span> is a form of computation where the same computation proceeds in parallel on different data elements. Rather than having concurrent computation tasks that communicate through the use of synchronization, in data-parallel programming, independent computations produce values that are eventually merged together in some way. An input to a data-parallel operation is usually a dataset such as a collection, and the output can be a value or another dataset.</p><p>In this chapter, we will study the following topics:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Starting a data-parallel operation</p></li><li style="list-style-type: disc"><p>Configuring the parallelism level of a data-parallel collection</p></li><li style="list-style-type: disc"><p>Measuring performance and why it is important</p></li><li style="list-style-type: disc"><p>Differences between using sequential and parallel collections</p></li><li style="list-style-type: disc"><p>Using parallel collections together with concurrent collections</p></li><li style="list-style-type: disc"><p>Implementing a custom parallel collection, such as a parallel string</p></li><li style="list-style-type: disc"><p>Alternative data-parallel frameworks</p></li></ul></div><p>In Scala, data-parallel programming was applied to the standard collection framework to accelerate bulk operations that are, by their nature, declarative and fit data parallelism well. Before studying data-parallel collections, we will present a brief overview of the Scala collection framework.</p></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec33"></a>Scala collections in a nutshell</h2></div></div><hr /></div><p>The Scala collections module is a package in the Scala standard library that contains a variety of general-purpose collection types. Scala collections provide a general and easy-to-use way of declaratively manipulating data using functional combinators. For example, in the following program, we use the <code class="literal">filter</code> combinator on a range of numbers to return a sequence of palindromes between 0 and 100,000; that is, numbers that are read in the same way in both the forward and reverse direction:</p><pre class="programlisting">(0 until 100000).filter(x =&gt; x.toString == x.toString.reverse)
</pre><p>Scala collections define three basic types of collections: <span class="strong"><strong>sequences</strong></span>, <span class="strong"><strong>maps</strong></span>, and <span class="strong"><strong>sets</strong></span>. Elements stored in sequences are ordered and can be retrieved using the <code class="literal">apply</code> method and an integer index. Maps store key-value pairs and can be used to retrieve a value associated with a specific key. Sets can be used to test the element membership with the <code class="literal">apply</code> method.</p><p>The Scala collection library makes a distinction between immutable collections, which cannot be modified after they are created, and mutable collections which can be updated after they are created. Commonly used immutable sequences are <code class="literal">List</code> and <code class="literal">Vector</code>, while <code class="literal">ArrayBuffer</code> is the mutable sequence of choice in most situations. Mutable <code class="literal">HashMap</code> and <code class="literal">HashSet</code> collections are maps and sets implemented using hash tables, while immutable <code class="literal">HashMap</code> and <code class="literal">HashSet</code> collections are based on the less widely known hash trie data structure.</p><p>Scala collections can be transformed into their parallel counterparts by calling the <code class="literal">par</code> method. The resulting collection is called a <span class="strong"><strong>parallel collection</strong></span>, and its operations are accelerated by using multiple processors simultaneously. The previous example can run in parallel, as follows:</p><pre class="programlisting">(0 until 100000).par.filter(x =&gt; x.toString == x.toString.reverse)
</pre><p>In the preceding code line, the filter combinator is a data-parallel operation. In this chapter, we will study parallel collections in more detail. We will see when and how to create parallel collections, study how they can be used together with sequential collections, and conclude by implementing a custom parallel collection class.</p></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec34"></a>Using parallel collections</h2></div></div><hr /></div><p>Most of the concurrent programming utilities we have studied so far are used in order to enable different threads of computation to exchange information. Atomic variables, the <code class="literal">synchronized</code> statement, concurrent queues, futures, and promises are focused on ensuring the correctness of a concurrent program. On the other hand, the parallel collection programming model is designed to be largely identical to that of sequential Scala collections; parallel collections exist solely in order to improve the running time of the program. In this chapter, we will measure the relative speedup of programs using parallel collections. To make this task easier, we will introduce the <code class="literal">timed</code> method to the package object used for the examples in this chapter. This method takes a block of code body, and returns the running time of the executing block of code <code class="literal">body</code>. It starts by recording the current time with the <code class="literal">nanoTime</code> method from the JDK <code class="literal">System</code> class. It then runs the body, records the time after the body executes, and computes the time difference:</p><pre class="programlisting">@volatile var dummy: Any = _
def timed[T](body: =&gt;T): Double = {
  val start = System.nanoTime
  dummy = body
  val end = System.nanoTime
  ((end - start) / 1000) / 1000.0
}
</pre><p>Certain runtime optimizations in the JVM, such as dead-code elimination, can potentially remove the invocation of the <code class="literal">body</code> block, causing us to measure an incorrect running time. To prevent this, we assign the return value of the <code class="literal">body</code> block to a volatile field named <code class="literal">dummy</code>.</p><p>Program performance is subject to many factors, and it is very hard to predict in practice. Whenever you can, you should validate your performance assumptions with measurements. In the following example, we use the Scala <code class="literal">Vector</code> class to create a vector with five million numbers and then shuffle that vector using the <code class="literal">Random</code> class from the <code class="literal">scala.util</code> package. We then compare the running time of the sequential and parallel <code class="literal">max</code> methods, which both find the greatest integer in the <code class="literal">numbers</code> collection:</p><pre class="programlisting">import scala.collection._
import scala.util.Random
object ParBasic extends App {
  val numbers = Random.shuffle(Vector.tabulate(5000000)(i =&gt; i))
  val seqtime = timed { numbers.max }
  log(s"Sequential time $seqtime ms")
  val partime = timed { numbers.par.max }
  log(s"Parallel time $partime ms")
}
</pre><p>Running this program on an Intel i7-4900MQ quad-core processor with hyper-threading and Oracle JVM Version 1.7.0_51, we find that the sequential <code class="literal">max</code> method takes 244 milliseconds, while its parallel version takes 35 milliseconds. This is partly because parallel collections are optimized better than their sequential counterparts, and partly because they use multiple processors. However, on different processors and JVM implementations, results will vary.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip51"></a>Tip</h3><p>Always validate assumptions about performance by measuring the execution time.</p></div><p>The <code class="literal">max</code> method is particularly well-suited for parallelization. Worker threads can independently scan subsets of the collection, such as <code class="literal">numbers</code>. When a worker thread finds the greatest integer in its own subset, it notifies the other processors and they agree on the greatest result. This final step takes much less time than searching for the greatest integer in a collection subset. We say that the <code class="literal">max</code> method is <span class="strong"><strong>trivially parallelizable</strong></span>.</p><p>In general, data-parallel operations require more inter-processor communication than the <code class="literal">max</code> method. Consider the <code class="literal">incrementAndGet</code> method on atomic variables from <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Traditional Building Blocks of Concurrency</em></span>. We can use this method once again to compute unique identifiers. This time, we will use parallel collections to compute a large number of unique identifiers:</p><pre class="programlisting">import java.util.concurrent.atomic._
object ParUid extends App {
  private val uid = new AtomicLong(0L)
  val seqtime = timed {
    for (i &lt;- 0 until 10000000) uid.incrementAndGet()
  }
  log(s"Sequential time $seqtime ms")
  val partime = timed {
    for (i &lt;- (0 until 10000000).par) uid.incrementAndGet()
  }
  log(s"Parallel time $partime ms")
}
</pre><p>This time, we use parallel collections in a <code class="literal">for</code> loop; recall that every occurrence of a <code class="literal">for</code> loop is desugared into the <code class="literal">foreach</code> call by the compiler. The parallel <code class="literal">for</code> loop from the preceding code is equivalent to the following:</p><pre class="programlisting">(0 until 10000000).par.foreach(i =&gt; uid.incrementAndGet())
</pre><p>When the <code class="literal">foreach</code> method is called on a parallel collection, collection elements are processed concurrently. This means that separate worker threads simultaneously invoke the specified function, so proper synchronization must be applied. In our case, this synchronization is ensured by the atomic variable, as explained in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Traditional Building Blocks of Concurrency</em></span>.</p><p>Running this program on our machine reveals that there is no increase in speed. In fact, the parallel version of the program is even slower; our program prints 320 milliseconds for the sequential <code class="literal">foreach</code> call, and 1,041 milliseconds for the parallel <code class="literal">foreach</code> call.</p><p>You might be surprised to see this; shouldn't a program be running at least four times faster on a quad-core processor with hyper-threading? As shown by the preceding example, this is not always the case. The parallel <code class="literal">foreach</code> call is slower because the worker threads simultaneously invoke the <code class="literal">incrementAndGet</code> method on the atomic variable, <code class="literal">uid</code>, and write to the same memory location at once.</p><p>Memory writes do not goÂ directly to <span class="strong"><strong>Random Access Memory</strong></span> (<span class="strong"><strong>RAM</strong></span>) in modern architectures, as this would be too slow. Instead, modern computer architectures separate the CPU from the RAM with multiple levels of caches: smaller, more expensive, and much faster memory units that hold copies of parts of the RAM that the processor is currently using. The cache level closest to the CPU is called the L1 cache. The L1 cache is divided into short contiguousÂ parts called <span class="strong"><strong>cache lines</strong></span>. Typically, a cache-line size is 64 bytes. Although multiple cores can read the same cache line simultaneously, in standard multicore processors, the cache line needs to be in exclusive ownership when a core writes to it. When another core requests to write to the same cache line, the cache line needs to be copied to that core's L1 cache. The cache coherence protocol that enablesÂ this is called <span class="strong"><strong>Modified Exclusive Shared Invalid</strong></span> (<span class="strong"><strong>MESI</strong></span>), and its specifics are beyond the scope of this book. All you need to know is that exchanging cache-line ownership can be relatively expensive in terms of the processor's time scale.</p><p>Since the <code class="literal">uid</code> variable is atomic, the JVM needs to ensure a happens-before relationship between the writes and reads of the <code class="literal">uid</code> variable, as we know from <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Concurrency on the JVM and the Java Memory Model</em></span>. To ensure the happens-before relationship, memory writes have to be visible to other processors. The only way to ensure this is to obtain the cache line inÂ exclusive mode before writing to it. In our example, different processor cores repetitively exchange the ownership of the cache line in which the <code class="literal">uid</code> variable is allocated, and the resulting program becomes much slower than its sequential version. This is shown in the following diagram:</p><div class="mediaobject"><img src="graphics/image_05_001.jpg" /></div><p>If different processors only read a shared memory location, then there is no slowdown. Writing to the same memory location is, on the other hand, an obstacle to scalability.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip52"></a>Tip</h3><p>Writing to the same memory location with proper synchronization leads to performance bottlenecks and contention; avoid this in data-parallel operations.</p></div><p>Parallel programs share other resources in addition to computing power. When different parallel computations request more resources than are currently available, an effect known as <span class="strong"><strong>resource contention</strong></span> occurs. The specific kind of resource contention that occurs in our example is called a <span class="strong"><strong>memory contention</strong></span>, a conflict over exclusive rights to write to a specific part of memory.</p><p>We can expect the same kind of performance degradation when using multiple threads to concurrently start the <code class="literal">synchronized</code> statement on the same object, repetitively modifying the same key in a concurrent map or simultaneously enqueueing elements to a concurrent queue; all these actions require writes to the same memory location. Nonetheless, this does not mean that threads should never write to the same memory locations. In some applications, concurrent writes occur very infrequently; the ratio between the time spent writing to contended memory locations and the time spent doing other work determines whether parallelization is beneficial or not. It is difficult to predict this ratio by just looking at the program; the <code class="literal">ParUid</code> example serves to illustrate that we should always measure in order to see the impact of contention.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec42"></a>Parallel collection class hierarchy</h3></div></div></div><p>As we saw, parallel collection operations execute on different worker threads simultaneously. At any point during the execution of a parallel operation, an element in a parallel collection can be processed by at most one worker thread executing that operation. The block of code associated with the parallel operation is executed on each of the elements separately; in the <code class="literal">ParUid</code> example, the <code class="literal">incrementAndGet</code> method is called concurrently many times. Whenever a parallel operation executes any side-effects, it must take care to use proper synchronization; the naive approach of using <code class="literal">var</code> to store <code class="literal">uid</code> causes data races as it did in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Concurrency on the JVM and the Java Memory Model</em></span>. This is not the case with sequential Scala collections.</p><p>The consequence is that a parallel collection cannot be a subtype of a sequential collection. If it were, then the <span class="emphasis"><em>Liskov substitution principle</em></span> would be violated. The Liskov substitution principle states that if type <code class="literal">S</code> is a subtype of <code class="literal">T</code>, then the object of type <code class="literal">T</code> can be replaced with objects of type <code class="literal">S</code> without affecting the correctness of the program.</p><p>In our case, if parallel collections are subtypes of sequential collections, then some methods can return a sequential sequence collection with the static type <code class="literal">Seq[Int]</code>, where the sequence object is a parallel sequence collection at runtime. Clients can call methods such as <code class="literal">foreach</code> on the collection without knowing that the body of the <code class="literal">foreach</code> method needs synchronization, and their programs would not work correctly. For these reasons, parallel collections form a hierarchy that is separate from the sequential collections, as shown in the following diagram:</p><div class="mediaobject"><img src="graphics/image_05_002.jpg" /><div class="caption"><p>Scala collection hierarchy</p></div></div><p>The preceding diagram shows the simplified Scala collection hierarchy with sequential collections on the left. The most general collection type is called <code class="literal">Traversable</code>. Different collection operations such as <code class="literal">find</code>, <code class="literal">map</code>, <code class="literal">filter</code>, or <code class="literal">reduceLeft</code> are implemented in terms of its abstract <code class="literal">foreach</code> method. Its <code class="literal">Iterable[T]</code> subtype offers additional operations such as <code class="literal">zip</code>, <code class="literal">grouped</code>, <code class="literal">sliding</code>, and <code class="literal">sameElements</code>, implemented using its <code class="literal">iterator</code> method. The <code class="literal">Seq</code>, <code class="literal">Map</code>, and <code class="literal">Set</code> traits are iterable collections that represent Scala sequences, maps, and sets, respectively. These traits are used to write code that is generic in the type of the concrete Scala collection. The following <code class="literal">nonNull</code> method copies elements from an <code class="literal">xs</code> collection that are different from <code class="literal">null</code>. Here, the <code class="literal">xs</code> collection can be a <code class="literal">Vector[T]</code>, <code class="literal">List[T]</code>, or some other sequence:</p><pre class="programlisting">def nonNull(xs: Seq[T]): Seq[T] = xs.filter(_ != null)
</pre><p>Parallel collections form a separate hierarchy. The most general parallel collection type is called <code class="literal">ParIterable</code>. Methods such as <code class="literal">foreach</code>, <code class="literal">map</code>, or <code class="literal">reduce</code> on a <code class="literal">ParIterable</code> object execute in parallel. The <code class="literal">ParSeq</code>, <code class="literal">ParMap</code>, and <code class="literal">ParSet</code> collections are parallel collections that correspond to <code class="literal">Seq</code>, <code class="literal">Map</code>, and <code class="literal">Set</code>, but are not their subtypes. We can rewrite the <code class="literal">nonNull</code> method to use parallel collections:</p><pre class="programlisting">def nonNull(xs: ParSeq[T]): ParSeq[T] = xs.filter(_ != null)
</pre><p>Although the implementation is identical, we can no longer pass a sequential collection to the <code class="literal">nonNull</code> method. We can call <code class="literal">.par</code> on the sequential <code class="literal">xs</code> collection before passing it to the <code class="literal">nonNull</code> method, but then the <code class="literal">filter</code> method will execute in parallel. Can we instead write code that is agnostic in the type of the collection? The generic collection types: <code class="literal">GenTraversable</code>, <code class="literal">GenIterable</code>, <code class="literal">GenSeq</code>, <code class="literal">GenMap</code>, and <code class="literal">GenSet</code> exist for this purpose. Each of them represents a supertype of the corresponding sequential or parallel collection type. For example, the <code class="literal">GenSeq</code> generic sequence type allows us to rewrite the <code class="literal">nonNull</code> method as follows:</p><pre class="programlisting">def nonNull(xs: GenSeq[T]): GenSeq[T] = xs.filter(_ != null)
</pre><p>When using generic collection types, we need to remember that they might be implemented either as a sequential collection or as a parallel collection. Thus, as a precaution, if operations invoked on a generic collection execute any side effects, you should use synchronization.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip53"></a>Tip</h3><p>Treat operations invoked on a generic collection type as if they are parallel.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec43"></a>Configuring the parallelism level</h3></div></div></div><p>Parallel collections use all the processors by default; their underlying executor has as many workers as there are processors. We can change this default behavior by changing the <code class="literal">TaskSupport</code> object of the parallel collection. The basic <code class="literal">TaskSupport</code> implementation is the <code class="literal">ForkJoinTaskSupport</code> class. It takes a <code class="literal">ForkJoinPool</code> collection and uses it to schedule parallel operations.</p><p>Therefore, to change the parallelism level of a parallel collection, we instantiate a <code class="literal">ForkJoinPool</code> collection with the desired parallelism level:</p><pre class="programlisting">import scala.concurrent.forkjoin.ForkJoinPool
object ParConfig extends App {
  val fjpool = new ForkJoinPool(2)
  val customTaskSupport = new parallel.ForkJoinTaskSupport(fjpool)
  val numbers = Random.shuffle(Vector.tabulate(5000000)(i =&gt; i))
  val partime = timed {
    val parnumbers = numbers.par
    parnumbers.tasksupport = customTaskSupport
    val n = parnumbers.max
    println(s"largest number $n")
  }
  log(s"Parallel time $partime ms")
}
</pre><p>Once a <code class="literal">TaskSupport</code> object is created, we can use it with different parallel collections. Every parallel collection has a <code class="literal">tasksupport</code> field that we use to assign the <code class="literal">TaskSupport</code> object to it.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec44"></a>Measuring the performance on the JVM</h3></div></div></div><p>To correctly measure the running time on the JVM is not an easy task. Under the hood, the JVM does a lot more than meets the eye. The Scala compiler does not produce machine code directly runnable on the CPU. Instead, the Scala compiler produces a special intermediate instruction code called <span class="strong"><strong>Java bytecode</strong></span>. When bytecode from the Scala compiler gets run inside the JVM, at first it executes in so-called <span class="strong"><strong>interpreted mode</strong></span>; the JVM interprets each bytecode instruction and simulates the execution of the program. Only when the JVM decides that the bytecode in a certain method was run often enough does it compile the bytecode to machine code, which can be executed directly on the processor. This process is called <span class="strong"><strong>just-in-time compilation</strong></span>.</p><p>The JVM needs standardized bytecode to be cross-platform; the same bytecode can be run on any processor or operating system that supports the JVM. However, the entire bytecode of a program cannot be translated to the machine code as soon as the program runs; this would be too slow. Instead, the JVM translates parts of the programs, such as specific methods, incrementally, in short compiler runs. In addition, the JVM can decide to additionally optimize certain parts of the program that execute very frequently. As a result, programs running on the JVM are usually slow immediately after they start, and eventually reach their optimal performance. Once this happens, we say that the JVM reached its steady state. When evaluating the performance on the JVM, we are usually interested in the <span class="strong"><strong>steady state</strong></span>; most programs run long enough to achieve it.</p><p>To witness this effect, assume that you want to find out what the <code class="literal">TEXTAREA</code> tag means in HTML. You write the program that downloads the HTML specification and searches for the first occurrence of the <code class="literal">TEXTAREA</code> string. Having mastered asynchronous programming in <a class="link" href="#" linkend="ch04">Chapter 4</a>, <span class="emphasis"><em>Asynchronous Programming with Futures and Promises</em></span>, you can implement the <code class="literal">getHtmlSpec</code> method, which starts an asynchronous computation to download the HTML specification and returns a future value with the lines of the HTML specification. You then install a callback; once the HTML specification is available, you can call the <code class="literal">indexWhere</code> method on the lines to find the line that matches the regular expression <code class="literal">.*TEXTAREA.*</code>:</p><pre class="programlisting">object ParHtmlSearch extends App {
  def getHtmlSpec() = Future {
    val url = "http://www.w3.org/MarkUp/html-spec/html-spec.txt"
    val specSrc = Source.fromURL(url)
    try specSrc.getLines.toArray finally specSrc.close()
  }
  getHtmlSpec() foreach { case specDoc =&gt;
    def search(d: GenSeq[String]): Double =
      timed { d.indexWhere(line =&gt; line.matches(".*TEXTAREA.*")) }
    val seqtime = search(specDoc)
    log(s"Sequential time $seqtime ms")
    val partime = search(specDoc.par)
    log(s"Parallel time $partime ms")
  }
}
</pre><p>Running this example several times from SBT shows that the times vary. At first, the sequential and parallel versions execute for 45 and 16 milliseconds, respectively. Next time, they take 36 and 10 milliseconds, and subsequently 10 and 4 milliseconds. Note that we only observe this effect when running the examples inside the same JVM process as SBT itself.</p><p>We can draw a false conclusion that the steady state was reached at this point. In truth, we should run this program many more times before the JVM properly optimizes it. Therefore, we add the <code class="literal">warmedTimed</code> method to our package object. This method runs the block of code <code class="literal">n</code> times before measuring its running time. We set the default value for the <code class="literal">n</code> variable to <code class="literal">200</code>; although there is no way to be sure that the JVM will reach a steady state after executing the block of code 200 times, this is a reasonable default:</p><pre class="programlisting">def warmedTimed[T](n: Int = 200)(body: =&gt;T): Double = {
  for (_ &lt;- 0 until n) body
  timed(body)
}
</pre><p>We can now call the <code class="literal">warmedTimed</code> method instead of the <code class="literal">timed</code> method in the <code class="literal">ParHtmlSearch</code> example:</p><pre class="programlisting">def search(d: GenSeq[String]) = warmedTimed() {
  d.indexWhere(line =&gt; line.matches(".*TEXTAREA.*"))
}
</pre><p>Doing so changes the running times on our machine to 1.5 and 0.5 milliseconds for the sequential and parallel versions of the program, respectively.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip54"></a>Tip</h3><p>Make sure that the JVM is in the steady state before drawing any premature conclusions about the running time of a program.</p></div><p>There are other reasons why measuring performance on the JVM is hard. Even if the JVM reached a steady state for the part of the program we measure, the <span class="strong"><strong>Just-In-Time</strong></span> (<span class="strong"><strong>JIT</strong></span>) compiler can at any point pause the execution and translate some other part of the program, effectively slowing down our measurement. Then, the JVM provides automatic memory management. In languages such as C++, an invocation of the <code class="literal">new</code> keyword, which is used to allocate an object, must be accompanied by the corresponding <code class="literal">delete</code> call that frees the memory occupied by the object so that it can be reused later. In languages such as Scala and Java, however, there is no <code class="literal">delete</code> statement; objects are eventually freed automatically during the process called <span class="strong"><strong>Garbage Collection</strong></span> (<span class="strong"><strong>GC</strong></span>). Periodically, the JVM stops the execution, scans the heap for all objects no longer used in the program, and frees the memory they occupy. If we measure the running time of code that frequently causes GC cycles, the chances are that GC will skew the measurements. In some cases, the performance of the same program can vary from one JVM process to the other because the objects get allocated in a way that causes a particular memory access pattern, impacting the program's performance.</p><p>To get really reliable running time values, we need to run the code many times by starting separate JVM processes, making sure that the JVM reached a steady state, and taking the average of all the measurements. Frameworks such as <span class="strong"><strong>ScalaMeter</strong></span>, introduced in <a class="link" href="#" linkend="ch09">Chapter 9</a>, <span class="emphasis"><em>Concurrency in Practice</em></span>, go a long way toward automating this process.</p></div></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec35"></a>Caveats with parallel collections</h2></div></div><hr /></div><p>Parallel collections were designed to provide a programming API similar to sequential Scala collections. Every sequential collection has a parallel counterpart and most operations have the same signature in both sequential and parallel collections. Still, there are some caveats when using parallel collections, and we will study them in this section.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec45"></a>Non-parallelizable collections</h3></div></div></div><p>Parallel collections use <span class="strong"><strong>splitters</strong></span>, represented with the <code class="literal">Splitter[T]</code> type, in order to provide parallel operations. A splitter is a more advanced form of an iterator; in addition to the iterator's <code class="literal">next</code> and <code class="literal">hasNext</code> methods, splitters define the <code class="literal">split</code> method, whichÂ divides the splitter <code class="literal">S</code> into a sequence of splitters that traverse parts of the <code class="literal">S</code> splitter:</p><pre class="programlisting">def split: Seq[Splitter[T]]
</pre><p>This method allows separate processors to traverse separate parts of the input collection. The <code class="literal">split</code> method must be implemented efficiently, as this method is invoked many times during the execution of a parallel operation. In the vocabulary of computational complexity theory, the allowed asymptotic running time of the <code class="literal">split</code> method is <span class="strong"><strong>O</strong></span>(log (<span class="emphasis"><em>N</em></span>)), where <span class="emphasis"><em>N</em></span> is the number of elements in the splitter. Splitters can be implemented for flat data structures such as arrays and hash tables, and tree-like data structures such as immutable hash maps and vectors. Linear data structures such as the Scala <code class="literal">List</code> and <code class="literal">Stream</code> collections cannot efficiently implement the <code class="literal">split</code> method. Dividing a long linked list of nodes into two parts requires traversing these nodes, which takes a time that is proportionate to the size of the collection.</p><p>Operations on Scala collections such as <code class="literal">Array</code>, <code class="literal">ArrayBuffer</code>, mutable <code class="literal">HashMap</code> and <code class="literal">HashSet</code>, <code class="literal">Range</code>, <code class="literal">Vector</code>, immutable <code class="literal">HashMap</code> and <code class="literal">HashSet</code>, and concurrent <code class="literal">TrieMap</code> can be parallelized. We call these collections <span class="emphasis"><em>parallelizable</em></span>. Calling the <code class="literal">par</code> method on these collections creates a parallel collection that shares the same underlying dataset as the original collection. No elements are copied and the conversion is fast.</p><p>Other Scala collections need to be converted to their parallel counterparts upon calling <code class="literal">par</code>. We can refer to them as <span class="emphasis"><em>non-parallelizable collections</em></span>. Calling the <code class="literal">par</code> method on non-parallelizable collections entails copying their elements into a new collection. For example, the <code class="literal">List</code> collection needs to be copied to a <code class="literal">Vector</code> collection when the <code class="literal">par</code> method is called, as shown in the following code snippet:</p><pre class="programlisting">object ParNonParallelizableCollections extends App {
  val list = List.fill(1000000)("")
  val vector = Vector.fill(1000000)("")
  log(s"list conversion time: ${timed(list.par)} ms")
  log(s"vector conversion time: ${timed(vector.par)} ms")
}
</pre><p>Calling <code class="literal">par</code> on <code class="literal">List</code> takes 55 milliseconds on our machine, whereas calling <code class="literal">par</code> on <code class="literal">Vector</code> takes 0.025 milliseconds. Importantly, the conversion from a sequential collection to a parallel one is not itself parallelized, and is a possible sequential bottleneck.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip55"></a>Tip</h3><p>Converting a non-parallelizable sequential collection to a parallel collection is not a parallel operation; it executes on the caller thread.</p></div><p>Sometimes, the cost of converting a non-parallelizable collection to a parallel one is acceptable. If the amount of work in the parallel operation far exceeds the cost of converting the collection, then we can bite the bullet and pay the cost of the conversion. Otherwise, it is more prudent to keep the program data in parallelizable collections and benefit from fast conversions. When in doubt, measure!</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec46"></a>Non-parallelizable operations</h3></div></div></div><p>While mostÂ parallel collection operations achieve superior performance by executing on several processors, some operations are inherently sequential, and their semantics do not allow them to execute in parallel. Consider the <code class="literal">foldLeft</code> method from the Scala collections API:</p><pre class="programlisting">def foldLeft[S](z: S)(f: (S, T) =&gt; S): S
</pre><p>This method visits elements of the collection going from left to right and adds them to the accumulator of type <code class="literal">S</code>. The accumulator is initially equal to the zero value <code class="literal">z</code>, and is updated with the function <code class="literal">f</code> that uses the accumulator and a collection element of type <code class="literal">T</code> to produce a new accumulator. For example, given a list of integers <code class="literal">List(1, 2, 3)</code>, we can compute the sum of its integers with the following expression:</p><pre class="programlisting">List(1, 2, 3).foldLeft(0)((acc, x) =&gt; acc + x)
</pre><p>This <code class="literal">foldLeft</code> method starts by assigning <code class="literal">0</code> to <code class="literal">acc</code>. It then takes the first element in the list <code class="literal">1</code> and calls the function <code class="literal">f</code> to evaluate <code class="literal">0 + 1</code>. The <code class="literal">acc</code> accumulator then becomes <code class="literal">1</code>. This process continues until the entire list of elements is visited, and the <code class="literal">foldLeft</code> method eventually returns the result <code class="literal">6</code>. In this example, the <code class="literal">S</code> type of the accumulator is set to theÂ <code class="literal">Int</code> type. In general, the accumulator can have any type. When converting a list of elements to a string, the zero value is an empty string and the function <code class="literal">f</code> concatenates a string and a number.</p><p>The crucial property of the <code class="literal">foldLeft</code> operation is that it traverses the elements of the list by going from left to right. This is reflected in the type of the function <code class="literal">f</code>; it accepts an accumulator of type <code class="literal">S</code> and a list value of type <code class="literal">T</code>. The function <code class="literal">f</code> cannot take two values of the accumulator type <code class="literal">S</code> and merge them into a new accumulator of type <code class="literal">S</code>. As a consequence, computing the accumulator cannot be implemented in parallel; the <code class="literal">foldLeft</code> method cannot merge two accumulators from two different processors. We can confirm this by running the following program:</p><pre class="programlisting">object ParNonParallelizableOperations extends App {
  ParHtmlSearch.getHtmlSpec() foreach { case specDoc =&gt;
    def allMatches(d: GenSeq[String]) = warmedTimed() {
      val results = d.foldLeft("") { (acc, line) =&gt;
        if (line.matches(".*TEXTAREA.*")) s"$acc\n$line" else acc
      }
    }
    val seqtime = allMatches(specDoc)
    log(s"Sequential time - $seqtime ms")
    val partime = allMatches(specDoc.par)
    log(s"Parallel time   - $partime ms")
  }
  Thread.sleep(2000)
}
</pre><p>In the preceding program, we use the <code class="literal">getHtmlSpec</code> method introduced earlier to obtain the lines of the HTML specification. We install a callback using the <code class="literal">foreach</code> call to process the HTML specification once it arrives; the <code class="literal">allMatches</code> method calls the <code class="literal">foldLeft</code> operation to accumulate the lines of the specification that contain the <code class="literal">TEXTAREA</code> string. Running the program reveals that both the sequential and parallel <code class="literal">foldLeft</code> operations take 5.6 milliseconds.</p><p>To specify how the accumulators produced by different processors should be merged together, we need to use the <code class="literal">aggregate</code> method. The <code class="literal">aggregate</code> method is similar to the <code class="literal">foldLeft</code> operation, but it does not specify that the elements are traversed from left to right. Instead, it only specifies that subsets of elements are visited going from left to right; each of these subsets can produce a separate accumulator. The <code class="literal">aggregate</code> method takes an additional function of type <code class="literal">(S, S) =&gt; S</code>, which is used to merge multiple accumulators:</p><pre class="programlisting">d.aggregate("")(
  (acc, line) =&gt;
  if (line.matches(".*TEXTAREA.*")) s"$acc\n$line" else acc,
  (acc1, acc2) =&gt; acc1 + acc2
)
</pre><p>Running the example again shows the difference between the sequential and parallel versions of the program; the parallel <code class="literal">aggregate</code> method takes 1.4 milliseconds to complete on our machine.</p><p>When doing these kinds of reduction operation in parallel, we can alternatively use the <code class="literal">reduce</code> or <code class="literal">fold</code> methods, which do not guarantee going from left to right. The <code class="literal">aggregate</code> method is more expressive, as it allows the accumulator type to be different from the type of the elements in the collection.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip56"></a>Tip</h3><p>Use the <code class="literal">aggregate</code> method to execute parallel reduction operations.</p></div><p>Other inherently sequential operations include <code class="literal">foldRight</code>, <code class="literal">reduceLeft</code>, <code class="literal">reduceRight</code>, <code class="literal">reduceLeftOption</code>, <code class="literal">reduceRightOption</code>, <code class="literal">scanLeft</code>, <code class="literal">scanRight</code>, and methods that produce non-parallelizable collections such as the <code class="literal">toList</code> method.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec47"></a>Side effects in parallel operations</h3></div></div></div><p>As their name implies, parallel collections execute on multiple threads concurrently. We have already learned in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Concurrency on the JVM and the Java Memory Model</em></span>, that multiple threads cannot correctly modify shared memory locations without the use of synchronization. Assigning to a mutable variable from a parallel collection operation may be tempting, but it is almost certainly incorrect. This is best illustrated by the following example, in which we construct two sets, <code class="literal">a</code> and <code class="literal">b</code>, where <code class="literal">b</code> is the subset of the elements in <code class="literal">a</code>, and then uses theÂ <code class="literal">total</code> mutable variable to count the size of the intersection:</p><pre class="programlisting">object ParSideEffectsIncorrect extends App {
  def intersectionSize(a: GenSet[Int], b: GenSet[Int]): Int = {
    var total = 0
    for (x &lt;- a) if (b contains x) total += 1
    total
  }
  val a = (0 until 1000).toSet
  val b = (0 until 1000 by 4).toSet
  val seqres = intersectionSize(a, b)
  val parres = intersectionSize(a.par, b.par)
  log(s"Sequential result - $seqres")
  log(s"Parallel result   - $parres")
}
</pre><p>Instead of returning <code class="literal">250</code>, the parallel version nondeterministically returns various wrong results. Note that you might have to change the sizes of the sets <code class="literal">a</code> and <code class="literal">b</code> to witness this:</p><pre class="programlisting">
<span class="strong"><strong>run-main-32: Sequential result - 250</strong></span>
<span class="strong"><strong>run-main-32: Parallel result   - 244</strong></span>
</pre><p>To ensure that the parallel version returns the correct results, we can use an atomic variable and its <code class="literal">incrementAndGet</code> method. However, this leads to the same scalability problems we had before. A better alternative is to use the parallel <code class="literal">count</code> method:</p><pre class="programlisting">a.count(x =&gt; b contains x)
</pre><p>If the amount of work executed per element is low and the matches are frequent, the parallel <code class="literal">count</code> method will result in better performance than the <code class="literal">foreach</code> method with an atomic variable.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip57"></a>Tip</h3><p>To avoid the need for synchronization and ensure better scalability, favor declarative-style parallel operations instead of the side effects in parallel <code class="literal">for</code> loops.</p></div><p>Similarly, we must ensure that the memory locations read by a parallel operation are protected from concurrent writes. In the last example, the <code class="literal">b</code> set should not be concurrently mutated by some thread while the parallel operation is executing; this leads to the same incorrect results as using mutable variables from within the parallel operation.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec48"></a>Nondeterministic parallel operations</h3></div></div></div><p>In <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Concurrency on the JVM and the Java Memory Model</em></span>, we saw that multithreaded programs can be nondeterministic; given the same inputs, they can produce different outputs depending on the execution schedule. The <code class="literal">find</code> collection operation returns an element matching a given predicate. The parallel <code class="literal">find</code> operation returns whichever element was found first by some processor. In the following example, we use <code class="literal">find</code> to search the HTML specification for occurrences of the <code class="literal">TEXTAREA</code> string; running the example several times gives different results, because the <code class="literal">TEXTAREA</code> string occurs in many different places in the HTML specification:</p><pre class="programlisting">object ParNonDeterministicOperation extends App {
  ParHtmlSearch.getHtmlSpec() foreach { case specDoc =&gt;
    val patt = ".*TEXTAREA.*"
    val seqresult = specDoc.find(_.matches(patt))
    val parresult = specDoc.par.find(_.matches(patt))
    log(s"Sequential result - $seqresult")
    log(s"Parallel result   - $parresult")
  }
  Thread.sleep(3000)
}
</pre><p>If we want to retrieve the first occurrence of the <code class="literal">TEXTAREA</code> string, we need to use <code class="literal">indexWhere</code> instead:</p><pre class="programlisting">val index = specDoc.par.indexWhere(_.matches(patt))
val parresult = if (index != -1) Some(specDoc(index)) else None
</pre><p>Parallel collection operations other than <code class="literal">find</code> are deterministic as long as their operators are <span class="strong"><strong>pure functions</strong></span>. A pure function is always evaluated to the same value, given the same inputs, and does not have any side effects. For example, the function <code class="literal">(x: Int) =&gt; x + 1</code> is a pure function. By contrast, the following function <code class="literal">f</code> is not pure, because it changes the state of the <code class="literal">uid</code> value:</p><pre class="programlisting">val uid = new AtomicInteger(0)
val f = (x: Int) =&gt; (x, uid.incrementAndGet())
</pre><p>Even if a function does not modify any memory locations, it is not pure if it reads memory locations that might change. For example, the following <code class="literal">g</code> function is not pure:</p><pre class="programlisting">val g = (x: Int) =&gt; (x, uid.get)
</pre><p>When used with a non-pure function, any parallel operation can become nondeterministic. Mapping the range of values to unique identifiers in parallel gives a nondeterministic result, as illustrated by the following call:</p><pre class="programlisting">val uids: GenSeq[(Int, Int)] = (0 until 10000).par.map(f)
</pre><p>The resulting sequence, <code class="literal">uids</code>, is different in separate executions. The parallel <code class="literal">map</code> operation retains the relative order of elements from the range <code class="literal">0 until 10000</code>, so the tuples in <code class="literal">uids</code> are ordered by their first elements from 0 until 10,000. On the other hand, the second element in each tuple is assigned nondeterministically; in one execution, the <code class="literal">uids</code> sequence can start with the <code class="literal">(0, 0), (1, 2), (2, 3), ...</code> and in another, with <code class="literal">(0, 0), (1, 4), (2, 9), ...</code>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec49"></a>Commutative and associative operators</h3></div></div></div><p>Parallel collection operations such as <code class="literal">reduce</code>, <code class="literal">fold</code>, <code class="literal">aggregate</code>, and <code class="literal">scan</code> take binary operators as part of their input. A binary operator is a functionÂ Â <code class="literal">op</code> that takes two arguments, <code class="literal">a</code> and <code class="literal">b</code>. We can say that the binary operator <code class="literal">op</code> is <span class="strong"><strong>commutative</strong></span> if changing the order of its arguments returns the same result, that is, <code class="literal">op(a, b) == op(b, a)</code>. For example, adding two numbers together is a commutative operation. Concatenating two strings is not a commutative operation; we get different strings depending on the concatenation order.</p><p>Binary operators for the parallel <code class="literal">reduce</code>, <code class="literal">fold</code>, <code class="literal">aggregate</code>, and <code class="literal">scan</code> operations never need to be commutative. Parallel collection operations always respect the relative order of the elements when applying binary operators, provided that the underlying collections have any ordering. Elements in sequence collections, such as <code class="literal">ArrayBuffer</code> collections, are always ordered. Other collection types can order their elementsÂ but are not required to do so.</p><p>In the following example, we can concatenate the strings inside an <code class="literal">ArrayBuffer</code> collection into one long string by using the sequential <code class="literal">reduceLeft</code> operation and the parallel <code class="literal">reduce</code> operation. We then convert the <code class="literal">ArrayBuffer</code> collection into a set, which does not have an ordering:</p><pre class="programlisting">object ParNonCommutativeOperator extends App {
  val doc = mutable.ArrayBuffer.tabulate(20)(i =&gt; s"Page $i, ")
  def test(doc: GenIterable[String]) {
    val seqtext = doc.seq.reduceLeft(_ + _)
    val partext = doc.par.reduce(_ + _)
    log(s"Sequential result - $seqtext\n")
    log(s"Parallel result   - $partext\n")
  }
  test(doc)
  test(doc.toSet)
}
</pre><p>We can see that the string is concatenated correctly when the parallel <code class="literal">reduce</code> operation is invoked on a parallel array, but the order of the pages is mangled both for the <code class="literal">reduceLeft</code> and <code class="literal">reduce</code> operations when invoked on a set; the default Scala set implementation does not order the elements.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note58"></a>Note</h3><p>Binary operators used in parallel operations do not need to be commutative.</p></div><p>An <code class="literal">op</code> binary operator is <span class="strong"><strong>associative</strong></span> if applying <code class="literal">op</code> consecutively to a sequence of values <code class="literal">a</code>, <code class="literal">b</code>, and <code class="literal">c</code> gives the same result regardless of the order in which the operator is applied, that is, <code class="literal">op(a, op(b, c)) == op(op(a, b), c)</code>. Adding two numbers together or computing the larger of the two numbers is an associative operation. Subtraction is not associative, as <code class="literal">1 - (2 - 3)</code> is different from <code class="literal">(1 - 2) - 3</code>.</p><p>Parallel collection operations usually require associative binary operators. While using subtraction with the <code class="literal">reduceLeft</code> operation means that all the numbers in the collection should be subtracted from the first number, using subtraction in the <code class="literal">reduce</code>, <code class="literal">fold</code>, or <code class="literal">scan</code> methods gives nondeterministic and incorrect results, as illustrated by the following code snippet:</p><pre class="programlisting">object ParNonAssociativeOperator extends App {
  def test(doc: GenIterable[Int]) {
    val seqtext = doc.seq.reduceLeft(_ - _)
    val partext = doc.par.reduce(_ - _)
    log(s"Sequential result - $seqtext\n")
    log(s"Parallel result   - $partext\n")
  }
  test(0 until 30)
}
</pre><p>While the <code class="literal">reduceLeft</code> operation consistently returns <code class="literal">-435</code>, the <code class="literal">reduce</code> operation returns meaningless results at random.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip59"></a>Tip</h3><p>Make sure that binary operators used in parallel operations are associative.</p></div><p>Parallel operations such as <code class="literal">aggregate</code> require the multiple binary operators, <code class="literal">sop</code> and <code class="literal">cop</code>:</p><pre class="programlisting">def aggregate[S](z: S)(sop: (S, T) =&gt; S, cop: (S, S) =&gt; S): S
</pre><p>The <code class="literal">sop</code> operator is of the same type as the operator required by the <code class="literal">reduceLeft</code> operation. It takes an accumulator and the collection element. The <code class="literal">sop</code> operator is used to fold elements within a subset assigned to a specific processor. The <code class="literal">cop</code> operator is used to merge the subsets togetherÂ and is of the same type as the operators for <code class="literal">reduce</code> and <code class="literal">fold</code>. The <code class="literal">aggregate</code> operation requires that <code class="literal">cop</code> is associative and that <code class="literal">z</code> is the <span class="strong"><strong>zero element</strong></span> for the accumulator, that is, <code class="literal">cop(z, a) == a</code>. Additionally, theÂ <code class="literal">sop</code> and <code class="literal">cop</code>Â operators must give the same result irrespective of the order in which element subsets are assigned to processors, that is, <code class="literal">cop(sop(z, a), sop(z, b)) == cop(z, sop(sop(z, a), b))</code>.</p></div></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec36"></a>Using parallel and concurrent collections together</h2></div></div><hr /></div><p>We have already seen that parallel collection operations are not allowed to access mutable states without the use of synchronization. This includes modifying sequential Scala collections from within a parallel operation. Recall that we used a mutable variable in the section on side effects to count the size of the intersection. In the following example, we will download the URL and HTML specifications, convert them to sets of words, and try to find an intersection of their words. In the <code class="literal">intersection</code> method, we use a <code class="literal">HashSet</code> collection and update it in parallel. Collections in the <code class="literal">scala.collection.mutable</code> package are not thread-safe. The following example nondeterministically drops elements, corrupts the buffer state, or throws exceptions:</p><pre class="programlisting">object ConcurrentWrong extends App {
  import ParHtmlSearch.getHtmlSpec
  import ch4.FuturesCallbacks.getUrlSpec
  def intersection(a: GenSet[String], b: GenSet[String]) = {
    val result = new mutable.HashSet[String]
    for (x &lt;- a.par) if (b contains x) result.add(x)
    result
  }
  val ifut = for {
    htmlSpec &lt;- getHtmlSpec()
    urlSpec &lt;- getUrlSpec()
  } yield {
    val htmlWords = htmlSpec.mkString.split("\\s+").toSet
    val urlWords = urlSpec.mkString.split("\\s+").toSet
    intersection(htmlWords, urlWords)
  }
  ifut onComplete { case t =&gt; log(s"Result: $t") }
  Thread.sleep(3000)
}
</pre><p>We learned in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Traditional Building Blocks of Concurrency</em></span>, that concurrent collections can be safely modified by multiple threads without the risk of data corruption. We use the concurrent skip list collection from the JDK to accumulate words that appear in both specifications. The <code class="literal">decorateAsScala</code> object is used to add the <code class="literal">asScala</code> method to Java collections:</p><pre class="programlisting">import java.util.concurrent.ConcurrentSkipListSet
import scala.collection.convert.decorateAsScala._
def intersection(a: GenSet[String], b: GenSet[String]) = {
  val skiplist = new ConcurrentSkipListSet[String]
  for (x &lt;- a.par) if (b contains x) skiplist.add(x)
  val result: Set[String] = skiplist.asScala
  result
}
</pre><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec50"></a>Weakly consistent iterators</h3></div></div></div><p>As we saw in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Traditional Building Blocks of Concurrency</em></span>, iterators on most concurrent collections are weakly consistent. This means that they are not guaranteed to correctly traverse the data structure if some thread concurrently updates the collection during traversal.</p><p>When executing a parallel operation on a concurrent collection, the same limitation applies; the traversal is weakly consistent and might not reflect the state of the data structure at the point when the operation started. The Scala <code class="literal">TrieMap</code> collection is an exception to this rule. In the following example, we will create a <code class="literal">TrieMap</code> collection called <code class="literal">cache</code> containing numbers between 0 and 100, mapped to their string representation. We will then start a parallel operation that traverses theseÂ numbers and adds the mappings for their negative values to the map:</p><pre class="programlisting">object ConcurrentTrieMap extends App {
  val cache = new concurrent.TrieMap[Int, String]()
  for (i &lt;- 0 until 100) cache(i) = i.toString
  for ((number, string) &lt;- cache.par) cache(-number) = s"-$string"
  log(s"cache - ${cache.keys.toList.sorted}")
}
</pre><p>The parallel <code class="literal">foreach</code> operation does not traverse entries added after the parallel operation started; only positive numbers are reflected in the traversal. The <code class="literal">TrieMap</code> collection is implemented using the Ctrie concurrent data structure, which atomically creates a snapshot of the collection when the parallel operation starts. Snapshot creation is efficient and does not require you to copy the elements; subsequent update operations incrementally rebuild parts of the <code class="literal">TrieMap</code> collection.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip60"></a>Tip</h3><p>Whenever program data needs to be simultaneously modified and traversed in parallel, use the <code class="literal">TrieMap</code> collection.</p></div></div></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec37"></a>Implementing custom parallel collections</h2></div></div><hr /></div><p>Parallel collections in the Scala standard library are sufficient for most tasks, but in some cases we want to add parallel operations to our own collections. The Java <code class="literal">String</code> class does not have a direct parallel counterpart in the parallel collections framework. In this section, we will study how to implement a custom <code class="literal">ParString</code> class that supports parallel operations. We will then use our custom parallel collection class in several example programs.</p><p>The first step in implementing a custom parallel collection is to extend the correct parallel collection trait. A parallel string is a sequence of characters, so we need to extend the <code class="literal">ParSeq</code> trait with the <code class="literal">Char</code> type argument. Once a string is created, it can no longer be modified; we say that the string is an immutable collection. For this reason, we extend a subtype of the <code class="literal">scala.collection.parallel.ParSeq</code> trait, the <code class="literal">ParSeq</code> trait from the <code class="literal">scala.collection.parallel.immutable</code> package:</p><pre class="programlisting">class ParString(val str: String) extends immutable.ParSeq[Char] {
  def apply(i: Int) = str.charAt(i)
  def length = str.length
  def splitter = new ParStringSplitter(str, 0, str.length)
  def seq = new collection.immutable.WrappedString(str)
}
</pre><p>When we extend a parallel collection, we need to implement its <code class="literal">apply</code>, <code class="literal">length</code>, <code class="literal">splitter</code>, and <code class="literal">seq</code> methods. The <code class="literal">apply</code> method returns an element at position <code class="literal">i</code> in the sequence, and the <code class="literal">length</code> method returns the total number of elements in the sequence. These methods are equivalent to the methods on sequential collections, so we use the <code class="literal">String</code> class's <code class="literal">charAt</code> and <code class="literal">length</code> methods to implement them. Where defining a custom regular sequence requires implementing its <code class="literal">iterator</code> method, custom parallel collections need a <code class="literal">splitter</code> method. Calling <code class="literal">splitter</code> returns an object of the <code class="literal">Splitter[T]</code> type, a special iterator that can be split into subsets. We implement the <code class="literal">splitter</code> method to return a <code class="literal">ParStringSplitter</code> object, which we will show you shortly. Finally, parallel collections need a <code class="literal">seq</code> method, which returns a sequential Scala collection. Since <code class="literal">String</code> itself comes from Java and is not a Scala collection, we will use its <code class="literal">WrappedString</code> wrapper class from the Scala collections library.</p><p>Our custom parallel collection class is almost complete; we only need to provide the implementation for the <code class="literal">ParStringSplitter</code> object. We will study how to do this next.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec51"></a>Splitters</h3></div></div></div><p>A splitter is an iterator that can be efficiently split into disjoint subsets. Here, efficient means that the splitter's <code class="literal">split</code> method must have <span class="strong"><strong>O</strong></span>(<span class="emphasis"><em>log</em></span>(<span class="emphasis"><em>N</em></span>)) running time, where <span class="emphasis"><em>N</em></span> is the number of elements in the splitter. Stated informally, a splitter is not allowed to copy large parts of the collection when split; if it did, the computational overhead from splitting would overcome any benefits from parallelization and become a serial bottleneck.</p><p>The easiest way to define a new <code class="literal">Splitter</code> class for the Scala parallel collection framework is to extend the <code class="literal">IterableSplitter[T]</code> trait, which has the following simplified interface:</p><pre class="programlisting">trait IterableSplitter[T] extends Iterator[T] {
  def dup: IterableSplitter[T]
  def remaining: Int
  def split: Seq[IterableSplitter[T]]
}
</pre><p>The splitter interface declares the <code class="literal">dup</code> method which duplicates the current splitter. This method simply returns a new splitter pointing to the same subset of the collection. Splitters also define the <code class="literal">remaining</code> method, which returns the number of elements that the splitter can traverse by calling <code class="literal">next</code> before the <code class="literal">hasNext</code> method returns <code class="literal">false</code>. The <code class="literal">remaining</code> method does not change the state of the splitter and can be called as many times as necessary.</p><p>However, the <code class="literal">split</code> method can be called only once and it invalidates the splitter; none of the splitter's methods should be called after calling the <code class="literal">split</code> method. The <code class="literal">split</code> method returns a sequence of splitters that iterate over the disjoint subsets of the original splitter. If the original splitter has two or more elements remaining, then none of the resulting splitters should be empty, and the <code class="literal">split</code> method should return at least two splitters. If the original splitter has a single element or no elements remaining, then <code class="literal">split</code> is allowed to return empty splitters. Importantly, the splitters returned by <code class="literal">split</code> should be approximately equal in size; this helps the parallel collection scheduler achieve good performance.</p><p>To allow sequence-specific operations such as <code class="literal">zip</code>, <code class="literal">sameElements</code>, and <code class="literal">corresponds</code>, parallel sequence collections use a more refined subtype of the <code class="literal">IterableSplitter</code> trait, called the <code class="literal">SeqSplitter</code> trait:</p><pre class="programlisting">trait SeqSplitter[T] extends IterableSplitter[T] {
  def psplit(sizes: Int*): Seq[SeqSplitter[T]]
}
</pre><p>Sequence splitters declare an additional method, <code class="literal">psplit</code>, which takes the list of sizes for the splitter partitions and returns as many splitters andÂ elements as specified by the <code class="literal">sizes</code> parameter. If <code class="literal">sizes</code> specifies more elements than there are available in the splitter, additional empty splitters are returned at the end of the resulting sequence. For example, calling <code class="literal">s.psplit(10, 20, 15)</code> on a splitter with only 15 elements yields three splitters with sizes 10, five, and zero.</p><p>Similarly, if the <code class="literal">sizes</code> parameter specifies fewer elements than there are in the splitter, an additional splitter with the remaining elements is appended at the end.</p><p>Our parallel string class is a parallel sequence, so we need to implement a sequence splitter. We can start by extending the <code class="literal">SeqSplitter</code> class with the <code class="literal">Char</code> type parameter:</p><pre class="programlisting">class ParStringSplitter
  (val s: String, var i: Int, val limit: Int)
extends SeqSplitter[Char] {
</pre><p>We add the <code class="literal">s</code> field pointing to the underlying <code class="literal">String</code> object in the <code class="literal">ParStringSplitter</code> constructor. A parallel string splitter must represent a subset of the elements in the string, so we add an <code class="literal">i</code> field to represent the position of the next character that will be traversed by the splitter. Note that <code class="literal">i</code> does not need to be synchronized; the splitter is only used by one processor at a time. The <code class="literal">limit</code> field contains the position after the last character in the splitter. This way, our splitter class represents substrings of the original string.</p><p>Implementing methods inherited from the <code class="literal">Iterator</code> trait is easy. As long as <code class="literal">i</code> is less than <code class="literal">limit</code>, <code class="literal">hasNext</code> must return <code class="literal">true</code>. The <code class="literal">next</code> method uses <code class="literal">i</code> to read the character at that position, increment <code class="literal">i</code>, and return the character:</p><pre class="programlisting">  final def hasNext = i &lt; limit
  final def next = {
    val r = s.charAt(i)
    i += 1
    r
  }
</pre><p>The <code class="literal">dup</code> and <code class="literal">remaining</code> methods are straightforward; the <code class="literal">dup</code> method creates a new parallel string splitter using the state of the current splitter, and the <code class="literal">remaining</code> method uses <code class="literal">limit</code> and <code class="literal">i</code> to compute the number of remaining elements:</p><pre class="programlisting">  def dup = new ParStringSplitter(s, i, limit)
  def remaining = limit - i
</pre><p>The main parts of a splitter are its <code class="literal">split</code> and <code class="literal">psplit</code> methods. Luckily, <code class="literal">split</code> can be implemented in terms of <code class="literal">psplit</code>. If there is more than one element remaining, we call the <code class="literal">psplit</code> method. Otherwise, if there are no elements to split, we return the <code class="literal">this</code> splitter:</p><pre class="programlisting">  def split = {
    val rem = remaining
    if (rem &gt;= 2) psplit(rem / 2, rem - rem / 2)
    else Seq(this)
  }
</pre><p>The <code class="literal">psplit</code> method uses <code class="literal">sizes</code> to peel off parts of the original splitter. It does so by incrementing the <code class="literal">i</code> variable and creating a new splitter for each size <code class="literal">sz</code> in the <code class="literal">sizes</code> parameter. Recall that the current splitter is considered invalidated after calling the <code class="literal">split</code> or <code class="literal">psplit</code> method, so we are allowed to mutate its <code class="literal">i</code> field:</p><pre class="programlisting">  def psplit(sizes: Int*): Seq[ParStringSplitter] = {
    val ss = for (sz &lt;- sizes) yield {
      val nlimit = (i + sz) min limit
      val ps = new ParStringSplitter(s, i, nlimit)
      i = nlimit
      ps
    }
    if (i == limit) ss
    else ss :+ new ParStringSplitter(s, i, limit)
  }
}
</pre><p>Note that we never copy the string underlying the splitter; instead, we update the indices that mark the beginning and the end of the splitter.</p><p>We have now completed our <code class="literal">ParString</code> class; we can use it to execute parallel operations on strings. We can also use it to count the number of uppercase characters in the string as follows:</p><pre class="programlisting">object CustomCharCount extends App {
  val txt = "A custom text " * 250000
  val partxt = new ParString(txt)
  val seqtime = warmedTimed(50) {
    txt.foldLeft(0) { (n, c) =&gt;
      if (Character.isUpperCase(c)) n + 1 else n
    }
  }
  log(s"Sequential time - $seqtime ms")
  val partime = warmedTimed(50) {
    partxt.aggregate(0)(
      (n, c) =&gt; if (Character.isUpperCase(c)) n + 1 else n,
      _ + _)
  }
  log(s"Parallel time   - $partime ms")
}
</pre><p>On our machine, the sequential <code class="literal">foldLeft</code> call takes 57 milliseconds, and the parallel <code class="literal">aggregate</code> call takes 19 milliseconds. This is a good indication that we have implemented parallel strings efficiently.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec52"></a>Combiners</h3></div></div></div><p>Collection methods in the Scala standard library are divided into two major groups: <span class="strong"><strong>accessor</strong></span> and <span class="strong"><strong>transformer</strong></span> methods. Accessor methods, such asÂ <code class="literal">foldLeft</code>, <code class="literal">find</code>, or <code class="literal">exists</code>, return a single value from the collection. By contrast, transformer methods, such as <code class="literal">map</code>, <code class="literal">filter</code>, or <code class="literal">groupBy</code>, create new collections and return them as results.</p><p>To generically implement transformer operations, the Scala collection framework uses an abstraction called a <span class="strong"><strong>builder</strong></span>, which has roughly the following interface:</p><pre class="programlisting">trait Builder[T, Repr] { // simplified interface
  def +=(x: T): Builder[T, Repr]
  def result: Repr
  def clear(): Unit
}
</pre><p>Here, the <code class="literal">Repr</code> type is of aÂ collection that a specific builder can produce, and <code class="literal">T</code> is the type of its elements. A builder is used by repetitively calling its <code class="literal">+=</code> method to add more elements, and eventually calling the <code class="literal">result</code> method to obtain the collection. After the <code class="literal">result</code> method is called, the contents of the builder are undefined. The <code class="literal">clear</code> method can be used to reset the state of the builder.</p><p>Every collection defines a custom builder used in various transformer operations. For example, the <code class="literal">filter</code> operation is defined in the <code class="literal">Traversable</code> trait, roughly as follows:</p><pre class="programlisting">def newBuilder: Builder[T, Traversable[T]]
def filter(p: T =&gt; Boolean): Traversable[T] = {
  val b = newBuilder
  for (x &lt;- this) if (p(x)) b += x
  b.result
}
</pre><p>In the preceding example, the <code class="literal">filter</code> implementation relies on the abstract <code class="literal">newBuilder</code> method, which is implemented in subclasses of the <code class="literal">Traversable</code> trait. This design allows defining all collection methods once, and only provide the <code class="literal">foreach</code> method (or the iterator) and the <code class="literal">newBuilder</code> method when declaring a new collection type.</p><p><span class="strong"><strong>Combiners</strong></span> are a parallel counterpart of standard builders, and are represented with the <code class="literal">Combiner[T, Repr]</code> type, which subtypes the <code class="literal">Builder[T, Repr]</code> type:</p><pre class="programlisting">trait Combiner[T, Repr] extends Builder[T, Repr] {
  def size: Int
  def combine[N &lt;: T, NewRepr &gt;: Repr]
    (that: Combiner[N, NewRepr]): Combiner[N, NewRepr]
}
</pre><p>The <code class="literal">size</code> method is self-explanatory. The <code class="literal">combine</code> method takes another combiner called <code class="literal">that</code>, and produces a third combiner that contains the elements of the <code class="literal">this</code> and <code class="literal">that</code> combiners. After the <code class="literal">combine</code> method returns, the contents of both the <code class="literal">this</code> and <code class="literal">that</code> combiners are undefined, and should not be used again. This constraint allows reusing the <code class="literal">this</code> or <code class="literal">that</code> combiner object as the resulting combiner. Importantly, if that combiner is the same runtime object as the <code class="literal">this</code> combiner, the <code class="literal">combine</code> method should just return the <code class="literal">this</code> combiner.</p><p>There are three ways to implement a custom combiner, as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><span class="strong"><strong>Merging</strong></span>: Some data structures have an efficient merge operation that can be used to implement the <code class="literal">combine</code> method.</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Two-phase evaluation</strong></span>: Here, elements are first partially sorted into buckets that can be efficiently concatenated, and placed into the final data structure once it is allocated.</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Concurrent data structure</strong></span>: The <code class="literal">+=</code> method is implemented by modifying a concurrent data structure shared between different combiners, and the <code class="literal">combine</code> method does not do anything.</p></li></ul></div><p>Most data structures do not have an efficient merge operation, so we usually have to use two-phase evaluation in the combiner implementation. In the following example, we implement the combiners for parallel strings using two-phase evaluation. The <code class="literal">ParStringCombiner</code> class contains a resizable array, called <code class="literal">chunks</code>, containing <code class="literal">StringBuilder</code> objects. Invoking the <code class="literal">+=</code> method adds a character to the rightmost <code class="literal">StringBuilder</code> object in this array:</p><pre class="programlisting">class ParStringCombiner extends Combiner[Char, ParString] {
  private val chunks = new ArrayBuffer += new StringBuilder
  private var lastc = chunks.last
  var size = 0
  def +=(elem: Char) = {
    lastc += elem
    size += 1
    this
  }
</pre><p>The <code class="literal">combine</code> method takes the <code class="literal">StringBuilder</code> objects of the <code class="literal">that</code> combiner, and adds them to the <code class="literal">chunks</code> array of the <code class="literal">this</code> combiner. It then returns a reference to the <code class="literal">this</code> combiner:</p><pre class="programlisting">  def combine[N &lt;: Char, NewRepr &gt;: ParString]
    (that: Combiner[U, NewTo]) = {
    if (this eq that) this else that match {
      case that: ParStringCombiner =&gt;
        size += that.size
        chunks ++= that.chunks
        lastc = chunks.last
        this
  }
</pre><p>Finally, the <code class="literal">result</code> method allocates a new <code class="literal">StringBuilder</code> object and adds the characters from all the chunks into the resulting string:</p><pre class="programlisting">  def result: ParString = {
    val rsb = new StringBuilder
    for (sb &lt;- chunks) rsb.append(sb)
    new ParString(rsb.toString)
  }
}
</pre><p>We test the performance of the parallel <code class="literal">filter</code> method with the following snippet:</p><pre class="programlisting">val txt = "A custom txt" * 25000
val partxt = new ParString(txt)
val seqtime = warmedTimed(250) { txt.filter(_ != ' ') }
val partime = warmedTimed(250) { partxt.filter(_ != ' ') }
</pre><p>Running this snippet on our machine takes 11 milliseconds for the sequential version, and 6 milliseconds for the parallel one.</p></div></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec38"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we learned how to use parallel collections to improve program performance. We have seen that sequential operations on large collections can be easily parallelized and learned the difference between parallelizable and non-parallelizable collections. We investigated how mutability and side effects impact correctness and determinism of parallel operationsÂ and saw the importance of using associative operators for parallel operations. Finally, we studied how to implement our custom parallel collection class.</p><p>We also found, however, that tuning program performance is tricky. Effects such as memory contention, garbage collection, and dynamic compilation may impact the performance of the program in ways that are hard to predict by looking at the source code. Throughout this section, we urged you to confirm suspicions and claims about program performance by experimentally validating them. Understanding the performance characteristics of your program is the first step toward optimizing it.</p><p>Even when you are sure that parallel collections improve program performance, you should think twice before using them. Donald Knuth once coined the phrase <span class="emphasis"><em>Premature optimization is the root of all evil</em></span>. It is neither desirable nor necessary to use parallel collections wherever possible. In some cases, parallel collections give negligible or no increase in speed. In other situations, they could be speeding up a part of the program that is not the real bottleneck. Before using parallel collections, make sure to investigate which part of the program takes the most time, and whether it is worth parallelizing. The only practical way of doing so is by correctly measuring the running time of the parts of your application. In <a class="link" href="#" linkend="ch09">Chapter 9</a>, <span class="emphasis"><em>Concurrency in Practice</em></span>, we will introduce a framework called ScalaMeter, which offers a more robust way to measure program performance than what we saw in this chapter.</p><p>This chapter briefly introduced concepts such as Random Access Memory, cache lines, and the MESI protocol. If you would like to learn more about this, you should read the article, <span class="emphasis"><em>What Every Programmer Should Know About Memory</em></span>, by Ulrich Drepper. To gain a more in-depth knowledge about the Scala collections hierarchy, we recommend you to search for the document entitled <span class="emphasis"><em>The Architecture of Scala Collections</em></span>, by Martin Odersky and Lex Spoon, or the paper <span class="emphasis"><em>Fighting Bit Rot with Types</em></span>, by Martin Odersky and Adriaan Moors. To understand how data-parallel frameworks work under the hood, consider reading the doctoral thesis entitled <span class="emphasis"><em>Data Structures and Algorithms for Data-Parallel Computing in a Managed Runtime</em></span>, by Aleksandar Prokopec.</p><p>So far, we've assumed that all the collection elements are available when the data-parallel operation starts. A collection does not change its contents during the data-parallel operation. This makes parallel collections ideal in situations where we already have the dataset, and we want to process it in bulk. In other applications, data elements are not immediately available, but arrive asynchronously. In the next chapter, we will learn about an abstraction called an event stream, which is used when asynchronous computations produce multiple intermediate results.</p></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec39"></a>Exercises</h2></div></div><hr /></div><p>In the following exercises, you will use data-parallel collections in several concrete parallel collection use cases, and implement custom parallel collections. In all examples, a specialÂ emphasis is put on measuring the performance gains from parallelization. Even when it is not asked for explicitly, you should ensure that your program is not only correctÂ but also faster than a corresponding sequential program:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Measure the average running time for allocating a simple object on the JVM.</p></li><li><p>Count the occurrences of the whitespace character in a randomly generated string, where the probability of a whitespace at each position is determined by a <code class="literal">p</code> parameter. Use the parallel <code class="literal">foreach</code> method. Plot a graph that correlates the running time of this operation with the <code class="literal">p</code> parameter.</p></li><li><p>Implement a program that renders the Mandelbrot set in parallel.</p></li><li><p>Implement a program that simulates a cellular automaton in parallel.</p></li><li><p>Implement a parallel <span class="emphasis"><em>Barnes-Hut N-body</em></span> simulation algorithm.</p></li><li><p>Explain how you can improve the performance of the <code class="literal">result</code> method in the <code class="literal">ParStringCombiner</code> class, as shown in this chapter. Can you parallelize this method?</p></li><li><p>Implement a custom splitter for the binary heap data structure.</p></li><li><p>The binomial heap, described in the doctoral thesis of Chris Okasaki entitled <span class="emphasis"><em>Purely Functional Data Structures</em></span>, is an immutable data structure that efficiently implements a priority queue with four basic operations: insert the element, find theÂ smallest element, remove theÂ smallest element, and merge two binomial heaps:
</p><pre class="programlisting">            class BinomialHeap[T] extends Iterable[T] {
              def insert(x: T): BinomialHeap[T]
              def remove: (T, BinomialHeap[T])
              def smallest: T
              def merge(that: BinomialHeap[T]): BinomialHeap[T]
            }
</pre><p>
</p><p>Implement the <code class="literal">BinomialHeap</code> class. Then, implement splitters and combiners for the binomial heap, and override the <code class="literal">par</code> operation.</p></li><li><p>Implement the <code class="literal">Combiner</code> trait for the Red-Black tree from the Scala standard library. Use it to provide a parallel version of the <code class="literal">SortedSet</code> trait.</p></li><li><p>Implement a <code class="literal">parallelBalanceParentheses</code> method, which returns <code class="literal">true</code> if the parentheses in a string are properly balanced, or <code class="literal">false</code> otherwise. Parentheses are balanced if, going from left to right, the count of left parenthesis occurrences is always larger than, or equal to, the count of right parenthesis occurrences, and the total count of the left parentheses is equal to the total count of the right parentheses. For example, string <code class="literal">0(1)(2(3))4</code> is balanced, but strings <code class="literal">0)2(1(3)</code> and <code class="literal">0((1)2</code> are not. You should use the <code class="literal">aggregate</code> method.</p></li></ol></div></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch06"></a>ChapterÂ 6.Â Concurrent Programming with Reactive Extensions</h2></div></div></div><div class="blockquote"><table border="0" width="100%" cellspacing="0" cellpadding="0" class="blockquote" summary="Block quote"><tr><td width="10%" valign="top">Â </td><td width="80%" valign="top"><p><span class="emphasis"><em>"Your mouse is a database."</em></span></p></td><td width="10%" valign="top">Â </td></tr><tr><td width="10%" valign="top">Â </td><td colspan="2" align="right" valign="top">--<span class="attribution"><span class="emphasis"><em>Erik Meijer</em></span></span></td></tr></table></div><p>The futures and promises from <a class="link" href="#" linkend="ch04">Chapter 4</a>, <span class="emphasis"><em>Asynchronous Programming with Futures and Promises</em></span>, push concurrent programming to a new level. First, they avoid blocking when transferring the result of the computation from the producer to the consumer. Second, they allow you to idiomatically compose simple future objects into more complex ones, resulting in programs that are more concise. Futures encapsulate patterns of asynchronous communication in a way that is clear and easily understandable.</p><p>One disadvantage of futures is that they can only deal with a single result. For HTTP requests or asynchronous computations that compute a single value, futures can be adequate, but sometimes we need to react to many different events coming from the same computation. For example, it is cumbersome to track the progress status of a file download with futures. Event streams are a much better tool for this use case; unlike futures, they can produce any number of values, which we call events. First-class event streams, which we will learn about in this chapter, can be used inside expressions as if they were regular values. Just as with futures, first-class event streams can be composed and transformed using functional combinators.</p><p>In computer science,Â <span class="strong"><strong>event-driven programming</strong></span> is a programming style in which the flow of the program is determined by events such as external inputs, user actions, or messages coming from other computations. Here, a user action might be a mouse click, and an external input can be a network interface. Both futures and event streams can be classified as event-driven programming abstractions.</p><p><span class="strong"><strong>Reactive programming</strong></span>, which deals with the propagation of change and the flow of data in the program, is a closely related discipline. Traditionally, reactive programming is defined as a programming style that allows you to express various constraints between the data values in the program. For example, when we say <code class="literal">a = b + 1</code> in an imperative programming model, it means that <code class="literal">a</code> is assigned the current value of <code class="literal">b</code> increased by <code class="literal">1</code>. If the value <code class="literal">b</code> later changes, the value of <code class="literal">a</code> does not change. By contrast, in reactive programming, whenever the value <code class="literal">b</code> changes, the value <code class="literal">a</code> is updated using the constraint <code class="literal">a = b + 1</code>. With the rising demand for concurrency, the need for event-driven and reactive programming grows even larger. Traditional callback-based and imperative APIs have shown to be inadequate for this task: they obscure the program flow, mix concurrency concerns with program logic, and rely on mutable state. In larger applications, swarms of unstructured callback declarations lead to an effect known as the callback hell, in which the programmer can no longer make sense of the control flow of the program. In a way, callbacks are the <code class="literal">GOTO</code> statement of reactive programming. <span class="strong"><strong>Event stream composition</strong></span> captures patterns of callback declarations, allowing the programmer to express them more easily. It is a much more structured approach for building event-based systems.</p><p><span class="strong"><strong>Reactive Extensions</strong></span> (<span class="strong"><strong>Rx</strong></span>) is a programming framework for composing asynchronous and event-driven programs using event streams. In Rx, an event stream that produces events of typeÂ <code class="literal">T</code>Â is represented with the typeÂ <code class="literal">Observable[T]</code>. As we will learn in this chapter, the Rx framework incorporates principles present both in reactive and in event-driven programming. The fundamental concept around Rx is that events and data can be manipulated in a similar way.</p><p>In this chapter, we will study the semantics of <code class="literal">RxObservable</code> objects, and learn how to use them to build event-driven and reactive applications. Concretely, we will cover the following topics:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Creating and subscribing to the <code class="literal">Observable</code> objects</p></li><li style="list-style-type: disc"><p>The observable contract and how to implement custom <code class="literal">Observable</code> objects</p></li><li style="list-style-type: disc"><p>Using the subscriptions to cancel event sources</p></li><li style="list-style-type: disc"><p>Composing observable objects using Rx combinators</p></li><li style="list-style-type: disc"><p>Controlling concurrency with Rx scheduler instances</p></li><li style="list-style-type: disc"><p>Using Rx subjects for designing larger applications</p></li></ul></div><p>We will start with simple examples that show you how to create and manipulate theÂ <code class="literal">Observable</code> objects, and illustrate how they propagate events.</p></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec40"></a>Creating Observable objects</h2></div></div><hr /></div><p>In this section, we will study various ways of creating <code class="literal">Observable</code> objects. We will learn how to subscribe to different kinds of event produced by <code class="literal">Observable</code> instances and learn how to correctly create custom <code class="literal">Observable</code> objects. Finally, we will discuss the difference between cold and hot observables.</p><p>An <code class="literal">Observable</code> object is an object that has a method called <code class="literal">subscribe</code>, which takes an object called an observer as a parameter. The observer is a user-specified object with custom event-handling logic. When we call the <code class="literal">subscribe</code> method with a specific observer, we can say that the observer becomes subscribed to the respective <code class="literal">Observable</code> object. Every time the <code class="literal">Observable</code> object produces an event, its subscribed observers get notified.</p><p>The Rx implementation for Scala is not a part of the Scala standard library. To use Rx in Scala, we need to add the following dependency to our <code class="literal">build.sbt</code> file:</p><pre class="programlisting">libraryDependencies +=
  "com.netflix.rxjava" % "rxjava-scala" % "0.19.1"
</pre><p>Now, we can import the contents of the <code class="literal">rx.lang.scala</code> package to start using Rx. Let's say that we want to create a simple <code class="literal">Observable</code> object that first emits several <code class="literal">String</code> events and then completes the execution. We use the <code class="literal">items</code> factory method on the <code class="literal">Observable</code> companion object to create an <code class="literal">Observable</code> object <code class="literal">o</code>. We then call the <code class="literal">subscribe</code> method, which is similar to the <code class="literal">foreach</code> method on futures introduced in <a class="link" href="#" linkend="ch04">Chapter 4</a>, <span class="emphasis"><em>Asynchronous Programming with Futures and Promises</em></span>. The <code class="literal">subscribe</code> method takes a callback function and instructs the <code class="literal">Observable</code> object <code class="literal">o</code> to invoke the callback function for each event that is emitted. It does so by creating an <code class="literal">Observer</code> object behind the scenes. The difference is that, unlike futures, the <code class="literal">Observable</code> objects can emit multiple events. In our example, the callback functions print the events to the screen by calling the <code class="literal">log</code> statement, as follows:</p><pre class="programlisting">import rx.lang.scala._
object ObservablesItems extends App {
  val o = Observable.items("Pascal", "Java", "Scala")
  o.subscribe(name =&gt; log(s"learned the $name language"))
  o.subscribe(name =&gt; log(s"forgot the $name language"))
}
</pre><p>Upon running this example, we notice two things. First, all the <code class="literal">log</code> statements are executed on the main program thread. Second, the callback associated with the first <code class="literal">subscribe</code> call is invoked for all three programming languages before the callback associated with the second <code class="literal">subscribe</code> call is called for these three languages:</p><pre class="programlisting">run-main-0: learned the Pascal language
run-main-0: learned the Java language
run-main-0: learned the Scala language
run-main-0: forgot the Pascal language
run-main-0: forgot the Java language
run-main-0: forgot the Scala language
</pre><p>We can conclude that the <code class="literal">subscribe</code> call executes synchronously--it invokes callback for all the events emitted by the event stream <code class="literal">o</code> before returning. However, this is not always the case. The <code class="literal">subscribe</code> call can also return the control to the main thread immediately, and invoke the callback functions asynchronously. This behavior depends on the implementation of the <code class="literal">Observable</code> object. In this Rx implementation, the <code class="literal">Observable</code> objects created using the <code class="literal">items</code> method have their events available when the <code class="literal">Observable</code> object is created, so their <code class="literal">subscribe</code> method is synchronous.</p><p>In the previous example, the <code class="literal">Observable</code> object feels almost like an immutable Scala collection, and the <code class="literal">subscribe</code> method acts as if it is a <code class="literal">foreach</code> method on a collection. However, the <code class="literal">Observable</code> objects are more general. We will see an <code class="literal">Observable</code> object that emits events asynchronously next.</p><p>Let's assume that we want the <code class="literal">Observable</code> object that emits an event after a certain period of time has elapsed. We use the <code class="literal">timer</code> factory method to create such an <code class="literal">Observable</code> object and set the timeout to 1 second. We then call the <code class="literal">subscribe</code> method with two different callbacks, as shown in the following code snippet:</p><pre class="programlisting">import scala.concurrent.duration._
object ObservablesTimer extends App {
  val o = Observable.timer(1.second)
  o.subscribe(_ =&gt; log("Timeout!"))
  o.subscribe(_ =&gt; log("Another timeout!"))
  Thread.sleep(2000)
}
</pre><p>This time, the <code class="literal">subscribe</code> method calls are asynchronous; it makes no sense to block the main thread for an entire second and wait until the timeout event appears. Running the example shows that the main thread continues before the callback functions are invoked:</p><pre class="programlisting">RxComputationThreadPool-2: Another timeout!
RxComputationThreadPool-1: Timeout!
</pre><p>Furthermore, the <code class="literal">log</code> statements reveal that the callback functions are invoked on the thread pool internally used by Rx, in an unspecified order.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip61"></a>Tip</h3><p>The <code class="literal">Observable</code> objects can emit events either synchronously or asynchronously, depending on the implementation of the specific <code class="literal">Observable</code> object.</p></div><p>As we will see, in most use cases, events are not available when calling the <code class="literal">subscribe</code> method. This is the case with UI events, file modification events, or HTTP responses. To avoid blocking the thread that calls the <code class="literal">subscribe</code> method, the <code class="literal">Observable</code> objects emit such events asynchronously.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec53"></a>Observables and exceptions</h3></div></div></div><p>In <a class="link" href="#" linkend="ch04">Chapter 4</a>, <span class="emphasis"><em>Asynchronous Programming with Futures and Promises</em></span>, we saw that asynchronous computations sometimes throw exceptions. When that happens, the <code class="literal">Future</code> object associated with the exception fails; instead of being completed with the result of the computation, the <code class="literal">Future</code> object is completed with the exception that failed the asynchronous computation. The clients of the <code class="literal">Future</code> objects can react to exceptions by registering callbacks with the <code class="literal">failed.foreach</code> or <code class="literal">onComplete</code> methods.</p><p>Computations that produce events in <code class="literal">Observable</code> objects can also throw exceptions. To respond to exceptions produced by the <code class="literal">Observable</code> objects, we can use an overload of the <code class="literal">subscribe</code> method that takes two callback arguments to create an observer--the callback function for the events and the callback function for the exception.</p><p>The following program creates an <code class="literal">Observable</code> object that emits numbers <code class="literal">1</code> and <code class="literal">2</code>, and then produces a <code class="literal">RuntimeException</code>. The <code class="literal">items</code> factory method creates the <code class="literal">Observable</code> object with the numbers, and the <code class="literal">error</code> factory method creates another <code class="literal">Observable</code> object with an exception. We then concatenate the two together with the <code class="literal">++</code> operator on <code class="literal">Observable</code> instances. The first callback logs the numbers to the standard output and ignores the exception. Conversely, the second callback logs the <code class="literal">Throwable</code> objects and ignores the numbers. This is shown in the following code snippet:</p><pre class="programlisting">object ObservablesExceptions extends App {
  val exc = new RuntimeException
  val o = Observable.items(1, 2) ++ Observable.error(exc)
  o.subscribe(
    x =&gt; log(s"number $x"),
    t =&gt; log(s"an error occurred: $t")
  )
}
</pre><p>The program first prints numbers <code class="literal">1</code> and <code class="literal">2</code>, and then prints the exception object. Without the second callback function being passed to the <code class="literal">subscribe</code> method, the exception will be emitted by the <code class="literal">Observable</code> object <code class="literal">o</code>, but never passed to the observer. Importantly, after an exception is emitted, the <code class="literal">Observable</code> object is not allowed to emit any additional events. We can redefine the <code class="literal">Observable</code> object <code class="literal">o</code> as follows:</p><pre class="programlisting">import Observable._
val o = items(1, 2) ++ error(exc) ++ items(3, 4)
</pre><p>We might expect the program to print events <code class="literal">3</code> and <code class="literal">4</code>, but they are not emitted by the <code class="literal">Observable</code> object <code class="literal">o</code>. When an <code class="literal">Observable</code> object produces an exception, we say that it is in the error state.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip62"></a>Tip</h3><p>When an <code class="literal">Observable</code> object produces an exception, it enters the error state and cannot emit more events.</p></div><p>Irrespective of whether the <code class="literal">Observable</code> object is created using a factory method, or is a custom <code class="literal">Observable</code> implementation described in the subsequent sections, an <code class="literal">Observable</code> object is not allowed to emit events after it produces an exception. In the next section, we will examine this contract in more detail.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec54"></a>The Observable contract</h3></div></div></div><p>Now that we have seen how to create simple <code class="literal">Observable</code> objects and react to their events, it is time to take a closer look at the lifetime of an <code class="literal">Observable</code> object. Every <code class="literal">Observable</code> object can be in three states: uncompleted, error, or completed. As long as the <code class="literal">Observable[T]</code> object is uncompleted, it can emit events of type <code class="literal">T</code>. As we already learned, an <code class="literal">Observable</code> object can produce an exception to indicate that it failed to produce additional data. When this happens, the <code class="literal">Observable</code> object enters the error state and cannot emit any additional events. Similarly, when an <code class="literal">Observable</code> object decides that it will not produce any additional data, it might enter the completed state. After an <code class="literal">Observable</code> object is completed, it is not allowed to emit any additional events.</p><p>In Rx, an object that subscribes to events from an <code class="literal">Observable</code> object is called an <code class="literal">Observer</code> object. The <code class="literal">Observer[T]</code> trait comes with three methods: <code class="literal">onNext</code>, <code class="literal">onError</code>, and <code class="literal">onCompleted</code>, which get invoked when an <code class="literal">Observable</code> object emits an event, produces an error, or is completed, respectively. This trait is shown in the following code snippet:</p><pre class="programlisting">trait Observer[T] {
  def onNext(event: T): Unit
  def onError(error: Throwable): Unit
  def onCompleted(): Unit
}
</pre><p>In the previous examples, whenever we called the <code class="literal">subscribe</code> method, Rx created an <code class="literal">Observer</code> object and assigned it to the <code class="literal">Observable</code> instance. Alternatively, we can provide an <code class="literal">Observer</code> object directly to an overloaded version of the <code class="literal">subscribe</code> method. The following program uses the <code class="literal">from</code> factory method whichÂ converts a list of movie titles into an <code class="literal">Observable</code> object. It then creates an <code class="literal">Observer</code> object and passes it to the <code class="literal">subscribe</code> method:</p><pre class="programlisting">object ObservablesLifetime extends App {
  val classics = List("Good, bad, ugly", "Titanic", "Die Hard")
  val movies = Observable.from(classics)
  movies.subscribe(new Observer[String] {
    override def onNext(m: String) = log(s"Movies Watchlist - $m")
    override def onError(e: Throwable) = log(s"Ooops - $e!")
    override def onCompleted() = log(s"No more movies.")
  })
}
</pre><p>This program first prints our favorite movies, and terminates after calling <code class="literal">onCompleted</code> and printing <code class="literal">"No more movies"</code>. The <code class="literal">Observable</code> object <code class="literal">movies</code> is created from a finite collection of strings; after these events are emitted, the <code class="literal">movies</code> event stream calls the <code class="literal">onCompleted</code> method. In general, the <code class="literal">Observable</code> objects can only call the <code class="literal">onCompleted</code> method after it is certain that there will be no more events.</p><p>Every <code class="literal">Observable</code> object can call the <code class="literal">onNext</code> method on its <code class="literal">Observer</code> objects zero or more times. An <code class="literal">Observable</code> object might then enter the completed or error state by calling the <code class="literal">onCompleted</code> or <code class="literal">onError</code> method on its <code class="literal">Observer</code> objects. This is known as the <code class="literal">Observable</code> contract, and is shown graphically in the following state diagram, where different nodes denote <code class="literal">Observable</code> states, and links denote calls to different <code class="literal">Observer</code> methods:</p><div class="mediaobject"><img src="graphics/image_06_001.jpg" /></div><p>Note that an <code class="literal">Observable</code> object can call the <code class="literal">onCompleted</code> or <code class="literal">onError</code> method if it knows that it will not emit additional events, but it is free to call neither. Some Observable objects, such as <code class="literal">items</code>, know when they emit the last event. On the other hand, an <code class="literal">Observable</code> instance that emits mouse or keyboard events never calls the <code class="literal">onCompleted</code> method.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note63"></a>Note</h3><p>An <code class="literal">Observable</code> object can call the <code class="literal">onNext</code> method on the subscribed <code class="literal">Observer</code> objects an unlimited number of times. After optionally calling the <code class="literal">onCompleted</code> or <code class="literal">onError</code> method, an <code class="literal">Observable</code> object is not allowed to call any <code class="literal">Observer</code> methods.</p></div><p>The <code class="literal">Observable</code> objects produced by the Rx API implement the <code class="literal">Observable</code> contract. In practice, we do not need to worry about the <code class="literal">Observable</code> contract, unless we are implementing our own custom <code class="literal">Observable</code> object. This is the topic of the next section.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec55"></a>Implementing custom Observable objects</h3></div></div></div><p>To create a custom <code class="literal">Observable</code> object, we can use the <code class="literal">Observable.create</code> factory method as follows:</p><pre class="programlisting">def create(f: Observer[T] =&gt; Subscription): Observable[T]
</pre><p>The preceding method takes a function <code class="literal">f</code> from an <code class="literal">Observer</code> to a <code class="literal">Subscription</code> object and returns a new <code class="literal">Observable</code> object. Whenever the <code class="literal">subscribe</code> method gets called, the function <code class="literal">f</code> is called on the corresponding <code class="literal">Observer</code> object. The function <code class="literal">f</code> returns a <code class="literal">Subscription</code> object, which can be used to unsubscribe the <code class="literal">Observer</code> object from the <code class="literal">Observable</code> instance. The <code class="literal">Subscription</code> trait defines a single method called <code class="literal">unsubscribe</code>:</p><pre class="programlisting">trait Subscription {
  def unsubscribe(): Unit
}
</pre><p>We will talk about theÂ <code class="literal">Subscription</code> objects in more detail in a subsequent section. For now, we only use the empty <code class="literal">Subscription</code> object, which does not unsubscribe the <code class="literal">Observer</code> object.</p><p>To illustrate how to use the <code class="literal">Observable.create</code> method, we implement an <code class="literal">Observable</code> object <code class="literal">vms</code>, which emits names of popular virtual machine implementations. In <code class="literal">Observable.create</code>, we take care to first call <code class="literal">onNext</code> with all the VM names, and then call <code class="literal">onCompleted</code> once. Finally, we return the empty <code class="literal">Subscription</code> object. This is shown in the following program:</p><pre class="programlisting">object ObservablesCreate extends App {
  val vms = Observable.apply[String] { obs =&gt;
    obs.onNext("JVM")
    obs.onNext("DartVM")
    obs.onNext("V8")
    obs.onCompleted()
    Subscription()
  }
  vms.subscribe(log _, e =&gt; log(s"oops - $e"), () =&gt; log("Done!"))
}
</pre><p>The <code class="literal">Observable</code> object <code class="literal">vms</code> has a synchronous <code class="literal">subscribe</code> method. All the events are emitted to an <code class="literal">obs</code> observer before returning the control to the thread that called the <code class="literal">subscribe</code> method. In general, we can use the <code class="literal">Observable.create</code> method in order to create an <code class="literal">Observable</code> instance that emits events asynchronously. We will study how to convert a <code class="literal">Future</code> object into an <code class="literal">Observable</code> object next.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec56"></a>Creating Observables from futures</h3></div></div></div><p>Futures are objects that represent the result of an asynchronous computation. One can consider an <code class="literal">Observable</code> object as a generalization of a <code class="literal">Future</code> object. Instead of emitting a single success or failure event, an <code class="literal">Observable</code> object emits a sequence of events, before failing or completing successfully.</p><p>Scala APIs that deal with asynchronous computations generally return the <code class="literal">Future</code> objects, and not <code class="literal">Observable</code> instances. In some cases, it is useful to be able to convert a <code class="literal">Future</code> object into an <code class="literal">Observable</code> object. Here, after a <code class="literal">Future</code> object is completed successfully, the corresponding <code class="literal">Observable</code> object must emit an event with the future value, and then call the <code class="literal">onCompleted</code> method. If the <code class="literal">Future</code> object fails, the corresponding <code class="literal">Observable</code> object should call the <code class="literal">onError</code> method. Before we begin, we need to import the contents of the <code class="literal">scala.concurrent</code> package and the global <code class="literal">ExecutionContext</code> object, as shown in the following code snippet:</p><pre class="programlisting">import scala.concurrent._
import ExecutionContext.Implicits.global
</pre><p>We then use the <code class="literal">Observable.create</code> method to create an <code class="literal">Observable</code> object <code class="literal">o</code>. Instead of calling the <code class="literal">onNext</code>, <code class="literal">onError</code>, and <code class="literal">onCompleted</code> methods directly on the <code class="literal">Observer</code> object, we will install callbacks on the <code class="literal">Future</code> object <code class="literal">f</code>, as shown in the following program:</p><pre class="programlisting">object ObservablesCreateFuture extends App {
  val f = Future { "Back to the Future(s)" }
  val o = Observable.create[String] { obs =&gt;
    f foreach { case s =&gt; obs.onNext(s); obs.onCompleted() }
    f.failed foreach { case t =&gt; obs.onError(t) }
    Subscription()
  }
  o.subscribe(log _)
}
</pre><p>This time, the <code class="literal">subscribe</code> method is asynchronous. It returns immediately after installing the callback on the <code class="literal">Future</code> object. In fact, this pattern is so common that Rx comes with the <code class="literal">Observable.from</code> factory method that converts a <code class="literal">Future</code> object into an <code class="literal">Observable</code> object directly, as shown by the following code snippet:</p><pre class="programlisting">val o = Observable.from(Future { "Back to the Future(s)" })
</pre><p>Still, learning how to convert a <code class="literal">Future</code> object into an <code class="literal">Observable</code> object is handy. The <code class="literal">Observable.create</code> method is the preferred way to convert callback-based APIs to <code class="literal">Observable</code> objects, as we will see in subsequent sections.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip64"></a>Tip</h3><p>Use the <code class="literal">Observable.create</code> factory method to create the <code class="literal">Observable</code> objects from callback-based APIs.</p></div><p>In the examples so far, we have always returned an empty <code class="literal">Subscription</code> object. Calling the <code class="literal">unsubscribe</code> method on such a <code class="literal">Subscription</code> object has no effect. Sometimes, the <code class="literal">Subscription</code> objects need to release resources associated with the corresponding <code class="literal">Observable</code> instance. We will study how to implement and work with such <code class="literal">Subscription</code> objects next.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec57"></a>Subscriptions</h3></div></div></div><p>Recall the example monitoring the filesystem for changes in <a class="link" href="#" linkend="ch04">Chapter 4</a>, <span class="emphasis"><em>Asynchronous Programming with Futures and Promises</em></span>, where we used the file monitoring package from the Apache Commons IO library to complete a <code class="literal">Future</code> object when a new file is created. A <code class="literal">Future</code> object can be completed only once, so the future was completed with the name of the first file that was created. It is more natural to use <code class="literal">Observable</code> objects for this use case, as files in a filesystem can be created and deleted many times. In an application such as a file browser or an FTP server, we would like to receive all such events.</p><p>Later in the program, we might want to unsubscribe from the events in the <code class="literal">Observable</code> object. We will now see how to use the <code class="literal">Subscription</code> object to achieve this. We first import the contents of the <span class="strong"><strong>Apache Commons IO file monitoring</strong></span> package, as follows:</p><pre class="programlisting">import org.apache.commons.io.monitor._
</pre><p>We define the <code class="literal">modified</code> method, which returns an <code class="literal">Observable</code> object with filenames of the modified files in the specified directory. The <code class="literal">Observable.create</code> method bridges the gap between the Commons IO callback-based API and Rx. When the <code class="literal">subscribe</code> method is called, we create a <code class="literal">FileAlterationMonitor</code> object, which uses a separate thread to scan the filesystem and emit filesystem events every 1000 milliseconds, a <code class="literal">FileAlterationObserver</code> object, which specifies a directory to monitor; and a <code class="literal">FileAlterationListener</code> object, which reacts to file events by calling the <code class="literal">onNext</code> method on the Rx <code class="literal">Observer</code> object. We then call the <code class="literal">start</code> method on the <code class="literal">fileMonitor</code> object.</p><p>Finally, we return a custom <code class="literal">Subscription</code> object, which calls <code class="literal">stop</code> on the <code class="literal">fileMonitor</code> object. The <code class="literal">modified</code> method is shown in the following code snippet:</p><pre class="programlisting">def modified(directory: String): Observable[String] = {
  Observable.create { observer =&gt;
    val fileMonitor = new FileAlterationMonitor(1000)
    val fileObs = new FileAlterationObserver(directory)
    val fileLis = new FileAlterationListenerAdaptor {
      override def onFileChange(file: java.io.File) {
        observer.onNext(file.getName)
      }
    }
    fileObs.addListener(fileLis)
    fileMonitor.addObserver(fileObs)
    fileMonitor.start()
    Subscription { fileMonitor.stop() }
  }
}
</pre><p>We used the <code class="literal">apply</code> factory method on the <code class="literal">Subscription</code> companion object in the preceding code snippet. When the <code class="literal">unsubscribe</code> method is called on the resulting <code class="literal">Subscription</code> object, the specified block of code is run. Importantly, calling the <code class="literal">unsubscribe</code> method, the second time will not run the specified block of code again. We say that the <code class="literal">unsubscribe</code> method is <span class="strong"><strong>idempotent</strong></span>; calling it multiple times has the same effect as calling it only once. In our example, the <code class="literal">unsubscribe</code> method calls the <code class="literal">stop</code> method of the <code class="literal">fileMonitor</code> object at most once. When sub-classing the <code class="literal">Subscription</code> trait, we need to ensure that the <code class="literal">unsubscribe</code> method is idempotent, and the <code class="literal">Subscription.apply</code> method is a convenience method that ensures idempotence automatically.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip65"></a>Tip</h3><p>Implementations of the <code class="literal">unsubscribe</code> method in the <code class="literal">Subscription</code> trait need to be idempotent. Use the <code class="literal">Subscription.apply</code> method to create the <code class="literal">Subscription</code> objects that are idempotent by default.</p></div><p>We use the <code class="literal">modified</code> method to track file changes in our project. After we call the <code class="literal">subscribe</code> method on the <code class="literal">Observable</code> object returned by the <code class="literal">modified</code> method, the main thread suspends for 10 seconds. If we save files in our editor during this time, the program will log file modification events to the standard output. This is shown in the following program:</p><pre class="programlisting">object ObservablesSubscriptions extends App {
  log(s"starting to monitor files")
  val sub = modified(".").subscribe(n =&gt; log(s"$n modified!"))
  log(s"please modify and save a file")
  Thread.sleep(10000)
  sub.unsubscribe()
  log(s"monitoring done")
}
</pre><p>Note that, in this example, the <code class="literal">FileAlterationMonitor</code> object is only created if the program invokes the <code class="literal">subscribe</code> method. The <code class="literal">Observable</code> instance returned by the <code class="literal">modified</code> method does not emit events unless there exists an <code class="literal">Observer</code> object subscribed to it. In Rx, the <code class="literal">Observable</code> objects that emit events only when subscriptions exist are called <span class="strong"><strong>cold observables</strong></span>. On the other hand, some <code class="literal">Observable</code> objects emit events even when there are no associated subscriptions. This is usually the case with <code class="literal">Observable</code> instances that handle user input, such as keyboard or mouse events. <code class="literal">Observable</code> objects that emit events regardless of their subscriptions are called <span class="strong"><strong>hot observables</strong></span>. We now reimplement an <code class="literal">Observable</code> object that tracks file modifications as a hot observable. We first instantiate and start the <code class="literal">FileAlterationMonitor</code> object, as follows:</p><pre class="programlisting">val fileMonitor = new FileAlterationMonitor(1000)
fileMonitor.start()
</pre><p>The <code class="literal">Observable</code> object uses the <code class="literal">fileMonitor</code> object to specify the directory in order to monitor. The downside is that our <code class="literal">Observable</code> object now consumes computational resources even when there are no subscriptions. The advantage of using a hot observable is that multiple subscriptions do not need to instantiate multiple <code class="literal">FileAlterationMonitor</code> objects, which are relatively heavyweight. We implement the hot <code class="literal">Observable</code> object in the <code class="literal">hotModified</code> method, as shown in the following code:</p><pre class="programlisting">def hotModified(directory: String): Observable[String] = {
  val fileObs = new FileAlterationObserver(directory)
  fileMonitor.addObserver(fileObs)
  Observable.create { observer =&gt;
    val fileLis = new FileAlterationListenerAdaptor {
      override def onFileChange(file: java.io.File) {
        observer.onNext(file.getName)
      }
    }
    fileObs.addListener(fileLis)
    Subscription { fileObs.removeListener(fileLis) }
  }
}
</pre><p>The <code class="literal">hotModified</code> method creates an <code class="literal">Observable</code> object with file changes for a given directory by registering the specified directory with the <code class="literal">fileMonitor</code> object, and only then calls the <code class="literal">Observable.create</code> method. When the <code class="literal">subscribe</code> method is called on the resulting <code class="literal">Observable</code> object, we instantiate and add a new <code class="literal">FileAlterationListener</code> object. In the <code class="literal">Subscription</code> object, we remove the <code class="literal">FileAlterationListener</code> object in order to avoid receiving additional file modification events, but we do not call the <code class="literal">stop</code> method on the <code class="literal">fileMonitor</code> object until the program terminates.</p></div></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec41"></a>Composing Observable objects</h2></div></div><hr /></div><p>Having seen different ways of creating various types of the <code class="literal">Observable</code> objects, subscribing to their events, and using the <code class="literal">Subscription</code> objects, we turn our attention to composing the <code class="literal">Observable</code> objects into larger programs. From what we have seen so far, the advantages of using the <code class="literal">Observable</code> objects over a callback-based API are hardly worth the trouble.</p><p>The true power of Rx becomes apparent when we start composing the <code class="literal">Observable</code> objects using various combinators. We can think of an <code class="literal">Observable</code> object in a similar way as we think of Scala sequence collections. In a Scala sequence, represented byÂ the <code class="literal">Seq[T]</code> trait, elements of type <code class="literal">T</code> are ordered in the memory according to their indices. In an <code class="literal">Observable[T]</code> trait, events of type <code class="literal">T</code> are ordered in time.</p><p>Let's use the <code class="literal">Observable.interval</code> factory method in order to create an <code class="literal">Observable</code> object, which asynchronously emits a number every 0.5 seconds, and then output the first five odd numbers. To do this, we first call <code class="literal">filter</code> on the <code class="literal">Observable</code> object in order to obtain an intermediate <code class="literal">Observable</code> object that emits only odd numbers. Note that calling the <code class="literal">filter</code> on an <code class="literal">Observable</code> object is similar to callingÂ <code class="literal">filter</code> method on a Scala collection. Similarly, we obtain another <code class="literal">Observable</code> object by calling the <code class="literal">map</code> method in order to transform each odd number into a string. We then call <code class="literal">take</code> to create an <code class="literal">Observable</code> object <code class="literal">odds</code>, whichÂ  contains only the first five events. Finally, we subscribe to <code class="literal">odds</code> so that we can print the events it emits. This is shown in the following program:</p><pre class="programlisting">object CompositionMapAndFilter extends App {
  val odds = Observable.interval(0.5.seconds)
    .filter(_ % 2 == 1).map(n =&gt; s"num $n").take(5)
  odds.subscribe(
    log _, e =&gt; log(s"unexpected $e"), () =&gt; log("no more odds"))
  Thread.sleep(4000)
}
</pre><p>To concisely explain the semantics of different Rx combinators, we often rely on marble diagrams. These diagrams graphically represent events in an <code class="literal">Observable</code> object and transformations between different <code class="literal">Observable</code> objects. The marble diagram represents every <code class="literal">Observable</code> object with a timeline containing its events. The first three intermediate <code class="literal">Observable</code> objects never call the <code class="literal">onCompleted</code> method on its observers.</p><p>The <code class="literal">Observable</code> object <code class="literal">odds</code> contains at most five events, so it calls <code class="literal">onCompleted</code> after emitting them. We denote a call to the <code class="literal">onCompleted</code> method with a vertical bar in the marble diagram, as shown in the following diagram:</p><div class="mediaobject"><img src="graphics/image_06_002.jpg" /></div><p>Note that the preceding diagram is a high-level illustration of the relationships between different <code class="literal">Observable</code> objects, but some of these events can be omitted during execution. The particular Rx implementation can detect that the events <code class="literal">11</code> and <code class="literal">12</code> cannot be observed by the <code class="literal">subscribe</code> invocation, so these events are not emitted to save computational resources.</p><p>As an expert on sequential programming in Scala, you probably noticed that we can rewrite the previous program more concisely using the for-comprehensions. For example, we can output the first five even natural numbers with the following for-comprehension:</p><pre class="programlisting">val evens = for (n &lt;- Observable.from(0 until 9); if n % 2 == 0)
yield s"even number $n"
evens.subscribe(log _)
</pre><p>Before moving on to more complex for-comprehensions, we will study a special kind of <code class="literal">Observable</code> object whose events are other <code class="literal">Observable</code> objects.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec58"></a>Nested Observables</h3></div></div></div><p>A nested observable, also called a higher-order event stream, is an <code class="literal">Observable</code> object that emits events that are themselves <code class="literal">Observable</code> objects. A higher-order function such as the <code class="literal">foreach</code> statement is called a higher-order function because it has a nested function inside itsÂ <code class="literal">(T =&gt; Unit) =&gt; Unit</code> type. Similarly, higher-order event streams earned this fancy name because they have a typeÂ <code class="literal">Observable[T]</code>Â as part of their type <code class="literal">Observable[Observable[T]]</code>. In this section, we will study when the <code class="literal">nestedObservable</code> objects are useful and how to manipulate them.</p><p>Let's assume that we are writing a book and we want to add a famous quote at the beginning of each chapter. Choosing the right quote for a chapter is a hard job and we want to automate it. We write a short program that uses <code class="literal">Observable</code> objects to fetch random quotes from the <span class="emphasis"><em>I Heart Quotes</em></span> website every 0.5 seconds and prints them to the screen. Once we see a nice quote, we have to quickly copy it to our book chapter.</p><p>We will start by defining a <code class="literal">fetchQuote</code> method that returns a <code class="literal">Future</code> object with the text of the quote. Luckily, the HTTP API of the <span class="emphasis"><em>I Heart Quotes</em></span> website returns plain text, so we do not need to parse any JSON or XML. We use the <code class="literal">scala.io.Source</code> object to fetch the contents of the proper URL, as follows:</p><pre class="programlisting">import scala.io.Source
def fetchQuote(): Future[String] = Future {
  blocking {
    val url = "http://quotes.stormconsultancy.co.uk/random.json" +
      "show_permalink=false&amp;show_source=false"
    Source.fromURL(url).getLines.mkString
  }
}
</pre><p>Recall that we can convert a <code class="literal">Future</code> object to an <code class="literal">Observable</code> object using the <code class="literal">from</code> factory method:</p><pre class="programlisting">def fetchQuoteObservable(): Observable[String] = {
  Observable.from(fetchQuote())
}
</pre><p>We now use the <code class="literal">Observable.interval</code> factory method in order to create an <code class="literal">Observable</code> object that emits a number every 0.5 seconds. For the purposes of our example, we take only the first four numbers. Then, we map each of these numbers into an <code class="literal">Observable</code> object that emits a quote, prefixed with the ordinal number of the quote. To do this, we call the <code class="literal">fetchQuoteObservable</code> method and map the quotes using a nested <code class="literal">map</code> call, as shown in the following code snippet:</p><pre class="programlisting">def quotes: Observable[Observable[String]] =
  Observable.interval(0.5 seconds).take(4).map {
    n =&gt; fetchQuoteObservable().map(txt =&gt; s"$n) $txt")
  }
</pre><p>Note that the inner <code class="literal">map</code> call transforms an <code class="literal">Observable[String]</code> instance, which contains the quote text, to another <code class="literal">Observable[String]</code> instance, which contains the quote prefixed with a number. The outer <code class="literal">map</code> call transforms the <code class="literal">Observable[Long]</code> object, which contains the first four numbers, to an <code class="literal">Observable[Observable[String]]</code> instance, which contains <code class="literal">Observable</code> objects emitting separate quotes. The <code class="literal">Observable</code> objects created by the <code class="literal">quotes</code> method are shown in the following marble diagram. Events in the nested <code class="literal">Observable</code> objects presented last are themselves <code class="literal">Observable</code> objects that contain a single event: the text of the quote returned in the <code class="literal">Future</code> object. Note that we omit the nested <code class="literal">map</code> call from the diagram to make it more readable:</p><div class="mediaobject"><img src="graphics/image_06_003.jpg" /></div><p>Drawing a marble diagram makes the contents of this <code class="literal">Observable</code> object more understandable, but how do we subscribe to events in an <code class="literal">Observable[Observable[String]]</code> object? Calling the <code class="literal">subscribe</code> method on quotes requires observers to handle the <code class="literal">Observable[String]</code> objects, and not the <code class="literal">String</code> events directly.</p><p>Once again, an analogy with Scala sequence collections is useful in order to understand how to solve this issue. Whenever we have a nested sequence, say <code class="literal">Seq[Seq[T]]</code>, we can flatten it to a <code class="literal">Seq[T]</code> collection by calling the <code class="literal">flatten</code> method. When we do this, elements of the nested sequences are simply concatenated together. The Rx API provides similar methods that flatten the <code class="literal">Observable</code> objects, but they must deal with the additional complexity associated with the timing of events. There are different ways of flattening the <code class="literal">Observable</code> objects depending on the time when their events arrive.</p><p>The first method, called <code class="literal">concat</code>, concatenates the <code class="literal">nestedObservable</code> objects by ordering all the events in one nested <code class="literal">Observable</code> object before the events in a subsequent <code class="literal">Observable</code> object. An <code class="literal">Observable</code> object that appears earlier must complete before the events from a subsequent <code class="literal">Observable</code> object can be emitted. The marble diagram for the <code class="literal">concat</code> operation is shown in the following figure.Â Although the quote <span class="strong"><strong>Veni, vidi, vici.</strong></span>, arrives before the quote <span class="strong"><strong>Carpe diem</strong></span>., the quote <span class="strong"><strong>Veni, vidi, vici.</strong></span> is emitted only after the <code class="literal">Observable</code> object associated with the quote <span class="strong"><strong>Carpe diem.</strong></span>Â completes. The resulting <code class="literal">Observable</code> object completes only after the <code class="literal">Observable</code> object <code class="literal">quotes</code> and all the nested <code class="literal">Observable</code> objects complete:</p><div class="mediaobject"><img src="graphics/image_06_004.jpg" /></div><p>The second method is called <code class="literal">flatten</code>, analogously to the similar method in the Scala collections API. This method emits events from the nested <code class="literal">Observable</code> objects in the order in which they arrive in time, regardless of when the respective nested <code class="literal">Observable</code> object started. An <code class="literal">Observable</code> object that appears earlier is not required to complete before events from a subsequent <code class="literal">Observable</code> object are emitted.</p><p>This is illustrated in the following marble diagram. A quote is emitted to the resulting <code class="literal">Observable</code> object as soon as it appears on any of the nested <code class="literal">Observable</code> objects. Once <code class="literal">quotes</code> and all the nested <code class="literal">Observable</code> objects complete, the resulting <code class="literal">Observable</code> object completes as well.Â </p><div class="mediaobject"><img src="graphics/image_06_005.jpg" /></div><p>To test the difference between the <code class="literal">concat</code> and <code class="literal">flatten</code> methods, we subscribe to events in <code class="literal">quotes</code> using each of these two methods. If our network is unreliable or has particularly nondeterministic latency, the order in which the second <code class="literal">subscribe</code> call prints the <code class="literal">quotes</code> object can be mangled. We can reduce the interval between queries from 0.5 to 0.01 seconds to witness this effect. The ordinal numbers preceding each quote become unordered when using the <code class="literal">flatten</code> method. This is illustrated in the following program:</p><pre class="programlisting">object CompositionConcatAndFlatten extends App {
  log(s"Using concat")
  quotes.concat.subscribe(log _)
  Thread.sleep(6000)
  log(s"Now using flatten")
  quotes.flatten.subscribe(log _)
  Thread.sleep(6000)
}
</pre><p>How do we choose between the <code class="literal">concat</code> and <code class="literal">flatten</code> methods? The <code class="literal">concat</code> method has the advantage that it maintains the relative order between events coming from different <code class="literal">Observable</code> objects. If we had been fetching and printing quotes in a lexicographic order, then the <code class="literal">concat</code> method would be the correct way to flatten the nested <code class="literal">Observable</code> objects.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip66"></a>Tip</h3><p>Use <code class="literal">concat</code> to flatten nested <code class="literal">Observable</code> objects whenever the order of events between different nested <code class="literal">Observable</code> objects needs to be maintained.</p></div><p>The <code class="literal">concat</code> method does not subscribe to subsequent <code class="literal">Observable</code> objects before the current <code class="literal">Observable</code> object completes. If one of the nested <code class="literal">Observable</code> objects takes a long time to complete or does not complete at all, the events from the remaining <code class="literal">Observable</code> objects are postponed or never emitted. The <code class="literal">flatten</code> method subscribes to a nested <code class="literal">Observable</code> object as soon as the nested <code class="literal">Observable</code> object is emitted, and emits events as soon as they arrive.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip67"></a>Tip</h3><p>If at least one of the nested <code class="literal">Observable</code> objects has an unbounded number of events or never completes, use the <code class="literal">flatten</code> method instead of the <code class="literal">concat</code> method.</p></div><p>We can also traverse events from multiple <code class="literal">Observable</code> objects in a <code class="literal">for</code> comprehension. The <code class="literal">Observable</code> objects come with the <code class="literal">flatMap</code> method, and this allows you to use them in <code class="literal">for</code> comprehensions. Calling the <code class="literal">flatMap</code> method on an <code class="literal">Observable</code> object is equivalent to mapping each of its events into a nested <code class="literal">Observable</code> object, and then calling the <code class="literal">flatten</code> method. Thus, we can rewrite the <code class="literal">quotes.flatten</code> method as follows:</p><pre class="programlisting">Observable.interval(0.5 seconds).take(5).flatMap({
  n =&gt; fetchQuoteObservable().map(txt =&gt; s"$n) $txt")
}).subscribe(log _)
</pre><p>Having already mastered <code class="literal">for</code> comprehensions on Scala collections and <code class="literal">for</code> comprehensions on futures, this pattern of <code class="literal">flatMap</code> and <code class="literal">map</code> calls immediately rings a bell, and we recognize the previous expression as the following <code class="literal">for</code> comprehension:</p><pre class="programlisting">val qs = for {
  n   &lt;- Observable.interval(0.5 seconds).take(5)
  txt &lt;- fetchQuoteObservable()
} yield s"$n) $txt"
qs.subscribe(log _)
</pre><p>This is much more concise and understandable, and almost feels like we're back withÂ collections land. Still, we need to be careful, because for-comprehensions on <code class="literal">Observable</code> objects do not maintain the relative order of the events in the way that the for-comprehensions on collections do. In the preceding example, as soon as we can pair a <code class="literal">n</code> number with some quote <code class="literal">txt</code>, the <code class="literal">s"$n) $txt"</code> event is emitted, irrespective of the events associated with the preceding <code class="literal">n</code> number.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note68"></a>Note</h3><p>Calling the <code class="literal">flatMap</code> method or using <code class="literal">Observable</code> objects in <code class="literal">for</code> comprehensions emits events in the order in which they arrive, and it does not maintain ordering between events from different <code class="literal">Observable</code> objects. Invoking the <code class="literal">flatMap</code> method is semantically equivalent to calling <code class="literal">map</code> followed by the <code class="literal">flatten</code> call.</p></div><p>An attentive reader will notice that we did not consider the case where one of the nested <code class="literal">Observable</code> objects terminates by calling the <code class="literal">onError</code> method. When this happens, both <code class="literal">concat</code> and <code class="literal">flatten</code> call the <code class="literal">onError</code> method with the same exception. Similarly, <code class="literal">map</code> and <code class="literal">filter</code> fail the resulting <code class="literal">Observable</code> object if the input <code class="literal">Observable</code> object produces an exception, so it is unclear how to compose failed <code class="literal">Observable</code> objects. This is the focus of the next section.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec59"></a>Failure handling in Observables</h3></div></div></div><p>If you ran the previous examples yourself, you might have noticed that some of the quotes are long and tedious to read. We don't want to put a long quote at the beginning of the chapter. If we did that, our readers might lose interest. The best quotes are short and straight to the point.</p><p>Our next goal will be to replace quotes longer than 100 characters with a string <code class="literal">Retrying...</code> and print the first quote shorter than 100 characters. This time, we define an <code class="literal">Observable</code> object called <code class="literal">randomQuote</code>, which emits a random quote every time we subscribe to it. We use the <code class="literal">Observable.create</code> method in order to obtain a random quote as before and emit the quote to the observer. We then return an empty <code class="literal">Subscription</code> object. This is shown in the following code snippet:</p><pre class="programlisting">def randomQuote = Observable.create[String] { obs =&gt;
  val url = "http://www.iheartquotes.com/api/v1/random?" +
    "show_permalink=false&amp;show_source=false"
  obs.onNext(Source.fromURL(url).getLines.mkString)
  obs.onCompleted()
  Subscription()
}
</pre><p>There is a subtle difference between the <code class="literal">Observable</code> object returned by the <code class="literal">randomQuote</code> method and the one returned by the <code class="literal">fetchQuoteObservable</code> method, defined earlier. The <code class="literal">fetchQuoteObservable</code> method creates a <code class="literal">Future</code> object in order to obtain a quote and emits the quote in that <code class="literal">Future</code> object to every observer. By contrast, the <code class="literal">randomQuote</code> method fetches a new quote every time the <code class="literal">subscribe</code> method is called. In the previously introduced terminology, the <code class="literal">randomQuote</code> method creates cold <code class="literal">Observable</code> objects, which emit events only when we subscribe to them, whereas the <code class="literal">fetchQuoteObservable</code> method creates hot <code class="literal">Observable</code> objects, which emit the same quote to all their observers.</p><p>To re-subscribe to a failed <code class="literal">Observable</code> object, we can use the <code class="literal">retry</code> combinator. The <code class="literal">retry</code> combinator takes an input <code class="literal">Observable</code>, and returns another <code class="literal">Observable</code> object that emits events from the input <code class="literal">Observable</code> object until it either completes or fails. If the input <code class="literal">Observable</code> object fails, the <code class="literal">retry</code> combinator subscribes to the input <code class="literal">Observable</code> object again.</p><p>We now use the <code class="literal">retry</code> combinator with the <code class="literal">randomQuote</code> method to fetch quotes until we obtain a quote shorter than 100 characters. We first transform the long quotes from the <code class="literal">randomQuote</code> method into failed observables, which enables <code class="literal">retry</code> to subscribe again to obtain another quote. To do this, we define a new <code class="literal">Observable</code> object called <code class="literal">errorMessage</code>, which emits a string <code class="literal">"Retrying..."</code> and then fails. We then traverse the <code class="literal">text</code> quote from <code class="literal">randomQuote</code> in a <code class="literal">for</code> comprehension. If the <code class="literal">text</code> quote is shorter than 100 characters, we traverse an <code class="literal">Observable</code> object that emits text. Otherwise, we traverse the <code class="literal">errorMessage</code> object to output <code class="literal">"Retrying..."</code> instead of <code class="literal">text</code>. This <code class="literal">for</code> comprehension defines an <code class="literal">Observable</code> object <code class="literal">quoteMessage</code>, which either emits a short quote, or emits <code class="literal">"Retrying..."</code> and fails. The marble diagram of the resulting <code class="literal">Observable</code> object, called <code class="literal">quoteMessage</code>, is shown for these two cases, in which the exception in the <code class="literal">Observable</code> object is shown with a cross symbol:</p><div class="mediaobject"><img src="graphics/image_06_006.jpg" /></div><p>Finally, we call the <code class="literal">retry</code> method on the <code class="literal">quoteMessage</code> object and subscribe to it. We specify that we want to retry up to five times, as omitting the argument would retry forever. We implement the <code class="literal">Observable</code> object <code class="literal">quoteMessage</code> in the following program:</p><pre class="programlisting">object CompositionRetry extends App {
  import Observable._
  def errorMessage = items("Retrying...") ++ error(new Exception)
  def quoteMessage = for {
    text    &lt;- randomQuote
    message &lt;- if (text.size &lt; 100) items(text) else errorMessage
  } yield message
  quoteMessage.retry(5).subscribe(log _)
  Thread.sleep(2500)
}
</pre><p>Run this program several times. You will notice that a short quote is either printed right away, or after a few retries, depending on some random distribution of the quotes. You may be wondering how many quotes are on average longer than 100 characters. It turns out that it is easy to do this statistic in Rx. We introduce two new combinators. The first one is called <code class="literal">repeat</code>, and it is very similar to <code class="literal">retry</code>. Instead of re-subscribing to an <code class="literal">Observable</code> object when it fails, it re-subscribes when an <code class="literal">Observable</code> object completes. The second combinator is called <code class="literal">scan</code> and it is similar to the <code class="literal">scanLeft</code> operator on collections. Given an input <code class="literal">Observable</code> object and a starting value for the accumulation, it emits the value of the accumulation by applying the specified binary operator to the accumulation and the event, updating the accumulation as the events arrive. The usage of the <code class="literal">repeat</code> and <code class="literal">scan</code> combinators is illustrated in the following program:</p><pre class="programlisting">object CompositionScan extends App {
  CompositionRetry.quoteMessage.retry.repeat.take(100).scan(0) {
    (n, q) =&gt; if (q == "Retrying...") n + 1 else n
  } subscribe(n =&gt; log(s"$n / 100"))
}
</pre><p>In the preceding example, we use the <code class="literal">Observable</code> object <code class="literal">quoteMessage</code> defined earlier in order to obtain a short quote or a message <code class="literal">"Retrying..."</code> followed by an exception. We retry quotes that have failed because of being too long, and repeat whenever a quote is short enough. We take 100 quotes in total, and use the <code class="literal">scan</code> operator to count the short quotes. When we ran this program, it turned out that 57 out of 100 quotes are too long for our book.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note69"></a>Note</h3><p>The <code class="literal">retry</code> method is used in order to repeat events from failed <code class="literal">Observable</code> objects. Similarly, the <code class="literal">repeat</code> method is used in order to repeat events from completed <code class="literal">Observable</code> objects.</p></div><p>In the examples shown so far, we use the same <code class="literal">Observable</code> object to re-subscribe and emit additional events if that <code class="literal">Observable</code> object fails. In some cases, we want to emit specific events when we encounter an exception, or fall back to a different <code class="literal">Observable</code> object. Recall that this is what we did with <code class="literal">Future</code> objects previously. The Rx methods that replace an exception with an event, or multiple events from another <code class="literal">Observable</code> object, are called <code class="literal">onErrorReturn</code> and <code class="literal">onErrorResumeNext</code>, respectively. In the following program, we first replace the exception from <code class="literal">status</code> with a string <code class="literal">"exception occurred."</code>. We then replace the exception with strings from another <code class="literal">Observable</code> object:</p><pre class="programlisting">object CompositionErrors extends App {
  val status = items("ok", "still ok") ++ error(new Exception)
  val fixedStatus =
    status.onErrorReturn(e =&gt; "exception occurred.")
  fixedStatus.subscribe(log _)
  val continuedStatus =
    status.onErrorResumeNext(e =&gt; items("better", "much better"))
  continuedStatus.subscribe(log _)
}
</pre><p>Having seen various ways to compose <code class="literal">Observable</code> objects, we turn to the concurrency features of Rx. So far, we did not pay close attention to the thread on which an <code class="literal">Observable</code> object emits events. In the next section, we will study how to transfer events between <code class="literal">Observable</code> objects on different threads, and learn when this can be useful.</p></div></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec42"></a>Rx schedulers</h2></div></div><hr /></div><p>At the beginning of this chapter, we observed that different <code class="literal">Observable</code> objects emit events on different threads. A synchronous <code class="literal">Observable</code> object emits on the caller thread when the <code class="literal">subscribe</code> method gets invoked. The <code class="literal">Observable.timer</code> object emits events asynchronously on threads internally used by Rx. Similarly, events in <code class="literal">Observable</code> objects created from <code class="literal">Future</code> objects are emitted on <code class="literal">ExecutionContext</code> threads. What if we want to use an existing <code class="literal">Observable</code> object to create another <code class="literal">Observable</code> object bound to a specific thread?</p><p>To encapsulate the choice of the thread on which an <code class="literal">Observable</code> object should emit events, Rx defines a special class called <code class="literal">Scheduler</code>. A <code class="literal">Scheduler</code> class is similar to the <code class="literal">Executor</code> and <code class="literal">ExecutionContext</code> interfaces we saw in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Traditional Building Blocks of Concurrency</em></span>. The <code class="literal">Observable</code> objects come with a combinator called <code class="literal">observeOn</code>. This combinator returns a new <code class="literal">Observable</code> object that emits events using the specified <code class="literal">Scheduler</code> class. In the following program, we instantiate a <code class="literal">Scheduler</code> object called <code class="literal">ComputationScheduler</code>, which emits events using an internal thread pool. We then emit events with and without calling the <code class="literal">observeOn</code> combinator:</p><pre class="programlisting">object SchedulersComputation extends App {
  val scheduler = schedulers.ComputationScheduler()
  val numbers = Observable.from(0 until 20)
  numbers.subscribe(n =&gt; log(s"num $n"))
  numbers.observeOn(scheduler).subscribe(n =&gt; log(s"num $n"))
  Thread.sleep(2000)
}
</pre><p>From the output, we can see that the second <code class="literal">subscribe</code> call uses a thread pool:</p><pre class="programlisting">run-main-42: num 0
...
run-main-42: num 19
RxComputationThreadPool-1: num 0
...
RxComputationThreadPool-1: num 19
</pre><p>The <code class="literal">ComputationScheduler</code> object maintains a pool of threads intended for computational tasks. If processing the events blocks or waits for I/O operations, we must use the <code class="literal">IOScheduler</code> object, which automatically spawns new threads when necessary. Exceptionally, if processing each event is a very coarse-grained task, we can use the <code class="literal">NewThreadScheduler</code> object, which spawns a new thread for each event.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec60"></a>Using custom schedulers for UI applications</h3></div></div></div><p>Built-in Rx schedulers are useful for most tasks, but in some cases we need more control. Most UI toolkits only allow you to read and modify UI elements from a special thread. This thread is called the <span class="strong"><strong>event-dispatching</strong></span> thread. This approach simplifies the design and the implementation of a UI toolkit, and protects clients from subtle concurrency errors. Since the UI usually does not usually represent a computational bottleneck, this approach has been widely adopted; the Swing toolkit uses an <code class="literal">EventDispatchThread</code> object in order to propagate events.</p><p>The <code class="literal">Observable</code> objects are particularly useful when applied to UI applications; a user interface is all about events. In subsequent examples, we will use the Scala Swing library to illustrate the usefulness of Rx in UI code. We start by adding the following dependency to our project:</p><pre class="programlisting">libraryDependencies +=
  "org.scala-lang.modules" %% "scala-swing" % "1.0.1"
</pre><p>We will start by creating a simple Swing application with a single button. Clicking on this button will print a message to the standard output. This application illustrates how to convert Swing events into an <code class="literal">Observable</code> object. We will start by importing the relevant Scala Swing packages as follows:</p><pre class="programlisting">import scala.swing._
import scala.swing.event._
</pre><p>To create a Swing application, we need to extend the <code class="literal">SimpleSwingApplication</code> class. This class has a single abstract method, <code class="literal">top</code>, which needs to return a <code class="literal">Frame</code> object. The Swing's abstract <code class="literal">Frame</code> class represents the application window. We return a new <code class="literal">MainFrame</code> object, which is a subclass of the <code class="literal">Frame</code> object. In the <code class="literal">MainFrame</code> constructor, we set the window title bar text to <code class="literal">Swing Observables</code>, and instantiate a new <code class="literal">Button</code> object with the <code class="literal">Click</code> text. We then set the contents of the <code class="literal">MainFrame</code> constructor to that button.</p><p>So much for the UI elements and their layout; we now want to add some logic to this simple application. Traditionally, we would make a Swing application interactive by installing callbacks to various UI elements. Using Rx, we instead convert callbacks into event streams; we define an <code class="literal">Observable</code> object called <code class="literal">buttonClicks</code> that emits an event every time the button element is clicked on. We use the <code class="literal">Observable.create</code> method in order to register a <code class="literal">ButtonClicked</code> callback that calls the <code class="literal">onNext</code> method on the observer. To log clicks to the standard output, we subscribe to <code class="literal">buttonClicks</code>. The complete Swing application is shown in the following code snippet:</p><pre class="programlisting">object SchedulersSwing extends SimpleSwingApplication {
  def top = new MainFrame {
    title = "Swing Observables"
    val button = new Button {
      text = "Click"
    }
    contents = button
    val buttonClicks = Observable.create[Button] { obs =&gt;
      button.reactions += {
        case ButtonClicked(_) =&gt; obs.onNext(button)
      }
      Subscription()
    }
    buttonClicks.subscribe(_ =&gt; log("button clicked"))
  }
}
</pre><p>Running this application opens the window, shown in the following screenshot. Clicking on the <span class="strong"><strong>Click</strong></span> button prints a string to the standard output. We can see that the events are emitted on the thread called <code class="literal">AWT-EventQueue-0</code>, which is the event-dispatching thread in Swing:</p><div class="mediaobject"><img src="graphics/image_06_007.jpg" /></div><p>One downside of single-threaded UI toolkits is that long-running computations on the event-dispatching thread block the UI and harm the user experience. If we issue a blocking HTTP request each time the user clicks on a button, we willÂ witness a noticeable lag after each click. Luckily, this is easy to address by executing long-running computations asynchronously.</p><p>Usually, we are not content with just starting an asynchronous computation. Once the asynchronous computation produces a result, we would like to display it in the application. Recall that we are not allowed to do this directly from the computation thread; we need to return the control back to event-dispatching thread. Swing defines the <code class="literal">invokeLater</code> method, which schedules tasks on Swing's event-dispatching thread. On the other hand, Rx has a <code class="literal">Schedulers.from</code> built-in method that converts an <code class="literal">Executor</code> object into a <code class="literal">Scheduler</code> object. To bridge the gap between Swing's <code class="literal">invokeLater</code> method and Rx schedulers, we implement a custom <code class="literal">Executor</code> object that wraps a call to <code class="literal">invokeLater</code>, and we pass this <code class="literal">Executor</code> object to <code class="literal">Schedulers.from</code>. The custom <code class="literal">swingScheduler</code> object is implemented as follows:</p><pre class="programlisting">import java.util.concurrent.Executor
import rx.schedulers.Schedulers.{from =&gt; fromExecutor}
import javax.swing.SwingUtilities.invokeLater
val swingScheduler = new Scheduler {
  val asJavaScheduler = fromExecutor(new Executor {
    def execute(r: Runnable) = invokeLater(r)
  })
}
</pre><p>We can use the newly-defined <code class="literal">swingScheduler</code> object in order to send events back to Swing. To illustrate this, let's implement a small web browser application. Our browser consists of a <code class="literal">urlfield</code> address bar and the <span class="strong"><strong>Feeling lucky</strong></span> button. Typing into the address bar displays suggestions for the URL, and clicking on the button displays the raw HTML of the webpage. The browser is not a trivial application, so we separate the implementation of the UI layout from the UI logic. We start by defining the <code class="literal">BrowserFrame</code> class, which describes the layout of the UI elements:</p><pre class="programlisting">abstract class BrowserFrame extends MainFrame {
  title = "MiniBrowser"
  val specUrl = "http://www.w3.org/Addressing/URL/url-spec.txt"
  val urlfield = new TextField(specUrl)
  val pagefield = new TextArea
  val button = new Button {
    text = "Feeling Lucky"
  }
  contents = new BorderPanel {
    import BorderPanel.Position._
    layout(new BorderPanel {
      layout(new Label("URL:")) = West
      layout(urlfield) = Center
      layout(button) = East
    }) = North
    layout(pagefield) = Center
  }
  size = new Dimension(1024, 768)
}
</pre><p>Scala Swing was implemented long before the introduction of Rx, so it does not come with event streams. We use Scala's extension method pattern in order to enrich the existing UI element classes with <code class="literal">Observable</code> objects, and add implicit classes, <code class="literal">ButtonOps</code> and <code class="literal">TextFieldOps</code>, with methods, <code class="literal">clicks</code> and <code class="literal">texts</code>, respectively. The <code class="literal">clicks</code> method returns an <code class="literal">Observable</code> object that emits an event each time the corresponding button is clicked on. Similarly, the <code class="literal">texts</code> method emits an event each time the content of a text field changes:</p><pre class="programlisting">implicit class ButtonOps(val self: Button) {
  def clicks = Observable.create[Unit] { obs =&gt;
    self.reactions += {
      case ButtonClicked(_) =&gt; obs.onNext(())
    }
    Subscription()
  }
}
implicit class TextFieldOps(val self: TextField) {
  def texts = Observable.create[String] { obs =&gt;
    self.reactions += {
      case ValueChanged(_) =&gt; obs.onNext(self.text)
    }
    Subscription()
  }
}
</pre><p>We now have the necessary utilities to concisely define the logic of our web browser. We implement the browser logic in a trait called <code class="literal">BrowserLogic</code>, annotated with a self-type <code class="literal">BrowserFrame</code> object. The typeÂ <code class="literal">self</code>Â allows you to mix the <code class="literal">BrowserLogic</code> trait only into classes that extend the <code class="literal">BrowserFrame</code> object. This makes sense; the browser logic needs to know about UI events to react to them.</p><p>There are two main functionalities supported by the web browser. First, the browser needs to suggest possible URLs while the user types into the address bar. To facilitate this, we define a helper method, <code class="literal">suggestRequest</code>, which takes a term from the address bar and returns an <code class="literal">Observable</code> object with the possible completions. This <code class="literal">Observable</code> object uses Google's query suggestion service to get a list of possible URLs. To cope with network errors, the <code class="literal">Observable</code> object will time-out after 0.5 seconds if there is no reply from the server, and emit an error message.</p><p>Second, our browser needs to display the contents of the specified URL, when we click on the <span class="strong"><strong>Feeling lucky</strong></span> button. To achieve this, we define another helper method named <code class="literal">pageRequest</code>, which returns an <code class="literal">Observable</code> object with the raw HTML of the web page. This <code class="literal">Observable</code> object times-out after four seconds if the page is not loaded by that time.</p><p>Using these helper methods and the UI element <code class="literal">Observable</code> objects, we can encode the browser logic more easily. Each <code class="literal">urlField</code> text modification event maps into a nested <code class="literal">Observable</code> object with the suggestion. The call to <code class="literal">concat</code> then flattens the nested <code class="literal">Observable</code> object. The suggestion events transfer back to the Swing event-dispatching thread using the <code class="literal">observeOn</code> combinator. We subscribe to the events on the Swing event-dispatching thread in order to modify the contents of the <code class="literal">pagefield</code> text area. We subscribe to <code class="literal">button.clicks</code> in a similar way:</p><pre class="programlisting">trait BrowserLogic {
  self: BrowserFrame =&gt;
  def suggestRequest(term: String): Observable[String] = {
    val url = "http://suggestqueries.google.com/" +
      s"complete/search?client=firefox&amp;q=$term"
    val request = Future { Source.fromURL(url).mkString }
    Observable.from(request)
              .timeout(0.5.seconds)
              .onErrorReturn(e =&gt; "(no suggestion)")
  }
  def pageRequest(url: String): Observable[String] = {
    val request = Future { Source.fromURL(url).mkString }
    Observable.from(request)
              .timeout(4.seconds)
              .onErrorReturn(e =&gt; s"Could not load page: $e")
  }
  urlfield.texts.map(suggestRequest).concat
                .observeOn(swingScheduler)
                .subscribe(response =&gt; pagefield.text = response)
  button.clicks.map(_ =&gt; pageRequest(urlfield.text)).concat
               .observeOn(swingScheduler)
               .subscribe(response =&gt; pagefield.text = response)
}
</pre><p>After defining both the UI layout and the UI logic, we only need to instantiate the browser frame in a Swing application:</p><pre class="programlisting">object SchedulersBrowser extends SimpleSwingApplication {
  def top = new BrowserFrame with BrowserLogic
}
</pre><p>Running the application opens the browser frame, and we can start surfing in our very own Rx-based web browser. The guys at Mozilla and Google will surely be impressed when they see the following screenshot:</p><div class="mediaobject"><img src="graphics/image_06_008.jpg" /></div><p>Although our web browser is very simple, we managed to separate its functionality into the UI layout and browser logic layers. The UI layout layer defines <code class="literal">Observable</code> objects such as <code class="literal">urlfield.texts</code> and <code class="literal">button.clicks</code> as part of its interface. The browser logic layer relies on the functionality from the UI layout layer; for example, we could not describe the updates to the <code class="literal">pagefield</code> UI element without referencing the <code class="literal">Observable</code> object <code class="literal">button.clicks</code>.</p><p>We say that the browser logic depends on the UI layout, but not vice versa. For a UI application, this can be acceptable, but other applications require a more loosely coupled design, in which different layers do not refer to each other directly.</p></div></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec43"></a>Subjects and top-down reactive programming</h2></div></div><hr /></div><p>Composing <code class="literal">Observable</code> objects is similar to composing functions, collections, or futures. Complex <code class="literal">Observable</code> objects are formed from simpler parts using functional composition. This is a very Scala-idiomatic pattern, and it results in concise and understandable programs.</p><p>A not-so-obvious downside of functional composition is that it favors theÂ <span class="strong"><strong>bottom-up programming style</strong></span>. An <code class="literal">Observable</code> object cannot be created without a reference to another <code class="literal">Observable</code> object that it depends on. For instance, we cannot create an <code class="literal">Observable</code> object using the <code class="literal">map</code> combinator without having an input <code class="literal">Observable</code> object to call the <code class="literal">map</code> method on. In a bottom-up programming style, we build complex programs by implementing the simplest parts first, and then gradually working our way up. By contrast, in a <span class="strong"><strong>top-down programming style</strong></span>, we first define the complex parts of the system, and then gradually divide them into successively smaller pieces. The top-down programming style allows first declaring an <code class="literal">Observable</code> object, and defining its dependencies later.</p><p>To allow building systems in a top-down programming style, Rx defines an abstraction called a subject, represented by the <code class="literal">Subject</code> trait. A <code class="literal">Subject</code> trait is simultaneously an <code class="literal">Observable</code> object and an <code class="literal">Observer</code> object. As an <code class="literal">Observable</code> object, a <code class="literal">Subject</code> trait can emit events to its subscribers. As an <code class="literal">Observer</code> object, a <code class="literal">Subject</code> trait can subscribe to different input <code class="literal">Observable</code> objects and forward their events to its own subscribers.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note70"></a>Note</h3><p>A <code class="literal">Subject</code> trait is an <code class="literal">Observable</code> object whose inputs can change after its creation.</p></div><p>To see how to use a Subject trait in practice, let's assume that we are building our own operating system. Having witnessed how practical the Rx event streams are, we decide to use them throughout our operating system, which we name <span class="strong"><strong>RxOS</strong></span>. To make RxOS pluggable, its functionality is divided into separate components called kernel modules. Each kernel module might define a certain number of <code class="literal">Observable</code> objects. For example, a <code class="literal">TimeModule</code> module exposes an <code class="literal">Observable</code> object named <code class="literal">systemClock</code>, which outputs a string with the system uptime every second:</p><pre class="programlisting">object TimeModule {
  import Observable._
  val systemClock = interval(1.seconds).map(t =&gt; s"systime: $t")
}
</pre><p>System output is an essential part of every operating system. We want RxOS to output important system events such as the system uptime. We already know how to do this by calling <code class="literal">subscribe</code> on the <code class="literal">systemClock</code> object from the <code class="literal">TimeModule</code> module, as shown in the following code:</p><pre class="programlisting">object RxOS {
  val messageBus = TimeModule.systemClock.subscribe(log _)
}
</pre><p>Let's say that another team now independently develops another kernel module named <code class="literal">FileSystemModule</code>, which exposes an <code class="literal">Observable</code> object called <code class="literal">fileModifications</code>. This <code class="literal">Observable</code> object emits a filename each time a file is modified:</p><pre class="programlisting">object FileSystemModule {
  val fileModifications = modified(".")
}
</pre><p>Our core development team now decides that the <code class="literal">fileModifications</code> objects are important system events and wants to log these events as part of the <code class="literal">messageBus</code> subscription. We now need to redefine the singleton object <code class="literal">RxOS</code>, as shown in the following code snippet:</p><pre class="programlisting">object RxOS {
  val messageBus = Observable.items(
    TimeModule.systemClock,
    FileSystemModule.fileModifications
  ).flatten.subscribe(log _)
}
</pre><p>This patch solves the situation, but what if another kernel module introduces another group of important system events? With our current approach, we will have to recompile the RxOS kernel each time some third-party developer implements a kernel module. Even worse, the <code class="literal">RxOS</code> object definition references kernel modules, and thus, depends on them. Developers who want to build custom, reduced versions of RxOS now need to tweak the kernel source code.</p><p>This is the classic culprit of the bottom-up programming style; we are unable to declare the <code class="literal">messageBus</code> object without declaring its dependencies, and declaring them binds us to specific kernel modules.</p><p>We now redefine the <code class="literal">messageBus</code> object as an Rx subject. We create a new <code class="literal">Subject</code> instance that emits strings, and we then subscribe to it, as shown in the following example:</p><pre class="programlisting">object RxOS {
  val messageBus = Subject[String]()
  messageBus.subscribe(log _)
}
</pre><p>At this point, the <code class="literal">messageBus</code> object is not subscribed to any <code class="literal">Observable</code> objects and does not emit any events. We can now define the RxOS boot sequence separately from the modules and the kernel code. The boot sequence specifies which kernel modules to subscribe with the <code class="literal">messageBus</code> object, and stores their subscriptions into the <code class="literal">loadedModules</code> list:</p><pre class="programlisting">object SubjectsOS extends App {
  log(s"RxOS boot sequence starting...")
  val loadedModules = List(
    TimeModule.systemClock,
    FileSystemModule.fileModifications
  ).map(_.subscribe(RxOS.messageBus))
  log(s"RxOS boot sequence finished!")
  Thread.sleep(10000)
  for (mod &lt;- loadedModules) mod.unsubscribe()
  log(s"RxOS going for shutdown")
}
</pre><p>The boot sequence first subscribes the <code class="literal">messageBus</code> object to each of the required modules. We can do this because the <code class="literal">messageBus</code> object is an <code class="literal">Observer</code> object, in addition to being an <code class="literal">Observable</code> object. The <code class="literal">RxOS</code> then stays up for 10 seconds before calling <code class="literal">unsubscribe</code> on the modules and shutting down. During this time, the system clock emits an event to the <code class="literal">messageBus</code> object every second. Similarly, the <code class="literal">messageBus</code> object outputs the name of the modified file every time a file modification occurs, as shown in the following diagram:</p><div class="mediaobject"><img src="graphics/image_06_009.jpg" /></div><p>The difference between the two approaches is shown in the preceding figure. In the bottom-up approach, we first need to define all the kernel modules and then make RxOS depend on them. In the top-down approach, RxOS does not depend on the kernel modules. Instead, it is glued together with them by the boot sequence module. The RxOS clients no longer need to tweak or recompile the kernel code if they want to add a new kernel module. In fact, the new design even allows hot-plugging kernel modules into a running RxOS instance, long after the boot sequence is completed.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip71"></a>Tip</h3><p>Use <code class="literal">Subject</code> instances when you need to create an <code class="literal">Observable</code> object whose inputs are not available when the <code class="literal">Observable</code> object is created.</p></div><p>In our example, designing a web browser is a lot like ordering a MacBook. After specifying the preferred processor type and the hard disk size, the MacBook is assembled, and its components cannot be exchanged easily. Analogously, after implementing the browser's UI layout, the event streams that describe the interaction between UI components are declared only once, and cannot change if the UI components are replaced. On the other hand, building an OS is more like building a desktop computer from custom components. After putting the motherboard into the case, we can plug in components such as the graphics card or the RAID controller independently. Similarly, after declaring the <code class="literal">messageBus</code> subject, we can plug in any number of kernel modules at any time during the execution of the program.</p><p>Although the <code class="literal">Subject</code> interface is more flexible than the <code class="literal">Observable</code> interface, you should not always use the <code class="literal">Subject</code> instances and rely exclusively on the top-down programming style. While declaring the dependencies of an <code class="literal">Observable</code> object at its creation point makes the application less flexible, it also makes it more declarative and easier to understand. Modern large-scale applications usually combine both bottom-up and top-down approaches.</p><p>Rx defines several other types of subject. The typeÂ <code class="literal">ReplaySubject</code>Â is a <code class="literal">Subject</code> implementation that buffers the events it receives as an <code class="literal">Observer</code> object. When another <code class="literal">Observer</code> object subscribes to a <code class="literal">ReplaySubject</code> instance, all the events previously buffered by the <code class="literal">ReplaySubject</code> instance are replayed. In the following code snippet, we define a <code class="literal">ReplaySubject</code> instance called <code class="literal">messageLog</code> in <code class="literal">RxOS</code>:</p><pre class="programlisting">object RxOS {
  val messageBus = Subject[String]()
  val messageLog = subjects.ReplaySubject[String]()
  messageBus.subscribe(log _)
  messageBus.subscribe(messageLog)
}
</pre><p>The <code class="literal">messageLog</code> object subscribes to the <code class="literal">messageBus</code> object in order to buffer all the system messages. If, for example, we now want to dump all the messages into a log file, we can subscribe to the <code class="literal">messageLog</code> object immediately before the application ends, as shown in the following example:</p><pre class="programlisting">log(s"RxOS dumping the complete system event log")
RxOS.messageLog.subscribe(logToFile)
log(s"RxOS going for shutdown")
</pre><p>Rx also defines two other subjects called <code class="literal">BehaviorSubject</code> and <code class="literal">AsyncSubject</code>. The <code class="literal">BehaviorSubject</code> class buffers only the most recent event, and the <code class="literal">AsyncSubject</code> class only emits the event immediately preceding <code class="literal">onComplete</code>. We will not study their exact semantics and use case here, but we refer you to the online documentation to find out more about them.</p></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec44"></a>Summary</h2></div></div><hr /></div><p>First-class event streams are an extremely expressive tool for modelling dynamic, event-based systems with time-varying values. Rx <code class="literal">Observable</code> objects are an event stream implementation designed to build scalable, concurrent, event-based applications. In this chapter, we saw how to create Rx <code class="literal">Observable</code> objects and how to subscribe to their events. We studied the <code class="literal">Observable</code> contract and learned how to compose complex <code class="literal">Observable</code> objects from simple ones. We investigated various ways of recovering from failures and saw how to use Rx schedulers to transfer events between threads. Finally, we learned how to design loosely coupled systems with Rx subjects. These powerful tools together allow us to build a plethora of different applications, ranging from web browsers, FTP servers, the music and video players to real-time games and trading platforms, and even operating systems.</p><p>Due to the increasing popularity of reactive programming, a number of frameworks similar to Rx have appeared in the recent years: REScala, Akka Streams, and Reactive Collections, to name a few. We did not study the semantics of these frameworks in this chapter, but leave it to the readers to explore them on their own.</p><p>We have seen that <code class="literal">Observable</code> objects are very declarative in nature, making the Rx programming model easy to use and understand. Nevertheless, it is sometimes useful to model a system imperatively, using explicit state. In the next chapter, we will study software transactional memory, which allows accessing shared program state without the risk of deadlocks and race conditions, which we learned about in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Concurrency on the JVM and the Java Memory Model</em></span>.</p></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec45"></a>Exercises</h2></div></div><hr /></div><p>In the following exercises, you will need to implement different <code class="literal">Observable</code> objects. The exercises show different use cases for <code class="literal">Observable</code> objects, and contrast the different ways of creating <code class="literal">Observable</code> objects. Also, some of the exercises introduce new reactive programming abstractions, such as reactive maps and reactive priority queues.</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Implement a custom <code class="literal">Observable[Thread]</code> object that emits an event when it detects that a thread was started. The implementation is allowed to miss some of the events.</p></li><li><p>Implement an <code class="literal">Observable</code> object that emits an event every 5 seconds and every 12 seconds, but not if the elapsed time is a multiple of 30 seconds. Use functional combinators on <code class="literal">Observable</code> objects.</p></li><li><p>Use the <code class="literal">randomQuote</code> method from this section in order to create an <code class="literal">Observable</code> object with the moving average of the quote lengths. Each time a new quote arrives, a new average value should be emitted.</p></li><li><p>Implement the reactive signal abstraction, represented with the <code class="literal">Signal[T]</code> type. The typeÂ <code class="literal">Signal[T]</code>Â comes with the method <code class="literal">apply</code>, used to query the last event emitted by this signal, and several combinators with the same semantics as the corresponding <code class="literal">Observable</code> methods:
</p><pre class="programlisting">            class Signal[T] {
              def apply(): T = ???
              def map(f: T =&gt; S): Signal[S] = ???
              def zip[S](that: Signal[S]): Signal[(T, S)] = ???
              def scan[S](z: S)(f: (S, T) =&gt; S) = ???
            }
</pre><p>
</p><p>Then, add the method <code class="literal">toSignal</code> to the typeÂ <code class="literal">Observable[T]</code>, which converts an <code class="literal">Observable</code> object to a reactive signal:</p><p>
</p><pre class="programlisting">            def toSignal: Signal[T] = ???
</pre><p>
</p><p>Consider using Rx subjects for this task.</p></li><li><p>Implement the reactive cell abstraction, represented with the typeÂ <code class="literal">RCell[T]</code>:
</p><pre class="programlisting">            class RCell[T] extends Signal[T] {
              def :=(x: T): Unit = ???
            }
</pre><p>
</p><p>A reactive cell is simultaneously a reactive signal from the previous exercise. Calling the <code class="literal">:=</code> method sets a new value to the reactive cell, and emits an event.</p></li><li><p>Implement the reactive map collection, represented with the <code class="literal">RMap</code> class:
</p><pre class="programlisting">            class RMap[K, V] {
              def update(k: K, v: V): Unit
              def apply(k: K): Observable[V]
            }
</pre><p>
</p><p>The <code class="literal">update</code> method behaves like the update on a regular <code class="literal">Map</code> collection. Calling <code class="literal">apply</code> on a reactive map returns an <code class="literal">Observable</code> object with all the subsequent updates of the specific key.</p></li><li><p>Implement the reactive priority queue, represented byÂ the <code class="literal">RPriorityQueue</code> class:
</p><pre class="programlisting">            class RPriorityQueue[T] {
              def add(x: T): Unit = ???
              def pop(): T = ???
              def popped: Observable[T] = ???
            }
</pre><p>
</p><p>The reactive priority queue exposes the <code class="literal">Observable</code> object <code class="literal">popped</code>, which emits events whenever the smallest element in the priority queue gets removed by calling <code class="literal">pop</code>.</p></li><li><p>Implement the <code class="literal">copyFile</code> method, which copies a file specified with the <code class="literal">src</code> parameter to the destination specified with the <code class="literal">dest</code> parameter. The method returns an <code class="literal">Observable[Double]</code> object, which emits an event with the file transfer progress every 100 milliseconds:
</p><pre class="programlisting">            def copyFile(src: String, dest: String): Observable[Double]
</pre><p>
</p><p>The resulting <code class="literal">Observable</code> object must complete if the file transfer completes successfully, or otherwise fail with an exception.</p></li><li><p>Create a custom Swing component, called <code class="literal">RxCanvas</code>, which exposes mouse events using <code class="literal">Observable</code> objects:
</p><pre class="programlisting">            class RxCanvas extends Component {
              def mouseMoves: Observable[(Int, Int)]
              def mousePresses: Observable[(Int, Int)]
              def mouseReleases: Observable[(Int, Int)]
            }
</pre><p>
</p><p>Use the <code class="literal">RxCanvas</code> component to build your own Paint program, in which you can drag lines on the canvas using a brush, and save the contents of the canvas to an image file. Consider using nested <code class="literal">Observable</code> objects to implement dragging.</p></li><li><p>Implement a method called <code class="literal">scatterGather</code> on the typeÂ <code class="literal">Observable</code>, which forwards every event to one of the worker threads, performs some work on those threads, and emits the computed results on a new <code class="literal">Observable</code> object. The signature of this method is as follows, where type <code class="literal">T</code> is the type of the events in the original <code class="literal">Observable</code>:
</p><pre class="programlisting">            def scatterGather[S](f: T =&gt; S): Observable[S]
</pre></li><li><p>Implement the <code class="literal">sorted</code> method on the typeÂ <code class="literal">Observable</code>, which emits incoming events in the sorted order. The events can be emitted only after the original <code class="literal">Observable</code> terminates.</p></li></ol></div></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch07"></a>ChapterÂ 7.Â Software Transactional Memory</h2></div></div></div><div class="blockquote"><table border="0" width="100%" cellspacing="0" cellpadding="0" class="blockquote" summary="Block quote"><tr><td width="10%" valign="top">Â </td><td width="80%" valign="top"><p><span class="emphasis"><em>"Everybody who learns concurrency and thinks they understand it, ends up finding mysterious races they thought weren't possible, and discovers that they didn't actually understand it yet after all."</em></span></p></td><td width="10%" valign="top">Â </td></tr><tr><td width="10%" valign="top">Â </td><td colspan="2" align="right" valign="top">--<span class="attribution"><span class="emphasis"><em>Herb Sutter</em></span></span></td></tr></table></div><p>While investigating the fundamental primitives of concurrency in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Concurrency on the JVM and the Java Memory Model</em></span>, we recognized the need for protecting parts of the program from shared access. We saw that a basic way of achieving this isolation is the <code class="literal">synchronized</code> statement, which uses intrinsic object locks to ensure that at most a single thread executes a specific part of the program at the same time. The disadvantage of using locks is that they can easily cause deadlocks, a situation in which the program cannot progress.</p><p>In this chapter, we will introduce <span class="strong"><strong>Software Transactional Memory</strong></span> (<span class="strong"><strong>STM</strong></span>), a concurrency control mechanism for controlling access to shared memory, which greatly reduces the risk of deadlocks and races. An STM is used to designate critical sections of the code. Instead of using locks in order to protect critical sections, STM tracks the reads and writes to shared memory, and serializes critical sections with interleaving reads and writes. The <code class="literal">synchronized</code> statement is replaced with the atomic blocks that express segments of the program that need to be executed in isolation. STM is safer and easier to use, and at the same time, guarantees relatively good scalability.</p><p>The idea of <span class="emphasis"><em>memory transactions</em></span> stems from database transactions, which ensure that a sequence of database queries occurs in isolation. A memory transaction is a sequence of reads and writes to shared memory that logically occur at a single point in time. When a memory transaction T occurs, concurrent memory transactions observe the state of the memory either before the transaction T started, or after the transaction T completed, but not the intermediate states during the execution of T. This property is called <span class="strong"><strong>isolation</strong></span>.</p><p>As we will see, <span class="strong"><strong>composability</strong></span> is another important advantage of using an STM. Consider a lock-based hash table implementation with thread-safe <code class="literal">insert</code> and <code class="literal">remove</code> operations. While the individual <code class="literal">insert</code> and <code class="literal">remove</code> operations can be safely invoked by different threads, it is impossible to implement a method that removes an element from one hash table and adds it to another hash table, without exposing the intermediate state in which the element is not present in either hash table. Traditionally, STM was proposed as a part of the programming language with the advantage that certain transaction limitations can be ensured at compile time. Since this approach requires intrusive changes to a language, many software transactional memories are implemented as libraries. ScalaSTM is one such example. We will use ScalaSTM as the concrete STM implementation. Concretely, we cover the following topics in this chapter:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The disadvantages of atomic variables</p></li><li style="list-style-type: disc"><p>The semantics and internals of STM</p></li><li style="list-style-type: disc"><p>Transactional references</p></li><li style="list-style-type: disc"><p>The interaction between transactions and external side effects</p></li><li style="list-style-type: disc"><p>Semantics of single operation transactions and nested transactions</p></li><li style="list-style-type: disc"><p>Retrying transactions conditionally and timing out transactions</p></li><li style="list-style-type: disc"><p>Transaction-local variables, transactional arrays, and transactional maps</p></li></ul></div><p>We already learned in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Traditional Building Blocks of Concurrency</em></span>, that using atomic variables and concurrent collections allows expressing lock-free programs. Why not just use atomic variables to express concurrently shared data? To better emphasizeÂ the need for STM, we will start by presenting a situation in which atomic variables prove inadequate.</p></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec46"></a>The trouble with atomic variables</h2></div></div><hr /></div><p>Atomic variables from <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Traditional Building Blocks of Concurrency</em></span>, are one of the fundamental synchronization mechanisms. We already know that volatile variables, introduced in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Concurrency on the JVM and the Java Memory Model</em></span>, allow race conditions, in which the program correctness is subject to the precise execution schedule of different threads. Atomic variables can ensure that no thread concurrently modifies the variable between a read and a write operation. At the same time, atomic variables reduce the risk of deadlocks. Regardless of their advantages, there are situations when using atomic variables is not satisfactory.</p><p>In <a class="link" href="#" linkend="ch06">Chapter 6</a>, <span class="emphasis"><em>Concurrent Programming with Reactive Extensions</em></span>, we implemented a minimalistic web browser using the <span class="strong"><strong>Rx</strong></span> framework. Surfing around the Web is great, but we would like to have some additional features in our browser. For example, we would like to maintain the browser's history--the list of URLs that were previously visited. We decide to keep the list of URLs in the Scala <code class="literal">List[String]</code> collection. Additionally, we decide to track the total character length of all the URLs. If we want to copy the URL strings into an array, this information allows us to quickly allocate an array of an appropriate size.</p><p>Different parts of our browser execute asynchronously, so we need to synchronize access to this mutable state. We can keep the list of URLs and their total character length in private mutable fields and use the <code class="literal">synchronized</code> statement to access them. However, having seen the culprits of the <code class="literal">synchronized</code> statement in earlier chapters, we decide to avoid locks. Instead, we will use atomic variables. We will store the list of URLs and their total character length in two atomic variables, that are <code class="literal">urls</code> and <code class="literal">clen</code>:</p><pre class="programlisting">import java.util.concurrent.atomic._
val urls = new AtomicReference[List[String]](Nil)
val clen = new AtomicInteger(0)
</pre><p>Whenever the browser opens URL, we need to update these atomic variables. To do this more easily, we define a helper method called <code class="literal">addUrl</code>:</p><pre class="programlisting">import scala.annotation.tailrec
def addUrl(url: String): Unit = {
  @tailrec def append(): Unit = {
    val oldUrls = urls.get
    val newUrls = url :: oldUrls
    if (!urls.compareAndSet(oldUrls, newUrls)) append()
  }
  append()
  clen.addAndGet(url.length + 1)
}
</pre><p>As we learned in the introductory chapters, we need to use atomic operations on atomic variables to ensure that their values consistently change from one state to another. In the previous code snippet, we use the <code class="literal">compareAndSet</code> operation to atomically replace the old list of URLs called <code class="literal">oldUrls</code> with the updated version <code class="literal">newUrls</code>. As discussed at length in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Traditional Building Blocks of Concurrency</em></span>, the <code class="literal">compareAndSet</code> operation can fail when two threads call it simultaneously on the same atomic variable. For this reason, we define a nested, tail-recursive method, <code class="literal">append</code>, whichÂ calls the <code class="literal">compareAndSet</code> method and restarts if the <code class="literal">compareAndSet</code> method fails. Updating the <code class="literal">clen</code> field is easier. We just call the atomic <code class="literal">addAndGet</code> method defined on atomic integers.</p><p>Other parts of the web browser can use the <code class="literal">urls</code> and <code class="literal">clen</code> variables to render the browsing history, dump it to a <code class="literal">log</code> file or to export browser data, in case our users decide they like Firefox better. For convenience, we define a <code class="literal">getUrlArray</code> auxiliary method that returns a character array in which the URLs are separated with a newline character. The <code class="literal">clen</code> field is a quick way to get the required size of the array. We call the <code class="literal">get</code> method to read the value of the <code class="literal">clen</code> field and allocate the array. We then call <code class="literal">get</code> to read the current list of URLs, append the newline character to each URL, flatten the list of strings into a single list, zip the characters with their indices, and store them into the array:</p><pre class="programlisting">def getUrlArray(): Array[Char] = {
  val array = new Array[Char](clen.get)
  val urlList = urls.get
  for ((ch, i) &lt;- urlList.map(_ + "\n").flatten.zipWithIndex) {
    array(i) = ch
  }
  array
}
</pre><p>To test these methods, we can simulate user interaction with two asynchronous computations. The first asynchronous computation calls the <code class="literal">getUrlArray</code> method to dump the browsing history to a file. The second asynchronous computation visits three separate URLs by calling the <code class="literal">addURL</code> method three times, and then prints the <code class="literal">"done browsing"</code> string to the standard output:</p><pre class="programlisting">import scala.concurrent._
import ExecutionContext.Implicits.global
object AtomicHistoryBad extends App {
  Future {
    try { log(s"sending: ${getUrlArray().mkString}") }
    catch { case e: Exception =&gt; log(s"Houston... $e!") }
  }
  Future {
    addUrl("http://scala-lang.org")
    addUrl("https://github.com/scala/scala")
    addUrl("http://www.scala-lang.org/api")
    log("done browsing")
  }
  Thread.sleep(1000)
}
</pre><p>Running this program several times reveals a bug. The program sometimes mysteriously crashes with an <code class="literal">ArrayIndexOutOfBoundsException</code> exception. By analyzing the <code class="literal">getUrlArray</code> method, we find the cause to the bug. This bug occurs when the retrieved value of the <code class="literal">clen</code> field is not equal to the length of the list. The <code class="literal">getUrlArray</code> method first reads the <code class="literal">clen</code> atomic variable, and later reads the list of the URLs from the <code class="literal">urls</code> atomic variable. Between these two reads, the first thread modifies the <code class="literal">urls</code> variable by adding an additional URL string. By the time <code class="literal">getUrlArray</code> reads the <code class="literal">urls</code> variable, the total character length becomes longer than the allocated array, and we eventually get an exception.</p><p>This example illustrates an important disadvantage of atomic variables. Although specific atomic operations are themselves atomic and occur at a single point in time, invoking multiple atomic operations is typically not atomic. When multiple threads simultaneously execute multiple atomic operations, the operations might interleave in unforeseen ways and lead to the same kind of race conditions that result from using volatile variables. Note that swapping the updates to the <code class="literal">clen</code> and <code class="literal">urls</code> variables does not solve the problem. Although there are other ways to ensure atomicity in our example, they are not immediately obvious.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note72"></a>Note</h3><p>Reading multiple atomic variables is not an atomic operation and it can observe the program data in an inconsistent state.</p></div><p>When all threads in the program observe that an operation occurs at the same, single point in time, we can say that the operation is <span class="emphasis"><em>linearizable</em></span>. The point in time at which the operation occurs is called a <span class="strong"><strong>linearization point</strong></span>. The <code class="literal">compareAndSet</code> and <code class="literal">addAndGet</code> operations are inherently linearizable operations. They execute atomically, usually as a single processor instruction and at a single point in time, from the perspective of all the threads. The <code class="literal">append</code> nested method in the previous example is also linearizable. Its linearization point is a successful <code class="literal">compareAndSet</code> operation, because that is the only place where <code class="literal">append</code> modifies the program state. On the other hand, the <code class="literal">addUrl</code> and <code class="literal">getUrlArray</code> methods are not linearizable. They contain no single atomic operation that modifies or reads the state of the program. The <code class="literal">addUrl</code> method modifies the program state twice. First, it calls the <code class="literal">append</code> method and then it calls the <code class="literal">addAndGet</code> method. Similarly, the <code class="literal">getUrlArray</code> method reads the program state with two separate atomic <code class="literal">get</code> operations. This is a commonly misunderstood point when using atomic variables, and we say that atomic variables do not compose into larger programs.</p><p>We can fix our example by removing the <code class="literal">clen</code> atomic variable, and computing the required array length after reading the <code class="literal">urls</code> variable once. Similarly, we can use a single atomic reference to store a tuple with the URL list and the size of that list. Both approaches would make the <code class="literal">addUrl</code> and <code class="literal">getUrlArray</code> methods linearizable.</p><p>Concurrent programming experts have proven that it is possible to express any program state using atomic variables, and arbitrarily modify this state with linearizable operations. In practice, implementing such linearizable operations efficiently can be quite challenging. It is generally hard to implement arbitrary linearizable operations correctly, and it is even harder to implement them efficiently.</p><p>Unlike atomic variables, multiple <code class="literal">synchronized</code> statements can be used together more easily. We can modify multiple fields of an object when we use the <code class="literal">synchronized</code> statement, and we can even nest multiple <code class="literal">synchronized</code> statements. We are thus left with a dilemma. We can use atomic variables and risk race conditions when composing larger programs, or we can revert to using the <code class="literal">synchronized</code> statement, but risk deadlocks. Luckily, STM is a technology that offers the best of both worlds; it allows you to compose simple atomic operations into more complex atomic operations, without the risk of deadlocks.</p></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec47"></a>Using Software Transactional Memory</h2></div></div><hr /></div><p>In this section, we will study the basics of using STM. Historically, multiple STM implementations were introduced for Scala and the JVM platform. The particular STM implementation described in this chapter is called <span class="strong"><strong>ScalaSTM</strong></span>. There are two reasons that ScalaSTM is our STM of choice. First, ScalaSTM was authored by a group of STM experts whoÂ agreed on a standardized set of APIs and features. Future STM implementations for Scala are strongly encouraged to implement these APIs. Second, the ScalaSTM API is designed for multiple STM implementations, and comes with an efficient default implementation. Different STM implementations can be chosen when the program starts. Users can write applications using a standardized API, and seamlessly switch to a different STM implementation later.</p><p>The <code class="literal">atomic</code> statement is a fundamental abstraction at the core of every STM. When the program executes a block of code marked with theÂ <code class="literal">atomic</code> symbol, it starts a memory transaction, a sequence of reads and writes operations to memory thatÂ occur atomically for other threads in the program. The <code class="literal">atomic</code> statement is similar to the <code class="literal">synchronized</code> statement, and ensures that a block of code executes in isolation, without the interference of other threads, thus avoiding race conditions. Unlike the <code class="literal">synchronized</code> statement, the <code class="literal">atomic</code> statement does not cause deadlocks.</p><p>The following methods, <code class="literal">swap</code> and <code class="literal">inc</code>, show how to use the <code class="literal">atomic</code> statement on a high level. The <code class="literal">swap</code> method atomically exchanges the contents of two memory locations, <code class="literal">a</code> and <code class="literal">b</code>. Between the time that a thread reads the memory location <code class="literal">a</code> (or <code class="literal">b</code>) and the time that the <code class="literal">atomic</code> statement ends, no other thread can effectively modify the value at location <code class="literal">a</code> (or <code class="literal">b</code>). Similarly, the <code class="literal">inc</code> method atomically increments the integer value at the memory location <code class="literal">a</code>. When a thread, which calls the <code class="literal">inc</code> method, reads the value of <code class="literal">a</code> in the <code class="literal">atomic</code> statement, no other thread can change the value of the memory location <code class="literal">a</code> until the <code class="literal">atomic</code> statement ends:</p><pre class="programlisting">def swap() = atomic { // not actual code
  val tmp = a
  a = b
  b = tmp
}
def inc() = atomic { a = a + 1 }
</pre><p>The ways in which an STM implements deadlock-freedom and ensures that no two threads simultaneously modify the same memory locations are quite complex. In most STM implementations, the <code class="literal">atomic</code> statement maintains a log of read and write operations. Every time a memory location is read during a memory transaction, the corresponding memory address is added to the log. Similarly, whenever a memory location is written during a memory transaction, the memory address and the proposed value are written to the log. Once the execution reaches the end of the <code class="literal">atomic</code> block, all the writes from the transaction log are written to the memory. When this happens, we say that the transaction is committed. On the other hand, during the transaction, the STM might detect that another concurrent transaction performed by some other thread is concurrently reading or writing the same memory location. This situation is called a <span class="strong"><strong>transactional conflict</strong></span>. When a transactional conflict occurs, one or both of the transactions are cancelled, and re-executed serially, one after another. We say that the STM <span class="emphasis"><em>rolls back</em></span> these transactions. Such STMs are called <span class="strong"><strong>optimistic</strong></span>. Optimistic STMs try to execute a transaction under the assumption that it will succeed, and roll back when they detect a conflict. When we say that a transaction is completed, we mean that it was either committed or rolled back, and re-executed.</p><p>To illustrate how a memory transaction works, we consider the scenario in which two threads, <span class="strong"><strong>T1</strong></span> and <span class="strong"><strong>T2</strong></span>, simultaneously call the <code class="literal">swap</code> and <code class="literal">inc</code> methods. Since both the <code class="literal">atomic</code> statements in these methods modify the memory location <code class="literal">a</code>, the execution results in a runtime transactional conflict. During the execution of the program, the STM detects that the entries in the transactional logs overlap: the transaction associated with the <code class="literal">swap</code> method has both memory locations <code class="literal">a</code> and <code class="literal">b</code> in its read and write sets, while the <code class="literal">inc</code> method has <code class="literal">a</code> in its read and write sets. This indicates a potential conflict. Both the transactions can be rolled back, and then executed serially one after another, as shown in the following diagram:</p><div class="mediaobject"><img src="graphics/image_07_001.jpg" /></div><p>We will not delve more deeply into the internals of the ScalaSTM implementation, as this is beyond the scope of this book. Instead, we will focus on how to use ScalaSTM to easily write concurrent applications. Where reasonable, we hint at some implementation details to better understand the reasons behind the ScalaSTM semantics.</p><p>In some STMs, the <code class="literal">atomic</code> statement tracks all the reads and writes to the memory. ScalaSTM only tracks specially marked memory locations within transactions. There are several reasons for this. First, an STM cannot ensure safety if some parts of the program access memory locations outside the <code class="literal">atomic</code> statements, while other parts access the same memory locations inside the <code class="literal">atomic</code> statements. ScalaSTM avoids accidental uses outside transactions by explicitly marking the memory locations that can only be used in transactions. Second, STM frameworks for the JVM need to use post-compilation or bytecode introspection in order to accurately capture all the reads and writes. ScalaSTM is a library-only STM implementation, so it cannot analyze and transform the program in the same way a compiler can.</p><p>In ScalaSTM, the effects of the <code class="literal">atomic</code> statement are limited to special objects called <span class="strong"><strong>transactional references</strong></span>. Before showing how to use the <code class="literal">atomic</code> statement to perform memory transactions, we will study how to create transactional references.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec61"></a>Transactional references</h3></div></div></div><p>In this section, we will study how to declare transactional references. A transactional reference is a memory location that provides transactional read and write access to a single memory location. In ScalaSTM, transactional references to the values of type <code class="literal">T</code> are encapsulated within the objects of the <code class="literal">Red[T]</code> type:</p><p>Before we begin using STM in Scala, we need to add an external dependency to our project, since ScalaSTM is not a part of the Scala standard library:</p><pre class="programlisting">libraryDependencies += "org.scala-stm" %% "scala-stm" % "0.7"
</pre><p>To use the ScalaSTM <code class="literal">atomic</code> statement in a compilation unit, we import the contents of the <code class="literal">scala.concurrent.stm</code> package:</p><pre class="programlisting">import scala.concurrent.stm._
</pre><p>To instantiate a <code class="literal">Ref</code> object, we use the <code class="literal">Ref.apply</code> factory method on the <code class="literal">Ref</code> companion object. Let's rewrite our browser history example using transactional memory. We start by replacing atomic variables with transactional references. We pass the initial value of each transactional reference to the <code class="literal">Ref.apply</code> method:</p><pre class="programlisting">val urls = Ref[List[String]](Nil)
val clen = Ref(0)
</pre><p>Calling the <code class="literal">apply</code> method on a transactional reference returns its value, and calling the <code class="literal">update</code> method modifies it. However, we cannot call these methods from outside of a transaction. The <code class="literal">apply</code> and <code class="literal">update</code> methods take an implicit argument of type <code class="literal">InTxn</code> (which stands for <span class="emphasis"><em>in transaction</em></span>), which designates that a transaction is under way. Without the <code class="literal">InTxn</code> object, we cannot call the <code class="literal">apply</code> and <code class="literal">update</code> methods. This constraint protects us from accidentally circumventing the ScalaSTM safety mechanisms.</p><p>To read and modify transactional references, we must first start a transaction that provides the implicit <code class="literal">InTxn</code> object. We will study how to do this next.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec62"></a>Using the atomic statement</h3></div></div></div><p>After redefining the <code class="literal">urls</code> and <code class="literal">clen</code> variables as transactional references, we redefine the <code class="literal">addUrl</code> method. Instead of separately updating two atomic variables, we start a memory transaction with the <code class="literal">atomic</code> statement. In ScalaSTM, the <code class="literal">atomic</code> statement takes a block of type <code class="literal">InTxn =&gt; T</code>, where <code class="literal">InTxn</code> is the type of the previously mentioned transaction object, and <code class="literal">T</code> is the type of the return value of the transaction. Note that we can annotate the <code class="literal">InTxn</code> parameter with the <code class="literal">implicit</code> keyword:</p><pre class="programlisting">def addUrl(url: String): Unit = atomic { implicit txn =&gt;
  urls() = url :: urls()
  clen() = clen() + url.length + 1
}
</pre><p>The new definition of <code class="literal">addUrl</code> is surprisingly simple. It first reads the value of the <code class="literal">urls</code> list, prepends a new URL to the list, and assigns the updated list back to theÂ <code class="literal">urls</code> variable. Then, it reads the current value of the total character length <code class="literal">clen</code>, increments it by the length of the new URL, and assigns the new value back to <code class="literal">clen</code>. Note that the new definition of the <code class="literal">addUrl</code> method looks almost identical to a single-threaded implementation.</p><p>An important limitation of the <code class="literal">atomic</code> statement in ScalaSTM is that it does not track reads and writes to ordinary local variables and object fields. As we will see later, these are considered as arbitrary side effects, and are not allowed inside the transaction.</p><p>We reimplement the <code class="literal">getUrlArray</code> method in a similar fashion. We start by creating a transaction with the <code class="literal">atomic</code> statement. The value of the <code class="literal">clen</code> variable is used in order to allocate a character array of an appropriate size. We then read the <code class="literal">urls</code> list and assign its characters to the array in a <code class="literal">for</code> loop. Again, the implementation of theÂ <code class="literal">getUrlArray</code>Â method looks surprisingly similar to the corresponding single-threaded implementation:</p><pre class="programlisting">def getUrlArray(): Array[Char] = atomic { implicit txn =&gt;
  val array = new Array[Char](clen())
  for ((ch, i) &lt;- urls().map(_ + "\n").flatten.zipWithIndex) {
    array(i) = ch
  }
  array
}
</pre><p>This time, there is no danger of seeing inconsistent values of the <code class="literal">clen</code> and <code class="literal">urls</code> variables. When used in a transaction, the two values are always consistent with each other, as shown in the following program:</p><pre class="programlisting">object AtomicHistorySTM extends App {
  Future {
    addUrl("http://scala-lang.org")
    addUrl("https://github.com/scala/scala")
    addUrl("http://www.scala-lang.org/api")
    log("done browsing")
  }
  Thread.sleep(25)
  Future {
    try { log(s"sending: ${getUrlArray().mkString}") }
    catch { case e: Exception =&gt; log(s"Ayayay... $e") }
  }
  Thread.sleep(5000)
}
</pre><p>Note that we added the <code class="literal">sleep</code> statement in the main program, as this sets the timing of the two asynchronous computations to occur approximately at the same time. You can tweak the duration of the <code class="literal">sleep</code> statement in order to observe the various interleavings of the two asynchronous computations. Convince yourself with the fact that dumping the browsing history to the <code class="literal">log</code> file always observes some prefix of the three <code class="literal">addUrl</code> calls, and does not throw an exception.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip73"></a>Tip</h3><p>When encoding a complex program state, use multiple transactional references. To atomically perform multiple changes on the program state, use the <code class="literal">atomic</code> statement.</p></div><p>Having seen basic way of using the <code class="literal">atomic</code> statement with transactional references, we will proceed to show more advanced examples and study the STM semantics in more detail.</p></div></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec48"></a>Composing transactions</h2></div></div><hr /></div><p>When used correctly, transactional memory is a powerful tool for building concurrent applications that modify shared data. Nevertheless, no technology is a silver bullet, and neither is STM. In this section, we will study how to compose transactions in larger programs and learn how transactional memory interacts with other features of Scala. We investigate some of the caveats of STM, and go beyond transactional references and the <code class="literal">atomic</code> statement blocks to show how to use STM more effectively.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec63"></a>The interaction between transactions and side effects</h3></div></div></div><p>Previously, we learned that an STM may roll back and retry a transaction. An attentive reader might notice that retrying a transaction means re-executing its side effects. Here, the side effects are arbitrary reads and writes to regular <code class="literal">object</code> fields and variables.</p><p>Sometimes, side effects are not a problem. Transactional references cannot be modified outside a transaction, and inside a transaction their modifications are aborted when retrying. Still, the other kinds of side effect are not rolled back. Consider the following program:</p><pre class="programlisting">object CompositionSideEffects extends App {`
  val myValue = Ref(0)
  def inc() = atomic { implicit txn =&gt;
    log(s"Incrementing ${myValue()}")
    myValue() = myValue() + 1
  }
  Future { inc() }
  Future { inc() }
  Thread.sleep(5000)
}
</pre><p>The preceding program declares a <code class="literal">myValue</code> transactional reference, and an <code class="literal">inc</code> method that increments <code class="literal">myValue</code> inside an <code class="literal">atomic</code> block. The <code class="literal">inc</code> method also contains a <code class="literal">log</code> statement whichÂ prints the current value of theÂ <code class="literal">myValue</code> reference. The program asynchronously calls theÂ <code class="literal">inc</code>Â method twice. Upon executing this program, we get the following output:</p><pre class="programlisting">ForkJoinPool-1-worker-1: Incrementing 0
ForkJoinPool-1-worker-3: Incrementing 0
ForkJoinPool-1-worker-3: Incrementing 1
</pre><p>The two asynchronous computations call the <code class="literal">inc</code> method at the same time, and both start a transaction. One of the transactions adds the <code class="literal">myValue</code> reference to its read set, calls the <code class="literal">log</code> statement with the <code class="literal">0</code> value, and proceeds to increment the <code class="literal">myValue</code> reference by adding the <code class="literal">myValue</code> reference to its write set. In the meantime, the other transaction first logs the <code class="literal">0</code> value, then attempts to read <code class="literal">myValue</code> again, and detects that <code class="literal">myValue</code> is in a write set of another active transaction. The second transaction is rolled back, and retried after the first transaction commits. The second transaction reads theÂ <code class="literal">myValue</code> reference once more, prints <code class="literal">1</code>, and then increments <code class="literal">myValue</code>. The two transactions commit, but the side-effecting <code class="literal">log</code> call is executed three times as a result of the rollback.</p><p>It might not be harmful to execute a simple <code class="literal">log</code> statement multiple times, but repeating arbitrary side effects can easily break the correctness of a program. Avoiding side effects in transactions is a recommended practice.</p><p>Recall that an operation is idempotent if executing it multiple times has the same effect as executing it once, as discussed in <a class="link" href="#" linkend="ch06">Chapter 6</a>, <span class="emphasis"><em>Concurrent Programming with Reactive Extensions</em></span>. You might conclude that, if a side-effecting operation is idempotent, then it is safe to execute it in a transaction. After all, the worst thing that can happen is that the idempotent operation gets executed more than once, right? Unfortunately, this reasoning is flawed. After a transaction is rolled back and retried, the values of the transactional references might change. The second time a transaction is executed, the arguments to the idempotent operation might be different, or the idempotent operation might not be invoked at all. The safest way to avoid such situations is to avoid external side effects altogether.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip74"></a>Tip</h3><p>Avoid external side effects inside the transactions, as transactions can be re-executed multiple times.</p></div><p>In practice, we usually want to execute a side effect only if the transaction commits, that is, after we are sure that the changes to the transactional references are visible to other threads. To do this, we use the <code class="literal">Txn</code> singleton object, which can schedule multiple operations that execute after the transaction commits or rolls back.</p><p>After a rollback, these operations are removed, and potentially re-registered when retrying the transaction. Its methods can only be called from inside an active transaction. In the following code, we rewrite the <code class="literal">inc</code> method to call the <code class="literal">Txn</code> object's <code class="literal">afterCommit</code> method, and schedule the <code class="literal">log</code> statement to execute after the transaction commits:</p><pre class="programlisting">def inc() = atomic { implicit txn =&gt;
  val valueAtStart = myValue()
  Txn.afterCommit { _ =&gt;
    log(s"Incrementing $valueAtStart")
  }
  myValue() = myValue() + 1
}
</pre><p>Note that we read the <code class="literal">myValue</code> reference inside the transaction and assign the value to a local variable <code class="literal">valueAtStart</code>. The value of the <code class="literal">valueAtStart</code> local variable is later printed to the standard output. This is different from reading the <code class="literal">myValue</code> reference inside the <code class="literal">afterCommit</code> block:</p><pre class="programlisting">def inc() = atomic { implicit txn =&gt;
  Txn.afterCommit { _ =&gt;
    log(s"Incrementing ${myValue()}") // don't do this!
  }
  myValue() = myValue() + 1
}
</pre><p>Calling the last version of <code class="literal">inc</code> fails with an exception. Although the transactional context <code class="literal">txn</code> exists when the <code class="literal">afterCommit</code> method is called, the <code class="literal">afterCommit</code> block is executed later, after the transaction is already over and the <code class="literal">txn</code> object is no longer valid. It is illegal to read or modify transactional references outside a transaction. Before using it in an <code class="literal">afterCommit</code> block, we need to store the value of the transactional reference into a local variable in the transaction itself.</p><p>Why does accessing a transactional reference inside the <code class="literal">afterCommit</code> block only fail at runtime, when the transaction executes, instead of failing during compilation? The <code class="literal">afterCommit</code> method is in the <span class="strong"><strong>static scope</strong></span> of the transaction, or, in other words is statically nested within an <code class="literal">atomic</code> statement. For this reason, the compiler resolves the <code class="literal">txn</code> object of the transaction, and allows you to access the transactional references, such as <code class="literal">myValue</code>. However, the <code class="literal">afterCommit</code> block is not executed in the dynamic scope of the transaction. In other words, the <code class="literal">afterCommit</code> block is run <span class="emphasis"><em>after</em></span> the <code class="literal">atomic</code> block returns.</p><p>By contrast, accessing a transactional reference outside of the <code class="literal">atomic</code> block is not in the static scope of a transaction, so the compiler detects this and reports an error.</p><p>In general, the <code class="literal">InTxn</code> objects must not escape the transaction block. For example, it is not legal to start an asynchronous operation from within the transaction, and use the <code class="literal">InTxn</code> object to access transactional references.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip75"></a>Tip</h3><p>Only use the transactional context within the thread that started the transaction.</p></div><p>In some cases, we want to execute some side-effecting operations when a rollback occurs. For instance, we would like to log each rollback to track the contention in our program. This information can help us restructure the program and eliminate potential performance bottlenecks. To achieve this, we use the <code class="literal">afterRollback</code> method:</p><pre class="programlisting">  def inc() = atomic { implicit txn =&gt;
    Txn.afterRollback { _ =&gt;
      log(s"rollin' back")
    }
    myValue() = myValue() + 1
  }
</pre><p>Importantly, after a rollback, the transaction is no longer under way. Just as in the <code class="literal">afterCommit</code> blocks, it is illegal to access the transactional references in the <code class="literal">afterRollback</code> blocks.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip76"></a>Tip</h3><p>Use the <code class="literal">Txn</code> object's <code class="literal">afterCommit</code> and <code class="literal">afterRollback</code> methods to perform side-effecting operations in transactions without the danger of executing them multiple times.</p></div><p>Not all side-effecting operations inside the transactions are bad. As long as the side effects are confined to mutating objects that are created inside the transaction, we are free to use them. In fact, such side effects are sometimes necessary. To demonstrate this, let's define the <code class="literal">Node</code> class for a transactional linked list collection. A transactional list is a concurrent, thread-safe linked list that is modified using memory transactions. Similar to a functional cons list, represented by the <code class="literal">List</code> class in Scala, the transactional <code class="literal">Node</code> class contains two fields that we call <code class="literal">elem</code> and <code class="literal">next</code>. The <code class="literal">elem</code> field contains the value of the current node. To keep things simple, the <code class="literal">elem</code> field is a value field and can only contain integers.</p><p>The <code class="literal">next</code> field is a transactional reference containing the next node in the linked list. We can read and modify the <code class="literal">next</code> field only inside memory transactions:</p><pre class="programlisting">case class Node(elem: Int, next: Ref[Node])
</pre><p>We now define a <code class="literal">nodeToString</code> method, which takes a transactional linked list node <code class="literal">n</code>, and creates a <code class="literal">String</code> representation of the transactional list starting with the <code class="literal">n</code> node:</p><pre class="programlisting">def nodeToString(n: Node): String = atomic { implicit txn =&gt;
  val b = new StringBuilder
  var curr = n
  while (curr != null) {
    b ++= s"${curr.elem}, "
    curr = curr.next()
  }
  b.toString
}
</pre><p>In the preceding code snippet, we were careful to confine the side effects to objects that were created inside the transaction, in this case, the <code class="literal">StringBuilder</code> object <code class="literal">b</code>. Had we instantiated the <code class="literal">StringBuilder</code> object before the transaction started, the <code class="literal">nodeToString</code> method would not work correctly:</p><pre class="programlisting">def nodeToStringWrong(n: Node): String = {
  val b = new StringBuilder // very bad
  atomic { implicit txn =&gt;
    var curr = n
    while (curr != null) {
      b ++= s"${curr.elem}, "
      curr = curr.next()
    }
  }
  b.toString
}
</pre><p>If the transaction gets rolled back in the <code class="literal">nodeToStringWrong</code> example, the contents of the <code class="literal">StringBuilder</code> object are not cleared. The second time a transaction runs, it will modify the already existing, non-empty <code class="literal">StringBuilder</code> object and return a string representation that does not correspond to the state of the transactional list.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip77"></a>Tip</h3><p>When mutating an object inside a transaction, make sure that the object is created inside the transaction and that the reference to it does not escape the scope of the transaction.</p></div><p>Having seen how to manage side effects inside transactions, we now examine several special kinds of transactions and study how to compose smaller transactions into larger ones.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec64"></a>Single-operation transactions</h3></div></div></div><p>In some cases, we only want to read or modify a single transactional reference. It can be cumbersome to type the <code class="literal">atomic</code> keyword and the implicit <code class="literal">txn</code> argument just to read a single <code class="literal">Ref</code> object. To alleviate this, ScalaSTM defines single-operation transactions on transactional references. Single-operation transactions are executed by calling a single method on a <code class="literal">Ref</code> object. This method returns a <code class="literal">Ref.View</code> object, which has the same interface as a <code class="literal">Ref</code> object, but its methods can be called from outside a transaction. Each operation on a <code class="literal">Ref.View</code> object acts like a single-operation transaction.</p><p>Recall the <code class="literal">Node</code> class for transactional linked lists from the previous section, which stored integers in an <code class="literal">elem</code> field, and the reference to the next node in the transactional reference called <code class="literal">next</code>. Let's augment <code class="literal">Node</code> with two linked list methods. The <code class="literal">append</code> method takes a single <code class="literal">Node</code> argument <code class="literal">n</code>, and inserts <code class="literal">n</code> after the current node. The <code class="literal">nextNode</code> method returns the reference to the next node, or <code class="literal">null</code> if the current node is at the end of the list:</p><pre class="programlisting">case class Node(val elem: Int, val next: Ref[Node]) {
  def append(n: Node): Unit = atomic { implicit txn =&gt;
    val oldNext = next()
    next() = n
    n.next() = oldNext
  }
  def nextNode: Node = next.single()
}
</pre><p>The <code class="literal">nextNode</code> method does a single-operation transaction. It calls single on the <code class="literal">next</code> transactional reference, and then calls the <code class="literal">apply</code> method in order to obtain the value of the next node. This is equivalent to the following definition:</p><pre class="programlisting">def nextNode: Node = atomic { implicit txn =&gt;
  next()
}
</pre><p>We can use our transactional <code class="literal">Node</code> class to declare a linked list called <code class="literal">nodes</code>, initially containing values <code class="literal">1</code>, <code class="literal">4</code>, and <code class="literal">5</code>, and then concurrently modify it. We start two futures <code class="literal">f</code> and <code class="literal">g</code>, which call <code class="literal">append</code> to add nodes with the values <code class="literal">2</code> and <code class="literal">3</code>, respectively. After the futures complete, we call <code class="literal">nextNode</code> and print the value of the next node. The following code snippet will print the node with either the value <code class="literal">2</code> or <code class="literal">3</code>, depending on which future completes last:</p><pre class="programlisting">val nodes = Node(1, Ref(Node(4, Ref(Node(5, Ref(null))))))
val f = Future { nodes.append(Node(2, Ref(null))) }
val g = Future { nodes.append(Node(3, Ref(null))) }
for (_ &lt;- f; _ &lt;- g) log(s"Next node is: ${nodes.nextNode}")
</pre><p>We can also use the <code class="literal">single</code> method to invoke other transactional reference operations. In the following code snippet, we use the <code class="literal">transform</code> operation to define an <code class="literal">appendIfEnd</code> method on the <code class="literal">Node</code> class, which appends a node <code class="literal">n</code> after the current node only if the current node is followed by <code class="literal">null</code>:</p><pre class="programlisting">def appendIfEnd(n: Node) = next.single.transform {
  oldNext =&gt; if (oldNext == null) n else oldNext
}
</pre><p>The <code class="literal">transform</code> operation on a <code class="literal">Ref</code> object containing the values of type <code class="literal">T</code> takes a transformation function of type <code class="literal">T =&gt; T</code>. It atomically performs a read of the transactional reference, applies the transformation function to the current value, and writes the new value back. Other single-operation transactions include <code class="literal">update</code>, <code class="literal">compareAndSet</code>, and <code class="literal">swap</code> operations. We refer the readers to the online documentation to learn their precise semantics.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip78"></a>Tip</h3><p>Use single-operation transactions for single read, write, and CAS-like operations in order to avoid the syntactic boilerplate associated with the <code class="literal">atomic</code> blocks.</p></div><p>Single-operation transactions are convenience methods that are easier to type, and are possibly more efficient, depending on the underlying STM implementation. They can be useful, but as programs grow, we are more interested in building larger transactions from simple ones. We will investigate how to do this in the next section.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec65"></a>Nesting transactions</h3></div></div></div><p>Recall from <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Concurrency on the JVM and the Java Memory Model</em></span>, that a <code class="literal">synchronized</code> statement can be nested inside other <code class="literal">synchronized</code> statements. This property is essential when composing programs from multiple software modules. For example, a money transfer module in a banking system must call operations from a logging module to persist the transactions. Both the modules might internally use arbitrary sets of locks, without the knowledge of other modules. An unfortunate disadvantage of arbitrarily nested <code class="literal">synchronized</code> statements is that they allow the possibility of a deadlock.</p><p>Separate <code class="literal">atomic</code> statements can also nest arbitrarily. The motivation for this is the same as with the <code class="literal">synchronized</code> statement. A transaction inside a software module must be able to invoke operations inside other software modules, which themselves might start the transactions. Not having to know about the transactions inside an operation allows a better separation between different software components.</p><p>Let's illustrate this with a concrete example. Recall the <code class="literal">Node</code> class from the previous section, which was used for transactional linked lists. The <code class="literal">Node</code> class was somewhat low-level. We can only call theÂ <code class="literal">append</code>Â method to insert new nodes after the specified node, and call <code class="literal">nodeToString</code> on a specific node to convert its elements to a <code class="literal">String</code> object.</p><p>In this section, we define the transactional sorted list class, represented by the <code class="literal">TSortedList</code> class. This class stores integers in ascending order. It maintains a single transactional reference <code class="literal">head</code>, which points to the head of the linked list of the <code class="literal">Node</code> objects. We define the <code class="literal">toString</code> method on the <code class="literal">TSortedList</code> class to convert its contents into a textual representation. The <code class="literal">toString</code> method needs to read the transactional reference <code class="literal">head</code>, so it starts by creating a new transaction. After reading the value of the <code class="literal">head</code> transactional reference into a local value <code class="literal">headNode</code>, the <code class="literal">toString</code> method can reuse the <code class="literal">nodeToString</code> method that we defined earlier:</p><pre class="programlisting">class TSortedList {
  val head = Ref[Node](null)
  override def toString: String = atomic { implicit txn =&gt;
    val h = head()
    nodeToString(h)
  }
}
</pre><p>Recall that the <code class="literal">nodeToString</code> method starts another transaction to read the next references in each node. When the <code class="literal">toString</code> method calls <code class="literal">nodeToString</code>, the second transaction becomes <span class="emphasis"><em>nested</em></span> in the transaction started by <code class="literal">toString</code>. The <code class="literal">atomic</code> block in the <code class="literal">nodeToString</code> method does not start a new, separate transaction. Instead, the nested transaction becomes a part of the existing transaction. This has two important consequences. First, if the nested transaction fails, it is not rolled back to the start of its <code class="literal">atomic</code> block in the <code class="literal">nodeToString</code> method. Instead, it rolls back to the start of the <code class="literal">atomic</code> block in the <code class="literal">toString</code> method. We say that the start of the transaction is determined by the dynamic scope, rather than the static scope. Similarly, the nested transaction does not commit when it reaches the end of the <code class="literal">atomic</code> block in the <code class="literal">nodeToString</code> method. The changes induced by the nested transaction become visible when the initial transaction commits. We say that the scope of the transaction is always that of the top-level transaction.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note79"></a>Note</h3><p>Nested <code class="literal">atomic</code> blocks result in a transaction that starts when the top-level <code class="literal">atomic</code> block starts, and can commit only after the top-level <code class="literal">atomic</code> block completes. Similarly, rollbacks retry the transaction starting from the top-level <code class="literal">atomic</code> block.</p></div><p>We now study another example of using nested transactions. Atomically converting transactional sorted lists to their string representation is useful, but we also need to insert elements in the list. We define the <code class="literal">insert</code> method, which takes an integer and inserts it into a proper position in the transactional list.</p><p>Since <code class="literal">insert</code> can modify both the transactional reference <code class="literal">head</code> and the nodes in the list, it starts by creating a transaction. It then checks for two special cases. A list can be empty, in this case we set <code class="literal">head</code> to a new node containing <code class="literal">x</code>. Likewise, the <code class="literal">x</code> integer might be smaller than the first value in the list; in which case, the <code class="literal">head</code> reference is set to a new node containing <code class="literal">x</code>, and its <code class="literal">next</code> field is set to the previous value of the <code class="literal">head</code> reference. If neither of these conditions applies, we call a tail-recursive, nested method <code class="literal">insert</code> to process the remainder of the list:</p><pre class="programlisting">import scala.annotation.tailrec
def insert(x: Int): this.type = atomic { implicit txn =&gt;
  @tailrec def insert(n: Node): Unit = {
    if (n.next() == null || n.next().elem &gt; x)
      n.append(new Node(x, Ref(null)))
    else insert(n.next())
  }
  if (head() == null || head().elem &gt; x)
    head() = new Node(x, Ref(head()))
  else insert(head())
  this
}
</pre><p>The nested <code class="literal">insert</code> method traverses the linked list in order to find the correct position for the <code class="literal">x</code> integer. It takes the current node <code class="literal">n</code> and checks if the node is followed by <code class="literal">null</code>, indicating the end of the list, or if the next element is greater than <code class="literal">x</code>. In both cases, we call the <code class="literal">append</code> method on the node. If the node following <code class="literal">n</code> is not <code class="literal">null</code>, and its <code class="literal">elem</code> field is less than or equal to <code class="literal">x</code>, we call <code class="literal">insert</code> recursively on the next node.</p><p>Note that the tail-recursive, nested method <code class="literal">insert</code> uses the transactional context <code class="literal">txn</code> of the enclosing <code class="literal">atomic</code> block. We can also define a separate tail-recursive method <code class="literal">insert</code> outside the scope of the transaction. In this case, we need to encode the transactional context <code class="literal">txn</code> as a separate implicit parameter:</p><pre class="programlisting">@tailrec
final def insert(n: Node, x: Int)(implicit txn: InTxn): Unit = {
  if (n.next() == null || n.next().elem &gt; x)
    n.append(new Node(x, Ref(null)))
  else insert(n.next(), x)
}
</pre><p>Alternatively, we can omit the implicit <code class="literal">txn</code> transactional context parameter, but then we have to start a nested transaction inside the tail-recursive <code class="literal">insert</code> method. This might be slightly less efficient than the previous approach, but it is semantically equivalent:</p><pre class="programlisting">@tailrec
final def insert(n: Node, x: Int): Unit = atomic { implicit txn =&gt;
  if (n.next() == null || n.next().elem &gt; x)
    n.append(new Node(x, Ref(null)))
  else insert(n.next(), x)
}
</pre><p>We test our transactional sorted list with the following snippet. We instantiate an empty transactional sorted list and insert several integers concurrently from the asynchronous computations <code class="literal">f</code> and <code class="literal">g</code>. After both the corresponding futures complete execution, we print the contents of the sorted list:</p><pre class="programlisting">val sortedList = new TSortedList
val f = Future { sortedList.insert(1); sortedList.insert(4) }
val g = Future { sortedList.insert(2); sortedList.insert(3) }
for (_ &lt;- f; _ &lt;- g) log(s"sorted list - $sortedList")
</pre><p>Running the preceding snippet always outputs the elements <code class="literal">1</code>, <code class="literal">2</code>, <code class="literal">3</code>, and <code class="literal">4</code> in the same sorted order, regardless of the execution schedule of the futures. We created a thread-safe transactional sorted list class, and the implementation is almost identical to the corresponding sequential sorted list implementation. This example shows the true potential of STM. It allows you to create concurrent data structures and thread-safe data models without having to worry too much about concurrency.</p><p>There is one more aspect of transactions that we have not yet considered. What happens if a transaction fails due to an exception? For example, the tail-recursive <code class="literal">insert</code> method can get called with a <code class="literal">null</code> value instead of a valid <code class="literal">Node</code> reference. This results in throwing a <code class="literal">NullPointerException</code>, but how does it affect the transaction? We will explore the exception semantics of the transactions in the following section.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec66"></a>Transactions and exceptions</h3></div></div></div><p>From what we've learned about transactions so far, it is not clear what happens with a transaction if it throws an exception. An exception could roll back the transaction, or it could commit its changes. ScalaSTM does a rollback, by default, but this behavior can be overridden.</p><p>Let's assume that the clients of our transactional sorted list want to use it as a concurrent priority queue. A <span class="emphasis"><em>priority queue</em></span> is a collection that contains ordered elements, such as integers. An arbitrary element can be inserted into a priority queue using the <code class="literal">insert</code> method. At each point, we can retrieve the smallest element currently in the priority queue using the <code class="literal">head</code> method. The priority queue also allows you to remove the smallest element with the <code class="literal">pop</code> method.</p><p>The transactional sorted list is already sorted and supports element insertion with the <code class="literal">insert</code> method, however, once added, elements cannot be removed. To make our transactional sorted list usable as a priority queue, we define a <code class="literal">pop</code> method, which removes the first <code class="literal">n</code> elements from a transactional list <code class="literal">xs</code>. We start a transaction inside the <code class="literal">pop</code> method, and declare a local variable <code class="literal">left</code>, initializing it with the number of removed elements <code class="literal">n</code>. We then use a <code class="literal">while</code> loop to remove nodes from <code class="literal">head</code> and decrease theÂ <code class="literal">left</code> variable until it becomes 0:</p><pre class="programlisting">def pop(xs: TSortedList, n: Int): Unit = atomic { implicit txn =&gt;
  var left = n
  while (left &gt; 0) {
    xs.head() = xs.head().next()
    left -= 1
  }
}
</pre><p>To test the <code class="literal">pop</code> method, we declare a new transactional list <code class="literal">lst</code>, and insert integers <code class="literal">4</code>, <code class="literal">9</code>, <code class="literal">1</code>, and <code class="literal">16</code>. The list is sorted, so the integers appear in the list in the order <code class="literal">1</code>, <code class="literal">4</code>, <code class="literal">9</code>, and <code class="literal">16</code>:</p><pre class="programlisting">val lst = new TSortedList
lst.insert(4).insert(9).insert(1).insert(16)
</pre><p>Next, we start an asynchronous computation that removes the first two integers in the list by calling <code class="literal">pop</code>. After the asynchronous computation is successfully completed, we print the contents of the transactional list to the standard output:</p><pre class="programlisting">Future { pop(lst, 2) } foreach {
  case _ =&gt; log(s"removed 2 elements; list = $lst")
}
</pre><p>So far, so good. The <code class="literal">log</code> statement outputs the list with the elements <code class="literal">9</code> and <code class="literal">16</code>. We proceed by starting another asynchronous computation, which removes the first three elements from the transactional list:</p><pre class="programlisting">Future { pop(lst, 3) } onComplete {
  case Failure(t) =&gt; log(s"whoa $t; list = $lst")
}
</pre><p>However, when we call the <code class="literal">pop</code> method again, it throws a <code class="literal">NullPointerException</code>; there are only two elements left in the transactional list. As a result, the reference <code class="literal">head</code> is eventually assigned <code class="literal">null</code> during the transaction. When the <code class="literal">pop</code> method tries to call <code class="literal">next</code> on <code class="literal">null</code>, an exception is thrown.</p><p>In the <code class="literal">onComplete</code> callback, we output the name of the exception and the contents of the transactional list. It turns out that the transactional list still contains the elements <code class="literal">9</code> and <code class="literal">16</code>, although the <code class="literal">head</code> reference of the transactional list had been set to <code class="literal">null</code> in the transaction. When an exception is thrown, the effects of the transaction are reverted.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note80"></a>Note</h3><p>When an exception is thrown inside a transaction, the transaction is rolled back and the exception is rethrown at the point where the top-level <code class="literal">atomic</code> block started.</p></div><p>Importantly, the nested transactions are also rolled back. In the following code snippet, the nested <code class="literal">atomic</code> block in the <code class="literal">pop</code> method completes successfully, but its changes are not committed. Instead, the entire transaction is rolled back when the <code class="literal">sys.error</code> call throws a <code class="literal">RuntimeException</code> in the enclosing top-level <code class="literal">atomic</code> block:</p><pre class="programlisting">Future {
  atomic { implicit txn =&gt;
    pop(lst, 1)
    sys.error("")
  }
} onComplete {
  case Failure(t) =&gt; log(s"oops again $t - $lst")
}
</pre><p>Unlike ScalaSTM, some other STM implementations do not roll back transactions when an exception is thrown; instead, they commit the transaction. STM experts have not yet reached a consensus on what the exception semantics should be. ScalaSTM uses a hybrid approach. Most exceptions roll back the transaction, but Scala's <span class="strong"><strong>control exceptions</strong></span> are excluded from this rule. Control exceptions are exceptions that are used for control flow in Scala programs. They extend the <code class="literal">ControlThrowable</code> trait from the <code class="literal">scala.util.control</code> package, and are sometimes treated differently by the Scala compiler and runtime. When a control exception is thrown inside a transaction, ScalaSTM does not roll back the transaction. Instead, the transaction is committed.</p><p>Control exceptions are used to support the <code class="literal">break</code> statement in Scala, which is not a native language construct. The <code class="literal">break</code> statement throws a control exception, which is then caught by the enclosing breakable block. In the next example, we define a breakable block for the <code class="literal">break</code> statement and start a transaction that calls <code class="literal">pop</code> in a <code class="literal">for</code> loop with the values <code class="literal">1</code>, <code class="literal">2</code>, and <code class="literal">3</code>. After the first iteration, we break the loop. The example shows that the changes in the first <code class="literal">pop</code> statement are committed. The transactional list now contains only the element <code class="literal">16</code>:</p><pre class="programlisting">import scala.util.control.Breaks._
Future {
  breakable {
    atomic { implicit txn =&gt;
      for (n &lt;- List(1, 2, 3)) {
        pop(lst, n)
        break
      }
    }
  }
  log(s"after removing - $lst")
}
</pre><p>Furthermore, it is possible to override how a specific transaction handles exceptions by calling the <code class="literal">withControlFlowRecognizer</code> method on the atomic block. This method takes a partial function from <code class="literal">Throwable</code> to <code class="literal">Boolean</code>, and uses it to decide whether a particular exception is to be considered as a control exception or not. If the partial function is not defined for particular exception, the decision is deferred to the default control flow recognizer.</p><p>In the following example, the <code class="literal">atomic</code> block overrides the default control flow recognizer. For this specific transaction, subclasses of the <code class="literal">ControlThrowable</code> trait are considered as regular exceptions. The <code class="literal">pop</code> call removes the last element of the transactional list as part of this transaction, but when we call <code class="literal">break</code>; the transaction is rolled back. The <code class="literal">log</code> statement at the end of the asynchronous computation shows that the list still contains the number <code class="literal">16</code>:</p><pre class="programlisting">import scala.util.control._
Future {
  breakable {
    atomic.withControlFlowRecognizer {
      case c: ControlThrowable =&gt; false
    } { implicit txn =&gt;
      for (n &lt;- List(1, 2, 3)) {
        pop(lst, n)
        break
      }
    }
  }
  log(s"after removing - $lst")
}
</pre><p>Note that the exceptions thrown inside the transactions can also be intercepted using the <code class="literal">catch</code> statement. In this case, the effects of the nested transactions are aborted, and the execution proceeds fromÂ the point where the exception was caught. In the following example, we catch the exception thrown by the second <code class="literal">pop</code> call:</p><pre class="programlisting">val lst = new TSortedList
lst.insert(4).insert(9).insert(1).insert(16)
atomic { implicit txn =&gt;
  pop(lst, 2)
  log(s"lst = $lst")
  try { pop(lst, 3) }
  catch { case e: Exception =&gt; log(s"Houston... $e!") }
  pop(lst, 1)
}
log(s"result - $lst")
</pre><p>The second <code class="literal">pop</code> method call should not remove any elements from the list, so we expect to see the element <code class="literal">16</code> at the end. Running this code snippet results in the following output:</p><pre class="programlisting">
<span class="strong"><strong>run-main-26: lst = 9, 16,</strong></span>
<span class="strong"><strong>run-main-26: lst = 9, 16,</strong></span>
<span class="strong"><strong>run-main-26: Houston... java.lang.NullPointerException!</strong></span>
<span class="strong"><strong>run-main-26: result - 16,</strong></span>
</pre><p>Interestingly, the output reveals that the first <code class="literal">log</code> statement is invoked twice. The reason is that, when the exception is thrown the first time, both the nested and the top-level transactions are rolled back. This is an optimization in the ScalaSTM implementation, since it is more efficient to flatten the nested and the top-level transaction during the first execution attempt. Note that, after the transactional block is executed the second time, the exception from the nested transaction is correctly handled.</p><p>These examples are useful in understanding the semantics of exceptions inside the transactions. Still, the clients of our transactional sorted list want more than an exception when they call the <code class="literal">pop</code> method on an empty sorted list. In some cases, like the producer-consumer pattern from <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Traditional Building Blocks of Concurrency</em></span>, a thread has to wait and repeat the transaction when the sorted list becomes non-empty. This is called retrying, and is the topic of the next section.</p></div></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec49"></a>Retrying transactions</h2></div></div><hr /></div><p>In sequential computing, a single thread is responsible for executing the program. If a specific value is not available, the single thread is responsible for producing it. In concurrent programming, the situation is different. When a value is not available, some other thread, called a <span class="strong"><strong>producer</strong></span>, might eventually produce the value. The thread consuming the value, called a <span class="strong"><strong>consumer</strong></span>, can either block the execution until the value becomes available, or temporarily execute some other work before checking for the value again. We have seen various mechanisms for achieving this relationship, ranging from monitors and the <code class="literal">synchronized</code> statement from <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Concurrency on the JVM and the Java Memory Model</em></span>, concurrent queues from <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Traditional Building Blocks of Concurrency</em></span>; futures and promises in <a class="link" href="#" linkend="ch04">Chapter 4</a>, <span class="emphasis"><em>Asynchronous Programming with Futures and Promises</em></span>; to event-streams in <a class="link" href="#" linkend="ch06">Chapter 6</a>, <span class="emphasis"><em>Concurrent Programming with Reactive Extensions</em></span>.</p><p>Syntactically, the <code class="literal">atomic</code> statement best corresponds to the <code class="literal">synchronized</code> statement. Recall that the <code class="literal">synchronized</code> statement support the guarded block pattern, in which the thread acquires a monitor, checks for some condition, and then calls <code class="literal">wait</code> on the monitor. When some other thread fulfills this condition, it calls the <code class="literal">notify</code> method on the same monitor, indicating that the first thread should wake up and continue its work. Although sometimes fragile, this mechanism allows us to circumvent busy-waiting.</p><p>From what we have learned about STMs so far, monitors and the <code class="literal">notify</code> method have no direct counterpart in the <code class="literal">atomic</code> statement. Without them, busy-waiting is the only option when a transaction needs to wait for a specific condition to proceed. To illustrate this, let's consider the transactional sorted lists from the last section. We would like to augment the transactional sorted lists with the <code class="literal">headWait</code> methodÂ which takes a list and returns the first integer in the list if the list is non-empty. Otherwise, the execution should block until the list becomes non-empty:</p><pre class="programlisting">def headWait(lst: TSortedList): Int = atomic { implicit txn =&gt;
  while (lst.head() == null) {} // never do this
  lst.head().elem
}
</pre><p>The <code class="literal">headWait</code> method starts a transaction, and busy-waits until the <code class="literal">head</code> reference of the transactional list <code class="literal">lst</code> becomes different from <code class="literal">null</code>. To test this method, we create an empty transaction sorted list, and start an asynchronous computation that calls the <code class="literal">headWait</code> method. After one second, we start another asynchronous computation that adds the number <code class="literal">1</code> to the list. During the one-second delay, the first asynchronous computation repetitively busy-waits:</p><pre class="programlisting">object RetryHeadWaitBad extends App {
  val myList = new TSortedList
  Future {
    val headElem = headWait(myList)
    log(s"The first element is $headElem")
  }
  Thread.sleep(1000)
  Future { myList.insert(1) }
  Thread.sleep(1000)
}
</pre><p>The first time we ran this example, it completed successfully after one second and reported that the first element of the list is <code class="literal">1</code>. However, this example is likely to fail. ScalaSTM will eventually detect that there is a conflict between the transaction in the <code class="literal">headWait</code> method and the transaction in the <code class="literal">insert</code> method, and will serialize the two transactions. In the case where the STM chooses the <code class="literal">headWait</code> method to execute first, number <code class="literal">1</code> is never inserted into <code class="literal">myList</code> value. Effectively, this program ends up in a deadlock. This example illustrates that busy-waiting in a transaction is just as bad as busy-waiting inside a <code class="literal">synchronized</code> statement.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip81"></a>Tip</h3><p>Avoid long-running transactions whenever possible. Never execute an infinite loop inside a transaction, as it can cause deadlocks.</p></div><p>An STM is more than just support for executing isolated memory transactions. To fully replace monitors and the <code class="literal">synchronized</code> statement, an STM must provide an additional utility for transactions that block until a specific condition is fulfilled. ScalaSTM defines the <code class="literal">retry</code> statement for this purpose. When the execution inside the transaction reaches a <code class="literal">retry</code> statement, the transaction is rolled back to the enclosing top-level <code class="literal">atomic</code> block with a special exception, and the calling thread is blocked. After the rollback, the read set of the transaction is saved.</p><p>Values from the transactional references in the read set are the reason why the transaction decides to call the <code class="literal">retry</code> method. If and when some transactional reference in the read set changes its value from within another transaction, the blocked transaction can be retried.</p><p>We now reimplement the <code class="literal">headWait</code> method so that it calls the <code class="literal">retry</code> method if the <code class="literal">head</code> value of the transactional list is <code class="literal">null</code>, indicating that the list is empty:</p><pre class="programlisting">def headWait(lst: TSortedList): Int = atomic { implicit txn =&gt;
  if (lst.head() != null) lst.head().elem
  else retry
}
</pre><p>We rerun the complete program. Calling the <code class="literal">headWait</code> method is a potential blocking operation, so we need to use the <code class="literal">blocking</code> call inside the asynchronous computation. The transaction in <code class="literal">headWait</code> reads the transactional reference <code class="literal">head</code>, and puts it into the read set after calling the <code class="literal">retry</code> method. When the reference <code class="literal">head</code> later changes, the transaction is automatically retried:</p><pre class="programlisting">object RetryHeadWait extends App {
  val myList = new TSortedList
  Future {
    blocking {
      log(s"The first element is ${headWait(myList)}")
    }
  }
  Thread.sleep(1000)
  Future { myList.insert(1) }
  Thread.sleep(1000)
}
</pre><p>This time, the program runs as expected. The first asynchronous computation is suspended until the second asynchronous computation adds <code class="literal">1</code> to the list. This awakens the first asynchronous computation and repeats the transaction.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip82"></a>Tip</h3><p>Use the <code class="literal">retry</code> statement to block the transaction until a specific condition is fulfilled, and retry the transaction automatically once its read set changes.</p></div><p>In some cases, when a specific condition is not fulfilled and the transaction cannot proceed, we would like to retry a different transaction. Assume that there are many producer threads in the program, and a single consumer thread. To decrease contention between the producers, we decide to introduce two transactional sorted lists called <code class="literal">queue1</code> and <code class="literal">queue2</code>. To avoid creating contention by simultaneously accessing both lists, the consumer thread must check the contents of these transactional sorted lists in two separate transactions. The <code class="literal">orAtomic</code> construct allows you to do this.</p><p>The following snippet illustrates how to use <code class="literal">orAtomic</code> in this situation. We instantiate two empty transactional sorted lists:Â <code class="literal">queue1</code> and <code class="literal">queue2</code>. We then start an asynchronous computation that represents the consumer and starts a transaction that calls the <code class="literal">headWait</code> method on the <code class="literal">queue1</code> list. We call the <code class="literal">orAtomic</code> method after the first transaction. This specifies an alternative transaction if the first transaction calls <code class="literal">retry</code>. In the <code class="literal">orAtomic</code> block, we call the <code class="literal">headWait</code> method on the <code class="literal">queue2</code> list. When the first <code class="literal">atomic</code> block calls the <code class="literal">retry</code> method, the control is passed to the <code class="literal">orAtomic</code> block, and a different transaction starts.</p><p>Since both the transactional lists, <code class="literal">queue1</code> and <code class="literal">queue2</code>, are initially empty, the second transaction also calls the <code class="literal">retry</code> method, and the transaction chain is blocked until one of the transactional lists changes:</p><pre class="programlisting">val queue1 = new TSortedList
val queue2 = new TSortedList
val consumer = Future {
  blocking {
    atomic { implicit txn =&gt;
      log(s"probing queue1")
      log(s"got: ${headWait(queue1)}")
    } orAtomic { implicit txn =&gt;
      log(s"probing queue2")
      log(s"got: ${headWait(queue2)}")
    }
  }
}
</pre><p>We now simulate several producers that call the <code class="literal">insert</code> method 50 milliseconds later:</p><pre class="programlisting">Thread.sleep(50)
Future { queue2.insert(2) }
Thread.sleep(50)
Future { queue1.insert(1) }
Thread.sleep(2000)
</pre><p>The consumer first prints the <code class="literal">"probing queue1"</code> string, calls the <code class="literal">retry</code> method inside the <code class="literal">headWait</code> method, and proceeds to the next transaction. It prints theÂ <code class="literal">"probing queue2"</code> string in the same way and then blocks its execution. After the first producer computation inserts <code class="literal">2</code> into the second transactional list, the consumer retries the chain of transactions again. It attempts to execute the first transaction and prints theÂ <code class="literal">"probing queue1"</code> string again before finding that the <code class="literal">queue1</code> list is empty. It then prints theÂ <code class="literal">"probing queue2"</code> string and successfully outputs the element <code class="literal">2</code> from the <code class="literal">queue2</code> list.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec67"></a>Retrying with timeouts</h3></div></div></div><p>We have seen that it is useful to suspend a transaction until a specific condition gets fulfilled. In some cases, we want to prevent a transaction from being blocked forever. The <code class="literal">wait</code> method on the object monitors comes with an overload that takes the timeout argument. When the timeout elapses without a <code class="literal">notify</code> call from some other thread, an <code class="literal">InterruptedException</code> is thrown. The ScalaSTM <code class="literal">withRetryTimeout</code> method is a similar mechanism for handling timeouts.</p><p>In the following code snippet, we create a <code class="literal">message</code> transactional reference that initially contains an empty string. We then start an <code class="literal">atomic</code> block whose timeout is set to <code class="literal">1000</code> milliseconds. If the <code class="literal">message</code> transactional reference does not change its value within that time, the transaction fails by throwing an <code class="literal">InterruptedException</code>:</p><pre class="programlisting">val message = Ref("")
Future {
  blocking {
    atomic.withRetryTimeout(1000) { implicit txn =&gt;
      if (message() != "") log(s"got a message - ${message()}")
      else retry
    }
  }
}
Thread.sleep(1025)
message.single() = "Howdy!"
</pre><p>We deliberately set the timeout to <code class="literal">1025</code> milliseconds to create a race condition. This program will either print the <code class="literal">"Howdy!"</code> message or fail with an exception.</p><p>We use the <code class="literal">withRetryTimeout</code> method when timing out is an exceptional behavior. Shutting down the application is one example of such a behavior. We want to avoid having a blocked transaction that prevents the program from terminating. Another example is waiting for a network reply. If there is no reply after some duration of time, we want to fail the transaction.</p><p>In some cases, a timeout is a part of a normal program behavior. In this case, we wait for a specific amount of time for conditions relevant to the transaction to change. If they do, we roll back and retry the transaction, as before. If the specified amount of time elapses without any changes, the transaction should continue. In ScalaSTM, the method that does this is called <code class="literal">retryFor</code>. In the following code snippet, we rewrite the previous example using the <code class="literal">retryFor</code> method:</p><pre class="programlisting">Future {
  blocking {
    atomic { implicit txn =&gt;
      if (message() == "") {
        retryFor(1000)
        log(s"no message.")
      } else log(s"got a message - '${message()}'")
    }
  }
}
Thread.sleep(1025)
message.single() = "Howdy!"
</pre><p>This time, the transaction inside the asynchronous computation does not throw an exception. Instead, the transaction prints the <code class="literal">"no message."</code> string if a timeout occurs.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip83"></a>Tip</h3><p>When a timeout represents exceptional program behavior, use the <code class="literal">withRetryTimeout</code> method to set the timeout duration in the transaction. When the transaction proceeds normally after a timeout, use the <code class="literal">retryFor</code> method.</p></div><p>The different <code class="literal">retry</code> variants are the ScalaSTM powerful additions to the standard STM model. They are as expressive as the <code class="literal">wait</code> and <code class="literal">notify</code> calls, and much safer to use. Together with the <code class="literal">atomic</code> statement, they unleash the full potential of synchronization.</p></div></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec50"></a>Transactional collections</h2></div></div><hr /></div><p>In this section, we take a step away from transactional references, and study more powerful transactional constructs, called, transactional collections. While transactional references can only hold a single value at once, transactional collections can manipulate multiple values. In principle, the <code class="literal">atomic</code> statements and transactional references are sufficient to express any kind of transaction over shared data. However, ScalaSTM's transactional collections are deeply integrated with the STM. They can be used to express shared data operations more conveniently and execute the transactions more efficiently.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec68"></a>Transaction-local variables</h3></div></div></div><p>We have already seen that some transactions need to create a local mutable state that exists only during the execution of the transaction. Sometimes, we need to re-declare the same state over and over again for multiple transactions. In such cases, we would like to declare the same state once, and reuse it in multiple transactions. A construct that supports this in ScalaSTM is called a <span class="strong"><strong>transaction-local variable</strong></span>.</p><p>To declare a transaction-local variable, we instantiate an object of the <code class="literal">TxnLocal[T]</code> type, giving it an initial value of type <code class="literal">T</code>. In the following code, we instantiate a <code class="literal">myLog</code> transaction-local variable. We will use <code class="literal">myLog</code> inside the transactional sorted list operations to log the flow of different transactions:</p><pre class="programlisting">val myLog = TxnLocal("")
</pre><p>The value of the <code class="literal">myLog</code> transaction-local variable is seen separately by each transaction. When a transaction starts, the value of <code class="literal">myLog</code> is equal to an empty string, as specified when <code class="literal">myLog</code> was declared. When the transaction updates the value of theÂ <code class="literal">myLog</code> variable, this change is only visible to that specific transaction. Other transactions behave as if they have their own separate copies of <code class="literal">myLog</code> variable.</p><p>We now declare a <code class="literal">clearList</code> method that atomically removes all elements from the specified transactional sorted list. This method uses the <code class="literal">myLog</code> variable to log the elements that were removed:</p><pre class="programlisting">def clearList(lst: TSortedList): Unit = atomic { implicit txn =&gt;
  while (lst.head() != null) {
    myLog() = myLog() + "\nremoved " + lst.head().elem
    lst.head() = lst.head().next()
  }
}
</pre><p>Usually, we are not interested in the contents of the <code class="literal">myLog</code> variable. However, we might occasionally want to inspect the <code class="literal">myLog</code> variable for debugging purposes. Hence, we declare the <code class="literal">clearWithLog</code> method that clears the list and then returns the contents of <code class="literal">myLog</code>. We then call the <code class="literal">clearWithLog</code> method on a non-empty transactional list from two separate asynchronous computations. After both asynchronous computations complete execution, we output their logs:</p><pre class="programlisting">val myList = new TSortedList().insert(14).insert(22)
def clearWithLog(): String = atomic { implicit txn =&gt;
  clearList(myList)
  myLog()
}
val f = Future { clearWithLog() }
val g = Future { clearWithLog() }
for (h1 &lt;- f; h2 &lt;- g) log(s"Log for f: $h1\nLog for g: $h2")
</pre><p>Since the <code class="literal">clearList</code> operation is atomic, only one of the transactions can remove all the elements. The contents of the <code class="literal">myLog</code> object reflect this. Depending on the timing between the asynchronous computations, elements <code class="literal">14</code> and <code class="literal">22</code>Â both appear either in the log of the <code class="literal">f</code> future or in the log of the <code class="literal">g</code> future. This shows that each of the two transactions sees a separate duplicate of the <code class="literal">myLog</code> variable.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip84"></a>Tip</h3><p>Transaction-local variables are syntactically more lightweight than creating transactional references and passing them between different methods.</p></div><p>Transaction-local variables are used while logging or gathering statistics on the execution of the program. The <code class="literal">TxnLocal</code> constructor additionally allows you to specify the <code class="literal">afterCommit</code> and <code class="literal">afterRollback</code> callbacks, invoked on the transaction-local variable when the transaction commits or rolls back, respectively. We refer the reader to the online documentation to find out how to use them. To build more complex concurrent data models, we use transactional arrays and maps, which we will study in the next section.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec69"></a>Transactional arrays</h3></div></div></div><p>Transactional references are a handy way to encapsulate a transactional state, but they come with certain overheads. First, a <code class="literal">Ref</code> object is more heavyweight than a simple object reference and consumes more memory. Second, every access to a new <code class="literal">Ref</code> object needs to add an entry in the transaction's read set. When we are dealing with many <code class="literal">Ref</code> objects, these overheads can become substantial. Let's illustrate this with an example.</p><p>Assume that we are working in the marketing department of a company that does Scala consulting. We are asked to write a program that updates the content of the company website with the marketing information about the Scala 2.10 release. Naturally, we decide to use ScalaSTM for this task. The website consists of five separate pages, each represented with a string. We declare the contents of the website in a sequence called <code class="literal">pages</code>. We then assign the content of the pages to an array of transactional references. If some page changes later, we can update its transactional reference in a transaction:</p><pre class="programlisting">val pages: Seq[String] = Seq.fill(5)("Scala 2.10 is out, " * 7)
val website: Array[Ref[String]] = pages.map(Ref(_)).toArray
</pre><p>This solution is not satisfactory. We created a lot of transactional reference objects, and the definition of <code class="literal">website</code> is not easily understandable. Luckily, ScalaSTM has an alternative called a <span class="strong"><strong>transactional array</strong></span>. A transactional array, represented with the <code class="literal">TArray</code> class, is similar to an ordinary Scala array, but can be accessed only from within a transaction. Its modifications are only made visible to the other threads when a transaction commits. Semantically, a <code class="literal">TArray</code> class corresponds to an array of transactional references, but it is more memory-efficient and concise:</p><pre class="programlisting">val pages: Seq[String] = Seq.fill(5)("Scala 2.10 is out, " * 7)
val website: TArray[String] = TArray(pages)
</pre><p>Scala development proceeds at an amazing pace. Not long after Scala 2.10 wasÂ announced, the 2.11 release of Scala became available. The marketing team asks us to update the contents of the website. All occurrences of the <code class="literal">"2.10"</code> string should be replaced with the <code class="literal">"2.11"</code> string. We write a <code class="literal">replace</code> method that does this:</p><pre class="programlisting">def replace(p: String, s: String): Unit = atomic { implicit txn =&gt;
  for (i &lt;- 0 until website.length)
    website(i) = website(i).replace(p, s)
}
</pre><p>Using the <code class="literal">TArray</code> class is much nicer than storing transactional references in an array. Not only does it spare us from a parenthesis soup resulting from calling the <code class="literal">apply</code> operation on the transactional references in the array, but it also occupies less memory. This is because a single contiguous array object is created for the <code class="literal">TArray[T]</code> object, whereas an <code class="literal">Array[Ref[T]]</code> object requires many <code class="literal">Ref</code> objects, each of which has a memory overhead.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip85"></a>Tip</h3><p>Use the <code class="literal">TArray</code> class instead of arrays of transactional references to optimize memory usage and make programs more concise.</p></div><p>Let's test the <code class="literal">TArray</code> class and the <code class="literal">replace</code> method in a short program. We first define an additional method, <code class="literal">asString</code>, which concatenates the contents of all the website pages. We then replace all occurrences of the <code class="literal">2.10</code> string with the <code class="literal">2.11</code> string. To test whether <code class="literal">replace</code> works correctly, we concurrently replace all occurrences of the <code class="literal">out</code> word with <code class="literal">"released"</code>:</p><pre class="programlisting">def asString = atomic { implicit txn =&gt;
  var s: String = ""
  for (i &lt;- 0 until website.length)
    s += s"Page $i\n======\n${website(i)}\n\n"
  s
}
val f = Future { replace("2.10", "2.11") }
val g = Future { replace("out", "released") }
for (_ &lt;- f; _ &lt;- g) log(s"Document\n$asString")
</pre><p>The <code class="literal">asString</code> method captured all the entries in the transactional array. In effect, the <code class="literal">asString</code> method atomically produced a snapshot of the state of the <code class="literal">TArray</code> object. Alternatively, we could have copied the contents of <code class="literal">website</code> into another <code class="literal">TArray</code> object, instead of a string. In either case, computing the snapshot of a <code class="literal">TArray</code> object requires traversing all its entries, and can conflict with the transactions that modify only a subset of the <code class="literal">TArray</code> class.</p><p>Recall the transactional conflict example from the beginning of this chapter. A transaction with many reads and writes, as in the <code class="literal">asString</code> method, can be inefficient, because all the other transactions need to serialize with the <code class="literal">asString</code> method when a conflict occurs. When the array is large, this creates a scalability bottleneck. In the next section, we will examine another collection capable of producing atomic snapshots in a much more scalable manner, namely, the transactional maps.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec70"></a>Transactional maps</h3></div></div></div><p>Similar to transactional arrays, transactional maps avoid the need to store transactional reference objects inside a map. As a consequence, they reduce memory consumption, improve the transaction performance, and provide a more intuitive syntax. In ScalaSTM, transactional maps are represented with the <code class="literal">TMap</code> class.</p><p>ScalaSTM's <code class="literal">TMap</code> class has an additional advantage. It exposes a scalable, constant-time, atomic <code class="literal">snapshot</code> operation. The <code class="literal">snapshot</code> operation returns an immutable <code class="literal">Map</code> object with the contents of the <code class="literal">TMap</code> object at the time of the snapshot. Let's declare a transactional map, <code class="literal">alphabet</code>, which maps character strings to their position in the alphabet:</p><pre class="programlisting">val alphabet = TMap("a" -&gt; 1, "B" -&gt; 2, "C" -&gt; 3)
</pre><p>We are unsatisfied with the fact that the letter <code class="literal">A</code> is in lowercase. We start a transaction that atomically replaces the lowercase letter <code class="literal">a</code> with the uppercase letter <code class="literal">A</code>. Simultaneously, we start another asynchronous computation that calls the <code class="literal">snapshot</code> operation on the <code class="literal">alphabet</code> map. We tune the timing of the second asynchronous computation so that it creates a race condition with the first transaction:</p><pre class="programlisting">Future {
  atomic { implicit txn =&gt;
    alphabet("A") = 1
    alphabet.remove("a")
  }
}
Thread.sleep(23)
Future {
  val snap = alphabet.single.snapshot
  log(s"atomic snapshot: $snap")
}
Thread.sleep(2000)
</pre><p>In this example, the <code class="literal">snapshot</code> operation cannot interleave with the two updates in the <code class="literal">atomic</code> block. We can run the program several times to convince ourselves of this. The second asynchronous computation prints either the map with the lowercase letter <code class="literal">a</code>, or the map with the uppercase letter <code class="literal">A</code>, but it can never output a map with both the lowercase and the uppercase occurrence of the letter <code class="literal">A</code>.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip86"></a>Tip</h3><p>Use <code class="literal">TMap</code>Â (instead of maps of transactional references) to optimize memory usage, make programs more concise, and efficiently retrieve atomic snapshots.</p></div></div></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec51"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we learned how STM works and how to apply it in concurrent programs. We saw the advantages of using STM's transactional references and <code class="literal">atomic</code> blocks over the <code class="literal">synchronized</code> statements, and investigated their interaction with side effects. We studied the semantics of exception handling inside transactions and learned how to retry and conditionally re-execute transactions. Finally, we learned about transactional collections, which allow us to encode shared program data more efficiently.</p><p>These features together enable a concurrent programming model in which the programmer can focus on expressing the meaning of the program, without having to worry about handling lock objects, or avoiding deadlocks and race conditions. This is especially important when it comes to modularity. It is hard or near impossible to reason about deadlocks or race conditions in the presence of separate software components. STM exists to liberate the programmer from such concerns, and is essential when composing large concurrent programs from simpler modules.</p><p>These advantages come with a cost, however, as using an STM for data access is slower than using locks and the <code class="literal">synchronized</code> statement. For many applications, the performance penalty of using an STM is acceptable. When it is not, we need to revert to simpler primitives, such as locks, atomic variables, and concurrent data structures.</p><p>To learn more about STMs, we recommend reading the related chapter in the book <span class="emphasis"><em>The Art of Multiprocessor Programming</em></span>, <span class="emphasis"><em>Maurice Herlihy and Nir Shavit</em></span>, <span class="emphasis"><em>Morgan Kauffman</em></span>. There are many different STM implementations in the wild, and you will need to study various research articles to obtain an in-depth understanding of STMs. An extensive list of STM research literature is available at <a class="ulink" href="http://research.cs.wisc.edu/trans-memory/biblio/index.html" target="_blank">http://research.cs.wisc.edu/trans-memory/biblio/index.html</a>. To learn more about the specifics of ScalaSTM, consider reading the doctoral thesis entitled <span class="emphasis"><em>Composable Operations on High-Performance Concurrent Collections</em></span>, <span class="emphasis"><em>Nathan G. Bronson</em></span>.</p><p>In the next chapter, we will study the actor programming model, which takes a different approach to achieving memory consistency. As we will see, separate computations never access each other's regions of memory in the actor model, and communicate mainly by exchanging messages.</p></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec52"></a>Exercises</h2></div></div><hr /></div><p>In the following exercises, you will use ScalaSTM to implement various transactional programming abstractions. In most cases, their implementation will closely resemble a sequential implementation, while usingÂ transactions. In some cases, you might need to consult external literature or ScalaSTM documentation to correctly solve the exercise.</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Implement the transactional pair abstraction, represented with the <code class="literal">TPair</code> class:
</p><pre class="programlisting">        class TPair[P, Q](pinit: P, qinit: Q) {
          def first(implicit txn: InTxn): P = ???
          def first_=(x: P)(implicit txn: InTxn): P = ???
          def second(implicit txn: InTxn): Q = ???
          def second_=(x: Q)(implicit txn: InTxn): Q = ???
          def swap()(implicit e: P =:= Q, txn: InTxn): Unit = ???
        }
</pre><p>
</p><p>In addition to getters and setters for the two fields, the transactional pair defines the <code class="literal">swap</code> method that swaps the fields, and can only be called if types <code class="literal">P</code> and <code class="literal">Q</code> are the same.</p></li><li><p>Use ScalaSTM to implement the mutable location abstraction from Haskell, represented with the <code class="literal">MVar</code> class:
</p><pre class="programlisting">        class MVar[T] {
          def put(x: T)(implicit txn: InTxn): Unit = ???
          def take()(implicit txn: InTxn): T = ???
        }
</pre><p>
</p><p>An <code class="literal">MVar</code> object can be either full or empty. Calling <code class="literal">put</code> on a full <code class="literal">MVar</code> object blocks until the <code class="literal">MVar</code> object becomes empty, and adds an element. Similarly, calling <code class="literal">take</code> on an empty <code class="literal">MVar</code> object blocks until the <code class="literal">MVar</code> object becomes full, and removes the element. Now, implement a method called <code class="literal">swap</code>, which takes two <code class="literal">MVar</code> objects and swaps their values:</p><p>
</p><pre class="programlisting">        def swap[T](a: MVar[T], b: MVar[T])(implicit txn: InTxn)
</pre><p>
</p><p>Contrast the <code class="literal">MVar</code> class with the <code class="literal">SyncVar</code> class from <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Concurrency on the JVM and the Java Memory Model</em></span>. Is it possible to implement the <code class="literal">swap</code> method for <code class="literal">SyncVar</code> objects without modifying the internal implementation of the SyncVar class?</p></li><li><p>Implement the <code class="literal">atomicRollbackCount</code> method, which is used to track how many times a transaction was rolled back before it completed successfully:
</p><pre class="programlisting">        def atomicRollbackCount[T](block: InTxn =&gt; T): (T, Int)
</pre></li><li><p>Implement the <code class="literal">atomicWithRetryMax</code> method, which is used to start a transaction that can be retried at most <code class="literal">n</code> times:
</p><pre class="programlisting">       def atomicWithRetryMax[T](n: Int)(block: InTxn =&gt; T): T
</pre><p>
</p><p>Reaching the maximum number of retries throws an exception.</p><p>
</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip87"></a>Tip</h3><p>Â  Â  Â  Â  Use the <code class="literal">Txn</code> object.</p></div></li><li><p>Implement a transactional <span class="strong"><strong>First In First Out</strong></span> (<span class="strong"><strong>FIFO</strong></span>) queue, represented with the <code class="literal">TQueue</code> class:
</p><pre class="programlisting">        class TQueue[T] {
          def enqueue(x: T)(implicit txn: InTxn): Unit = ???
          def dequeue()(implicit txn: InTxn): T = ???
</pre><p>
</p><p>The <code class="literal">TQueue</code> class has similar semantics as <code class="literal">scala.collection.mutable.Queue</code>, but calling <code class="literal">dequeue</code> on an empty queue blocks until a value becomes available.</p></li><li><p>Use ScalaSTM to implement a thread-safe <code class="literal">TArrayBuffer</code> class, which extends the <code class="literal">scala.collection.mutable.Buffer</code> interface.</p></li><li><p>The <code class="literal">TSortedList</code> class described in this chapter is always sorted, but accessing the last element requires traversing the entire list, and can be slow. An AVL tree can be used to address this problem. There are numerous descriptions of AVL trees available online. Use ScalaSTM to implement the thread-safe transactional sorted set as an AVL tree:
</p><pre class="programlisting">        class TSortedSet[T] {
          def add(x: T)(implicit txn: InTxn): Unit = ???
          def remove(x: T)(implicit txn: InTxn): Boolean = ???
          def apply(x: T)(implicit txn: InTxn): Boolean = ???
        }
</pre><p>
</p><p>The <code class="literal">TSortedSet</code> class has similar semantics as <code class="literal">scala.collection.mutable.Set</code>.</p></li><li><p>Use ScalaSTM to implement a banking system that tracks amounts of money on user accounts. Different threads can call the <code class="literal">send</code> method to transfer money from one account to another, the <code class="literal">deposit</code> and <code class="literal">withdraw</code> methods whichÂ deposit to or withdraw money from a specific account, respectively, and the <code class="literal">totalStock</code> method whichÂ returns the total amount of money currently deposited in the bank. Finally, implement theÂ <code class="literal">totalStockIn</code>Â method that returns the total amount of money currently deposited in the specified set of banks.</p></li><li><p>Implement the generic transactional priority queue class, represented with the type <code class="literal">TPriorityQueue</code>, used to sort elements. Then implement a method called <code class="literal">scheduleTask</code>, which adds a task to the priority queue. Each task has a priority level. A set of workers must wait for the queue to become non-empty, at which point they repetitively remove tasks with the highest priority, and execute them.</p></li><li><p>Implement a generic transactional directed graph data structure, whose nodes are represented with the <code class="literal">Node</code> class. Then implement a method <code class="literal">scheduleTask</code>, which adds a task to into the graph. Each task has the list of dependencies - other tasks in the graph that must be executed before it begins; and this list represents the directed edges in the graph. A set of workers repetitively queries the graph, and schedules tasks for execution. A task can only be executed after its dependencies are done executing.</p></li></ol></div></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch08"></a>ChapterÂ 8.Â   Actors  </h2></div></div></div><div class="blockquote"><table border="0" width="100%" cellspacing="0" cellpadding="0" class="blockquote" summary="Block quote"><tr><td width="10%" valign="top">Â </td><td width="80%" valign="top"><p><span class="emphasis"><em>"A distributed system is one in which the failure of a computer you didn't even know existed can render your own computer unusable."</em></span></p></td><td width="10%" valign="top">Â </td></tr><tr><td width="10%" valign="top">Â </td><td colspan="2" align="right" valign="top">--<span class="attribution"><span class="emphasis"><em>Leslie Lamport</em></span></span></td></tr></table></div><p>Throughout this book, we have concentrated on many different abstractions for concurrent programming. Most of these abstractions assume the presence of shared memory. Futures and promises, concurrent data structures, and software transactional memory, are best suited toÂ shared-memory systems. While the shared-memory assumption ensures that these facilities are efficient, it also limits them to applications running on a single computer. In this chapter, we consider a programming model that is equally applicable to a shared-memory machine or a distributed system, namely, the <span class="strong"><strong>actor model</strong></span>. In the actor model, the program is represented by a large number of entities that execute computations independently, and communicate by passing messages. These independent entities are called <span class="strong"><strong>actors</strong></span>.</p><p>The actor model aims to resolve issues associated with using shared memory, such as data races or synchronization, by eliminating the need for shared memory altogether. <span class="emphasis"><em>Mutable</em></span> state is confined within the boundaries of one actor, and is potentially modified when the actor receives a message. Messages received by the actor are handled serially, one after another. This ensures that the mutable state within the actor is never accessed concurrently. However, separate actors can process the received messages concurrently. In a typical actor-based program, the number of actors can be orders of magnitude greater than the number of processors. This is similar to the relationship between processors and threads in multi-threaded programs. The actor model implementation decides when to assign processor time to specific actors, to allow them to process messages.</p><p>The true advantage of the actor model becomes apparent when we start distributing the application across multiple computers. Implementing programs that span across multiple machines and devices that communicate through a computer network is called <span class="strong"><strong>distributed programming</strong></span>. The actor model allows you to write programs that run inside a single process, multiple processes on the same machine, or on multiple machines that are connected to a computer network. Creating actors and sending messages is oblivious to, and independent of, the location of the actor. In distributed programming, this is called <span class="strong"><strong>location transparency</strong></span>. Location transparency allows you to design distributed systems without having the knowledge about the relationships in the computer network.</p><p>In this chapter, we will use the Akka <code class="literal">actor</code> framework to learn about the actor concurrency model. Specifically, we cover the following topics:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Declaring actor classes and creating actor instances</p></li><li style="list-style-type: disc"><p>Modeling actor state and complex actor behaviors</p></li><li style="list-style-type: disc"><p>Manipulating the actor hierarchy and the lifecycle of an actor</p></li><li style="list-style-type: disc"><p>The different message-passing patterns used in actor communication</p></li><li style="list-style-type: disc"><p>Error recovery using the built-in actor supervision mechanism</p></li><li style="list-style-type: disc"><p>Using actors to transparently build concurrent and distributed programs</p></li></ul></div><p>We will start by studying the important concepts and terminology in the actor model, and learning the basics of the actor model in Akka.</p></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec53"></a>Working with actors</h2></div></div><hr /></div><p>In the actor programming model, the program is run by a set of concurrently executing entities called actors. Actor systems resemble human organizations, such as companies, governments, or other large institutions. To understand this similarity, we consider the example of a large software company.</p><p>In a software company such as Google, Microsoft, Amazon, or Typesafe, there are many goals that need to be achieved concurrently. Hundreds or thousands of employees work toward achieving these goals, and are usually organized in a hierarchical structure. Different employees work at different positions. A team leader makes important technical decisions for a specific project, a software engineer implements and maintains various parts of a software product, and a system administrator makes sure that the personal workstations, servers, and various equipment are functioning correctly. Many employees, such as the team leader, delegate their own tasks to other employees who are lower in the hierarchy than themselves. To be able to work and make decisions efficiently, employees use e-mails to communicate.</p><p>When an employee comes to work in the morning, he inspects his e-mail client and responds to the important messages. Sometimes, these messages contain work tasks that come from his boss, or requests from other employees. When an e-mail is important, the employee must compose the answer right away. While the employee is busy answering one e-mail, additional e-mails can arrive, and these e-mails are enqueued in his e-mail client. Only once the employee is done with one e-mail is he able to proceed to the next one.</p><p>In the preceding scenario, the workflow of the company is divided into a number of functional components. It turns out that these components closely correspond to different parts of an actor framework. We will now identify these similarities by defining the parts of an actor system, and relating them to their analogs in the software company.</p><p>An <span class="strong"><strong>actor system</strong></span> is a hierarchical group of actors that share common configuration options. An actor system is responsible for creating new actors, locating actors within the actor system, and logging important events. An actor system is an analog of the software company itself.</p><p>An <span class="strong"><strong>actor class</strong></span> is a template that describes a state internal to the actor, and how the actor processes the messages. Multiple actors can be created from the same actor class. An actor class is an analogy for a specific position within the company, such as a software engineer, a marketing manager, or a recruiter.</p><p>An <span class="strong"><strong>actor instance</strong></span> is an entity that exists at runtime and is capable of receiving messages. An actor instance might contain mutable state, and can send messages to other actor instances. The difference between an actor class and an actor instance directly corresponds to the relationship between a class and an object instance of that class in object-oriented programming. In the context of the software company example, an actor instance is analogous to a specific employee.</p><p>A <span class="strong"><strong>message</strong></span> is a unit of communication that actors use to communicate. In Akka, any object can be a message. Messages are analogous to e-mails sent within the company. When an actor sends a message, it does not wait until some other actor receives the message. Similarly, when an employee sends an e-mail, he does not wait until the e-mail is received or read by the other employees. Instead, he proceeds with his own work; an employee is too busy to wait. Multiple e-mails might be sent to the same person concurrently.</p><p>The <span class="strong"><strong>mailbox</strong></span> is a part of memory that is used to buffer messages, specific to each actor instance. This buffer is necessary, as an actor instance can process only a single message at a time. The mailbox corresponds to an e-mail client used by an employee. At any point, there might be multiple unread e-mails buffered in the e-mail client, but the employee can only read and respond to them one at a time.</p><p>An <span class="strong"><strong>actor reference</strong></span> is an object that allows you to send messages to a specific actor. This object hides information about the location of the actor from the programmer. An actor might run within separate processes or on different computers. The actor reference allows you to send a message to an actor irrespective of where the actor is running. From the software-company perspective, an actor reference corresponds to the e-mail address of a specific employee. The e-mail address allows us to send an e-mail to an employee, without knowing anything about the physical location of the employee. The employee might be in his office, on a business trip, or on vacation, but the e-mail will eventually reach him no matter where he goes.</p><p>A <span class="strong"><strong>dispatcher</strong></span> is a component that decides when actors are allowed to process messages, and lends them computational resources to do so. In Akka, every dispatcher is, at the same time, an execution context. The dispatcher ensures that actors with non-empty mailboxes eventually get run by a specific thread, and that these messages are handled serially. A dispatcher is best compared to the e-mail answering policy in the software company. Some employees, such as the technical support specialists, are expected to answer e-mails as soon as they arrive. Software engineers sometimes have more liberty-they can choose to fix several bugs before inspecting their e-mails. The janitor spends his day working around the office building, and only takes a look at his e-mail client in the morning.</p><p>To make these concepts more concrete, we start by creating a simple actor application. This is the topic of the following section, in which we learn how to create actor systems and actor instances.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec71"></a>Creating actor systems and actors</h3></div></div></div><p>When creating an object instance in an object-oriented language, we start by declaring a class, which can be reused by multiple object instances. We then specify arguments for the constructor of the object. Finally, we instantiate an object using the <code class="literal">new</code> keyword and obtain a reference to the object.</p><p>Creating an actor instance in Akka roughly follows the same steps as creating an object instance. First, we need to define an actor class, which defines the behavior of the actor. Then, we need to specify the configuration for a specific actor instance. Finally, we need to tell the actor system to instantiate the actor using the given configuration. The actor system then creates an actor instance and returns an actor reference to that instance. In this section, we will study these steps in more detail.</p><p>An actor class is used to specify the behavior of an actor. It describes how the actor responds to messages and communicates with other actors, encapsulates actor state, and defines the actor's startup and shutdown sequences. We declare a new actor class by extending the <code class="literal">Actor</code> trait from the <code class="literal">akka.actor</code> package. This trait comes with a single abstract method, <code class="literal">receive</code>. The <code class="literal">receive</code> method returns a partial function object of the type <code class="literal">PartialFunction[Any, Unit]</code>. This partial function is used when an actor receives a message of the <code class="literal">Any</code> type. If the partial function is not defined for the message, the message is discarded.</p><p>In addition to defining how an actor receives messages, the actor class encapsulates references to objects used by the actor. These objects comprise the actor's state. Throughout this chapter, we use Akka's <code class="literal">Logging</code> object to print to the standard output. In the following code, we declare a <code class="literal">HelloActor</code> actor class, which reacts to a <code class="literal">hello</code> message specified with the <code class="literal">hello</code> constructor argument. The <code class="literal">HelloActor</code> class contains a <code class="literal">Logging</code> object, <code class="literal">log</code>, as part of its state. The <code class="literal">Logging</code> object is created using the <code class="literal">context.system</code> reference to the current actor system, and the <code class="literal">this</code> reference to the current actor. The <code class="literal">HelloActor</code> class defines a partial function in the <code class="literal">receive</code> method, which determines if the message is equal to the <code class="literal">hello</code> string argument, or to some other object called <code class="literal">msg</code>.</p><p>When an actor defined by the <code class="literal">HelloActor</code> class receives a <code class="literal">hello</code> string message, it prints the message using the <code class="literal">Logging</code> object <code class="literal">log</code>. Otherwise, it prints that it received an unexpected message, and stops by calling the <code class="literal">context.stop</code> method on the actor reference <code class="literal">self</code>, which represents the current actor. This is shown in the following code snippet:</p><pre class="programlisting">import akka.actor._ 
import akka.event.Logging 
class HelloActor(val hello: String) extends Actor { 
  val log = Logging(context.system, this) 
  def receive = { 
    case `hello` =&gt; 
      log.info(s"Received a '$hello'... $hello!") 
    case msg     =&gt; 
      log.info(s"Unexpected message '$msg'") 
      context.stop(self) 
  } 
} 
</pre><p>Declaring an actor class does not create a running actor instance. Instead, the actor class serves as a blueprint for creating actor instances. The same actor class can be shared by many actor instances. To create an actor instance in Akka, we need to pass information about the actor class to the actor system. However, an actor class such as <code class="literal">HelloActor</code> is not sufficient for creating an actor instance; we also need to specify the <code class="literal">hello</code> argument. To bundle the information required for creating an actor instance, Akka uses objects called <span class="strong"><strong>actor configurations</strong></span>.</p><p>An actor configuration contains information about the actor class, its constructor arguments, mailbox, and dispatcher implementation. In Akka, an actor configuration is represented with the <code class="literal">Props</code> class. A <code class="literal">Props</code> object encapsulates all the information required to create an actor instance, and can be serialized or sent over the network.</p><p>To create <code class="literal">Props</code> objects, it is recommended practice to declare Factory methods in the companion object of the actor class. In the following companion object, we declare two Factory methods, called <code class="literal">props</code> and <code class="literal">propsAlt</code>, which return <code class="literal">Props</code> objects for the <code class="literal">HelloActor</code> class, given the <code class="literal">hello</code> argument:</p><pre class="programlisting">object HelloActor { 
  def props(hello: String) = Props(new HelloActor(hello)) 
  def propsAlt(hello: String) = Props(classOf[HelloActor], hello) 
} 
</pre><p>The <code class="literal">props</code> method uses an overload of the <code class="literal">Props.apply</code> factory method, which takes a block of code by creating the <code class="literal">HelloActor</code> class. This block of code is invoked every time an actor system needs to create an actor instance. The <code class="literal">propsAlt</code> method uses another <code class="literal">Props.apply</code> overload, which creates an actor instance from the <code class="literal">Class</code> object of the actor class, and a list of constructor arguments. The two declarations are semantically equivalent.</p><p>The first <code class="literal">Props.apply</code> method overload takes a closure that calls the actor class constructor. If we are not careful, the closure can easily catch references to the enclosing scope. When this happens, these references become a part of the <code class="literal">Props</code> object. Consider the <code class="literal">defaultProps</code> method in the following utility class:</p><pre class="programlisting">class HelloActorUtils { 
  val defaultHi = "Aloha!" 
  def defaultProps() = Props(new HelloActor(defaultHi)) 
} 
</pre><p>Sending the <code class="literal">Props</code> object that is returned by the <code class="literal">defaultProps</code> method over the network requires sending the enclosing <code class="literal">HelloActorUtils</code> object captured by the closure, incurring additional network costs.</p><p>Furthermore, it is particularly dangerous to declare a <code class="literal">Props</code> object within an actor class, as it can catch a <code class="literal">this</code> reference to the enclosing actor instance. It is safer to create the <code class="literal">Props</code> objects exactly as they were shown in the <code class="literal">propsAlt</code> method.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip88"></a>Tip</h3><p>Avoid creating the <code class="literal">Props</code> objects within actor classes to prevent accidentally capturing the actor's <code class="literal">this</code> reference. Wherever possible, declare <code class="literal">Props</code> inside factory methods in top-level singleton objects.</p></div><p>The third overload of the <code class="literal">Props.apply</code> method is a convenience method that can be used with actor classes with zero-argument constructors. If <code class="literal">HelloActor</code> defines no constructor arguments, we can write <code class="literal">Props[HelloActor]</code> to create a <code class="literal">Props</code> object.</p><p>To instantiate an actor, we pass an actor configuration to the <code class="literal">actorOf</code> method of the actor system. Throughout this chapter, we will use our custom actor system instance called <code class="literal">ourSystem</code>. We define the <code class="literal">ourSystem</code> variable using the <code class="literal">ActorSystem.apply</code> factory method:</p><pre class="programlisting">lazy val ourSystem = ActorSystem("OurExampleSystem") 
</pre><p>We can now create and run the <code class="literal">HelloActor</code> class by calling the <code class="literal">actorOf</code> method on the actor system. When creating a new actor, we can specify a unique name for the actor instance with the <code class="literal">name</code> argument. Without explicitly specifying the <code class="literal">name</code> argument, the actor system automatically assigns a unique name to the new actor instance. The <code class="literal">actorOf</code> method does not return an instance of the <code class="literal">HelloActor</code> class. Instead, it returns an actor reference object of the <code class="literal">ActorRef</code> type.</p><p>After creating a <code class="literal">HelloActor</code> instance <code class="literal">hiActor</code>, which recognizes the <code class="literal">hi</code> messages, we send it a message, <code class="literal">hi</code>. To send a message to an Akka actor, we use the <code class="literal">!</code> operator (pronounced as <span class="emphasis"><em>tell</em></span> or <span class="emphasis"><em>bang</em></span>). For clarity, we then pause the execution for one second by calling <code class="literal">sleep</code>, and give the actor some time to process the message. We then send another message, <code class="literal">hola</code>, and wait one more second. Finally, we terminate the actor system by calling its <code class="literal">shutdown</code> method. This is shown in the following program:</p><pre class="programlisting">object ActorsCreate extends App { 
  val hiActor: ActorRef = 
    ourSystem.actorOf(HelloActor.props("hi"), name = "greeter") 
  hiActor ! "hi" 
  Thread.sleep(1000) 
  hiActor ! "hola" 
  Thread.sleep(1000) 
  ourSystem.shutdown() 
} 
</pre><p>Upon running this program, the <code class="literal">hiActor</code> instance first prints that it received a <code class="literal">hi</code> message. After one second, it prints that it received a <code class="literal">hola</code> string as a message, an unexpected message, and terminates.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec72"></a>Managing unhandled messages</h3></div></div></div><p>The <code class="literal">receive</code> method in the <code class="literal">HelloActor</code> example was able to handle any kind of message. When the message was different from the pre-specified <code class="literal">hello</code> argument, such as <code class="literal">hi</code>, used previously, the <code class="literal">HelloActor</code> actor reported this in the default case. Alternatively, we could have left the default case unhandled. When an actor receives a message that is not handled by its <code class="literal">receive</code> method, the message is wrapped into an <code class="literal">UnhandledMessage</code> object and forwarded to the actor system's event stream. Usually, the actor system's event stream is used for logging purposes.</p><p>We can override this default behavior by overriding the <code class="literal">unhandled</code> method in the actor class. By default, this method publishes the unhandled messages on the actor system's event stream. In the following code, we declare a <code class="literal">DeafActor</code> actor class, whose <code class="literal">receive</code> method returns an empty partial function. An empty partial function is not defined for any type of message, so all the messages sent to this actor get passed to the <code class="literal">unhandled</code> method. We override it to output the <code class="literal">String</code> messages to the standard output. We pass all other types of message to the actor system's event stream by calling the <code class="literal">super.unhandled</code> method. The following code snippet shows the <code class="literal">DeafActor</code> implementation:</p><pre class="programlisting">class DeafActor extends Actor { 
  val log = Logging(context.system, this) 
  def receive = PartialFunction.empty 
  override def unhandled(msg: Any) = msg match { 
    case msg: String =&gt; log.info(s"I do not hear '$msg'") 
    case msg         =&gt; super.unhandled(msg) 
  } 
} 
</pre><p>Let's test a <code class="literal">DeafActor</code> class in an example. The following program creates a <code class="literal">DeafActor</code> instance named <code class="literal">deafy</code>, and assigns its actor reference to the value <code class="literal">deafActor</code>. It then sends the two messages, <code class="literal">deafy</code> and <code class="literal">1234</code>, to <code class="literal">deafActor</code>, and shuts down the actor system:</p><pre class="programlisting">object ActorsUnhandled extends App { 
  val deafActor: ActorRef = 
    ourSystem.actorOf(Props[DeafActor], name = "deafy") 
  deafActor ! "hi" 
  Thread.sleep(1000) 
  deafActor ! 1234 
  Thread.sleep(1000) 
  ourSystem.shutdown() 
} 
</pre><p>Running this program shows that the first message, the <code class="literal">deafy</code> string, is caught and printed by the <code class="literal">unhandled</code> method. The <code class="literal">1234</code> message is forwarded to the actor system's event stream, and is never shown on the standard output.</p><p>An attentive reader might have noticed that we could have avoided the <code class="literal">unhandled</code> call by moving the case into the <code class="literal">receive</code> method, as shown in the following <code class="literal">receive</code> implementation:</p><pre class="programlisting">def receive = { 
  case msg: String =&gt; log.info(s"I do not hear '$msg'") 
} 
</pre><p>This definition of the <code class="literal">receive</code> method is more concise, but is inadequate for more complex actors. In the preceding example, we have fused the treatment of unhandled messages together with how the actor handles regular messages. Stateful actors often change the way they handle regular messages, and it is essential to separate the treatment of unhandled messages from the normal behavior of the actor. We will study how to change the actor behavior in the following section.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec73"></a>Actor behavior and state</h3></div></div></div><p>When an actor changes its state, it is often necessary to change the way it handles incoming messages. The way that the actor handles regular messages is called the <span class="strong"><strong>behavior</strong></span> of the actor. In this section, we will study how to manipulate actor behavior.</p><p>We have previously learned that we define the initial behavior of the actor by implementing the <code class="literal">receive</code> method. Note that the <code class="literal">receive</code> method must always return the same partial function. It is not correct to return different partial functions from the <code class="literal">receive</code> method depending on the current state of the actor. Let's assume we want to define a <code class="literal">CountdownActor</code> actor class, which decreases its <code class="literal">n</code> integer field every time it receives a <code class="literal">count</code> message, until it reaches zero. After the <code class="literal">CountdownActor</code> class reaches zero, it should ignore all subsequent messages. The following definition of the <code class="literal">receive</code> method is not allowed in Akka:</p><pre class="programlisting">class CountdownActor extends Actor { 
  var n = 10 
  def receive = if (n &gt; 0) { // never do this 
    case "count" =&gt; 
      log(s"n = $n") 
      n -= 1 
  } else PartialFunction.empty 
} 
</pre><p>To correctly change the behavior of the <code class="literal">CountdownActor</code> class after it reaches zero, we use the <code class="literal">become</code> method on the actor's <code class="literal">context</code> object. In the correct definition of the <code class="literal">CountdownActor</code> class, we define two methods, <code class="literal">counting</code> and <code class="literal">done</code>, which return two different behaviors. The <code class="literal">counting</code> behavior reacts to the <code class="literal">count</code> messages and calls <code class="literal">become</code> to change to the <code class="literal">done</code> behavior once the <code class="literal">n</code> field is zero. The <code class="literal">done</code> behavior is just an empty partial function, which ignores all the messages.</p><p>This is shown in the following implementation of the <code class="literal">CountdownActor</code> class:</p><pre class="programlisting">class CountdownActor extends Actor { 
  val log = Logging(context.system, this) 
  var n = 10 
  def counting: Actor.Receive = { 
    case "count" =&gt; 
      n -= 1 
      log.info(s"n = $n") 
      if (n == 0) context.become(done) 
  } 
  def done = PartialFunction.empty 
  def receive = counting 
} 
</pre><p>The <code class="literal">receive</code> method defines the initial behavior of the actor, which must be the <code class="literal">counting</code> behavior.Â Note that we are using the type alias <code class="literal">Receive</code> from the Actor companion object, which is just a shorthand for the <code class="literal">PartialFunction[Any, Unit] type</code>.</p><p>When modeling complex actors, it is helpful to think of them as <span class="strong"><strong>state machines</strong></span>. A state machine is a mathematical model that represents a system with some number of states and transitions between these states. In an actor, each behavior corresponds to a state in the state machine. A transition exists between two states if the actor potentially calls the <code class="literal">become</code> method when receiving a certain message. In the following figure, we illustrate the state machine corresponding to the <code class="literal">CountdownActor</code> class. The two circles represent the states corresponding to the behaviors <code class="literal">counting</code> and <code class="literal">done</code>. The initial behavior is <span class="strong"><strong>counting</strong></span>, so we draw an arrow pointing to the corresponding state. We represent the transitions between the states with arrows starting and ending at a state.</p><p>When the actor receives the <span class="strong"><strong>count</strong></span> message and the <span class="strong"><strong>n</strong></span> field is larger than <span class="strong"><strong>1</strong></span>, the behavior does not change. However, when the actor receives the <span class="strong"><strong>count</strong></span> message and the <span class="strong"><strong>n</strong></span> field is decreased to <code class="literal">0</code>, the actor changes its behavior to <span class="strong"><strong>done</strong></span>:</p><div class="mediaobject"><img src="graphics/image_08_001.jpg" /></div><p>The following short program tests the correctness of our actor. We use the actor system to create a new <code class="literal">countdown</code> actor, and send it 20 <code class="literal">count</code> messages. The actor only reacts to the first 10 messages, before switching to the <code class="literal">done</code> behavior:</p><pre class="programlisting">object ActorsCountdown extends App { 
  val countdown = ourSystem.actorOf(Props[CountdownActor]) 
  for (i &lt;- 0 until 20) countdown ! "count" 
  Thread.sleep(1000) 
  ourSystem.shutdown() 
} 
</pre><p>Whenever an actor responds to the incoming messages differently depending on its current state, you should decompose different states into partial functions and use the <code class="literal">become</code> method to switch between states. This is particularly important when actors get more complex, and ensures that the actor logic is easier to understand and maintain.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip89"></a>Tip</h3><p>When a stateful actor needs to change its behavior, declare a separate partial function for each of its behaviors. Implement the <code class="literal">receive</code> method to return the method corresponding to the initial behavior.</p></div><p>We now consider a more refined example, in which we define an actor that checks if a given word exists in a dictionary and prints it to the standard output. We want to be able to change the dictionary that the actor is using during runtime. To set the dictionary, we send the actor an <code class="literal">Init</code> message with the path to the dictionary. After that, we can check if a word is in the dictionary by sending the actor the <code class="literal">IsWord</code> message. Once we're done using the dictionary, we can ask the actor to unload the dictionary by sending it the <code class="literal">End</code> message. After that, we can initialize the actor with some other dictionary.</p><p>The following state machine models this logic with two behaviors, called <code class="literal">uninitialized</code> and <code class="literal">initialized</code>:</p><div class="mediaobject"><img src="graphics/image_08_002.jpg" /></div><p>It is a recommended practice to define the datatypes for the different messages in the companion object of the actor class. In this case, we add the case classes <code class="literal">Init</code>, <code class="literal">IsWord</code>, and <code class="literal">End</code> to the companion object of the <code class="literal">DictionaryActor</code> class:</p><pre class="programlisting">object DictionaryActor { 
  case class Init(path: String) 
  case class IsWord(w: String) 
  case object End 
} 
</pre><p>We next define the <code class="literal">DictionaryActor</code> actor class. This class defines a private <code class="literal">Logging</code> object <code class="literal">log</code>, and a <code class="literal">dictionary</code> mutable set, which is initially empty and can be used to store words. The <code class="literal">receive</code> method returns the <code class="literal">uninitialized</code> behavior, which only accepts the <code class="literal">Init</code> message type. When an <code class="literal">Init</code> message arrives, the actor uses its <code class="literal">path</code> field to fetch the dictionary from a file, load the words, and call <code class="literal">become</code> to switch to the <code class="literal">initialized</code> behavior. When an <code class="literal">IsWord</code> message arrives, the actor checks if the word exists and prints it to the standard output. If an <code class="literal">End</code> message arrives, the actor clears the dictionary and switches back to the <code class="literal">uninitialized</code> behavior. This is shown in the following code snippet:</p><pre class="programlisting">class DictionaryActor extends Actor { 
  private val log = Logging(context.system, this) 
  private val dictionary = mutable.Set[String]() 
  def receive = uninitialized 
  def uninitialized: PartialFunction[Any, Unit] = { 
    case DictionaryActor.Init(path) =&gt; 
      val stream = getClass.getResourceAsStream(path) 
      val words = Source.fromInputStream(stream) 
      for (w &lt;- words.getLines) dictionary += w 
      context.become(initialized) 
  } 
  def initialized: PartialFunction[Any, Unit] = { 
    case DictionaryActor.IsWord(w) =&gt; 
      log.info(s"word '$w' exists: ${dictionary(w)}") 
    case DictionaryActor.End =&gt; 
      dictionary.clear() 
      context.become(uninitialized) 
  } 
  override def unhandled(msg: Any) = { 
    log.info(s"cannot handle message $msg in this state.") 
  } 
} 
</pre><p>Note that we have overridden the <code class="literal">unhandled</code> method in the <code class="literal">DictionaryActor</code> class. In this case, using the <code class="literal">unhandled</code> method reduces code duplication, and makes the <code class="literal">DictionaryActor</code> class easier to maintain, as there is no need to list the <code class="literal">default</code> case twice in both the <code class="literal">initialized</code> and <code class="literal">uninitialized</code> behaviors.</p><p>If you are using a Unix system, you can load the list of words, separated by a newline character, from the file in the <code class="literal">/usr/share/dict/words</code> location. Alternatively, download the source code for this book and find the <code class="literal">words.txt</code> file, or create a dummy file with several words, and save it to the <code class="literal">src/main/resources/org/learningconcurrency/</code> directory. You can then test the correctness of the <code class="literal">DictionaryActor</code> class using the following program:</p><pre class="programlisting">val dict = ourSystem.actorOf(Props[DictionaryActor], "dictionary") 
 
dict ! DictionaryActor.IsWord("program") 
Thread.sleep(1000) 
 
dict ! DictionaryActor.Init("/org/learningconcurrency/words.txt") 
Thread.sleep(1000) 
</pre><p>The first message sent to the actor results in an error message. We cannot send an <code class="literal">IsWord</code> message before initializing the actor. After sending the <code class="literal">Init</code> message, we can check if words are present in the dictionary. Finally, we send an <code class="literal">End</code> message and shut down the actor system, as shown in the following code snippet:</p><pre class="programlisting">dict ! DictionaryActor.IsWord("program") 
Thread.sleep(1000) 
 
dict ! DictionaryActor.IsWord("balaban") 
Thread.sleep(1000) 
 
dict ! DictionaryActor.End 
Thread.sleep(1000) 
 
ourSystem.shutdown() 
</pre><p>Having learned about actor behaviors, we will study how actors are organized into a hierarchy in the following section.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec74"></a>Akka actor hierarchy</h3></div></div></div><p>In large organizations, people are assigned roles and responsibilities for different tasks in order to reach a specific goal. The CEO of the company chooses a specific goal, such as launching a software product. He then delegates parts of the work tasks to various teams within the company-the marketing team investigates potential customers for the new product, the design team develops the user interface of the product, and the software engineering team implements the logic of the software product. Each of these teams can be further decomposed into sub-teams with different roles and responsibilities, depending on the size of the company. For example, the software engineering team can be composed into two developer sub-teams, responsible for implementing the backend of the software product, such as the server-side code, and the frontend, such as the website or a desktop UI.</p><p>Similarly, sets of actors can form hierarchies in which actors that are closer to the root work on more general tasks and delegate work items to more specialized actors lower in the hierarchy. Organizing parts of the system into hierarchies is a natural and systematic way to decompose a complex program into its basic components. In the context of actors, a correctly chosen actor hierarchy can also guarantee better scalability of the application, depending on how the work is balanced between the actors. Importantly, a hierarchy between actors allows isolating and replacing parts of the system that fail more easily.</p><p>In Akka, actors implicitly form a hierarchy. Every actor can have some number of child actors, and it can create or stop child actors using the <code class="literal">context</code> object. To test this relationship, we will define two actor classes to represent the parent and child actors. We start by defining the <code class="literal">ChildActor</code> actor class, which reacts to the <code class="literal">sayhi</code> messages by printing the reference to its parent actor. The reference to the parent is obtained by calling the <code class="literal">parent</code> method on the <code class="literal">context</code> object. Additionally, we will override the <code class="literal">postStop</code> method of the <code class="literal">Actor</code> class, which is invoked after the actor stops. By doing this, we will be able to see precisely when a child actor is stopped. The <code class="literal">ChildActor</code> template is shown in the following code snippet:</p><pre class="programlisting">class ChildActor extends Actor { 
  val log = Logging(context.system, this) 
  def receive = { 
    case "sayhi" =&gt; 
      val parent = context.parent 
      log.info(s"my parent $parent made me say hi!") 
  } 
  override def postStop() { 
    log.info("child stopped!") 
  } 
} 
</pre><p>We now define an actor class called <code class="literal">ParentActor</code>, which can accept the messages <code class="literal">create</code>, <code class="literal">sayhi</code>, and <code class="literal">stop</code>. When <code class="literal">ParentActor</code> receives a <code class="literal">create</code> message, it creates a new child by calling <code class="literal">actorOf</code> on the <code class="literal">context</code> object. When the <code class="literal">ParentActor</code> class receives a <code class="literal">sayhi</code> message, it forwards the message to its children by traversing the <code class="literal">context.children</code> list, and resending the message to each child. Finally, when the <code class="literal">ParentActor</code> class receives a <code class="literal">stop</code> message, it stops itself:</p><pre class="programlisting">class ParentActor extends Actor { 
  val log = Logging(context.system, this) 
  def receive = { 
    case "create" =&gt; 
      context.actorOf(Props[ChildActor]) 
      log.info(s"created a kid; children = ${context.children}") 
    case "sayhi" =&gt; 
      log.info("Kids, say hi!") 
      for (c &lt;- context.children) c ! "sayhi" 
    case "stop" =&gt; 
      log.info("parent stopping") 
      context.stop(self) 
  } 
} 
</pre><p>We test the actor classes <code class="literal">ParentActor</code> and <code class="literal">ChildActor</code> in the following program. We first create the <code class="literal">ParentActor</code> instance, <code class="literal">parent</code>, and then send two <code class="literal">create</code> messages to <code class="literal">parent</code>. The <code class="literal">parent</code> actor prints that it created a child actor twice. We then send a <code class="literal">sayhi</code> message to <code class="literal">parent</code>, and witness how the child actors output a message after the parent forwards the <code class="literal">sayhi</code> message to them. Finally, we send a <code class="literal">stop</code> message to stop the <code class="literal">parent</code> actor. This is shown in the following program:</p><pre class="programlisting">object ActorsHierarchy extends App { 
  val parent = ourSystem.actorOf(Props[ParentActor], "parent") 
  parent ! "create" 
  parent ! "create" 
  Thread.sleep(1000) 
  parent ! "sayhi" 
  Thread.sleep(1000) 
  parent ! "stop" 
  Thread.sleep(1000) 
  ourSystem.shutdown() 
} 
</pre><p>By studying the standard output, we find that each of the two child actors output a <code class="literal">sayhi</code> message immediately after the <code class="literal">parent</code> actor prints that it is about to stop. This is the normal behavior of Akka actors-a child actor cannot exist without its parent. As soon as the parent actor stops, its child actors are stopped by the actor system as well.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note90"></a>Note</h3><p>When an actor is stopped, its child actors are also automatically stopped.</p></div><p>If you ran the preceding example program, you might have noticed that printing an actor reference reflects the actor's position in the actor hierarchy. For example, printing the child actor reference shows the <code class="literal">akka://OurExampleSystem/user/parent/$a</code> string. The first part of this string, <code class="literal">akka://</code>, denotes that this reference points to a local actor. The <code class="literal">OurExampleSystem</code> part is the name of the actor system that we are using in this example. The <code class="literal">parent/$a</code> part reflects the name of the parent actor and the automatically generated name <code class="literal">$a</code> of the child actor. Unexpectedly, the string representation of the actor reference also contains a reference to an intermediate actor, called <code class="literal">user</code>.</p><p>In Akka, an actor that resides at the top of the actor hierarchy is called the <span class="strong"><strong>guardian actor</strong></span>, which exists to perform various internal tasks, such as logging and restarting user actors. Every top-level actor created in the application is placed under the <code class="literal">user</code> predefined guardian actor. There are other guardian actors. For example, actors internally used by the actor system are placed under the <code class="literal">system</code> guardian actor. The actor hierarchy is shown in the following figure, where the guardian actors <code class="literal">user</code> and <code class="literal">system</code> form two separate hierarchies in the actor system called <code class="literal">OurExampleSystem</code>:</p><div class="mediaobject"><img src="graphics/image_08_003.jpg" /></div><p>In this section, we saw that Akka actors form a hierarchy, and learned about the relationships between actors in this hierarchy. Importantly, we learned how to refer to immediate neighbors of an actor using the <code class="literal">parent</code> and <code class="literal">children</code> methods of the <code class="literal">context</code> object. In the following section, we will see how to refer to an arbitrary actor within the same actor system.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec75"></a>Identifying actors</h3></div></div></div><p>In the previous section, we learned that actors are organized in a hierarchical tree, in which every actor has a parent and some number of children. Thus, every actor lies on a unique path from the root of this hierarchy, and can be assigned a unique sequence of actor names on this path. The <code class="literal">parent</code> actor was directly beneath the <code class="literal">user</code> guardian actor, so its unique sequence of actor names is <code class="literal">/user/parent</code>. Similarly, the unique sequence of actor names for the <code class="literal">parent</code> actor's child actor is <code class="literal">$a</code> is <code class="literal">/user/parent/$a</code>. An <span class="strong"><strong>actor path</strong></span> is a concatenation of the protocol, the actor system name, and the actor names on the path from the top guardian actor to a specific actor. The actor path of the <code class="literal">parent</code> actor from the previous example is <code class="literal">akka://OurExampleSystem/user/parent</code>.</p><p>Actor paths closely correspond to file paths in a filesystem. Every file path uniquely designates a file location, just as an actor path uniquely designates the location of the actor in the hierarchy. Just as a file path in a filesystem does not mean that a file exists, an actor path does not imply that there is an actor on that file path in the actor system. Instead, an actor path is an identifier used to obtain an actor reference if one exists. Also, parts of the names in the actor path can be replaced with wildcards and the <code class="literal">..</code> symbol, similar to how parts of filenames can be replaced in a shell. In this case, we obtain a <span class="strong"><strong>path selection</strong></span>. For example, the path selection <code class="literal">..</code> references the parent of the current actor. The selection <code class="literal">../*</code> references the current actor and all its siblings.</p><p>Actor paths are different from actor references; we cannot send a message to an actor using its actor path. Instead, we must first use the actor path to identify an actor on that actor path. If we successfully find an actor reference behind an actor path, we can send messages to it.</p><p>To obtain an actor reference corresponding to an actor path, we call the <code class="literal">actorSelection</code> method on the context object of an actor. This method takes an actor path, or a path selection. Calling the <code class="literal">actorSelection</code> method might address zero actors if no actors correspond to the actor path. Similarly, it might address multiple actors if we use a path selection. Thus, instead of returning an <code class="literal">ActorRef</code> object, the <code class="literal">actorSelection</code> method returns an <code class="literal">ActorSelection</code> object, which might represent zero, one, or more actors. We can use the <code class="literal">ActorSelection</code> object to send messages to these actors.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip91"></a>Tip</h3><p>Use the <code class="literal">actorSelection</code> method on the <code class="literal">context</code> object to communicate with arbitrary actors in the actor system.</p></div><p>If we compare the <code class="literal">ActorRef</code> object to a specific e-mail address, an <code class="literal">ActorSelection</code> object can be compared to a mailing list address. Sending an e-mail to a valid e-mail address ensures that the e-mail reaches a specific person. On the other hand, when we send an e-mail to a mailing list, the e-mail might reach zero, one, or more people, depending on the number of mailing list subscribers.</p><p>An <code class="literal">ActorSelection</code> object does not tell us anything about the concrete paths of the actors, in a similar way to how a mailing list does not tell us anything about its subscribers. For this purpose, Akka defines a special type of message called <code class="literal">Identify</code>. When an Akka actor receives an <code class="literal">Identify</code> message, it will automatically reply by sending back an <code class="literal">ActorIdentity</code> message with its <code class="literal">ActorRef</code> object. If there are no actors in the actor selection, the <code class="literal">ActorIdentity</code> message is sent back to the sender of <code class="literal">Identify</code> without an <code class="literal">ActorRef</code> object.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip92"></a>Tip</h3><p>Send <code class="literal">Identify</code> messages to the <code class="literal">ActorSelection</code> objects to obtain actor references of arbitrary actors in the actor system.</p></div><p>In the following example, we define a <code class="literal">CheckActor</code> actor class, which describes actors that check and print actor references whenever they receive a message with an actor path. When the actor of type <code class="literal">CheckActor</code> receives a string with an actor path or a path selection, it obtains an <code class="literal">ActorSelection</code> object and sends it an <code class="literal">Identify</code> message. This message is forwarded to all actors in the selection, which then respond with an <code class="literal">ActorIdentity</code> message. The <code class="literal">Identify</code> message also takes a <code class="literal">messageId</code> argument. If an actor sends out multiple <code class="literal">Identify</code> messages, the <code class="literal">messageId</code> argument allows disambiguating between the different <code class="literal">ActorIdentity</code> responses. In our example, we use the <code class="literal">path</code> string as the <code class="literal">messageId</code> argument. When <code class="literal">CheckActor</code> receives an <code class="literal">ActorIdentity</code> message, it either prints the actor reference or reports that there is no actor on the specified path. The <code class="literal">CheckActor</code> class is shown in the following code snippet:</p><pre class="programlisting">class CheckActor extends Actor { 
  val log = Logging(context.system, this) 
  def receive = { 
    case path: String =&gt; 
      log.info(s"checking path $path") 
      context.actorSelection(path) ! Identify(path) 
    case ActorIdentity(path, Some(ref)) =&gt; 
      log.info(s"found actor $ref at $path") 
    case ActorIdentity(path, None) =&gt; 
      log.info(s"could not find an actor at $path") 
  } 
} 
</pre><p>Next, we instantiate a <code class="literal">checker</code> actor of the <code class="literal">CheckActor</code> class, and send it the path selection, <code class="literal">../*</code>. This references all the child actors of the <code class="literal">checker</code> parent-the <code class="literal">checker</code> actor itself and its siblings:</p><pre class="programlisting">val checker = ourSystem.actorOf(Props[CheckActor], "checker") 
checker ! "../*" 
</pre><p>We did not instantiate any top-level actors besides the <code class="literal">checker</code> actor, so checker receives only a single <code class="literal">ActorIdentity</code> message and prints its own actor path. Next, we try to identify all the actors one level above the <code class="literal">checker</code> actor. Recall the earlier figure. Since <code class="literal">checker</code> is a top-level actor, this should identify the guardian actors in the actor system:</p><pre class="programlisting">checker ! "../../*" 
</pre><p>As expected, the <code class="literal">checker</code> actor prints the actor paths of the <code class="literal">user</code> and <code class="literal">system</code> guardian actors. We are curious to learn more about the system-internal actors from the <code class="literal">system</code> guardian actor. This time, we send an absolute path selection to <code class="literal">checker</code>:</p><pre class="programlisting">checker ! "/system/*" 
</pre><p>The <code class="literal">checker</code> actor prints the actor paths of the internal actors <code class="literal">log1-Logging</code> and <code class="literal">deadLetterListener</code>, which are used for logging and for processing unhandled messages, respectively. We next try identifying a non-existing actor:</p><pre class="programlisting">checker ! "/user/checker2" 
</pre><p>There are no actors named <code class="literal">checker2</code>, so <code class="literal">checker</code> receives an <code class="literal">ActorIdentity</code> message with the <code class="literal">ref</code> field set to <code class="literal">None</code> and prints that it cannot find an actor on that path.</p><p>Using the <code class="literal">actorSelection</code> method and the <code class="literal">Identify</code> message is the fundamental method for discovering unknown actors in the same actor system. Note that we will always obtain an actor reference, and never obtain a pointer to the actor object directly. To better understand the reasons for this, we will study the lifecycle of actors in the next section.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec76"></a>The actor lifecycle</h3></div></div></div><p>Recall that the <code class="literal">ChildActor</code> class from the previous section overrode the <code class="literal">postStop</code> method to produce some logging output when the actor is stopped. In this section, we investigate when exactly the <code class="literal">postStop</code> method gets called, along with the other important events that comprise the lifecycle of the actor.</p><p>To understand why the actor lifecycle is important, we consider what happens if an actor throws an exception while processing an incoming message. In Akka, such an exception is considered abnormal behavior, so top-level user actors that throw an exception are by default restarted. Restarting creates a fresh actor object, and effectively means that the actor state is reinitialized. When an actor is restarted, its actor reference and actor path remain the same. Thus, the same <code class="literal">ActorRef</code> object might refer to many different physical actor objects during the logical existence of the same actor. This is one of the reasons why an actor must never allow its <code class="literal">this</code> reference to leak. Doing so allows other parts of the program to refer to an old actor object, consequently invalidating the transparency of the actor reference. Additionally, revealing the <code class="literal">this</code> reference of the actor can reveal the internals of the actor implementation, or even cause data corruption.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip93"></a>Tip</h3><p>Never pass an actor's <code class="literal">this</code> reference to other actors, as it breaks actor encapsulation.</p></div><p>Let's examine the complete actor lifecycle. As we have learned, a logical actor instance is created when we call the <code class="literal">actorOf</code> method. The <code class="literal">Props</code> object is used to instantiate a physical actor object. This object is assigned a mailbox, and can start receiving input messages. The <code class="literal">actorOf</code> method returns an actor reference to the caller, and the actors can execute concurrently. Before the actor starts processing messages, its <code class="literal">preStart</code> method is called. The <code class="literal">preStart</code> method is used to initialize the logical actor instance.</p><p>After creation, the actor starts processing messages. At some point, an actor might need to be restarted due to an exception. When this happens, the <code class="literal">preRestart</code> method is first called. All the child actors are then stopped. Then, the <code class="literal">Props</code> object, previously used in order to create the actor with the <code class="literal">actorOf</code> method, is reused to create a new actor object. The <code class="literal">postRestart</code> method is called on the newly created actor object. After <code class="literal">postRestart</code> returns, the new actor object is assigned the same mailbox as the old actor object, and it continues to process messages that were in the mailbox before the restart.</p><p>By default, the <code class="literal">postRestart</code> method calls the <code class="literal">prestart</code> method. In some cases, we want to override this behavior. For example, a database connection might need to be opened only once during <code class="literal">preStart</code>, and closed when the logical actor instance is terminated.</p><p>Once the logical actor instance needs to stop, the <code class="literal">postStop</code> method gets called. The actor path associated with the actor is released, and returned to the actor system. By default, the <code class="literal">preRestart</code> method calls the <code class="literal">postStop</code> method. The complete actor lifecycle is illustrated in the following figure:</p><div class="mediaobject"><img src="graphics/image_08_004.jpg" /></div><p>Note that, during the actor lifecycle, the rest of the actor system observes the same actor reference, regardless of how many times the actor restarts. Actor failures and restarts occur transparently for the rest of the system.</p><p>To experiment with the lifecycle of an actor, we declare two actor classes, <code class="literal">StringPrinter</code> and <code class="literal">LifecycleActor</code>. The <code class="literal">StringPrinter</code> actor prints a logging statement for each message that it receives. We override its <code class="literal">preStart</code> and <code class="literal">postStop</code> methods to precisely track when the actor has started and stopped, as shown in the following snippet:</p><pre class="programlisting">class StringPrinter extends Actor { 
  val log = Logging(context.system, this) 
  def receive = { 
    case msg =&gt; log.info(s"printer got message '$msg'") 
  } 
  override def preStart(): Unit = log.info(s"printer preStart.") 
  override def postStop(): Unit = log.info(s"printer postStop.") 
} 
</pre><p>The <code class="literal">LifecycleActor</code> class maintains a <code class="literal">child</code> actor reference to a <code class="literal">StringPrinter</code> actor. The <code class="literal">LifecycleActor</code> class reacts to the <code class="literal">Double</code> and <code class="literal">Int</code> messages by printing them, and to the <code class="literal">List</code> messages by printing the first element of the list. When it receives a <code class="literal">String</code> message, the <code class="literal">LifecycleActor</code> instance forwards it to the <code class="literal">child</code> actor:</p><pre class="programlisting">class LifecycleActor extends Actor { 
  val log = Logging(context.system, this) 
  var child: ActorRef = _ 
  def receive = { 
    case num: Double  =&gt; log.info(s"got a double - $num") 
    case num: Int     =&gt; log.info(s"got an integer - $num") 
    case lst: List[_] =&gt; log.info(s"list - ${lst.head}, ...") 
    case txt: String  =&gt; child ! txt 
  } 
} 
</pre><p>We now override different lifecycle hooks. We start with the <code class="literal">preStart</code> method to output a logging statement and instantiate the <code class="literal">child</code> actor. This ensures that the <code class="literal">child</code> reference is initialized before the actor starts processing any messages:</p><pre class="programlisting">override def preStart(): Unit = { 
  log.info("about to start") 
  child = context.actorOf(Props[StringPrinter], "kiddo") 
} 
</pre><p>Next, we override the <code class="literal">preRestart</code> and <code class="literal">postRestart</code> methods. In the <code class="literal">preRestart</code> and <code class="literal">postRestart</code> methods, we log the exception that caused the failure. The <code class="literal">postRestart</code> method calls the <code class="literal">preStart</code> method by default, so the new actor object gets initialized with a new <code class="literal">child</code> actor after a restart:</p><pre class="programlisting">override def preRestart(t: Throwable, msg: Option[Any]): Unit = { 
  log.info(s"about to restart because of $t, during message $msg") 
  super.preRestart(t, msg) 
} 
override def postRestart(t: Throwable): Unit = { 
  log.info(s"just restarted due to $t") 
  super.postRestart(t) 
} 
</pre><p>Finally, we override the <code class="literal">postStop</code> method to track when the actor is stopped:</p><pre class="programlisting">override def postStop() = log.info("just stopped") 
</pre><p>We now create an instance of the <code class="literal">LifecycleActor</code> class called <code class="literal">testy</code>, and send a <code class="literal">math.Pi</code> message to it. The actor prints that it is about to start in its <code class="literal">preStart</code> method, and creates a new <code class="literal">child</code> actor. It then prints that it received the value <code class="literal">math.Pi</code>. Importantly, the <code class="literal">child about to start</code> logging statement is printed after the <code class="literal">math.Pi</code> message is received. This shows that actor creation is an asynchronous operation-when we call <code class="literal">actorOf</code>, creating the actor is delegated to the actor system, and the program immediately proceeds:</p><pre class="programlisting">val testy = ourSystem.actorOf(Props[LifecycleActor], "testy") 
testy ! math.Pi 
</pre><p>We then send a string message to <code class="literal">testy</code>. The message is forwarded to the <code class="literal">child</code> actor, which prints a logging statement, indicating that it received the message:</p><pre class="programlisting">testy ! "hi there!" 
</pre><p>Finally, we send a <code class="literal">Nil</code> message to <code class="literal">testy</code>. The <code class="literal">Nil</code> object represents an empty list, so <code class="literal">testy</code> throws an exception when attempting to fetch the <code class="literal">head</code> element. It reports that it needs to restart. After that, we witness that the <code class="literal">child</code> actor prints the message that it needs to stop; recall that the child actors are stopped when an actor is restarted. Finally, <code class="literal">testy</code> prints that it is about to restart, and the new <code class="literal">child</code> actor is instantiated. These events are caused by the following statement:</p><pre class="programlisting">testy ! Nil 
</pre><p>Testing the actor lifecycle revealed an important property of the <code class="literal">actorOf</code> method. When we call the <code class="literal">actorOf</code> method, the execution proceeds without waiting for the actor to fully initialize itself. Similarly, sending a message does not block execution until the message is received or processed by another actor; we say that message sends are asynchronous. In the following section, we will examine various communication patterns that address this asynchronous behavior.</p></div></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec54"></a>Communication between actors</h2></div></div><hr /></div><p>We have learned that actors communicate by sending messages. While actors running on the same machine can access shared parts of memory in the presence of proper synchronization, sending messages allows isolating the actor from the rest of the system and ensures location transparency. The fundamental operation that allows you to send a message to an actor is the <code class="literal">!</code> operator.</p><p>We have learned that the <code class="literal">!</code> operator is a non-blocking operation-sending a message does not block the execution of the sender until the message is delivered. This way of sending messages is sometimes called the <span class="strong"><strong>fire-and-forget</strong></span> pattern, because it does not wait for a reply from the message receiver, nor does it ensure that the message is delivered.</p><p>Sending messages in this way improves the throughput of programs built using actors, but can be limiting in some situations. For example, we might want to send a message and wait for the response from the target. In this section, we learn about patterns used in actor communication that go beyond fire-and-forget.</p><p>While the fire-and-forget pattern does not guarantee that the message is delivered, it guarantees that the message is delivered <span class="strong"><strong>at most once</strong></span>. The target actor never receives duplicate messages. Furthermore, the messages are guaranteed to be ordered for a given pair of sender and receiver actors. If an actor <span class="strong"><strong>A</strong></span> sends messages <span class="strong"><strong>X</strong></span> and <span class="strong"><strong>Y</strong></span> in that order, the actor <span class="strong"><strong>B</strong></span> will receive no duplicate messages, only the message <span class="strong"><strong>X</strong></span>, only the message <span class="strong"><strong>Y</strong></span>, or the message <span class="strong"><strong>X</strong></span>, followed by the message <span class="strong"><strong>Y</strong></span>.</p><p>This is shown on the left in the following figure:</p><div class="mediaobject"><img src="graphics/image_08_005.jpg" /></div><p>However, the delivery order is not ensured for a group of three or more actors. For example, as shown on the right in the preceding figure, actor <span class="strong"><strong>A</strong></span> performs the following actions:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Sends a message <span class="strong"><strong>X</strong></span> to the actor <span class="strong"><strong>B</strong></span></p></li><li style="list-style-type: disc"><p>Sends a message <span class="strong"><strong>Y</strong></span> to another actor, <span class="strong"><strong>C</strong></span></p></li><li style="list-style-type: disc"><p>Actor <span class="strong"><strong>C</strong></span> sends a message <span class="strong"><strong>Z</strong></span> to the actor <span class="strong"><strong>B</strong></span> after having received <span class="strong"><strong>Y</strong></span></p></li></ul></div><p>In this situation, the delivery order between messages <span class="strong"><strong>X</strong></span> and <span class="strong"><strong>Z</strong></span> is not guaranteed. The actor <span class="strong"><strong>B</strong></span> might receive the messages <span class="strong"><strong>X</strong></span> and <span class="strong"><strong>Z</strong></span> in any order. This property reflects the characteristics of most computer networks, and is adopted to allow actors to run transparently on network nodes that may be remote.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note94"></a>Note</h3><p>The order in which an actor <span class="strong"><strong>B</strong></span> receives messages from an actor <span class="strong"><strong>A</strong></span> is the same as the order in which these messages are sent from the actor <span class="strong"><strong>A</strong></span>.</p></div><p>Before we study various patterns of actor communication, note that the <code class="literal">!</code> operator was not the only non-blocking operation. The methods <code class="literal">actorOf</code> and <code class="literal">actorSelection</code> are also non-blocking. These methods are often called while an actor is processing a message. Blocking the actor while the message is processed prevents the actor from processing subsequent messages in the mailbox and severely compromises the throughput of the system. For these reasons, most of the actor API is non-blocking. Additionally, we must never start blocking the operations from third-party libraries from within an actor.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip95"></a>Tip</h3><p>Messages must be handled without blocking indefinitely. Never start an infinite loop and avoid long-running computations in the <code class="literal">receive</code> block, the <code class="literal">unhandled</code> method, and within actor lifecycle hooks.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec77"></a>The ask pattern</h3></div></div></div><p>Not being able to block from within an actor prevents the request-respond communication pattern. In this pattern, an actor interested in certain information sends a request message to another actor. It then needs to wait for a response message from the other actor. In Akka, this communication pattern is also known as the <span class="emphasis"><em>ask pattern</em></span>.</p><p>The <code class="literal">akka.pattern</code> package defines the use of convenience methods in actor communication. Importing its contents allows us to call the <code class="literal">?</code> operator (pronounced ask) on actor references. This operator sends a message to the target actor, such as the tell operator. Additionally, the ask operator returns a future object with the response from the target actor.</p><p>To illustrate the usage of the ask pattern, we will define two actors that play ping pong with each other. A <code class="literal">Pingy</code> actor will send a <code class="literal">ping</code> request message to another actor, of type <code class="literal">Pongy</code>. When the <code class="literal">Pongy</code> actor receives the <code class="literal">ping</code> message, it sends a <code class="literal">pong</code> response message to the sender. We start by importing the <code class="literal">akka.pattern</code> package:</p><pre class="programlisting">import akka.pattern._ 
</pre><p>We first define the <code class="literal">Pongy</code> actor class. To respond to the <code class="literal">ping</code> incoming message, the <code class="literal">Pongy</code> actor needs an actor reference of the sender. While processing a message, every actor can call the <code class="literal">sender</code> method of the <code class="literal">Actor</code> class to obtain the actor reference of the sender of the current message. The <code class="literal">Pongy</code> actor uses the <code class="literal">sender</code> method to send <code class="literal">ping</code> back to the <code class="literal">Pingy</code> actor. The <code class="literal">Pongy</code> implementation is shown in the following code snippet:</p><pre class="programlisting">class Pongy extends Actor { 
  val log = Logging(context.system, this) 
  def receive = { 
    case "ping" =&gt; 
      log.info("Got a ping -- ponging back!") 
      sender ! "pong" 
      context.stop(self) 
  } 
  override def postStop() = log.info("pongy going down") 
} 
</pre><p>Next, we define the <code class="literal">Pingy</code> actor class, which uses the ask operator to send a request to the <code class="literal">Pongy</code> actor. When the <code class="literal">Pingy</code> class receives a <code class="literal">pongyRef</code> actor reference of <code class="literal">Pongy</code>, it creates an implicit <code class="literal">Timeout</code> object set to two seconds. Using the ask operator requires an implicit <code class="literal">Timeout</code> object in scope; the future is failed with an <code class="literal">AskTimeoutException</code> exception if the response message does not arrive within the given timeframe. Once <code class="literal">Pingy</code> class sends the <code class="literal">ping</code> message, it is left with an <code class="literal">f</code> future object. The <code class="literal">Pingy</code> actor uses the special <code class="literal">pipeTo</code> combinator that sends the value in the future to the sender of the <code class="literal">pongyRef</code> actor reference, as shown in the following code:</p><pre class="programlisting">import akka.util.Timeout 
import scala.concurrent.duration._ 
class Pingy extends Actor { 
  val log = Logging(context.system, this) 
  def receive = { 
    case pongyRef: ActorRef =&gt; 
      implicit val timeout = Timeout(2 seconds) 
      val f = pongyRef ? "ping" 
      f pipeTo sender 
  } 
} 
</pre><p>The message in the future object can be manipulated using the standard future combinators seen in <a class="link" href="#" linkend="ch04">Chapter 4</a>, <span class="emphasis"><em>Asynchronous Programming with Futures and Promises</em></span>. However, the following definition of the <code class="literal">Pingy</code> actor would not be correct:</p><pre class="programlisting">class Pingy extends Actor { 
  val log = Logging(context.system, this) 
  def receive = { 
    case pongyRef: ActorRef =&gt; 
      implicit val timeout = Timeout(2 seconds) 
      val f = pongyRef ? "ping" 
      f onComplete { case v =&gt; log.info(s"Response: $v") } // bad! 
  } 
} 
</pre><p>Although it is perfectly legal to call the <code class="literal">onComplete</code> on the <code class="literal">f</code> future, the subsequent asynchronous computation should not access any mutable actor state. Recall that the actor state should be visible only to the actor, so concurrently accessing it opens the possibility of data races and race conditions. The <code class="literal">log</code> object should only be accessed by the actor that owns it. Similarly, we should not call the <code class="literal">sender</code> method from within the <code class="literal">onComplete</code> handler. By the time the future is completed with the response message, the actor might be processing a different message with a different sender, so the <code class="literal">sender</code> method can return arbitrary values.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip96"></a>Tip</h3><p>When starting an asynchronous computation from within the <code class="literal">receive</code> block, the <code class="literal">unhandled</code> method, or a lifecycle hook, never let the closure capture any mutable actor state.</p></div><p>To test <code class="literal">Pingy</code> and <code class="literal">Pongy</code> in action, we define the <code class="literal">Master</code> actor class that instantiates them. Upon receiving the <code class="literal">start</code> message, the <code class="literal">Master</code> actor passes the <code class="literal">pongy</code> reference to the <code class="literal">pingy</code> reference. Once the <code class="literal">pingy</code> actor returns a <code class="literal">pong</code> message from <code class="literal">pongy</code>, the <code class="literal">Master</code> actor stops. This is shown in the following <code class="literal">Master</code> actor template:</p><pre class="programlisting">class Master extends Actor { 
  val pingy = ourSystem.actorOf(Props[Pingy], "pingy") 
  val pongy = ourSystem.actorOf(Props[Pongy], "pongy") 
  def receive = { 
    case "start" =&gt; 
      pingy ! pongy 
    case "pong" =&gt; 
      context.stop(self) 
  } 
  override def postStop() = log.info("master going down") 
} 
val masta = ourSystem.actorOf(Props[Master], "masta") 
masta ! "start" 
</pre><p>The ask pattern is useful because it allows you to send requests to multiple actors and obtain futures with their responses. Values from multiple futures can be combined within <code class="literal">for</code> comprehensions to compute a value from several responses. Using the fire-and-forget pattern when communicating with multiple actors requires changing the actor behavior, and is a lot more cumbersome than the ask pattern.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec78"></a>The forward pattern</h3></div></div></div><p>Some actors exist solely to forward messages to other actors. For example, an actor might be responsible for load-balancing request messages between several worker actors, or it might forward the message to its mirror actor to ensure better availability. In such cases, it is useful to forward the message without changing the <code class="literal">sender</code> field of the message. The <code class="literal">forward</code> method on actor references serves this purpose.</p><p>In the following code, we use the <code class="literal">StringPrinter</code> actor from the previous section to define a <code class="literal">Router</code> actor class. A <code class="literal">Router</code> actor instantiates four child <code class="literal">StringPrinter</code> actors and maintains an <code class="literal">i</code> field with the index of the list child it forwarded the message to. Whenever it receives a message, it forwards the message to a different <code class="literal">StringPrinter</code> child before incrementing the <code class="literal">i</code> field:</p><pre class="programlisting">class Router extends Actor { 
  var i = 0 
  val children = for (_ &lt;- 0 until 4) yield 
    context.actorOf(Props[StringPrinter]) 
  def receive = { 
    case msg =&gt; 
      children(i) forward msg 
      i = (i + 1) % 4 
  } 
} 
</pre><p>In the following code, we create a <code class="literal">Router</code> actor and test it by sending it two messages. We can observe that the messages are printed to the standard output by two different <code class="literal">StringPrinter</code> actors, denoted with actors on the actor paths <code class="literal">/user/router/$b</code> and <code class="literal">/user/router/$a</code>:</p><pre class="programlisting">val router = ourSystem.actorOf(Props[Router], "router") 
router ! "Hola" 
router ! "Hey!" 
</pre><p>The forward pattern is typically used in router actors, which use specific knowledge to decide about the destination of the message; replicator actors, which send the message to multiple destinations; or load balancers, which ensure that the workload is spread evenly between a set of worker actors.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec79"></a>Stopping actors</h3></div></div></div><p>So far, we have stopped different actors by making them call <code class="literal">context.stop</code>. Calling the <code class="literal">stop</code> method on the <code class="literal">context</code> object terminates the actor immediately after the current message is processed. In some cases, we want to have more control over how an actor gets terminated. For example, we might want to allow the actor to process its remaining messages or wait for the termination of some other actors. In Akka, there are several special message types that assist us in doing so, and we study them in this section.</p><p>In many cases, we do not want to terminate an actor instance, but simply restart it. We have previously learned that an actor is automatically restarted when it throws an exception. An actor is also restarted when it receives the <code class="literal">Kill</code> message-when we send a <code class="literal">Kill</code> message to an actor, the actor automatically throws an <code class="literal">ActorKilledException</code> and fails.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip97"></a>Tip</h3><p>Use the <code class="literal">Kill</code> message to restart the target actor without losing the messages in the mailbox.</p></div><p>Unlike the <code class="literal">stop</code> method, the <code class="literal">Kill</code> message does not terminate the actor, but only restarts it. In some cases, we want to terminate the actor instance, but allow it to process the messages from its mailbox. Sending a <code class="literal">PoisonPill</code> message to an actor has the same effect as calling <code class="literal">stop</code>, but allows the actor to process the messages that were in the mailbox before the <code class="literal">PoisonPill</code> message arrives.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip98"></a>Tip</h3><p>Use the <code class="literal">PoisonPill</code> message to stop the actor, but allow it to process the messages received before the <code class="literal">PoisonPill</code> message.</p></div><p>In some cases, allowing the actor to process its message using <code class="literal">PoisonPill</code> is not enough. An actor might have to wait for other actors to terminate before terminating itself. An orderly shutdown is important in some cases, as actors might be involved in sensitive operations, such as writing to a file on the disk. We do not want to forcefully stop them when we end the application. A facility that allows an actor to track the termination of other actors is called <span class="strong"><strong>DeathWatch</strong></span> in Akka.</p><p>Recall the earlier example with the <code class="literal">Pingy</code> and <code class="literal">Pongy</code> actors. Let's say that we want to terminate the <code class="literal">Pingy</code> actor, but only after the <code class="literal">Pongy</code> actor has already been terminated. We define a new <code class="literal">GracefulPingy</code> actor class for this purpose. The <code class="literal">GracefulPingy</code> actor class calls the <code class="literal">watch</code> method on the <code class="literal">context</code> object when it gets created. This ensures that, after <code class="literal">Pongy</code> actor terminates and its <code class="literal">postStop</code> method completes, <code class="literal">GracefulPingy</code> actor receives a <code class="literal">Terminated</code> message with the actor reference to <code class="literal">Pongy</code> actor.</p><p>Upon receiving the <code class="literal">Terminated</code> message, <code class="literal">GracefulPingy</code> stops itself, as shown in the following <code class="literal">GracefulPingy</code> implementation:</p><pre class="programlisting">class GracefulPingy extends Actor { 
  val pongy = context.actorOf(Props[Pongy], "pongy") 
  context.watch(pongy) 
  def receive = { 
    case "Die, Pingy!" =&gt; 
      context.stop(pongy) 
    case Terminated(`pongy`) =&gt; 
      context.stop(self) 
  } 
} 
</pre><p>Whenever we want to track the termination of an actor from inside an actor, we use DeathWatch, as in the previous example. When we need to wait for the termination of an actor from outside an actor, we use the <span class="emphasis"><em>graceful stop pattern</em></span>. The <code class="literal">gracefulStop</code> method from the <code class="literal">akka.pattern</code> package takes an actor reference, a timeout, and a shutdown message. It returns a future and asynchronously sends the shutdown message to the actor. If the actor terminates within the allotted timeout, the future is successfully completed. Otherwise, the future fails.</p><p>In the following code, we create a <code class="literal">GracefulPingy</code> actor instance and call the <code class="literal">gracefulStop</code> method:</p><pre class="programlisting">object CommunicatingGracefulStop extends App { 
  val grace = ourSystem.actorOf(Props[GracefulPingy], "grace") 
  val stopped = 
    gracefulStop(grace, 3.seconds, "Die, Pingy!") 
  stopped onComplete { 
    case Success(x) =&gt; 
      log("graceful shutdown successful") 
      ourSystem.shutdown() 
    case Failure(t) =&gt; 
      log("grace not stopped!") 
      ourSystem.shutdown() 
  } 
} 
</pre><p>We typically use DeathWatch inside the actors, and the graceful stop pattern in the main application thread. The graceful stop pattern can be used withinÂ actors as well, as long as we are careful that the callbacks on the future returned by the <code class="literal">gracefulStop</code> method do not capture actor state. Together, DeathWatch and the graceful stop pattern allow safely shutting down actor-based programs.</p></div></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec55"></a>Actor supervision</h2></div></div><hr /></div><p>When studying the actor lifecycle, we said that top-level user actors are by default restarted when an exception occurs. We now take a closer inspection at how this works. In Akka, every actor acts as a supervisor for its children. When a child fails, it suspends the processing messages, and sends a message to its parent to decide what to do about the failure. The policy that decides what happens to the parent and the child after the child fails is called the <span class="strong"><strong>supervision strategy</strong></span>. The parent might decide to do the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Restart the actor, indicated with the <code class="literal">Restart</code> message</p></li><li style="list-style-type: disc"><p>Resume the actor without a restart, indicated with the <code class="literal">Resume</code> message</p></li><li style="list-style-type: disc"><p>Permanently stop the actor, indicated with the <code class="literal">Stop</code> message</p></li><li style="list-style-type: disc"><p>Fail itself with the same exception, indicated with the <code class="literal">Escalate</code> message</p></li></ul></div><p>By default, the <code class="literal">user</code> guardian actor comes with a supervision strategy that restarts the failed children actors. User actors stop their children by default. Both supervision strategies can be overridden.</p><p>To override the default supervision strategy in user actors, we override the <code class="literal">supervisorStrategy</code> field of the <code class="literal">Actor</code> class. In the following code, we define a particularly troublesome actor class called <code class="literal">Naughty</code>. When the <code class="literal">Naughty</code> class receives a <code class="literal">String</code> type message, it prints a logging statement. For all other message types, it throws the <code class="literal">RuntimeException</code>, as shown in the following implementation:</p><pre class="programlisting">class Naughty extends Actor { 
  val log = Logging(context.system, this) 
  def receive = { 
    case s: String =&gt; log.info(s) 
    case msg =&gt; throw new RuntimeException 
  } 
  override def postRestart(t: Throwable) = 
    log.info("naughty restarted") 
} 
</pre><p>Next, we declare a <code class="literal">Supervisor</code> actor class, which creates a childÂ actor of the <code class="literal">Naughty</code> type. The <code class="literal">Supervisor</code> actor does not handle any messages, but overrides the default supervision strategy. If a <code class="literal">Supervisor</code> actor's child actor fails because of throwing an <code class="literal">ActorKilledException</code>, it is restarted. However, if its child actor fails with any other exception type, the exception is escalated to the <code class="literal">Supervisor</code> actor. We override the <code class="literal">supervisorStrategy</code> field with the value <code class="literal">OneForOneStrategy</code>, a supervision strategy that applies fault handling specifically to the actor that failed:</p><pre class="programlisting">class Supervisor extends Actor { 
  val child = context.actorOf(Props[StringPrinter], "naughty") 
  def receive = PartialFunction.empty 
  override val supervisorStrategy = 
    OneForOneStrategy() { 
      case ake: ActorKilledException =&gt; Restart 
      case _ =&gt; Escalate 
    } 
} 
</pre><p>We test the new supervisor strategy by creating an actor instance, <code class="literal">super</code>, of the <code class="literal">Supervisor</code> actor class. We then create an actor selection for all the children of <code class="literal">super</code>, and send them a <code class="literal">Kill</code> message. This fails the <code class="literal">Naughty</code> actor, but <code class="literal">super</code> restarts it due to its supervision strategy. We then apologize to the <code class="literal">Naughty</code> actor by sending it a <code class="literal">String</code> message. Finally, we convert a <code class="literal">String</code> message to a list of characters, and send it to the <code class="literal">Naughty</code> actor, which then throws a <code class="literal">RuntimeException</code>. This exception is escalated by <code class="literal">super</code>, and both actors are terminated, as shown in the following code snippet:</p><pre class="programlisting">ourSystem.actorOf(Props[Supervisor], "super") 
ourSystem.actorSelection("/user/super/*") ! Kill 
ourSystem.actorSelection("/user/super/*") ! "sorry about that" 
ourSystem.actorSelection("/user/super/*") ! "kaboom".toList 
</pre><p>In this example, we saw how the <code class="literal">OneForOneStrategy</code> works. When an actor fails, that specific actor is resumed, restarted, or stopped, depending on the exception that caused it to fail. The alternative <code class="literal">AllForOneStrategy</code> applies the fault-handling decision to all the children. When one of the child actors stops, all the other children are resumed, restarted, or stopped.</p><p>Recall our minimalistic web browser implementation from <a class="link" href="#" linkend="ch06">Chapter 6</a>, <span class="emphasis"><em>Concurrent Programming with Reactive Extensions</em></span>. A more advanced web browser requires a separate subsystem that handles concurrent file downloads. Usually, we refer to such a software component as a download manager. We now consider a larger example, in which we apply our knowledge of actors in order to implement the infrastructure for a simple download manager.</p><p>The download manager will be implemented as an actor, represented by the <code class="literal">DownloadManager</code> actor class. The two most important tasks of every download manager are to download the resources at the requested URL, and to track the downloads that are currently in progress. To be able to react to download requests and download completion events, we define the message types <code class="literal">Download</code> and <code class="literal">Finished</code> in the <code class="literal">DownloadManager</code> companion object. The <code class="literal">Download</code> message encapsulates the URL of the resource and the destination file for the resource, while the <code class="literal">Finished</code> message encodes the destination file where the resource is saved:</p><pre class="programlisting">object DownloadManager { 
  case class Download(url: String, dest: String) 
  case class Finished(dest: String) 
} 
</pre><p>The <code class="literal">DownloadManager</code> actor will not execute the downloads itself. Doing so would prevent it from receiving any messages before the download completes. Furthermore, this will serialize different downloads and prevent them from executing concurrently. Thus, the <code class="literal">DownloadManager</code> actor must delegate the task of downloading the files to different actors. We represent these actors with the <code class="literal">Downloader</code> actor class. A <code class="literal">DownloadManager</code> actor maintains a set of <code class="literal">Downloader</code> children, and tracks which children are currently downloading a resource. When a <code class="literal">DownloadManager</code> actor receives a <code class="literal">Download</code> message, it picks one of the non-busy <code class="literal">Downloader</code> actors, and forwards the <code class="literal">Download</code> message to it.</p><p>Once the download is complete, the <code class="literal">Downloader</code> actor sends a <code class="literal">Finished</code> message to its parent. This is illustrated in the following figure:</p><div class="mediaobject"><img src="graphics/image_08_006.jpg" /></div><p>We first show the implementation of the <code class="literal">Downloader</code> actor class. When a <code class="literal">Downloader</code> actor receives a <code class="literal">Download</code> message, it downloads the contents of the specified URL and writes them to a destination file. It then sends the <code class="literal">Finished</code> message back to the sender of the <code class="literal">Download</code> message, as shown in the following implementation:</p><pre class="programlisting">class Downloader extends Actor { 
  def receive = { 
    case DownloadManager.Download(url, dest) =&gt; 
      val content = Source.fromURL(url) 
      FileUtils.write(new java.io.File(dest), content.mkString) 
      sender ! DownloadManager.Finished(dest) 
  } 
} 
</pre><p>The <code class="literal">DownloadManager</code> actor class needs to maintain state to track which of its <code class="literal">Downloader</code> actors is currently downloading a resource. If there are more download requests than there are available <code class="literal">Downloader</code> instances, the <code class="literal">DownloadManager</code> actor needs to enqueue the download requests until a <code class="literal">Downloader</code> actor becomes available. The <code class="literal">DownloadManager</code> actor maintains a <code class="literal">downloaders</code> queue with actor references to non-busy <code class="literal">Downloader</code> actors. It maintains another queue, the <code class="literal">pendingWork</code> queue, with <code class="literal">Download</code> requests that cannot be assigned to any <code class="literal">Downloader</code> instances.</p><p>Finally, it maintains a map called <code class="literal">workItems</code> that associates actor references of the busy <code class="literal">Downloader</code> instances with their <code class="literal">Download</code> requests. This is shown in the following <code class="literal">DownloadManager</code> implementation:</p><pre class="programlisting">class DownloadManager(val downloadSlots: Int) extends Actor { 
  import DownloadManager._ 
  val log = Logging(context.system, this) 
  val downloaders = mutable.Queue[ActorRef]() 
  val pendingWork = mutable.Queue[Download]() 
  val workItems = mutable.Map[ActorRef, Download]() 
  private def checkDownloads(): Unit = { 
    if (pendingWork.nonEmpty &amp;&amp; downloaders.nonEmpty) { 
      val dl = downloaders.dequeue() 
      val item = pendingWork.dequeue() 
      log.info( 
        s"$item starts, ${downloaders.size} download slots left") 
      dl ! item 
      workItems(dl) = item 
    } 
  } 
  def receive = { 
    case msg @ DownloadManager.Download(url, dest) =&gt; 
      pendingWork.enqueue(msg) 
      checkDownloads() 
    case DownloadManager.Finished(dest) =&gt; 
      workItems.remove(sender) 
      downloaders.enqueue(sender) 
      log.info( 
        s"'$dest' done, ${downloaders.size} download slots left") 
      checkDownloads() 
  } 
} 
</pre><p>The <code class="literal">checkDownloads</code> private method maintains the <code class="literal">DownloadManager</code> actor's invariant-the <code class="literal">pendingWork</code> and the <code class="literal">downloaders</code> queue cannot be non-empty at the same time. As soon as both the queues become non-empty, a <code class="literal">Downloader</code> actor reference <code class="literal">dl</code> is dequeued from <code class="literal">downloaders</code> and a <code class="literal">Download</code> request item is dequeued from the <code class="literal">pendingWork</code> queue. The <code class="literal">item</code> value is then sent as a message to the <code class="literal">dl</code> actor, and the <code class="literal">workItems</code> map is updated.</p><p>Whenever the <code class="literal">DownloadManager</code> actor receives a <code class="literal">Download</code> message, it adds it to the <code class="literal">pendingWork</code> queue and calls the <code class="literal">checkDownloads</code> method. Similarly, when the <code class="literal">Finished</code> message arrives, the <code class="literal">Downloader</code> actor is removed from the <code class="literal">workItems</code> queue and enqueued on the <code class="literal">downloaders</code> list.</p><p>To ensure that the <code class="literal">DownloadManager</code> actor is created with the specified number of <code class="literal">Downloader</code> child actors, we override the <code class="literal">preStart</code> method to create the <code class="literal">Downloaders</code> list and add their actor references to the <code class="literal">downloaders</code> queue:</p><pre class="programlisting">override def preStart(): Unit = { 
  for (i &lt;- 0 until downloadSlots) { 
    val dl = context.actorOf(Props[Downloader], s"dl$i") 
    downloaders.enqueue() 
  } 
} 
</pre><p>Finally, we must override the <code class="literal">supervisorStrategy</code> field of the <code class="literal">DownloadManager</code> actor. We use the <code class="literal">OneForOneStrategy</code> field again, but specify that the actor can be restarted or resumed only up to 20 times within a two-second interval.</p><p>We expect that some URLs might be invalid, in which case the actor fails with a <code class="literal">FileNotFoundException</code>. We need to remove such an actor from the <code class="literal">workItems</code> collection and add it back to the <code class="literal">downloaders</code> queue. It does not make sense to restart the <code class="literal">Downloader</code> actors, because they do not contain any state. Instead of restarting, we simply resume a <code class="literal">Downloader</code> actor that cannot resolve a URL. If the <code class="literal">Downloader</code> instances fail due to any other messages, we escalate the exception and fail the <code class="literal">DownloadManager</code> actor, as shown in the following <code class="literal">supervisorStrategy</code> implementation:</p><pre class="programlisting">override val supervisorStrategy = 
  OneForOneStrategy( 
    maxNrOfRetries = 20, withinTimeRange = 2 seconds 
  ) { 
    case fnf: java.io.FileNotFoundException =&gt; 
      log.info(s"Resource could not be found: $fnf") 
      workItems.remove(sender) 
      downloaders.enqueue(sender) 
      Resume // ignores the exception and resumes the actor 
    case _ =&gt; 
      Escalate 
  } 
</pre><p>To test the download manager, we create a <code class="literal">DownloadManager</code> actor with four download slots, and send it several <code class="literal">Download</code> messages:</p><pre class="programlisting">  val downloadManager = 
    ourSystem.actorOf(Props(classOf[DownloadManager], 4), "man") 
  downloadManager ! Download( 
    "http://www.w3.org/Addressing/URL/url-spec.txt", 
    "url-spec.txt")   
</pre><p>An extra copy of the URL specification cannot hurt, so we download it to our computer. The download manager logs that there are only three download slots left. Once the download completes, the download manager logs that there are four remaining download slots again. We then decide that we would like to contribute to the Scala programming language, so we download the <code class="literal">README</code> file from the official Scala repository. Unfortunately, we enter an invalid URL, and observe a warning from the download manager saying that the resource cannot be found:</p><pre class="programlisting">downloadManager ! Download( 
  "https://github.com/scala/scala/blob/master/README.md", 
  "README.md") 
</pre><p>The simple implementation of the basic actor-based download manager illustrates both how to achieve concurrency by delegating work to child actors, and how to treat failures in child actors. Delegating work is important, both for decomposing the program into smaller, isolated components, and to achieve better throughput and scalability. Actor supervision is the fundamental mechanism for handling failures in isolated components implemented in separate actors.</p></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec56"></a>Remote actors</h2></div></div><hr /></div><p>So far in this book, we have mostly concentrated on writing programs on a single computer. Concurrent programs are executed within a single process on one computer, and they communicate using shared memory. Seemingly, actors described in this chapter communicate by passing messages. However, the message passing used throughout this chapter is implemented by reading and writing to shared memory under the hood.</p><p>In this section, we study how the actor model ensures location transparency by taking existing actors and deploying them in a distributed program. We take two existing actor implementations, namely, <code class="literal">Pingy</code> and <code class="literal">Pongy</code>, and deploy them inside different processes. We will then instruct the <code class="literal">Pingy</code> actor to send a message to the <code class="literal">Pongy</code> actor, as before, and wait until the <code class="literal">Pingy</code> actor returns the <code class="literal">Pongy</code> actor's message. The message exchange will occur transparently, although the <code class="literal">Pingy</code> and <code class="literal">Pongy</code> actor's were previously implemented without knowing that they might exist inside separate processes, or even different computers.</p><p>The Akka actor framework is organized into several modules. To use the part of Akka that allows communicating with actors in remote actor systems, we need to add the following dependency to our build definition file:</p><pre class="programlisting">libraryDependencies += "com.typesafe.akka" %% "akka-remote" % "2.3.2" 
</pre><p>Before creating our ping-pong actors inside two different processes, we need to create an actor system that is capable of communicating with remote actors. To do this, we create a custom actor system configuration string. The actor system configuration string can be used to configure a range of different actor system properties; we are interested in using a custom <code class="literal">ActorRef</code> factory object called <code class="literal">RemoteActorRefProvider</code>. The <code class="literal">ActorRef</code> factory object allows the actor system to create actor references that can be used to communicate over the network. Furthermore, we configure the actor system toÂ use the <span class="strong"><strong>Netty</strong></span> networking library with the TCP network layer and the desired TCP port number. We declare the <code class="literal">remotingConfig</code> method for this task:</p><pre class="programlisting">import com.typesafe.config._ 
def remotingConfig(port: Int) = ConfigFactory.parseString(s""" 
akka { 
  actor.provider = "akka.remote.RemoteActorRefProvider" 
  remote { 
    enabled-transports = ["akka.remote.netty.tcp"] 
    netty.tcp { 
      hostname = "127.0.0.1" 
      port = $port 
    } 
  } 
} 
""") 
</pre><p>We then define a <code class="literal">remotingSystem</code> factory method that creates an actor system object using the given name and port. We use the <code class="literal">remotingConfig</code> method, defined earlier, to produce the configuration object for the specified network port:</p><pre class="programlisting">def remotingSystem(name: String, port: Int): ActorSystem = 
  ActorSystem(name, remotingConfig(port)) 
</pre><p>Now we are ready to create the <code class="literal">Pongy</code> actor system. We declare an application called <code class="literal">RemotingPongySystem</code>, which instantiates an actor system called <code class="literal">PongyDimension</code> using the network port <code class="literal">24321</code>. We arbitrarily picked a network port that was free on our machine. If the creation of the actor system fails because the port is not available, you can pick a different port in the range <code class="literal">1024</code> to <code class="literal">65535</code>. Make sure that you don't have a firewall running, as it can block the network traffic for arbitrary applications.</p><p>The <code class="literal">RemotingPongySystem</code> application is shown in the following example:</p><pre class="programlisting">object RemotingPongySystem extends App { 
  val system = remotingSystem("PongyDimension", 24321) 
  val pongy = system.actorOf(Props[Pongy], "pongy") 
  Thread.sleep(15000) 
  system.shutdown() 
} 
</pre><p>The <code class="literal">RemotingPongySystem</code> application creates a <code class="literal">Pongy</code> actor and shuts down after 15 seconds. After we start it, we will only have a short time to start another application running the <code class="literal">Pingy</code> actor. We will call this second application <code class="literal">RemotingPingySystem</code>. Before we implement it, we create another actor called <code class="literal">Runner</code>, which will instantiate <code class="literal">Pingy</code>, obtain the <code class="literal">Pongy</code> actor's reference, and give it to the <code class="literal">Pingy</code> actor; recall that the ping-pong game from the earlier section starts when the <code class="literal">Pingy</code> actor obtains the <code class="literal">Pongy</code> actor's reference.</p><p>When the <code class="literal">Runner</code> actor receives a <code class="literal">start</code> message, it constructs the actor path for the <code class="literal">Pongy</code> actor. We use the <code class="literal">akka.tcp</code> protocol and the name of the remote actor system, along with its IP address and port number. The <code class="literal">Runner</code> actor sends an <code class="literal">Identify</code> message to the actor selection in order to obtain the actor reference to the remote <code class="literal">Pongy</code> instance. The complete <code class="literal">Runner</code> implementation is shown in the following code snippet:</p><pre class="programlisting">class Runner extends Actor { 
  val log = Logging(context.system, this) 
  val pingy = context.actorOf(Props[Pingy], "pingy") 
  def receive = { 
    case "start" =&gt; 
      val pongySys = "akka.tcp://PongyDimension@127.0.0.1:24321" 
      val pongyPath = "/user/pongy" 
      val url = pongySys + pongyPath 
      val selection = context.actorSelection(url) 
      selection ! Identify(0) 
    case ActorIdentity(0, Some(ref)) =&gt; 
      pingy ! ref 
    case ActorIdentity(0, None) =&gt; 
      log.info("Something's wrong - ain't no pongy anywhere!") 
      context.stop(self) 
    case "pong" =&gt; 
      log.info("got a pong from another dimension.") 
      context.stop(self) 
  } 
} 
</pre><p>Once the <code class="literal">Runner</code> actor sends the <code class="literal">Pongy</code> actor reference to <code class="literal">Pingy</code>, the game ofÂ remote ping pong can begin. To test it, we declare the <code class="literal">RemotingPingySystem</code> application, which starts the <code class="literal">Runner</code> actor and sends it a <code class="literal">start</code> message:</p><pre class="programlisting">object RemotingPingySystem extends App { 
  val system = remotingSystem("PingyDimension", 24567) 
  val runner = system.actorOf(Props[Runner], "runner") 
  runner ! "start" 
  Thread.sleep(5000) 
  system.shutdown() 
} 
</pre><p>We now need to start the <code class="literal">RemotingPongySystem</code> application, and the <code class="literal">RemotingPingySystem</code> application after that; we only have 15 seconds until the <code class="literal">RemotingPongySystem</code> application shuts itself down. The easiest way to do this is to start two SBT instances in your project folder and run the two applications at the same time. After the <code class="literal">RemotingPingySystem</code> application starts, we soon observe a <code class="literal">pong</code> message from another dimension.</p><p>In the previous example, the actor system configuration and the <code class="literal">Runner</code> actor were responsible for setting up the network communication, and were not location-transparent. This is typically the case with distributed programs; a part of the program is responsible for initializing and discovering actors within remote actor systems, while the application-specific logic is confined within separate actors.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip99"></a>Tip</h3><p>In larger actor programs, separate deployment logic from application logic.</p></div><p>To summarize, remote actor communication requires the following steps:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Declaring an actor system with an appropriate remoting configuration</p></li><li style="list-style-type: disc"><p>Starting two actor systems in separate processes or on separate machines</p></li><li style="list-style-type: disc"><p>Using actor path selection to obtain actor references</p></li><li style="list-style-type: disc"><p>Using actor references to transparently send messages</p></li></ul></div><p>While the first three steps are not location-transparent, the application logic is usually confined within the fourth step, as we saw in this section. This is important, as it allows separating the deployment logic from the application semantics and building distributed systems that can be deployed transparently to different network configurations.</p></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec57"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we learned what actors are and how to use them to build concurrent programs. Using the Akka actor framework, we studied how to create actors, organize them into hierarchies, manage their lifecycle, and recover them from errors. We examined important patterns in actor communication and learned how to model actor behavior. Finally, we saw how the actor model can ensure location transparency, and serve as a powerful tool to seamlessly build distributed systems.</p><p>Still, there are many Akka features that we omitted in this chapter. Akka comes with detailed online documentation, which is one of the best sources of information on Akka. To obtain an in-depth understanding of distributed programming, we recommend the books <span class="emphasis"><em>Distributed Algorithms</em></span>, <span class="emphasis"><em>Nancy A. Lynch</em></span>, published by Elsevier and <span class="emphasis"><em>Introduction to Reliable and Secure Distributed Programming</em></span>, <span class="emphasis"><em>Christian Cachin</em></span>,<span class="emphasis"><em> Rachid Guerraoui, Luis Rodrigues</em></span>, published by Springer.</p><p>In the following chapter, we will summarize the different concurrency libraries we learned about in this book, examine the typical use cases for each of them, and see how they work together in larger applications.</p></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec58"></a>Exercises</h2></div></div><hr /></div><p>The following exercises test your understanding of the actor programming model, and distributed programming in general. The first few exercises are straightforward, and deal with the basics of the actor API in Akka. Subsequent exercises are more involved, and go deeper into the territory of fault-tolerant distributed programming. Try to solve these exercises by first assuming that no machines fail, and then consider what happens if some of the machines fail during the execution of the program:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Implement the timer actor with the <code class="literal">TimerActor</code> class. After receiving a <code class="literal">Register</code> message containing the <code class="literal">t</code> timeout in milliseconds, the timer actor sends a <code class="literal">Timeout</code> message back after <code class="literal">t</code> milliseconds. The timer must accept multiple <code class="literal">Register</code> messages.</p></li><li><p>Recall the bank account example from <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Concurrency on the JVM and the Java Memory Model</em></span>. Implement different bank accounts as separate actors, represented by the <code class="literal">AccountActor</code> class. When an <code class="literal">AccountActor</code> class receives a <code class="literal">Send</code> message, it must transfer the specified amount of money to the target actor. What will happen if either of the actors receives a <code class="literal">Kill</code> message at any point during the money transaction?</p></li><li><p>Implement the <code class="literal">SessionActor</code> class for actors that control access to other actors:
</p><pre class="programlisting">        class SessionActor(password: String, r: ActorRef) 
        extends Actor { 
          def receive = ??? 
        } 
</pre><p>
</p><p>After the <code class="literal">SessionActor</code> instance receives the <code class="literal">StartSession</code> message with the correct password, it forwards all the messages to the actor reference <code class="literal">r</code>, until it receives the <code class="literal">EndSession</code> message. Use behaviors to model this actor.</p></li><li><p>Use actors to implement the <code class="literal">ExecutionContext</code> interface, described in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Traditional Building Blocks of Concurrency</em></span>.</p></li><li><p>Implement the <code class="literal">FailureDetector</code> actor, which sends <code class="literal">Identify</code> messages to the specified actors every <code class="literal">interval</code> seconds. If an actor does not reply with any <code class="literal">ActorIdentity</code> messages within <code class="literal">threshold</code> seconds, the <code class="literal">FailureDetector</code> actor sends a <code class="literal">Failed</code> message to its parent actor, which contains the actor reference of the failed actor.</p></li><li><p>A distributed hash map is a collection distributed across multiple computers, each of which contains part of the data, called a <span class="strong"><strong>shard</strong></span>. When there are <code class="literal">2^n</code> shards, the first <code class="literal">n</code> bits of the hash code of the key are used to decide which shard a key-value pair should go to. Implement the distributed hash map with the <code class="literal">DistributedMap</code> class:</p><pre class="programlisting">        class DistributedMap[K, V](shards: ActorRef*) { 
          def update(key: K, value: V): Future[Unit] = ??? 
          def get(key: K): Future[Option[V]] = ??? 
        } 
</pre><p>
</p><p>The <code class="literal">DistributedMap</code> class takes a list of actor references to the <code class="literal">ShardActor</code> instances, whose actor template you also need to implement. You might assume that the length of the <code class="literal">shards</code> list is a power of two. The <code class="literal">update</code> and <code class="literal">get</code> methods are asynchronous, and return the result in a future object.</p></li><li><p>Implement an abstract <code class="literal">BroadcastActor</code> class, which defines the <code class="literal">broadcast</code> method:
</p><pre class="programlisting">        def broadcast(refs: ActorRef*)(msg: Any): Unit = ??? 
</pre><p>
</p><p>The <code class="literal">broadcast</code> method sends the <code class="literal">msg</code> message to all the actors specified in the <code class="literal">refs</code> list. The actor invoking the <code class="literal">broadcast</code> method might, for reasons such as power loss, fail at any point during the execution of the <code class="literal">broadcast</code> method. Nevertheless, the <code class="literal">broadcast</code> method must have <span class="strong"><strong>reliable delivery</strong></span>: if at least one actor from the <code class="literal">refs</code> list receives the <code class="literal">msg</code> message, then all the actors from the <code class="literal">refs</code> list must eventually receive <code class="literal">msg</code>.</p></li><li><p>Implement a <code class="literal">FlowRateActor</code> class for an actor that forwards incoming messages to a target actor. This actor must ensure that the number of messages forwarded per second does not exceed a rate specified in its constructor.</p></li><li><p>Implement a <code class="literal">Sequencer</code> actor, which forwards messages to the target actor. If the message is a two-element tuple where the first element is a <code class="literal">Long</code> value, then the <code class="literal">Long</code> value is interpreted as a sequence number. All such messages must be forwarded in the proper sequence number order, starting from number <code class="literal">0</code>.</p></li><li><p>Implement a <code class="literal">MasterWorker[T]</code> actor that, given a number of worker parameters, creates a set of worker actors and forwards task messages of type <code class="literal">() =&gt; T</code> to those workers. When the worker actors complete a task, they send the result back to the <code class="literal">MasterWorker</code> actor, which sends the reply back to the client actor that originally sent the task.</p></li></ol></div></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch09"></a>ChapterÂ 9.Â Concurrency in Practice</h2></div></div></div><div class="blockquote"><table border="0" width="100%" cellspacing="0" cellpadding="0" class="blockquote" summary="Block quote"><tr><td width="10%" valign="top">Â </td><td width="80%" valign="top"><p><span class="emphasis"><em>"The best theory is inspired by practice."</em></span></p></td><td width="10%" valign="top">Â </td></tr><tr><td width="10%" valign="top">Â </td><td colspan="2" align="right" valign="top">--<span class="attribution"><span class="emphasis"><em>Donald Knuth</em></span></span></td></tr></table></div><p>We have studied a plethora of different concurrency facilities in this book. By now, you will have learned about dozens of different ways of starting concurrent computations and accessing shared data. Knowing how to use different styles of concurrency is useful, but it might not yet be obvious when to use which.</p><p>The goal of this final chapter is to introduce the big picture of concurrent programming. We will study the use cases for various concurrency abstractions, see how to debug concurrent programs, and how to integrate different concurrency libraries in larger applications. In this chapter, we will perform the following tasks:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Summarize the characteristics and typical uses of different concurrency frameworks introduced in the earlier chapters</p></li><li style="list-style-type: disc"><p>Investigate how to deal with various kinds of bugs appearing in concurrent applications</p></li><li style="list-style-type: disc"><p>Learn how to identify and resolve performance bottlenecks</p></li><li style="list-style-type: disc"><p>Apply the previous knowledge about concurrency to implement a larger concurrent application, namely, a remote file browser</p></li></ul></div><p>We start with an overview of the important concurrency frameworks we have learned about in this book, and a summary of when to use each of them.</p></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec59"></a>Choosing the right tools for the job</h2></div></div><hr /></div><p>In this section, we present an overview of the different concurrency libraries that we learned about. We take a step back and look at the differences between these libraries, and what they have in common. This summary will give us an insight into what different concurrency abstractions are useful for.</p><p>A concurrency framework usually needs to address several concerns:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>It must provide a way to declare data that is shared between concurrent executions</p></li><li style="list-style-type: disc"><p>It must provide constructs for reading and modifying program data</p></li><li style="list-style-type: disc"><p>It must be able to express conditional execution, triggered when a certain set of conditions are fulfilled</p></li><li style="list-style-type: disc"><p>It must define a way to start concurrent executions</p></li></ul></div><p>Some of the frameworks from this book address all these concerns; others address only a subset, and transfer part of the responsibility to another framework.</p><p>Typically, in a concurrent programming model, we express concurrently shared data differently from data intended to be accessed only from a single thread. This allows the JVM runtime to optimize sequential parts of the program more effectively. So far, we've seen a lot of different ways to express concurrently shared data, ranging from the low-level facilities to advanced high-level abstractions. We summarize different data abstractions in the following table:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /><col /></colgroup><thead><tr><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Data abstraction</strong></span></p>
</th><th style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Datatype or annotation</strong></span></p>
</th><th style="border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Description</strong></span></p>
</th></tr></thead><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Volatile variables</p><p>
</p><p>(JDK)</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>@volatile</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Ensures visibility and the happens-before relationship on class fields and local variables that are captured in closures.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Atomic variables</p><p>
</p><p>(JDK)</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>AtomicReference[T]</p><p>
</p><p>AtomicInteger</p><p>
</p><p>AtomicLong</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Provide basic composite atomic operations, such as <span class="strong"><strong>
<code class="literal">compareAndSet</code>
</strong></span> and <span class="strong"><strong>
<code class="literal">incrementAndGet</code>
</strong></span>.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Futures and promises</p><p>
</p><p>(<span class="strong"><strong>
<code class="literal">scala.concurrent</code>
</strong></span>)</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Future[T]</p><p>
</p><p>Promise[T]</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Sometimes called single-assignment variables, these express values that might not be computed yet but will eventually become available.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Observables and subjects</p><p>
</p><p>(Rx)</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Observable[T]</p><p>
</p><p>Subject[T]</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Also known as first-class event streams, these describe many different values that arrive one after another in time.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; ">
<p>Transactional references</p><p>
</p><p>(ScalaSTM)</p>
</td><td style="border-right: 0.5pt solid ; ">
<p>Ref[T]</p>
</td><td style="">
<p>These describe memory locations that can only be accessed from within memory transactions. Their modifications only become visible after the transaction successfully commits.</p>
</td></tr></tbody></table></div><p>The next important concern is providing access to shared data, which includes reading and modifying shared memory locations. Usually, a concurrent program uses special constructs to express such accesses. We summarize the different data access constructs in the following table:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Data abstraction</strong></span></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Data access constructs</strong></span></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Description</strong></span></p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Arbitrary data</p><p>
</p><p>(JDK)</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>synchronized</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Uses intrinsic object locks to exclude access to arbitrary shared data.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Atomic variables and classes</p><p>
</p><p>(JDK)</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>compareAndSet</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Atomically exchanges the value of a single memory location. It allows implementing lock-free programs.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Futures and promises</p><p>
</p><p>(<span class="strong"><strong>
<code class="literal">scala.concurrent</code>
</strong></span>)</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>value</p><p>
</p><p>tryComplete</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Used to assign a value to a promise, or to check the value of the corresponding future. The <span class="strong"><strong>
<code class="literal">value</code>
</strong></span> method is not a preferred way to interact with a future.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; ">
<p>Transactional references and classes</p><p>
</p><p>(Scala STM)</p>
</td><td style="border-right: 0.5pt solid ; ">
<p>atomic</p><p>
</p><p>orAtomic</p><p>
</p><p>single</p>
</td><td style="">
<p>Atomically modifies the values of a set of memory locations. Reduces the risk of deadlocks, but disallows side effects inside the transactional block.</p>
</td></tr></tbody></table></div><p>Concurrent data access is not the only concern of a concurrency framework. As we have learned in previous chapters, concurrent computations sometimes need to proceed only after a certain condition is met. In the following table, we summarize different constructs that enable this:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Concurrency framework</strong></span></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Conditional execution constructs</strong></span></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Description</strong></span></p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>JVM concurrency</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>wait</p><p>
</p><p>notify</p><p>
</p><p>notifyAll</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Used to suspend the execution of a thread until some other thread notifies that the conditions are met.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Futures and promises</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>onComplete</p><p>
</p><p>Await.ready</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Conditionally schedules an asynchronous computation. The <span class="strong"><strong>
<code class="literal">Await.ready</code>
</strong></span> method suspends the thread until the future completes.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Reactive extensions</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>subscribe</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Asynchronously or synchronously executes a computation when an event arrives.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Software transactional memory</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>retry</p><p>
</p><p>retryFor</p><p>
</p><p>withRetryTimeout</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Retries the current memory transaction when some of the relevant memory locations change.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; ">
<p>Actors</p>
</td><td style="border-right: 0.5pt solid ; ">
<p>receive</p>
</td><td style="">
<p>Executes the actor's <span class="strong"><strong>
<code class="literal">receive</code>
</strong></span> block when a message arrives.</p>
</td></tr></tbody></table></div><p>Finally, a concurrency model must define a way to start a concurrent execution. We summarize different concurrency constructs in the following table:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col /><col /><col /></colgroup><tbody><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Concurrency framework</strong></span></p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Concurrency constructs</strong></span></p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p><span class="strong"><strong>Description</strong></span></p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>JVM concurrency</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Thread.start</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Starts a new thread of execution.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Execution contexts</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>execute</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Schedules a block of code for execution on a thread pool.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Futures and promises</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Future.apply</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Schedules a block of code for execution, and returns the future value with the result of the execution.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Parallel collections</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>par</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>Allows invoking data-parallel versions of collection methods.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Reactive extensions</p>
</td><td style="border-right: 0.5pt solid ; border-bottom: 0.5pt solid ; ">
<p>Observable.create</p><p>
</p><p>observeOn</p>
</td><td style="border-bottom: 0.5pt solid ; ">
<p>The <span class="strong"><strong>
<code class="literal">create</code>
</strong></span> method defines an event source. The <span class="strong"><strong>
<code class="literal">observeOn</code>
</strong></span> method schedules the handling of events on different threads.</p>
</td></tr><tr><td style="border-right: 0.5pt solid ; ">
<p>Actors</p>
</td><td style="border-right: 0.5pt solid ; ">
<p>actorOf</p>
</td><td style="">
<p>Schedules a new actor object for execution.</p>
</td></tr></tbody></table></div><p>This breakdown shows us that different concurrency libraries focus on different tasks. For example, parallel collections do not have conditional waiting constructs, because a data-parallel operation proceeds on separate elements independently. Similarly, software transactional memory does not come with a construct to express concurrent computations, and focuses only on protecting access to shared data. Actors do not have special constructs for modeling shared data and protecting access to it, because data is encapsulated within separate actors and accessed serially only by the actor that owns it.</p><p>Having classified concurrency libraries according to how they model shared data and express concurrency, we present a summary of what different concurrency libraries are good for:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The classical JVM concurrency model uses threads, the <code class="literal">synchronized</code> statement, volatile variables, and atomic primitives for low-level tasks. Uses include implementing a custom concurrency utility, a concurrent data structure, or a concurrency framework optimized for specific tasks.</p></li><li style="list-style-type: disc"><p>Futures and promises are best suited for referring to concurrent computations that produce a single result value. Futures model latency in the program, and allow composing values that become available later during the execution of the program. Uses include performing remote network requests and waiting for replies, referring to the result of an asynchronous long-running computation, or reacting to the completion of an I/O operation. Futures are usually the glue of a concurrent application, binding the different parts of a concurrent program together. We often use futures to convert single-event callback APIs into a standardized representation based on the <code class="literal">Future</code> type.</p></li><li style="list-style-type: disc"><p>Parallel collections are best suited for efficiently executing data-parallel operations on large datasets. Uses include file searching, text processing, linear algebra applications, numerical computations, and simulations. Long-running Scala collection operations are usually good candidates for parallelization.</p></li><li style="list-style-type: disc"><p>Reactive extensions are used to express asynchronous event-based programs. Unlike parallel collections, in reactive extensions, data elements are not available when the operation starts, but arrive while the application is running. Uses include converting callback-based APIs, modeling events in user interfaces, modeling events external to the application, manipulating program events with collection-style combinators, streaming data from input devices or remote locations, or incrementally propagating changes in the data model throughout the program.</p></li><li style="list-style-type: disc"><p>Use STM to protect program data from getting corrupted by concurrent accesses. An STM allows building complex data models and accessing them with the reduced risk of deadlocks and race conditions. A typical use is to protect concurrently accessible data while retaining good scalability between threads whose accesses to data do not overlap.</p></li><li style="list-style-type: disc"><p>Actors are suitable for encapsulating concurrently accessible data, and seamlessly building distributed systems. Actor frameworks provide a natural way to express concurrent tasks that communicate by explicitly sending messages. Uses include serializing concurrent access to data to prevent corruption, expressing stateful concurrency units in the system, and building distributed applications such as trading systems, P2P networks, communication hubs, or data-mining frameworks.</p></li></ul></div><p>Advocates of specific programming languages, libraries, or frameworks might try to convince you that their technology is the best for any task and any situation, often with the intent of selling it. Richard Stallman once said that "computer science is the only industry more fashion-driven than women's fashion." As engineers, we need to know better than to succumb to programming fashion and marketing propaganda. Different frameworks are tailored towards specific use cases, and the correct way to choose a technology is to carefully weigh its advantages and disadvantages when applied to a specific situation.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip100"></a>Tip</h3><p>There is no one-size-fits-all technology. Use your own best judgment when deciding which concurrency framework to use for a specific programming task.</p></div><p>Sometimes, choosing the best-suited concurrency utility is easier said than done. It takes a great deal of experience to choose the correct technology. In many cases, we do not even know enough about the requirements of the system to make an informed decision. Regardless, a good rule of thumb is to apply several concurrency frameworks to different parts of the same application, each best suited for a specific task. Often, the real power of different concurrency frameworks becomes apparent when they are used together. This is the topic we will cover in the following section.</p></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec60"></a>Putting it all together - a remote file browser</h2></div></div><hr /></div><p>In this section, we use our knowledge about different concurrency frameworks to build a remote file browser. This larger application example illustrates how different concurrency libraries work together, and how to apply them to different situations. We will name our remote file browser ScalaFTP.</p><p>The ScalaFTP browser is divided into two main components: the server and the client process. The server process will run on the machine whose filesystem we want to manipulate. The client will run on our own computer, and comprise of a graphical user interface used to navigate the remote filesystem. To keep things simple, the protocol that the client and the server will use to communicate will not really be FTP, but a custom communication protocol. By choosing the correct concurrency libraries to implement different parts of ScalaFTP, we will ensure that the complete ScalaFTP implementation fits inside just 500 lines of code.</p><p>Specifically, the ScalaFTP browser will implement the following features:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Displaying the names of the files and the directories in a remote filesystem, and allowing navigation through the directory structure</p></li><li style="list-style-type: disc"><p>Copying files between directories in a remote filesystem</p></li><li style="list-style-type: disc"><p>Deleting files in a remote filesystem</p></li></ul></div><p>To implement separate pieces of this functionality, we will divide the ScalaFTP server and client programs into layers. The task of the server program is to answer to incoming copy and delete requests, and to answer queries about the contents of specific directories. To make sure that its view of the filesystem is consistent, the server will cache the directory structure of the filesystem. We divide the server program into two layers: the filesystem API and the server interface. The filesystem API will expose the data model of the server program, and define useful utility methods to manipulate the filesystem. The server interface will receive requests and send responses back to the client.</p><p>Since the server interface will require communicating with the remote client, we decide to use the Akka actor framework. Akka comes with remote communication facilities, as we learned in <a class="link" href="#" linkend="ch08">Chapter 8</a>, <span class="emphasis"><em>Actors</em></span>. The contents of the filesystem, that is, its state, will change over time. We are therefore interested in choosing proper constructs for data access.</p><p>In the filesystem API, we can use object monitors and locking to synchronize access to shared state, but we will avoid these due to the risk of deadlocks. We similarly avoid using atomic variables, because they are prone to race conditions. We could encapsulate the filesystem state within an actor, but note that this can lead to a scalability bottleneck: an actor would serialize all accesses to the filesystem state. Therefore, we decide to use the ScalaSTM framework to model the filesystem contents. An STM avoids the risk of deadlocks and race conditions, and ensures good horizontal scalability, as we learned in <a class="link" href="#" linkend="ch07">Chapter 7</a>, <span class="emphasis"><em>Software Transactional Memory</em></span>.</p><p>The task of the client program will be to graphically present the contents of the remote filesystem, and communicate with the server. We divide the client program into three layers of functionality. The GUI layer will render the contents of the remote filesystem and register user requests, such as button clicks. We will implement the GUI using the Swing and Rx frameworks, similarly to how we implemented the web browser in <a class="link" href="#" linkend="ch06">Chapter 6</a>, <span class="emphasis"><em>Concurrent Programming with Reactive Extensions</em></span>. The client API will replicate the server interface on the client side and communicate with the server. We will use Akka to communicate with the server, but expose the results of remote operations as futures. Finally, the client logic will be a gluing layer, which binds the GUI and the client API together.</p><p>The architecture of the ScalaFTP browser is illustrated in the following diagram, in which we indicate which concurrency libraries will be used by separate layers. The dashed line represents the communication path between the client and the server:</p><div class="mediaobject"><img src="graphics/image_09_001.jpg" /></div><p>We now start by implementing the ScalaFTP server, relying on the bottom-up design approach. In the following section, we will describe the internals of the filesystem API.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec80"></a>Modeling the filesystem</h3></div></div></div><p>In <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Traditional Building Blocks of Concurrency</em></span>, we used atomic variables and concurrent collections to implement a non-blocking, thread-safe filesystem API, which allowed copying files and retrieving snapshots of the filesystem. In this section, we repeat this task using STM. We will see that it is much intuitive and less error-prone to use STM.</p><p>We start by defining the different states that a file can be in. As in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Traditional Building Blocks of Concurrency</em></span>, the file can be currently created, in the idle state, being copied, or being deleted. We model this with a sealed <code class="literal">State</code> trait, and its four cases:</p><pre class="programlisting">sealed trait State
case object Created extends State
case object Idle extends State
case class Copying(n: Int) extends State
case object Deleted extends State
</pre><p>A file can only be deleted if it is in the idle state, and it can only be copied if it is in the idle state or in the copied state. Since a file can be copied to multiple destinations at a time, the <code class="literal">Copying</code> state encodes how many copies are currently under way. We add the methods <code class="literal">inc</code> and <code class="literal">dec</code> to the <code class="literal">State</code> trait, which return a new state with one more or one fewer copy, respectively. For example, the implementation of <code class="literal">inc</code> and <code class="literal">dec</code> for the <code class="literal">Copying</code> state is as follows:</p><pre class="programlisting">def inc: State = Copying(n + 1)
def dec: State = if (n &gt; 1) Copying(n - 1) else Idle
</pre><p>Similar to the <code class="literal">File</code> class in the <code class="literal">java.io</code> package, we represent both the files and directories with the same entity, and refer to them more generally as files. Each file is represented by the <code class="literal">FileInfo</code> class that encodes the path, its name, its parent directory and the date of the last modification to the file, a Boolean value denoting if the file is a directory, the size of the file, and its <code class="literal">State</code> object. The <code class="literal">FileInfo</code> class is immutable, and updating the state of the file will require creating a fresh <code class="literal">FileInfo</code> object:</p><pre class="programlisting">case class FileInfo(path: String, name: String,
  parent: String, modified: String, isDir: Boolean,
  size: Long, state: State)
</pre><p>We separately define the factory methods <code class="literal">apply</code> and <code class="literal">creating</code> that take a <code class="literal">File</code> object and return a <code class="literal">FileInfo</code> object in the <code class="literal">Idle</code> or <code class="literal">Created</code> state, respectively.</p><p>Depending on where the server is started, the root of the ScalaFTP directory structure is a different subdirectory in the actual filesystem. A <code class="literal">FileSystem</code> object tracks the files in the given <code class="literal">rootpath</code> directory, using a transactional map called <code class="literal">files</code>:</p><pre class="programlisting">class FileSystem(val rootpath: String) {
  val files = TMap[String, FileInfo]()
}
</pre><p>We introduce a separate <code class="literal">init</code> method to initialize the <code class="literal">FileSystem</code> object. The <code class="literal">init</code> method starts a transaction, clears the contents of the <code class="literal">files</code> map, and traverses the files and directories under <code class="literal">rootpath</code> using the Apache Commons IO library. For each file and directory, the <code class="literal">init</code> method creates a <code class="literal">FileInfo</code> object and adds it to the <code class="literal">files</code> map, using its path as the key:</p><pre class="programlisting">def init() = atomic { implicit txn =&gt;
  files.clear()
  val rootDir = new File(rootpath)
  val all = TrueFileFilter.INSTANCE
  val fileIterator =
    FileUtils.iterateFilesAndDirs(rootDir, all, all).asScala
  for (file &lt;- fileIterator) {
    val info = FileInfo(file)
    files(info.path) = info
  }
}
</pre><p>Recall that the ScalaFTP browser must display the contents of the remote filesystem. To enable directory queries, we first add the <code class="literal">getFileList</code> method to the <code class="literal">FileSystem</code> class, which retrieves the files in the specified <code class="literal">dir</code> directory. The <code class="literal">getFileList</code> method starts a transaction and filters the files whose direct parent is equal to <code class="literal">dir</code>:</p><pre class="programlisting">def getFileList(dir: String): Map[String, FileInfo] =
  atomic { implicit txn =&gt;
    files.filter(_._2.parent == dir)
  }
</pre><p>We implement the copying logic in the filesystem API with the <code class="literal">copyFile</code> method. This method takes a path to the <code class="literal">src</code> source file and the <code class="literal">dest</code> destination file, and starts a transaction. After checking whether the <code class="literal">dest</code> destination file exists or not, the <code class="literal">copyFile</code> method inspects the state of the source file entry, and fails unless the state is <code class="literal">Idle</code> or <code class="literal">Copying</code>. It then calls <code class="literal">inc</code> to create a new state with the increased copy count, and updates the source file entry in the <code class="literal">files</code> map with the new state. Similarly, the <code class="literal">copyFile</code> method creates a new entry for the destination file in the <code class="literal">files</code> map. Finally, the <code class="literal">copyFile</code> method calls the <code class="literal">afterCommit</code> handler to physically copy the file to disk after the transaction completes. Recall that it is not legal to execute side-effecting operations from within the transaction body, so the private <code class="literal">copyOnDisk</code> method is called only after the transaction commits:</p><pre class="programlisting">def copyFile(src: String, dest: String) = atomic { implicit txn =&gt;
  val srcfile = new File(src)
  val destfile = new File(dest)
  val info = files(src)
  if (files.contains(dest)) sys.error(s"Destination exists.")
  info.state match {
    case Idle | Copying(_) =&gt;
      files(src) = info.copy(state = info.state.inc)
      files(dest) = FileInfo.creating(destfile, info.size)
      Txn.afterCommit { _ =&gt; copyOnDisk(srcfile, destfile) }
      src
  }
}
</pre><p>The <code class="literal">copyOnDisk</code> method calls the <code class="literal">copyFile</code> method on the <code class="literal">FileUtils</code> class from the Apache Commons IO library. After the file transfer completes, the <code class="literal">copyOnDisk</code> method starts another transaction, in which it decreases the copy count of the source file and sets the state of the destination file to <code class="literal">Idle</code>:</p><pre class="programlisting">private def copyOnDisk(srcfile: File, destfile: File) = {
  FileUtils.copyFile(srcfile, destfile)
  atomic { implicit txn =&gt;
    val ninfo = files(srcfile.getPath)
    files(srcfile.getPath) = ninfo.copy(state = ninfo.state.dec)
    files(destfile.getPath) = FileInfo(destfile)
  }
}
</pre><p>The <code class="literal">deleteFile</code> method deletes a file in a similar way. It changes the file state to <code class="literal">Deleted</code>, deletes the file, and starts another transaction to remove the file entry:</p><pre class="programlisting">def deleteFile(srcpath: String): String = atomic { implicit txn =&gt;
  val info = files(srcpath)
  info.state match {
    case Idle =&gt;
      files(srcpath) = info.copy(state = Deleted)
      Txn.afterCommit { _ =&gt;
        FileUtils.forceDelete(info.toFile)
        files.single.remove(srcpath)
      }
      srcpath
  }
}
</pre><p>Modeling the server data model with the STM allows for the seamless addition of different concurrent computations to the server program. In the following section, we will implement a server actor that uses the server API to execute filesystem operations.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip101"></a>Tip</h3><p>Use STM to model concurrently accessible data, as an STM works transparently with most concurrency frameworks.</p></div><p>Having completed the filesystem API, we now proceed to the server interface layer of the ScalaFTP browser.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec81"></a>The server interface</h3></div></div></div><p>The server interface comprises of a single actor called <code class="literal">FTPServerActor</code>. This actor will receive client requests and respond to them serially. If it turns out that the server actor is the sequential bottleneck of the system, we can simply add additional server interface actors to improve horizontal scalability.</p><p>We start by defining the different types of messages that the server actor can receive. We follow the convention of defining them inside the companion object of the <code class="literal">FTPServerActor</code> class:</p><pre class="programlisting">object FTPServerActor {
  sealed trait Command
  case class GetFileList(dir: String) extends Command
  case class CopyFile(src: String, dest: String) extends Command
  case class DeleteFile(path: String) extends Command
  def apply(fs: FileSystem) = Props(classOf[FTPServerActor], fs)
}
</pre><p>The actor template of the server actor takes a <code class="literal">FileSystem</code> object as a parameter. It reacts to the <code class="literal">GetFileList</code>, <code class="literal">CopyFile</code>, and <code class="literal">DeleteFile</code> messages by calling the appropriate methods from the filesystem API:</p><pre class="programlisting">class FTPServerActor(fileSystem: FileSystem) extends Actor {
  val log = Logging(context.system, this)
  def receive = {
    case GetFileList(dir) =&gt;
      val filesMap = fileSystem.getFileList(dir)
      val files = filesMap.map(_._2).to[Seq]
      sender ! files
    case CopyFile(srcpath, destpath) =&gt;
      Future {
        Try(fileSystem.copyFile(srcpath, destpath))
      } pipeTo sender
    case DeleteFile(path) =&gt;
      Future {
        Try(fileSystem.deleteFile(path))
      } pipeTo sender
  }
}
</pre><p>When the server receives a <code class="literal">GetFileList</code> message, it calls the <code class="literal">getFileList</code> method with the specified <code class="literal">dir</code> directory, and sends a sequence collection with the <code class="literal">FileInfo</code> objects back to the client. Since <code class="literal">FileInfo</code> is a case class, it extends the <code class="literal">Serializable</code> interface, and its instances can be sent over the network.</p><p>When the server receives a <code class="literal">CopyFile</code> or <code class="literal">DeleteFile</code> message, it calls the appropriate filesystem method asynchronously. The methods in the filesystem API throw exceptions when something goes wrong, so we need to wrap calls to them in <code class="literal">Try</code> objects. After the asynchronous file operations complete, the resulting <code class="literal">Try</code> objects are piped back as messages to the sender actor, using the Akka <code class="literal">pipeTo</code> method.</p><p>To start the ScalaFTP server, we need to instantiate and initialize a <code class="literal">FileSystem</code> object and start the server actor. We parse the network port command-line argument, and use it to create an actor system that is capable of remote communication. For this, we use the <code class="literal">remotingSystem</code> factory method that we introduced in <a class="link" href="#" linkend="ch08">Chapter 8</a>, <span class="emphasis"><em>Actors</em></span>. The remoting actor system then creates an instance of the <code class="literal">FTPServerActor</code>. This is shown in the following program:</p><pre class="programlisting">object FTPServer extends App {
  val fileSystem = new FileSystem(".")
  fileSystem.init()
  val port = args(0).toInt
  val actorSystem = ch8.remotingSystem("FTPServerSystem", port)
  actorSystem.actorOf(FTPServerActor(fileSystem), "server")
}
</pre><p>The ScalaFTP server actor can run inside the same process as the client application, in another process in the same machine, or on a different machine connected with a network. The advantage of the actor model is that we usually need not worry about where the actor runs until we integrate it into the entire application.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip102"></a>Tip</h3><p>When you need to implement a distributed application that runs on different machines, use an actor framework.</p></div><p>Our server program is now complete, and we can run it with the <code class="literal">run</code> command from SBT. We set the actor system to use the port <code class="literal">12345</code>:</p><pre class="programlisting">run 12345
</pre><p>In the following section, we will implement the file navigation API for the ScalaFTP client, which will communicate with the server interface over the network.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec82"></a>Client navigation API</h3></div></div></div><p>The client API exposes the server interfaces to the client program through asynchronous methods that return future objects. Unlike the server's filesystem API, which runs locally, the client API methods execute remote network requests. Futures are a natural way to model latency in the client API methods, and to avoid blocking during network requests.</p><p>Internally, the client API maintains an actor instance that communicates with the server actor. The client actor does not know the actor reference of the server actor when it is created. For this reason, the client actor starts in an <span class="strong"><strong>unconnected</strong></span> state. When it receives the <code class="literal">Start</code> message with the URL of the server actor system, the client constructs an actor path to the server actor, sends out an <code class="literal">Identify</code> message, and switches to the <span class="strong"><strong>connecting</strong></span> state. If the actor system is able to find the server actor, the client actor eventually receives the <code class="literal">ActorIdentity</code> message with the server actor reference. In this case, the client actor switches to the <span class="strong"><strong>connected</strong></span> state, and is able to forward commands to the server. Otherwise, the connection fails and the client actor reverts to the unconnected state. The state diagram of the client actor is shown in the following figure:</p><div class="mediaobject"><img src="graphics/image_09_002.jpg" /></div><p>We define the <code class="literal">Start</code> message in the client actor's companion object:</p><pre class="programlisting">object FTPClientActor {
  case class Start(host: String)
}
</pre><p>We then define the <code class="literal">FTPClientActor</code> class and give it an implicit <code class="literal">Timeout</code> parameter. The <code class="literal">Timeout</code> parameter will be used later in the Akka ask pattern, when forwarding client requests to the server actor. The stub of the <code class="literal">FTPClientActor</code> class is as follows:</p><pre class="programlisting">class FTPClientActor(implicit val timeout: Timeout)
extends Actor
</pre><p>Before defining the <code class="literal">receive</code> method, we define behaviors corresponding to different actor states. Once the client actor in the unconnected state receives the <code class="literal">Start</code> message with the host string, it constructs an actor path to the server and creates an actor selection object. The client actor then sends the <code class="literal">Identify</code> message to the actor selection, and switches its behavior to <code class="literal">connecting</code>. This is shown in the following behavior method, named <code class="literal">unconnected</code>:</p><pre class="programlisting">def unconnected: Actor.Receive = {
  case Start(host) =&gt;
    val serverActorPath =
      s"akka.tcp://FTPServerSystem@$host/user/server"
    val serverActorSel = context.actorSelection(serverActorPath)
    serverActorSel ! Identify(())
    context.become(connecting(sender))
}
</pre><p>The <code class="literal">connecting</code> method creates a behavior given an actor reference to the sender of the <code class="literal">Start</code> message. We call this actor reference <code class="literal">clientApp</code>, because the ScalaFTP client application will send the <code class="literal">Start</code> message to the client actor. Once the client actor receives an <code class="literal">ActorIdentity</code> message with the <code class="literal">ref</code> reference to the server actor, it can send <code class="literal">true</code> back to the <code class="literal">clientApp</code> reference, indicating that the connection was successful. In this case, the client actor switches to the <code class="literal">connected</code> behavior. Otherwise, if the client actor receives an <code class="literal">ActorIdentity</code> message without the server reference, the client actor sends <code class="literal">false</code> back to the application and reverts to the <code class="literal">unconnected</code> state:</p><pre class="programlisting">def connecting(clientApp: ActorRef): Actor.Receive = {
  case ActorIdentity(_, Some(ref)) =&gt;
    clientApp ! true
    context.become(connected(ref))
  case ActorIdentity(_, None) =&gt;
    clientApp ! false
    context.become(unconnected)
}
</pre><p>The <code class="literal">connected</code> state uses the <code class="literal">serverActor</code> server actor reference to forward the <code class="literal">Command</code> messages. To do so, the client actor uses the Akka ask pattern, which returns a future object with the server's response. The contents of the future are piped back to the original sender of the <code class="literal">Command</code> message. In this way, the client actor serves as an intermediary between the application, which is the sender, and the server actor. The <code class="literal">connected</code> method is shown in the following code snippet:</p><pre class="programlisting">def connected(serverActor: ActorRef): Actor.Receive = {
  case command: Command =&gt;
    (serverActor ? command).pipeTo(sender)
}
</pre><p>Finally, the <code class="literal">receive</code> method returns the <code class="literal">unconnected</code> behavior, in which the client actor is created:</p><pre class="programlisting">def receive = unconnected
</pre><p>Having implemented the client actor, we can proceed to the client API layer. We model it as a trait with a <code class="literal">connected</code> value, the concrete methods <code class="literal">getFileList</code>, <code class="literal">copyFile</code>, and <code class="literal">deleteFile</code>, and an abstract <code class="literal">host</code> method. The client API creates a private remoting actor system and a client actor. It then instantiates the <code class="literal">connected</code> future that computes the connection status by sending a <code class="literal">Start</code> message to the client actor. The methods <code class="literal">getFileList</code>, <code class="literal">copyFile</code>, and <code class="literal">deleteFile</code> are similar. They use the ask pattern on the client actor to obtain a future with the response.</p><p>Recall that the actor messages are not typed, and the ask pattern returns a <code class="literal">Future[Any]</code> object. For this reason, each method in the client API uses the <code class="literal">mapTo</code> future combinator to restore the type of the message:</p><pre class="programlisting">trait FTPClientApi {
  implicit val timeout: Timeout = Timeout(4 seconds)
  private val props = Props(classOf[FTPClientActor], timeout)
  private val system = ch8.remotingSystem("FTPClientSystem", 0)
  private val clientActor = system.actorOf(props)
  def host: String
  val connected: Future[Boolean] = {
    val f = clientActor ? FTPClientActor.Start
    f.mapTo[Boolean]
  }
  def getFileList(d: String): Future[(String, Seq[FileInfo])] = {
    val f = clientActor ? FTPServerActor.GetFileList(d)
    f.mapTo[Seq[FileInfo]].map(fs =&gt; (d, fs))
  }
  def copyFile(src: String, dest: String): Future[String] = {
    val f = clientActor ? FTPServerActor.CopyFile(src, dest)
    f.mapTo[Try[String]].map(_.get)
  }
  def deleteFile(srcpath: String): Future[String] = {
    val f = clientActor ? FTPServerActor.DeleteFile(srcpath)
    f.mapTo[Try[String]].map(_.get)
  }
}
</pre><p>Note that the client API does not expose the fact that it uses actors for remote communication. Moreover, the client API is similar to the server API, but the return types of the methods are futures instead of normal values. Futures encode the latency of a method without exposing the cause for the latency, so we often find them at the boundaries between different APIs. We can internally replace the actor communication between the client and the server with the remote <code class="literal">Observable</code> objects, but that would not change the client API.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip103"></a>Tip</h3><p>In a concurrent application, use futures at the boundaries of the layers to express latency.</p></div><p>Now that we can programmatically communicate with the remote ScalaFTP server, we turn our attention to the user interface of the client program.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec83"></a>The client user interface</h3></div></div></div><p>In this section, we create the static user interface for the ScalaFTP client program. This graphical frontend will make our ScalaFTP application easy and intuitive to use. We will rely on the Scala Swing library to implement the UI.</p><p>We will implement the client interface in an abstract <code class="literal">FTPClientFrame</code> class:</p><pre class="programlisting">abstract class FTPClientFrame extends MainFrame {
  title = "ScalaFTP"
}
</pre><p>In the remainder of this section, we augment the <code class="literal">FTPClientFrame</code> class with different UI components. These UI components will enable the end user to interact with the client application, and ultimately with the remote server. Therefore, we will implement the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>A menu bar with common application options</p></li><li style="list-style-type: disc"><p>A status bar that displays various user notifications, such as the connection state, status of the last requested operation, and various error messages</p></li><li style="list-style-type: disc"><p>A pair of file panes that display the path to a specific directory in the filesystem, along with its contents, and buttons that start a copy or delete operation</p></li></ul></div><p>After we are done, the ScalaFTP client program will look like the following screenshot:</p><div class="mediaobject"><img src="graphics/image_09_003.jpg" /></div><p>We start by implementing the menu bar. When creating Swing components in our UI, we can instantiate an anonymous class that extends a <code class="literal">Menu</code> or <code class="literal">MenuBar</code> class, and assign it to a local variable. However, using an anonymous class does not allow access to its custom members. If the anonymous UI component class contains nested components, we are not able to refer to them. Therefore, we will use nested singleton objects to instantiate UI components, as doing this allows us to refer to the object's nested components.</p><p>In the following code snippet, we create the <code class="literal">menu</code> singleton object that extends the <code class="literal">MenuBar</code> class. We create the <code class="literal">file</code> and the <code class="literal">help</code> menu, with the <code class="literal">exit</code> and <code class="literal">about</code> menu items, respectively, and take care to add each <code class="literal">Menu</code> component to the <code class="literal">contents</code> collection of the enclosing component:</p><pre class="programlisting">object menu extends MenuBar {
  object file extends Menu("File") {
    val exit = new MenuItem("Exit ScalaFTP")
    contents += exit
  }
  object help extends Menu("Help") {
    val about = new MenuItem("About...")
    contents += about
  }
  contents += file += help
}
</pre><p>Similarly, we implement the <code class="literal">status</code> object by extending the <code class="literal">BorderPanel</code> class. The <code class="literal">BorderPanel</code> components are used to hold other nested components: in our case, two nested <code class="literal">Label</code> objects. The anonymous <code class="literal">Label</code> object always contains the static <code class="literal">Status: </code>text, while the named <code class="literal">Label</code> object contains arbitrary status messages. We place the anonymous <code class="literal">Label</code> object to the left, and the <code class="literal">Label</code> object with the status messages in the center. This is shown in the following code snippet:</p><pre class="programlisting">object status extends BorderPanel {
  val label = new Label("connecting...", null, Alignment.Left)
  layout(new Label("Status: ")) = West
  layout(label) = Center
}
</pre><p>Finally, we implement a custom <code class="literal">FilePane</code> component that displays the contents of a directory in the remote filesystem. We will have two <code class="literal">FilePane</code> instances in the client program, so we declare a custom <code class="literal">FilePane</code> class, which itself extends the <code class="literal">BorderPanel</code> component type:</p><pre class="programlisting">class FilePane extends BorderPanel
</pre><p>We hierarchically decompose the <code class="literal">FilePane</code> class into three parts: the <code class="literal">pathBar</code> component that displays the path to the current directory, the <code class="literal">scrollPane</code> component that allows scrolling through the contents of the current directory, and the <code class="literal">buttons</code> component that contains the copy and delete buttons. In the following code snippet, we add a non-editable text field with the current path, and an <code class="literal">upButton</code> component that is used to navigate up the file hierarchy:</p><pre class="programlisting">object pathBar extends BorderPanel {
  val label = new Label("Path:")
  val filePath = new TextField(".") { editable = false }
  val upButton = new Button("^")
  layout(label) = West
  layout(filePath) = Center
  layout(upButton) = East
}
</pre><p>The <code class="literal">scrollPane</code> component contains a <code class="literal">Table</code> object named <code class="literal">fileTable</code>. The <code class="literal">fileTable</code> object will contain the columns named <code class="literal">Filename</code>, <code class="literal">Size</code>, and <code class="literal">Date modified</code>, and each table row will contain a file or a subdirectory within the current working directory. To prevent the user from modifying filenames, sizes, or modification dates, we install a custom <code class="literal">TableModel</code> object that disallows editing in every row and column. The complete implementation of the <code class="literal">scrollPane</code> component is as follows:</p><pre class="programlisting">object scrollPane extends ScrollPane {
  val columnNames =
    Array[AnyRef]("Filename", "Size", "Date modified")
  val fileTable = new Table {
    showGrid = true
    model = new DefaultTableModel(columnNames, 0) {
      override def isCellEditable(r: Int, c: Int) = false
    }
    selection.intervalMode = Table.IntervalMode.Single
  }
  contents = fileTable
}
</pre><p>The <code class="literal">buttons</code> singleton object is a <code class="literal">GridPanel</code> component with one row and two columns. Each column contains a single button, as shown in the following code snippet:</p><pre class="programlisting">object buttons extends GridPanel(1, 2) {
  val copyButton = new Button("Copy")
  val deleteButton = new Button("Delete")
  contents += copyButton += deleteButton
}
</pre><p>We then place these custom components inside the <code class="literal">FilePane</code> component:</p><pre class="programlisting">layout(pathBar) = North
layout(scrollPane) = Center
layout(buttons) = South
</pre><p>Finally, we add the <code class="literal">parent</code> directory field and the list of the files in the current directory, named <code class="literal">dirFiles</code>, into the <code class="literal">FilePane</code> class, as well as a few convenience methods to more easily access deeply nested UI components:</p><pre class="programlisting">var parent: String = "."
var dirFiles: Seq[FileInfo] = Nil
def table = scrollPane.fileTable
def currentPath = pathBar.filePath.text
</pre><p>Recall that we need one <code class="literal">FilePane</code> instance on the left-hand side of the client program, and another one on the right. We declare the <code class="literal">files</code> singleton object inside the <code class="literal">FTPClientFrame</code> class to hold the two <code class="literal">FilePane</code> instances, as follows:</p><pre class="programlisting">object files extends GridPanel(1, 2) {
  val leftPane = new FilePane
  val rightPane = new FilePane
  contents += leftPane += rightPane
  def opposite(pane: FilePane) =
    if (pane eq leftPane) rightPane else leftPane
}
</pre><p>Finally, we need to place the <code class="literal">menu</code>, <code class="literal">files</code>, and <code class="literal">status</code> components at the top, center, and bottom of the client program:</p><pre class="programlisting">contents = new BorderPanel {
  layout(menu) = North
  layout(files) = Center
  layout(status) = South
}
</pre><p>We can already run the client program at this point, and try to interact with it. Unfortunately, the client program does not do anything yet. Clicking on the <code class="literal">FilePane</code> component, the buttons, or the menu items currently does not have any effect, as we have not yet defined callbacks for various UI actions. In the following section, we will use Rx to complete the functionality of the client application.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec84"></a>Implementing the client logic</h3></div></div></div><p>We are now ready to add some life to the ScalaFTP client program. We will define the logic layer in the <code class="literal">FTPClientLogic</code> trait. We only want to allow mixing in the <code class="literal">FTPClientLogic</code> trait with classes that extend both the <code class="literal">FTPClientFrame</code> class and the <code class="literal">FTPClientApi</code> trait, as this allows the logic layer to refer to both UI components and use the client API. Therefore, we give this trait the self-type <code class="literal">FTPClientFrame</code> class with <code class="literal">FTPClientApi</code>:</p><pre class="programlisting">trait FTPClientLogic {
  self: FTPClientFrame with FTPClientApi =&gt;
}
</pre><p>Before we begin, recall that the Swing components can only be modified from the event-dispatching thread. Similar to how we ensured this using the <code class="literal">swingScheduler</code> object in <a class="link" href="#" linkend="ch06">Chapter 6</a>, <span class="emphasis"><em>Concurrent Programming with Reactive Extensions</em></span>, we now introduce the <code class="literal">swing</code> method, which takes a block of code and schedules it for execution on the Swing library's event-dispatching thread:</p><pre class="programlisting">def swing(body: =&gt;Unit) = {
  val r = new Runnable { def run() = body }
  javax.swing.SwingUtilities.invokeLater(r)
}
</pre><p>Throughout this section, we will rely on the <code class="literal">swing</code> method in order to ensure that the effect of asynchronous computations occur only on the Swing event-dispatching thread.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip104"></a>Tip</h3><p>The Swing toolkit permits modifying UI components only from the event-dispatching thread, but does not ensure this restriction at compile time, and can unexpectedly fail during runtime.</p></div><p>We begin by relating the connection status to the user interface. Recall that we introduced the <code class="literal">connected</code> future as part of the client API. Depending on the result of the <code class="literal">connected</code> future, we either modify the <code class="literal">text</code> value of the status label to display an error message, or report that the client program has successfully connected to the server. In the latter case, we call the <code class="literal">refreshPane</code> method to update the contents of the <code class="literal">FilePane</code> components that we will look at shortly. The following code snippet shows the <code class="literal">onComplete</code> callback:</p><pre class="programlisting">connected.onComplete {
  case Failure(t) =&gt;
    swing { status.label.text = s"Could not connect: $t" }
  case Success(false) =&gt;
    swing { status.label.text = "Could not find server." }
  case Success(true) =&gt;
    swing {
      status.label.text = "Connected!"
      refreshPane(files.leftPane)
      refreshPane(files.rightPane)
    }
}
</pre><p>There are two steps involved in updating the <code class="literal">FilePane</code> component. First, we need to get the contents of the remote directory from the server. Second, once these contents arrive, we need to refresh the <code class="literal">Table</code> object in the <code class="literal">FilePane</code> component. In the following code, we call the <code class="literal">getFileList</code> method from the client API, and refresh the <code class="literal">Table</code> object with the <code class="literal">updatePane</code> method:</p><pre class="programlisting">def refreshPane(pane: FilePane): Unit = {
  val dir = pane.pathBar.filePath.text
  getFileList(dir) onComplete {
    case Success((dir, files)) =&gt;
      swing { updatePane(pane, dir, files) }
    case Failure(t) =&gt;
      swing { status.label.text = s"Could not update pane: $t" }
  }
}
</pre><p>The <code class="literal">updatePane</code> method takes the <code class="literal">dir</code> directory name and the <code class="literal">files</code> list, and uses them to update the <code class="literal">FilePane</code> component <code class="literal">p</code>. It extracts the <code class="literal">DefaultTableModel</code> object, and clears its previous contents by setting the row count to <code class="literal">0</code>. It then updates the <code class="literal">parent</code> field in the <code class="literal">FilePane</code> object to the parent of the <code class="literal">dir</code> directory.</p><p>Finally, it stores the <code class="literal">files</code> list into the <code class="literal">dirFiles</code> field, and adds a row for each entry:</p><pre class="programlisting">def updatePane(p: FilePane, dir: String, files: Seq[FileInfo]) = {
  val table = p.scrollPane.fileTable
  table.model match {
    case d: DefaultTableModel =&gt;
      d.setRowCount(0)
      p.parent =
        if (dir == ".") "."
        else dir.take(dir.lastIndexOf(File.separator))
      p.dirFiles = files.sortBy(!_.isDir)
      for (f &lt;- p.dirFiles) d.addRow(f.toRow)
  }
}
</pre><p>In the preceding method, we relied on the <code class="literal">toRow</code> method to convert the <code class="literal">FileInfo</code> object into an array of <code class="literal">String</code> objects, which the <code class="literal">Table</code> component works with:</p><pre class="programlisting">def toRow = Array[AnyRef](
  name, if (isDir) "" else size / 1000 + "kB", modified)
</pre><p>So far, so good! Our client program is able to connect to the server and show the contents of the root directory. Next, we need to implement the UI logic that allows navigating through the remote filesystem.</p><p>When dealing with UI events in <a class="link" href="#" linkend="ch06">Chapter 6</a>, <span class="emphasis"><em>Concurrent Programming with Reactive Extensions</em></span>, we augmented our UI components with <code class="literal">Observable</code> objects. Recall that we added the <code class="literal">clicks</code> and <code class="literal">texts</code> methods in order to process events from the <code class="literal">Button</code> and <code class="literal">TextField</code> components. In the following code, we augment the <code class="literal">Table</code> component with the <code class="literal">rowDoubleClicks</code> method, which returns an <code class="literal">Observable</code> object with the indices of the rows that have been double-clicked on:</p><pre class="programlisting">implicit class TableOps(val self: Table) {
  def rowDoubleClicks = Observable[Int] { sub =&gt;
    self.peer.addMouseListener(new MouseAdapter {
      override def mouseClicked(e: java.awt.event.MouseEvent) {
        if (e.getClickCount == 2) {
          val row = self.peer.getSelectedRow
          sub.onNext(row)
        }
      }
    })
  }
}
</pre><p>To navigate through the remote filesystem, users need to click on the <code class="literal">FilePane</code> and <code class="literal">upButton</code> objects. We need to set up this functionality once for each pane, so we define the <code class="literal">setupPane</code> method for this purpose:</p><pre class="programlisting">def setupPane(pane: FilePane): Unit
</pre><p>The first step when reacting to the clicks on the <code class="literal">FilePane</code> component is mapping each user double-click to the name of the file or directory that has been clicked on. Then, if the double-clicked file is a directory, we update the current <code class="literal">filePath</code> method, and call the <code class="literal">refreshPane</code> method:</p><pre class="programlisting">val fileClicks =
  pane.table.rowDoubleClicks.map(row =&gt; pane.dirFiles(row))
fileClicks.filter(_.isDir).subscribe { fileInfo =&gt;
  pane.pathBar.filePath.text =
    pane.pathBar.filePath.text + File.separator + fileInfo.name
  refreshPane(pane)
}
</pre><p>Similarly, when the user clicks on the <code class="literal">upButton</code> component, we call the <code class="literal">refreshPane</code> method to navigate to the parent directory:</p><pre class="programlisting">pane.pathBar.upButton.clicks.subscribe { _ =&gt;
  pane.pathBar.filePath.text = pane.parent
  refreshPane(pane)
}
</pre><p>Navigating through the remote filesystem is informative, but we also want to be able to copy and delete the remote files. This requires reacting to UI button clicks, each of which needs to be mapped to the correct, currently selected file. The <code class="literal">rowActions</code> method produces an event stream with the files that were selected at the time, at the point when a button was clicked:</p><pre class="programlisting">def rowActions(button: Button): Observable[FileInfo] =
  button.clicks
    .map(_ =&gt; pane.table.peer.getSelectedRow)
    .filter(_ != -1)
    .map(row =&gt; pane.dirFiles(row))
</pre><p>Clicking on the copy button will copy the selected file to the directory selected in the opposite pane. We use the <code class="literal">rowActions</code> method to map the directory on the opposite pane, and call the <code class="literal">copyFile</code> method from the client API. Recall that the <code class="literal">copyFile</code> method returns a future, so we need to call the <code class="literal">onComplete</code> method to process its result asynchronously:</p><pre class="programlisting">rowActions(pane.buttons.copyButton)
  .map(info =&gt; (info, files.opposite(pane).currentPath))
  .subscribe { t =&gt;
    val (info, destDir) = t
    val dest = destDir + File.separator + info.name
    copyFile(info.path, dest) onComplete {
      case Success(s) =&gt;
        swing {
          status.label.text = s"File copied: $s"
          refreshPane(pane)
        }
    }
  }
</pre><p>We use the <code class="literal">rowActions</code> method in a similar way, in order to react to clicks on the delete button. Finally, we call the <code class="literal">setupPane</code> method once for each pane:</p><pre class="programlisting">setupPane(files.leftPane)
setupPane(files.rightPane)
</pre><p>Our remote file browser is now fully functional. To test it, we open two separate instances of the terminal, and run SBT in our project directory from both the terminals. We first run the server program:</p><pre class="programlisting">
<span class="strong"><strong>&gt; set fork := true</strong></span>
<span class="strong"><strong>&gt; run 12345</strong></span>
</pre><p>By making sure that the server is running on port <code class="literal">12345</code>, we can run the client from the second terminal as follows:</p><pre class="programlisting">
<span class="strong"><strong>&gt; set fork := true</strong></span>
<span class="strong"><strong>&gt; run 127.0.0.1:12345</strong></span>
</pre><p>Now, try copying some of our project files between different directories. If you've also implemented the delete functionality, make sure that you back up the project files before deleting anything, just in case. It's not always a good idea to test experimental file-handling utilities on our source code.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec85"></a>Improving the remote file browser</h3></div></div></div><p>If you successfully ran both the ScalaFTP server, client programs, and copied files around, you might have noticed that, if you delete a file on the disk from an external application, such as your source-code editor, the changes will not be reflected in the ScalaFTP server program. The reason for this is that the server actor does not monitor the filesystem for changes, and the server filesystem layer is not updated when we delete the file.</p><p>To account for filesystem changes external to the ScalaFTP server program, we need to monitor the filesystem for changes. This sounds like an ideal case for event streams. Recall that we already did this in <a class="link" href="#" linkend="ch06">Chapter 6</a>, <span class="emphasis"><em>Concurrent Programming with Reactive Extensions</em></span>, when we defined the <code class="literal">modified</code> method to track file modifications. This time, we define the <code class="literal">FileCreated</code>, <code class="literal">FileDeleted</code>, and <code class="literal">FileModified</code> types to denote three different kinds of filesystem events:</p><pre class="programlisting">sealed trait FileEvent
case class FileCreated(path: String) extends FileEvent
case class FileDeleted(path: String) extends FileEvent
case class FileModified(path: String) extends FileEvent
</pre><p>By implementing the additional methods in the <code class="literal">FileAlterationListener</code> interface, we ensure that the resulting <code class="literal">Observable</code> object produces any one of the three event types. In the following code snippet, we show the relevant part of the <code class="literal">fileSystemEvents</code> method that produces an <code class="literal">Observable[FileEvent]</code> object with the filesystem events:</p><pre class="programlisting">override def onFileCreate(file: File) =
  obs.onNext(FileCreated(file.getPath))
override def onFileChange(file: File) =
  obs.onNext(FileModified(file.getPath))
override def onFileDelete(file: File) =
  obs.onNext(FileDeleted(file.getPath))
</pre><p>Now that we have an event stream of file events, we can easily modify the filesystem model. We subscribe to the file event stream, and start single-operation transactions to update the <code class="literal">fileSystem</code> transactional map:</p><pre class="programlisting">fileSystemEvents(".").subscribe { e =&gt; e match {
    case FileCreated(path) =&gt;
      fileSystem.files.single(path) = FileInfo(new File(path))
    case FileDeleted(path) =&gt;
      fileSystem.files.single.remove(path)
    case FileModified(path) =&gt;
      fileSystem.files.single(path) = FileInfo(new File(path))
  }
}
</pre><p>Now, you can run the server and the client again, and experiment with either deleting or copying files in your editor after the server has started. You will notice that the filesystem changes are detected on the server, and eventually shown when the client is refreshed.</p><p>Note that this example was chosen to illustrate how all the different concurrency libraries described in this book work together. However, there is no need to use all of these concurrency libraries in every program. In many situations, we only need a few different concurrency abstractions. Depending on your programming task, you should decide which ones are the best fit.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip105"></a>Tip</h3><p>Never over-engineer your concurrent program. Only use those concurrency libraries that help you solve your specific programming task.</p></div><p>Having studied how to combine different concurrency libraries in a larger application, and having caught a glimpse of how to pick the correct concurrency library, we turn our attention to another aspect of dealing with concurrency, namely, debugging concurrent programs.</p></div></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec61"></a>Debugging concurrent programs</h2></div></div><hr /></div><p>Concurrent programming is much harder than sequential programming. There are multiple reasons for this. First, the details of the memory model are much more important in concurrent programming, resulting in increased programming complexity. Even on a platform with a well-defined memory model, such as the JVM, the programmer must take care to use proper memory access primitives in order to avoid data races. Then, it is harder to track the execution of a multithreaded program, simply because there are multiple executions proceeding simultaneously. Language debuggers are still focused on tracking the execution of a single thread at a time. Deadlocks and inherent nondeterminism are another source of bugs, neither of which is common in sequential programs. To make things worse, all these issues only have to do with ensuring the correctness of a concurrent program. Ensuring improved throughput and performance opens a separate set of problems, and is often harder than it sounds. Generally, a lot of effort is required to ensure that a concurrent program really runs faster, and performance debugging is an art of its own.</p><p>In this section, we survey some of the typical causes of errors in concurrent programs, and inspect different methods of dealing with them. We start with the simplest form of concurrency bugs, which are revealed by a lack of progress in the system.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec86"></a>Deadlocks and lack of progress</h3></div></div></div><p>Despite the scariness typically associated with the term deadlock, when it comes to debugging concurrent programs, deadlocks are one of the more benevolent forms of concurrency bugs you will encounter. The reason for this is that deadlocks are easy to track down and analyze. In this section, we study how to identify and resolve a deadlock in a concurrent program.</p><p>Before we begin, we will make sure that SBT starts the example programs in a separate JVM process. To do this, we enter the following command into the SBT interactive shell:</p><pre class="programlisting">
<span class="strong"><strong>&gt; set fork := true</strong></span>
</pre><p>In <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Concurrency on the JVM and the Java Memory Model</em></span>, we discussed at length what deadlocks are and why they occur. Here, we recall the bank account example introduced in that chapter, which is a canonical example of a deadlock. The bank account example consisted of an <code class="literal">Account</code> class and the <code class="literal">send</code> method, which locks two <code class="literal">Account</code> objects, and transfers a certain amount of money between them:</p><pre class="programlisting">class Account(var money: Int)

def send(a: Account, b: Account, n: Int) = a.synchronized {
  b.synchronized {
    a.money -= n
    b.money += n
  }
}
</pre><p>A deadlock nondeterministically occurs when we simultaneously make an attempt to transfer money from account <code class="literal">a</code> to account <code class="literal">b</code>, and vice versa, as shown in the following code snippet:</p><pre class="programlisting">val a = new Account(1000)
val b = new Account(2000)
val t1 = ch2.thread { for (i &lt;- 0 until 100) send(a, b, 1) }
val t2 = ch2.thread { for (i &lt;- 0 until 100) send(b, a, 1) }
t1.join()
t2.join()
</pre><p>In the preceding snippet, we are using the <code class="literal">thread</code> method for the thread creation from <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Concurrency on the JVM and the Java Memory Model</em></span>. This program never completes, as the <code class="literal">t1</code> and <code class="literal">t2</code> threads get suspended in the deadlock state. In a larger program, this effect manifests itself as a lack of response. When a concurrent program fails to produce a result or an end, this is a good indication that part of it is in the deadlock state.</p><p>Usually, the most difficult part in debugging a deadlock is localizing it. While this is easy to determine in our simple example, it is much harder in a larger application. However, a defining feature of a deadlock is the lack of any progress, and we can use this to our advantage to determine its cause; we simply need to find the threads that are in a blocked state, and determine their stack-traces.</p><p>The Java VisualVM tool, which comes bundled with newer JDK distributions, is the simplest way to determine the state of the running Scala and Java applications. Without exiting our deadlocked program, we run the <code class="literal">jvisualvm</code> program from another terminal instance as follows:</p><pre class="programlisting">
<span class="strong"><strong>$ jvisualvm</strong></span>
</pre><p>Once run, the Java VisualVM application shows all the active JVM processes on the current machine. In the following screenshot, the Java VisualVM application shows us the SBT process, our deadlock example program, and VisualVM itself, as the running instances:</p><div class="mediaobject"><img src="graphics/image_09_004.jpg" /></div><p>After clicking on the example process, we get the report shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/image_09_005.jpg" /></div><p>The preceding screenshot shows that there are multiple threads running inside the example process. Most of these threads are part of the virtual machine runtime, and not under the direct control of the programmer. Other threads, such as <span class="strong"><strong>main</strong></span>, <span class="strong"><strong>Thread-0</strong></span>, and <span class="strong"><strong>Thread-1</strong></span> are created by our program.</p><p>To determine the cause of the deadlock, we need to inspect the threads in the <code class="literal">BLOCKED</code> state. By examining their stack-traces, we can determine the cycle that is causing the deadlock. In this case, Java VisualVM was smart enough to automatically determine the cause of the deadlock, and displays the deadlocked threads with the red bar.</p><p>After clicking on the <span class="strong"><strong>Thread Dump</strong></span> button, Java VisualVM displays the stack traces of all the threads, as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/image_09_006.jpg" /></div><p>The stack traces in the preceding screenshot tell us exactly where in the program the threads are blocked, and why. Both <span class="strong"><strong>Thread-0</strong></span> and <span class="strong"><strong>Thread-1</strong></span> threads are suspended in line 15 of the <code class="literal">Debugging.scala</code> file. Inspecting these lines of code in our editor reveals that both the threads are blocked on the nested <code class="literal">synchronized</code> statement. We now know that the cause of the deadlock is the inverted locking order in the <code class="literal">send</code> method.</p><p>We've already discussed how to deal with this type of a deadlock in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Concurrency on the JVM and the Java Memory Model</em></span>. Enforcing a locking order in the <code class="literal">send</code> method is a textbook example of dealing with deadlocks, and is easy to ensure by assigning unique identifiers to different locks.</p><p>In some cases, we are not able to enforce the locking order to avoid deadlocks. For example, in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Traditional Building Blocks of Concurrency</em></span>, we learned that the lazy values initialization implicitly calls the <code class="literal">synchronized</code> statement without our control. There, we eluded deadlocks by avoiding the explicit <code class="literal">synchronized</code> statements on the object enclosing the lazy value. Another way of preventing deadlocks is to avoid blocking when a resource is not available. In <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Traditional Building Blocks of Concurrency</em></span>, we learned that custom locks can return an error value, letting the rest of the program decide how to proceed if a lock is not available.</p><p>Besides deadlocks, there are other kinds of concurrency bugs that are associated with a lack of progress. We've already seen examples of <span class="strong"><strong>starvation</strong></span>, in which a concurrent computation is denied access to the required resources. In <a class="link" href="#" linkend="ch04">Chapter 4</a>, <span class="emphasis"><em>Asynchronous Programming with Futures and Promises</em></span>, we started many futures simultaneously, and suspended them by calling the <code class="literal">sleep</code> method. As a result, the thread-pool underlying the <code class="literal">ExecutionContext</code> object became exhausted, and no additional futures could execute until the <code class="literal">sleep</code> method returned.</p><p>In a <span class="strong"><strong>livelock</strong></span>, different concurrent computations are not suspended, and constantly change their state, but are unable to make progress. A livelock is akin to the situation in which two people approach each other on the street, and constantly try to move to the opposite side in order to allow the other person to pass. As a result, neither person moves on, and they constantly move from one side to the other. What is common to these kinds of errors is that the system makes no or very little progress, making them easy to identify.</p><p>Looking for a deadlock is like hunting for a dead animal. Since it implies no progress, a deadlock is tracked down more easily than other kinds of concurrency bugs. In the following section, we will study a more malevolent class of concurrency errors that manifest themselves through incorrect program outputs.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec87"></a>Debugging incorrect program outputs</h3></div></div></div><p>In this section, we study a broader range of concurrency bugs that manifest themselves as incorrect outputs of the program. Generally, these kinds of errors are harder to track, because their effects become apparent long after the actual error took place. A real-world example of such an error is a piece of broken glass lying on the road. You don't see the glass when you drive your car, and accidentally run over it. By the time your tire runs flat and you realize what happened, it is difficult to figure out where exactly along the road the glass was.</p><p>There are two main ways in which an error can appear. First, the concurrent program can consistently produce the same erroneous outputs. When this happens, we can consider ourselves lucky, as we are able to consistently reproduce the error to study it. Conversely, the incorrect output might appear only occasionally, in some executions of the program. This is a much less desired situation. A buggy concurrent program might exhibit incorrect behavior only occasionally due to its inherent nondeterminism. We will look at both deterministic and nondeterministic errors in the rest of the section.</p><p>The goal of this section will be to implement the <code class="literal">fold</code> method on futures. Given a sequence of future objects, a zero value, and the <code class="literal">folding</code> operator, the <code class="literal">fold</code> method will return a future object with the <code class="literal">folding</code> operator that is applied between all the values. We will require the <code class="literal">folding</code> operator to be commutative, associative, and without side effects. The <code class="literal">fold</code> method will closely correspond to the <code class="literal">foldLeft</code> method on collections. The signature of the <code class="literal">fold</code> method on futures will be as follows:</p><pre class="programlisting">def fold[T](fs: Seq[Future[T]])(z: T)(op: (T, T) =&gt; T): Future[T]
</pre><p>One use case for the <code class="literal">fold</code> method is to compute the sum of the values in many different future objects, which cannot be done directly with the <code class="literal">foldLeft</code> method on collections. This is illustrated in the following code snippet:</p><pre class="programlisting">val fs: Seq[Future[Int]] = for (i &lt;- 0 until 5) yield Future { i }
val sum: Future[Int] = fold(fs)(0)(_ + _)
</pre><p>We will implement the <code class="literal">fold</code> method in two steps. First, we will accumulate the values from all the values in the <code class="literal">fs</code> sequence by applying the <code class="literal">op</code> operator on them. Accumulating the values will give us the accumulation value of the resulting future. Then, after all the futures complete, we will complete the resulting future with the accumulation value.</p><p>We start by implementing several basic concurrency abstractions that will help us implement the <code class="literal">fold</code> method. A <span class="strong"><strong>concurrent accumulator</strong></span> is a concurrency facility that allows you to keep track of an accumulation of values. Here, the values can be integers, and the accumulation can be their sum. A concurrent accumulator comes with the <code class="literal">add</code> method that is used to add new values, and the <code class="literal">apply</code> method that is used to obtain the current state of the accumulation. We present the simplest possible lock-free implementation of a concurrent accumulator, which uses atomic variables from <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Traditional Building Blocks of Concurrency</em></span>. The <code class="literal">Accumulator</code> class takes the type <code class="literal">T</code> of the accumulation, a <code class="literal">z</code> initial value, and an <code class="literal">op</code> reduction operator, and is shown in the following code snippet:</p><pre class="programlisting">class Accumulator[T](z: T)(op: (T, T) =&gt; T) {
  private val value = new AtomicReference(z)
  def apply(): T = value.get
  @tailrec final def add(v: T): Unit = {
    val ov = value.get
    val nv = op(ov, v)
    if (!value.compareAndSet(ov, nv)) add(v)
  }
}
</pre><p>The <code class="literal">Accumulator</code> implementation has a private atomic variable, named <code class="literal">value</code>, initialized with the <code class="literal">z</code> value, and is used to track the value of the accumulation. The <code class="literal">apply</code> method is easy to implement; we simply call the linearizable <code class="literal">get</code> method to obtain the current accumulation value. The <code class="literal">add</code> method must use the <code class="literal">compareAndSet</code> operation to atomically update the accumulation. Here, we read the <code class="literal">ov</code> current value of the atomic variable, call the <code class="literal">op</code> operator to compute the new <code class="literal">nv</code> accumulation value, and, finally, call the <code class="literal">compareAndSet</code> operation to replace the old <code class="literal">ov</code> accumulation value with the new <code class="literal">nv</code> value. If the <code class="literal">compareAndSet</code> operation returns <code class="literal">false</code>, then the accumulation was modified, as it was previously read, and the tail-recursive <code class="literal">add</code> operation must be retried. We studied this technique at length in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Traditional Building Blocks of Concurrency</em></span>.</p><p>Note that, because of the retries, the <code class="literal">op</code> operator can be invoked multiple times with the same <code class="literal">v</code> argument. Therefore, our lock-free concurrent accumulator implementation only works correctly with a reduction operator that is free from side effects.</p><p>Next, we will need a facility that allows different futures to synchronize. A <span class="strong"><strong>countdown latch</strong></span> is a synchronization primitive that performs a specific action once a specified number of threads agree that the action can be performed. Our <code class="literal">CountDownLatch</code> class takes the number of threads <code class="literal">n</code>, and an <code class="literal">action</code> block. The latch keeps an atomic integer variable, named <code class="literal">left</code>, with the current countdown value, and defines a <code class="literal">count</code> method, which decreases the value of the <code class="literal">left</code> atomic variable. After <code class="literal">n</code> calls of the <code class="literal">count</code> method, the <code class="literal">action</code> block is invoked once. This is shown in the following code snippet:</p><pre class="programlisting">class CountDownLatch(n: Int)(action: =&gt;Unit) {
  private val left = new AtomicInteger(n)
  def count() =
    if (left.decrementAndGet() &lt;= 1) action
}
</pre><p>We now have all the prerequisites for implementing the <code class="literal">fold</code> method. This method needs to return a future object, so we start by instantiating a promise object. The promise will enable us to return the future object corresponding to the promise. We have seen this pattern many times in <a class="link" href="#" linkend="ch04">Chapter 4</a>, <span class="emphasis"><em>Asynchronous Programming with Futures and Promises</em></span>. Next, we need some way of combining the values from the different futures, so we instantiate an <code class="literal">Accumulator</code> object with the initial <code class="literal">z</code> value and the <code class="literal">op</code> reduction operator. We can complete the promise with the value of the accumulator only after all the futures complete, so we create a countdown latch with the countdown value set to the number of the futures. The action associated with the countdown latch completes the promise with the value of the accumulator, and we decide to use the <code class="literal">trySuccess</code> method for this purpose. Finally, we need to install callbacks on all the futures, which update the accumulator, and then call the <code class="literal">count</code> method on the latch. The complete implementation of the <code class="literal">fold</code> method is shown in the following code snippet:</p><pre class="programlisting">def fold[T](fs: Seq[Future[T]])(z: T)(op: (T, T) =&gt; T) = {
  val p = Promise[T]()
  val accu = new Accumulator(z)(op)
  val latch = new CountDownLatch(fs.length)({
    p.trySuccess(accu()))
  })
  for (f &lt;- fs) f foreach { case v =&gt;
    accu.add(v)
    latch.count()
  }
  p.future
}
</pre><p>If you paid close attention, you might have noticed that we deliberately introduced an error somewhere in the <code class="literal">fold</code> implementation. Don't worry if you did not notice this error yet, as we will now analyze how the error manifests itself, and how to identify it. To test the <code class="literal">fold</code> method, we run the following example program:</p><pre class="programlisting">val fs = for (i &lt;- 0 until 5) yield Future { i }
val folded = fold(fs)(0)(_ + _)
folded foreach { case v =&gt; log(s"folded: $v") }
</pre><p>On our machine, running this program prints the correct value <code class="literal">10</code>. We already feel confident that we implemented the program correctly, but we run the program again, just to be sure. This time, however, the program outputs the value <code class="literal">7</code>. It turns out that we have a bug in our implementation of the <code class="literal">fold</code> method. Even worse, the bug manifests itself nondeterministically!</p><p>In sequential programming, the normal response would be to use the debugger, and proceed stepwise through the program, until we reach the buggy behavior. In concurrent programming, this approach often does not help. By tracking the progress of one thread in the debugger, we are arbitrarily delaying it, and changing the execution schedule of the program. The bug appears nondeterministically, so it might not appear when we run the program in the debugger.</p><p>Instead of going forward through the program, to find the culprit, we work our way backwards through the code. The future is completed with the incorrect value, meaning that some thread must have inserted the incorrect value into the corresponding promise. We should insert a breakpoint at the promise completion point and observe what happens. To keep things simple, we avoid using the debugger, and insert a simple <code class="literal">println</code> statement to track the value with which the promise is completed:</p><pre class="programlisting">val total = accu()
println(total)
p.trySuccess(total)
</pre><p>Running the program again gives the following output:</p><pre class="programlisting">
<span class="strong"><strong>8</strong></span>
<span class="strong"><strong>10</strong></span>
<span class="strong"><strong>ForkJoinPool-1-worker-1: folded: 8</strong></span>
</pre><p>This reveals a surprising fact: the promise is, in fact, completed twice. The first time, some thread uses the value <code class="literal">8</code> of the accumulator, and the second time, another thread uses the value <code class="literal">10</code>. This also means that the <code class="literal">action</code> block of the countdown latch was called twice, so we need to find out why. We therefore modify the <code class="literal">count</code> method in order to track when the <code class="literal">action</code> block is called:</p><pre class="programlisting">def count() = {
  val v = left.decrementAndGet()
  if (v &lt;= 1) {
    println(v)
    action
  }
}
</pre><p>The program output now shows the following content:</p><pre class="programlisting">
<span class="strong"><strong>1</strong></span>
<span class="strong"><strong>0</strong></span>
<span class="strong"><strong>ForkJoinPool-1-worker-15: folded: 7</strong></span>
</pre><p>It appears that the <code class="literal">action</code> block is called not only on the last decrement, but also on one before the last. This is because the <code class="literal">decrementAndGet</code> method first decrements the atomic integer, and then returns its value, rather than the other way around. The way to fix this is to either call the <code class="literal">getAndDecrement</code> method, or change the <code class="literal">if</code> statement. We reimplement the <code class="literal">count</code> method as follows:</p><pre class="programlisting">def count() =
  if (left.decrementAndGet() == 0) action
</pre><p>Note that, if we had used the <code class="literal">success</code> method in place of <code class="literal">trySuccess</code>, we would have learned about the error much earlier. Let's change the implementation of the <code class="literal">action</code> block in the <code class="literal">fold</code> method to use the <code class="literal">success</code> method:</p><pre class="programlisting">p.success(accu()))
</pre><p>Running the program with this change, and the previously incorrect <code class="literal">count</code> method, results in the following exception:</p><pre class="programlisting">java.lang.IllegalStateException: Promise already completed.
</pre><p>This is much better. The output of the program is incorrect, but the exception consistently occurs each time that the program is run. Along with the cause of the error, we consistently get a full stack-trace to quickly determine where the error has occurred. We say that the error occurs deterministically.</p><p>Recall that, in <a class="link" href="#" linkend="ch04">Chapter 4</a>, <span class="emphasis"><em>Asynchronous Programming with Futures and Promises</em></span>, we used the <code class="literal">tryComplete</code> method to implement the <code class="literal">or</code> combinator on futures. This combinator was inherently nondeterministic, so we were forced to use the <code class="literal">tryComplete</code> method. However, there is no need to use any of the <code class="literal">tryXYZ</code> methods in the <code class="literal">fold</code> implementation, as the <code class="literal">fold</code> method should always return a future with the same result. Wherever possible, you should use the <code class="literal">complete</code>, <code class="literal">success</code>, and <code class="literal">failure</code> methods in place of the <code class="literal">tryComplete</code>, <code class="literal">trySuccess</code>, and <code class="literal">tryFailure</code> methods. More generally, always strive for deterministic semantics, unless the program itself is inherently nondeterministic.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip106"></a>Tip</h3><p>Program defensively: check for consistency violations often, prefer determinism, and fail at an early stage. This simplifies the debugging process when program errors arise.</p></div><p>In the following section, we turn to a different correctness aspect in concurrent programs, namely, testing their performance.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec88"></a>Performance debugging</h3></div></div></div><p>When it comes to performance debugging, the field is virtually endless. A separate book on the subject would barely scratch the surface. The goal of this section is to show you two basic examples that will teach you the basics of analyzing and resolving performance problems in concurrent Scala programs.</p><p>In recent years, processor clock rates have reached a limit, and processor vendors have struggled to improve single processor performance. As a consequence, multicore processors have overwhelmed the consumer market. Their main goal is to offer increased performance by increasing parallelism. Ultimately, the goal of concurrent and parallel computing is to increase the program performance.</p><p>There are two ways in which program performance can be improved. The first is through optimizing the program, so that its sequential instance runs as fast as possible. The second approach is to run parts of the program in parallel. In concurrent and parallel computing, both approaches are key to achieving optimal performance. It does not make sense to parallelize a program that is much slower than the optimal sequential program.</p><p>Thus, we will study both, how to optimize, and how to parallelize a concurrent program. We will start with a single-threaded version of the program that uses a concurrent accumulator, and ensure that it runs efficiently. Then, we will ensure that the program is also scalable, that is, adding additional processors makes it faster.</p><p>The first step in debugging the performance of a parallel program is to measure its running time. As stated in <a class="link" href="#" linkend="ch05">Chapter 5</a>, <span class="emphasis"><em>Data-Parallel Collections</em></span>, benchmarking the program performance is the only principled way of knowing how fast the program is and finding its bottlenecks. This task can be complicated on the JVM, due to effects such as garbage collection, JIT compilation, and adaptive optimizations.</p><p>Fortunately, the Scala ecosystem comes with a tool called ScalaMeter, which is designed to easily test the performance of both Scala and Java programs. The ScalaMeter tool can be used in two ways. First, ScalaMeter allows defining performance regression tests, which are essentially unit tests for performance. Second, ScalaMeter allows inline benchmarking that is used to benchmark parts of the running application. In this section, we will keep things simple, and only use ScalaMeter's inline benchmarking feature. We add the following line to our <code class="literal">build.sbt</code> file:</p><pre class="programlisting">libraryDependencies +=
  "com.storm-enroute" %% "scalameter-core" % "0.6"
</pre><p>To use ScalaMeter inside our programs, we need to import the following package:</p><pre class="programlisting">import org.scalameter._
</pre><p>This package gives us access to the <code class="literal">measure</code> statement that is used to measure various performance metrics. By default, this method measures the running time of a snippet of code. Let's use it to measure how long it takes to add one million integers to the <code class="literal">Accumulator</code> object defined in the preceding section:</p><pre class="programlisting">val time = measure {
  val acc = new Accumulator(0)(_ + _)
  var i = 0
  val total = 1000000
  while (i &lt; total) {
    acc.add(i)
    i += 1
  }
}
</pre><p>Printing the <code class="literal">time</code> value gives us the following output:</p><pre class="programlisting">
<span class="strong"><strong>Running time: 34.60</strong></span>
</pre><p>From this, we might conclude that adding one million integers takes approximately 34 milliseconds. However, this conclusion is wrong. As discussed in <a class="link" href="#" linkend="ch05">Chapter 5</a>, <span class="emphasis"><em>Data-Parallel Collections</em></span>, after a JVM program is run, it goes through a warm-up phase. The program usually achieves the best possible performance only after the warm-up phase is completed. To measure the relevant running time more accurately, we need to first ensure that the JVM reached stable performance.</p><p>The good news is that ScalaMeter can do this automatically. In the following code, we configure the <code class="literal">measure</code> call to use the default warmer implementation, called <code class="literal">Warmer.Default</code>. We set several configuration parameters, such as the minimum number of warm-up runs, the maximum number of warm-up runs, and the number of benchmark runs that are used to compute the average running time. Finally, we set the <code class="literal">verbose</code> key to <code class="literal">true</code> in order to get more logging output about ScalaMeter's execution. This is shown in the following code snippet:</p><pre class="programlisting">val accTime = config(
  Key.exec.minWarmupRuns -&gt; 20,
  Key.exec.maxWarmupRuns -&gt; 40,
  Key.exec.benchRuns -&gt; 30,
  Key.verbose -&gt; true
) withWarmer(new Warmer.Default) measure {
  val acc = new Accumulator(0L)(_ + _)
  var i = 0
  val total = 1000000
  while (i &lt; total) {
    acc.add(i)
    i += 1
  }
}
println("Accumulator time: " + accTime)
</pre><p>When running this, make sure that there are no active applications running in the background on your computer. Running this snippet of code gives us the following output:</p><pre class="programlisting">
<span class="strong"><strong>18. warmup run running time: 17.285859</strong></span>
<span class="strong"><strong>GC detected.</strong></span>
<span class="strong"><strong>19. warmup run running time: 21.460975</strong></span>
<span class="strong"><strong>20. warmup run running time: 16.557505</strong></span>
<span class="strong"><strong>21. warmup run running time: 17.712535</strong></span>
<span class="strong"><strong>22. warmup run running time: 16.355897</strong></span>
<span class="strong"><strong>Steady-state detected.</strong></span>
<span class="strong"><strong>Accumulator time: 17.24</strong></span>
</pre><p>We can now see how the running time changes during the warm-up runs. Eventually, ScalaMeter detects a steady state and outputs the running time. We now have a value of <code class="literal">17.24</code> milliseconds, which is a good estimate.</p><p>A closer inspection of the ScalaMeter output reveals that, occasionally, a <span class="strong"><strong>Garbage Collection</strong></span> (<span class="strong"><strong>GC</strong></span>) cycle occurs. These GC cycles appear periodically during the execution of our code snippet, so we conclude that something in the <code class="literal">add</code> method allocates heap objects. However, the <code class="literal">add</code> implementation does not contain any <code class="literal">new</code> statements. The object allocation must be happening implicitly somehow.</p><p>Note that the <code class="literal">Accumulator</code> class is generic. It takes a <code class="literal">T</code> type parameter, which denotes the type of the accumulation. Scala allows using both the reference types, such as <code class="literal">String</code> or <code class="literal">Option</code>, and primitive types, such as <code class="literal">Int</code> or <code class="literal">Long</code>, as class-type parameters. Although this conveniently allows treating both the primitive and reference types in the same way, it has the unfortunate side effect that the primitive values passed to generic classes are converted into heap objects. This process is known as auto-boxing, and it hurts the performance in various ways. First, it is much slower than just passing a primitive value. Second, it causes GC cycles more frequently. Third, it affects cache-locality and might cause memory contention. In the case of the <code class="literal">Accumulator</code> class, each time we call the <code class="literal">add</code> method with a <code class="literal">Long</code> value, a <code class="literal">java.lang.Long</code> object is created on the heap.</p><p>In practice, boxing is sometimes problematic, and sometimes not. Generally, it should be avoided in high-performance code. In our case, we can avoid boxing by creating an accumulator specialized for the <code class="literal">Long</code> values. We show it in the following code snippet:</p><pre class="programlisting">class LongAccumulator(z: Long)(op: (Long, Long) =&gt; Long) {
  private val value = new AtomicLong(z)
  @tailrec final def add(v: Long): Unit = {
    val ov = value.get
    val nv = op(ov, v)
    if (!value.compareAndSet(ov, nv)) add(v)
  }
  def apply() = value.get
}
</pre><p>Re-running the program reveals that the new accumulator is almost twice as fast:</p><pre class="programlisting">
<span class="strong"><strong>Long accumulator time: 8.88</strong></span>
</pre><p>Boxing can slow down the program by a factor of anywhere between one and several dozen. This depends on the specific ratio of object allocations and other work, and it needs to be measured on a per-program basis.</p><p>An unfortunate side effect is that we can only use the new accumulator implementation for <code class="literal">Long</code> values. However, Scala allows us to retain the generic nature of the previous <code class="literal">Accumulator</code> implementation. The Scala specialization feature allows the annotation of class type parameters with the <code class="literal">@specialized</code> annotation, instructing the Scala compiler to automatically generate versions of the generic class for primitive types, such as <code class="literal">Long</code>, and avoid boxing. We will not dive any further into this topic, and instead let interested readers find out more on their own.</p><p>Now that we know how to identify performance issues and optimize sequential programs, we study how to improve the performance by increasing the parallelism level. Let's parallelize the previous program by adding one million integers from four separate threads. This is shown in the following code snippet:</p><pre class="programlisting">val intAccTime4 = config(
  Key.exec.minWarmupRuns -&gt; 20,
  Key.exec.maxWarmupRuns -&gt; 40,
  Key.exec.benchRuns -&gt; 30,
  Key.verbose -&gt; true
) withWarmer(new Warmer.Default) measure {
  val acc = new LongAccumulator(0L)(_ + _)
  val total = 1000000
  val p = 4
  val threads = for (j &lt;- 0 until p) yield ch2.thread {
    val start = j * total / p
    var i = start
    while (i &lt; start + total / p) {
      acc.add(i)
      i += 1
    }
  }
  for (t &lt;- threads) t.join()
}
println("4 threads integer accumulator time: " + intAccTime4)
</pre><p>In the preceding example, we distribute the work of adding 1 million integers across four different threads, so we expect the running time of the program to increase four times. Sadly, running the program reveals that our expectations were wrong:</p><pre class="programlisting">
<span class="strong"><strong>4 threads integer accumulator time: 95.85</strong></span>
</pre><p>As pointed out in <a class="link" href="#" linkend="ch05">Chapter 5</a>, <span class="emphasis"><em>Data-Parallel Collections</em></span>, perpetually writing to the same memory location from multiple threads results in memory contention issues. In most computer architectures, cache-lines need to be exchanged between the processors writing to the same memory location, and this slows down the program. In our case, the contention point is the <code class="literal">AtomicLong</code> object in the <code class="literal">LongAccumulator</code> class. Simultaneously invoking the <code class="literal">compareAndSet</code> operation on the same memory location does not scale.</p><p>To address the issue of memory contention, we need to somehow disperse the writes throughout different cache-lines. Instead of adding the accumulated value to a single memory location, we will maintain many memory locations with partial accumulation values. When some processor calls the <code class="literal">add</code> method, it will pick one of these memory locations and update the partial accumulation. When a processor calls the <code class="literal">apply</code> method, it will scan all the partial accumulations and add them together. In this implementation, we trade the performance of the <code class="literal">apply</code> method for the improved scalability of the <code class="literal">add</code> method. This trade-off is acceptable in many cases, including our <code class="literal">fold</code> method, where we call the <code class="literal">add</code> method many times, but the <code class="literal">apply</code> method only once.</p><p>Furthermore, note that the new <code class="literal">apply</code> implementation is not linearizable, as explained in <a class="link" href="#" linkend="ch07">Chapter 7</a>, <span class="emphasis"><em>Software Transactional Memory</em></span>. If some processor calls the <code class="literal">apply</code> method when multiple processors are calling the <code class="literal">add</code> method, the resulting accumulation value can be slightly incorrect. However, if no other processor calls the <code class="literal">add</code> method when the <code class="literal">apply</code> method is called, the resulting accumulation value will be correct. We say that the new <code class="literal">apply</code> implementation is <span class="strong"><strong>quiescently consistent</strong></span> with respect to the <code class="literal">add</code> method.</p><p>Note that this property is sufficient for ensuring the correctness of the preceding <code class="literal">fold</code> implementation, because the <code class="literal">fold</code> method only calls the <code class="literal">apply</code> method after all the <code class="literal">add</code> calls are completed.</p><p>We now show the implementation of the <code class="literal">ParLongAccumulator</code> class, which uses an <code class="literal">AtomicLongArray</code> object, named <code class="literal">values</code>, to keep the partial accumulation values. Atomic arrays are arrays on which we can call operations such as the <code class="literal">compareAndSet</code> method. Conceptually, an <code class="literal">AtomicLongArray</code> is equivalent to an array of <code class="literal">AtomicLong</code> objects, but is more memory-efficient.</p><p>The <code class="literal">ParLongAccumulator</code> class must choose a proper size for the <code class="literal">AtomicLongArray</code> object. Setting the size of the array to the number of processors will not make the memory contention problems go away. Recall from <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Traditional Building Blocks of Concurrency</em></span>, that a processor needs to own a cache-line in exclusive mode before writing to it. A cache-line size is typically 64 bytes. This means that on a 32-bit JVM, eight consecutive entries in an <code class="literal">AtomicLongArray</code> object fit inside a single cache-line. Even when different processors write to separate <code class="literal">AtomicLongArray</code> entries, memory contention occurs if these entries lie in the same cache-line. This effect is known as <span class="strong"><strong>false-sharing</strong></span>. A necessary precondition in avoiding false-sharing is to make the array size at least eight times larger than the number of processors.</p><p>A <code class="literal">ParLongAccumulator</code> object is used by many different threads simultaneously. In most programs, there are many more threads than processors. To reduce false-sharing, as much as possible, we set the size of the <code class="literal">values</code> array to 128 times the number of processors:</p><pre class="programlisting">import scala.util.hashing
class ParLongAccumulator(z: Long)(op: (Long, Long) =&gt; Long) {
  private val par = Runtime.getRuntime.availableProcessors * 128
  private val values = new AtomicLongArray(par)
  @tailrec final def add(v: Long): Unit = {
    val id = Thread.currentThread.getId.toInt
    val pos = math.abs(hashing.byteswap32(id)) % par
    val ov = values.get(pos)
    val nv = op(ov, v)
    if (!values.compareAndSet(pos, ov, nv)) add(v)
  }
  def apply(): Long = {
    var total = z
    for (i &lt;- 0 until values.length)
      total = op(total, values.get(i))
    total
  }
}
</pre><p>The new <code class="literal">add</code> implementation is similar to the previous one. The main difference is that the new implementation needs to pick the <code class="literal">pos</code> memory location for the partial accumulation value. Different processors should pick different memory locations based on their index. Unfortunately, standard APIs on the JVM do not provide the index of the current processor. An adequate approximation is to compute the <code class="literal">pos</code> partial accumulation location from the current thread ID. We additionally use the <code class="literal">byteswap32</code> hashing function to effectively randomize the location in the array. This decreases the likelihood that two threads with adjacent IDs end up writing to adjacent entries in the array, and reduces the possibility of false-sharing.</p><p>Running the program demonstrates that we reached our goal, and improved the program performance by a factor of almost three:</p><pre class="programlisting">
<span class="strong"><strong>Parallel integer accumulator time: 3.34</strong></span>
</pre><p>There are additional ways to improve our <code class="literal">ParLongAccumulator</code> class. One is to further reduce false sharing by choosing the entries in the <code class="literal">values</code> array more randomly. Another is to ensure that the <code class="literal">apply</code> method is not only quiescently consistent, but also linearizable. In the interest of keeping this section simple and clear, we do not dive further into these topics, but let interested readers explore them on their own.</p><p>In this and the preceding sections, we summarized the different styles of concurrency and studied the basics of dealing with concurrency bugs. This gave us a useful insight into the big picture, but the theory that we learned is only valuable if it can be applied in practice. We designed and implemented a remote file browser application, a practical example of a large concurrent application. This gave us insight into both the theoretical and practical side of concurrent programming.</p></div></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec62"></a>Summary</h2></div></div><hr /></div><p>Having seen the technical details of a variety of different concurrency libraries in the preceding chapters, we took a couple of steps back and presented a more cohesive view of Scala concurrency. After presenting a taxonomy of different styles of concurrency, we outlined the use cases for different concurrency frameworks. We then studied how to debug concurrent programs and analyze their performance. Finally, we combined the different concurrency frameworks together to implement a real-world distributed application: a remote file browser.</p><p>The best theory is inspired by practice, and the best practice is inspired by theory. This book has given you a fair amount of both. To deepen your understanding of concurrent computing, consider studying the references listed at the end of each chapter: you should already be able to grasp most of them. Importantly, to improve your practical concurrent programming skills, try to solve the exercises from this book. Finally, start building your own concurrent applications. By now, you must have understood both how high-level concurrency abstractions work and how to use them together, and are on the path to becoming a true concurrency expert.</p></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec63"></a>Exercises</h2></div></div><hr /></div><p>The following exercises will improve your skills in building practical concurrent applications. Some of them require extending the ScalaFTP program from this chapter, while others require implementing concurrent applications from scratch. Finally, several exercises are dedicated to testing the performance and scalability of concurrent programs:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Extend the ScalaFTP application to allow the addition of directories to the remote filesystem.</p></li><li><p>Extend the ScalaFTP application so that the changes in the server filesystem are automatically reflected in the client program.</p></li><li><p>Extend the ScalaFTP application so that it allows parallel regex searches over filenames in the remote filesystem.</p></li><li><p>Extend the ScalaFTP server so that it allows recursively copying directories.</p></li><li><p>Implement the download and upload functionality, and use <code class="literal">Observable</code> objects to display the file transfer progress in a Swing <code class="literal">ProgressBar</code> component.</p></li><li><p>Extend the ScalaFTP client implementation so that a <code class="literal">FilePane</code> can display either a remote or a local filesystem's contents.</p></li><li><p>Design and implement a distributed chat application.</p></li><li><p>Design and implement a Paint program with collaborative editing.</p></li><li><p>Compare the duration of creating and starting a new thread, and waiting for its termination, against the duration of starting a computation using <code class="literal">Future.apply</code> and waiting for the completion of the corresponding <code class="literal">Future</code> object.</p></li><li><p>A pool is one of the simplest collection abstractions, which allows the addition and extraction of elements. The <code class="literal">remove</code> operation returns any element that was previously added to the pool. A concurrent pool is represented by the <code class="literal">ConcurrentPool</code> class:
</p><pre class="programlisting">        class ConcurrentPool[T] {
          def add(x: T): Unit = ???
          def remove(): T = ???
          def isEmpty(): Boolean = ???
        }
</pre><p>
</p><p>Implement the concurrent pool and make sure that its operations are linearizable. Measure and ensure high performance and scalability of your implementation.</p></li><li><p>Compare the performance and scalability of the Treiber stack from the exercise in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Concurrency on the JVM and the Java Memory Model</em></span>, against the transactional sorted list from <a class="link" href="#" linkend="ch07">Chapter 7</a>, <span class="emphasis"><em>Software Transactional Memory</em></span>. How are they compared to the concurrent pool from the previous exercise?</p></li><li><p>Implement the <code class="literal">getUniqueId</code> method from <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Concurrency on the JVM and the Java Memory Model</em></span>. Measure and ensure high performance and scalability of your implementation.</p></li><li><p>Implement a lock-free concurrent linked list and a lock-based concurrent linked list that support linearizable prepend and append operations. Both implementations must be singly linked lists. Measure the performance of inserting many elements.</p></li><li><p>A barrier is a concurrent object that allows <span class="emphasis"><em>N</em></span> threads to synchronize at some point in the program. A barrier exposes a single method, <code class="literal">await</code>, which effectively blocks the thread until all <span class="emphasis"><em>N</em></span> threads call <code class="literal">await</code>. After all <span class="emphasis"><em>N</em></span> threads call <code class="literal">await</code>, the <code class="literal">await</code> invocations of all the threads immediately return. Blocking can be done, for example, by busy-waiting. Use atomic integers to implement a barrier. Measure the performance of your implementation for one, two, four, and eight threads, and some large number of calls to <code class="literal">await</code>.</p></li></ol></div></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch10"></a>ChapterÂ 10.Â    Reactors   </h2></div></div></div><div class="blockquote"><table border="0" width="100%" cellspacing="0" cellpadding="0" class="blockquote" summary="Block quote"><tr><td width="10%" valign="top">Â </td><td width="80%" valign="top"><p><span class="emphasis"><em>"Simplicity is prerequisite for reliability."</em></span></p></td><td width="10%" valign="top">Â </td></tr><tr><td width="10%" valign="top">Â </td><td colspan="2" align="right" valign="top">--<span class="attribution"><span class="emphasis"><em>Edsger W. Dijkstra</em></span></span></td></tr></table></div><p>Location-transparency, serializable event-handling, and non-blocking semantics of sends, make the actor model a powerful foundation for building distributed systems. However, the actor model has several important limitations, which only become apparent when building larger systems. First, actors cannot simultaneously contain multiple message entry points. All messages must arrive through the same <code class="literal">receive</code> block. Consequently, two different protocols cannot reuse the same message type, and must be aware of each other. The main example where we saw this was the <code class="literal">Identify</code> message, which required users to incorporate a unique token into the message. Second, actors cannot await specific combinations of messages. For example, it is cumbersome to simultaneously send a request message to two target actors, and proceed after both replies arrive. Third, the <code class="literal">receive</code> statement is not a first-class citizen. Event streams, which we saw in the Rx framework, are first-class citizens, and this improves program composition, modularity, and separation of concerns.</p><p>In this chapter, we study the reactor programming model for distributed computing, which retains the advantages of the actor model, but overcomes the above limitations. This framework allows creating complex concurrent and distributed applications more easily, by providing correct, robust, and composable abstractions for distributed programming. Similar to the actor model, the reactor model allows writing location-transparent programs. Clear separation between units of concurrency is achieved through special entities called reactors. This separation makes it easier to reason about concurrent programs, as was the case with actors. However, computations and message exchange patterns can be more easily subdivided into modular components in the reactor model. The improved composition at the core of the reactor model is the result of a careful integration of the traditional actor model and functional reactive programming concepts.</p><p>We use the Reactors framework throughout this chapter to learn about the reactor programming model. We will cover the following topics:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Utilizing and composing event-streams to structure logic within a reactor</p></li><li style="list-style-type: disc"><p>Defining reactors and starting reactor instances</p></li><li style="list-style-type: disc"><p>Customizing reactor instances and using custom schedulers</p></li><li style="list-style-type: disc"><p>Using reactor system services to access non-standard events, and defining custom services</p></li><li style="list-style-type: disc"><p>The basics of protocol composition, along with several concrete protocol examples</p></li></ul></div><p>We start by recounting what we learned about concurrent and distributed programming, and explaining why the reactor model is important.</p></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec64"></a>The need for reactors</h2></div></div><hr /></div><p>As you may have concluded by reading this book, writing concurrent and distributed programs is not easy. Ensuring program correctness, scalability, and fault-tolerance is harder than in a sequential program. Here, we recall some of the reasons for this:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>First of all, most concurrent and distributed computations are, by their nature, non-deterministic. This non-determinism is not a consequence of poor programming abstractions, but is inherent in systems that need to react to external events.</p></li><li style="list-style-type: disc"><p>Data races are a basic characteristic of most shared-memory multicore systems. Combined with inherent non-determinism, these lead to subtle bugs that are hard to detect or reproduce.</p></li><li style="list-style-type: disc"><p>When it comes to distributed computing, things get even more complicated. Random faults, network outages, or interruptions, present in distributed programming, compromise correctness and robustness of distributed systems.</p></li><li style="list-style-type: disc"><p>Furthermore, shared-memory programs do not work in distributed environments, and existing shared-memory programs are not easily ported to a distributed setup.</p></li></ul></div><p>There is one more reason why concurrent and distributed programming is hard. When building large systems, we would like to compose simpler program components into larger entities. However, it is often hard to correctly compose concurrent and distributed programs. Correctness of specific components is no guarantee for global program correctness when those components are used together. Deadlocks inherent to locks are one such example, and potential race conditions in actors are another.</p><p>Frameworks that we have seenÂ in this book strive to address the aforementioned problems in concurrent and distributed programming. Different concurrency models try to address these issues from different angles. The intent of the reactor model, described in this chapter, is to borrow some of the best characteristics of existing frameworks, such as location-transparency, serializability and data-race freedom, and especially address the issue of composability.</p><p>To achieve these goals, the reactor model employs several minimalist abstractions, which can compose into complex protocols, algorithms, and program components. In particular, the model is based on the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Location-transparent <span class="strong"><strong>reactors</strong></span>, lightweight entities that execute concurrently with each other, but are internally always single-threaded, and can be ported from a single machine to a distributed setting. Every reactor is created with one main event stream. A reactor is a generalization of an actor from the traditional actor model.</p></li><li style="list-style-type: disc"><p>Asynchronous first-class <span class="strong"><strong>event streams</strong></span> that can be reasoned about in a declarative, functional manner, and are the basis for composing components. An event stream is the reading end of a channel. Only the reactor that owns the channel can read from the corresponding event stream. Event streams cannot be shared between different reactors. To borrow the analogy from the actor model, an event stream is a counterpart of the <code class="literal">receive</code> statement.</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Channels</strong></span> that can be shared between reactors, and are used to send events asynchronously. A channel is the writing end of the corresponding event stream, and any number of reactors can write to a channel. A channel is a close equivalent of the actor reference that we saw in the actor model.</p></li></ul></div><p>These three unique abstractions are the core prerequisite for building powerful distributed computing abstractions. Most other utilities in the Reactors framework, which we study in this chapter, are built in terms of reactors, channels, and event streams.</p></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec65"></a>Getting started with Reactors</h2></div></div><hr /></div><p>This section contains instructions on how to get Reactors working in your project. The Reactors framework has multiple languages frontend, and works on multiple platforms. At the time of writing this book, Reactors can be used with Scala and Java as a JVM library, or alternatively on NodeJS or inside the browser if you are using the <code class="literal">Scala.js</code> frontend of Reactors.</p><p>If you are developing with SBT, the easiest way is to include Reactors into your project as a library dependency. To get started with <code class="literal">Reactors.IO</code>, you should grab the latest snapshot version distributed on <span class="strong"><strong>Maven</strong></span>. If you are using SBT, add the following to your project definition:</p><pre class="programlisting">resolvers ++= Seq( 
  "Sonatype OSS Snapshots" at "https://oss.sonatype.org/content/repositories/snapshots", 
  "Sonatype OSS Releases" at "https://oss.sonatype.org/content/repositories/releases" 
) 
libraryDependencies ++= Seq( 
  "io.reactors" %% "reactors" % "0.8") 
</pre><p>At the time of writing this, the latest version is <code class="literal">0.8</code> for Scala <code class="literal">2.11</code>. After a version of Reactors is released for Scala <code class="literal">2.12</code>, you might have to replace the <code class="literal">0.8</code> version in the preceding code.</p></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec66"></a>The "Hello World" program</h2></div></div><hr /></div><p>In this section, we go through a simple, working Hello World program. We will not goÂ into too much, yet we will provide deeper information in the subsequent sections. For now, we will just define a reactor that waits for one incoming event, prints a message to the standard output once this event arrives, and then terminate.</p><p>We start by importing the contents of the <code class="literal">io.reactors</code> package:</p><pre class="programlisting">import io.reactors._ 
</pre><p>This allows us to use the facilities provided by the Reactors framework. In the following snippet, we declare a simple reactor-based program:</p><pre class="programlisting">
object ReactorHelloWorld { 
  def main(args: Array[String]): Unit = { 
    val welcomeReactor = Reactor[String] { self =&gt; 
      self.main.events onEvent { name =&gt; 
        println(s"Welcome, $name!") 
        self.main.seal() 
      } 
    } 
    val system = ReactorSystem.default("test-system") 
    val ch = system.spawn(welcomeReactor) 
    ch ! "Alan" 
  } 
} 
</pre><p>The program above declares an anonymous reactor called <code class="literal">welcomeReactor</code>, which waits for a name to arrive on its main event stream, prints that name, and then seals its main channel, therefore terminating itself. The main program then creates a new reactor system, uses the reactor template to start a new running instance of the previously defined <code class="literal">welcomeReactor</code>, and sends an event <code class="literal">"Alan"</code> to it.</p><p>By analyzing the previous program, we conclude the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>A reactor is defined using the <code class="literal">Reactor[T]</code> constructor, where <code class="literal">T</code> is the type of the events that can be sent to the reactor on its main channel.</p></li><li style="list-style-type: disc"><p>A reactor reacts to incoming events as specified in the callback function passed to the <code class="literal">onEvent</code> method. We can call <code class="literal">onEvent</code>, for example, on the main event stream of the reactor, which is obtained with the expression <code class="literal">main.events</code>.</p></li><li style="list-style-type: disc"><p>Calling <code class="literal">main.seal()</code> terminates the reactor.</p></li><li style="list-style-type: disc"><p>A reactor with a specific definition is started with the <code class="literal">spawn</code> method, which returns the reactor's main channel.</p></li><li style="list-style-type: disc"><p>Events are sent to the reactor by calling the <code class="literal">!</code> operator on one of its channels.</p></li></ul></div><p>The subsequent sections will explain each of these features in greater depth.</p></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec67"></a>Event streams</h2></div></div><hr /></div><p>In this section, we study the basic data-type that drives most computations in the Reactors framework: an event stream. Event streams represent special program values that can occasionally produce events. Event streams are represented by the <code class="literal">Event[T]</code> type.</p><p>Semantically, an event stream is very similar to the <code class="literal">Observable</code> type, which we saw in <a class="link" href="#" linkend="ch06">Chapter 6</a>, <span class="emphasis"><em>Concurrent Programming with Reactive Extensions</em></span>. As we will see, the main difference between <code class="literal">Observable</code> and <code class="literal">Events</code> is that an <code class="literal">Observable</code> object can generally be used from different threads, and even emit events across different threads when the <code class="literal">observeOn</code> method is used. An <code class="literal">Events</code> object, by contrast, can only be used inside the reactor that owns that event stream.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip107"></a>Tip</h3><p>Never share an event stream between two reactors. An event stream can only be used by the reactor that owns the corresponding channel.</p></div><p>In the following, we show an example event stream called <code class="literal">myEvents</code>, which produces events of type <code class="literal">String</code>:</p><pre class="programlisting">val myEvents: Events[String] = createEventStreamOfStrings() 
</pre><p>For now, we assume that the method <code class="literal">createEventStreamOfStrings</code> is already defined, and that it returns an event stream of type <code class="literal">Events[String]</code>.</p><p>To be useful, an event stream must allow the users to somehow manipulate the events it produces. For this purpose, every event stream has a method called <code class="literal">onEvent</code>, which takes a user callback function and invokes it every time an event arrives:</p><pre class="programlisting">myEvents.onEvent(x =&gt; println(x)) 
</pre><p>The <code class="literal">onEvent</code> method is similar to what most callback-based frameworks expose: a way to provide an executable snippet of code that is invoked later, once an event becomes available. However, just like the <code class="literal">Observable</code> object in Reactive Extensions, the receiver of the <code class="literal">onEvent</code> method, that is, the event stream, is a first-class value. This subtle difference allows passing the event stream as an argument to other methods, and consequently allows writing more general abstractions. For example, we can implement a reusable <code class="literal">trace</code> method as follows:</p><pre class="programlisting">def trace[T](events: Events[T]): Unit = { 
  events.onEvent(println) 
} 
</pre><p>The <code class="literal">onEvent</code> method returns a special <code class="literal">Subscription</code> object. Events are propagated to the user-specified callback until the user decides to call the <code class="literal">unsubscribe</code> method of that <code class="literal">Subscription</code> object. These <code class="literal">Subscription</code> objects have similar semantics as those seen in the <span class="strong"><strong>Reactive Extensions</strong></span> framework.</p><p>Before we continue, we note that event streams are entirely a single-threaded entity. The same event stream will never concurrently produce two events at the same time, so the <code class="literal">onEvent</code> method will never be invoked by two different threads at the same time on the same event stream. As we will see, this property simplifies the programming model and makes event-based programs easier to reason about.</p><p>To understand this better, let's study a concrete event stream called an emitter, represented by the <code class="literal">Events.Emitter[T]</code> type. In the following, we instantiate an emitter:</p><pre class="programlisting">val emitter = new Events.Emitter[Int] 
</pre><p>An emitter is simultaneously an event stream and an event source. We can imperatively tell the emitter to produce an event by calling its <code class="literal">react</code> method. When we do that, the emitter invokes the callbacks previously registered with the <code class="literal">onEvent</code> method.</p><pre class="programlisting">var luckyNumber = 0 
emitter.onEvent(luckyNumber = _) 
emitter.react(7) 
assert(luckyNumber == 7) 
emitter.react(8) 
assert(luckyNumber == 8) 
</pre><p>By running the above snippet, we convince ourselves that the <code class="literal">react</code> call really forces the emitter to produce an event. Furthermore, the call <code class="literal">emitter.react(8)</code> will always execute after <code class="literal">emitter.react(7)</code>, and the callback will be first invoked with <code class="literal">7</code>, and then with <code class="literal">8</code>, but not concurrently. Event propagation will occur on the same thread on which <code class="literal">react</code> was called.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec89"></a>Lifecycle of an event stream</h3></div></div></div><p>We now take a closer look at the events that an event stream can produce. An event stream of type <code class="literal">Events[T]</code> usually emits events of type <code class="literal">T</code>. However, typeÂ <code class="literal">T</code> is not the only type of events that an event stream can produce. Some event streams are finite. After they emit all their events, they emit a special event that denotes that there will be no further events. Sometimes, event streams run into exceptional situations, and emit exceptions instead of normal events.</p><p>The <code class="literal">onEvent</code> method that we saw earlier can only react to normal events. To listen to other event kinds, event streams have the more general <code class="literal">onReaction</code> method. The <code class="literal">onReaction</code> method takes an <code class="literal">Observer</code> object as an argument. An <code class="literal">Observer</code> object has three different methods used to react to different event types. In the following code snippet, we instantiate an emitter and listen to all its events:</p><pre class="programlisting">var seen = List[Int]() 
var errors = List[String]() 
var done = 0 
val e = new Events.Emitter[Int] 
e.onReaction(new Observer[Int] { 
  def react(x: Int, hint: Any) = seen ::= x 
  def except(t: Throwable) = errors ::= t.getMessage 
  def unreact() = done += 1 
}) 
</pre><p>The type Â <code class="literal">Observer[T]</code>Â has three methods:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The <code class="literal">react</code> method, which is invoked when a normal event gets emitted. The second, optional hint argument may contain an additional value, but is usually set to <code class="literal">null</code>.</p></li><li style="list-style-type: disc"><p>The <code class="literal">except</code> method, which is invoked when the event stream produces an exception. An event stream can produce multiple exceptions. An exception, however, does not terminate the stream, and many exceptions can be emitted by the same event stream. This is one big difference with respect to the typeÂ <code class="literal">Observable</code>Â from Reactive Extensions.</p></li><li style="list-style-type: disc"><p>The <code class="literal">unreact</code> method, which is invoked when the event stream stops producing events. After this method is invoked on the observer, no further events or exceptions will be produced by the event stream.</p></li></ul></div><p>Let's assert that this contract is correct for <code class="literal">Events.Emitter</code>. We already learned that we can produce events with emitters by calling theÂ <code class="literal">react</code> method. We can similarly call <code class="literal">except</code> to produce exceptions, or theÂ <code class="literal">unreact</code>Â method to signal that there will be no more events. For example:</p><pre class="programlisting">e.react(1) 
e.react(2) 
e.except(new Exception("^_^")) 
e.react(3) 
assert(seen == 3 :: 2 :: 1 :: Nil) 
assert(errors == "^_^" :: Nil) 
assert(done == 0) 
e.unreact() 
assert(done == 1) 
e.react(4) 
e.except(new Exception("o_O")) 
assert(seen == 3 :: 2 :: 1 :: Nil) 
assert(errors == "^_^" :: Nil) 
assert(done == 1) 
</pre><p>If you run the preceding codeÂ snippet, you will see that, after calling theÂ <code class="literal">unreact</code> method, subsequent calls to theÂ <code class="literal">react</code> or <code class="literal">except</code>Â methods have no effect, and the <code class="literal">unreact</code> call effectively terminates the emitter. Not all event streams are as imperative as emitters, however. Most other event streams are created by functionally composing different event streams.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec90"></a>Functional composition of event streams</h3></div></div></div><p>Using event stream methods such as <code class="literal">onEvent</code> and <code class="literal">onReaction</code> can easily result in a callback hell: a program composed of a large number of unstructured <code class="literal">onXYZ</code> calls, which is hard to understand and maintain. Having first-class event streams is a step in the right direction, but it is not sufficient.</p><p>Event streams support functional composition, seen in the earlier chapters. This pattern allows declaratively forming complex values by composing simpler ones. Consider the following example, in which we compute the sum of squares of incoming events:</p><pre class="programlisting">var squareSum = 0 
val e = new Events.Emitter[Int] 
e.onEvent(x =&gt; squareSum += x * x) 
for (i &lt;- 0 until 5) e react i 
</pre><p>The example is fairly straightforward, but what if we want to make <code class="literal">squareSum</code> an event stream so that another part of the program can react to its changes? We would have to create another emitter and have our <code class="literal">onEvent</code> callback invoke the <code class="literal">react</code> method on that new emitter, passing it the value of <code class="literal">squareSum</code>. This could work, but it is not elegant, as shown in the following snippet:</p><pre class="programlisting">val ne = new Events.Emitter[Int] 
e onEvent { x =&gt; 
  squareSum += x * x 
  ne.react(squareSum) 
} 
</pre><p>We now rewrite the previous snippet using event stream combinators. Concretely, we use the <code class="literal">map</code> and <code class="literal">scanPast</code> combinators. The <code class="literal">map</code> combinator transforms events in one event stream into events for a derived event stream. We use the <code class="literal">map</code> combinatory to produce a square of each integer event. The <code class="literal">scanPast</code> combinator combines the last and the current event to produce a new event for the derived event stream. We use <code class="literal">scanPast</code> to add the previous value of the sum to the current one. For example, if an input event stream produces numbers <code class="literal">0</code>, <code class="literal">1</code>, and <code class="literal">2</code>, the event stream produced by <code class="literal">scanPast(0)(_ + _)</code> would produce numbers <code class="literal">0</code>, <code class="literal">1</code>, and <code class="literal">3</code>.</p><p>Here is how we can rewrite the previous example:</p><pre class="programlisting">val e = new Events.Emitter[Int] 
val sum = e.map(x =&gt; x * x).scanPast(0)(_ + _) 
for (i &lt;- 0 until 5) e react i 
</pre><p>The typeÂ <code class="literal">Events[T]</code>Â comes with a large number of predefined combinators. You can find other combinators in the online API documentation. A set of event streams composed using functional combinators forms a dataflow graph. Emitters are usually source nodes in this graph, event streams created by various combinators are inner nodes, and callback methods, such as <code class="literal">onEvent</code>, are sink nodes. Combinators such as <code class="literal">union</code> take several input event streams. Such event streams correspond to graph nodes with multiple input edges. Here is one example:</p><pre class="programlisting">val numbers = new Events.Emitter[Int] 
val even = numbers.filter(_ % 2 == 0) 
val odd = numbers.filter(_ % 2 == 1) 
val numbersAgain = even union odd 
</pre><p>Dataflow graphs induced by event streams are similar in nature to dataflow graphs induced by Scala futures and <code class="literal">Observable</code> objects from Reactive Extensions, so we will not study them further in this chapter. The most important thing to remember about event streams in the reactor model is that they are single-threaded entities. As we will see in the next section, each event stream can only belong to a single reactor.</p></div></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec68"></a>Reactors</h2></div></div><hr /></div><p>As we learned previously, event streams always propagate events on a single thread. This is useful from the standpoint of program comprehension, but we still need a way to express concurrency in our programs. In this section, we will see how to achieve concurrency by using entities called reactors.</p><p>A reactor is the basic unit of concurrency. While actors receive messages, we will adopt the terminology in which reactors receive events, in order to disambiguate. However, while an actor a in particular state has only a single point where it can receive a message, namely, the <code class="literal">receive</code> statement, a reactor can receive an event from many different sources at any time. Despite this flexibility, one reactor will always process, at most, one event at any time. We say that events received by a reactor are <span class="strong"><strong>serialized</strong></span>, similar to how messages received by an actor are serialized.</p><p>To be able to create new reactors, we need a <code class="literal">ReactorSystem</code> object, which tracks reactors in a single machine:</p><pre class="programlisting">val system = new ReactorSystem("test-system") 
</pre><p>Before we can start a reactor instance, we need to define its template. One way to do this is to call <code class="literal">Reactor.apply[T]</code> method, which returns a <code class="literal">Proto</code> object for the reactor. The <code class="literal">Proto</code> object is a reactor prototype, which can be used to start the reactor. The following reactor prints all the events it receives to the standard output:</p><pre class="programlisting">val proto: Proto[Reactor[String]] = Reactor[String] { self =&gt; 
  self.main.events onEvent { 
    x =&gt; println(x) 
  } 
} 
</pre><p>Let's examine this code more closely. The <code class="literal">Reactor.apply</code> method is called with the type argument <code class="literal">String</code>. This means that the reactor encoded in the resulting <code class="literal">Proto</code> object by default receives events whose type is <code class="literal">String</code>. This is the first difference with respect to the standard actor model, in which actors can receive messages of any type. Events received by reactors are well typed.</p><p>In the reactor model, every reactor can access a special event stream called <code class="literal">main.events</code>, which emits events that the reactor receives from other reactors. Since we are declaring an anonymous reactor with the <code class="literal">Reactor.apply</code> method, we need to add a prefix <code class="literal">self</code> to access members of the reactor. We previously learned that we can call <code class="literal">onEvent</code> to register callbacks to event streams, and we used it in this example to print the events using <code class="literal">println</code>.</p><p>After defining a reactor template, the next step is to spawn a new reactor. We do this by calling the <code class="literal">spawn</code> method on the reactor system:</p><pre class="programlisting">val ch: Channel[String] = system.spawn(proto) 
</pre><p>The <code class="literal">spawn</code> method takes a <code class="literal">Proto</code> object as a parameter. The <code class="literal">Proto</code> object can generally encode the reactor's constructor arguments, scheduler, name, and other options. In our example, we created a <code class="literal">Proto</code> object for an anonymous reactor with the <code class="literal">Reactor.apply</code> method, so we do not have access to any constructor arguments. We will later see alternative ways of declaring reactors and configuring prototypes.</p><p>The <code class="literal">spawn</code> method does two things. First, it registers and starts a new reactor instance. Second, it returns a <code class="literal">Channel</code> object, which is used to send events to the newly created reactor. We show the relationship between a reactor, its event stream, and the channel in the following figure:</p><div class="mediaobject"><img src="graphics/image_10_001.jpg" /></div><p>The only way for the outside world to access the inside of a reactor is to send events to its channel. These events are eventually delivered to the corresponding event stream, which the reactor can listen to. The channel and event stream can only pass events whose type corresponds to the type of the reactor.</p><p>Let's send an event to our reactor. We do this by calling the bang operator <code class="literal">!</code> on the channel:</p><pre class="programlisting">ch ! "Hola!" 
</pre><p>Running the last statement should print the string <code class="literal">"Hola!"</code> to the standard output.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec91"></a>Defining and configuring reactors</h3></div></div></div><p>In earlier sections, we saw how to define a reactor using the <code class="literal">Reactor.apply</code> method. In this section, we take a look at an alternative way of defining a reactor--by extending the <code class="literal">Reactor</code> base class. Recall that the <code class="literal">Reactor.apply</code> method defines an anonymous reactor template. Extending the <code class="literal">Reactor</code> class declares a named reactor template.</p><p>In the following, we declare the <code class="literal">HelloReactor</code> class, which must be top-level:</p><pre class="programlisting">class HelloReactor extends Reactor[String] { 
  main.events onEvent { 
    x =&gt; println(x) 
  } 
} 
</pre><p>To run this reactor, we first create a prototype to configure it. The method <code class="literal">Proto.apply</code> takes the type of the reactor and returns a prototype for that reactor type. We then call the <code class="literal">spawn</code> method with that <code class="literal">Proto</code> object to start the reactor:</p><pre class="programlisting">val ch = system.spawn(Proto[HelloReactor]) 
ch ! "Howdee!" 
</pre><p>We can also use the prototype to, for example, set the scheduler that the reactor instance should use. If we want the reactor instance to run on its own dedicated thread to give it more priority, we can do the following:</p><pre class="programlisting">system.spawn( 
  Proto[HelloReactor].withScheduler(JvmScheduler.Key.newThread)) 
</pre><p>Note that if you are running Reactors on <code class="literal">Scala.js</code>, you will need to use a <code class="literal">Scala.js</code> specific scheduler. The reason for this is becauseÂ the JavaScript runtime, which <code class="literal">Scala.js</code> compiles to, is not multi-threaded. Asynchronous executions are placed on a single queue, and executed one after another. On <code class="literal">Scala.js</code>, you will need to use the <code class="literal">JsScheduler.Key.default</code> scheduler.</p><p>There are several other configuration options for <code class="literal">Proto</code> objects, and you can find out more about them in the online API documentation. We can summarize this section as follows. Starting a reactor is generally a three-step process:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>A named reactor template is created by extending the <code class="literal">Reactor</code> class.</p></li><li><p>A reactor configuration object is created with the <code class="literal">Proto.apply</code> method.</p></li><li><p>A reactor instance is started with the <code class="literal">spawn</code> method of the reactor system.</p></li></ol></div><p>For convenience, we can fuse the first two steps by using the <code class="literal">Reactor.apply</code> method, which creates an anonymous reactor template and directly returns a prototype object of type <code class="literal">Proto[I]</code>, for some reactor type <code class="literal">I</code>. Typically, this is what we do in the tests, or when trying things out in the Scala REPL.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec92"></a>Using channels</h3></div></div></div><p>Now that we understand how to create and configure reactors in different ways, we can take a closer look at channels, which are the reactor's means of communicating with its environment. As noted before, every reactor is created with a default channel called <code class="literal">main</code>, which is often sufficient. But sometimes a reactor needs to be able to receive more than just one type of an event, and needs additional channels for this purpose.</p><p>Let's declare a reactor that stores key-value pairs. The reactor must react to requests for storing key-value pairs, and for retrieving a value under a specific key. Since the reactor's input channel will have to serve two purposes, we need the following data type:</p><pre class="programlisting">trait Op[K, V] 
case class Put[K, V](k: K, v: V) extends Op[K, V] 
case class Get[K, V](k: K, ch: Channel[V]) extends Op[K, V] 
</pre><p>The <code class="literal">Op</code> datatype has two type parameters called <code class="literal">K</code> and <code class="literal">V</code>, which denote the types of keys and values being stored. The <code class="literal">Put</code> case class is used to store a value into the reactor, so it contains the new key and value. The <code class="literal">Get</code> case class is used to retrieve the value that was previously stored with some key, so it encodes the key and the channel of type <code class="literal">V</code>. When the reactor receives the <code class="literal">Get</code> event, it must look up the value associated with the key, and send the value along the channel.</p><p>With the <code class="literal">Op[K, V]</code> data type, we can define <code class="literal">MapReactor</code>, shown in the following snippet:</p><pre class="programlisting">class MapReactor[K, V] extends Reactor[Op[K, V]] { 
  val map = mutable.Map[K, V]() 
  main.events onEvent { 
    case Put(k, v) =&gt; map(k) = v 
    case Get(k, ch) =&gt; ch ! map(k) 
  } 
} 
</pre><p>Let's start <code class="literal">MapReactor</code> and test it. We will use the <code class="literal">MapReactor</code> to store some DNS aliases. We will map each alias <code class="literal">String</code> key to a URL, where the URLs are represented with the <code class="literal">List[String]</code> type. We first initialize as follows:</p><pre class="programlisting">val mapper = system.spawn(Proto[MapReactor[String, List[String]]]) 
</pre><p>We then send a couple of <code class="literal">Put</code> messages to store some alias values:</p><pre class="programlisting">mapper ! Put("dns-main", "dns1" :: "lan" :: Nil) 
mapper ! Put("dns-backup", "dns2" :: "com" :: Nil) 
</pre><p>Next, we create a client reactor that we control by sending it <code class="literal">String</code> events. This means that the reactor's type will be <code class="literal">Reactor[String]</code>. However, the client reactor will also have to contact the <code class="literal">MapReactor</code> and ask it for one of the URLs. Since the <code class="literal">MapReactor</code> can only send it backÂ <code class="literal">List[String]</code>Â events that do not correspond to the client's default channel type, the client's default channel is not be able to receive the reply. Therefore, the client will have to provide the <code class="literal">MapReactor</code> with a different channel. The following expression is used to create a new channel:</p><pre class="programlisting">val c: Connector[EventType] = system.channels.open[EventType] 
</pre><p>The expression <code class="literal">system.channels</code> returns a channel builder object, which provides methods such as <code class="literal">named</code> or <code class="literal">daemon</code>, used to customize the channel (see the online API docs for more details). In this example, we will create <span class="strong"><strong>daemon channel</strong></span>, to indicate that the channel does not need to be closed (more on that a bit later). To create a new channel, we call the <code class="literal">open</code> method on the channel builder with the appropriate type parameter.</p><p>The resulting <code class="literal">Connector</code> object contains two members: the <code class="literal">channel</code> field, which is the newly created channel, and the <code class="literal">events</code> field, which is the event stream corresponding to that channel. The event stream propagates all events that were sent and delivered on the channel, and can only be used by the reactor that created it. The channel, on the other hand, can be shared with other reactors.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip108"></a>Tip</h3><p>Use the <code class="literal">open</code> operation on the <code class="literal">system.channels</code> object to create new connectors. Each connector holds a pair of a channel and its event stream.</p></div><p>Let's define a client reactor that waits for a <code class="literal">"start"</code> message, and then checks a DNS entry. This reactor will use the <code class="literal">onMatch</code> handler instead of <code class="literal">onEvent</code>, to listen only to certain <code class="literal">String</code> events and ignore others:</p><pre class="programlisting">val ch = system.spawn(Reactor[String] { self =&gt; 
  self.main.events onMatch { 
    case "start" =&gt; 
      val reply = self.system.channels.daemon.open[List[String]] 
      mapper ! Get("dns-main", reply.channel) 
      reply.events onEvent { url =&gt; 
        println(url) 
      } 
    case "end" =&gt; 
      self.main.seal() 
  } 
}) 
</pre><p>In the preceding code snippet, when the reactor receives the <code class="literal">"start"</code> event from the main program, it opens a new <code class="literal">reply</code> channel that accepts <code class="literal">List[String]</code> events. It then sends a <code class="literal">Get</code> event to the <code class="literal">MapReactor</code> with the <code class="literal">"dns-main"</code> key and the <code class="literal">reply</code> channel. Finally, the reactor listens to events sent back along the <code class="literal">reply</code> channel, and prints the URL to the standard output. In the <code class="literal">"end"</code> case of the main pattern match, the reactor calls the <code class="literal">seal</code> method on the main channel to indicate that it will not receive any further events on that channel. Once all non-daemon channels become sealed, the reactor terminates.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip109"></a>Tip</h3><p>A reactor terminates either when all its non-daemon channels are sealed, or when its constructor or some event handler throws an exception.</p></div><p>Let's start the client reactor and see what happens:</p><pre class="programlisting">ch ! "start" 
</pre><p>At this point, we should witness the URL on the standard output. Finally, we can send the <code class="literal">"end"</code> message to the client reactor to stop it.</p><pre class="programlisting">ch ! "end" 
</pre><p>In the next section, we will see how to customize reactors with custom scheduling policies.</p></div></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec69"></a>Schedulers</h2></div></div><hr /></div><p>Each reactor template can be used to start multiple reactor instances, and each reactor instance can be started with a different reactor scheduler. Different schedulers have different characteristics in terms of execution priority, frequency, latency, and throughput. In this section, we take a look at how to use a non-default scheduler, and how to define custom schedulers when necessary.</p><p>We start by defining a reactor that logs incoming events, reports every time it gets scheduled, and ends after being scheduled three times. We will use the <code class="literal">sysEvents</code> stream of the reactor, which will be explained in the next section. For now, all you need to know is that the system event stream produces events when the reactor gets some execution time (that is, gets scheduled), and pauses its execution (that is, gets pre-empted).</p><p>The <code class="literal">Logger</code> reactor is shown in the following snippet:</p><pre class="programlisting">class Logger extends Reactor[String] { 
  var count = 3 
  sysEvents onMatch { 
    case ReactorScheduled =&gt; 
      println("scheduled") 
    case ReactorPreempted =&gt; 
      count -= 1 
      if (count == 0) { 
        main.seal() 
        println("terminating") 
      } 
  } 
  main.events.onEvent(println) 
</pre><p>Before starting an instance of the <code class="literal">Logger</code> reactor, we need to create a reactor system, as we learned in the earlier sections:</p><pre class="programlisting">val system = new ReactorSystem("test-system") 
</pre><p>Every reactor system is bundled with a default scheduler and some additional predefined schedulers. When a reactor is started, it uses the default scheduler, unless specified otherwise. In the following, we override the default scheduler with the one using Scala's global execution context, that is, Scala's own default thread pool:</p><pre class="programlisting">val proto = Proto[Logger].withScheduler( 
  JvmScheduler.Key.globalExecutionContext) 
val ch = system.spawn(proto) 
</pre><p>Running the snippet above should start the <code class="literal">Logger</code> reactor and print the <code class="literal">"scheduled"</code> string once, because starting a reactor schedules it even before any events arrive. If we now send an event to the main channel, we will see the <code class="literal">"scheduled"</code> string printed again, followed by the event itself. We do this as follows:</p><pre class="programlisting">ch ! "event 1" 
</pre><p>Sending the event again decrements the reactor's counter. The main channel gets sealed, leaving the reactor in a state without non-daemon channels, and the reactor terminates:</p><pre class="programlisting">ch ! "event 2" 
</pre><p>Reactor systems also allow registering custom scheduler instances. In the following, we create and register a custom <code class="literal">Timer</code> scheduler, which schedules the <code class="literal">Logger</code> reactor for execution once every 1,000 milliseconds:</p><pre class="programlisting">system.bundle.registerScheduler("customTimer", 
  new JvmScheduler.Timer(1000)) 
val periodic = system.spawn( 
  Proto[Logger].withScheduler("customTimer")) 
</pre><p>By running the code above, we can see that the reactor gets scheduled even if no events were sent to it. The <code class="literal">Timer</code> scheduler ensures that the reactor gets scheduled exactly once every N seconds, and then processes some of its pending events.</p></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec70"></a>Reactor lifecycle</h2></div></div><hr /></div><p>Every reactor goes through a certain set of stages during its lifetime, which are jointly called a <span class="strong"><strong>reactor lifecycle</strong></span>. When the reactor enters a specific stage, it emits a lifecycle event. These lifecycle events are dispatched on a special daemon event stream called <code class="literal">sysEvents</code>. Every reactor is created with this special event stream.</p><p>The reactor lifecycle can be summarized as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>After calling the <code class="literal">spawn</code> method, the reactor is scheduled for execution. Its constructor is started asynchronously, and immediately after that, a <code class="literal">ReactorStarted</code> event is dispatched.</p></li><li style="list-style-type: disc"><p>Then, whenever the reactor gets execution time, the <code class="literal">ReactorScheduled</code> event gets dispatched. After that, events get dispatched on normal event streams.</p></li><li style="list-style-type: disc"><p>When the scheduling system decides to pre-empt the reactor, the <code class="literal">ReactorPreempted</code> event is dispatched. This scheduling cycle can be repeated any number of times.</p></li><li style="list-style-type: disc"><p>Eventually, the reactor terminates, either by normal execution or exceptionally. If a user code exception terminates execution, a <code class="literal">ReactorDied</code> event is dispatched.</p></li><li style="list-style-type: disc"><p>In either normal or exceptional execution, aÂ <code class="literal">ReactorTerminated</code> event gets emitted.</p></li></ul></div><p>This reactor lifecycle is shown in the following diagram:</p><div class="mediaobject"><img src="graphics/image_10_002.jpg" /></div><p>To test this, we define the following reactor:</p><pre class="programlisting">class LifecycleReactor extends Reactor[String] { 
  var first = true 
  sysEvents onMatch { 
    case ReactorStarted =&gt; 
      println("started") 
    case ReactorScheduled =&gt; 
      println("scheduled") 
    case ReactorPreempted =&gt; 
      println("preempted") 
      if (first) first = false 
      else throw new Exception 
    case ReactorDied(_) =&gt; 
      println("died") 
    case ReactorTerminated =&gt; 
      println("terminated") 
  } 
} 
</pre><p>Upon creating the lifecycle reactor, the reactor gets the <code class="literal">ReactorStarted</code> event, and then the <code class="literal">ReactorStarted</code> and <code class="literal">ReactorScheduled</code> events. The reactor then gets suspended, and remains that way until the scheduler gives it more execution time.</p><pre class="programlisting">val ch = system.spawn(Proto[LifecycleReactor]) 
</pre><p>The scheduler executes the reactor again when it detects that there are pending messages for that reactor. If we send an event to the reactor now, we will see the same cycle of <code class="literal">ReactorScheduled</code> and <code class="literal">ReactorPreempted</code> events from the standard output. However, the <code class="literal">ReactorPreempted</code> handler this time throws an exception. The exception gets caught, and a <code class="literal">ReactorDied</code> event is emitted, followed by the mandatory <code class="literal">ReactorTerminated</code> event.</p><pre class="programlisting">ch ! "event" 
</pre><p>At this point, the reactor is fully removed from the reactor system.</p></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec71"></a>Reactor system services</h2></div></div><hr /></div><p>In the earlier sections, we learned that reactors delimit concurrent executions, and that event streams allow routing events within each reactor. This is already a powerful set of abstractions, and we can use reactors and event streams to write all kinds of distributed programs. However, such a model is restricted to reactor computations only. We cannot, for example, start blocking I/O operations, read from a temperature sensor implemented in hardware, wait until a GPU computation completes, or react to temporal events. In some cases, we need to interact with the native capabilities of the OS, or tap into a rich ecosystem of existing libraries. For this purpose, every reactor system has a set of <span class="strong"><strong>services</strong></span>: protocols that relate event streams to the outside world.</p><p>In this section, we will take a closer look at various services that are available by default, and also show how to implement custom services and plug them into reactor systems.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec93"></a>The logging service</h3></div></div></div><p>We start with the simplest possible service called <code class="literal">Log</code>. This service is used to print logging messages to the standard output. In the following, we create an anonymous reactor that uses the <code class="literal">Log</code> service. We start by importing the <code class="literal">Log</code> service:</p><pre class="programlisting">import io.reactors.services.Log 
</pre><p>Next, we create a reactor system, and start a reactor instance. The reactor invokes the <code class="literal">service</code> method on the reactor system, which returns the service singleton with the specified type. The reactor then calls the <code class="literal">apply</code> method on the <code class="literal">log</code> object to print a message, and seals itself.</p><p>This is shown in the following snippet:</p><pre class="programlisting">system.spawn(Reactor[String] { self =&gt; 
  val log = system.service[Log] 
  log("Test reactor started!") 
  self.main.seal() 
}) 
</pre><p>Running the above snippet prints the timestamped message to the standard output. This example is very simple, but we use it to describe some important properties of services:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Reactor system's method <code class="literal">service[S]</code> returns a service of type <code class="literal">S</code>.</p></li><li style="list-style-type: disc"><p>The service obtained this way is a lazily initialized singleton instance. There exists at most one instance of the service per reactor system, and it is created only after being requested by some reactor.</p></li><li style="list-style-type: disc"><p>Some standard services are eagerly initialized when the reactor system gets created. Such services are usually available as a standalone method on the <code class="literal">ReactorSystem</code> class. For example, <code class="literal">system.log</code> is an alternative way to obtain the <code class="literal">Log</code> service.</p></li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec94"></a>The clock service</h3></div></div></div><p>Having seen a trivial service example, let's take a look at a more involved service that connects reactors with the outside world of events, namely, the <code class="literal">Clock</code> service. The <code class="literal">Clock</code> service is capable of producing time-driven events, for example, timeouts, countdowns, or periodic counting. This service is standard, so it is available by calling either <code class="literal">system.clock</code> or <code class="literal">system.service[Clock]</code>.</p><p>In the following, we create an anonymous reactor that uses the <code class="literal">Clock</code> service to create a timeout event after one second. The <code class="literal">timeout</code> method of the clock service returns an event stream of the <code class="literal">Unit</code> type that always produces at most one event. We install a callback to the <code class="literal">timeout</code> event stream, which seals the main channel of this reactor. This is shown in the following snippet:</p><pre class="programlisting">import scala.concurrent.duration._ 
system.spawn(Reactor[String] { self =&gt; 
  system.clock.timeout(1.second) on { 
    println("done") 
    self.main.seal() 
  } 
}) 
</pre><p>The <code class="literal">Clock</code> service uses a separate timer thread under-the-hood, which sends events to the reactor when the timer thread decides it is time to do so. The events are sent on a special channel created by the <code class="literal">timeout</code> method, so they are seen only on the corresponding event stream combinator. This is summarized in the following figure:</p><div class="mediaobject"><img src="graphics/image_10_003.jpg" /></div><p>When the main channel gets sealed, the reactor terminates. This is because the <code class="literal">timeout</code> event stream creates a daemon channel under-the-hood, which does not prevent our anonymous reactor from terminating after non-daemon channels are gone.</p><p>The <code class="literal">Clock</code> service shows a general pattern: when a native entity or an external event needs to communicate with a reactor, it creates a new channel, and then asynchronously sends events to it.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec95"></a>The channels service</h3></div></div></div><p>Some services provide event streams that work with reactor system internals. The <code class="literal">Channels</code> service is one such example--it provides an event-driven view over all channels that exist in the current reactor system. This allows polling the channels that are currently available, or waiting until a channel with a specific name becomes available. Awaiting a channel is particularly useful, as it allows easier handling of asynchrony between reactors, which is inherent to distributed systems.</p><p>As a side-note, we actually saw and used the <code class="literal">Channels</code> service earlier, when we opened a second channel in a reactor. The expression <code class="literal">system.channels.open</code> actually calls the <code class="literal">open</code> method on the standard channel service. The channels service thus not only allows querying channels that exist in the reactor system, but also creating new channels within existing reactors.</p><p>To show basic usage of the <code class="literal">Channels</code> service, we construct two reactors. The first reactor will create a channel named <code class="literal">"hidden"</code> after some delay, and the second reactor will wait for that channel. When the channel appears, the second reactor will send an event to that channel. The first reactor prints the string <code class="literal">"event received"</code> after it receives the message, sealing its main channel. This is shown in the following snippet:</p><pre class="programlisting">val first = Reactor[String] { self =&gt; 
  system.clock.timeout(1.second) on { 
    val c = system.channels.daemon.named("hidden").open[Int] 
    c.events on { 
      println("event received") 
      self.main.seal() 
    } 
  } 
} 
system.spawn(first.withName("first")) 
system.spawn(Reactor[String] { self =&gt; 
  system.channels.await[Int]("first", "hidden") onEvent { ch =&gt; 
    ch ! 7 
    self.main.seal() 
  } 
}) 
</pre><p>In the precedingÂ program, we use the <code class="literal">Clock</code> service seen earlier to introduce a delay in the first reactor. In the second reactor, we use the <code class="literal">Channels</code> service to wait for the channel named <code class="literal">"hidden"</code> of the reactor named <code class="literal">"first"</code>. Both reactors start at approximately the same time.</p><p>After one second, the first reactor uses the <code class="literal">Channels</code> service to open a new daemon channel named <code class="literal">"hidden"</code>. The first reactor then installs a callback: when the first event arrives on the hidden channel, it prints a message to the standard output, and the main channel is sealed, to ensure that the reactor terminates. The second reactor gets an event from the <code class="literal">Channels</code> service, since a channel with the desired name now exists. This reactor sends a value <code class="literal">7</code> to the hidden channel, and terminates.</p><p>To conclude, waiting for channels to appear is important when establishing temporal order in an asynchronous system. In general, the creation of the hidden channel in the first reactor could have been delayed by an arbitrary amount by the reactor system, and the <code class="literal">Channels</code> service allows the computation to proceed only after specific channels in other reactors get created.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec96"></a>Custom services</h3></div></div></div><p>Having seen a few existing services, we now show how to create a custom service. To do this, we must implement the <code class="literal">Protocol.Service</code> trait, which has a single member method called <code class="literal">shutdown</code>:</p><pre class="programlisting">class CustomService(val system: ReactorSystem) 
extends Protocol.Service { 
  def shutdown(): Unit = ??? 
} 
</pre><p>The <code class="literal">shutdown</code> method is called when the corresponding reactor system gets shut down, and is used to free any resources that the service potentially has. Any custom service must additionally have a single parameter constructor that takes a <code class="literal">ReactorSystem</code> object, which allows the service to interact with and use the reactor system during its existence.</p><p>As noted earlier, a service is a mechanism that gives access to events that a reactor normally cannot obtain from other reactors. Let's implement a service that notifies a reactor when the enclosing reactor system gets shut down. For this, we will need to keep a map of the channels that subscribed to the shutdown event and a lock to protect access to that state. Finally, we will expose a method <code class="literal">state</code>, which creates an event stream that emits an event when the reactor system is shut down.</p><p>The <code class="literal">state</code> method will return a special kind of event stream called a <code class="literal">Signal</code>. The <code class="literal">Signal</code> type extends the <code class="literal">Events</code> type, and a signal object emits events whenever its value changes. Additionally, a <code class="literal">Signal</code> caches the value of the previously emitted event, which can be accessed with the signal's <code class="literal">apply</code> method. Any event stream can be converted into a signal by calling the <code class="literal">toSignal</code> method.</p><p>The <code class="literal">state</code> method, called by a specific reactor, must create a new daemon channel called <code class="literal">shut</code>. This channel is added to the <code class="literal">subscribers</code> set of the shutdown service. The event stream associated with this channel is converted into a signal with the initial value <code class="literal">false</code>, and returned to the caller.</p><p>The implementation of the <code class="literal">Shutdown</code> service is shown in the following snippet:</p><pre class="programlisting">class Shutdown(val system: ReactorSystem) 
extends Protocol.Service { 
  private val subscribers = mutable.Set[Channel[Boolean]]() 
  private val lock = new AnyRef 
  def state: Signal[Boolean] = { 
    val shut = system.channels.daemon.open[Boolean] 
    lock.synchronized { 
      subscribers += shut.channel 
    } 
    shut.events.toSignal(false) 
  } 
  def shutdown() { 
    lock.synchronized { 
      for (ch &lt;- subscribers) ch ! true 
    } 
  } 
} 
</pre><p>We can now use the <code class="literal">Shutdown</code> service in user programs. This is shown in the following snippet:</p><pre class="programlisting">val system = ReactorSystem.default("test-shutdown-system") 
system.spawn(Reactor[Unit] { self =&gt; 
  system.service[Shutdown].state on { 
    println("Releasing important resource.") 
    self.main.seal() 
  } 
}) 
</pre><p>Later, when we shut down the system, we expect that the code in the callback runs and completes the promise:</p><pre class="programlisting">system.shutdown() 
</pre><p>Note that, when implementing a custom service, we are no longer in the same ballpark as when writing normal reactor code. A service may be invoked by multiple reactors concurrently, and this is why we had to synchronize access to the subscribers map in the <code class="literal">Shutdown</code> implementation. In general, when implementing a custom service, we have to take care to:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Never block or acquire a lock in the service constructor</p></li><li style="list-style-type: disc"><p>Ensure that access to shared state of the service is properly synchronized</p></li></ul></div><p>In conclusion, you should use custom services when you have a native event-driven API that must deliver events to reactors in your program, or wish to expose access to internals of the reactor system, the OS or the underlying hardware. Often the implementation of a reactor system service will employ some lower-level concurrency primitives, but will expose a high-level API that relies on event streams and channels.</p></div></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec72"></a>Protocols</h2></div></div><hr /></div><p>Reactors, event streams, and channels form the cornerstone of the reactor programming model. These basic primitives allow composing powerful communication abstractions. In this section, we go through some of the basic communication protocols that the Reactors framework implements in terms of its basic primitives. What these protocols have in common is that they are not artificial extensions of the basic model. Rather, they are composed from basic abstractions and other simpler protocols.</p><p>We start with one of the simplest protocols, namely the <span class="strong"><strong>server-client</strong></span> protocol. First, we show how to implement a simple server-client protocol ourselves. After that, we show how to use the standard server-client implementation provided by the Reactors framework. In the later sections on protocols, we will not dive into the implementation, but instead immediately show how to use the protocol predefined in the framework.</p><p>This approach will serve several purposes. First, you should get an idea of how to implement a communication pattern using event streams and channels. Second, you will see that there is more than one way to implement a protocol and expose it to clients. Finally, you will see how protocols are structured and exposed in the Reactors framework.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec97"></a>Custom server-client protocol</h3></div></div></div><p>In this subsection, we implement the server-client protocol ourselves. Before we start, we have to create a default reactor system:</p><pre class="programlisting">val system = ReactorSystem.default("system") 
</pre><p>Let's now consider the server-client protocol more closely. This protocol proceeds as follows: first, the client sends a request value to the server. Then, the server uses the request to compute a response value and send it to the client. But to do that, the server needs a response channel, which serves as the destination to send the response value to. This means that the client must not only send the request value to the server, but also send a channel used for the reply. The request sent by the client is thus a tuple with a value and the reply channel. The server channel used by the server must accept such tuples. We capture these relationships with the following two types:</p><pre class="programlisting">type Req[T, S] = (T, Channel[S]) 
type Server[T, S] = Channel[Req[T, S]] 
</pre><p>Here, <code class="literal">T</code> is the type of the request value, and <code class="literal">S</code> is the type of the response value. The <code class="literal">Req</code> type represents the request: a tuple of the request value <code class="literal">T</code> and the reply channel for responses of type <code class="literal">S</code>. The <code class="literal">Server</code> type is then just a channel that accepts request objects.</p><p>Next, we ask ourselves--how do we create a <code class="literal">Server</code> channel? There are several requirements that a factory method for the <code class="literal">Server</code> channel should satisfy. First, the server method should be generic in the request and the response type. Second, it should be generic in how the request type is mapped to the response type. Third, when a request is sent to the server, the mapped response should be sent back to the server. Putting these requirements together, we arrive at the following implementation of the <code class="literal">server</code> method, which instantiates a new server:</p><pre class="programlisting">def server[T, S](f: T =&gt; S): Server[T, S] = { 
  val c = system.channels.open[Req[T, S]] 
  c.events onMatch { 
    case (x, reply) =&gt; reply ! f(x) 
  } 
  c.channel 
} 
</pre><p>The <code class="literal">server</code> method starts by creating a connector for <code class="literal">Req[T, S]</code> type. It then adds a callback to the event stream of the newly created connector. The callback decomposes the request tuple into the request value <code class="literal">x</code> of type <code class="literal">T</code> and the <code class="literal">reply</code> channel, then maps the input value using the specified mapping function <code class="literal">f</code>, and finally sends the mapped value of type <code class="literal">S</code> back along the <code class="literal">reply</code> channel. The <code class="literal">server</code> method returns the channel associated with this connector. We can use this method to start a server that maps request strings to uppercase strings, as follows:</p><pre class="programlisting">val proto = Reactor[Unit] { self =&gt; 
  val s = server[String, String](_.toUpperCase) 
} 
system.spawn(proto) 
</pre><p>Next, we will implement the client protocol. We will define a new method called <code class="literal">?</code> on the <code class="literal">Channel</code> type, which sends the request to the server. This method cannot immediately return the server's response, because the response arrives asynchronously. Instead, method <code class="literal">?</code> must return an event stream with the server's reply. So, the <code class="literal">?</code> method must create a reply channel, send the <code class="literal">Req</code> object to the server, and then return the event stream associated with the reply channel. This is shown in the following snippet:</p><pre class="programlisting">implicit class ServerOps[T, S: Arrayable](val s: Server[T, S]) { 
  def ?(x: T): Events[S] = { 
    val reply = system.channels.daemon.open[S] 
    s ! (x, reply.channel) 
    reply.events 
  } 
} 
</pre><p>In the code above, we defined an extension method <code class="literal">?</code> for objects of the <code class="literal">Server</code> type by declaring an implicit class <code class="literal">ServerOps</code>. The <code class="literal">Arrayable</code> context bound on type <code class="literal">S</code> is required in the Reactors framework to enable the creation of arrays. The Reactors framework requires the <code class="literal">Arrayable</code> type class whenever we want to open a channel of a generic type, which is in this case the type <code class="literal">S</code>.</p><p>We now show the interaction between the server and the client by instantiating the two protocols within the same reactor. The server just returns an uppercase version of the input string, while the client sends the request with the content <code class="literal">"hello"</code>, and prints the response to the standard output. This is shown in the following snippet:</p><pre class="programlisting">val serverClient = Reactor[Unit] { self =&gt; 
  val s = server[String, String](_.toUpperCase) 
 
  (s ? "hello") onEvent { upper =&gt; 
    println(upper) 
  } 
} 
system.spawn(serverClient) 
</pre><p>Our implementation works, but it is not very useful to start the server-client protocol inside a single reactor. Normally, the server and the client are separated by the network, or are at least different reactors running inside the same reactor system.</p><p>It turns out that, with our toy implementation of the server-client protocol, it is not straightforward to instantiate the protocol in two different reactors. The main reason for this is that once the server channel is instantiated within one reactor, we have no way of <span class="emphasis"><em>seeing</em></span> it in another reactor. The server channel is hidden inside the lexical scope of the server reactor. We will see how to easily overcome this problem with the standard server-client implementation that the Reactors framework provides.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec98"></a>Standard server-client protocol</h3></div></div></div><p>We have just seen an example implementation of the server-client protocol, which relies only on the basic primitives provided by the Reactors framework. However, the implementation that was presented is very simplistic, and it ignores several important concerns. For example, how do we stop the server protocol? Also, we instantiated the server-client protocol in a single reactor, but is it possible to instantiate server-client in two different reactors?</p><p>In this section, we take a closer look at how the server-client protocol is exposed in the Reactors framework, and explain how some of the above concerns are addressed. Most predefined protocols can be instantiated in several ways:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>By installing the protocol on the existing connector inside an existing reactor, which has an appropriate type for that protocol. The main benefit of this is that you can install the protocol on, for example, the main channel of a reactor. This also makes the protocol accessible to other reactors that are aware of that respective channel.</p></li><li style="list-style-type: disc"><p>By creating a new connector for the protocol, and then installing the protocol to that connector. The main benefit of this is that you can fully customize the protocol's connector (for example, name it), but you will need to find some way of sharing the protocol's channel with other reactors, for example, by using it on the <code class="literal">Channels</code> service, or by sending the channel to specific reactors.</p></li><li style="list-style-type: disc"><p>By creating a new <code class="literal">Proto</code> object for a reactor that exclusively runs a specific protocol. The main benefit of this is being able to fully configure the reactor that you wish to start (for example, specify a scheduler, reactor name, or transport).</p></li><li style="list-style-type: disc"><p>By immediately spawning a reactor that runs a specific protocol. This is the most concise option.</p></li></ul></div><p>These approaches are mostly equivalent, but they represent different trade-offs between convenience and customization. Let's take a look at the predefined server-client protocol to study these approaches in turn.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch10lvl3sec0"></a>Using an existing connector</h4></div></div></div><p>When using an existing connector, we need to ensure that the connector's type matches the type needed by the protocol. In the case of a server, the connector's event type must be <code class="literal">Server.Req</code>. In the following, we define a server prototype that multiplies the request integer by <code class="literal">2</code> to compute a response. To install the server-client protocol, we call the <code class="literal">serve</code> method on the connector:</p><pre class="programlisting">val proto = Reactor[Server.Req[Int, Int]] { self =&gt; 
  self.main.serve(x =&gt; x * 2) 
} 
val server = system.spawn(proto) 
</pre><p>The client can then query the <code class="literal">server</code> channel using the <code class="literal">?</code> operator. For convenience, we use the <code class="literal">spawnLocal</code> method, which simultaneously defines an anonymous reactor template and uses it to spawn a new client reactor. This is shown in the following snippet:</p><pre class="programlisting">system.spawnLocal[Unit] { self =&gt; 
  (server ? 7) onEvent { response =&gt; 
    println(response) 
  } 
} 
</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch10lvl3sec1"></a>Creating a new connector</h4></div></div></div><p>Let's say that the main channel is already used for something else. For example, the main channel could be accepting termination requests. Consequently, the main channel cannot be shared with the server protocol, as protocols usually need exclusive ownership of the respective channel. In such cases, we want to create a new connector for the protocol.</p><p>This approach is very similar to using an existing connector. The only difference is that we must first create the connector itself, giving us an opportunity to customize it. In particular, we will make the server a <code class="literal">daemon</code> channel, and we will assign it a specific name <code class="literal">"server"</code>, so that other reactors can find it. We will name the reactor itself <code class="literal">"Multiplier"</code>. To create a server connector, we use the convenience method called <code class="literal">server</code> on the channel builder object, to get a new connector of the appropriate type.</p><p>We can then call the <code class="literal">serve</code> method on the connector to start the protocol. This is shown in the following snippet:</p><pre class="programlisting">val proto = Reactor[String] { self =&gt; 
  self.main.events onMatch { 
    case "terminate" =&gt; self.main.seal() 
  } 
  self.system.channels.daemon.named("server") 
    .server[Int, Int].serve(_ * 2) 
} 
system.spawn(proto.withName("Multiplier")) 
</pre><p>The client must now query the name service to find the server channel, and from there on it proceeds as before, as shown in the following:</p><pre class="programlisting">system.spawnLocal[Unit] { self =&gt; 
  self.system.channels.await[Server.Req[Int, Int]]( 
    "Multiplier", "server" 
  ) onEvent { server =&gt; 
    (server ? 7) onEvent { response =&gt; 
      println(response) 
    } 
  } 
} 
</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch10lvl3sec2"></a>Creating a protocol-specific reactor prototype</h4></div></div></div><p>When we are sure that the reactor will exist only, or mainly, for the purposes of the server protocol, we can directly create a reactor server. To do this, we use the <code class="literal">server</code> method on the <code class="literal">Reactor</code> companion object. The <code class="literal">server</code> method returns the <code class="literal">Proto</code> object for the server, which can then be further customized before spawning the reactor. The <code class="literal">server</code> method takes a user function that is invoked each time a request arrives. This user function takes the state of the server and the request event, and returns the response event. This is shown in the following code snippet:</p><pre class="programlisting">val proto = Reactor.server[Int, Int]((state, x) =&gt; x * 2) 
val server = system.spawn(proto) 
 
system.spawnLocal[Unit] { self =&gt; 
  (server ? 7) onEvent { response =&gt; 
    println(response) 
  } 
} 
</pre><p>The <code class="literal">state</code> object for the server contains a <code class="literal">Subscription</code> object, which allows the users to stop the server if, for example, an unexpected event arrives.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch10lvl3sec3"></a>Spawning a protocol-specific reactor directly</h4></div></div></div><p>Finally, we can immediately start a server reactor, without any customization. This is done by passing a server function to the <code class="literal">server</code> method on the <code class="literal">ReactorSystem</code>, as follows:</p><pre class="programlisting">val server = system.server[Int, Int]((state, x) =&gt; x * 2) 
 
system.spawnLocal[Unit] { self =&gt; 
  (server ? 7) onEvent { response =&gt; println(response)  } 
} 
</pre><p>In the subsequent sections, we will take a look at some other predefined protocols, which have similar API as the server-client protocol.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec99"></a>Router protocol</h3></div></div></div><p>In this section, we take a look at a simple router protocol. Here, events coming to a specific channel are routed between a set of target channels, according to some user-specified policy. In practice, there are a number of applications of this protocol, ranging from data replication and sharding, to load-balancing and multicasting. The protocol is illustrated in the following figure:</p><div class="mediaobject"><img src="graphics/image_10_004.jpg" /></div><p>To show the router protocol in action, we will instantiate a master reactor that will route the incoming requests between two workers. In a real system, requests typically represent workloads, and workers execute computations based on those requests. For simplicity, requests will be just strings, and the workers will just print those strings to the standard output.</p><p>As was the case with the server-client protocol, there are several ways to instantiate the router protocol. First, the protocol can be started within an existing reactor, in which case it is just one of the protocols running inside that reactor. Alternatively, the protocol can be started as a standalone reactor, in which case that reactor is dedicated to the router protocol. In our example, we create an instance of the router protocol in an existing reactor.</p><p>We first start two workers, called <code class="literal">worker1</code> and <code class="literal">worker2</code>. These two reactors will print incoming events to the standard output. We use a shorthand method <code class="literal">spawnLocal</code>, to concisely start the reactors without creating the <code class="literal">Proto</code> object:</p><pre class="programlisting">val worker1 = system.spawnLocal[String] { self =&gt; 
  self.main.events.onEvent(x =&gt; println(s"1: ${x}")) 
} 
val worker2 = system.spawnLocal[String] { self =&gt; 
  self.main.events.onEvent(x =&gt; println(s"2: ${x}")) 
} 
</pre><p>Next, we declare a reactor whose main channel takes <code class="literal">Unit</code> events, since we will not be using the main channel for anything special. Inside that reactor, we first call theÂ <code class="literal">router</code> method on the <code class="literal">channels</code> service to open a connector with the appropriate type for the router. By just calling the <code class="literal">router</code> method, the router protocol does not yet start. We need to call the <code class="literal">route</code> method on the newly created connector to actually start routing.</p><p>The <code class="literal">route</code> method expects a <code class="literal">Router.Policy</code> object as an argument. The policy object contains a function that returns a channel for an event that we want to route. This function of type <code class="literal">T =&gt; Channel[T]</code> represents the routing logic for the router protocol.</p><p>In our example, we will use the simple round-robin policy. This policy can be instantiated with the <code class="literal">Router.roundRobin</code> factory method, which expects a list of channels for the round-robin policy, so we will pass a list with <code class="literal">worker1</code> and <code class="literal">worker2</code> channels. We show this in the following snippet:</p><pre class="programlisting">system.spawnLocal[Unit] { self =&gt; 
  val router = system.channels.daemon.router[String] 
    .route(Router.roundRobin(Seq(worker1, worker2))) 
  router.channel ! "one" 
  router.channel ! "two" 
} 
</pre><p>After starting the router protocol and sending the events <code class="literal">"one"</code> and <code class="literal">"two"</code> to the router channel, the two strings are delivered to the two different workers. The <code class="literal">roundRobin</code> policy does not specify which of the target channels is chosen first, so the output can either contain <code class="literal">"1: one"</code> and <code class="literal">"2: two"</code>, or <code class="literal">"1: two"</code> and <code class="literal">"2: one"</code>.</p><p>The round-robin routing policy does not have any knowledge about the two target channels, so it just picks one after another in succession, and then the first one again when it reaches the end of the target list. Effectively, this policy constitutes a very simple form of load-balancing.</p><p>There are other predefined policies that can be used with the router protocol. For example, the <code class="literal">Router.random</code> policy uses a random number generator to route events to different channels, which is more robust in scenarios when a high-load event gets sent periodically. Another policy is <code class="literal">Router.hash</code>, which computes the hash code of the event, and uses it to find the target channel. If either of these are not satisfactory, <code class="literal">deficitRoundRobin</code> strategy tracks the expected cost of each event, and biases its routing decisions to balance the total cost sent to each target. Users can also create custom routing policies for other use-cases.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec100"></a>Two-way protocol</h3></div></div></div><p>In this section, we show a two-way communication protocol. In two-way communication, two parties obtain a connection handle of type <code class="literal">TwoWay</code>, which allows them to simultaneously send and receive an unlimited number of events until they decide to close this connection. One party initiates the connection, so we call that party the client, and the other party the server. The <code class="literal">TwoWay</code> type has two type parameters <code class="literal">I</code> and <code class="literal">O</code>, which describe the types of input and output events, respectively, from the client's point of view. This is illustrated in the following figure:</p><div class="mediaobject"><img src="graphics/image_10_005.jpg" /></div><p>Note that these types are reversed depending on whether you are looking at the connection from the server-side or from the client-side. The type of the client-side two-way connection is:</p><pre class="programlisting">val clientTwoWay: TwoWay[In, Out] 
</pre><p>Whereas the type of the server sees the two-way connection as:</p><pre class="programlisting">val serverTwoWay: TwoWay[Out, In] 
</pre><p>Accordingly, the <code class="literal">TwoWay</code> object contains an output channel <code class="literal">output</code>, and an input event stream <code class="literal">input</code>. To close the connection, the <code class="literal">TwoWay</code> object contains a subscription object called <code class="literal">subscription</code>, which is used to close the connection and free the associated resources.</p><p>Let's create an instance of the two-way protocol. This protocol works in two phases. First, a client asks a two-way connection server to establish a two-way connection. After that, the client and the server use the two-way channel to communicate.</p><p>In what follows, we declare a reactor, and instantiate a two-way connection server within that reactor. For each established two-way connection, the two-way server will receive strings, and send back the length of those strings.</p><pre class="programlisting">val seeker = Reactor[Unit] { self =&gt; 
  val lengthServer = self.system.channels 
    .twoWayServer[Int, String].serveTwoWay() 
</pre><p>The two lines above declare a reactor <code class="literal">Proto</code> object, which instantiates a two-way server called <code class="literal">lengthServer</code>. We first called the <code class="literal">twoWayServer</code> method on the <code class="literal">Channels</code> service, and specified the input and the output type (from the point of view of the client). Then, we called the <code class="literal">serverTwoWay</code> method to start the protocol. In our case, we set the input type <code class="literal">I</code> to <code class="literal">Int</code>, meaning that the client will receive integers from the server, and the output type <code class="literal">O</code> to <code class="literal">String</code>, meaning that the client will be sending strings to the server.</p><p>The resulting object <code class="literal">lengthServer</code> represents the state of the connection. It contains an event stream called <code class="literal">connections</code>, which emits an event every time a client requests a connection. If we do nothing with this event stream, the server will remain silent - it will start new connections, but ignore events coming from the clients. How exactly the client and server communicate over the two-way connection (and when to terminate this connection) is up to the user to specify. To customize the two-way communication protocol with our own logic, we need to react to the <code class="literal">TwoWay</code> events emitted by the <code class="literal">connections</code> event stream, and install callbacks to the <code class="literal">TwoWay</code> objects.</p><p>In our case, for each incoming two-way connection, we want to react to <code class="literal">input</code> strings by computing the length of the string, and then sending that length back along the <code class="literal">output</code> channel. We can do this as follows:</p><pre class="programlisting">  lengthServer.connections.onEvent { serverTwoWay =&gt; 
    serverTwoWay.input.onEvent { s =&gt; 
      serverTwoWay.output ! s.length 
    } 
  } 
</pre><p>We now have a working instance of the two-way connection server. The current state of the reactor can be illustrated with the following figure, where our new channel appears alongside standard reactor channels:</p><div class="mediaobject"><img src="graphics/image_10_006.jpg" /></div><p>Next, let's start the client-side part of the protocol. The client must use the two-way server channel to request a connection. The <code class="literal">lengthServer</code> object that we saw earlier has a field called <code class="literal">channel</code> that must be used for this purpose. The client must know about this channel to start the connection. Note that only the <code class="literal">channel</code> must be shared, not the complete <code class="literal">lengthServer</code> object. To make things simple, we will instantiate the client-side part of the protocol inside the same reactor as the server-side part.</p><p>To connect to the server, the client must invoke the <code class="literal">connectTwoWay</code> extension method on the <code class="literal">channel</code>. This method is only available when the package <code class="literal">io.reactors.protocol</code> is imported, and works on two-way server channels. The <code class="literal">connect</code> method returns an event stream that emits a <code class="literal">TwoWay</code> object once the connection gets established.</p><p>In the following, we connect to the server. Once the server responds, we use the <code class="literal">TwoWay[Int, String]</code> object to send a string event, and then print the length event that we get back:</p><pre class="programlisting">  lengthServer.channel.connect() onEvent { clientTwoWay =&gt; 
    clientTwoWay.output ! "What's my length?" 
    clientTwoWay.input onEvent { len =&gt; 
      if (len == 17) println("received correct reply") 
      else println("reply incorrect: " + len) 
    } 
  } 
} 
 
system.spawn(seeker) 
</pre><p>After the connection is established, the state of the reactor and its connectors is as shown in the following diagram:</p><div class="mediaobject"><img src="graphics/image_10_007.jpg" /></div><p>Note that, in this case, the two-way channel has both endpoints in the same reactor. This is because we called <code class="literal">twoWayServe</code> and <code class="literal">connect</code> in the same reactor, for the purposes of demonstration. In real scenarios, we would typically invoke these two operations on separate reactors.</p></div></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec73"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we learned about the reactor model, and its implementation in the Reactors framework. We saw how to define and instantiate reactors, compose event streams, customize reactor names and assign schedulers, use reactor system services, and define custom ones. Importantly, we saw how to use a few basic low-level protocols such as the server-client, router, and the two-way connection protocol.</p><p>To learn more about reactors, you can find a lot of information on the website of the Reactors framework, at <a class="ulink" href="http://reactors.io/" target="_blank">http://reactors.io</a>. The Reactors framework is relatively new, but it is under constant development. As the framework matures and gains more features, you will find more and more information on the website. To learn more about the reactor programming model itself, the paper <span class="emphasis"><em>Reactors, Channels, and Event Streams for Composable Distributed Programming</em></span> is worth taking a look at.</p></div>
</div></div></div>
<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec74"></a>Exercises</h2></div></div><hr /></div><p>In the following exercises, you are expected to define several reactor protocols. In some cases, the task is to first investigate a specific algorithm online on your own, and then implement it using the Reactors framework. The exercises are ordered by their difficulty, and range from simple tasks to more complex ones.</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Define a method called <code class="literal">twice</code>, which takes a target channel, and returns a channel that forwards every event twice to the target.
</p><pre class="programlisting">        def twice[T](target: Channel[T]): Channel[T] 
</pre></li><li><p>Define a method called <code class="literal">throttle</code>, which throttles the rate at which events are forwarded to the target channel.</p><pre class="programlisting">        def throttle[T](target: Channel[T]): Channel[T] 
</pre><p>
</p><p><span class="strong"><strong>Hint</strong></span>: you will have to use the <code class="literal">Clock</code> service and the functional event stream composition.</p></li><li><p>The <code class="literal">Shutdown</code> service shown in this chapter can run out of memory if there are a lot of reactors subscribing to it. This is because the current implementation never removes entries from the service's <code class="literal">subscribers</code> map. Modify the custom <code class="literal">Shutdown</code> service so that the clients of the <code class="literal">state</code> signals can unsubscribe from listening to shutdown events. Additionally, ensure that when a reactor terminates, it unsubscribes from the <code class="literal">Shutdown</code> service if it was subscribed to it. Use the <code class="literal">sysEvents</code> event stream for this purpose.</p></li><li><p>Assume that normal <code class="literal">Channel</code> objects can occasionally lose some events or reorder them, but never duplicate or corrupt events. Implement a reliable channel protocol, which ensures that every event sent through a channel is delivered to its destination in the order it was sent. Define two methods <code class="literal">reliableServer</code> and <code class="literal">openReliable</code>, which are used to start the reliable connection server and open the reliable connection on the client, respectively. The methods must have the following signatures, where it is up to you to determine the types:
</p><pre class="programlisting">        def reliableServer[T](): Channel[Reliable.Req[T]] 
        def openReliable[T] 
        (s: Channel[Reliable.Req[T]]): Events[Channel[T]] 
</pre></li><li><p>Implement the <span class="emphasis"><em>best-effort broadcast protocol</em></span>, which delivers events to multiple targets. The broadcast method must implement the following interface, where events sent to the resulting channel must be forwarded to all the targets:
</p><pre class="programlisting">        def broadcast(targets: Seq[Channel[T]]): Channel[T] 
</pre></li><li><p>Investigate and learn about how the CRDT counter algorithm works. Then, use the best-effort broadcast protocol from an earlier exercise to implement the CRDT counter algorithm. Define a method called <code class="literal">crdt</code> to allow users to create the CRDT counter.</p></li><li><p>Implement a <code class="literal">failureDetector</code> method, which takes a heartbeat server of <code class="literal">Unit</code> request and response types, and returns a <code class="literal">Signal</code> object that denotes whether the server is suspected to have failed:
</p><pre class="programlisting">        def failureDetector(s: Server[Unit, Unit]): Signal[Boolean] 
</pre><p>
</p><p>The protocol started by this method must regularly send heartbeat signals to the server, and expect replies within a certain time period. The server is suspected to have failed when its response does not arrive before that time period elapses. Implement a unit test to validate that the resulting signal correctly detects server failure.</p></li><li><p>Implement the <span class="emphasis"><em>reliable broadcast algorithm</em></span>, which has the same interface as the best-effort broadcast from an earlier exercise, but guarantees delivery to either all or none of the targets even if the sender dies halfway during the send operation. Implement unit tests to validate the correctness of your implementation.</p></li></ol></div></div>
</div></div></div>
</div></div></div></body></html>
