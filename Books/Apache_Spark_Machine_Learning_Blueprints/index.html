<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="">

    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0"> <!--320-->
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no, user-scalable=no">

    <link rel="icon" href="../../mapt/images/favicon.ico">

    <link rel="stylesheet" href="../../mapt/css/font-awesome.css">
    <link rel="stylesheet" href="../../mapt/css/google-fonts.css">
    <link rel="stylesheet" href="../../mapt/css/devicon.css">

    <link rel="stylesheet" href="../../mapt/css/bootstrap.css">
    <link rel="stylesheet" href="../../mapt/css/bootstrap-xl.css">
    <link rel="stylesheet" href="../../mapt/css/magnific-popup.css">
    <link rel="stylesheet" href="../../mapt/css/prism.css">
    <link rel="stylesheet" href="../../mapt/css/hljs-github.css">

    <link rel="stylesheet" href="../../mapt/css/mapt.css">
    <link rel="stylesheet" href="../../mapt/css/custom.css">

    <script src="../../mapt/js/jquery.js"></script>
    <script src="../../mapt/js/bootstrap.js"></script>
    <script src="../../mapt/js/jquery.magnific-popup.js"></script>
    <script src="../../mapt/js/highlight.min.js"></script>

    <script src="../../mapt/js/custom.js"></script>
    
    <title>Apache Spark Machine Learning Blueprints</title>
</head>

<body class="home-body">
    <div id="wrapper">
        <div id="sidebar-wrapper">    
            <ul class="sidebar-nav">
                <div class="list-group" id="sidebar-nav" role="tablist">
                    <li>
                        <a href="../../index.html" class="sidenav-menu-holder back-btn" id="back-link">
                            <span class="sidenav-menu">Book List</span>
                            <span class="pull-left mr5"><i class="fa fa-chevron-left"></i></span>
                        </a>
                    </li>
                    
                    <li class="book-info copyright">
                        <span class="info text-nowrap"><span class="copyleft">&copy;</span><span><strong>RuTracker</strong>.org</span></span>
                    </li>          
                    <li class="book-info copyright">
                        <span class="info text-nowrap">Pub date: <strong>30 May 2016</strong></span>
                    </li>         
                    <li class="book-info">
                        <span class="info text-nowrap">Price: €<strong>28.99</strong></span>
                        <span class="info text-nowrap">ISBN: <strong>9781785880391</strong></span>
                    </li>     
            
                    <li>
                        <a href="graphics/cover.jpg" class="sidenav-menu-holder cover-img">
                            <img src="graphics/cover.jpg" class="cover-image">
                        </a>
                    </li>        
            
                    <div class="book_navigation">
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse1">
                                <div class="section-name">1: Spark for Machine Learning</div>
                            </a>
                        </li>
                        <div id="collapse1" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="1" class="sub-nav">
                                <a href="#ch01">
                                    <div class="section-name">Chapter 1: Spark for Machine Learning</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec08" class="sub-nav">
                                <a href="#ch01lvl1sec08">                    
                                    <div class="section-name">Spark overview and Spark advantages</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec09" class="sub-nav">
                                <a href="#ch01lvl1sec09">                    
                                    <div class="section-name">Spark computing for machine learning</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec10" class="sub-nav">
                                <a href="#ch01lvl1sec10">                    
                                    <div class="section-name">Machine learning algorithms</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec11" class="sub-nav">
                                <a href="#ch01lvl1sec11">                    
                                    <div class="section-name">MLlib</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec12" class="sub-nav">
                                <a href="#ch01lvl1sec12">                    
                                    <div class="section-name">Spark RDD and dataframes</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec13" class="sub-nav">
                                <a href="#ch01lvl1sec13">                    
                                    <div class="section-name">ML workflows and Spark pipelines</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec14" class="sub-nav">
                                <a href="#ch01lvl1sec14">                    
                                    <div class="section-name">ML workflow examples</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec15" class="sub-nav">
                                <a href="#ch01lvl1sec15">                    
                                    <div class="section-name">Spark notebooks</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec16" class="sub-nav">
                                <a href="#ch01lvl1sec16">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse2">
                                <div class="section-name">2: Data Preparation for Spark ML</div>
                            </a>
                        </li>
                        <div id="collapse2" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="2" class="sub-nav">
                                <a href="#ch02">
                                    <div class="section-name">Chapter 2: Data Preparation for Spark ML</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec17" class="sub-nav">
                                <a href="#ch02lvl1sec17">                    
                                    <div class="section-name">Accessing and loading datasets</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec18" class="sub-nav">
                                <a href="#ch02lvl1sec18">                    
                                    <div class="section-name">Data cleaning</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec19" class="sub-nav">
                                <a href="#ch02lvl1sec19">                    
                                    <div class="section-name">Identity matching</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec20" class="sub-nav">
                                <a href="#ch02lvl1sec20">                    
                                    <div class="section-name">Dataset reorganizing</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec21" class="sub-nav">
                                <a href="#ch02lvl1sec21">                    
                                    <div class="section-name">Dataset joining</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec22" class="sub-nav">
                                <a href="#ch02lvl1sec22">                    
                                    <div class="section-name">Feature extraction</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec23" class="sub-nav">
                                <a href="#ch02lvl1sec23">                    
                                    <div class="section-name">Repeatability and automation</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec24" class="sub-nav">
                                <a href="#ch02lvl1sec24">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse3">
                                <div class="section-name">3: A Holistic View on Spark</div>
                            </a>
                        </li>
                        <div id="collapse3" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="3" class="sub-nav">
                                <a href="#ch03">
                                    <div class="section-name">Chapter 3: A Holistic View on Spark</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec25" class="sub-nav">
                                <a href="#ch03lvl1sec25">                    
                                    <div class="section-name">Spark for a holistic view</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec26" class="sub-nav">
                                <a href="#ch03lvl1sec26">                    
                                    <div class="section-name">Methods for a holistic view</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec27" class="sub-nav">
                                <a href="#ch03lvl1sec27">                    
                                    <div class="section-name">Feature preparation</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec28" class="sub-nav">
                                <a href="#ch03lvl1sec28">                    
                                    <div class="section-name">Model estimation</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec29" class="sub-nav">
                                <a href="#ch03lvl1sec29">                    
                                    <div class="section-name">Model evaluation</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec30" class="sub-nav">
                                <a href="#ch03lvl1sec30">                    
                                    <div class="section-name">Results explanation</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec31" class="sub-nav">
                                <a href="#ch03lvl1sec31">                    
                                    <div class="section-name">Deployment</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec32" class="sub-nav">
                                <a href="#ch03lvl1sec32">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse4">
                                <div class="section-name">4: Fraud Detection on Spark</div>
                            </a>
                        </li>
                        <div id="collapse4" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="4" class="sub-nav">
                                <a href="#ch04">
                                    <div class="section-name">Chapter 4: Fraud Detection on Spark</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec33" class="sub-nav">
                                <a href="#ch04lvl1sec33">                    
                                    <div class="section-name">Spark for fraud detection</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec34" class="sub-nav">
                                <a href="#ch04lvl1sec34">                    
                                    <div class="section-name">Methods for fraud detection</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec35" class="sub-nav">
                                <a href="#ch04lvl1sec35">                    
                                    <div class="section-name">Feature preparation</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec36" class="sub-nav">
                                <a href="#ch04lvl1sec36">                    
                                    <div class="section-name">Model estimation</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec37" class="sub-nav">
                                <a href="#ch04lvl1sec37">                    
                                    <div class="section-name">Model evaluation</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec38" class="sub-nav">
                                <a href="#ch04lvl1sec38">                    
                                    <div class="section-name">Results explanation</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec39" class="sub-nav">
                                <a href="#ch04lvl1sec39">                    
                                    <div class="section-name">Deploying fraud detection</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec40" class="sub-nav">
                                <a href="#ch04lvl1sec40">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse5">
                                <div class="section-name">5: Risk Scoring on Spark</div>
                            </a>
                        </li>
                        <div id="collapse5" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="5" class="sub-nav">
                                <a href="#ch05">
                                    <div class="section-name">Chapter 5: Risk Scoring on Spark</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec41" class="sub-nav">
                                <a href="#ch05lvl1sec41">                    
                                    <div class="section-name">Spark for risk scoring</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec42" class="sub-nav">
                                <a href="#ch05lvl1sec42">                    
                                    <div class="section-name">Methods of risk scoring</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec43" class="sub-nav">
                                <a href="#ch05lvl1sec43">                    
                                    <div class="section-name">Data and feature preparation</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec44" class="sub-nav">
                                <a href="#ch05lvl1sec44">                    
                                    <div class="section-name">Model estimation</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec45" class="sub-nav">
                                <a href="#ch05lvl1sec45">                    
                                    <div class="section-name">Model evaluation</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec46" class="sub-nav">
                                <a href="#ch05lvl1sec46">                    
                                    <div class="section-name">Results explanation</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec47" class="sub-nav">
                                <a href="#ch05lvl1sec47">                    
                                    <div class="section-name">Deployment</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec48" class="sub-nav">
                                <a href="#ch05lvl1sec48">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse6">
                                <div class="section-name">6: Churn Prediction on Spark</div>
                            </a>
                        </li>
                        <div id="collapse6" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="6" class="sub-nav">
                                <a href="#ch06">
                                    <div class="section-name">Chapter 6: Churn Prediction on Spark</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec49" class="sub-nav">
                                <a href="#ch06lvl1sec49">                    
                                    <div class="section-name">Spark for churn prediction</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec50" class="sub-nav">
                                <a href="#ch06lvl1sec50">                    
                                    <div class="section-name">Methods for churn prediction</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec51" class="sub-nav">
                                <a href="#ch06lvl1sec51">                    
                                    <div class="section-name">Feature preparation</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec52" class="sub-nav">
                                <a href="#ch06lvl1sec52">                    
                                    <div class="section-name">Model estimation</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec53" class="sub-nav">
                                <a href="#ch06lvl1sec53">                    
                                    <div class="section-name">Model evaluation</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec54" class="sub-nav">
                                <a href="#ch06lvl1sec54">                    
                                    <div class="section-name">Results explanation</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec55" class="sub-nav">
                                <a href="#ch06lvl1sec55">                    
                                    <div class="section-name">Deployment</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec56" class="sub-nav">
                                <a href="#ch06lvl1sec56">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse7">
                                <div class="section-name">7: Recommendations on Spark</div>
                            </a>
                        </li>
                        <div id="collapse7" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="7" class="sub-nav">
                                <a href="#ch07">
                                    <div class="section-name">Chapter 7: Recommendations on Spark</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec57" class="sub-nav">
                                <a href="#ch07lvl1sec57">                    
                                    <div class="section-name">Apache Spark for a recommendation engine</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec58" class="sub-nav">
                                <a href="#ch07lvl1sec58">                    
                                    <div class="section-name">Methods for recommendation</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec59" class="sub-nav">
                                <a href="#ch07lvl1sec59">                    
                                    <div class="section-name">Data treatment with SPSS</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec60" class="sub-nav">
                                <a href="#ch07lvl1sec60">                    
                                    <div class="section-name">Model estimation</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec61" class="sub-nav">
                                <a href="#ch07lvl1sec61">                    
                                    <div class="section-name">Model evaluation</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec62" class="sub-nav">
                                <a href="#ch07lvl1sec62">                    
                                    <div class="section-name">Recommendation deployment</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec63" class="sub-nav">
                                <a href="#ch07lvl1sec63">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse8">
                                <div class="section-name">8: Learning Analytics on Spark</div>
                            </a>
                        </li>
                        <div id="collapse8" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="8" class="sub-nav">
                                <a href="#ch08">
                                    <div class="section-name">Chapter 8: Learning Analytics on Spark</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec64" class="sub-nav">
                                <a href="#ch08lvl1sec64">                    
                                    <div class="section-name">Spark for attrition prediction</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec65" class="sub-nav">
                                <a href="#ch08lvl1sec65">                    
                                    <div class="section-name">Methods of attrition prediction</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec66" class="sub-nav">
                                <a href="#ch08lvl1sec66">                    
                                    <div class="section-name">Feature preparation</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec67" class="sub-nav">
                                <a href="#ch08lvl1sec67">                    
                                    <div class="section-name">Model estimation</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec68" class="sub-nav">
                                <a href="#ch08lvl1sec68">                    
                                    <div class="section-name">Model evaluation</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec69" class="sub-nav">
                                <a href="#ch08lvl1sec69">                    
                                    <div class="section-name">Results explanation</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec70" class="sub-nav">
                                <a href="#ch08lvl1sec70">                    
                                    <div class="section-name">Deployment</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec71" class="sub-nav">
                                <a href="#ch08lvl1sec71">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse9">
                                <div class="section-name">9: City Analytics on Spark</div>
                            </a>
                        </li>
                        <div id="collapse9" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="9" class="sub-nav">
                                <a href="#ch09">
                                    <div class="section-name">Chapter 9: City Analytics on Spark</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec72" class="sub-nav">
                                <a href="#ch09lvl1sec72">                    
                                    <div class="section-name">Spark for service forecasting</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec73" class="sub-nav">
                                <a href="#ch09lvl1sec73">                    
                                    <div class="section-name">Data and feature preparation</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec74" class="sub-nav">
                                <a href="#ch09lvl1sec74">                    
                                    <div class="section-name">Model estimation</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec75" class="sub-nav">
                                <a href="#ch09lvl1sec75">                    
                                    <div class="section-name">Model evaluation</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec76" class="sub-nav">
                                <a href="#ch09lvl1sec76">                    
                                    <div class="section-name">Explanations of the results</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec77" class="sub-nav">
                                <a href="#ch09lvl1sec77">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse10">
                                <div class="section-name">10: Learning Telco Data on Spark</div>
                            </a>
                        </li>
                        <div id="collapse10" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="10" class="sub-nav">
                                <a href="#ch10">
                                    <div class="section-name">Chapter 10: Learning Telco Data on Spark</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec78" class="sub-nav">
                                <a href="#ch10lvl1sec78">                    
                                    <div class="section-name">Spark for using Telco Data</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec79" class="sub-nav">
                                <a href="#ch10lvl1sec79">                    
                                    <div class="section-name">Methods for learning from Telco Data</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec80" class="sub-nav">
                                <a href="#ch10lvl1sec80">                    
                                    <div class="section-name">Data and feature development</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec81" class="sub-nav">
                                <a href="#ch10lvl1sec81">                    
                                    <div class="section-name">Model estimation</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec82" class="sub-nav">
                                <a href="#ch10lvl1sec82">                    
                                    <div class="section-name">Model evaluation</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec83" class="sub-nav">
                                <a href="#ch10lvl1sec83">                    
                                    <div class="section-name">Results explanation</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec84" class="sub-nav">
                                <a href="#ch10lvl1sec84">                    
                                    <div class="section-name">Model deployment</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec85" class="sub-nav">
                                <a href="#ch10lvl1sec85">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse11">
                                <div class="section-name">11: Modeling Open Data on Spark</div>
                            </a>
                        </li>
                        <div id="collapse11" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="11" class="sub-nav">
                                <a href="#ch11">
                                    <div class="section-name">Chapter 11: Modeling Open Data on Spark</div>
                                </a>
                            </li>
                            <li data-chapter="11" data-section-id="ch11lvl1sec86" class="sub-nav">
                                <a href="#ch11lvl1sec86">                    
                                    <div class="section-name">Spark for learning from open data</div>
                                </a>
                            </li>
                            <li data-chapter="11" data-section-id="ch11lvl1sec87" class="sub-nav">
                                <a href="#ch11lvl1sec87">                    
                                    <div class="section-name">Data and feature preparation</div>
                                </a>
                            </li>
                            <li data-chapter="11" data-section-id="ch11lvl1sec88" class="sub-nav">
                                <a href="#ch11lvl1sec88">                    
                                    <div class="section-name">Model estimation</div>
                                </a>
                            </li>
                            <li data-chapter="11" data-section-id="ch11lvl1sec89" class="sub-nav">
                                <a href="#ch11lvl1sec89">                    
                                    <div class="section-name">Results explanation</div>
                                </a>
                            </li>
                            <li data-chapter="11" data-section-id="ch11lvl1sec90" class="sub-nav">
                                <a href="#ch11lvl1sec90">                    
                                    <div class="section-name">Deployment</div>
                                </a>
                            </li>
                            <li data-chapter="11" data-section-id="ch11lvl1sec91" class="sub-nav">
                                <a href="#ch11lvl1sec91">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapsebackindex">
                                <div class="section-name">Appendix A: Index</div>
                            </a>
                        </li>
                        <div id="collapsebackindex" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="backindex" class="sub-nav">
                                <a href="#backindex">
                                    <div class="section-name">Chapter Appendix A: Index</div>
                                </a>
                            </li>
                        </div>
                    </div>
                </div>
            </ul>
        </div>
        
        <div id="page-content-wrapper" class="book-page">
            <a href="#" id="menu-toggle" class="toggle-nav"><i class="fa fa-bars fa-2x mr5"></i></a>
            
            <a href="#" id="back_to_top" class="back-to-top"><img src="../../mapt/images/kopimi.svg"></a>
            
            <div class="col-sm-12 col-xl-offset-2 col-xl-8 col-lg-offset-1 col-lg-10">
                <div class="btn-group pull-right mt15 mb30" role="group">
                    <a href="#home" class="btn btn-default">
                        <i class="fa fa-share fa-lg no-text-padding"></i>
                        <span class="hidden-xs ml5">Book Home</span>
                    </a>
                    <button class="btn btn-default disabled" id="code-download">
                        <i class="fa fa-file fa-lg"></i>
                        <span class="hidden-xs ml5">Download Code Files</span>
                    </button>
                </div>
            </div>
            <div class="clearfix"></div>
            
            <div id="book-wrapper" class="container-fluid">
                <div class="col-sm-12 col-xl-offset-2 col-xl-8 col-lg-offset-1 col-lg-10" id="home">
                    <h2 class="product-title">Apache Spark Machine Learning Blueprints</h2>
                    <hr>
                    <div class="row">
                        <div class="col-sm-12">
                            <h5 class="mt10">By Alex Liu</h5>
                            <div>
                                <p class="mb20"><b>Develop a range of cutting-edge machine learning projects with Apache Spark using this actionable guide</b></p>
                                <a href="#ch01" class="btn btn-info btn-lg pull-right hidden-xs">
                                    Start Reading <i class="fa fa-chevron-right ml5"></i>
                                </a>
                                <a href="#ch01" class="btn btn-info btn-lg btn-block mt20 mb20 visible-xs">
                                    Start Reading <i class="fa fa-chevron-right ml5"></i>
                                </a>
                                <div class="clearfix"></div>
                                <div class="col-sm-12">
                                    <ul id="myTabs" class="nav nav-tabs nav-justified hidden-xs mt20" role="tablist">
                                        <li class="active">
                                            <a href="#info" role="tab" id="info-tab" data-toggle="tab">
                                                <h5>Info</h5>
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#content" role="tab" id="content-tab" data-toggle="tab">
                                                <h5>Contents</h5>
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#author" role="tab" id="author-tab" data-toggle="tab">
                                                <h5>Author</h5>
                                            </a>
                                        </li>
                                    </ul>
                
                                    <ul id="myTabsMobile" class="nav nav-pills text-center nav-stacked visible-xs mb60" role="tablist">
                                        <li class="active">
                                            <a href="#info" role="tab" id="info-tab-responsive" data-toggle="tab">
                                                Info
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#content" role="tab" id="content-tab-responsive" data-toggle="tab">
                                                Contents
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#author" role="tab" id="author-tab-responsive" data-toggle="tab">
                                                Author
                                            </a>
                                        </li>
                                    </ul>
                
                                    <div id="myTabContent" class="tab-content pt30">
                                    
                                        <div role="tabpanel" class="tab-pane active fade in" id="info">
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">Features</h5>
                                            </div>
                                            <div class="hidden-xs">
                                                <h5>Features</h5>
                                            </div>
                                            <hr>
                                            <div>
                                                <ul>
                <li>Customize Apache Spark and R to fit your analytical needs in customer research, fraud detection, risk analytics, and recommendation engine development</li>
                <li>Develop a set of practical Machine Learning applications that can be implemented in real-life projects</li>
                <li>A comprehensive, project-based guide to improve and refine your predictive models for practical implementation</li>
                </ul>
                                            </div>
                                            <br>
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">Learning</h5>
                                            </div>
                                            <div class="hidden-xs">
                                                <h5>Learning</h5>
                                            </div>
                                            <hr>
                                            <div>
                                                <ul>
                <li>Set up Apache Spark for machine learning and discover its impressive processing power</li>
                <li>Combine Spark and R to unlock detailed business insights essential for decision making</li>
                <li>Build machine learning systems with Spark that can detect fraud and analyze financial risks</li>
                <li>Build predictive models focusing on customer scoring and service ranking</li>
                <li>Build a recommendation systems using SPSS on Apache Spark</li>
                <li>Tackle parallel computing and find out how it can support your machine learning projects</li>
                <li>Turn open data and communication data into actionable insights by making use of various forms of machine learning</li>
                </ul>
                                            </div>
                                            <br>
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">About</h5>
                                            </div>
                                            <div class="hidden-xs">
                                                <h5>About</h5>
                                            </div>
                                            <hr>
                                            <div>
                                                <p>There's a reason why Apache Spark has become one of the most popular tools in Machine Learning – its ability to handle huge datasets at an impressive speed means you can be much more responsive to the data at your disposal. This book shows you Spark at its very best, demonstrating how to connect it with R and unlock maximum value not only from the tool but also from your data.</p>
                <p>Packed with a range of project "blueprints" that demonstrate some of the most interesting challenges that Spark can help you tackle, you'll find out how to use Spark notebooks and access, clean, and join different datasets before putting your knowledge into practice with some real-world projects, in which you will see how Spark Machine Learning can help you with everything from fraud detection to analyzing customer attrition. You'll also find out how to build a recommendation engine using Spark's parallel computing powers.</p>
                                            </div>
                                        </div>
                                        
                                        <div role="tabpanel" class="tab-pane fade in" id="content">
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">Contents</h5>
                                                <hr>
                                            </div>
                                            <ul>
                                                <div>
                                                    <li data-chapter="1">
                                                        <div class="section-name">1: Spark for Machine Learning</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="1" class="chapter-section">
                                                                    <a href="#ch01">        
                                                                        <div class="section-name">Chapter 1: Spark for Machine Learning</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec08" class="chapter-section">
                                                                    <a href="#ch01lvl1sec08">                    
                                                                        <div class="section-name">Spark overview and Spark advantages</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec09" class="chapter-section">
                                                                    <a href="#ch01lvl1sec09">                    
                                                                        <div class="section-name">Spark computing for machine learning</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec10" class="chapter-section">
                                                                    <a href="#ch01lvl1sec10">                    
                                                                        <div class="section-name">Machine learning algorithms</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec11" class="chapter-section">
                                                                    <a href="#ch01lvl1sec11">                    
                                                                        <div class="section-name">MLlib</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec12" class="chapter-section">
                                                                    <a href="#ch01lvl1sec12">                    
                                                                        <div class="section-name">Spark RDD and dataframes</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec13" class="chapter-section">
                                                                    <a href="#ch01lvl1sec13">                    
                                                                        <div class="section-name">ML workflows and Spark pipelines</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec14" class="chapter-section">
                                                                    <a href="#ch01lvl1sec14">                    
                                                                        <div class="section-name">ML workflow examples</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec15" class="chapter-section">
                                                                    <a href="#ch01lvl1sec15">                    
                                                                        <div class="section-name">Spark notebooks</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec16" class="chapter-section">
                                                                    <a href="#ch01lvl1sec16">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="2">
                                                        <div class="section-name">2: Data Preparation for Spark ML</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="2" class="chapter-section">
                                                                    <a href="#ch02">        
                                                                        <div class="section-name">Chapter 2: Data Preparation for Spark ML</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec17" class="chapter-section">
                                                                    <a href="#ch02lvl1sec17">                    
                                                                        <div class="section-name">Accessing and loading datasets</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec18" class="chapter-section">
                                                                    <a href="#ch02lvl1sec18">                    
                                                                        <div class="section-name">Data cleaning</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec19" class="chapter-section">
                                                                    <a href="#ch02lvl1sec19">                    
                                                                        <div class="section-name">Identity matching</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec20" class="chapter-section">
                                                                    <a href="#ch02lvl1sec20">                    
                                                                        <div class="section-name">Dataset reorganizing</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec21" class="chapter-section">
                                                                    <a href="#ch02lvl1sec21">                    
                                                                        <div class="section-name">Dataset joining</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec22" class="chapter-section">
                                                                    <a href="#ch02lvl1sec22">                    
                                                                        <div class="section-name">Feature extraction</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec23" class="chapter-section">
                                                                    <a href="#ch02lvl1sec23">                    
                                                                        <div class="section-name">Repeatability and automation</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec24" class="chapter-section">
                                                                    <a href="#ch02lvl1sec24">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="3">
                                                        <div class="section-name">3: A Holistic View on Spark</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="3" class="chapter-section">
                                                                    <a href="#ch03">        
                                                                        <div class="section-name">Chapter 3: A Holistic View on Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec25" class="chapter-section">
                                                                    <a href="#ch03lvl1sec25">                    
                                                                        <div class="section-name">Spark for a holistic view</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec26" class="chapter-section">
                                                                    <a href="#ch03lvl1sec26">                    
                                                                        <div class="section-name">Methods for a holistic view</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec27" class="chapter-section">
                                                                    <a href="#ch03lvl1sec27">                    
                                                                        <div class="section-name">Feature preparation</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec28" class="chapter-section">
                                                                    <a href="#ch03lvl1sec28">                    
                                                                        <div class="section-name">Model estimation</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec29" class="chapter-section">
                                                                    <a href="#ch03lvl1sec29">                    
                                                                        <div class="section-name">Model evaluation</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec30" class="chapter-section">
                                                                    <a href="#ch03lvl1sec30">                    
                                                                        <div class="section-name">Results explanation</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec31" class="chapter-section">
                                                                    <a href="#ch03lvl1sec31">                    
                                                                        <div class="section-name">Deployment</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec32" class="chapter-section">
                                                                    <a href="#ch03lvl1sec32">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="4">
                                                        <div class="section-name">4: Fraud Detection on Spark</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="4" class="chapter-section">
                                                                    <a href="#ch04">        
                                                                        <div class="section-name">Chapter 4: Fraud Detection on Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec33" class="chapter-section">
                                                                    <a href="#ch04lvl1sec33">                    
                                                                        <div class="section-name">Spark for fraud detection</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec34" class="chapter-section">
                                                                    <a href="#ch04lvl1sec34">                    
                                                                        <div class="section-name">Methods for fraud detection</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec35" class="chapter-section">
                                                                    <a href="#ch04lvl1sec35">                    
                                                                        <div class="section-name">Feature preparation</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec36" class="chapter-section">
                                                                    <a href="#ch04lvl1sec36">                    
                                                                        <div class="section-name">Model estimation</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec37" class="chapter-section">
                                                                    <a href="#ch04lvl1sec37">                    
                                                                        <div class="section-name">Model evaluation</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec38" class="chapter-section">
                                                                    <a href="#ch04lvl1sec38">                    
                                                                        <div class="section-name">Results explanation</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec39" class="chapter-section">
                                                                    <a href="#ch04lvl1sec39">                    
                                                                        <div class="section-name">Deploying fraud detection</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec40" class="chapter-section">
                                                                    <a href="#ch04lvl1sec40">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="5">
                                                        <div class="section-name">5: Risk Scoring on Spark</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="5" class="chapter-section">
                                                                    <a href="#ch05">        
                                                                        <div class="section-name">Chapter 5: Risk Scoring on Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec41" class="chapter-section">
                                                                    <a href="#ch05lvl1sec41">                    
                                                                        <div class="section-name">Spark for risk scoring</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec42" class="chapter-section">
                                                                    <a href="#ch05lvl1sec42">                    
                                                                        <div class="section-name">Methods of risk scoring</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec43" class="chapter-section">
                                                                    <a href="#ch05lvl1sec43">                    
                                                                        <div class="section-name">Data and feature preparation</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec44" class="chapter-section">
                                                                    <a href="#ch05lvl1sec44">                    
                                                                        <div class="section-name">Model estimation</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec45" class="chapter-section">
                                                                    <a href="#ch05lvl1sec45">                    
                                                                        <div class="section-name">Model evaluation</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec46" class="chapter-section">
                                                                    <a href="#ch05lvl1sec46">                    
                                                                        <div class="section-name">Results explanation</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec47" class="chapter-section">
                                                                    <a href="#ch05lvl1sec47">                    
                                                                        <div class="section-name">Deployment</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec48" class="chapter-section">
                                                                    <a href="#ch05lvl1sec48">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="6">
                                                        <div class="section-name">6: Churn Prediction on Spark</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="6" class="chapter-section">
                                                                    <a href="#ch06">        
                                                                        <div class="section-name">Chapter 6: Churn Prediction on Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec49" class="chapter-section">
                                                                    <a href="#ch06lvl1sec49">                    
                                                                        <div class="section-name">Spark for churn prediction</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec50" class="chapter-section">
                                                                    <a href="#ch06lvl1sec50">                    
                                                                        <div class="section-name">Methods for churn prediction</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec51" class="chapter-section">
                                                                    <a href="#ch06lvl1sec51">                    
                                                                        <div class="section-name">Feature preparation</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec52" class="chapter-section">
                                                                    <a href="#ch06lvl1sec52">                    
                                                                        <div class="section-name">Model estimation</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec53" class="chapter-section">
                                                                    <a href="#ch06lvl1sec53">                    
                                                                        <div class="section-name">Model evaluation</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec54" class="chapter-section">
                                                                    <a href="#ch06lvl1sec54">                    
                                                                        <div class="section-name">Results explanation</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec55" class="chapter-section">
                                                                    <a href="#ch06lvl1sec55">                    
                                                                        <div class="section-name">Deployment</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec56" class="chapter-section">
                                                                    <a href="#ch06lvl1sec56">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="7">
                                                        <div class="section-name">7: Recommendations on Spark</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="7" class="chapter-section">
                                                                    <a href="#ch07">        
                                                                        <div class="section-name">Chapter 7: Recommendations on Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec57" class="chapter-section">
                                                                    <a href="#ch07lvl1sec57">                    
                                                                        <div class="section-name">Apache Spark for a recommendation engine</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec58" class="chapter-section">
                                                                    <a href="#ch07lvl1sec58">                    
                                                                        <div class="section-name">Methods for recommendation</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec59" class="chapter-section">
                                                                    <a href="#ch07lvl1sec59">                    
                                                                        <div class="section-name">Data treatment with SPSS</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec60" class="chapter-section">
                                                                    <a href="#ch07lvl1sec60">                    
                                                                        <div class="section-name">Model estimation</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec61" class="chapter-section">
                                                                    <a href="#ch07lvl1sec61">                    
                                                                        <div class="section-name">Model evaluation</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec62" class="chapter-section">
                                                                    <a href="#ch07lvl1sec62">                    
                                                                        <div class="section-name">Recommendation deployment</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec63" class="chapter-section">
                                                                    <a href="#ch07lvl1sec63">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="8">
                                                        <div class="section-name">8: Learning Analytics on Spark</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="8" class="chapter-section">
                                                                    <a href="#ch08">        
                                                                        <div class="section-name">Chapter 8: Learning Analytics on Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec64" class="chapter-section">
                                                                    <a href="#ch08lvl1sec64">                    
                                                                        <div class="section-name">Spark for attrition prediction</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec65" class="chapter-section">
                                                                    <a href="#ch08lvl1sec65">                    
                                                                        <div class="section-name">Methods of attrition prediction</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec66" class="chapter-section">
                                                                    <a href="#ch08lvl1sec66">                    
                                                                        <div class="section-name">Feature preparation</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec67" class="chapter-section">
                                                                    <a href="#ch08lvl1sec67">                    
                                                                        <div class="section-name">Model estimation</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec68" class="chapter-section">
                                                                    <a href="#ch08lvl1sec68">                    
                                                                        <div class="section-name">Model evaluation</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec69" class="chapter-section">
                                                                    <a href="#ch08lvl1sec69">                    
                                                                        <div class="section-name">Results explanation</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec70" class="chapter-section">
                                                                    <a href="#ch08lvl1sec70">                    
                                                                        <div class="section-name">Deployment</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec71" class="chapter-section">
                                                                    <a href="#ch08lvl1sec71">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="9">
                                                        <div class="section-name">9: City Analytics on Spark</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="9" class="chapter-section">
                                                                    <a href="#ch09">        
                                                                        <div class="section-name">Chapter 9: City Analytics on Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec72" class="chapter-section">
                                                                    <a href="#ch09lvl1sec72">                    
                                                                        <div class="section-name">Spark for service forecasting</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec73" class="chapter-section">
                                                                    <a href="#ch09lvl1sec73">                    
                                                                        <div class="section-name">Data and feature preparation</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec74" class="chapter-section">
                                                                    <a href="#ch09lvl1sec74">                    
                                                                        <div class="section-name">Model estimation</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec75" class="chapter-section">
                                                                    <a href="#ch09lvl1sec75">                    
                                                                        <div class="section-name">Model evaluation</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec76" class="chapter-section">
                                                                    <a href="#ch09lvl1sec76">                    
                                                                        <div class="section-name">Explanations of the results</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec77" class="chapter-section">
                                                                    <a href="#ch09lvl1sec77">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="10">
                                                        <div class="section-name">10: Learning Telco Data on Spark</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="10" class="chapter-section">
                                                                    <a href="#ch10">        
                                                                        <div class="section-name">Chapter 10: Learning Telco Data on Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec78" class="chapter-section">
                                                                    <a href="#ch10lvl1sec78">                    
                                                                        <div class="section-name">Spark for using Telco Data</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec79" class="chapter-section">
                                                                    <a href="#ch10lvl1sec79">                    
                                                                        <div class="section-name">Methods for learning from Telco Data</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec80" class="chapter-section">
                                                                    <a href="#ch10lvl1sec80">                    
                                                                        <div class="section-name">Data and feature development</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec81" class="chapter-section">
                                                                    <a href="#ch10lvl1sec81">                    
                                                                        <div class="section-name">Model estimation</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec82" class="chapter-section">
                                                                    <a href="#ch10lvl1sec82">                    
                                                                        <div class="section-name">Model evaluation</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec83" class="chapter-section">
                                                                    <a href="#ch10lvl1sec83">                    
                                                                        <div class="section-name">Results explanation</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec84" class="chapter-section">
                                                                    <a href="#ch10lvl1sec84">                    
                                                                        <div class="section-name">Model deployment</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec85" class="chapter-section">
                                                                    <a href="#ch10lvl1sec85">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="11">
                                                        <div class="section-name">11: Modeling Open Data on Spark</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="11" class="chapter-section">
                                                                    <a href="#ch11">        
                                                                        <div class="section-name">Chapter 11: Modeling Open Data on Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="11" data-section-id="ch11lvl1sec86" class="chapter-section">
                                                                    <a href="#ch11lvl1sec86">                    
                                                                        <div class="section-name">Spark for learning from open data</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="11" data-section-id="ch11lvl1sec87" class="chapter-section">
                                                                    <a href="#ch11lvl1sec87">                    
                                                                        <div class="section-name">Data and feature preparation</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="11" data-section-id="ch11lvl1sec88" class="chapter-section">
                                                                    <a href="#ch11lvl1sec88">                    
                                                                        <div class="section-name">Model estimation</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="11" data-section-id="ch11lvl1sec89" class="chapter-section">
                                                                    <a href="#ch11lvl1sec89">                    
                                                                        <div class="section-name">Results explanation</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="11" data-section-id="ch11lvl1sec90" class="chapter-section">
                                                                    <a href="#ch11lvl1sec90">                    
                                                                        <div class="section-name">Deployment</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="11" data-section-id="ch11lvl1sec91" class="chapter-section">
                                                                    <a href="#ch11lvl1sec91">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="backindex">
                                                        <div class="section-name">Appendix A: Index</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="backindex" class="chapter-section">
                                                                    <a href="#backindex">
                                                                        <div class="section-name">Chapter Appendix A: Index</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                            </ul>
                                        </div>
                                        
                                        <div role="tabpanel" class="tab-pane fade" id="author">
                                            <div class="visible-xs">
                                                <h4 class="mobile-title">About the Author</h4>
                                                <hr>
                                            </div>
                                            <p><strong>Alex Liu</strong></p>
                                            <div>
                                                <p>Alex Liu is an expert in research methods and data science. He is currently one of IBM's leading experts in Big Data analytics and also a lead data scientist, where he serves big corporations, develops Big Data analytics IPs, and speaks at industrial conferences such as STRATA, Insights, SMAC, and BigDataCamp. In the past, Alex served as chief or lead data scientist for a few companies, including Yapstone, RS, and TRG. Before this, he was a lead consultant and director at RMA, where he provided data analytics consultation and training to many well-known organizations, including the United Nations, Indymac, AOL, Ingram Micro, GEM, Farmers Insurance, Scripps Networks, Sears, and USAID. At the same time, he taught advanced research methods to PhD candidates at University of Southern California and University of California at Irvine. Before this, he worked as a managing director for CATE/GEC and as a research fellow for the Asia/Pacific Research Center at Stanford University. Alex has a Ph.D. in quantitative sociology and a master's degree of science in statistical computing from Stanford University.</p>
                                            </div>
                                        </div>
                                        
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    
                    <div class="next-wrapper">
                        <div class="row ns">
                            <hr />
                            <span class="hidden-xs">
                                <h4 class="pull-left">
                                    <strong>Up Next: </strong><span class="section-title"></span>
                                </h4>
                                <a href="#" class="btn btn-primary pull-right btn-lg">
                                    Next Section
                                </a>
                            </span>
                            <span class="visible-xs">
                                <a href="#" class="btn btn-primary btn-block btn-lg">
                                    Next Section
                                </a>
                            </span>
                        </div>
                        <div class="row ns">
                            <hr>
                        </div>
                    </div>
                </div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch01"></a>Chapter 1. Spark for Machine Learning</h2></div></div></div><p>This chapter provides an introduction to Apache Spark from a <span class="strong"><strong>Machine Learning</strong></span> (<span class="strong"><strong>ML</strong></span>) and data analytics<a id="id0" class="indexterm"></a> perspective, and also discusses machine learning in relation to Spark computing. Here, we first present an overview of Apache Spark, as well as Spark's advantages for data analytics, in comparison to MapReduce and other computing platforms. Then we discuss five main issues, as below:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Machine learning algorithms and libraries</p></li><li style="list-style-type: disc"><p>Spark RDD and dataframes</p></li><li style="list-style-type: disc"><p>Machine learning frameworks</p></li><li style="list-style-type: disc"><p>Spark pipelines</p></li><li style="list-style-type: disc"><p>Spark notebooks</p></li></ul></div><p>All of the above are the most important topics that any data scientist or machine learning professional is expected to master, in order to fully take advantage of Apache Spark computing. Specifically, this chapter will cover all of the following six topics.</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Spark overview and Spark advantages</p></li><li style="list-style-type: disc"><p>ML algorithms and ML libraries for Spark</p></li><li style="list-style-type: disc"><p>Spark RDD and dataframes</p></li><li style="list-style-type: disc"><p>ML Frameworks, RM4Es and Spark computing</p></li><li style="list-style-type: disc"><p>ML workflows and Spark pipelines</p></li><li style="list-style-type: disc"><p>Spark notebooks introduction</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec08"></a>Spark overview and Spark advantages</h2></div></div><hr /></div><p>In this section, we<a id="id1" class="indexterm"></a> provide an overview of the Apache Spark computing platform <a id="id2" class="indexterm"></a>and a discussion about some advantages of utilizing Apache Spark, in comparison to using other computing platforms like MapReduce. Then, we briefly discuss how Spark computing fits modern machine learning and big data analytics.</p><p>After this section, readers will form a basic understanding of Apache Spark as well as a good understanding of some important machine learning benefits from utilizing Apache Spark.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec06"></a>Spark overview</h3></div></div></div><p>Apache Spark is a<a id="id3" class="indexterm"></a> computing framework for the fast processing of big data. This framework contains a distributed computing engine and a specially designed programming model. Spark was started as a research project at the AMPLab of the University of California at Berkeley in 2009, and then in 2010 it became fully open sourced as it was donated to the Apache Software Foundation. Since then, Apache Spark has experienced exponential growth, and now Spark is the most active open source project in the big data field.</p><p>Spark's computing utilizes an in-memory distributed computational approach, which makes Spark computing among the fastest, especially for iterative computation. It can run up to 100 times faster than Hadoop MapReduce, according to many tests that have been performed.</p><p>Apache Spark has a unified platform, which consists of the Spark core engine and four libraries: Spark SQL, Spark <a id="id4" class="indexterm"></a>Streaming, MLlib, and <span class="strong"><strong>GraphX</strong></span>. All of these four libraries have Python, Java and Scala programming APIs.</p><p>Besides the above mentioned four built-in libraries, there are also tens of packages available for Apache Spark, provided by third parties, which can be used for handling data sources, machine learning, and other tasks.</p><div class="mediaobject"><img src="graphics/B04883_01_01.jpg" /></div><p>Apache Spark has a 3 month circle for new releases, with Spark version 1.6.0 released on January 4 of 2016. Apache Spark release 1.3 had DataFrames API and ML Pipelines API included. Starting from Apache Spark release 1.4, the R interface (SparkR) is included as default.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note02"></a>Note</h3><p>To download Apache <a id="id5" class="indexterm"></a>Spark, readers should go to <a class="ulink" href="http://spark.apache.org/downloads.html" target="_blank">http://spark.apache.org/downloads.html</a>.</p><p>To install<a id="id6" class="indexterm"></a> Apache Spark and start running it, readers should consult its latest documentation at <a class="ulink" href="http://spark.apache.org/docs/latest/" target="_blank">http://spark.apache.org/docs/latest/</a>.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec07"></a>Spark advantages</h3></div></div></div><p>Apache Spark<a id="id7" class="indexterm"></a> has many advantages over MapReduce and other big data computing platforms. Among them, the distinguished two are that it is fast to run and fast to write.</p><p>Overall, Apache Spark has kept some of MapReduce's most important advantages like that of scalability and fault tolerance, but extended them greatly with new technologies.</p><p>In comparison to MapReduce, Apache Spark's engine is capable of executing a more general <span class="strong"><strong>Directed Acyclic Graph</strong></span> (<span class="strong"><strong>DAG</strong></span>) of operators. Therefore, when using Apache Spark to<a id="id8" class="indexterm"></a> execute MapReduce-style graphs, users can achieve higher performance batch processing in Hadoop.</p><p>Apache Spark has in-memory processing capabilities, and uses a new data abstraction method, <span class="strong"><strong>Resilient Distributed Dataset</strong></span> (<span class="strong"><strong>RDD</strong></span>), which enables highly iterative computing and reactive <a id="id9" class="indexterm"></a>applications. This also extended its fault tolerance capability.</p><p>At the same time, Apache Spark has made complex pipeline representation easy with only a few lines of code needed. It is best known for the ease with which it can be used to create algorithms that capture insight from complex and even messy data, and also enable users to apply that insight in-time to drive outcomes.</p><p>As summarized by the Apache Spark team, Spark enables:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Iterative algorithms in Machine Learning</p></li><li style="list-style-type: disc"><p>Interactive data mining and data processing</p></li><li style="list-style-type: disc"><p>Hive-compatible data warehousing that can run 100x faster</p></li><li style="list-style-type: disc"><p>Stream processing</p></li><li style="list-style-type: disc"><p>Sensor data processing</p></li></ul></div><p>To a practical data scientist working with the above, Apache Spark easily demonstrates its advantages when it is adopted for:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Parallel computing</p></li><li style="list-style-type: disc"><p>Interactive analytics</p></li><li style="list-style-type: disc"><p>Complex computation</p></li></ul></div><p>Most users are satisfied with Apache Spark's advantages in speed and performance, but some also<a id="id10" class="indexterm"></a> noted that Apache Spark is still in the process of maturing.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note03"></a>Note</h3><p>
<a class="ulink" href="http://www.svds.com/use-cases-for-apache-spark/" target="_blank">http://www.svds.com/use-cases-for-apache-spark/</a> has some examples <a id="id11" class="indexterm"></a>of materialized Spark benefits.</p></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec09"></a>Spark computing for machine learning</h2></div></div><hr /></div><p>With its innovations on RDD and in-memory processing, Apache Spark has truly made distributed <a id="id12" class="indexterm"></a>computing easily accessible to data scientists <a id="id13" class="indexterm"></a>and machine learning professionals. According to the Apache Spark team, Apache Spark runs on the Mesos cluster manager, letting it share resources with Hadoop and other applications. Therefore, Apache Spark can read from any Hadoop input source like HDFS.</p><div class="mediaobject"><img src="graphics/B04883_01_02.jpg" /></div><p>For the above, the Apache Spark computing model is very suitable to distributed computing for machine learning. Especially for rapid interactive machine learning, parallel computing, and complicated modelling at scale, Apache Spark should definitely be utilized.</p><p>According to the Spark development team, Spark's philosophy is to make life easy and productive for <a id="id14" class="indexterm"></a>data scientists and machine<a id="id15" class="indexterm"></a> learning professionals. Due to this, Apache Spark has:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Well documented, expressive API's</p></li><li style="list-style-type: disc"><p>Powerful domain specific libraries</p></li><li style="list-style-type: disc"><p>Easy integration with storage systems</p></li><li style="list-style-type: disc"><p>Caching to avoid data movement</p></li></ul></div><p>Per the introduction by Patrick Wendell, co-founder of Databricks, Spark is especially made for large scale data processing. Apache Spark supports agile data science to iterate rapidly, and Spark can be integrated with IBM and other solutions easily.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec10"></a>Machine learning algorithms</h2></div></div><hr /></div><p>In this section, we review algorithms that are needed for machine learning, and introduce machine learning libraries including Spark's MLlib and IBM's SystemML, then we discuss their<a id="id16" class="indexterm"></a> integration with Apache Spark.</p><p>After reading this section, readers will become familiar with various machine learning libraries including Spark's MLlib, and know how to make them ready for machine learning.</p><p>To complete a Machine Learning project, data scientists often employ some classification or regression algorithms to develop and evaluate predictive models, which are readily available in some Machine Learning tools like R or MatLab. To complete a machine learning project, besides data sets and computing platforms, these machine learning libraries, as collections of machine learning algorithms, are necessary.</p><p>For example, the strength and depth of the popular R mainly comes from the various algorithms that are readily provided for the use of Machine Learning professionals. The total number of R packages is over 1000. Data scientists do not need all of them, but do need some packages to:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Load data, with packages like <code class="literal">RODBC</code> or <code class="literal">RMySQL</code>
</p></li><li style="list-style-type: disc"><p>Manipulate data, with packages like <code class="literal">stringr</code> or <code class="literal">lubridate</code>
</p></li><li style="list-style-type: disc"><p>Visualize data, with packages like <code class="literal">ggplot2</code> or <code class="literal">leaflet</code>
</p></li><li style="list-style-type: disc"><p>Model data, with packages like <code class="literal">Random Forest</code> or <code class="literal">survival</code>
</p></li><li style="list-style-type: disc"><p>Report results, with packages like <code class="literal">shiny</code> or <code class="literal">markdown</code>
</p></li></ul></div><p>According to a<a id="id17" class="indexterm"></a> recent ComputerWorld survey, the most downloaded R packages are:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>PACKAGE</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p># of DOWNLOADS</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>
<code class="literal">Rcpp</code>
</p>
</td><td style="" align="left" valign="top">
<p>162778 </p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">ggplot2</code>
</p>
</td><td style="" align="left" valign="top">
<p>146008 </p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">plyr</code>
</p>
</td><td style="" align="left" valign="top">
<p>123889 </p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">stringr</code>
</p>
</td><td style="" align="left" valign="top">
<p>120387 </p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">colorspace</code>
</p>
</td><td style="" align="left" valign="top">
<p>118798 </p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">digest</code>
</p>
</td><td style="" align="left" valign="top">
<p>113899 </p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">reshape2</code>
</p>
</td><td style="" align="left" valign="top">
<p>109869 </p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">RColorBrewer</code>
</p>
</td><td style="" align="left" valign="top">
<p>100623 </p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">scales</code>
</p>
</td><td style="" align="left" valign="top">
<p>92448 </p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">manipulate</code>
</p>
</td><td style="" align="left" valign="top">
<p>88664 </p>
</td></tr></tbody></table></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note04"></a>Note</h3><p>For more info, please visit <a class="ulink" href="http://www.computerworld.com/article/2920117/business-intelligence/most-downloaded-r-packages-last-month.html" target="_blank">http://www.computerworld.com/article/2920117/business-intelligence/most-downloaded-r-packages-last-month.html</a>
</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec11"></a>MLlib</h2></div></div><hr /></div><p>MLlib is <a id="id18" class="indexterm"></a>Apache Spark's machine learning library. It is scalable, and consists of many commonly-used machine learning algorithms. Built-in to MLlib are algorithms for:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Handling data types in forms of vectors and matrices</p></li><li style="list-style-type: disc"><p>Computing basic statistics like summary statistics and correlations, as well as producing simple random and stratified samples, and conducting simple hypothesis testing</p></li><li style="list-style-type: disc"><p>Performing classification and regression modeling</p></li><li style="list-style-type: disc"><p>Collaborative filtering</p></li><li style="list-style-type: disc"><p>Clustering</p></li><li style="list-style-type: disc"><p>Performing dimensionality reduction</p></li><li style="list-style-type: disc"><p>Conducting feature extraction and transformation</p></li><li style="list-style-type: disc"><p>Frequent pattern mining</p></li><li style="list-style-type: disc"><p>Developing optimization</p></li><li style="list-style-type: disc"><p>Exporting PMML models</p></li></ul></div><p>The Spark MLlib is <a id="id19" class="indexterm"></a>still under active development, with new algorithms expected to be added for every new release.</p><p>In line with Apache Spark's computing philosophy, the MLlib is built for easy use and deployment, with high performance.</p><p>MLlib uses the linear algebra package <code class="literal">Breeze</code>, which depends on <code class="literal">netlib-java</code>, and <code class="literal">jblas</code>. The packages <code class="literal">netlib-java</code> and <code class="literal">jblas</code> also depend on native Fortran routines. Users need to install the <code class="literal">gfortran</code> runtime library if it is not already present on their nodes. MLlib will throw a linking error if it cannot detect these libraries automatically.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note05"></a>Note</h3><p>For MLlib use<a id="id20" class="indexterm"></a> cases and further details on how to use MLlib, please visit:</p><p>
<a class="ulink" href="http://spark.apache.org/docs/latest/mllib-guide.html" target="_blank">http://spark.apache.org/docs/latest/mllib-guide.html</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec08"></a>Other ML libraries</h3></div></div></div><p>As discussed in previous part, MLlib has made available many frequently used algorithms like regression and classification. But these basics are not enough for complicated machine learning.</p><p>If we wait for the Apache Spark team to add all the needed ML algorithms it may take a long time. For this, the good news is that many third parties have contributed ML libraries to Apache Spark.</p><p>IBM has contributed<a id="id21" class="indexterm"></a> its machine learning library, SystemML, to Apache Spark.</p><p>Besides what MLlib provides, SystemML offers a lot more additional ML algorithms like the ones on missing data<a id="id22" class="indexterm"></a> imputation, SVM, GLM, ARIMA, and non-linear optimizers, and some graphical modelling and matrix factonization algorithms.</p><p>As developed by the IBM Almaden Research group, IBM's SystemML is an engine for distributed machine learning and it can scale to arbitrary large data sizes. It provides the following benefits:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Unifies the fractured machine learning environments</p></li><li style="list-style-type: disc"><p>Gives the core Spark ecosystem a complete set of DML</p></li><li style="list-style-type: disc"><p>Allows a data scientist to focus on the algorithm, not the implementation</p></li><li style="list-style-type: disc"><p>Improves time to value for data science teams</p></li><li style="list-style-type: disc"><p>Establishes a de facto standard for reusable machine learning routines</p></li></ul></div><p>SystemML is modeled after R syntax and semantics, and provides the ability to author new algorithms via its own language.</p><p>Through a <a id="id23" class="indexterm"></a>good integration with R by SparkR, Apache Spark users also have<a id="id24" class="indexterm"></a> the potential to utilize thousands of R packages for machine learning algorithms, when needed. As will be discussed in later sections of this chapter, the SparkR notebook will make this operation very easy.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note06"></a>Note</h3><p>For more <a id="id25" class="indexterm"></a>about IBM SystemML, please visit <a class="ulink" href="http://researcher.watson.ibm.com/researcher/files/us-ytian/systemML.pdf" target="_blank">http://researcher.watson.ibm.com/researcher/files/us-ytian/systemML.pdf</a>
</p></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec12"></a>Spark RDD and dataframes</h2></div></div><hr /></div><p>In this section, our focus turns to data and how Apache Spark represents data and organizes data. Here, we will provide an introduction to the Apache Spark RDD and Apache Spark dataframes.</p><p>After this section, readers will master these two fundamental Spark concepts, RDD and Spark dataframe, and be ready to utilize them for Machine Learning projects.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec09"></a>Spark RDD</h3></div></div></div><p>Apache Spark's primary data abstraction is in the form of a distributed collection of items, which is called <span class="strong"><strong>Resilient Distributed Dataset</strong></span> (<span class="strong"><strong>RDD</strong></span>). RDD is Apache Spark's key innovation<a id="id26" class="indexterm"></a>, which makes its computing faster and more <a id="id27" class="indexterm"></a>efficient than others.</p><p>Specifically, an RDD is an immutable collection of objects, which spreads across a cluster. It is statically typed, for example RDD[T] has objects of type T. There are RDD of strings, RDD of integers, and RDD of objects.</p><p>On the other hand, RDDs:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Are collections of objects across a cluster with user controlled partitioning</p></li><li style="list-style-type: disc"><p>Are built via parallel transformations like <code class="literal">map</code> and <code class="literal">filter</code>
</p></li></ul></div><p>That is, an RDD is physically distributed across a cluster, but manipulated as one logical entity. RDDs on Spark have fault tolerant properties such that they can be automatically rebuilt on failure.</p><p>New RDDs can be created from Hadoop Input Formats (such as HDFS files) or by transforming other RDDs.</p><p>To create RDDs, users<a id="id28" class="indexterm"></a> can either:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Distribute a collection of objects from the driver program (using the parallelize method of the Spark context)</p></li><li style="list-style-type: disc"><p>Load an external dataset</p></li><li style="list-style-type: disc"><p>Transform an existing RDD</p></li></ul></div><p>Spark's team call the above two types of RDD operations <span class="emphasis"><em>action</em></span> and <span class="emphasis"><em>transformation</em></span>.</p><p>RDDs can be operated by <span class="emphasis"><em>actions</em></span>, which return values, or by <span class="emphasis"><em>transformations</em></span>, which return pointers to new RDDs. Some examples of RDD actions are <code class="literal">collect</code>, <code class="literal">count</code> and <code class="literal">take</code>.</p><p>Transformations are lazy evaluations. Some examples of RDD transformations are <code class="literal">map</code>, <code class="literal">filter</code>, and <code class="literal">join</code>.</p><p>RDD actions and transformations may be combined to form complex computations.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note07"></a>Note</h3><p>To learn more<a id="id29" class="indexterm"></a> about RDD, please read the article at</p><p>
<a class="ulink" href="https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf" target="_blank">https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf</a>
</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec10"></a>Spark dataframes</h3></div></div></div><p>A Spark dataframe<a id="id30" class="indexterm"></a> is a distributed collection of data as organized by columns, actually a distributed collection of data as grouped into named columns, that is, an RDD with a schema. In other words, Spark dataframe is an extension of Spark RDD.</p><p>
<span class="emphasis"><em>Data frame = RDD</em></span> where columns are named and can be manipulated by name instead of by index value.</p><p>A Spark dataframe is conceptually equivalent to a dataframe in R, and is similar to a table in a relational database, which helped Apache Spark to be quickly accepted by the machine learning community. With Spark dataframes, users can directly work with data elements like columns, which are not available when working with RDDs. With data scheme knowledge on hand, users can also apply their familiar SQL types of data re-organization techniques to data. Spark dataframes can be built from many kinds of raw data such as structured relational data files, Hive tables, or existing RDDs.</p><p>Apache Spark has built a special dataframe API and a Spark SQL to deal with Spark dataframes. The Spark SQL and Spark dataframe API are both available for Scala, Java, Python, and R. As an extension to the existing RDD API, the DataFrames API features:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Ability to scale from kilobytes of data on a single laptop to petabytes on a large cluster</p></li><li style="list-style-type: disc"><p>Support for a wide array of data formats and storage systems</p></li><li style="list-style-type: disc"><p>State-of-the-art optimization and code generation through the Spark SQL Catalyst optimizer</p></li><li style="list-style-type: disc"><p>Seamless integration with all big data tooling and infrastructure via Spark</p></li></ul></div><p>The Spark SQL<a id="id31" class="indexterm"></a> works with Spark DataFrame very well, which allows users to do ETL easily, and also to work on subsets of any data easily. Then, users can transform them and make them available to other users including R users. Spark SQL can also be used alongside HiveQL, and runs very fast. With Spark SQL, users write less code as well, a lot less than working with Hadoop, and also less than working directly on RDDs.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note08"></a>Note</h3><p>For<a id="id32" class="indexterm"></a> more, please visit <a class="ulink" href="http://spark.apache.org/docs/latest/sql-programming-guide.html" target="_blank">http://spark.apache.org/docs/latest/sql-programming-guide.html</a>
</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec11"></a>Dataframes API for R</h3></div></div></div><p>A dataframe is an essential element for machine learning programming. Apache Spark has made a dataframe API available for R as well as for Java and Python, so that users can operate Spark dataframes easily in their familiar environment with their familiar language. In this <a id="id33" class="indexterm"></a>section, we provide a simple introduction to operating<a id="id34" class="indexterm"></a> Spark dataframes, with some simple examples for R to start leading our readers into actions.</p><p>The entry point into all relational functionality in Apache Spark is its <code class="literal">SQLContext</code> class, or one of its descendents. To create a basic <code class="literal">SQLContext</code>, all users need is a SparkContext command as below:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sqlContext &lt;- sparkRSQL.init(sc)</strong></span>
</pre></div><p>To create a Spark dataframe, users may perform the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sqlContext &lt;- SQLContext(sc)</strong></span>
<span class="strong"><strong>df &lt;- jsonFile(sqlContext, "examples/src/main/resources/people.json")</strong></span>
<span class="strong"><strong># Displays the content of the DataFrame to stdout</strong></span>
<span class="strong"><strong>showDF(df)</strong></span>
</pre></div><p>For Spark dataframe operations, the following are some examples:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sqlContext &lt;- sparkRSQL.init(sc)</strong></span>
<span class="strong"><strong># Create the DataFrame</strong></span>
<span class="strong"><strong>df &lt;- jsonFile(sqlContext, "examples/src/main/resources/people.json")</strong></span>
<span class="strong"><strong># Show the content of the DataFrame</strong></span>
<span class="strong"><strong>showDF(df)</strong></span>
<span class="strong"><strong>## age  name</strong></span>
<span class="strong"><strong>## null Michael</strong></span>
<span class="strong"><strong>## 30   Andy</strong></span>
<span class="strong"><strong>## 19   Justin</strong></span>

<span class="strong"><strong># Print the schema in a tree format</strong></span>
<span class="strong"><strong>printSchema(df)</strong></span>
<span class="strong"><strong>## root</strong></span>
<span class="strong"><strong>## |-- age: long (nullable = true)</strong></span>
<span class="strong"><strong>## |-- name: string (nullable = true)</strong></span>

<span class="strong"><strong># Select only the "name" column</strong></span>
<span class="strong"><strong>showDF(select(df, "name"))</strong></span>
<span class="strong"><strong>## name</strong></span>
<span class="strong"><strong>## Michael</strong></span>
<span class="strong"><strong>## Andy</strong></span>
<span class="strong"><strong>## Justin</strong></span>

<span class="strong"><strong># Select everybody, but increment the age by 1</strong></span>
<span class="strong"><strong>showDF(select(df, df</strong></span>
<span class="strong"><strong>$name, df$age + 1))</strong></span>
<span class="strong"><strong>## name    (age + 1)</strong></span>
<span class="strong"><strong>## Michael null</strong></span>
<span class="strong"><strong>## Andy    31</strong></span>
<span class="strong"><strong>## Justin  20</strong></span>

<span class="strong"><strong># Select people older than 21</strong></span>
<span class="strong"><strong>showDF(where(df, df$age &gt; 21))</strong></span>
<span class="strong"><strong>## age name</strong></span>
<span class="strong"><strong>## 30  Andy</strong></span>
<span class="strong"><strong># Count people by age </strong></span>
<span class="strong"><strong>showDF(count(groupBy(df, "age")))</strong></span>
<span class="strong"><strong>## age  count</strong></span>
<span class="strong"><strong>## null 1</strong></span>
<span class="strong"><strong>## 19   1</strong></span>
<span class="strong"><strong>## 30   1</strong></span>
</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note09"></a>Note</h3><p>For more<a id="id35" class="indexterm"></a> info, please visit <a class="ulink" href="http://spark.apache.org/docs/latest/sql-programming-guide.html#creating-dataframes" target="_blank">http://spark.apache.org/docs/latest/sql-programming-guide.html#creating-dataframes</a>
</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec12"></a>ML frameworks, RM4Es and Spark computing</h3></div></div></div><p>In this section, we discuss machine learning frameworks with RM4Es as one of its examples, in relation to Apache Spark computing.</p><p>After this <a id="id36" class="indexterm"></a>section, readers will master the concept of machine learning frameworks and some examples, and then be ready to combine them with Spark computing for planning and implementing machine learning projects.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec13"></a>ML frameworks</h3></div></div></div><p>As discussed in earlier sections, Apache Spark computing is very different from Hadoop MapReduce. Spark is faster and easier to use than Hadoop MapReduce. There are many benefits to adopting Apache Spark computing for machine learning.</p><p>However, all the benefits for machine learning professionals will materialize only if Apache Spark can enable good ML frameworks. Here, an ML framework means a system or an approach that combines all the ML elements including ML algorithms to make ML most effective to its users. And specifically, it refers to the ways that data is represented and processed, how predictive models are represented and estimated, how modeling results are evaluated, and are utilized. From this perspective, ML Frameworks are different from each other, for their handling of data sources, conducting data pre-processing, implementing algorithms, and for their support for complex computation.</p><p>There are many ML frameworks, as there are also various computing platforms supporting these frameworks. Among the available ML frameworks, the frameworks stressing iterative computing and interactive manipulation are considered among the best, because these features can facilitate complex predictive model estimation and good researcher-data interaction. Nowadays, good ML frameworks also need to cover big data capabilities or fast processing at scale, as well as fault tolerance capabilities. Good frameworks always include a large number of machine learning algorithms and statistical tests ready to be used.</p><p>As mentioned in previous sections, Apache Spark has excellent iterative computing performance and is highly cost-effective, thanks to in-memory data processing. It's compatible with all of Hadoop's data sources and file formats and, thanks to friendly APIs that they are available in several languages, it also has a faster learning curve. Apache Spark even includes graph processing and machine-learning capabilities. For these reasons, Apache Spark based ML frameworks are favored by ML professionals.</p><p>However, Hadoop MapReduce is a more mature platform and it was built for batch processing. It can be more cost-effective than Spark, for some big data that doesn't fit in memory and also due to the greater availability of experienced staff. Furthermore, the Hadoop MapReduce ecosystem is currently bigger thanks to many supporting projects, tools, and cloud services.</p><p>But even if Spark looks like the big winner, the chances are that ML professionals won't use it on its own, ML professionals may still need HDFS to store the data and may want to use HBase, Hive, Pig, Impala, or other Hadoop projects. For many cases, this means ML professionals <a id="id37" class="indexterm"></a>still need to run Hadoop and MapReduce alongside Apache Spark for a full Big Data package.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec14"></a>RM4Es</h3></div></div></div><p>In a previous section, we have had some general discussion about machine learning frameworks. Specifically, a ML framework covers how to deal with data, analytical methods, analytical computing, results evaluation, and results utilization, which RM4Es represents nicely as a framework. The <span class="strong"><strong>RM4Es</strong></span> (<span class="strong"><strong>Research Methods Four Elements</strong></span>) is<a id="id38" class="indexterm"></a> a good framework to summarize Machine Learning components and processes. The RM4Es include:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Equation</strong></span>: Equations <a id="id39" class="indexterm"></a>are used to represent the models for our research</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Estimation</strong></span>: Estimation is the link between equations (models) and the data used for <a id="id40" class="indexterm"></a>our research</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Evaluation</strong></span>: Evaluation<a id="id41" class="indexterm"></a> needs to be performed to assess the fit between models and the data</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Explanation</strong></span>: Explanation<a id="id42" class="indexterm"></a> is the link between equations (models) and our research purposes. How we explain our research results often depends on our research purposes and also on the subject we are studying</p></li></ul></div><p>The RM4Es are the key four aspects that distinguish one machine learning method from another. The RM4Es are sufficient to represent an ML status at any given moment. Furthermore, using RM4Es can easily and sufficiently represent ML workflows.</p><p>Related to what we discussed so far, Equation is like ML libraries, Estimation represents how computing is done, Evaluation is about how to tell whether a ML is better, and, as for iterative computer, whether we should continue or stop. Explanation is also a key part for ML as our goal is to turn data into insightful results that can be used.</p><p>Per the above, a good ML framework needs to deal with data abstraction and data pre-processing at scale, and also needs to deal with fast computing, interactive evaluation at scale and speed, as well as easy results interpretation and deployment.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec15"></a>The Spark computing framework</h3></div></div></div><p>Earlier in the chapter, we discussed how Spark computing supports iterative ML computing. After reviewing machine learning frameworks and how Spark computing relates to<a id="id43" class="indexterm"></a> ML frameworks, we are ready to understand more about why Spark computing should be selected for ML.</p><p>Spark was built to serve ML and data science, to make ML at scale and ML deployment easy. As discussed, Spark's core innovation on RDDs enables fast and easy computing, with good fault tolerance.</p><p>Spark is a general computing platform, and its program contains two programs: a driver program and a worker program.</p><p>To program, developers need to write a driver program that implements the high-level control flow of their application and also launches various operations in parallel. All the worker programs developed will run on cluster nodes or in local threads, and RDDs operate across all workers.</p><p>As mentioned, Spark provides two main abstractions for parallel programming: resilient distributed datasets and parallel operations on these datasets (invoked by passing a function to apply on a dataset).</p><p>In addition, Spark supports two restricted types of shared variables:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Broadcast variables</strong></span>: If a large read-only piece of data (e.g., a lookup table) is used<a id="id44" class="indexterm"></a> in multiple parallel operations, it is<a id="id45" class="indexterm"></a> preferable to distribute it to the workers only once instead of packaging it with every closure.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Accumulators</strong></span>: These are variables that workers can only <span class="emphasis"><em>add</em></span> to using an associative <a id="id46" class="indexterm"></a>operation, and that only the driver <a id="id47" class="indexterm"></a>can read. They can be used to implement counters as in MapReduce and to provide a more imperative syntax for parallel sums. Accumulators can be defined for any type that has an <span class="emphasis"><em>add</em></span> operation and a <span class="emphasis"><em>zero</em></span> value. Due to their <span class="emphasis"><em>add-only</em></span> semantics, they are easy to make fault-tolerant.</p></li></ul></div><p>With all the above, the Apache Spark computing framework is capable of supporting various machine learning frameworks that need fast parallel computing with fault tolerance.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note10"></a>Note</h3><p>See <a class="ulink" href="http://people.csail.mit.edu/matei/papers/2010/hotcloud_spark.pdf" target="_blank">http://people.csail.mit.edu/matei/papers/2010/hotcloud_spark.pdf</a> for more.</p></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec13"></a>ML workflows and Spark pipelines</h2></div></div><hr /></div><p>In this <a id="id48" class="indexterm"></a>section, we provide an introduction to machine learning <a id="id49" class="indexterm"></a>workflows, and also Spark pipelines, and then discuss how Spark pipeline can serve as a good tool of computing ML workflows.</p><p>After this section, readers will master these two important concepts, and be ready to program and implement Spark pipelines for machine learning workflows.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec16"></a>ML as a step-by-step workflow</h3></div></div></div><p>Almost all ML projects involve cleaning data, developing features, estimating models, evaluating models, and then interpreting results, which all can be organized into some step by step workflows. These workflows are sometimes called analytical processes.</p><p>Some people even define machine learning as workflows of turning data into actionable insights, for which some people will add business understanding or problem definition into the workflows as their starting points.</p><p>In the data mining field, <span class="strong"><strong>Cross Industry Standard Process for Data Mining</strong></span> (<span class="strong"><strong>CRISP-DM</strong></span>) is<a id="id50" class="indexterm"></a> a widely accepted workflow standard, which is still widely adopted. And many standard ML workflows are just some form of revision to the CRISP-DM workflow.</p><div class="mediaobject"><img src="graphics/B04883_01_03.jpg" /></div><p>As illustrated in the above picture, for any standard CRISP-DM workflow, we need all the following 6 steps:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Business understanding</p></li><li><p>Data understanding</p></li><li><p>Data preparation</p></li><li><p>Modeling</p></li><li><p>Evaluation</p></li><li><p>Deployment</p></li></ol></div><p>To which some<a id="id51" class="indexterm"></a> people may add analytical approaches selection and results explanation, to make it more complete. For complicated machine learning projects, there will be some branches and feedback loops to make workflows very complex.</p><p>In other words, for some machine learning projects, after we complete model evaluation, we may go back to the step of modeling or even data preparation. After the data preparation step, we may branch out for more than two types of modeling.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec14"></a>ML workflow examples</h2></div></div><hr /></div><p>To further understand machine learning workflows, let us review some examples here.</p><p>In the later <a id="id52" class="indexterm"></a>chapters of this book, we will work on risk modelling, fraud detection, customer view, churn prediction, and recommendation. For many of these types of projects, the goal is often to identify causes of certain problems, or to build a causal model. Below is one example of a workflow to develop a causal model.</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Check data structure to ensure a good understanding of the data:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Is the data a cross sectional data? Is implicit timing incorporated?</p></li><li style="list-style-type: disc"><p>Are categorical variables used?</p></li></ul></div></li><li><p> Check missing values:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Don't know or forget as an answer may be recoded as neutral or treated as a special category</p></li><li style="list-style-type: disc"><p>Some variables may have a lot of missing values</p></li><li style="list-style-type: disc"><p>To recode some variables as needed</p></li></ul></div></li><li><p>Conduct some descriptive studies to begin telling stories:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Use comparing means and crosstabulations</p></li><li style="list-style-type: disc"><p>Check variability of some key variables (standard deviation and variance)</p></li></ul></div></li><li><p>Select groups of <code class="literal">ind</code> variables (exogenous variables):</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>As candidates of causes</p></li></ul></div></li><li><p>Basic descriptive statistics:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Mean, standard deviaton, and frequencies for all variables</p></li></ul></div></li><li><p> Measurement work:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p> Study dimensions of some measurements (efa exploratory factor analysis may be useful here)</p></li><li style="list-style-type: disc"><p>May form measurement models</p></li></ul></div></li><li><p>Local<a id="id53" class="indexterm"></a> models:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Identify sections out from the whole picture to explore relationship</p></li><li style="list-style-type: disc"><p>Use crosstabulations</p></li><li style="list-style-type: disc"><p>Graphical plots</p></li><li style="list-style-type: disc"><p>Use logistic regression</p></li><li style="list-style-type: disc"><p>Use linear regression</p></li></ul></div></li><li><p>Conduct some partial correlation analysis to help model specification.</p></li><li><p>Propose structural equation models by using the results of (8):</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Identify main structures and sub structures</p></li><li style="list-style-type: disc"><p>Connect measurements with structure models</p></li></ul></div></li><li><p>Initial fits:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Use <span class="emphasis"><em>spss</em></span> to create data sets for <span class="emphasis"><em>lisrel</em></span> or <span class="emphasis"><em>mplus</em></span>
</p></li><li style="list-style-type: disc"><p>Programming in lisrel or mplus</p></li></ul></div></li><li><p>Model modification:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Use SEM results (mainly model fit indices) to guide</p></li><li style="list-style-type: disc"><p>Re-analyze partial correlations</p></li></ul></div></li><li><p>Diagnostics:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Distribution</p></li><li style="list-style-type: disc"><p>Residuals</p></li><li style="list-style-type: disc"><p>Curves</p></li></ul></div></li><li><p>Final model estimation may be reached here:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>If not repeat step 13 and 14</p></li></ul></div></li><li><p>Explaining the model (causal effects identified and quantified).</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note11"></a>Note</h3><p>Also<a id="id54" class="indexterm"></a> refer to <a class="ulink" href="http://www.researchmethods.org/step-by-step1.pdf" target="_blank">http://www.researchmethods.org/step-by-step1.pdf</a>, <span class="emphasis"><em>Spark Pipelines</em></span>
</p></div></li></ol></div><p>The Apache Spark team has recognized the importance of machine learning workflows and they have developed Spark Pipelines to enable good handling of them.</p><p>Spark ML represents a ML workflow as a pipeline, which consists of a sequence of <span class="emphasis"><em>PipelineStages</em></span> to be run in a specific order.</p><p>PipelineStages <a id="id55" class="indexterm"></a>include Spark Transformers, Spark Estimators and Spark Evaluators.</p><p>ML workflows<a id="id56" class="indexterm"></a> can be very complicated, so that creating and tuning them is very time consuming. The Spark ML Pipeline was created to make the construction and tuning of ML workflows easy, and especially to represent the following main stages:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Loading data</p></li><li><p>Extracting features</p></li><li><p>Estimating models</p></li><li><p>Evaluating models</p></li><li><p>Explaining models</p></li></ol></div><p>With regards to the above tasks, Spark Transformers can be used to extract features. Spark Estimators can be used to train and estimate models, and Spark Evaluators can be used to evaluate models.</p><p>Technically, in Spark, a Pipeline is specified as a sequence of stages, and each stage is either a Transformer, an Estimator, or an Evaluator. These stages are run in order, and the input dataset is modified as it passes through each stage. For Transformer stages, the <code class="literal">transform()</code> method is called on the dataset. For estimator stages, the <code class="literal">fit()</code> method is called to produce a Transformer (which becomes part of the PipelineModel, or fitted Pipeline), and that Transformer's <code class="literal">transform()</code> method is called on the dataset.</p><p>The specifications given above are all for linear Pipelines. It is possible to create non-linear Pipelines<a id="id57" class="indexterm"></a> as long as the data flow graph forms a <span class="strong"><strong>Directed Acyclic Graph</strong></span> (<span class="strong"><strong>DAG</strong></span>).</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note12"></a>Note</h3><p>For more<a id="id58" class="indexterm"></a> info on Spark pipeline, please visit:</p><p>
<a class="ulink" href="http://spark.apache.org/docs/latest/ml-guide.html#pipeline" target="_blank">http://spark.apache.org/docs/latest/ml-guide.html#pipeline</a>
</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec15"></a>Spark notebooks</h2></div></div><hr /></div><p>In this section, we<a id="id59" class="indexterm"></a> first discuss about notebook approaches for machine learning. Then we provide a full introduction to R Markdown as a mature notebook example, and then introduce Spark's R notebook to complete this section.</p><p>After this section, readers will master these notebook approaches as well as some related concepts, and be ready to use them for managing and programming machine learning projects.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec17"></a>Notebook approach for ML</h3></div></div></div><p>Notebook<a id="id60" class="indexterm"></a> became a favored <a id="id61" class="indexterm"></a>machine learning approach, not <a id="id62" class="indexterm"></a>only for its dynamics, but also for reproducibility.</p><p>Most notebook interfaces are comprised of a series of code blocks, called cells. The development process is a discovery type, for which a developer can develop and run codes in one cell, and then can continue to write code in a subsequent cell depending on the results from the first cell. Particularly when analyzing large datasets, this interactive type of approach allows machine learning professionals to quickly discover patterns or insights into data. Therefore, notebook-style development processes provide some exploratory and interactive ways to write code and immediately examine results.</p><p>Notebook allows users to seamlessly mix code, outputs, and markdown comments all in the same document. With everything in one document, it makes it easier for machine learning professionals to reproduce their work at a later stage.</p><p>This notebook approach was adopted to ensure reproducibility, to align analysis with computation, and to align analysis with presentation, so to end the copy and paste way of research management.</p><p>Specifically, using notebook allows users to:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Analyze iteratively</p></li><li style="list-style-type: disc"><p>Report transparently</p></li><li style="list-style-type: disc"><p>Collaborate seamlessly</p></li><li style="list-style-type: disc"><p>Compute with clarity</p></li><li style="list-style-type: disc"><p>Assess reasoning, not only results</p></li><li style="list-style-type: disc"><p>The note book approach also provides a unified way to integrate many analytical tools for machine learning practice.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note13"></a>Note</h3><p>For more about adopting an approach for reproducibility, please visit <a class="ulink" href="http://chance.amstat.org/2014/09/reproducible-paradigm/" target="_blank">http://chance.amstat.org/2014/09/reproducible-paradigm/</a> <span class="emphasis"><em>R Markdown</em></span>
</p></div></li></ul></div><p>R Markdown<a id="id63" class="indexterm"></a> is a very popular tool helping data scientists and machine<a id="id64" class="indexterm"></a> learning professionals to<a id="id65" class="indexterm"></a> generate dynamic reports, and also making their analytical workflows reproducible. R Markdown is one of the pioneer notebook tools.</p><p>According to RStudio</p><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>"R Markdown is a format that enables easy authoring of reproducible web reports from R. It combines the core syntax of Markdown (an easy-to-write plain text format for web content) with embedded R code chunks that are run so their output can be included in the final document".</em></span></p></blockquote></div><p>Therefore, we<a id="id66" class="indexterm"></a> can use R and the Markdown package plus some other dependent packages like <code class="literal">knitr</code>, to author reproducible analytical reports. However, utilizing RStudio and the <code class="literal">Markdown</code> package together makes things easy for data scientists.</p><p>Using the Markdown is very easy for R users. As an example, let us create a report in the following three simple steps:</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec01"></a>Step 1: Getting the software ready</h4></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Download<a id="id67" class="indexterm"></a> R studio at : <a class="ulink" href="http://rstudio.org/" target="_blank">http://rstudio.org/</a>
</p></li><li><p>Set options <a id="id68" class="indexterm"></a>for R studio: <span class="strong"><strong>Tools</strong></span> &gt; <span class="strong"><strong>Options</strong></span> &gt; Click on <span class="strong"><strong>Sweave</strong></span> and choose <span class="strong"><strong>Knitr</strong></span> at <span class="strong"><strong>Weave Rnw files using Knitr</strong></span>.</p></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec02"></a>Step 2: Installing the Knitr package</h4></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>To<a id="id69" class="indexterm"></a> install a package in RStudio, you<a id="id70" class="indexterm"></a> use <span class="strong"><strong>Tools</strong></span> &gt; <span class="strong"><strong>Install Packages</strong></span> and then select a CRAN mirror and package to install. Another way to install packages is to use the function <code class="literal">install.packages()</code>.</p></li><li><p>To install the <code class="literal">knitr</code> package from the Carnegi Mellon Statlib CRAN mirror, we can use: <code class="literal">install.packages("knitr", repos = "http://lib.stat.cmu.edu/R/CRAN/")</code>
</p></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec03"></a>Step 3: Creating a simple report</h4></div></div></div><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Create <a id="id71" class="indexterm"></a>a blank R Markdown file: <span class="strong"><strong>File</strong></span> &gt; <span class="strong"><strong>New</strong></span> &gt; <span class="strong"><strong>R Markdown</strong></span>. You will open a new <code class="literal">.Rmd</code> file.</p></li><li><p>When you create the blank file, you can see an already-written module. One simple way to go is to replace the corresponding parts with your own information.</p><div class="mediaobject"><img src="graphics/B04883_01_04.jpg" /></div></li><li><p>After all your information is entered, click <span class="strong"><strong>Knit HTML</strong></span>.</p><div class="mediaobject"><img src="graphics/B04883_01_05.jpg" /></div></li><li><p>Now you will see that you have generated an <code class="literal">.html</code> file.</p></li></ol></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec18"></a>Spark notebooks</h3></div></div></div><p>There are a few notebooks compatible with Apache Spark computing. Among them, Databricks is one<a id="id72" class="indexterm"></a> of the best, as it was developed by the original <a id="id73" class="indexterm"></a>Spark team. The Databricks Notebook is similar to the R Markdown, but is seamlessly integrated with Apache Spark.</p><p>Besides SQL, Python, and Scala, now the Databricks notebook is also available for R, and Spark 1.4 includes the SparkR package by default. That is, from now on, data scientists and machine learning professionals can effortlessly benefit from the power of Apache Spark in their R environment, by writing and running R notebooks on top of Spark.</p><p>In addition to SparkR, any R package can be easily installed into the Databricks R notebook by using <code class="literal">install.packages()</code>. So, with the Databricks R notebook, data scientists and machine learning professionals can have the power of R Markdown on top of Spark. By using SparkR, data scientists and machine learning professionals can access and manipulate very large data sets (e.g. terabytes of data) from distributed storage (e.g. Amazon S3) or data warehouses (e.g. Hive). Data scientists and machine learning professionals can even collect a SparkR DataFrame to local data frames.</p><p>Visualization is a critical part of any machine learning project. In R Notebooks, data scientists and machine learning professionals can use any R visualization library, including R's base plotting, <code class="literal">ggplot</code>, or Lattice. Like R Markdown, plots are displayed inline in the R notebook. Users can apply Databricks' built-in <code class="literal">display()</code> function on any R DataFrame or SparkR DataFrame. The result will appear as a table in the notebook, which can then be plotted with one click. Similar to other Databricks notebooks like the Python notebook, data scientists can also use <code class="literal">displayHTML()</code> function in R notebooks to produce any HTML and Javascript visualization.</p><p>Databricks' end-to-end solution also makes building a machine learning pipeline easy from ingest to production, which applies to R Notebooks as well: Data scientists can schedule their R notebooks to run as jobs on Spark clusters. The results of each job, including visualizations, are immediately available to browse, making it much simpler and faster to turn the work into production.</p><p>To sum up, R Notebooks in Databricks let R users take advantage of the power of Spark through simple<a id="id74" class="indexterm"></a> Spark cluster management, rich one-click visualizations, and<a id="id75" class="indexterm"></a> instant deployment to production jobs. It also offers a 30-day free trial.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note14"></a>Note</h3><p>Please <a id="id76" class="indexterm"></a>visit: <a class="ulink" href="https://databricks.com/blog/2015/07/13/introducing-r-notebooks-in-databricks.html" target="_blank">https://databricks.com/blog/2015/07/13/introducing-r-notebooks-in-databricks.html</a>
</p></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec16"></a>Summary</h2></div></div><hr /></div><p>This chapter covers all the basics of Apache Spark, which all machine learning professionals are expected to understand in order to utilize Apache Spark for practical machine learning projects. We focus our discussion on Apache Spark computing, and relate it to some of the most important machine learning components, in order to connect Apache Spark and machine learning together to fully prepare our readers for machine learning projects.</p><p>First, we provided a Spark overview, and also discussed Spark's advantages as well as Spark's computing model for machine learning.</p><p>Second, we reviewed machine learning algorithms, Spark's MLlib libraries, and other machine learning libraries.</p><p>In the third section, Spark's core innovations of RDD and DataFrame has been discussed, as well as Spark's DataFrame API for R.</p><p>Fourth, we reviewed some ML frameworks, and specifically discussed a RM4Es framework for machine learning as an example, and then further discussed Spark computing frameworks for machine learning.</p><p>Fifth, we discussed machine learning as workflows, went through one workflow example, and then reviewed Spark's pipelines and its API.</p><p>Finally, we studied the notebook approach for machine learning, and reviewed R's famous notebook Markdown, then we discussed a Spark Notebook provided by Databricks, so we can use Spark Notebook to unite all the above Spark elements for machine learning practice easily.</p><p>With all the above Spark basics covered, the readers should be ready to start utilizing Apache Spark for some machine learning projects from here on. Therefore, we will work on data preparation on Spark in the next chapter, then jump into our first real life machine learning projects in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>A Holistic View on Spark</em></span>.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch02"></a>Chapter 2. Data Preparation for Spark ML</h2></div></div></div><p>Machine learning professionals and data scientists often spend 70% or 80% of their time preparing data for their machine learning projects. Data preparation can be very hard work, but it is necessary and extremely important as it affects everything to follow. Therefore, in this chapter, we will cover all the necessary data preparation parts for our machine learning, which often runs from data accessing, data cleaning, datasets joining, and then to feature development so as to get our datasets ready to develop ML models on Spark. Specifically, we will discuss the following six data preparation tasks mentioned before and then end our chapter with a discussion of repeatability and automation:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Accessing and loading datasets</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Publicly available datasets for ML</p></li><li style="list-style-type: disc"><p>Loading datasets into Spark easily</p></li><li style="list-style-type: disc"><p>Exploring and visualizing data with Spark</p></li></ul></div></li><li style="list-style-type: disc"><p>Data cleaning</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Dealing with missing cases and incompleteness</p></li><li style="list-style-type: disc"><p>Data cleaning on Spark</p></li><li style="list-style-type: disc"><p>Data cleaning made easy</p></li></ul></div></li><li style="list-style-type: disc"><p>Identity matching</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Dealing with identity issues</p></li><li style="list-style-type: disc"><p>Data matching on Spark</p></li><li style="list-style-type: disc"><p>Data matching made better</p></li></ul></div></li><li style="list-style-type: disc"><p>Data reorganizing</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Data reorganizing tasks</p></li><li style="list-style-type: disc"><p>Data reorganizing on Spark</p></li><li style="list-style-type: disc"><p>Data reorganizing made easy</p></li></ul></div></li><li style="list-style-type: disc"><p>Joining data</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Spark SQL to join datasets</p></li><li style="list-style-type: disc"><p>Joining data with Spark SQL</p></li><li style="list-style-type: disc"><p>Joining data made easy</p></li></ul></div></li><li style="list-style-type: disc"><p>Feature extraction</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Feature extraction challenges</p></li><li style="list-style-type: disc"><p>Feature extraction on Spark</p></li><li style="list-style-type: disc"><p>Feature extraction made easy</p></li></ul></div></li><li style="list-style-type: disc"><p>Repeatability and automation</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Dataset preprocessing workflows</p></li><li style="list-style-type: disc"><p>Spark pipelines for preprocessing</p></li><li style="list-style-type: disc"><p>Dataset preprocessing automation</p></li></ul></div></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec17"></a>Accessing and loading datasets</h2></div></div><hr /></div><p>In this section, we will review some publicly available datasets and cover methods of loading some <a id="id77" class="indexterm"></a>of these datasets into Spark. Then, we will review several methods of exploring and visualizing these datasets on Spark.</p><p>After this section, we will be able to find some datasets to use, load them into Spark, and then start to explore and visualize this data.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec19"></a>Accessing publicly available datasets</h3></div></div></div><p>As there is an <a id="id78" class="indexterm"></a>open source movement to make software free, there is also a very active open data movement that made a lot of datasets freely accessible to every researcher and analyst. At a worldwide scale, most governments make their collected datasets open to the public. For example, on <a class="ulink" href="http://www.data.gov/" target="_blank">http://www.data.gov/</a>, there are more than 140,000 datasets available to be used freely, which are spread over agriculture, finance, and education.</p><p>Besides open data coming from various governmental organizations, many research institutions also collect a lot of very useful datasets and make them available for public use. For our use in this book, the following is a list:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>A very rich <a id="id79" class="indexterm"></a>click dataset is provided by University of Indiana with 53.5 billion HTTP requests. To access this data, go to <a class="ulink" href="http://cnets.indiana.edu/groups/nan/webtraffic/click-dataset/" target="_blank">http://cnets.indiana.edu/groups/nan/webtraffic/click-dataset/</a>.</p></li><li style="list-style-type: disc"><p>The well known UC Irvine Machine Learning Repository archive offers more than 300 datasets for exploration. To access their datasets, go to <a class="ulink" href="https://archive.ics.uci.edu/ml/datasets.html" target="_blank">https://archive.ics.uci.edu/ml/datasets.html</a>.</p></li><li style="list-style-type: disc"><p>Project Tycho® at University of Pittsburgh provides data from all weekly notifiable<a id="id80" class="indexterm"></a> disease reports in the United States dating back to 1888. To access their datasets, go to <a class="ulink" href="http://www.tycho.pitt.edu/" target="_blank">http://www.tycho.pitt.edu/</a>.</p></li><li style="list-style-type: disc"><p>ICPSR has many datasets that are not very big but are of good quality for research. To access their data, go to <a class="ulink" href="http://www.icpsr.umich.edu/index.html" target="_blank">http://www.icpsr.umich.edu/index.html</a>.</p></li><li style="list-style-type: disc"><p>The airline<a id="id81" class="indexterm"></a> performance data from 1987 to 2008 is a well-known dataset and is also very big, with 120 million records, as it has been used in many researches and a few competitions. To access this data, go to <a class="ulink" href="http://stat-computing.org/dataexpo/2009/the-data.html" target="_blank">http://stat-computing.org/dataexpo/2009/the-data.html</a>.</p></li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec20"></a>Loading datasets into Spark</h3></div></div></div><p>There are many <a id="id82" class="indexterm"></a>ways of loading datasets into Spark or directly connecting to a data source for Spark. As Apache Spark develops with new releases once every three weeks, newer and easier methods are expected to become available to users as well as ways of representing imported data.</p><p>For example, <code class="literal">JdbcRDD</code> was the preferred way to connect with a relational data source and transfer data elements to RDD up until Spark 1.3. However, from Spark 1.4 onwards, there is an built in Spark datasource API available to connect to a JDBC source using DataFrames.</p><p>Loading data is not a simple task as it often involves converting or parsing raw data and dealing with data format transformation. The Spark datasource API allows users to use libraries based on the Data Source API to read and write dataframes between various formats from various systems. Also Spark's datasource API's data access is very efficient as it is powered by the Spark SQL query optimizer.</p><p>To load datasets in as a DataFrame, it is best to use <code class="literal">sqlContext.load</code>, for which we need to specify the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Data source name</strong></span>: This is the source that we load from</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Options</strong></span>: These are parameters for a specific data source—for example, the path of data</p></li></ul></div><p>For example, we can use the following code:</p><div class="informalexample"><pre class="programlisting">    df1 = sqlContext.read  \
        . format("json")  \  data format is json
        . option("samplingRatio", "0.01") \ set sampling ratio as 1%
        . load("/home/alex/data1,json")  \ specify data name and location</pre></div><p>To export data sets, users can use <code class="literal">dataframe.save</code> or <code class="literal">df.write</code> to save the processed DataFrame to a<a id="id83" class="indexterm"></a> source; for this, we need to specify the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Data source name</strong></span>: The source that we are saving to</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Save mode</strong></span>: This is what we should do when the data already exists</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Options</strong></span>: These are the parameters for a specific data source—for example, the path of data</p></li></ul></div><p>The<code class="literal"> creatExternalTable</code> and <code class="literal">SaveAsTable</code> commands are also very useful.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note15"></a>Note</h3><p>For more<a id="id84" class="indexterm"></a> information on using the Spark DataSource API, go to:</p><p>
<a class="ulink" href="https://databricks.com/blog/2015/01/09/spark-sql-data-sources-api-unified-data-access-for-the-spark-platform.html" target="_blank">https://databricks.com/blog/2015/01/09/spark-sql-data-sources-api-unified-data-access-for-the-spark-platform.html</a>
</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec21"></a>Exploring and visualizing datasets</h3></div></div></div><p>Within Apache<a id="id85" class="indexterm"></a> Spark, there are many ways to conduct some initial exploration<a id="id86" class="indexterm"></a> and visualization of the datasets loaded with various tools. Users may use Scala or Python direct with Spark shells. Alternatively, users can take a notebook approach, which is to use the R or Python notebook in a Spark environment, similar to that of DataBricks Workspace. Another option is to utilize Spark's MLlib.</p><p>Alternatively, users can directly use Spark SQL and its associated libraries, such as the popular Panda library, for some simple data exploration.</p><p>If datasets are already transformed into Spark DataFrames, users may use <code class="literal">df.describe().show()</code> to obtain some simple statistics with values of total count of cases, mean, standard deviation, min, and max for all the columns (variables).</p><p>If DataFrame has a lot of columns, users should specify columns in <code class="literal">df.describe(column1, column2, …).show()</code> to just obtain simple descriptive statistics of the columns they are interested in. You may also just use this command to select the statistics you need:</p><div class="informalexample"><pre class="programlisting">df.select([mean('column1'),min('column1'),max('column1')]).show()</pre></div><p>Beyond this, some commonly used commands for covariance, correlation, and cross-tabulation tables are as follows:</p><div class="informalexample"><pre class="programlisting">df.stat.cov('column1', 'column2')
df.stat.corr('column1', 'column2')
df.stat.crosstab("column1", "column2").show()</pre></div><p>If using the DataBricks workspace, users can create an R notebook; then they will be back to the familiar R <a id="id87" class="indexterm"></a>environment with access to all the R packages and can<a id="id88" class="indexterm"></a> take advantage of the notebook approach for an interactive exploration and visualization of datasets. Take a look at the following:</p><div class="informalexample"><pre class="programlisting">&gt; summary(x1)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  0.000   0.600   2.000   2.667   4.000   8.000 
&gt; plot(x1)</pre></div><div class="mediaobject"><img src="graphics/B04883_02_01.jpg" /></div><p>As we will start using the DataBricks Workspace a lot from now on, it is recommended for users to sign <a id="id89" class="indexterm"></a>up at <a class="ulink" href="https://accounts.cloud.databricks.com/registration.html#signup" target="_blank">https://accounts.cloud.databricks.com/registration.html#signup</a> for a trial test. Go to its main menu to its upper-left corner and set up some clusters, as follows:</p><div class="mediaobject"><img src="graphics/B04883_02_02.jpg" /></div><p>Then, users <a id="id90" class="indexterm"></a>can go to the same main menu, click on the down arrow on the<a id="id91" class="indexterm"></a> right-hand side of <span class="strong"><strong>Workspace</strong></span> and navigate to <span class="strong"><strong>Create</strong></span> | <span class="strong"><strong>New Notebook</strong></span> as follows to create a notebook:</p><div class="mediaobject"><img src="graphics/B04883_02_03.jpg" /></div><p>When the <span class="strong"><strong>Create Notebook</strong></span> dialog appears, you need to perform the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Enter a<a id="id92" class="indexterm"></a> unique <span class="emphasis"><em>name</em></span> for your notebook</p></li><li style="list-style-type: disc"><p>For <span class="emphasis"><em>language</em></span>, click <a id="id93" class="indexterm"></a>on the drop-down menu and select R</p></li><li style="list-style-type: disc"><p>For <span class="emphasis"><em>cluster</em></span>, click on the drop–down menu and select the cluster you previously created</p></li></ul></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec18"></a>Data cleaning</h2></div></div><hr /></div><p>In this section, we will review some methods for data cleaning on Spark with a focus on data incompleteness. Then, we will discuss some of Spark's special features for data cleaning and also some data cleaning solutions made easy with Spark.</p><p>After this <a id="id94" class="indexterm"></a>section, we will be able to clean data and make datasets ready for machine learning.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec22"></a>Dealing with data incompleteness</h3></div></div></div><p>For machine learning, the more the data the better. However, as is often the case, the more the data, the dirtier it could be—that is, the more the work to clean the data.</p><p>There are <a id="id95" class="indexterm"></a>many issues to deal with data quality control, which can be as simple as data entry errors or data duplications. In principal, the methods of treating them are similar—for example, utilizing data logic for discovery and subject matter knowledge and analytical logic to correct them. For this reason, in this section, we will focus on missing value treatment so as to illustrate our usage of Spark for this topic. Data cleaning covers data accuracy, completeness, uniqueness, timeliness, and consistency.</p><p>Treating missing values and dealing with incompleteness is not an easy task, though it may sound simple. It involves many issues and often requires the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Counting the missing percentage.</p><p>If the percentage is lower than 5% or 10% then, depending on the studies, we may not need to spend time on it.</p></li><li><p>Studying the missing patterns.</p><p>There are two patterns of missing data: completely at random or not at random. If they are missing completely at random, we can ignore this issue.</p></li><li><p>Deciding the methods to deal with missing patterns.</p><p>There are several commonly used methods to deal with missing cases. Filling with mean, deleting the missing cases, and imputation are among the main ones.</p></li><li><p>Performing<a id="id96" class="indexterm"></a> filling for missing patterns.</p><p>To work with missing cases and incompleteness, data scientists and machine learning professionals often utilize their familiar SQL tools or R programming. Fortunately, within the Spark environment, there are Spark SQL and R notebooks for users to continue their familiar paths, for which we will have detailed reviews in the following two sections.</p></li></ol></div><p>There are also other issues with data cleaning, such as treating data entry errors and outliers.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec23"></a>Data cleaning in Spark</h3></div></div></div><p>In the preceding section, we discussed working with data incompleteness.</p><p>With Spark<a id="id97" class="indexterm"></a> installed, we can easily use the Spark SQL and R notebook on DataBricks Workspace for the data cleaning work described in the previous section.</p><p>Especially, the <code class="literal">sql</code> function on <code class="literal">sqlContext</code> enables applications to run SQL queries programmatically and return the result as a DataFrame.</p><p>For example, with R notebook, we can use the following to perform SQL commands and turn the results into a <code class="literal">data.frame</code>:</p><div class="informalexample"><pre class="programlisting">sqlContext &lt;- sparkRSQL.init(sc)
df &lt;- sql(sqlContext, "SELECT * FROM table")</pre></div><p>Data cleaning is a very tedious and time-consuming work and, in this section, we would like to bring your attention to SampleClean, which can make data cleaning, and especially distributed data cleaning, easy for machine learning professionals.</p><p>SampleClean is a scalable data cleaning library built on AMPLab <span class="strong"><strong>Berkeley Data Analytics Stack</strong></span> (<span class="strong"><strong>BDAS</strong></span>). The library uses Apache Spark SQL 1.2.0 and above as well as Apache Hive to <a id="id98" class="indexterm"></a>support distributed data cleaning operations and related query processing on dirty data. SampleClean implements a set of interchangeable and composable physical and logical data cleaning operators, which makes quick construction and adaptation of data cleaning pipelines possible.</p><p>To get our work started, let's first import Spark and SampleClean with the following commands:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import sampleclean.api.SampleCleanContext</pre></div><p>To begin<a id="id99" class="indexterm"></a> using <code class="literal">SampleClean</code>, we need to create an object called <code class="literal">SampleCleanContext</code>, and then use this context to manage all of the information for working sessions and provide the API primitives to interact with the data. <code class="literal">SampleCleanContext</code> is constructed with a <code class="literal">SparkContext</code> object, as follows:</p><div class="informalexample"><pre class="programlisting">new SampleCleanContext(sparkContext)</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec24"></a>Data cleaning made easy</h3></div></div></div><p>With SampleClean and <a id="id100" class="indexterm"></a>Spark together, we can make data<a id="id101" class="indexterm"></a> cleaning easy, which is to write less code and utilize less data.</p><p>Overall, SampleClean employs a good strategy; it uses asynchrony to hide latency and sampling to hide scale. Also, SampleClean combines all the three (<span class="emphasis"><em>Algorithms</em></span>, <span class="emphasis"><em>Machines</em></span>, and <span class="emphasis"><em>People</em></span>) in one system to become more efficient than others.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note16"></a>Note</h3><p>For more<a id="id102" class="indexterm"></a> information on using <code class="literal">SampleClean</code>, go to: <a class="ulink" href="http://sampleclean.org/guide/" target="_blank">http://sampleclean.org/guide/</a> and <a class="ulink" href="http://sampleclean.org/release.html" target="_blank">http://sampleclean.org/release.html</a>.</p></div><p>For the purposes of illustration, let's imagine a machine learning project with four data tables:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">Users(userId INT, name String, email STRING,age INT, latitude: DOUBLE, longitude: DOUBLE,subscribed: BOOLEAN)</code>
</p></li><li style="list-style-type: disc"><p>
<code class="literal">Events(userId INT, action INT, Default)</code>
</p></li><li style="list-style-type: disc"><p>
<code class="literal">WebLog(userId, webAction)</code>
</p></li><li style="list-style-type: disc"><p>
<code class="literal">Demographic(memberId, age, edu, income)</code>
</p></li></ul></div><p>To clean this dataset, we need to:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Count how many are missing for each variable, either with the SQL or R commands</p></li><li style="list-style-type: disc"><p>Fill in the missing cases with the mean value if this is the strategy we agree to</p></li></ul></div><p>Even though the <a id="id103" class="indexterm"></a>preceding are very easy to implement, it could be very<a id="id104" class="indexterm"></a> time consuming if our data is huge. Therefore, for efficiency, we may need to divide the data into many subsets and complete the previous steps in parallel, for which Spark becomes the best computing platform to use.</p><p>In the Databricks R notebook environment, we can first create notebooks with the R command <code class="literal">sum(is.na(x)) </code>to count the missing cases.</p><p>To replace the missing cases with the mean, we can use the following code:</p><div class="informalexample"><pre class="programlisting">for(i in 1:ncol(data)){
  data[is.na(data[,i]), i] &lt;- mean(data[,i], na.rm = TRUE)
}</pre></div><p>In Spark, we can easily schedule to implement R notebooks in all the data clusters.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec19"></a>Identity matching</h2></div></div><hr /></div><p>In this section, we will cover one important data preparation topic, which is about identity matching and <a id="id105" class="indexterm"></a>related solutions. We will discuss some of Spark's special features for solving identity issues and also some data matching solutions made easy with Spark.</p><p>After this section, we will be capable of taking care of some common data identity problems with Apache Spark.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec25"></a>Identity issues</h3></div></div></div><p>For data preparation, we often need to deal with some data elements that belong to the same person or units, but which do not look similar to them. For example, we may have purchased some data for customer Larry Z. and web activity data for L. Zhang. Is Larry Z a same person<a id="id106" class="indexterm"></a> as L. Zhang? Are there many identity variations in the data?</p><p>Matching entities is a big challenge for machine learning data preparation as these types of entity variation are very common and could be caused by many different reasons, such as duplications, errors, name variants, and intentional aliasing. Sometimes, it could be very difficult to complete the matching or even just to find the linking, and this work is definitely very time consuming. However, it is necessary and extremely important as any kind of mismatching will produce a lot of errors, and no matching will produce biases. At the same time, a correct matching also has additional values as an aid to group detection, such as with terror cells and drug cartels.</p><p>Some new methods, such as fuzzy matching, have been developed to attack this issue. However, in this section, we will focus on some commonly used methods. These commonly used <a id="id107" class="indexterm"></a>approaches include:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Manual search with SQL queries.</p><p>This is a labor intensive with few discoveries but good accuracy.</p></li><li style="list-style-type: disc"><p>Automated data cleansing.</p><p>This type of approach often adopts a few rules that use the most informative attributes.</p></li><li style="list-style-type: disc"><p>Lexical similarity.</p><p>This approach is rational and useful but can generate many false alarms.</p></li><li style="list-style-type: disc"><p>Feature and relationship statistics.</p><p>This approach is a good one but does not address nonlinear effects.</p></li></ul></div><p>The accuracy of any of the preceding methods often depends on the sparseness and size of the data and also on whether these tasks are to resolve duplications, errors, variants, or aliases.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec26"></a>Identity matching on Spark</h3></div></div></div><p>Similarly to the<a id="id108" class="indexterm"></a> previous section, we would like to review some methods utilizing SampleClean to deal with entity matching issues even though the most commonly used tools are <code class="literal">SparkSQL</code> or R.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec27"></a>Entity resolution</h3></div></div></div><p>SampleClean<a id="id109" class="indexterm"></a> provides an easy-to-use interface for <a id="id110" class="indexterm"></a>some basic entity matching tasks. It provides the <code class="literal">EntityResolution</code> class that wraps some common deduplication programming patterns.</p><p>A basic <code class="literal">EntityResolution</code> class involves the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Identifying a column of inconsistent categorical attributes.</p></li><li><p>Linking together similar attributes.</p></li><li><p>Selecting a single canonical representation of the linked attributes.</p></li><li><p>Applying changes to the data.</p></li></ol></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec04"></a>Short string comparison</h4></div></div></div><p>Here, we<a id="id111" class="indexterm"></a> have a column of short strings that are inconsistently represented (for example, United States, United States). The <code class="literal">EntityResolution.shortAttributeCanonicalize</code> function takes as input the current context, the name of the working set to clean, the column to fix, and a threshold in [0,1] (0 merges all, and 1 merges only the exact matches). It uses <code class="literal">EditDistance</code> as its default similarity metric. The following is a coding example:</p><div class="informalexample"><pre class="programlisting">val algorithm = EntityResolution.shortAttributeCanonicalize(scc,workingSetName,columnName,threshold)</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec05"></a>Long string comparison</h4></div></div></div><p>Here, we <a id="id112" class="indexterm"></a>have a column of long strings, such as addresses, that are close but not exact. The basic strategy is to tokenize these strings and compare the set of words rather than the whole string. It uses the <code class="literal">WeightedJaccard</code> similarity metric as default. The following is a coding example:</p><div class="informalexample"><pre class="programlisting">longAttributeCanonicalize(scc,workingSetName,columnName,threshold)</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec06"></a>Record deduplication</h4></div></div></div><p>A more <a id="id113" class="indexterm"></a>advanced deduplication task is when records, rather than individual columns, are inconsistent. That is, there are multiple records that refer to the same real entity. <code class="literal">RecordDeduplication</code> uses <span class="emphasis"><em>Long Attribute</em></span> similarity metrics as default. The following is a coding example:</p><div class="informalexample"><pre class="programlisting">RecordDeduplication.deduplication(scc, workingSetName, columnProjection, threshold)</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note17"></a>Note</h3><p>For <a id="id114" class="indexterm"></a>more information on the <code class="literal">SampleClean</code> guide, visit <a class="ulink" href="http://sampleclean.org/guide/" target="_blank">http://sampleclean.org/guide/</a>.</p></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec28"></a>Identity matching made better</h3></div></div></div><p>Similarly to data cleaning, with SampleClean and Spark together we can make things easy—that is<a id="id115" class="indexterm"></a> write less code and utilize less data—as <a id="id116" class="indexterm"></a>demonstrated in the previous section. As discussed, automated cleaning is easy and fast, but its accuracy may not be good. A common approach to make things better is to utilize more people for labor-intensive approval based on crowd sourcing.</p><p>Here, SampleClean combines Algorithms, Machines, and People, all in its crowd-sourced deduplication.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec07"></a>Crowdsourced deduplication</h4></div></div></div><p>As crowdsourcing scales poorly to very large datasets, the SampleClean system asks the crowd to <a id="id117" class="indexterm"></a>deduplicate only a sample of the data and then train predictive models to generalize the crowd's work to the entire dataset. In particular, SampleClean applies Active Learning to sample points that lead to a good model quickly.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec08"></a>Configuring the crowd</h4></div></div></div><p>To clean data<a id="id118" class="indexterm"></a> using crowd workers, SampleClean uses the open source AMPCrowd service to support multiple crowd platforms and provide automated quality control. Thus, users must have a running installation of AMPCrowd. In addition, crowd operators must be configured to point to the AMPCrowd server by passing the <code class="literal">CrowdConfiguration</code> objects.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec09"></a>Using the crowd</h4></div></div></div><p>SampleClean <a id="id119" class="indexterm"></a>currently provides one main crowd operator: ActiveLearningMatcher. This is an add-on step to an existing <code class="literal">EntityResolution</code> algorithm that trains a crowd-supervised model to predict duplicates. Take a look at the following code:</p><div class="informalexample"><pre class="programlisting">createCrowdMatcher(scc:SampleCleanContext, attribute:String, workingSetName:String) 
val crowdMatcher = EntityResolution.createCrowdMatcher(scc,attribute,workingSetName)</pre></div><p>Make sure to configure the matcher here, as follows:</p><div class="informalexample"><pre class="programlisting">crowdMatcher.alstrategy.setCrowdParameters(crowdConfig)</pre></div><p>To add this matcher to existing algorithms, use the following function:</p><div class="informalexample"><pre class="programlisting">addMatcher(matcher:Matcher)
algorithm.components.addMatcher(crowdMatcher)</pre></div></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec20"></a>Dataset reorganizing</h2></div></div><hr /></div><p>In this section, we <a id="id120" class="indexterm"></a>will cover dataset reorganization techniques. Then, we will discuss some of Spark's special features for data reorganizing and also some of R's special methods for data reorganizing that can be used with the Spark notebook.</p><p>After this section, we will be able to reorganize datasets for various machine learning needs.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec29"></a>Dataset reorganizing tasks</h3></div></div></div><p>Reorganizing datasets sounds easy but could be very challenging and also often very time consuming.</p><p>Two common data reorganizing tasks are—firstly, to obtain a subset of the data for modeling and, <a id="id121" class="indexterm"></a>secondly, to aggregate data to a higher level. For example, we have students' data, but we need to have a dataset at the classroom level. For this, we will need to calculate some attributes for students and then reorganize it into new data.</p><p>To work with data reorganizing, data scientists and machine learning professionals often utilize their familiar SQL or R programming tools. Fortunately within the Spark environment, there are Spark SQL and R notebooks for users to continue their familiar paths; we will have detailed reviews in the following two sections for this.</p><p>Overall, we recommend using SparkSQL to reorganizing datasets. However, for the learning purpose, in this section, our focus will be on the utilization of R Notebook from Databricks Workspace.</p><p>R and Spark nicely complement each other for several important use cases in statistics and data science. The Databricks R notebooks include the SparkR package by default so that data scientists can effortlessly benefit from the power of Apache Spark in their R analyses. In addition to SparkR, any R package can be easily installed into the notebook. In this blog post, I will highlight a few of the features in our R notebooks.</p><div class="mediaobject"><img src="graphics/B04883_02_04.jpg" /></div><p>To get started with R in Databricks, simply choose R as the language when creating a notebook. Since SparkR is a recent addition to Spark, remember to attach the R notebook to any cluster running Spark version 1.4 or higher. The SparkR package is imported and configured<a id="id122" class="indexterm"></a> by default. You can run Spark queries in R.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec30"></a>Dataset reorganizing with Spark SQL</h3></div></div></div><p>In the last <a id="id123" class="indexterm"></a>section, we discussed using SparkSQL to<a id="id124" class="indexterm"></a> reorganize datasets.</p><p>SQL can be a powerful tool to perform complex aggregations with many familiar examples to machine learning professionals.</p><p>
<code class="literal">SELECT</code> is a command to obtain some data subsets.</p><p>For data aggregation, machine learning professionals may use some of SpartSQL's <code class="literal">simple.aggregate</code> or window functions.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note18"></a>Note</h3><p>For more<a id="id125" class="indexterm"></a> information about SparkSQL's various aggregation functions, go to <a class="ulink" href="https://spark.apache.org/docs/1.4.0/api/scala/index.html#org.apache.spark.sql.functions$" target="_blank">https://spark.apache.org/docs/1.4.0/api/scala/index.html#org.apache.spark.sql.functions$</a>.</p><p>For more information on SparkSQL's window functions, go to <a class="ulink" href="https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html" target="_blank">https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html</a>.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec31"></a>Dataset reorganizing with R on Spark</h3></div></div></div><p>R has a<a id="id126" class="indexterm"></a> subset command to create subsets with the following<a id="id127" class="indexterm"></a> formats:</p><div class="informalexample"><pre class="programlisting"># using subset function newdata &lt;- subset(olddata, var1 &gt;= 20, select=c(ID, var2))</pre></div><p>Also, we may use the aggregate command from R, as follows:</p><div class="informalexample"><pre class="programlisting">aggdata &lt;-aggregate(mtcars, by=list(cyl,vs), FUN=mean, na.rm=TRUE)</pre></div><p>However, data often has multiple levels of grouping (nested treatments, split plot designs, or repeated measurements) and typically requires investigation at multiple levels. For example, from a long-term clinical study, we may be interested in investigating relationships over time or between times or patients or treatments. To make your job even more difficult, the data probably has been collected and stored in a way optimized for ease and accuracy of collection and in no way resembles the form you need for statistical analysis. You need to be able to fluently and fluidly reshape the data to meet your needs, but most software packages make it difficult to generalize these tasks, and new code needs to be written for each new case.</p><p>Especially, R has a <code class="literal">reshape</code> package that was specially designed for data reorganization. The package <code class="literal">reshape</code> uses a paradigm of <span class="emphasis"><em>melting and casting</em></span>, where the data is <span class="emphasis"><em>melted</em></span> into a form which distinguishes measured and identifying variables and then "casts" it into a<a id="id128" class="indexterm"></a> new shape, whether it be a data frame, list, or highly <a id="id129" class="indexterm"></a>dimensional array.</p><p>As we may recall, in section <span class="emphasis"><em>Data cleaning made easy</em></span>, we had four tables for the purposes of illustration:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">Users(userId INT, name String, email STRING,age INT, latitude: DOUBLE, longitude: DOUBLE,subscribed: BOOLEAN)</code>
</p></li><li style="list-style-type: disc"><p>
<code class="literal">Events(userId INT, action INT, Default)</code>
</p></li><li style="list-style-type: disc"><p>
<code class="literal">WebLog(userId, webAction)</code>
</p></li><li style="list-style-type: disc"><p>
<code class="literal">Demographic(memberId, age, edu, income)</code>
</p></li></ul></div><p>For this example, we often need to obtain a subset from the first data and aggregate the fourth data.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec21"></a>Dataset joining</h2></div></div><hr /></div><p>In this section, we will cover dataset joining techniques. We will also discuss some of Spark's special<a id="id130" class="indexterm"></a> features for data joining plus some data joining solutions made easy with Spark.</p><p>After this section, we will be able to join data for various machine learning needs.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec32"></a>Dataset joining and its tool – the Spark SQL</h3></div></div></div><p>In preparing<a id="id131" class="indexterm"></a> datasets for a machine learning project, we <a id="id132" class="indexterm"></a>often need to combine data from multiple datasets. For relational tables, the task is to join tables through a primary and foreign key relationship.</p><p>Joining two or more datasets together sounds easy, but can be very challenging and time consuming. In SQL, <code class="literal">SELECT</code> is the most frequently used command. As an example, the following is a typical SQL code to perform a join:</p><div class="informalexample"><pre class="programlisting">SELECT column1, column2, …
FROM table1, table2
WHERE table1.joincolumn = table2.joincolumn
AND search_condition(s);</pre></div><p>To work with the table joining tasks mentioned before, data scientists and machine learning professionals often utilize their familiar SQL tools. Within the Spark environment, the Spark SQL was created for this task.</p><p>The Spark SQL lets users query structured data inside Spark programs using either SQL or a familiar DataFrame API, which is usable in the R notebook as well. The Spark SQL reuses the <a id="id133" class="indexterm"></a>Hive frontend and metastore, giving full compatibility <a id="id134" class="indexterm"></a>with existing Hive data and queries.</p><p>The Spark SQL includes a cost-based optimizer, columnar storage, and code generation to make queries fast. At the same time, it scales to thousands of nodes and multihour queries using the Spark engine, which provides full mid-query fault tolerance.</p><p>The two main components<a id="id135" class="indexterm"></a> when using the Spark SQL are <span class="strong"><strong>DataFrame</strong></span> and<a id="id136" class="indexterm"></a> <span class="strong"><strong>SQLContext</strong></span>.</p><p>As discussed before, DataFrame is a distributed collection of data organized into named columns. It is based on the data frame concept in R language and is similar to a database table in a relational database. The Spark SQL provides SQLContext to encapsulate all relational functionality in Spark.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec33"></a>Dataset joining in Spark</h3></div></div></div><p>Here, with<a id="id137" class="indexterm"></a> some concrete examples, we will demonstrate some methods and related processes of using Spark SQL.</p><p>For the purposes of illustration, imagine an application with the following four tables:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">Users(userId INT, name String, email STRING,age INT, latitude: DOUBLE, longitude: DOUBLE,subscribed: BOOLEAN)</code>
</p></li><li style="list-style-type: disc"><p>
<code class="literal">Events(userId INT, action INT, Default)</code>
</p></li><li style="list-style-type: disc"><p>
<code class="literal">WebLog(userId, webAction)</code>
</p></li><li style="list-style-type: disc"><p>
<code class="literal">Demographic(memberId, age, edu, income)</code>
</p></li></ul></div><p>At least, we need to join the <code class="literal">User</code> and <code class="literal">Events</code> tables together; we can do this with the following code:</p><div class="informalexample"><pre class="programlisting">val trainingDataTable = sql("""
  SELECT e.action
         u.age,
         u.latitude,
         u.logitude
  FROM Users u
  JOIN Events e
  ON u.userId = e.userId""")</pre></div><p>As the results of the Spark SQL are also stored in RDDs, interfacing with other Spark libraries is trivial. The returned results from the preceding can be directly used for machine learning.</p><p>In the preceding example, the Spark SQL made it easy to join various datasets, preparing them for the machine learning algorithm. Furthermore, the Spark SQL allows developers to close the loop by making it easy to manipulate and join the output of these algorithms, producing the desired final result.</p><p>For more<a id="id138" class="indexterm"></a> information about using the Spark SQL, please <a id="id139" class="indexterm"></a>go to:</p><p>
<a class="ulink" href="http://spark.apache.org/docs/1.0.0/sql-programming-guide.html" target="_blank">http://spark.apache.org/docs/1.0.0/sql-programming-guide.html</a>
</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec34"></a>Dataset joining with the R data table package</h3></div></div></div><p>The Spark technology has made data work faster and data analytics easier than before.</p><p>According to the Spark development team, the Spark SQL was created for:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Writing less code</p></li><li style="list-style-type: disc"><p>Reading less data</p></li><li style="list-style-type: disc"><p>Passing the hard work to optimizer</p></li></ul></div><p>This was achieved by utilizing DataFrames and the Spark SQL commands, <code class="literal">sqlContext.read</code> and <code class="literal">df.write</code>.</p><p>Besides Spark SQL, users may also use R to join tables, for which the <code class="literal">data.table</code> R package is very powerful and should be used. The <code class="literal">data.table</code> package is created for:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Fast aggregation of large data (for example, 100 GB in RAM) and fast ordered joins</p></li><li style="list-style-type: disc"><p>Fast adding/modifying/deleting of columns by group, using no copies at all</p></li><li style="list-style-type: disc"><p>Listing columns and a fast file reader (<code class="literal">fread</code>)</p></li></ul></div><p>This package offers a natural and flexible syntax for faster development as well.</p><p>To use <code class="literal">data.table</code> for joining, you need to create a <code class="literal">data.frame</code> first, which is easy.</p><p>Then, just use <code class="literal">X[Y]</code> to join two tables.</p><p>This is also<a id="id140" class="indexterm"></a> known as <span class="strong"><strong>Last Observation Carried Forward</strong></span> (<span class="strong"><strong>LOCF</strong></span>) or a rolling join.</p><p>
<code class="literal">X[Y]</code> is a join between <code class="literal">data.table</code> <code class="literal">X</code> and <code class="literal">data.table</code> <code class="literal">Y</code>. If <code class="literal">Y</code> has two columns, the first column is matched to the first column of the key of <code class="literal">X</code>, and the second column is matched to the second. An <span class="emphasis"><em>equi-join</em></span> is performed by default, meaning that the values must be equal.</p><p>Instead of an equi-join, a rolling join is as follows:</p><div class="informalexample"><pre class="programlisting">X[Y,roll=TRUE]</pre></div><p>As before, the first column of <code class="literal">Y</code> is matched to <code class="literal">X</code>, where the values are equal. However, the last join column in <code class="literal">Y</code>, the second one in this example, is treated specially. If no match is found, then the row before is returned, provided the first column still matches.</p><p>Further <a id="id141" class="indexterm"></a>controls are rolling forwards, rolling backwards, rolling to the nearest, and limited staleness.</p><p>For example, type the following and follow the output at the prompt:</p><div class="informalexample"><pre class="programlisting">example(data.table)</pre></div><p>The R <code class="literal">data.table</code> package provides an enhanced version of <code class="literal">data.frame</code>, including:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Fast aggregation of large data—for example, 100 GB in RAM (take a look at the benchmarks on up to two billion rows)</p></li><li style="list-style-type: disc"><p>Fast ordered joins—for example, rolling forwards, rolling backwards, rolling to the nearest, and limited staleness</p></li><li style="list-style-type: disc"><p>Fast overlapping range joins—for example, GenomicRanges</p></li></ul></div><p>As we may recall, in section <span class="emphasis"><em>Data cleaning made easy</em></span>, we had four tables for the purposes of illustration, as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">Users(userId INT, name String, email STRING,age INT, latitude: DOUBLE, longitude: DOUBLE,subscribed: BOOLEAN)</code>
</p></li><li style="list-style-type: disc"><p>
<code class="literal">Events(userId INT, action INT, Default)</code>
</p></li><li style="list-style-type: disc"><p>
<code class="literal">WebLog(userId, webAction)</code>
</p></li><li style="list-style-type: disc"><p>
<code class="literal">Demographic(memberId, age, edu, income)</code>
</p></li></ul></div><p>For this example, we obtained a subset from the first data and aggregated the fourth data in last section. Now, we need to join them together. As per the preceding section, mixing the Spark SQL with R on the R notebook could make data joining very easy.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec22"></a>Feature extraction</h2></div></div><hr /></div><p>In this section, we <a id="id142" class="indexterm"></a>will turn our focus to feature extraction, which is to develop new features or variables from the available features or information of working datasets. At the same time, we will discuss some of Apache Spark's special capabilities for feature extraction as well as some related feature solutions made easy with Spark.</p><p>After this section, we will be able to develop and organize features for various machine learning projects.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec35"></a>Feature development challenges</h3></div></div></div><p>For most big data machine learning projects, with many big datasets, we often cannot use them immediately. For example, when we take in some web log data, it is very messy and often in a <a id="id143" class="indexterm"></a>form such as a collection of random text, from which we need to extract useful information and draw out useful features ready for machine learning. For example, we need to extract <span class="emphasis"><em>number of clicks</em></span> and <span class="emphasis"><em>number of impressions</em></span> out from web log data, for which many text mining tools and algorithms are ready to be used.</p><p>With any feature extraction, machine learning professionals need to decide:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>What information to use and what features to create</p></li><li style="list-style-type: disc"><p>What methods and algorithms to use</p></li></ul></div><p>What feature to extract depends on the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Data availability and also data properties, such as how easy it is to handle missing cases</p></li><li style="list-style-type: disc"><p>The available algorithms, as there are a lot of algorithms available for the numeric combination of data elements but less on text manipulation</p></li><li style="list-style-type: disc"><p>The domain knowledge as the explained ability of features is often concerned</p></li></ul></div><p>Overall, there are a few commonly used techniques to track features:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Data description</p></li><li style="list-style-type: disc"><p>Data aggregation</p></li><li style="list-style-type: disc"><p>Time series transformations</p></li><li style="list-style-type: disc"><p>Geographical</p></li><li style="list-style-type: disc"><p>PCA</p></li></ul></div><p>Another task for feature preparation is to select features from hundreds or perhaps thousands of available features and then make them available for our ML projects. In machine learning, specifically in supervised learning, the general problem at hand is always to predict an outcome from a set of predictive features. At first glance, in our big data era, it is tempting to say that the more features we have, the better our predictions will be. However, there are problems that arise as the number of features increases, such as an increase in computing time, which may cause difficulties in interpreting results.</p><p>In most cases, for the feature preparation stage, machine learning professionals often use feature selection methods and algorithms, which are associated with regression modeling.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec36"></a>Feature development with Spark MLlib</h3></div></div></div><p>Feature extraction could be implemented with the Spark SQL, while Spark's MLlib also has some<a id="id144" class="indexterm"></a> special functions for this task, such as<a id="id145" class="indexterm"></a> TF-IDF and Word2Vec.</p><p>Both MLlib and R have packages for principal component analysis, which are often employed for feature development.</p><p>As we may recall, in section <span class="emphasis"><em>Data cleaning made easy</em></span>, we have four data tables to work with for the purpose of illustration:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">Users(userId INT, name String, email STRING,age INT, latitude: DOUBLE, longitude: DOUBLE,subscribed: BOOLEAN)</code>
</p></li><li style="list-style-type: disc"><p>
<code class="literal">Events(userId INT, action INT, Default)</code>
</p></li><li style="list-style-type: disc"><p>
<code class="literal">WebLog(userId, webAction)</code>
</p></li><li style="list-style-type: disc"><p>
<code class="literal">Demographic(memberId, age, edu, income)</code>
</p></li></ul></div><p>Here, we can apply our feature extraction techniques to the third data and then apply feature selection to the final merged (joined) dataset.</p><p>With Spark MLlib, we can apply TF-IDF with the following commands:</p><div class="informalexample"><pre class="programlisting">val hashingTF = new HashingTF()
val tf: RDD[Vector] = hashingTF.transform(documents)</pre></div><p>Alternatively, we can apply <code class="literal">Word2Vec</code> as illustrated by the following example. The following example (in Scala) first loads a text file, parses it as an RDD of <code class="literal">Seq[String]</code>, constructs a <code class="literal">Word2Vec</code> instance, and then fits <code class="literal">Word2VecModel</code> with the data. Then, we can display the top 40 synonyms of the specified word. Here, we will assume that the extracted file named <code class="literal">text8</code> is in same directory as you run the Spark shell. Run the following code:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark._
import org.apache.spark.rdd._
import org.apache.spark.SparkContext._
import org.apache.spark.mllib.feature.{Word2Vec, Word2VecModel}

val input = sc.textFile("text8").map(line =&gt; line.split(" ").toSeq)

val word2vec = new Word2Vec()

val model = word2vec.fit(input)

val synonyms = model.findSynonyms("china", 40)

for((synonym, cosineSimilarity) &lt;- synonyms) {
  println(s"$synonym $cosineSimilarity")
}

// Save and load model
model.save(sc, "myModelPath")
val sameModel = Word2VecModel.load(sc, "myModelPath")</pre></div><p>For <a id="id146" class="indexterm"></a>more information about using Spark MLlib for <a id="id147" class="indexterm"></a>feature <a id="id148" class="indexterm"></a>extraction, go to:</p><p>
<a class="ulink" href="http://spark.apache.org/docs/latest/mllib-feature-extraction.html" target="_blank">http://spark.apache.org/docs/latest/mllib-feature-extraction.html</a>
</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec37"></a>Feature development with R</h3></div></div></div><p>As for the<a id="id149" class="indexterm"></a> four tables mentioned before, take a look at the <a id="id150" class="indexterm"></a>following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">Users(userId INT, name String, email STRING,age INT, latitude: DOUBLE, longitude: DOUBLE,subscribed: BOOLEAN)</code>
</p></li><li style="list-style-type: disc"><p>
<code class="literal">Events(userId INT, action INT, Default)</code>
</p></li><li style="list-style-type: disc"><p>
<code class="literal">WebLog(userId, webAction)</code>
</p></li><li style="list-style-type: disc"><p>
<code class="literal">Demographic(memberId, age, edu, income)</code>
</p></li></ul></div><p>As discussed, we can apply our feature extraction techniques to the third data and then apply feature selection to the final merged (joined) data set.</p><p>If we implement them in R, with the R notebook in Spark, we need to utilize some of the R packages. If we use <code class="literal">ReporteRs</code>, we can execute the following commands:</p><div class="informalexample"><pre class="programlisting">## Not run: 
doc = docx( title = "My example", template = file.path( 
  find.package("ReporteRs"), "templates/bookmark_example.docx") )
text_extract( doc )
text_extract( doc, header = FALSE, footer = FALSE )
text_extract( doc, bookmark = "author" )

## End(Not run)</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note19"></a>Note</h3><p>For more<a id="id151" class="indexterm"></a> information on the <code class="literal">ReporteRs</code> R package, go to <a class="ulink" href="https://cran.r-project.org/web/packages/ReporteRs/ReporteRs.pdf" target="_blank">https://cran.r-project.org/web/packages/ReporteRs/ReporteRs.pdf</a>.</p></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec23"></a>Repeatability and automation</h2></div></div><hr /></div><p>In this section, we <a id="id152" class="indexterm"></a>will discuss some methods of organizing datasets, preprocessing<a id="id153" class="indexterm"></a> into workflows, and then use the Apache Spark pipeline to represent as well as implement these workflows. Then, we will review data preprocessing automation solutions.</p><p>After this section, we will be able to use Spark pipelines to represent and implement datasets preprocessing workflows and understand some automation solutions made available by Apache Spark.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec38"></a>Dataset preprocessing workflows</h3></div></div></div><p>Our data preparation work from <span class="emphasis"><em>Data cleaning to Identity matching</em></span> to <span class="emphasis"><em>Data re-organization to Feature extraction</em></span> were organized in a way to reflect our step-by-step orderly process of preparing datasets for machine learning. In other words, all the data preparation work can be organized into a workflow.</p><p>Organizing data <a id="id154" class="indexterm"></a>cleaning into workflows can<a id="id155" class="indexterm"></a> help achieve repeatability and also possible automation, which is often the most valuable for machine learning professionals as ML professionals and data scientists often spend 80% of their time on data cleaning and preprocessing.</p><p>For most ML projects, including the ones to be discussed in later chapters, data scientists need to split their data into training, testing, and validation sets; here, the same preprocessing of the training set needs to be repeated on the testing and validation sets. For this reason alone, utilizing workflows to repeat will save ML professionals a lot of time and also help avoiding many mistakes.</p><p>Using Spark to represent and implement data preprocessing workflows has special advantages, which include:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Seamless Data Flow Integration between different sources.</p><p>This is the first but very important step.</p></li><li style="list-style-type: disc"><p>Availability of data processing libraries MLlib and GraphX.</p><p>As we can note from the previous sections, the libraries built on MLIB and GraphX make data cleaning easy.</p></li><li style="list-style-type: disc"><p>Avoiding slow offline Table Joins.</p><p>Spark SQL is faster than SQL.</p></li><li style="list-style-type: disc"><p>The significantly quicker execution of operations that could be naturally parallelized.</p><p>Parallelized computation is what is naturally offered by Apache Spark; also, optimization is another advantage offered by Spark.</p></li></ul></div><p>The<a id="id156" class="indexterm"></a> Spark pipeline API makes it especially easy to<a id="id157" class="indexterm"></a> develop and deploy data cleaning and data preprocessing workflows.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec39"></a>Spark pipelines for dataset preprocessing</h3></div></div></div><p>As an example, SampleClean was used as one of the systems for data preprocessing—specifically<a id="id158" class="indexterm"></a> for the work of cleaning and entity<a id="id159" class="indexterm"></a> analytics.</p><p>For learning purposes, we encourage users to combine SampleClearn with the R notebook and then utilize Apache Spark Pipeline to organize workflows.</p><p>As discussed in previous sections, to complete a data preprocessing and make it available, we need at least the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Data cleaning to deal with missing cases.</p></li><li><p>Entity analytics to resolve entity problems.</p></li><li><p>Reorganizing data to cover subsetting and aggregating data.</p></li><li><p>Joining some data together.</p></li><li><p>Developing new features from the existing features.</p></li></ol></div><p>For some of the most basic preprocessing, we may be able to organize the workflow with a few R codes, including the following:</p><div class="informalexample"><pre class="programlisting">df$var[is.na(df$var)] &lt;- mean(df$var, na.rm = TRUE)</pre></div><p>We will then use the R functions, <code class="literal">subset</code>, <code class="literal">aggregate</code>, and <code class="literal">merge</code>, to reorganizing and join datasets.</p><p>The preceding R work on the R Notebook in combination with SampleClean and feature development should complete our workflow.</p><p>However, in reality, the preprocessing workflows can be a lot more complicated and may involve feedbacks as well.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec40"></a>Dataset preprocessing automation</h3></div></div></div><p>Spark's new<a id="id160" class="indexterm"></a> pipelines are good to represent workflows.</p><p>Once all the data preprocessing steps get organized into workflows, automation becomes easy.</p><p>Databricks is an end-to-end solution to make building a data pipeline easier—from ingest to production. The same concept applies to R notebooks as well: You can schedule your R notebooks to run as jobs on existing or new Spark clusters. The results of each job run, including visualizations, are available to browse, making it much simpler and faster to turn the work of <a id="id161" class="indexterm"></a>data scientists into production.</p><div class="mediaobject"><img src="graphics/B04883_02_05.jpg" /></div><p>An important point here is that the data preparation will turn its outputs into DataFrames. Then, this can be easily combined with machine learning pipelines to automate all together.</p><p>For example, the most common advanced analytic tasks can be specified using the new pipeline API in MLlib. For example, the following code creates a simple text classification pipeline consisting of a tokenizer, a hashing term frequency feature extractor, and logistic regression:</p><div class="informalexample"><pre class="programlisting">tokenizer = Tokenizer(inputCol="text", outputCol="words")
hashingTF = HashingTF(inputCol="words", outputCol="features")
lr = LogisticRegression(maxIter=10, regParam=0.01)
pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])</pre></div><p>Once the pipeline is set up, we can use it to train on a DataFrame directly:</p><div class="informalexample"><pre class="programlisting">df = context.load("/path/to/data")
model = pipeline.fit(df)</pre></div><p>For the preceding code, we will discuss more in later chapters.</p><p>As we may recall, in section <span class="emphasis"><em>Data cleaning made easy</em></span>, we had four tables for the purposes of illustration, as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">Users(userId INT, name String, email STRING,age INT, latitude: DOUBLE, longitude: DOUBLE,subscribed: BOOLEAN)</code>
</p></li><li style="list-style-type: disc"><p>
<code class="literal">Events(userId INT, action INT, Default)</code>
</p></li><li style="list-style-type: disc"><p>
<code class="literal">WebLog(userId, webAction)</code>
</p></li><li style="list-style-type: disc"><p>
<code class="literal">Demographic(memberId, age, edu, income)</code>
</p></li></ul></div><p>For this<a id="id162" class="indexterm"></a> group of datasets, we performed:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Data cleaning.</p></li><li><p>Identity matching.</p></li><li><p>Datasets reorganizing.</p></li><li><p>Datasets joining.</p></li><li><p>Feature extraction, then data joining, and then feature selection.</p></li></ol></div><p>To implement the preceding, we can use an R notebook to organize them into a workflow for automation and also Spark Pipeline for help.</p><p>With all the preceding completed, we are now ready for machine learning.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec24"></a>Summary</h2></div></div><hr /></div><p>Machine learning professionals and data scientists often spend 80% or more of their time on data preparation, which makes data preparation the most important task to perform even though it could be the most boiling task.</p><p>In this chapter, after discussing locating datasets and loading them into Apache Spark, we covered the methods of completing the six critical data preparation tasks, which include:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Treating dirty data with a focus on missing cases</p></li><li style="list-style-type: disc"><p>Resolving entity problems to match datasets</p></li><li style="list-style-type: disc"><p>Reorganizing datasets, with creating subsets and aggregating data as examples</p></li><li style="list-style-type: disc"><p>Joining tables together</p></li><li style="list-style-type: disc"><p>Developing features</p></li><li style="list-style-type: disc"><p>Organizing data preparation workflows and automating them</p></li></ul></div><p>In covering these, we studied the Spark SQL and R as two primary tools in combination with some special Spark packages, such as SampleClean, and some R packages, such as <code class="literal">reshape</code>. We also explored ways of making data preparation easy and fast.</p><p>After this chapter, we should master all the necessary data preparation methods plus a few advanced methods and become capable of cleaning datasets, such as the four used as examples in this chapter. From now on, we should be able to complete data preparation tasks fast with a workflow approach and be ready for practical machine learning tasks.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch03"></a>Chapter 3. A Holistic View on Spark</h2></div></div></div><p>After setting the Apache Spark system up as per <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Spark for Machine Learning</em></span>, and completing our data preparation work according to what we discussed in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Data Preparation for Spark ML</em></span>, we will now move to a new stage of utilizing Apache Spark-based systems to turn data into insight.</p><p>According to the research done by Gartner and others, many organizations lost a large amount of value simply due to the lack of a holistic view of their business. In this chapter, we will review machine learning methods and processes of obtaining a holistic view of business. Then, we will discuss how Apache Spark fits in to making the related computing easy and fast and, at the same time, illustrate this process of developing holistic views from data using Apache Spark computing with one real-life example step by step.</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Spark for a holistic view</p></li><li style="list-style-type: disc"><p>Methods for a holistic view</p></li><li style="list-style-type: disc"><p>Feature preparation</p></li><li style="list-style-type: disc"><p>Model estimation</p></li><li style="list-style-type: disc"><p>Model evaluation</p></li><li style="list-style-type: disc"><p>Results explanation</p></li><li style="list-style-type: disc"><p>Deployment</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec25"></a>Spark for a holistic view</h2></div></div><hr /></div><p>Spark is <a id="id163" class="indexterm"></a>very suitable <a id="id164" class="indexterm"></a>for machine learning projects, such as obtaining a holistic view of business, as it enables us to process huge amounts of data fast and code complicated computations easily. In this section, we will first describe a real business case and then how to prepare Spark computing for our project.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec41"></a>The use case</h3></div></div></div><p>The<a id="id165" class="indexterm"></a> company IFS sells and distributes thousands of IT products and has a lot of data on marketing, training, team management, promotion, and products. The company wants to understand how various kinds of actions, such as those in marketing and training, affect the sales team's success. In other words, IFS is interested in finding out how much impact marketing, training, or promotion generates separately.</p><p>In the past, IFS has done a lot of analytical work, but all of it was completed by individual departments on soloed datasets. That is, they have analytical results about how marketing affects sales from using marketing data alone and how training affects sales from analyzing training data alone.</p><p>When the decision makers collect all the results together and prepare to make use of them, they find that some of the results contradict each other. For example, when they add all the effects together, the total impact is beyond what is intuitively imagined.</p><p>This is a typical problem every organization faces. A soloed approach with soloed data will produce not only an incomplete view, but also often a biased view or even conflicting views. To solve this problem, the analytical team needs to take a holistic view of all the company data and gather it in one place and then utilize new machine learning approaches to gain a holistic view of the business.</p><p>To do so, companies also need to care for:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The completeness of causes</p></li><li style="list-style-type: disc"><p>Advanced analytics to account for the complexity of relationships</p></li><li style="list-style-type: disc"><p>Computing complexity related to subgroups and a large number of products or services</p></li></ul></div><p>For this example, we have eight datasets that include one for marketing with 48 features, one for training with 56 features, and one data set for team administration with 73 features; the following table is a complete summary:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Category</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Number of Features</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>Team</p>
</td><td style="" align="left" valign="top">
<p>73</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Marketing</p>
</td><td style="" align="left" valign="top">
<p>48</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Training</p>
</td><td style="" align="left" valign="top">
<p>56</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Staffing</p>
</td><td style="" align="left" valign="top">
<p>103</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Product</p>
</td><td style="" align="left" valign="top">
<p>77</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Promotion</p>
</td><td style="" align="left" valign="top">
<p>43</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Total</p>
</td><td style="" align="left" valign="top">
<p>400</p>
</td></tr></tbody></table></div><p>In this company, researchers understood that pooling all the datasets together and building a complete <a id="id166" class="indexterm"></a>model was the solution, but they were not able to achieve it for several reasons. Besides organizational issues inside the corporation, the tech capability to store all the data, process it quickly with the right methods, and present all the results in the right ways with reasonable speed are other challenges.</p><p>At the same time, the company has more than 100 products to offer, for which data was pooled together to study the impacts of company interventions. That is, calculated impacts are average impacts, but variations among products are too large to ignore. If we need to assess the impact of each product, parallel computing is preferred and needs to be implemented at good speed. Without utilizing a good computing platform such as Apache Spark, meeting the requirements described previously is a big challenge for this company.</p><p>In the sections that follow, we will use modern machine learning over Apache Spark to attack this business use case and help the company gain a holistic view of their business. In order to help you learn machine learning in Spark effectively, discussions in the following sections are all based on work about this real business use case. However, we left some details out to protect the company's privacy and also to keep everything brief.</p><p>As discussed in the previous section, parallel computing is needed for our project; for this, we should set up clusters and worker notes. Then we can use the driver program and cluster manager to manage the computing to be done in each worker node.</p><p>We discussed preparing Spark in <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Spark for Machine Learning</em></span>, and for more information you may refer to <a class="ulink" href="http://spark.apache.org/docs/latest/configuration.html" target="_blank">http://spark.apache.org/docs/latest/configuration.html</a>.</p><p>As an example, assume that we choose to work within the Databricks environment; then we can perform the following steps to set up the clusters:</p><div class="mediaobject"><img src="graphics/B04883_03_01.jpg" /></div><p>Go to the <a id="id167" class="indexterm"></a>preceding main menu, click on <span class="strong"><strong>Clusters</strong></span> and then a window will open for users to create a name for the cluster. Here, select a version of Spark and then specify the number of workers.</p><p>Once clusters are created, we can go to the preceding illustrated main menu, click on the down arrow on the right-hand side of <span class="strong"><strong>Tables</strong></span>, and then select <span class="strong"><strong>Create Tables</strong></span> to import our datasets that are cleaned and prepared as per the <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Data Preparation for Spark ML</em></span>, discussion, as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/B04883_03_02.jpg" /></div><p>For the data<a id="id168" class="indexterm"></a> source, the options include S3, DBFS, JDBC, and Files (for local fields). As described in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Data Preparation for Spark ML</em></span>, our data is separated into two subsets—one for training and one to test for each product—as we need to train a few models per product.</p><p>In Apache Spark, we need to direct workers to complete the computation on each node; for this, on Databricks, we will use a scheduler to get the Notebook computation completed and collect the results back, which will be discussed in the <span class="emphasis"><em>Model estimation</em></span> section.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec42"></a>Fast and easy computing</h3></div></div></div><p>One <a id="id169" class="indexterm"></a>of the most important advantages of utilizing Apache Spark is to make coding easy, for which several approaches are available.</p><p>Here, for this project, we will focus our efforts on the notebook approach; specifically, we will use R notebooks to develop and organize code. At the same time, with an effort to illustrate the Spark technology more thoroughly, we will also use MLlib directly to code some of our needed algorithms as MLlib is seamlessly integrated with Spark.</p><p>In the Databricks environment, setting up notebooks requires the following steps:</p><div class="mediaobject"><img src="graphics/B04883_03_03.jpg" /></div><p>As shown in the <a id="id170" class="indexterm"></a>preceding screenshot, users can go to the Databricks main menu, click on the down arrow on the right-hand side of <span class="strong"><strong>Workspace</strong></span>, and select <span class="strong"><strong>Create</strong></span> -&gt; <span class="strong"><strong>Notebook</strong></span> to create a new notebook. For this, a table will pop up for users to create a name and also select a language (R, Python, Scala, or SQL).</p><p>In order to make our work repeatable and also easy to be understood, we will adopt a workflow approach consistent with the RM4Es framework described in <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Spark for Machine Learning</em></span>. Also, we will adopt Spark's ML Pipeline tools to represent our workflows whenever possible. Specifically, for the training dataset, we need to estimate the models, evaluate them, and then maybe reestimate them before we can finalize them. So we need to use Spark's Transformer, Estimator, and Evaluator to organize an ML pipeline for this project. In practice, we can also organize these workflows within the R notebook environment.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note20"></a>Note</h3><p>For more information about pipeline programming, go to <a class="ulink" href="http://spark.apache.org/docs/latest/ml-guide.html#example-pipeline" target="_blank">http://spark.apache.org/docs/latest/ml-guide.html#example-pipeline</a> and <a class="ulink" href="http://spark.apache.org/docs/latest/ml-guide.html" target="_blank">http://spark.apache.org/docs/latest/ml-guide.html</a>.</p></div><p>Once our computing platform gets set up and our framework is made clear, everything becomes clear too. In the following sections, we will move forward step by step. That is, we will use our RM4Es framework and related processes discussed in <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Spark for Machine Learning</em></span>, first to identify equations or methods and prepare features, second to complete model <a id="id171" class="indexterm"></a>estimations, third to evaluate models, fourth to explain our results, and finally to deploy the models.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec26"></a>Methods for a holistic view</h2></div></div><hr /></div><p>As discussed <a id="id172" class="indexterm"></a>in the previous section, in this section, we need to select our analytical methods or models (equations) to complete the task of mapping our business use case to machine learning methods.</p><p>To assess the <a id="id173" class="indexterm"></a>impact of various factors on the sales team's success, there are many suitable models for us to use. As an exercise, we will select (a) regression models, (b) structural equation models, and (c) decision trees, mainly for their ease of interpretation as well as their implementablility on Spark.</p><p>Once we finalize our decision for analytical methods or models, we will need to prepare the dependent variable and also prepare for coding; we will discuss these one by one in the following section.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec43"></a>Regression modeling</h3></div></div></div><p>To get ready<a id="id174" class="indexterm"></a> for regression modeling on Spark, there are three issues for us to take care of:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Linear regression or logistic regression.</p><p>Regression is the most mature and also the most widely used model to represent the impact of various factors on one dependent variable. Whether to use linear regression or logistic regression depends on whether the relationship is linear or not. Here, we are not sure, so we will adopt both and then compare their results to decide on which to deploy.</p></li><li style="list-style-type: disc"><p>Preparing the dependent variable.</p><p>In order to use logistic regression, we need to recode the target variable or dependent variable (the sales team's success variable, now with a rating from 0 to 100) to be 0 versus 1 by separating it with the medium value.</p></li><li style="list-style-type: disc"><p>Preparing coding.</p><p>In MLlib, we can use the following code for regression modeling, as we will use Spark MLlib's <span class="strong"><strong>Linear Regression with Stochastic Gradient Descent</strong></span> (<span class="strong"><strong>LinearRegressionWithSGD</strong></span>):</p><div class="informalexample"><pre class="programlisting">val numIterations = 90
val model = LinearRegressionWithSGD.train(TrainingData, numIterations)</pre></div><p>For Logistic regression, we use the following code:</p><div class="informalexample"><pre class="programlisting">val model = new LogisticRegressionWithSGD()
  .setNumClasses(2)
  .run(training)</pre></div></li></ul></div><p>For more information <a id="id175" class="indexterm"></a>about using MLlib for regression modeling, you can go to:</p><p>
<a class="ulink" href="http://spark.apache.org/docs/latest/mllib-linear-methods.html#linear-least-squares-lasso-and-ridge-regression" target="_blank">http://spark.apache.org/docs/latest/mllib-linear-methods.html#linear-least-squares-lasso-and-ridge-regression</a>.</p><p>In R, we can use the <code class="literal">lm</code> function for linear regression, and the <code class="literal">glm</code> function for logistic regression with <code class="literal">family=binomial()</code>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec44"></a>The SEM approach</h3></div></div></div><p>To get ready <a id="id176" class="indexterm"></a>for <a id="id177" class="indexterm"></a>
<span class="strong"><strong>Structural Equation Modeling</strong></span> (<span class="strong"><strong>SEM</strong></span>) on Spark, there are also three issues for us to take care of:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>SEM introduction specification.</p><p>SEM may be considered an extension of regression modeling as it consists of several linear equations similar to regression equations. However, this method estimates all the equations at the same time with regard to their internal relations, so it is less biased than regression modeling. SEM consists of both structural modeling and latent variable modeling; however, we will only use structural modeling.</p></li><li style="list-style-type: disc"><p>Preparing the dependent variable.</p><p>We can just use the sales team's success scale (with a rating of 0 to 100) as our target variable here.</p></li><li style="list-style-type: disc"><p>Preparing the coding.</p><p>We will adopt R notebook within the Databricks environment, for which we should use the <code class="literal">SEM</code> R package. There are also other SEM packages, such as <code class="literal">lavaan</code>, available for use; however, for this project, we will use the <code class="literal">sem</code> package for its ease of learning.</p><p>To load an <code class="literal">SEM</code> package into the R notebook, we will use <code class="literal">install.packages("sem", repos="http://R-Forge.R-project.org")</code>. Then, we need to execute the R code, <code class="literal">library(sem)</code>.</p><p>After this, we <a id="id178" class="indexterm"></a>need to use the <code class="literal">specify.model() </code>function to write some code to specify the <a id="id179" class="indexterm"></a>models into our R notebook, for which the following code is needed:</p><div class="informalexample"><pre class="programlisting">mod.no1 &lt;- specifyModel()
s1 &lt;- x1, gam31
s1 &lt;- x2, gam32</pre></div></li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec45"></a>Decision trees</h3></div></div></div><p>To get <a id="id180" class="indexterm"></a>ready for the decision tree modeling on Spark, there are again three issues for us to take care of:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Decision tree selection</p><p>Decision trees aim to model classifying cases, which is about classifying them into successful or not successful for our use case in sequence. It is also one of the most mature and widely used methods. It could even lead to overfitting, which requires methods such as afterward regularization. For this exercise, we will only use the simple linear decision tree and not venture into any more complicated trees such as random forests.</p></li><li style="list-style-type: disc"><p>Preparing the dependent variable</p><p>To use the decision tree model here, we will separate the sales team ratings into two categories—SUCCESS and NOT—as we did for logistic regression.</p></li><li style="list-style-type: disc"><p>Preparing the coding</p><p>For MLlib, we can use the following code:</p><div class="informalexample"><pre class="programlisting">val numClasses = 2
val categoricalFeaturesInfo = Map[Int, Int]()
val impurity = "gini"
val maxDepth = 6
val maxBins = 32
val model = DecisionTree.trainClassifier(trainingData, numClasses, categoricalFeaturesInfo,
  impurity, maxDepth, maxBins)</pre></div></li></ul></div><p>For more <a id="id181" class="indexterm"></a>information on using MLlib for decision trees, go to <a class="ulink" href="http://spark.apache.org/docs/latest/mllib-decision-tree.html" target="_blank">http://spark.apache.org/docs/latest/mllib-decision-tree.html</a>.</p><p>As for the R notebook on Spark, we need to use the <code class="literal">rpart</code> R package and then the <code class="literal">rpart</code> functions for all the calculation. For <code class="literal">rpart</code>, we need to specify the classifier and also all the features to be used.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec27"></a>Feature preparation</h2></div></div><hr /></div><p>In the<a id="id182" class="indexterm"></a> previous section, we selected our models and also prepared our dependent variable for our supervised machine learning. In this section, we need to move forward to prepare our independent variables, which are all the features representing the factors impacting our dependent variable: the sales team success. Specifically, for this important work, we need to reduce our four hundred of features to a reasonable group for final modeling. For this, we will employ PCA, utilize some subject knowledge, and then perform some feature selection tasks.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec46"></a>PCA</h3></div></div></div><p>PCA <a id="id183" class="indexterm"></a>is a <a id="id184" class="indexterm"></a>very mature and also commonly used feature reduction method that is often used to find a small set of variables that counts for most of the variance. Technically, the goal of PCA is to find a low dimensional subspace that captures as much of the variance of a dataset as possible.</p><p>If you are using <a id="id185" class="indexterm"></a>MLlib, <a class="ulink" href="http://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html#principal-component-analysis-pca" target="_blank">http://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html#principal-component-analysis-pca</a> has a few example codes that users may adopt and modify to run<a id="id186" class="indexterm"></a> PCA on Spark. For more on MLlib, go to <a class="ulink" href="https://spark.apache.org/docs/1.2.1/mllib-dimensionality-reduction.html" target="_blank">https://spark.apache.org/docs/1.2.1/mllib-dimensionality-reduction.html</a>.</p><p>Here for this project, we will use R only for its richness of PCA algorithms. In R, there are at least five functions to compute PCAs, which are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">prcomp()</code> (stats)</p></li><li style="list-style-type: disc"><p>
<code class="literal">princomp()</code> (stats)</p></li><li style="list-style-type: disc"><p>
<code class="literal">PCA()</code> (FactoMineR)</p></li><li style="list-style-type: disc"><p>
<code class="literal">dudi.pca()</code> (ade4)</p></li><li style="list-style-type: disc"><p>
<code class="literal">acp()</code> (amap)</p></li></ul></div><p>The <code class="literal">prcomp</code> and <code class="literal">princomp</code> from the basic package stats are commonly used, and we also have good functions for the results summary and plots. Therefore, we will use these two functions.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec47"></a>Grouping by category to use subject knowledge</h3></div></div></div><p>As is always the<a id="id187" class="indexterm"></a> case, if some subject knowledge can be used the feature reduction results can be improved greatly.</p><p>For our example, data categories are good to start with. They are:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Marketing</p></li><li style="list-style-type: disc"><p>Training</p></li><li style="list-style-type: disc"><p>Promotion</p></li><li style="list-style-type: disc"><p>Team administration</p></li><li style="list-style-type: disc"><p>Staffing</p></li><li style="list-style-type: disc"><p>Products</p></li></ul></div><p>So, we will execute six PCA algorithms, one for each data category. For example, for the Team category, we need to run a PCA algorithm on 73 features or variables to identify factors or dimensions that can fully represent the information we have about TEAM. As for this exercise, we found two dimensions for the Team category's 73 features.</p><p>Also, for the Staffing category, we need to execute a PCA algorithm on 103 features or variables to identify the factors or dimensions that can fully represent the information we have about Staffing. As for this exercise, we also found two dimensions for the Staffing category's 103 features. Take a look at the following table:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Category</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Number of Factors</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Factor Names</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>Team</p>
</td><td style="" align="left" valign="top">
<p>2</p>
</td><td style="" align="left" valign="top">
<p>T1, T2</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Marketing</p>
</td><td style="" align="left" valign="top">
<p>3</p>
</td><td style="" align="left" valign="top">
<p>M1, M2, M3</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Training</p>
</td><td style="" align="left" valign="top">
<p>3</p>
</td><td style="" align="left" valign="top">
<p>Tr1, Tr2, Tr3</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Staffing</p>
</td><td style="" align="left" valign="top">
<p>2</p>
</td><td style="" align="left" valign="top">
<p>S1, S2</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Product</p>
</td><td style="" align="left" valign="top">
<p>4</p>
</td><td style="" align="left" valign="top">
<p>P1, P2, P3, P4</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Promotion</p>
</td><td style="" align="left" valign="top">
<p>3</p>
</td><td style="" align="left" valign="top">
<p>Pr1, Pr2, Pr3</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Total</p>
</td><td style="" align="left" valign="top">
<p> 17</p>
</td><td style="" align="left" valign="top"> </td></tr></tbody></table></div><p>At the end of this PCA exercise, we obtained two to four features for each category, as summarized in the preceding table.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec48"></a>Feature selection</h3></div></div></div><p>Feature selection<a id="id188" class="indexterm"></a> is often used to remove redundant or irrelevant features but is often used at least for the following reasons:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Making models easier to understand</p></li><li style="list-style-type: disc"><p>Creating fewer chances for overfitting</p></li><li style="list-style-type: disc"><p>Saving time and space for model estimation</p></li></ul></div><p>In MLlib, we can use the <code class="literal">ChiSqSelector</code> algorithm, as follows:</p><div class="informalexample"><pre class="programlisting">// Create ChiSqSelector that will select top 25 of 400 features
val selector = new ChiSqSelector(25)
// Create ChiSqSelector model (selecting features)
val transformer = selector.fit(TrainingData)</pre></div><p>In R, we can use some R packages to make computation easy. Among the available packages, <code class="literal">CARET</code> is one of the commonly used packages.</p><p>First, as an exercise, we performed feature selection on all the 400 features.</p><p>Then, we started with all the features selected from our PCA work. We also performed feature selection so that we could keep all of them.</p><p>Therefore, at the end, we have 17 features to use, which are as follows:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Features</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>T1, T2 for Team</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>M1, M2, M3 for Marketing</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Tr1, Tr2, Tr3 for Training</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>S1, S2 for Staffing</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>P1, P2, P3, P4 for Product</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Pr1, Pr2, Pr3 for Promotion</p>
</td></tr></tbody></table></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note21"></a>Note</h3><p>For more about feature selection on Spark, go to <a class="ulink" href="http://spark.apache.org/docs/latest/mllib-feature-extraction.html" target="_blank">http://spark.apache.org/docs/latest/mllib-feature-extraction.html</a>.</p></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec28"></a>Model estimation</h2></div></div><hr /></div><p>Once <a id="id189" class="indexterm"></a>feature sets get finalized in our last section, what follows is to estimate the parameters of the selected models, for which we can use either MLlib or R here, and we need to arrange the distributed computing.</p><p>To simplify, we can utilize Databricks' Job feature. Specifically, within the Databricks environment, we can go to <span class="strong"><strong>Jobs</strong></span> and then create jobs, as shown in the following image:</p><div class="mediaobject"><img src="graphics/B04883_03_04.jpg" /></div><p>Then, users can select notebooks to run, specify clusters, and schedule jobs. Once scheduled, users can also monitor the running and then collect the results.</p><p>In section, <span class="emphasis"><em>Methods for a holistic view</em></span>, we prepared some codes for each of the three models selected. Now, we need to modify them with the final set of features selected in the last section so as to create our final notebooks.</p><p>In other words, we have one dependent variable prepared and 17 features selected out from our PCA and feature selection work. Therefore, we need to insert all them into the codes developed in section II to finalize our notebook. Then we will use the Spark Job feature to get these notebooks implemented in a distributed way.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec49"></a>MLlib implementation</h3></div></div></div><p>First, we need to <a id="id190" class="indexterm"></a>prepare our data with the s1 dependent variable for linear regression and s2 dependent variable for logistic regression or decision tree. Then, we need to add the selected 17 features into them to form the datasets ready for our use.</p><p>For linear regression, we will use the following code:</p><div class="informalexample"><pre class="programlisting">val numIterations = 90
val model = LinearRegressionWithSGD.train(TrainingData, numIterations)</pre></div><p>For logistic regression, we will use the following code:</p><div class="informalexample"><pre class="programlisting">val model = new LogisticRegressionWithSGD()
  .setNumClasses(2)</pre></div><p>For decision <a id="id191" class="indexterm"></a>tree, we will use the following code:</p><div class="informalexample"><pre class="programlisting">val model = DecisionTree.trainClassifier(trainingData, numClasses, categoricalFeaturesInfo,
  impurity, maxDepth, maxBins)</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec50"></a>The R notebooks' implementation</h3></div></div></div><p>For better <a id="id192" class="indexterm"></a>comparison, it is a good idea to write linear regression and SEM into an R notebook and also write logistic regression and decision tree into the same R notebook.</p><p>Then, the main task left here is to schedule the estimation for each worker and then collect the results using the JOB feature mentioned before in the Databricks environment.</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>For linear regression and SEM, execute the following code:</p><div class="informalexample"><pre class="programlisting">lm.est1 &lt;- lm(s1 ~ T1+T2+M1+ M2+ M3+ Tr1+ Tr2+ Tr3+ S1+ S2+ P1+ P2+ P3+ P4+ Pr1+ Pr2+ Pr3)
mod.no1 &lt;- specifyModel()
s1 &lt;- x1, gam31
s1 &lt;- x2, gam32</pre></div></li><li style="list-style-type: disc"><p>For logistic regression and decision tree, run the following script:</p><div class="informalexample"><pre class="programlisting">logit.est1 &lt;- glm(s2~ T1+T2+M1+ M2+ M3+ Tr1+ Tr2+ Tr3+ S1+ S2+ P1+ P2+ P3+ P4+ Pr1+ Pr2+ Pr3,family=binomial())

 dt.est1 &lt;- rpart(s2~ T1+T2+M1+ M2+ M3+ Tr1+ Tr2+ Tr3+ S1+ S2+ P1+ P2+ P3+ P4+ Pr1+ Pr2+ Pr3, method="class")</pre></div></li></ul></div><p>After we get all the models estimated as per each product, for simplicity, we will focus on one product to complete our discussion on model evaluation and deployment.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec29"></a>Model evaluation</h2></div></div><hr /></div><p>In the last <a id="id193" class="indexterm"></a>section, we completed our model estimation task. Now, it is time for us to evaluate the estimated models to see whether they meet our model quality criteria so that we can either move to our next stage for the results explanation or go back to some previous stages to refine our models.</p><p>To perform our model evaluation, in<a id="id194" class="indexterm"></a> this section, we will focus our effort on utilizing <span class="strong"><strong>RMSE</strong></span> (<span class="strong"><strong>Root-Mean-Square Error</strong></span>) and <span class="strong"><strong>ROC</strong></span> (<span class="strong"><strong>Receiver Operating Characteristic</strong></span>) curves<a id="id195" class="indexterm"></a> to assess the quality of fit for our models. To calculate RMSEs and ROC curves, we need to use our test data rather than training data used to estimate our models.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec51"></a>Quick evaluations</h3></div></div></div><p>Many packages <a id="id196" class="indexterm"></a>have already included some algorithms for users to assess models quickly. For example, both MLlib and R have algorithms to return confusion matrix for logistic regression models and even get false positive numbers calculated.</p><p>Specifically, MLlib has the <code class="literal">confusionMatrix</code> and <code class="literal">numFalseNegatives() </code>functions for us to use and even some algorithms to calculate MSE quickly, as follows:</p><div class="informalexample"><pre class="programlisting">MSE = valuesAndPreds.(lambda (v, p): (v - p)**2).mean()
print("Mean Squared Error = " + str(MSE))</pre></div><p>Also, R has the <code class="literal">confusion.matrix</code> function for us to use. In R, there are even many tools to produce some quick graphical plots that can be used to gain a quick evaluation of models.</p><p>For example, we can perform plots of predicted versus actual values and also residuals on predicted values.</p><p>Intuitively, the methods of comparing predicted versus actual values are easiest to understand and give us a quick model evaluation. The following is a calculated confusion matrix for one of the company products, which shows a reasonable fit of our model. Take a look at the following table:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Success or not</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Predicted as Success</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Predicted as NOT</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>Actual Success</p>
</td><td style="" align="left" valign="top">
<p>83%</p>
</td><td style="" align="left" valign="top">
<p>17%</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Actual Not</p>
</td><td style="" align="left" valign="top">
<p>9%</p>
</td><td style="" align="left" valign="top">
<p>91%</p>
</td></tr></tbody></table></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec52"></a>RMSE</h3></div></div></div><p>In MLlib, we <a id="id197" class="indexterm"></a>can<a id="id198" class="indexterm"></a> use the following codes to calculate RMSE:</p><div class="informalexample"><pre class="programlisting">val valuesAndPreds = test.map { point =&gt;
    val prediction = new_model.predict(point.features)
    val r = (point.label, prediction)
    r
    }
val residuals = valuesAndPreds.map {case (v, p) =&gt; math.pow((v - p), 2)}
val MSE = residuals.mean();
val RMSE = math.pow(MSE, 0.5)</pre></div><p>Besides <a id="id199" class="indexterm"></a>the preceding, MLlib also has some functions in the <code class="literal">RegressionMetrics</code> and <code class="literal">RankingMetrics</code> classes for us to use for RMSE calculation.</p><p>In R, we can compute RMSE, as follows:</p><div class="informalexample"><pre class="programlisting">RMSE &lt;- sqrt(mean((y-y_pred)^2))</pre></div><p>Before this, we need to obtain the predicted values with the following commands:</p><div class="informalexample"><pre class="programlisting">&gt; # build a model &gt; RMSElinreg &lt;- lm(s1 ~ . ,data= data1) &gt; &gt; #score the model &gt; score &lt;- predict(RMSElinreg, data2</pre></div><p>After obtaining the RMSE values for all the estimated models, we will compare them to evaluate the linear regression model with the logistic regression model and the decision tree model. For our case, the linear regression model turned out to be the best.</p><p>Then we will also compare the RMSE values across products and send back some product models for refinement.</p><p>For another <a id="id200" class="indexterm"></a>example of obtaining RMSE, go to <a class="ulink" href="http://www.cakesolutions.net/teamblogs/spark-mllib-linear-regression-example-and-vocabulary" target="_blank">http://www.cakesolutions.net/teamblogs/spark-mllib-linear-regression-example-and-vocabulary</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec53"></a>ROC curves</h3></div></div></div><p>As an example, we <a id="id201" class="indexterm"></a>will calculate ROC curves to assess our logistic models.</p><p>In MLlib, we can use the MLlib function <code class="literal">metrics.areaUnderROC()</code> to calculate ROC once we apply our estimated model to our test data and get labels to test cases.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note22"></a>Note</h3><p>For more on using MLlib to obtain ROC, go to <a class="ulink" href="http://web.cs.ucla.edu/~mtgarip/linear.html" target="_blank">http://web.cs.ucla.edu/~mtgarip/linear.html</a>.</p></div><p>In R, using the <code class="literal">pROC</code> package, we can run the following to calculate and plot the ROC curves:</p><div class="informalexample"><pre class="programlisting">mylogit &lt;- glm(s2 ~ ., family = "binomial")
summary(mylogit)
prob=predict(mylogit,type=c("response"))
testdata1$prob=prob
library(pROC)
g &lt;- roc(s2 ~ prob, data = testdata1)
plot(g)</pre></div><p>As discussed, once the ROC curves get calculated, we can use them to compare our logistic models <a id="id202" class="indexterm"></a>against decision tree models or compare models across products. For our case, logistic models perform better than decision tree models:</p><div class="mediaobject"><img src="graphics/B04883_03_05.jpg" /></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec30"></a>Results explanation</h2></div></div><hr /></div><p>Once we pass <a id="id203" class="indexterm"></a>our model evaluation and decide to select the estimated model as our final model, we need to interpret the results to the company executives and also their technicians.</p><p>In the following, we discuss some commonly used ways of interpreting our results, one using tables and another using graphs, with our focus on impact assessment.</p><p>Some users may prefer to interpret our results in terms of ROIs, for which the cost and benefit data is needed. Once we have the cost and benefit data, our results here can be easily expanded to cover the ROI issues. Also, some optimization may need to be applied for real decision making.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec54"></a>Impact assessments</h3></div></div></div><p>As discussed <a id="id204" class="indexterm"></a>in section, <span class="emphasis"><em>Spark for a holistic view</em></span>, the main purpose of this project is to gain a holistic view of sales team success. For example, the company wishes to understand the impact of marketing on sales success in comparison to training and other factors.</p><p>As we have our linear regression model estimated, one easy way of comparing impacts is to summarize <a id="id205" class="indexterm"></a>the variance explained by each feature group using ANOVA, with the results as shown by the following table:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Feature Group</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>% </p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>Team</p>
</td><td style="" align="left" valign="top">
<p>8.5</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Marketing</p>
</td><td style="" align="left" valign="top">
<p>7.6</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Training</p>
</td><td style="" align="left" valign="top">
<p> 5.7</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Staffing</p>
</td><td style="" align="left" valign="top">
<p>12.9</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Product</p>
</td><td style="" align="left" valign="top">
<p> 8.9</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Promotion</p>
</td><td style="" align="left" valign="top">
<p>14.6</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Total </p>
</td><td style="" align="left" valign="top">
<p>58.2</p>
</td></tr></tbody></table></div><p>Also, the following image is another example of using graphs to display the results discussed:</p><div class="mediaobject"><img src="graphics/B04883_03_06.jpg" /></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec31"></a>Deployment</h2></div></div><hr /></div><p>Some users may <a id="id206" class="indexterm"></a>have some deployment systems in place already for which exporting the developed models to users' desired forms could be good enough.</p><p>For linear regression and logistic regression, MLlib supports model exporting to <a id="id207" class="indexterm"></a>
<span class="strong"><strong>Predictive Model Markup Language</strong></span> (<span class="strong"><strong>PMML</strong></span>).</p><p>For more information about exporting to PMML from MLlib, visit <a class="ulink" href="https://spark.apache.org/docs/latest/mllib-pmml-model-export.html" target="_blank">https://spark.apache.org/docs/latest/mllib-pmml-model-export.html</a>.</p><p>For the R notebook, it can be run on another environment directly. Also, with the R package PMML, R models can be exported.</p><p>For more information on the <a id="id208" class="indexterm"></a>R package PMML, go to <a class="ulink" href="http://journal.r-project.org/archive/2009-1/RJournal_2009-1_Guazzelli+et+al.pdf" target="_blank">http://journal.r-project.org/archive/2009-1/RJournal_2009-1_Guazzelli+et+al.pdf</a>.</p><p>It is also possible to deploy the models for decision making directly on Apache Spark and make the results easily available to users.</p><p>Two commonly used methods of deploying results are (1) dashboard and (2) rule-based decision making. Which one to select depends on who we will supply our result to.</p><p>Here, we will discuss them only briefly as a full deployment for decision making will need optimization that is not covered in this chapter. In later chapters, we will spend a little more time on deployment for readers to learn more.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec55"></a>Dashboard</h3></div></div></div><p>For real-time <a id="id209" class="indexterm"></a>analytical dashboard, most users use Spark Streaming together with other tools.</p><p>For our work here, we will take an easy dashboard approach, which is to use graphs and tables to quickly present our analytical results to consumers. All the dashboards are interactive as every plot depends on one or more features. When these features get updated, the algorithms behind each plot can be automatically reexecuted, and the plot will be regenerated.</p><p>Starting from our R notebooks, we can use the <code class="literal">shiny</code> and <code class="literal">shinydashboard</code> R packages to quickly build a dashboard.</p><p>For more information about using the <code class="literal">shinydashboard</code> package, go to <a class="ulink" href="https://rstudio.github.io/shinydashboard/" target="_blank">https://rstudio.github.io/shinydashboard/</a>.</p><p>Databricks' new version also has a dashboard builder. To use it, just go to <span class="strong"><strong>Workspace</strong></span> -&gt; <span class="strong"><strong>Create</strong></span> -&gt; <span class="strong"><strong>Dashboard</strong></span>.</p><p>This Databricks dashboard builder is very powerful and intuitive. Once built, users can then publish a <a id="id210" class="indexterm"></a>dashboard just with the click of a button to other employees in the organization or to their customers.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec56"></a>Rules</h3></div></div></div><p>To turn all the <a id="id211" class="indexterm"></a>modeling results into rules is easy as many tools are available. Especially for R results, there are several tools to help extract rules from developed predictive models.</p><p>For decision tree models, we should use the <code class="literal">rpart.utils</code> R package, which can extract rules and export them in various formats, including RODBC.</p><p>For more information about the <code class="literal">rpart.utils</code> R package, go to <a class="ulink" href="https://cran.r-project.org/web/packages/rpart.utils/rpart.utils.pdf" target="_blank">https://cran.r-project.org/web/packages/rpart.utils/rpart.utils.pdf</a>.</p><p>For a discussion on extracting rules from MLlib, go to:</p><p>
<a class="ulink" href="http://stackoverflow.com/questions/31782288/how-to-extract-rules-from-decision-tree-spark-mllib" target="_blank">http://stackoverflow.com/questions/31782288/how-to-extract-rules-from-decision-tree-spark-mllib</a>.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec32"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we went through a step-by-step process from data to a holistic view of business, from which we processed a large amount of data on Spark and then built a model to produce a holistic view of the sales team's success for the IFS company.</p><p>Specifically, we first selected models as per business needs after we prepared Spark computing and loaded in preprocessed data. Second, we prepared and reduced features. Third, we estimated model coefficients. Fourth, we evaluated the estimated models. Then, we interpreted the analytical results. And finally, we deployed our estimated models.</p><p>The preceding process is similar to the process of working with small data. However, in dealing with big data, we need parallel computing, for which Apache Spark is utilized. Also, during the previously described process, Apache Spark makes things easy and fast.</p><p>After this chapter, readers will have gained a full understanding of how Apache Spark can be utilized to make our work easier and faster in obtaining a holistic view of business. At the same time, readers should become familiar with the RM4Es modeling processes of processing large amounts of data and developing predictive models and especially become capable of producing their own holistic view of business.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch04"></a>Chapter 4. Fraud Detection on Spark</h2></div></div></div><p>In <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Spark for Machine Learning</em></span>, we discussed how to get the Apache Spark system ready, and in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Data Preparation for Spark ML</em></span>, we listed detailed instructions for data preparation. Now, in chapters 4 to 6, we will move to a new stage of utilizing Apache Spark-based systems to turn data into insights for some specific projects, which is fraud detection for this chapter; risk modeling for <a class="link" href="#" linkend="ch05">Chapter 5</a>, <span class="emphasis"><em>Risk Scoring on Spark</em></span>; and churn prediction for <a class="link" href="#" linkend="ch06">Chapter 6</a>, <span class="emphasis"><em>Churn Prediction on Spark</em></span>.</p><p>Specifically, in this chapter, we will review machine learning methods and analytical processes for a fraud detection project, and also discuss how Apache Spark makes them easy and fast. At the same time, with a real-life fraud detection example, we will illustrate our step-by-step process of obtaining fraud insight from big data.</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Spark for fraud detection</p></li><li style="list-style-type: disc"><p>Methods of fraud detection</p></li><li style="list-style-type: disc"><p>Feature preparation</p></li><li style="list-style-type: disc"><p>Model estimation</p></li><li style="list-style-type: disc"><p>Model evaluation</p></li><li style="list-style-type: disc"><p>Result explanation</p></li><li style="list-style-type: disc"><p>Deploying fraud detection</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec33"></a>Spark for fraud detection</h2></div></div><hr /></div><p>In this section, we will <a id="id212" class="indexterm"></a>start with a real business case of fraud detection to further illustrate our step-by-step machine learning process and then describe how to prepare Spark for this fraud detection project.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec57"></a>The use case</h3></div></div></div><p>The ABC Corporation is a billion-dollar company that processes payments for thousands of clients in many industries, including real estate and vacation travel. Many kinds of frauds happened <a id="id213" class="indexterm"></a>to this company and cost a lot. Most of the frauds happened online.</p><p>In order to prevent frauds from happening, the company collected a lot of data on its clients relating to payment processing transactions and also about past online activities for each client. Also, the company purchased a lot of data from third parties about the computer devices and bank accounts their clients use.</p><p>As for this project, our unit of analysis can be an individual company or person (ABC's client). Our unit of analysis can also be a payment transaction. In real practice, we performed modeling on both. However, as for the practice here, we will focus on analytics and transactions. Therefore, in terms of data and features, for each transaction online, we have web log data, data about its owner/user, and also data on the computer devices and bank accounts used.</p><p>In practice, the ABC company hopes to quickly score each transaction as per the likelihood of fraud and hopes to immediately stop a transaction if it is highly suspicious. Also, the company hoped to identify suspicious clients before the company approved them. In other words, the company needs to utilize the fraud detection systems for underwriting as well as for real-time transaction monitoring. As for this exercise, we will focus on scoring transactions with a suspicious score or fraud likelihood score and use this score to monitor all the transactions so the ABC corporation can take actions to stop potential frauds.</p><p>To sum up, for this project we have a target variable of fraud and web log data for each transaction, plus account, computing device, and user data.</p><p>Through some preliminary analysis, the company understands some of its data challenges, as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Data is not ready to use; the web log data especially needs to be extracted into features ready for modeling</p></li><li style="list-style-type: disc"><p>There are many kinds of fraud cases per payment transaction service with very different behaviors</p></li><li style="list-style-type: disc"><p>Less information exists for some new and less active clients</p></li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec58"></a>Distributed computing</h3></div></div></div><p>Similarly to the<a id="id214" class="indexterm"></a> previous chapter, for our project, parallel<a id="id215" class="indexterm"></a> computing is needed due to the many kinds of frauds for which we should set up clusters and worker notes as before.</p><p>Let's assume we continue to work within the Databricks environment:</p><div class="mediaobject"><img src="graphics/B04883_04_01.jpg" /></div><p>Then, we will<a id="id216" class="indexterm"></a> need to go to the preceding main menu, click on<a id="id217" class="indexterm"></a> Clusters, then create a name for the cluster, select the newest version of Spark, and then specify the number of workers.</p><p>Once the clusters are created, we can go to the preceding illustrated main menu, click on the down arrow to the right of <span class="strong"><strong>Tables</strong></span>, and select <span class="strong"><strong>Create Tables</strong></span> to import all of our cleaned and prepared datasets, as per the instructions discussed in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Data Preparation for Spark ML</em></span>.</p><p>For this project, we will need to import a lot of web log data, structured data about the individual users or companies, the computer device used, and also on the bank accounts used.</p><p>As before, in Apache Spark, we need to direct workers to complete the computation on each note, for which we will use a scheduler on Databricks to get our R Notebook computation completed, and then collect results.</p><p>Also here, we will continue to take an R notebook approach.</p><p>In the Databricks environment, setting up notebooks will need us to go into the following menu:</p><div class="mediaobject"><img src="graphics/B04883_04_02.jpg" /></div><p>In the preceding<a id="id218" class="indexterm"></a> main menu, click on the down arrow to the<a id="id219" class="indexterm"></a> right of <span class="strong"><strong>Workspace</strong></span> and select <span class="strong"><strong>Create</strong></span> -&gt; <span class="strong"><strong>New Notebook</strong></span> to create a notebook.</p><p>If users do not want to use the R notebook provided by Databricks, one option is to use Zepperlin. To <a id="id220" class="indexterm"></a>build a free notebook on Spark using Zeppelin, go to:</p><p>
<a class="ulink" href="http://hortonworks.com/blog/introduction-to-data-science-with-apache-spark/" target="_blank">http://hortonworks.com/blog/introduction-to-data-science-with-apache-spark/</a>.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec34"></a>Methods for fraud detection</h2></div></div><hr /></div><p>In the previous section, we described our business use case and also prepared our Spark computing platform as well as our datasets. In this section, we need to select our analytical methods or predictive models (equations) for this fraud detection project, which is to<a id="id221" class="indexterm"></a> complete a task of mapping our business use case to machine learning methods.</p><p>For fraud detection, both supervised machine learning and unsupervised machine learning are commonly used. However, for this case, we will perform a supervised machine learning because we do have good data for our target variable of fraud and also because our practical goal is to reduce frauds while continuing business transactions.</p><p>To model and predict frauds, there are many suitable models, including logistic regression and the decision tree. Selecting one among them can sometimes become extremely difficult as it depends on the data to be used. One solution is to first run all the models and then select the best ones using model evaluation indices. As in many situations, after applying evaluation methods, there may be no one best model but many best models. In<a id="id222" class="indexterm"></a> this case, we will ensemble all of them to improve our model's performance.</p><p>To adopt the preceding mentioned strategy, we will need to develop a few models of neural network, logistic regression, SVM, and decision trees. However, for this exercise, we will focus our effort on Random forest and decision trees, as to demonstrate machine learning on Apache Spark and also to demonstrate their usefulness to meet the special needs of this use case.</p><p>As always, once we finalize our decision for analytical methods or models, we will need to prepare the related dependent variable and also prepare for coding.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec59"></a>Random forest</h3></div></div></div><p>
<span class="strong"><strong>Random forest</strong></span> is a<a id="id223" class="indexterm"></a> quite popular machine learning method<a id="id224" class="indexterm"></a> because its interpretation is very intuitive, and it usually leads to good results. There are many algorithms developed in R, Java, and others to get Random forest implemented, so the preparation is relatively easy.</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Random fores</strong></span>t: Random forest is an ensemble learning method for classification and regression that builds hundreds or even more decision trees at the training stage and then combines their output for the final prediction.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Preparing the dependent variable</strong></span>: To use Random forest, we need to recode the variable to be 0 versus 1 by transforming FRAUD to 1 and NOT FRAUD to 0.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Preparing coding</strong></span>: In MLlib, we can use the following codes for Random forest:</p><div class="informalexample"><pre class="programlisting">// To train a RandomForest model.
val treeStrategy = Strategy.defaultStrategy("Classification")
val numTrees = 300 
val featureSubsetStrategy = "auto" // Let the algorithm choose.
val model = RandomForest.trainClassifier(trainingData,
  treeStrategy, numTrees, featureSubsetStrategy, seed = 12345)</pre></div></li></ul></div><p>For a good <a id="id225" class="indexterm"></a>example of using MLlib for Random forest, go to <a class="ulink" href="https://databricks.com/blog/2015/01/21/random-forests-and-boosting-in-mllib.html" target="_blank">https://databricks.com/blog/2015/01/21/random-forests-and-boosting-in-mllib.html</a>.</p><p>In R, we need to use the R <a id="id226" class="indexterm"></a>package Random forest.</p><p>For a <a id="id227" class="indexterm"></a>good example of running random forest on Spark, go to <a class="ulink" href="https://spark-summit.org/2014/wp-content/uploads/2014/07/Sequoia-Forest-Random-Forest-of-Humongous-Trees-Sung-Chung.pdf" target="_blank">https://spark-summit.org/2014/wp-content/uploads/2014/07/Sequoia-Forest-Random-Forest-of-Humongous-Trees-Sung-Chung.pdf</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec60"></a>Decision trees</h3></div></div></div><p>Random forest comes from a set of trees with good functions to produce scores and rank independent <a id="id228" class="indexterm"></a>variables by their impact on a target variable.</p><p>However, the <a id="id229" class="indexterm"></a>mean results of hundreds of trees somehow cover details so that a decision tree explanation can still be very intuitive and valuable, as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Decision tree introduction</strong></span>: Decision tree aims to model classifying cases, which is about classifying into fraud or not fraud for our use case, in sequence</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Prepare dependent variable</strong></span>: Our target variable has already been coded as fraud or not, which is ready for our machine learning</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Prepare coding</strong></span>: As before, within MLlib, we can use the following codes:</p><div class="informalexample"><pre class="programlisting">val numClasses = 2
val categoricalFeaturesInfo = Map[Int, Int]()
val impurity = "gini"
val maxDepth = 6
val maxBins = 32
val model = DecisionTree.trainClassifier(trainingData, numClasses, categoricalFeaturesInfo,
  impurity, maxDepth, maxBins) </pre></div></li></ul></div><p>As for the R notebook on Spark, we will continue to use the package <code class="literal">rpart</code> and then the <code class="literal">rpart</code> function for all the calculation. For <code class="literal">rpart</code>, we need to specify the classifier and also all the features to be used.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec35"></a>Feature preparation</h2></div></div><hr /></div><p>In section, <span class="emphasis"><em>Feature extraction</em></span>, of <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Data Preparation for Spark ML</em></span>, we reviewed a few <a id="id230" class="indexterm"></a>methods for feature extraction and discussed their implementation on Apache Spark. All the techniques discussed there can be applied to our data here, especially the ones utilizing time series and feature comparison to create new features.</p><p>For this project, feature extraction is one of the most important tasks because all the fraud happens online and the web log is the most important and most recent data to predict frauds, which needs <a id="id231" class="indexterm"></a>extraction to produce features ready for modeling.</p><p>Also, as we have features for transactions, users, bank accounts, and computer devices, a lot of work is needed to merge all these features together to form a complete data file ready for machine learning.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec61"></a>Feature extraction from LogFile</h3></div></div></div><p>Log files are<a id="id232" class="indexterm"></a> always unstructured, similarly to a collection of<a id="id233" class="indexterm"></a> random symbols and numbers. One example of this is as follows:</p><div class="informalexample"><pre class="programlisting">May 23 12:19:11 elcap siu: 'siu root' succeeded for tjones on /dev/ttyp0 www.abccorp.com/pay w</pre></div><p>Parsing them and making sense of them needs a lot of work as well as some subject knowledge. Most people will work manually with some sample data and then use the patterns discovered to develop codes in R or others to parse and turn extracted information into features.</p><p>For this project, our strategy is not to parse all the log files into as many features as possible but only to extract a few features useful for our machine learning.</p><p>Through some special programming<a id="id234" class="indexterm"></a> in <span class="strong"><strong>SparkSQL</strong></span> and R for this project, we were able to extract a few good features from the log file. These features include the number of clicks, the time between clicks, type of clicks, and others.</p><p>After feature extraction, we will perform some feature selection work on LogFile features as well as on other features from other datasets, which result in a selection of features, as summarized by the following table:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Category</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Number of Features</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>Web Log</p>
</td><td style="" align="left" valign="top">
<p>3</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Account</p>
</td><td style="" align="left" valign="top">
<p>4</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Computer devise</p>
</td><td style="" align="left" valign="top">
<p>3</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>User</p>
</td><td style="" align="left" valign="top">
<p>5</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Business</p>
</td><td style="" align="left" valign="top">
<p>3</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Total</p>
</td><td style="" align="left" valign="top">
<p> 18</p>
</td></tr></tbody></table></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec62"></a>Data merging</h3></div></div></div><p>As discussed in section, <span class="emphasis"><em>Spark for fraud detection</em></span>, we have five datasets for web log, accounts, computer devices, users, and business. In other words, for each transaction, there is always a user using one computer device<a id="id235" class="indexterm"></a> to complete a payment transaction for a business to an account.</p><p>In the previous section, we extracted features from web logs and then selected features for each dataset.</p><p>Now, we need to merge all the data together to form a table with each feature organized together with the target variable, so we can build predictive models on them.</p><p>To merge them together, let's follow what was discussed in section, <span class="emphasis"><em>Joining data sets</em></span> of <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Data Preparation for Spark ML</em></span>, for which we can use SparkSQL or the <code class="literal">data.table</code> R package.</p><p>After data gets merged, we can create some new features by comparison between features in different datasets. For example, we can do a comparison between addresses and computer device languages to form new features. Therefore, for this case, we added three more features to form a set of 21 features. Then, we can perform some feature reduction and selection to explore our feature space.</p><p>After the preceding, we will split the data into the training and test sets.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec36"></a>Model estimation</h2></div></div><hr /></div><p>Once feature sets get finalized in our last section, what follows is to estimate the parameters of the selected models, for which we can use either MLlib or R. As before, we need to arrange distributed computing.</p><p>To simplify, we <a id="id236" class="indexterm"></a>can utilize Databricks' Job feature. Specifically, within the Databricks environment, we can go to <span class="strong"><strong>Jobs</strong></span> to create jobs.</p><div class="mediaobject"><img src="graphics/B04883_04_03.jpg" /></div><p>Then, users can select R notebooks to run specific clusters, and then schedule jobs. Once scheduled, users can also monitor the running and then collect the results.</p><p>In section, <span class="emphasis"><em>Methods for fraud detection</em></span>, we<a id="id237" class="indexterm"></a> prepared some codes for each of the three models selected. Now, we need to modify them with the final set of features, so we can create notebooks.</p><p>For now, we have one target variable prepared and 18 features, so we need to insert all of them into the code developed in section, <span class="emphasis"><em>Methods for fraud detection</em></span> to finalize our notebook. Then, we will use Spark's distributed computing to get the notebook implemented in a distributed way.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec63"></a>MLlib implementation</h3></div></div></div><p>Besides the<a id="id238" class="indexterm"></a> preceding R notebook approach, another <a id="id239" class="indexterm"></a>option is to use MLlib, which is a built-in ML library for Apache Spark. With MLlib, we can use the following code for Random forest:</p><div class="informalexample"><pre class="programlisting">// Train a RandomForest model.
val treeStrategy = Strategy.defaultStrategy("Classification")
val numTrees = 300  
val featureSubsetStrategy = "auto" // Let the algorithm choose.
val model = RandomForest.trainClassifier(trainingData,
  treeStrategy, numTrees, featureSubsetStrategy, seed = 12345)</pre></div><p>For decision trees, we use:</p><div class="informalexample"><pre class="programlisting">val model = DecisionTree.trainClassifier(trainingData, numClasses, categoricalFeaturesInfo,
  impurity, maxDepth, maxBins)</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec64"></a>R notebooks implementation</h3></div></div></div><p>Now, the <a id="id240" class="indexterm"></a>main task here is to schedule the <a id="id241" class="indexterm"></a>estimation for each worker and then collect the results using the JOB feature mentioned before in the Databricks environment:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Random forest</strong></span>: The following codes are needed for the R notebook:</p><div class="informalexample"><pre class="programlisting">library(randomForest)
randomForest((fraud~ ., data=NULL, ..., subset, na.action=na.fail))</pre></div></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Decision tree</strong></span>: The following codes are needed for the R notebook to estimate decision trees:</p><div class="informalexample"><pre class="programlisting">f.est1 &lt;- rpart(fraud~ r1 + … + r21, method="class")</pre></div></li></ul></div><p>After we get all the models estimated as per each fraud type and customer group, we will need to <a id="id242" class="indexterm"></a>calculate some averages and other<a id="id243" class="indexterm"></a> statistics. However, for simplicity, we will focus on one group for discussion in the following sections.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec37"></a>Model evaluation</h2></div></div><hr /></div><p>In the last section, we completed our model estimation. Now, it is the time for us to evaluate these estimated models to see whether they fit our client's criteria so that we can either move to results explanation or go back to some previous stages to refine our predictive models.</p><p>To perform our <a id="id244" class="indexterm"></a>model evaluation, in this section, we will focus on utilizing confusion matrix and FalsePositive numbers to assess the goodness of fit for our models. To calculate them, we need to use our test data rather than training data.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec65"></a>A quick evaluation</h3></div></div></div><p>As discussed before, both MLlib and R have algorithms to return a confusion matrix and even false<a id="id245" class="indexterm"></a> positive numbers.</p><p>MLlib has <code class="literal">confusionMatrix</code> and <code class="literal">numFalseNegatives()</code> to use.</p><p>The following code calculates error ratios:</p><div class="informalexample"><pre class="programlisting">// Evaluate model on test instances and compute test error
val testErr = testData.map { point =&gt;
  val prediction = model.predict(point.features)
  if (point.label == prediction) 1.0 else 0.0
}.mean()
println("Test Error = " + testErr)
println("Learned Random Forest:n" + model.toDebugString)</pre></div><p>To visualize the performance of our classifiers, we can use the R package ROCR. For more information on <a id="id246" class="indexterm"></a>using ROCR, readers may visit <a class="ulink" href="https://rocr.bioinf.mpi-sb.mpg.de/" target="_blank">https://rocr.bioinf.mpi-sb.mpg.de/</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec66"></a>Confusion matrix and false positive ratios</h3></div></div></div><p>In MLlib, we <a id="id247" class="indexterm"></a>can use the following code to produce a <a id="id248" class="indexterm"></a>confusion matrix and related false <a id="id249" class="indexterm"></a>positive<a id="id250" class="indexterm"></a> ratios:</p><div class="informalexample"><pre class="programlisting">// compute confusion matrix
val metrics = new MulticlassMetrics(predictionsAndLabels)
println(metrics.confusionMatrix)</pre></div><p>In R, we can produce a confusion matrix and related false positive ratios with the following code:</p><div class="informalexample"><pre class="programlisting">model$confusion</pre></div><p>For the fraud type social engineering, we produced the following confusion matrix, which shows a good result:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Type of Fraud</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Predicted as Fraud</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Predicted as NOT</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>Actual Fraud</p>
</td><td style="" align="left" valign="top">
<p>81%</p>
</td><td style="" align="left" valign="top">
<p>19%</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Actual Not</p>
</td><td style="" align="left" valign="top">
<p>12%</p>
</td><td style="" align="left" valign="top">
<p>88%</p>
</td></tr></tbody></table></div><p>For this project, the preceding table is the most important evaluation as the company wants to<a id="id251" class="indexterm"></a> increase the ratio in the upper-left cell, which <a id="id252" class="indexterm"></a>is to catch fraud as much as possible. However, they also need to reduce the ratio in the lower-left cell, which is to reduce<a id="id253" class="indexterm"></a> the false positive ratio as much as possible.</p><p>As discussed<a id="id254" class="indexterm"></a> earlier, the low ratio in the upper-left cell means that many frauds will not be caught, which may lead to big losses.</p><p>The high false positive ratio often leads to wasted labor on the company's side and also incovenience to customers, which could lead to low customer satisfaction and even loss of customers.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec38"></a>Results explanation</h2></div></div><hr /></div><p>After we passed our model evaluation stage and decided to select the estimated and evaluated model <a id="id255" class="indexterm"></a>as our final model, our next task is to interpret results to the company executives and technicians.</p><p>Here, we will work on results explanation with a focus on large influencing variables.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec67"></a>Big influencers and their impacts</h3></div></div></div><p>As we briefly<a id="id256" class="indexterm"></a> discussed before, quality and freshness are very different for each dataset. Each data has its own weakness, as summarized in the following:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Category</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Weakness</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>Web Log</p>
</td><td style="" align="left" valign="top">
<p>incomplete</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Account</p>
</td><td style="" align="left" valign="top">
<p>old</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Computer device</p>
</td><td style="" align="left" valign="top">
<p>incomplete</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>User</p>
</td><td style="" align="left" valign="top">
<p>old</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Business</p>
</td><td style="" align="left" valign="top">
<p>Incomplete and old</p>
</td></tr></tbody></table></div><p>Due to the preceding issues, we often do not have enough data to score each transaction or score it with good accuracy, and we can only score it later. Because of this, the company hopes to identify some special signals or insights that can be used to take action quickly and easily.</p><p>The<a id="id257" class="indexterm"></a> following briefly summarizes some of the result samples that we use some functions from <code class="literal">randomForest</code> and decision tree to produce.</p><p>With the <code class="literal">randomForest</code> package in R, a simple code of <code class="literal">estimatedModel$importance</code> will return a ranking of variables by their importance in determining frauds.</p><p>Tables for Impact Assessment:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Feature</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Impacts</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>Click speed</p>
</td><td style="" align="left" valign="top">
<p> 1</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Account</p>
</td><td style="" align="left" valign="top">
<p> 2</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>ComputerDevice</p>
</td><td style="" align="left" valign="top">
<p> 3</p>
</td></tr></tbody></table></div><p>Here, obtaining variable importance through the <code class="literal">randomForest</code> functions needs a full model estimated and will complete all data. So, it does not really solve our problems.</p><p>What customers really needed is actually to use a partial set of available features to estimate a model with limited variables and then assess how good this partial model is, which is to tell the fraud catching and false positive ratio. To complete this task, Apache Spark's advantage of fast computing is utilized, which helps get results.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec39"></a>Deploying fraud detection</h2></div></div><hr /></div><p>As discussed <a id="id258" class="indexterm"></a>before, MLlib supports model exporting to <span class="strong"><strong>Predictive Model Markup Language</strong></span> (<span class="strong"><strong>PMML</strong></span>). For the R notebook, it could run on other environments<a id="id259" class="indexterm"></a> as well as, and with the PMML R package, R models could be exported. Also, it is possible to deploy models for decision making directly on Apache Spark and make results easily available to users. Therefore, we do export some developed models to PMML for this project.</p><p>However, in practice, the users of this project will be more interested in rule-based decision making to use some of our insights and also in score-based decision making to prevent frauds.</p><p>Here, we will discuss each one of them only briefly as a full deployment for decision making will need an optimization that is not covered in this chapter.</p><p>Turning estimated models into rules and scores is not very challenging and could be done under nonSpark platforms. However, Apache Spark makes things easy and fast. The advantage of <a id="id260" class="indexterm"></a>utilizing Apache Spark is to allow us to quickly produce new rules and scores when data and customer requirements get changed.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec68"></a>Rules</h3></div></div></div><p>As discussed before, for R results, there are several tools to help extract rules out from developed<a id="id261" class="indexterm"></a> predictive models.</p><p>For the decision tree model we developed, we should use the <code class="literal">rpart.utils</code> R package, which can extract rules and export them in various formats, such as RODBC.</p><p>The <code class="literal">rpart.rules.table(model1)  *</code> package returns an unpivoted table of variable values (factor levels) associated with each branch.</p><p>However, for this project, partially due to the issue of data incompleteness, we will need to utilize some insights to derive rules directly. That is, we need to use insights discussed in the last section. For example, we can do the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>If the online click speed is dramatically different from the past, contact the user by phone</p></li><li style="list-style-type: disc"><p>If the bank account is not a real bank account or just a debit card or the bank account is very new, some actions are needed</p></li></ul></div><p>From an analytical perspective, we face the same issue here to minimize false positives while catching enough frauds.</p><p>The company had a high false positive ratio from their past rules, and as a result of this, too many alerts were sent out that became a burden for manual inspection and also caused a lot of customer complaints.</p><p>Therefore, by taking advantage of Spark's fast computing, we carefully produced rules and, for each rule, we supplied false positive ratios that helped the company utilize the rules.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec69"></a>Scoring</h3></div></div></div><p>From the <a id="id262" class="indexterm"></a>coefficients of our predictive models, we can derive a suspicious score for fraud, but that takes some work.</p><p>In R, <code class="literal">model$predicted</code> will return the case class as FRAUD or NOT. However, <code class="literal">prob=predict(model,x,type="prob")</code> will produce a probability value, which can be used directly as a score.</p><p>However, in order to use the score, we need to select a cutting-out score. For example, we can decide to take actions when the suspicious score is over 80.</p><p>Different score cutting points will produce different fraud positive ratios and also the ratios of catching frauds; for this, users need to make a decision about how to balance the results here.</p><p>By taking <a id="id263" class="indexterm"></a>advantage of Spark's fast computing, results can be calculated quickly, which allows the company to select cutting points instantly and make changes any time when needed.</p><p>Another way to deal with this issue is to use the <code class="literal">OptimalCutpoints</code> R package.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec40"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we went through a step-by-step process, from big data to a rapid development of fraud detection systems from which we processed data on Spark and then built several models to predict frauds. With this, we then developed rules and scores to help the ABC company prevent frauds.</p><p>Specifically, we first selected a supervised machine learning approach with a focus on Random forest and decision trees as per business needs, after we prepared Spark computing and loaded preprocessed data. Second, we worked on feature extraction and selection. Third, we estimated model coefficients. Fourth, we evaluated these estimated models using a confusion matrix and false positive ratios. Then, we interpreted our machine learning results. Finally, we deployed our machine learning results, with a focus on scoring but also used insights to develop rules.</p><p>The preceding process is similar to the process of working with small data. However, in dealing with big data, we need parallel computing, which Apache Spark is utilized for. Also, during the process described before, Apache Spark makes things easy and fast so that we are able to solve a few difficult problems, such as incomplete data. This means that we could take advantage of Apache Spark's fast computing to meet ABC Corporation's special analytical needs.</p><p>After this chapter, you will have gained a full understanding of how Apache Spark can be utilized to make our work easier and faster in conducting supervised machine learning, and developing fraud detection systems. Also, you now understand how fast computing can turn into analytical capabilities.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch05"></a>Chapter 5. Risk Scoring on Spark</h2></div></div></div><p>Starting with this chapter, we will go deep into some technologies for using Apache Spark for machine learning. While this chapter focuses on notebooks for Spark, <a class="link" href="#" linkend="ch06">Chapter 6</a>, <span class="emphasis"><em>Churn Prediction on Spark</em></span> will focus on machine learning libraries including MLlib, and <a class="link" href="#" linkend="ch07">Chapter 7</a>, <span class="emphasis"><em>Recommendations on Spark</em></span>, will focus on SPSS with Spark.</p><p>Specifically, in this chapter, we will review machine learning methods and analytical processes for a risk scoring project, and get them implemented by R notebooks on Apache Spark in a special DataScientistWorkbench environment. We will also discuss how Apache Spark notebooks help to get everything well-organized and easy. The following topics will be covered in this chapter:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Spark for risk scoring</p></li><li style="list-style-type: disc"><p>Methods for risk scoring</p></li><li style="list-style-type: disc"><p>Data and feature preparation</p></li><li style="list-style-type: disc"><p>Model estimation</p></li><li style="list-style-type: disc"><p>Model evaluation</p></li><li style="list-style-type: disc"><p>Results explanation</p></li><li style="list-style-type: disc"><p>Deployment of risk scoring</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec41"></a>Spark for risk scoring</h2></div></div><hr /></div><p>In this section, we <a id="id264" class="indexterm"></a>will start with a real business case, and then describe how to prepare an Apache Spark environment to use R notebooks to work on this real life risk scoring project.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec70"></a>The use case</h3></div></div></div><p>XST Corp <a id="id265" class="indexterm"></a>provides loans and other financial assistance to millions of individuals who need the cash either to continue their business or to take care of some emergent personal needs. This company accepts applications online and then makes instant decisions on most of the received applications. For this purpose, they use the data collected from the online applications and data collected in the past in their data warehouse, along with additional data provided by third parties.</p><p>Their online <a id="id266" class="indexterm"></a>applications provide identity data and some financial data of the applicants. The company's compiled data consists of information related to location, economy, and others. The third-party data has a lot of rich data related to past credit, current employment, and others.</p><p>This company works in a fast-changing industry with many competitors. So they are constantly looking for better risk scoring models that can enable them to outperform their competitors. Specifically, the model should predict defaults more accurately than their competitors, and should be easily deployed to allow the company to approve more applicants, with low default risks on the approved applicants.</p><p>With these three sets of data, the company has more than two thousand features (variables) to use for their machine learning. Therefore, feature selection is a big task, and so is data preparation, because the data quality is not as good as it should be, with a lot of missing values.</p><p>The company has clear ideas about evaluating their models by following the industry standards as well as by meeting their own goal of approving more applicants with low risk. It is also clear on how to deploy their models. But all these tasks need to be completed in the minimum amount of time, or even be automated if possible, to enable their instant decision-making needs and their need for constantly refining models. For this reason, a notebook approach is ideal, as notebook facilitates replication and iterative computing with options for quick modification. At the same time, new data comes in frequently, so the models need to be refined very often just to accommodate the new data.</p><p>As for the machine learning part, for this project, we do have a target variable of loan default, and applicant data from online applications along with credit data, consumer data, public record data, and social media data from the three data sources mentioned earlier.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec71"></a>Apache Spark notebooks</h3></div></div></div><p>As <a id="id267" class="indexterm"></a>mentioned in the previous section, for <a id="id268" class="indexterm"></a>this project, we will need to organize our machine learning for replication and possibly automation. For this, we will use notebooks to organize all the code, and then get them implemented on Apache Spark. Notebooks facilitate replication, and also provide a good foundation for future automation.</p><p>Most R users are familiar with the R package <code class="literal">Markdown</code>, which makes it easy to create R notebooks that enable easy creation of dynamic documents, analytics, presentations, and reports from R.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note23"></a>Note</h3><p>Readers unfamiliar with Markdown may visit the following web links to gain a quick understanding, and<a id="id269" class="indexterm"></a> also view an example of an R notebook:</p><p>
<a class="ulink" href="http://rmarkdown.rstudio.com/" target="_blank">http://rmarkdown.rstudio.com/</a> and <a class="ulink" href="http://ramnathv.github.io/rNotebook/" target="_blank">http://ramnathv.github.io/rNotebook/</a>
</p></div><p>To prepare <a id="id270" class="indexterm"></a>notebooks on Apache Spark, one option is to use <a id="id271" class="indexterm"></a>Zeppelin, which is an open source product, and has been used widely. For building a notebook on Spark by using Zeppelin, the following two links explain everything very clearly:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<a class="ulink" href="http://www.r-bloggers.com/interactive-data-science-with-r-in-apache-zeppelin-notebook/" target="_blank">http://www.r-bloggers.com/interactive-data-science-with-r-in-apache-zeppelin-notebook/</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://hortonworks.com/blog/introduction-to-data-science-with-apache-spark/" target="_blank">http://hortonworks.com/blog/introduction-to-data-science-with-apache-spark/</a>
</p></li></ul></div><p>But this will take a lot of coding and system configuration work and, for R notebook, you will even need to use an R interpreter.</p><p>You can also use <a id="id272" class="indexterm"></a>R on the <code class="literal">Jupyter</code> notebook, for which you can find clear instructions at the following website:</p><p>
<a class="ulink" href="http://blog.revolutionanalytics.com/2015/09/using-r-with-jupyter-notebooks.html" target="_blank">http://blog.revolutionanalytics.com/2015/09/using-r-with-jupyter-notebooks.html</a>.</p><p>There have been many successful efforts at using the <code class="literal">Jupyter</code> notebook to organize R programming, with one example at <a class="ulink" href="http://nbviewer.ipython.org/github/carljv/Will_it_Python/blob/master/MLFH/CH2/ch2.ipynb" target="_blank">http://nbviewer.ipython.org/github/carljv/Will_it_Python/blob/master/MLFH/CH2/ch2.ipynb</a>.</p><p>Like Zeppelin, using <code class="literal">Jupyter</code> also needs a lot of work on coding and system configuration. If you want to avoid too much coding and configuration work, you can use the Databricks environment as described in the previous chapters, where R notebooks can be easily implemented on Apache Spark and data clusters.</p><p>Besides Databricks, another option is to utilize the <a id="id273" class="indexterm"></a>IBM Data Scientist Workbench at <a class="ulink" href="https://datascientistworkbench.com/" target="_blank">https://datascientistworkbench.com/</a>.</p><div class="mediaobject"><img src="graphics/B04883_05_01.jpg" /></div><p>The <a id="id274" class="indexterm"></a>DataScientistWorkbench has Apache Spark installed, and also has an integrated data-cleaning system, OpenRefine, so that our data preparation work can be made easier and more organized.</p><div class="mediaobject"><img src="graphics/B04883_05_02.jpg" /></div><p>For this project, we <a id="id275" class="indexterm"></a>will use the DataScientistWorkbench for data cleaning, R notebook creation, and Apache Spark implementation. For this setup, some of the Apache Spark techniques described in the previous chapters may apply.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec42"></a>Methods of risk scoring</h2></div></div><hr /></div><p>Having <a id="id276" class="indexterm"></a>described our business use case, and prepared our Apache Spark computing platform, in this section, we need to select our analytical methods or predictive models (equations) for this machine learning project for risk scoring, which is to complete a task of mapping our risk modelling case to machine learning methods.</p><p>To model and predict loan defaults, logistic regression and decision tree are among the most utilized methods. For our exercise, we will use both. But we will focus on logistic regression, because <a id="id277" class="indexterm"></a>logistic regression, if well developed in combination with decision trees, can outperform most of the other methods.</p><p>As always, once we finalize our decision for analytical methods or models, we will need to prepare our coding, which will be in R for this chapter.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec72"></a>Logistic regression</h3></div></div></div><p>Logistic regression<a id="id278" class="indexterm"></a> measures the relationship between one categorical dependent variable and one or more independent variables by estimating probabilities using a logistic function, which is the cumulative logistic distribution. Logistic regression can be seen as a special case of <span class="strong"><strong>Generalized Linear Model</strong></span> (<span class="strong"><strong>GLM</strong></span>), and thus, it is analogous to linear regression.</p><div class="mediaobject"><img src="graphics/B04883_05_03.jpg" /></div><p>We have chosen to focus on logistic regression for this real life use case mainly for two reasons besides the performance as mentioned previously:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Logistic regression can be interpreted easily with some simple calculations</p></li><li style="list-style-type: disc"><p>Most financial corporations have implemented logistic regression in the past; so it becomes easy for our clients to compare our results against what they have received in the past</p></li></ul></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl3sec10"></a>Preparing coding in R</h4></div></div></div><p>With R, there <a id="id279" class="indexterm"></a>are many ways to code for logistic regression.</p><p>In the previous chapters, we used the R function <code class="literal">glm</code> with the following code:</p><div class="informalexample"><pre class="programlisting">Model1 &lt;-glm(good_bad ~.,data=train,family=binomial())</pre></div><p>For consistency, we will continue to use the <code class="literal">glm</code> function here.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec73"></a>Random forest and decision trees</h3></div></div></div><p>Random forest is <a id="id280" class="indexterm"></a>an ensemble learning method for classification and regression that builds hundreds or more decision trees at the training stage, and then combines their output for the final prediction.</p><p>Random forest is quite a popular machine learning method, because its interpretation is very intuitive and it usually leads to good results with less effort than that needed for logistic regression. There are many algorithms developed in R, Java, and others for implementing Random forest, so preparation is relatively easy.</p><p>As our focus for this project is on logistic regression, Random forest comes in to assist our logistic regression for feature selection and for calculating the importance of features.</p><p>As mentioned before, decision trees, in <a id="id281" class="indexterm"></a>combination with logistic regression, often provide good results. So we bring in decision tree modeling here, and also use the decision tree model for our client to test rule-based solutions, and compare them to our score-based solutions.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl3sec11"></a>Preparing coding</h4></div></div></div><p>In R, we need to use the <a id="id282" class="indexterm"></a>R package <code class="literal">randomForest</code>, as originally developed by Leo Breiman and Adele Cutler.</p><p>To get a random forest model estimated, we can use the following R code, where we use the training data and <code class="literal">2000</code> trees.</p><div class="informalexample"><pre class="programlisting">library(randomForest)
Model2 &lt;- randomForest(default ~ ., data=train, importance=TRUE, ntree=2000)</pre></div><p>Once the models get estimated, we can use functions <code class="literal">getTree</code> and <code class="literal">importance</code> to obtain the results.</p><p>For decision trees, there are a few ways of coding in R:</p><div class="informalexample"><pre class="programlisting">Model3 &lt;- rpart(default ~ ., data=train)</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note24"></a>Note</h3><p>For a good example of running Random forest on Spark, please go to <a class="ulink" href="https://spark-summit.org/2014/wp-content/uploads/2014/07/Sequoia-Forest-Random-Forest-of-Humongous-Trees-Sung-Chung.pdf" target="_blank">https://spark-summit.org/2014/wp-content/uploads/2014/07/Sequoia-Forest-Random-Forest-of-Humongous-Trees-Sung-Chung.pdf</a>.</p></div></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec43"></a>Data and feature preparation</h2></div></div><hr /></div><p>In the <a id="id283" class="indexterm"></a>section <span class="emphasis"><em>Feature extraction</em></span> of <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Data Preparation for Spark ML</em></span>, we have reviewed a few methods for feature extraction, and discussed their implementation in Apache Spark. All the techniques discussed there can be applied to the risk scoring project here.</p><p>For this project, as mentioned earlier, the main concern is to get everything organized as workflows for repeatability, and possibly automation. So we will adopt OpenRefine for data and feature preparation. We will use OpenRefine within the DataScientistWorkbench environment where it has been integrated.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec74"></a>OpenRefine</h3></div></div></div><p>
<span class="strong"><strong>OpenRefine</strong></span>, formerly <span class="emphasis"><em>Google Refine</em></span>, is <a id="id284" class="indexterm"></a>an open source application for data cleaning.</p><p>To use <a id="id285" class="indexterm"></a>OpenRefine, please go to: <a class="ulink" href="https://datascientistworkbench.com/" target="_blank">https://datascientistworkbench.com/</a>
</p><p>After logging in, you will see the following screen:</p><div class="mediaobject"><img src="graphics/B04883_05_04.jpg" /></div><p>Then, please <a id="id286" class="indexterm"></a>click on the <span class="strong"><strong>OpenRefine</strong></span> button on the upper-right corner of the screen:</p><div class="mediaobject"><img src="graphics/B04883_05_05.jpg" /></div><p>Here, you can import datasets from your computer or from a URL address.</p><p>Then you can create an OpenRefine project for data cleaning and preparation. After that, you can export the prepared data, or send the data to a notebook by drag and drop.</p><p>For this project, we specially used OpenRefine for identity matching (reconciliation), deleting duplicates, and merging datasets.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec44"></a>Model estimation</h2></div></div><hr /></div><p>In this section, we <a id="id287" class="indexterm"></a>will describe the methods and procedures for utilizing R notebooks within the DataScientistWorkbench to complete our model estimation.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec75"></a>The DataScientistWorkbench for R notebooks</h3></div></div></div><p>As soon as we <a id="id288" class="indexterm"></a>get our data ready by using OpenRefine, we should develop an R notebook within the  which applies the codes prepared in section, <span class="emphasis"><em>Methods for risk scoring</em></span> and the features prepared in section, <span class="emphasis"><em>Data and feature preparation</em></span> to the data.</p><p>As seen in the following screenshot, the DataScientistWorkbench allows us to create an interactive R notebook, run it, and share it as well.</p><p>R studio, a favorite with R users, is also integrated with the DataScientistWorkbench:</p><div class="mediaobject"><img src="graphics/B04883_05_06.jpg" /></div><p>To start a notebook, you can click on <span class="strong"><strong>Build Analytics</strong></span>, and then on <span class="strong"><strong>Notebook,</strong></span> or you can directly click on the <span class="strong"><strong>Notebook</strong></span> blue button as seen in the following screenshot:</p><div class="mediaobject"><img src="graphics/B04883_05_07.jpg" /></div><p>Once an R <a id="id289" class="indexterm"></a>notebook is developed, it can be seen under <span class="strong"><strong>Recent Notebooks</strong></span>, and you can run it to obtain results as in other environments.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec76"></a>R notebooks implementation</h3></div></div></div><p>For estimating models, the <a id="id290" class="indexterm"></a>main task <a id="id291" class="indexterm"></a>is to schedule the implementation of R notebooks within the Data Scientist Workbench environment. To do so, we need to use R notebooks started in the previous section, for which we need to insert all the following R code:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Logistic regression</strong></span>: The <a id="id292" class="indexterm"></a>following code is needed for the R notebook:</p><div class="informalexample"><pre class="programlisting">Model1 &lt;-glm(good_bad ~.,data=train,family=binomial())</pre></div></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Random Forest</strong></span>: The <a id="id293" class="indexterm"></a>following code is needed for the R notebook:</p><div class="informalexample"><pre class="programlisting">library(randomForest)
randomForest(default~ ., data=train, na.action=na.fail, importance=TRUE, ntree=2000)</pre></div></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Decision tree</strong></span>: The <a id="id294" class="indexterm"></a>following codes is needed for the R notebook to estimate decision trees:</p><div class="informalexample"><pre class="programlisting">f.est1 &lt;- rpart(default~ r1 + … + r21, data=train, method="class")</pre></div></li></ul></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec45"></a>Model evaluation</h2></div></div><hr /></div><p>After completing<a id="id295" class="indexterm"></a> our model estimation as described in the preceding section, we need to evaluate these estimated models to see if they fit our client's criterion so that we can either move to the explanation of results or go back to some previous stage to refine our predictive models.</p><p>To perform our model evaluation, in this section, we will utilize confusion matrix numbers to assess the quality of fit for our models, and then expand to other statistics.</p><p>As always, to calculate them, we need to use our test data rather than the training data.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec77"></a>Confusion matrix</h3></div></div></div><p>In R, we can <a id="id296" class="indexterm"></a>produce the model's performance indices with the following code:</p><div class="informalexample"><pre class="programlisting">model$confusion</pre></div><p>Once a cutting point is determined, the following confusion matrix is produced, which shows a good result:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Model's Performance</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Predicted as Default</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Predicted as NOT (Good)</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>Actual Default</p>
</td><td style="" align="left" valign="top">
<p>89%</p>
</td><td style="" align="left" valign="top">
<p>11%</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Actual Not (Good)</p>
</td><td style="" align="left" valign="top">
<p>12%</p>
</td><td style="" align="left" valign="top">
<p>88%</p>
</td></tr></tbody></table></div><p>For this project, the preceding table is the most important evaluation, as the company wants to increase the ratio in the top-left cell, which is to disapprove risky applicants as much as possible. But, they also need to reduce the ratio in the bottom-left cell, which is to reduce the false positive ratio as much as possible, that is, not to reject any good customers.</p><p>However, the preceding results  depend on how to choose a cutting point, so it is not the best table for <a id="id297" class="indexterm"></a>comparing scores. For this reason, we need to use ROCs and KS, as they are summary statistics, rather than statistics relying on a single point.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec78"></a>ROC</h3></div></div></div><p>A <a id="id298" class="indexterm"></a>
<span class="strong"><strong>Receiver Operating Characteristic Curve</strong></span> (<span class="strong"><strong>ROC</strong></span>) is a<a id="id299" class="indexterm"></a> standard technique for summarizing classifier performance <a id="id300" class="indexterm"></a>over a range of trade-offs between <span class="strong"><strong>True Positive</strong></span> (<span class="strong"><strong>TP</strong></span>) and <span class="strong"><strong>False Positive</strong></span> (<span class="strong"><strong>FP</strong></span>) error rates. The ROC curve<a id="id301" class="indexterm"></a> is a plot of sensitivity (the ability of the model to predict an event correctly) versus 1-specificity for the possible cut-off classification probability values, π0.</p><p>Here, <span class="emphasis"><em>sensitivity=P(y^=1|y=1)</em></span> and <span class="emphasis"><em>specificity=P(y^=0|y=0)</em></span>.</p><p>The ROC curve is more informative than the confusion matrix, since it summarizes the predictive power of all possible cutting-off points.</p><p>As shown in the following graph, the area under the ROC curve as 1 implies a perfect model, while the area under the ROC curve as 0.5 implies a worthless model.</p><div class="mediaobject"><img src="graphics/B04883_05_08.jpg" /></div><div class="informalexample"><pre class="programlisting">#load library
library(ROCR)
#score test data set
test$score&lt;-predict(m,type='response',test)
pred&lt;-prediction(test$score,test$good_bad)
perf &lt;- performance(pred,"tpr","fpr")
plot(perf)</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec79"></a>Kolmogorov-Smirnov</h3></div></div></div><p>
<span class="strong"><strong>The Kolmogorov-Smirnov</strong></span> (<span class="strong"><strong>KS</strong></span>) value <a id="id302" class="indexterm"></a>is the <a id="id303" class="indexterm"></a>maximum difference between the cumulative true positive and cumulative false positive rate, which is heavily used in the industry to assess models.</p><p>This code builds on the ROCR library by taking the <span class="emphasis"><em>max delt</em></span> between the cumulative bad and good rates being plotted by ROCR:</p><div class="informalexample"><pre class="programlisting">max(attr(perf,'y.values')[[1]]-attr(perf,'x.values')[[1]])</pre></div><p>With the R codes in this section combined with the R codes from the previous sections, we have a complete process for estimating our logistic regression models, and for obtaining ROC values and the KS value for the estimated models.</p><p>For various financial services' products and for various customer segments along with various feature combinations, we've got a lot of models to run, but we can complete them quickly with our notebook approach towards DataScientistWorkbench on Apache Spark.</p><p>For all the models estimated, our clients first select the ones with the KS values greater than 0.40 and ROC values greater than 0.67. After that, they use subject knowledge to finalize the selections.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec46"></a>Results explanation</h2></div></div><hr /></div><p>As before, once <a id="id304" class="indexterm"></a>we pass the model evaluation stage and select the estimated model as our final model, the next task is to interpret the results for the company executives and technicians.</p><p>In the next section, we will work on results explanation  focusing on some big influencing variables. With the big influencing variables identified, the company could use them to improve their marketing effort to recruit the right customers.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec80"></a>Big influencers and their impacts</h3></div></div></div><p>With logistic regression <a id="id305" class="indexterm"></a>results, we can explain the impact of each feature by using regression coefficients, and identify big influencers by comparing those coefficients.</p><p>With the same logic, we can also rank each feature by its effects as calculated by the logistic regression coefficients.</p><p>Another way is to use the R package of effect, which was created by John Fox and others especially for the display of the effects of linear and generalized linear models. By using this package, we can obtain a list and some graphical displays with the function of plot (effect) .</p><p>To take this to a higher level, users may consider using the R package <code class="literal">relaimpo</code>, specially created to assess the relative importance of predictors, for which the following code should be used:</p><div class="informalexample"><pre class="programlisting">library(relaimpo)
calc.relimp(model1, type = c("lmg"), rela = TRUE)</pre></div><p>Here, <code class="literal">"lmg"</code> refers to the authors Lindeman, Merenda, and Gold for their special methods.</p><p>As used in the previous chapters, with the <code class="literal">randomForest</code> package in R, a simple code of <code class="literal">estimatedModel$importance</code> will return a ranking of variables by order of their importance in determining the default risk.</p><p>However, to obtain the importance of variables through <code class="literal">randomForest</code> functions, we need a full model estimated, with all data complete. So it does not really solve our problems.</p><p>For this project, we <a id="id306" class="indexterm"></a>will rely on results obtained from logistic regression.</p><p>As for our client, we find a few very interesting insights, among which are a few features with big impacts, which include the length of stay at the current address and social media influence.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec47"></a>Deployment</h2></div></div><hr /></div><p>As demonstrated in the<a id="id307" class="indexterm"></a> previous chapters, turning estimated models into scores is not very challenging, and could be done under non-Spark platforms. However, Apache Spark makes things easy and fast as demonstrated.</p><p>With the notebook approach adopted in this chapter, we will fully achieve the advantage to quickly produce new scores when data and customer requirements get changed.</p><p>Users will find some similarity to the deployment work in the last chapter—the deployment of scoring for fraud detection.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec81"></a>Scoring</h3></div></div></div><p>From coefficients <a id="id308" class="indexterm"></a>of our predictive models, we can derive a risk score for possible default, which takes some work. But it gives the client the flexibility of changing it whenever needed.</p><p>With logistic regression, the process of producing scores is relatively easy—it uses the following formulae for logistic regression:</p><div class="mediaobject"><img src="graphics/B04883_05_09.jpg" /></div><p>Specifically, <code class="literal">Prob(Yi=1) = exp(BXi)/(1+exp(BXi))</code> produces the default probability, with <code class="literal">Y=1</code> as the default, and X is a sum of all the features. In R, <code class="literal">exp(coef(logit_model))</code> returns the needed odd ratios.</p><p>In R, the quick way is to use the function of predict as follows:</p><div class="informalexample"><pre class="programlisting">prob=predict(model,x,type="prob")</pre></div><p>Specifically, this<a id="id309" class="indexterm"></a> function will produce a probability value for default, which can be used directly as a score for this project.</p><p>However, in order to use the score, we still need to select a cutting out score. For example, we can choose to take action only when the risk score is over 90.</p><p>Different score cutting points will produce different false positive ratios, and also the ratios of excluding bad applicants, for which the users need to make a decision about how to balance the results.</p><p>By taking advantage of Spark's fast computing, results can be calculated fast, which allows the company to select a cutting point instantly and to make changes when needed.</p><p>As similar to other applications, another way to deal with this issue is to use the R package, <code class="literal">OptimalCutpoints</code>.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec48"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we have turned our focus to a notebook approach to Apache Spark, and specifically developed R notebooks for estimating and assessing models, with which we developed risk scores to help the company XST to improve their risk management.</p><p>We first selected a few machine learning methods with our focus on the logistic regression method, along with random forest and decision trees. We then worked on data cleaning and feature development by using a special tool called OpenRefine. Next, we estimated the model coefficients. We then evaluated these estimated models by using a confusion matrix, ROC, and KS. Then we interpreted our machine learning results. And finally, we deployed our machine learning results with a scoring approach.</p><p>With a notebook approach, all the preceding machine learning steps are implemented in R, with all the R codes stored in notebooks so that the process is repeatable and can be partially automated. To get everything organized well and integrated with Apache Spark, we used the DataScientistWorkbench here.</p><p>After this chapter, readers will have gained a full understanding of the notebook approach to Apache Spark as well as some machine learning techniques for risk scoring and the DataScientistWorkbench. To sum up, readers will gain good knowledge about R, notebook, DataScientistWorkbench, and Spark. For more information on Apache Spark and DataScientistWorkbench, you can go to <a class="ulink" href="http://www.db2dean.com/Previous/Spark1.html" target="_blank">http://www.db2dean.com/Previous/Spark1.html</a>.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch06"></a>Chapter 6. Churn Prediction on Spark</h2></div></div></div><p>In this chapter, we will focus on the utilization of some Apache Spark machine learning libraries, especially MLlib, as applied to a churn predictive modeling project.</p><p>Specifically, in this chapter, we will first review machine learning methods and the related computing for a churn prediction project, and will then discuss how Apache Spark MLlib makes things easy and fast. At the same time, with a real life churn prediction example, we will illustrate the step-by-step process of predicting churns with big data. The following topics will be covered in this chapter:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Spark for churn prediction</p></li><li style="list-style-type: disc"><p>Methods for churn prediction</p></li><li style="list-style-type: disc"><p>Feature preparation</p></li><li style="list-style-type: disc"><p>Model estimation</p></li><li style="list-style-type: disc"><p>Model evaluation </p></li><li style="list-style-type: disc"><p>Results explanation</p></li><li style="list-style-type: disc"><p>Model deployment</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec49"></a>Spark for churn prediction</h2></div></div><hr /></div><p>In this section, we <a id="id310" class="indexterm"></a>will start with a real-life business case description, and then review the steps for preparing the Apache Spark computing for our churn prediction project.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec82"></a>The use case</h3></div></div></div><p>The YST Corporation is a big auto corporation selling and leasing vehicles to millions of customers. The <a id="id311" class="indexterm"></a>company wishes to improve customer retention by using machine learning with big data, as they understand that consumers today go through a complex decision making process before purchasing or leasing a car, that it is becoming increasingly important to proactively identify customers that have a tendency to leave, and take preventive interventions to retain such customers.</p><p>The company has collected a lot of customer satisfaction data through their dealers and service centers as well as through their frequently conducted customer surveys. At the same time, the company has collected data for customers' online behavior from their web sites along with some social media data. Of course, the company has its transaction data for each purchase and car lease, and also a lot of data about their products and their services besides the various promotions and interventions they implemented in the past. The goal of this machine learning project is to build a predictive model for the company to understand how their product features and service improvements, together with promotion interventions, affect customer satisfaction, and then customer churns.</p><p>To sum up, for<a id="id312" class="indexterm"></a> this project, we have a target variable, customer defection, and a lot of data about customer behavior, products, and services as well as company interventions such as promotions to form features as predictors.</p><p>Through some preliminary analysis, the company understands some of their data challenges as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Data is not ready to use; the web log data, especially, needs to be extracted into features ready for machine learning</p></li><li style="list-style-type: disc"><p>There are many kinds of cars with various leasing and purchasing options for various kinds of customers, for which the customer churn patterns are very different from each other</p></li><li style="list-style-type: disc"><p>Data exists in different silos, which needs to be merged together</p></li></ul></div><p>To deal with the aforementioned challenges, in the actual process of delivering good machine learning results for this real-life project, we utilized some techniques presented in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>A Holistic View on Spark</em></span> to merge all the datasets together, and also some feature extraction techniques along with some distributed computing techniques discussed in the previous chapters. In this chapter, we will focus our efforts on utilizing machine learning libraries to attack problems, and to complete good machine learning.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec83"></a>Spark computing</h3></div></div></div><p>As seen in the preceding chapter, for this machine learning project of customer churn prediction, parallel computing is needed due to the many kinds of cars for various customer<a id="id313" class="indexterm"></a> segments. For this, we need to set up clusters and worker nodes as before, while completing our Apache Spark installation.</p><p>As discussed in section, <span class="emphasis"><em>Spark overview</em></span> of <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Spark for Machine Learning,</em></span> Apache Spark has a unified platform which consists of the Spark core engine and four libraries that include Spark SQL, Spark Streaming, MLlib, and GraphX. All four libraries have Python, Java, and Scala programming APIs. </p><p>Among the four libraries, MLlib is the one that is most needed for this chapter. Besides the aforementioned built-in library MLlib, there are also many machine learning packages available for Apache Spark, as provided by third parties. One example is IBM's SystemML, which <a id="id314" class="indexterm"></a>contains a lot more algorithms than those offered by MLlib. SystemML is being integrated with Apache Spark.</p><div class="mediaobject"><img src="graphics/B04883_06_01.jpg" /></div><p>Because MLlib is Apache Spark's built-in machine learning library, there is not much work needed to prepare it, which is a great advantage over other machine learning libraries. Another advantage is that it is scalable, and consists of many commonly used machine learning algorithms such as algorithms for:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Performing classification and regression modeling</p></li><li style="list-style-type: disc"><p>Collaborative filtering</p></li><li style="list-style-type: disc"><p>Performing dimensionality reduction</p></li><li style="list-style-type: disc"><p>Conducting feature extraction and transformation</p></li><li style="list-style-type: disc"><p>Exporting PMML models</p></li></ul></div><p>We will need all the preceding algorithms for this project. Spark MLlib is still under active development, with new algorithms expected to be added with every new release.</p><p>To download<a id="id315" class="indexterm"></a> Apache Spark, readers can go to <a class="ulink" href="http://spark.apache.org/downloads.html" target="_blank">http://spark.apache.org/downloads.html</a>.</p><p>To install Apache Spark and<a id="id316" class="indexterm"></a> start running it, readers can<a id="id317" class="indexterm"></a> consult its latest documentation at <a class="ulink" href="http://spark.apache.org/docs/latest/" target="_blank">http://spark.apache.org/docs/latest/</a>.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec50"></a>Methods for churn prediction</h2></div></div><hr /></div><p>In the previous section, we have completed our task of describing the business use case, and that of preparing our Spark computing platform and our datasets. In this section, we need to select our analytical methods or predictive models (equations) for this churn prediction project, that is, to map our business use case to machine learning methods. </p><p>As per the <a id="id318" class="indexterm"></a>research done over a period of many years, customer satisfaction professionals believe that product and services features affect the quality of services, which affects customer satisfaction, finally affecting customer churns. Therefore, we should somehow incorporate this piece of knowledge into our model design or equation specification.</p><p>From an analytical perspective, there are many suitable models for modelling and predicting customer churns, and among them, the most commonly used are logistic regression and decision trees. For this exercise, we will use both, and then use evaluation to determine which one is the best.</p><p>As always, once we finalize our decision for analytical methods or models, we will need to prepare the related target variable and also prepare for coding, in this case with the Spark machine learning libraries.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec84"></a>Regression models</h3></div></div></div><p>Regression is <a id="id319" class="indexterm"></a>one of the most commonly<a id="id320" class="indexterm"></a> used methods for prediction, and has been used to model customer churns by many machine learning professionals.</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Types of regression models</strong></span>: There are two main kinds of regression models that are suitable for churn <a id="id321" class="indexterm"></a>prediction. One<a id="id322" class="indexterm"></a> is <span class="emphasis"><em>linear regression,</em></span> and the<a id="id323" class="indexterm"></a> other is <span class="emphasis"><em>logistic regression</em></span>. For<a id="id324" class="indexterm"></a> this project, logistic regression is more suitable, as we have a target variable about whether the customer departed, with discrete values. But, for the real-life project, we have also used linear regression to model customer satisfaction, as many predictors impact customer satisfaction and, thus, customer churns. But in this case, as an example, our focus will be on logistic regression. To further improve our model performance, we may try <span class="emphasis"><em>LassoModel</em></span> and <span class="emphasis"><em>RidgeRegressionModel</em></span>, which are available in MLlib.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Preparing coding</strong></span>: In MLlib, for linear regression, we will use the same code used earlier as follows:</p><div class="informalexample"><pre class="programlisting">val numIterations = 95
val model = LinearRegressionWithSGD.train(TrainingData, numIterations)</pre></div><p>Also for <a id="id325" class="indexterm"></a>logistic regression, we <a id="id326" class="indexterm"></a>will the code used earlier as follows:</p><div class="informalexample"><pre class="programlisting">val model = new LogisticRegressionWithSGD()
  .setNumClasses(2)
  .run(training)</pre></div></li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec85"></a>Decision trees and Random forest</h3></div></div></div><p>Both decision trees<a id="id327" class="indexterm"></a> and random forest aim to model<a id="id328" class="indexterm"></a> classifying cases, which is about <a id="id329" class="indexterm"></a>classifying into departed or not departed for our<a id="id330" class="indexterm"></a> use case, in sequence with results to be illustrated by trees.</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Introduction to Decision trees and Random forest.</p><p>Specifically, decision tree modeling uses tree branching based on value comparisons to illustrate the impacts of predictive features, which in comparison to logistic regression, is easy to use and also robust with missing data. Robustness with missing data has a big advantage for this use case, as we do have a significant amount of data incompleteness here.</p><p>Random Forest comes from a set of trees, and often hundreds of trees, with ready-to-use functions for producing risk scores (churn probabilities), and for ranking predictive variables by their impact on the target variable, which is very useful for us to help identify bigger interventions for reducing customer churns. </p><p>However, the mean results of hundreds and hundreds of trees somehow obscures the details, so a decision tree explanation can still be very intuitive and valuable.</p></li><li style="list-style-type: disc"><p>Prepare<a id="id331" class="indexterm"></a> coding.</p><p>As<a id="id332" class="indexterm"></a> done earlier, within<a id="id333" class="indexterm"></a> MLlib, we can use the<a id="id334" class="indexterm"></a> following code:</p><div class="informalexample"><pre class="programlisting">val numClasses = 2
val categoricalFeaturesInfo = Map[Int, Int]()
val impurity = "gini"
val maxDepth = 6
val maxBins = 32

val model = DecisionTree.trainClassifier(trainingData, numClasses, categoricalFeaturesInfo,
  impurity, maxDepth, maxBins) </pre></div><p>We may also expand our work to Random Forest and, in MLlib, we can use the following code for random forest:</p><div class="informalexample"><pre class="programlisting">// To train a RandomForest model.
val treeStrategy = Strategy.defaultStrategy("Classification")
val numTrees = 300 
val featureSubsetStrategy = "auto" // Let the algorithm choose.
val model = RandomForest.trainClassifier(trainingData,
  treeStrategy, numTrees, featureSubsetStrategy, seed = 12345)</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note25"></a>Note</h3><p>More guidance <a id="id335" class="indexterm"></a>about coding for decisions can be found at:</p><p>
<a class="ulink" href="http://spark.apache.org/docs/latest/mllib-decision-tree.html" target="_blank">http://spark.apache.org/docs/latest/mllib-decision-tree.html</a>
</p><p>and for<a id="id336" class="indexterm"></a> Random Forest at:</p><p>
<a class="ulink" href="http://spark.apache.org/docs/latest/mllib-ensembles.html" target="_blank">http://spark.apache.org/docs/latest/mllib-ensembles.html</a>
</p></div></li></ul></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec51"></a>Feature preparation</h2></div></div><hr /></div><p>In section, <span class="emphasis"><em>Feature extraction</em></span> of <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Data Preparation for Spark ML,</em></span> we have reviewed a few methods for feature extraction and discussed their implementation in Apache Spark. All the techniques discussed there can be applied to our data here, especially the ones for utilizing time series and feature comparison to create new features. For example, the customer<a id="id337" class="indexterm"></a> satisfaction response change over time is considered as possibly an excellent predictor.</p><p>For this project, we will need to conduct both feature extraction and feature selection, which will allow us to utilize all the techniques discussed in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Data Preparation for Spark ML</em></span> and also <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>A Holistic View on Spark</em></span>.</p><p>The data merging part is also necessary, but its implementation is similar to what was described in the previous chapters, to be completed at ease.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec86"></a>Feature extraction</h3></div></div></div><p>In the previous <a id="id338" class="indexterm"></a>chapters, we used Spark SQL and R for feature extraction and, for this real-life project, we will try to use MLlib for feature extraction; even in reality, users may use all the tools available.</p><p>A complete<a id="id339" class="indexterm"></a> guide for MLlib feature extraction can be found at <a class="ulink" href="http://spark.apache.org/docs/latest/mllib-feature-extraction.html" target="_blank">http://spark.apache.org/docs/latest/mllib-feature-extraction.html</a>.</p><p>Here, we will use the <code class="literal">Word2Vec</code> method for extracting features from the social media data. The following code can be used to load a text file, parse it as an RDD of <code class="literal">Seq[String]</code>, construct a <code class="literal">Word2Vec</code> instance, and then fit a <code class="literal">Word2VecModel</code> with the input data. Finally, we display the top 40 synonyms of some specific words such as  leave or bad service.</p><div class="informalexample"><pre class="programlisting">import org.apache.spark._
import org.apache.spark.rdd._
import org.apache.spark.SparkContext._
import org.apache.spark.mllib.feature.{Word2Vec, Word2VecModel}

val input = sc.textFile("text8").map(line =&gt; line.split(" ").toSeq)

val word2vec = new Word2Vec()

val model = word2vec.fit(input)

val synonyms = model.findSynonyms("china", 40)

for((synonym, cosineSimilarity) &lt;- synonyms) {
  println(s"$synonym $cosineSimilarity")
}

// Save and load model
model.save(sc, "myModelPath")
val sameModel = Word2VecModel.load(sc, "myModelPath")</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec87"></a>Feature selection</h3></div></div></div><p>MLlib also<a id="id340" class="indexterm"></a> has a few functions to be used for feature selection, which are similar to what you learned in the previous chapters, so we are not going to repeat them here.</p><p>An online<a id="id341" class="indexterm"></a> guide on feature selection with MLlib can be found at <a class="ulink" href="http://spark.apache.org/docs/latest/mllib-feature-extraction.html#feature-selection" target="_blank">http://spark.apache.org/docs/latest/mllib-feature-extraction.html#feature-selection</a>.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec52"></a>Model estimation</h2></div></div><hr /></div><p>Once the feature<a id="id342" class="indexterm"></a> sets get finalized in our last section, what follows is the estimation of parameters of the selected models, for which we will use MLlib. As earlier, we need to arrange distributed computing, especially for this case with various cars for various customer segments.</p><p>As MLlib is a built-in package for Apache Spark, the computation is a straightforward process for which the readers may consult <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Spark for Machine Learning</em></span>
</p><p>One of the main reasons for our client utilizing Apache Spark is to take advantage of its computation speed and the ease of implementing parallel computing. For this project, as we need to build models for more than 40 products and many customer segments, we will perform machine learning only against segments by age.</p><div class="mediaobject"><img src="graphics/B04883_06_02.jpg" /></div><p>For updated<a id="id343" class="indexterm"></a> information about implementing parallel computing with Spark and especially about submitting and monitoring application jobs, users should always consult Apache Spark's updated and detailed guidelines at <a class="ulink" href="http://spark.apache.org/docs/latest/cluster-overview.html" target="_blank">http://spark.apache.org/docs/latest/cluster-overview.html</a>.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec88"></a>Spark implementation with MLlib</h3></div></div></div><p>With MLlib<a id="id344" class="indexterm"></a> for random forest, we use the following code:</p><div class="informalexample"><pre class="programlisting">// Train a RandomForest model.
val treeStrategy = Strategy.defaultStrategy("Classification")
val numTrees = 300  
val featureSubsetStrategy = "auto" // Let the algorithm choose.
val model = RandomForest.trainClassifier(trainingData,
  treeStrategy, numTrees, featureSubsetStrategy, seed = 12345)</pre></div><p>For <a id="id345" class="indexterm"></a>decision tree, we use the following code:</p><div class="informalexample"><pre class="programlisting">val categoricalFeaturesInfo = Map[Int, Int]()
val impurity = "variance"
val maxDepth = 5
val maxBins = 64 # larger = higher accuracy

val model = DecisionTree.trainClassifier(trainingData, numClasses, categoricalFeaturesInfo,
  impurity, maxDepth, maxBins)</pre></div><p>In MLlib for linear regression, we will use:</p><div class="informalexample"><pre class="programlisting">val numIterations = 90
val model = LinearRegressionWithSGD.train(TrainingData, numIterations)</pre></div><p>For logistic regression, we will use:</p><div class="informalexample"><pre class="programlisting">val model = new LogisticRegressionWithSGD()
  .setNumClasses(2)</pre></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec53"></a>Model evaluation</h2></div></div><hr /></div><p>Once our model gets estimated as in the preceding section, it is time for us to evaluate these estimated<a id="id346" class="indexterm"></a> models to see if they fit our client's criteria so that we can either move to the results explanation stage or go back to some previous stage to refine our predictive models.</p><p>From the client's perspective, there are two common error types in machine learning for churn prediction.</p><p>The first one is False Negative (Type I Error), which is about failing to identify a customer who has a high propensity to depart.</p><p>From a business perspective, this is the least desirable error as the customer is very likely to leave, and the company does not know that it lost the chance to act to keep the customers, thus adversely affecting the the company's revenue.</p><p>The second one is False Positive (Type II Error), which is about classifying a good, satisfied customer as one who is one likely to churn. </p><p>From a business perspective, this may be acceptable as it does not impact revenue, but will create confusion leading to some negative consequences, and may waste some of the company's expenses, as the company will act or even offer some discounts to save these customers. </p><p>To sum up, in order to perform our model evaluation, in this section we will use the aforementioned error numbers that are parts of a confusion matrix, and also RMSE to assess our regression models. </p><p>To calculate<a id="id347" class="indexterm"></a> them, we need to use our test data rather than training data.</p><p>As discussed earlier, MLlib has algorithms to return RMSE values, confusion matrices, and even false positive numbers directly.</p><p>In MLlib, we can use the following code to calculate RMSE:</p><div class="informalexample"><pre class="programlisting">val valuesAndPreds = test.map { point =&gt;
    val prediction = new_model.predict(point.features)
    val r = (point.label, prediction)
    r
    }
val residuals = valuesAndPreds.map {case (v, p) =&gt; math.pow((v - p), 2)}
val MSE = residuals.mean();
val RMSE = math.pow(MSE, 0.5)</pre></div><p>Besides this, MLlib also has some functions in the <code class="literal">RegressionMetrics</code> and <code class="literal">RankingMetrics</code> classes for us to use for RMSE calculation apart from <code class="literal">confusionMatrix</code> and <code class="literal">numFalseNegatives()</code>
</p><p>The following code calculates error ratios:</p><div class="informalexample"><pre class="programlisting">// Evaluate model on test instances and compute test error
val testErr = testData.map { point =&gt;
  val prediction = model.predict(point.features)
  if (point.label == prediction) 1.0 else 0.0
}.mean()
println("Test Error = " + testErr)
println("Learned Random Forest:n" + model.toDebugString)</pre></div><p>The following code may be used to obtain evaluation metrics for the estimated models:</p><div class="informalexample"><pre class="programlisting">// Get evaluation metrics.
val metrics = new MulticlassMetrics(predictionAndLabels)
val precision = metrics.precision
println("Precision = " + precision)</pre></div><p>In our case, we <a id="id348" class="indexterm"></a>used multiple algorithms on test datasets to obtain RMSEs, a confusion matrix, and error numbers. The following tables shows some example results for two of the models:</p><p>
<span class="strong"><strong>Model 1: Decision Tree:</strong></span>
</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr style="border-bottom: 0.5pt solid ; "><th style="" rowspan="2" colspan="2" align="left" valign="bottom">
<p>CONFUSION MATRIX</p>
<p>Departed</p>
</th><th style="border-bottom: 0.5pt solid ; " colspan="2" align="left" valign="bottom">
<p>Predicted</p>
</th></tr><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Stay</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom"> </th></tr></thead><tbody><tr><td style="" rowspan="2" align="left" valign="top">
<p>Actual</p>
</td><td style="" align="left" valign="top">
<p>Departed</p>
<p>(10.5%)</p>
</td><td style="" align="left" valign="top">
<p>10%</p>
</td><td style="" align="left" valign="top">
<p>0.5%</p>
<p>Type I Error</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Stay</p>
<p>(89.5%)</p>
</td><td style="" align="left" valign="top">
<p>7%</p>
<p>Type II Error</p>
</td><td style="" align="left" valign="top">
<p> 82.5%</p>
</td></tr></tbody></table></div><p>
<span class="strong"><strong>Model 2: Random Forest:</strong></span>
</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr style="border-bottom: 0.5pt solid ; "><th style="" rowspan="2" colspan="2" align="left" valign="bottom">
<p>CONFUSION MATRIX</p>
<p>Churn=1</p>
</th><th style="border-bottom: 0.5pt solid ; " colspan="2" align="left" valign="bottom">
<p>Predicted</p>
</th></tr><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>NOT</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom"> </th></tr></thead><tbody><tr><td style="" rowspan="2" align="left" valign="top">
<p>Actual</p>
</td><td style="" align="left" valign="top">
<p>Churn=1</p>
<p>(10.5%)</p>
</td><td style="" align="left" valign="top">
<p>10%</p>
</td><td style="" align="left" valign="top">
<p>0.5%</p>
<p>Type I Error</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>NOT</p>
<p>(89.5%)</p>
</td><td style="" align="left" valign="top">
<p>6%</p>
<p>Type II Error</p>
</td><td style="" align="left" valign="top">
<p>83.5%</p>
</td></tr></tbody></table></div><p>For this project, as we need to evaluate tens of models, the best thing is to calculate error numbers with <code class="literal">numFalseNegatives</code>
<code class="literal">()</code> so that we can easily sort all the models and then compare to select the good ones.</p><p>With the preceding two tables as our example, our program will look for Type I Errors first, for which the two models perform the same. In this case, the program will look at Type II Error for which the second model is better than the first one.</p><p>In practice, we should use RMSEs to evaluate models, but for this project, the client prefers using error numbers.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec54"></a>Results explanation</h2></div></div><hr /></div><p>After passing <a id="id349" class="indexterm"></a>our model evaluation stage, and deciding to select the estimated and evaluated model as our final model, our next task is to interpret the results for the company executives and technicians.</p><p>In terms of explaining the machine learning results, the company is particularly interested in understanding how their past interventions affected customer churns, and also how their product features and services influence customer churns.</p><p>So, we will work on results explanation, focusing on calculating the effects of several interventions or some product and service features for which MLlib does not offer good functions now. Therefore, in reality, we export the estimated models, and use other tools for results explanation and visualization. However, it is expected that the future releases of MLlib will have these easy functions included soon.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec89"></a>Calculating the impact of interventions</h3></div></div></div><p>With<a id="id350" class="indexterm"></a> logistic regression, the process of producing scores is relatively easy; it uses the following formulae for logistic regression:</p><div class="mediaobject"><img src="graphics/B04883_06_03.jpg" /></div><p>Specifically, <code class="literal">Prob(Yi=1) = exp(BXi)/(1+exp(BXi))</code> produces the default probability, with <code class="literal">Y=1</code> as default, <code class="literal">X</code> as a sum of all the features, and <code class="literal">B</code> as a vector of coefficients.</p><p>So, we <a id="id351" class="indexterm"></a>will need to write some code with coefficients obtained from the previous sections to directly produce the impact assessments.</p><p>On the other hand, we can use the following code to produce the needed impact assessments for which we can load new data, calculate the predicted values, and then export them.</p><div class="informalexample"><pre class="programlisting">// Compute raw scores on the test set.

val predictionAndLabels = test.map { case LabeledPoint(label, features) =&gt;
  val prediction = model.predict(features)
  (prediction, label)
}</pre></div><p>According to the model evaluation work described in the previous section, <code class="literal">randomForest</code> models perform the best. With <code class="literal">randomForest</code>, we can list out all the features by their importance, which gives another insight for interpreting the effects of features on customer churn.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec55"></a>Deployment</h2></div></div><hr /></div><p>As<a id="id352" class="indexterm"></a> discussed earlier, MLlib supports <a id="id353" class="indexterm"></a>model export to <span class="strong"><strong>Predictive Model Markup Language</strong></span> (<span class="strong"><strong>PMML</strong></span>). Therefore, we do export some developed models to PMML for this project. However, in practice, the users for this project are more interested in rule-based decision making to use some of our insights besides score-based decision making to prevent frauds. </p><p>As for this <a id="id354" class="indexterm"></a>project, the client is interested in applying our results for the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Deciding what interventions to use for a combination of car products or services with a special customer segment</p></li><li style="list-style-type: disc"><p>When the company needs to start some interventions depending on the customer churn score</p></li></ul></div><p>Therefore, we need to produce a customer churn risk score for the client with which the client will start some intervention when the score is above a cutting value. At the same time, we need to use the results from our logistic regression to recommend interventions.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note26"></a>Note</h3><p>For more on <a id="id355" class="indexterm"></a>exporting results from MLlib to PMML, please go to <a class="ulink" href="https://spark.apache.org/docs/1.5.2/mllib-pmml-model-export.html" target="_blank">https://spark.apache.org/docs/1.5.2/mllib-pmml-model-export.html</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec90"></a>Scoring</h3></div></div></div><p>Similar to the<a id="id356" class="indexterm"></a> deployment used in <a class="link" href="#" linkend="ch05">Chapter 5</a>, <span class="emphasis"><em>Risk Scoring on Spark</em></span>, here we can use the churn probabilities as our scores, and employ the same methods to obtain them:</p><div class="informalexample"><pre class="programlisting">// Compute raw scores on the test set.
val predictionAndLabels = test.map { case LabeledPoint(label, features) =&gt;
  val prediction = model.predict(features)
  (prediction, label)
}</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec91"></a>Intervention recommendations</h3></div></div></div><p>From last section's work on results explanation, we also gain an understanding about which interventions as <a id="id357" class="indexterm"></a>well as product or service features have bigger effects than the others. Therefore, we can make good recommendations based on them.</p><p>As for the client for this project, a good churn probability score and some recommendations satisfies them as this provides real help for them to improve customer loyalty.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec56"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we have refocused our efforts on machine learning libraries, especially the MLlib with which we processed data on Spark, and then built models to predict customer churns and develop scores to help the company YST to improve their customer retention.</p><p>Specifically, we first selected regression models and decision tree models as per business needs after we prepared Spark computing and loaded in pre-processed data. We then worked on feature extraction with MLlib. Then we estimated the model coefficients with distributed computing. Further, we evaluated these estimated models by using a confusion matrix and false positive ratios as well as RMSE. Then we interpreted our machine learning results. And finally, we deployed our machine learning results with our focus on scoring along with using insights to design interventions.</p><p>After this chapter, readers will have gained a better understanding of how Apache Spark, with its machine learning libraries, can be utilized to make our work easier and faster in conducting supervised machine learning, and developing customer retention systems.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch07"></a>Chapter 7. Recommendations on Spark</h2></div></div></div><p>In this chapter, we will switch our focus to SPSS on Apache Spark as SPSS is a widely used tool for machine learning and data science computing.</p><p>Specifically, in this <a id="id358" class="indexterm"></a>chapter, with a process similar to what we used in previous chapters, we will start with discussing setting up our SPSS on a Spark system for a <a id="id359" class="indexterm"></a>recommendation project, together with a full description of this real-life project. Then, we will select machine learning methods and prepare the data. With SPSS Analytic Server, we will estimate models on Spark and then evaluate models with a focus on using error ratios. Finally, we will deploy the models for our client. Here are the topics that will be covered in this chapter:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Spark for a recommendation engine</p></li><li style="list-style-type: disc"><p>Methods for recommendation development</p></li><li style="list-style-type: disc"><p>Data treatment</p></li><li style="list-style-type: disc"><p>Model estimation</p></li><li style="list-style-type: disc"><p>Model evaluation</p></li><li style="list-style-type: disc"><p>Recommendation deployment</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec57"></a>Apache Spark for a recommendation engine</h2></div></div><hr /></div><p>In this <a id="id360" class="indexterm"></a>section, we will continue to demonstrate Spark's computation speed and ease of coding for a real-life project of movie recommendation, but to be completed by SPSS on Apache Spark.</p><p>SPSS is a widely used software package for statistical analysis. SPSS originally stood for Statistical Package for Social Science, but it is also used by market researchers, health researchers, survey companies, government, education researchers, marketing organizations, data miners, and others. Long produced by SPSS Inc., it was acquired by IBM in 2009. Since then, IBM further developed it and turned it into a popular tool for data scientists and machine learning professionals. To make Spark available to SPSS users, IBM developed technologies making SPSS Spark integration easy, which will be covered in this chapter.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec92"></a>The use case</h3></div></div></div><p>This<a id="id361" class="indexterm"></a> project is to help movie rental company ZHO improve its movie recommendations to its customers.</p><p>The main data set contains tens of million ratings from more than 20,000 users on more than 10,000 movies.</p><p>Using the preceding rich dataset, the client hopes to improve its recommendation engine so that the recommendations are more useful to its customers. At the same time, the company wishes to take advantage of Spark so that it can update models quickly and also take advantage of Spark's parallel computing to develop recommendations for various movie categories as per special customer segmentations.</p><p>The company's analytical team learned about using Spark MLlib for movie recommendation cases and is familiar with the related literature, such as the one at <a class="ulink" href="http://ampcamp.berkeley.edu/big-data-mini-course/movie-recommendation-with-mllib.html" target="_blank">http://ampcamp.berkeley.edu/big-data-mini-course/movie-recommendation-with-mllib.html</a>.</p><p>However, the company's IT teams have utilized SPSS and SPSS Modeler for their analytics for many years, with a lot of analytical assets built on SPSS already, and their teams have used SPSS Modeler to organize analytical workflows for a long time; this is because they are heading toward some analytics automation, so the team prefers the approach of using SPSS on Spark.</p><p>Another reason for ZHO to adopt SPSS is to follow Cross-Industry Standard Process for Data Mining, which is an industry-proven standard process for machine learning, as shown in the following graph:</p><div class="mediaobject"><img src="graphics/B04883_07_01.jpg" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec93"></a>SPSS on Spark</h3></div></div></div><p>To use <a id="id362" class="indexterm"></a>SPSS on Spark, we will need to use IBM SPSS Modeler 17.1 and IBM SPSS Analytics Server 2.1, which have good integration with Apache Spark.</p><p>Also, to <a id="id363" class="indexterm"></a>adopt MLlib collaborative filtering on SPSS Modeler, you need to download IBM Predictive Extensions, as described in <a class="ulink" href="https://developer.ibm.com/predictiveanalytics/downloads/#tab2" target="_blank">https://developer.ibm.com/predictiveanalytics/downloads/#tab2</a>.</p><p>To install <a id="id364" class="indexterm"></a>IBM Predictive Extensions, perform the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Download the extension at <span class="strong"><strong>Download</strong></span> </p></li><li><p>Close IBM SPSS Modeler. Save the <code class="literal">.cfe</code> file in the CDB directory, which is located by default on Windows in <code class="literal">C:\ProgramData\IBM\SPSS\Modeler\17.1\CDB</code> or under your IBM SPSS Modeler installation directory.</p></li><li><p>Restart IBM SPSS Modeler, and the node will now appear in the <span class="strong"><strong>Model</strong></span> palette.</p><div class="mediaobject"><img src="graphics/B04883_07_02.jpg" /></div></li></ol></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note27"></a>Note</h3><p>A more complete summary of SPSS on Spark can be founded at <a class="ulink" href="https://developer.ibm.com/predictiveanalytics/2015/11/06/spss-algorithms-optimized-for-apache-spark-spark-algorithms-extending-spss-modeler/" target="_blank">https://developer.ibm.com/predictiveanalytics/2015/11/06/spss-algorithms-optimized-for-apache-spark-spark-algorithms-extending-spss-modeler/</a>.</p></div><p>The following<a id="id365" class="indexterm"></a> is a screenshot of IBM SPSS Modeler, as you can see. SPSS users can move nodes into the central box to build modeling streams and then run them to obtain results:</p><div class="mediaobject"><img src="graphics/B04883_07_03.jpg" /></div><p>With the <a id="id366" class="indexterm"></a>SPSS Spark integration as described previously, SPSS Modeler users now gain a lot more advantages. Users can now create new Modeler nodes to exploit MLlib algorithms and share them.</p><p>For example, users can also use the custom dialog builder to access Python for Spark. The following screenshot shows the usage of <span class="strong"><strong>Custom Dialog Builder</strong></span> for <span class="strong"><strong>Python for Spark</strong></span>:</p><div class="mediaobject"><img src="graphics/B04883_07_05.jpg" /></div><p>Specifically, <span class="strong"><strong>Custom Dialog Builder</strong></span> <a id="id367" class="indexterm"></a>adds Python for Spark support, which provides access to:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Spark and its machine learning library (MLlib)</p></li><li style="list-style-type: disc"><p>The other common Python libraries such as Numpy, Scipy, Scikit-learn, and Pandas</p></li></ul></div><p>After doing so, users can create new Modeler nodes (extensions) that exploit algorithms from MLlib and other PySpark processes.</p><div class="mediaobject"><img src="graphics/B04883_07_06.jpg" /></div><p>These <a id="id368" class="indexterm"></a>nodes can be shared with others to democratize the access to Spark capabilities. Here, Spark becomes usable for nonprogrammers with code abstracted behind a GUI.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec58"></a>Methods for recommendation</h2></div></div><hr /></div><p>In the <a id="id369" class="indexterm"></a>previous section, we described the use case of building a movie recommendation engine for the company ZHO and also prepared SPSS on the Spark computing platform. In this section, as before, we need to select our analytical methods (equations) for this movie recommendation project, which again means mapping our use case to machine learning methods.</p><p>For this exercise, we will use collaborative filtering because this analytical method is well developed and tested on many recommendation projects. At the same time, analytical processes and related algorithms are also well-developed for this method, which are available in R as well as MLlib.</p><p>By following the same methodology, once we finalize our decision for analytical methods or models, we will then need to prepare the coding.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec94"></a>Collaborative filtering</h3></div></div></div><p>Collaborative filtering is a <a id="id370" class="indexterm"></a>method used very commonly to build recommender systems. Simply speaking, collaborative filtering is an analytical method of producing predictions (filtering) about the interests of a user with preferences of many other users (collaborating). The underlying assumption of this analytical approach is as follows:</p><p>If user A has the same opinion as user B on a movie, user A is more likely to have user B's opinion on a different movie x than to have the opinion on x of another user chosen randomly.</p><p>Specifically, the<a id="id371" class="indexterm"></a> techniques of collaborative filtering here aim to fill in the missing entries of a user-movie association matrix. MLlib currently supports model-based collaborative filtering, in which users and movies are modeled by a set of latent factors that can be used to predict missing entries.</p><p>MLlib uses the <a id="id372" class="indexterm"></a>
<span class="strong"><strong>Alternating Least Squares</strong></span> (<span class="strong"><strong>ALS</strong></span>) algorithm to learn these latent factors. Its implementation in MLlib has the following parameters:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="emphasis"><em>numBlocks</em></span> is the<a id="id373" class="indexterm"></a> number of blocks used to parallelize computation (set to -1 to autoconfigure)</p></li><li style="list-style-type: disc"><p>
<span class="emphasis"><em>rank</em></span> is the <a id="id374" class="indexterm"></a>number of latent factors in the model</p></li><li style="list-style-type: disc"><p>
<span class="emphasis"><em>iterations</em></span> is the <a id="id375" class="indexterm"></a>number of iterations to run</p></li><li style="list-style-type: disc"><p>
<span class="emphasis"><em>lambda</em></span> specifies<a id="id376" class="indexterm"></a> the regularization parameter in ALS</p></li><li style="list-style-type: disc"><p>
<span class="emphasis"><em>implicitPrefs</em></span> specifies<a id="id377" class="indexterm"></a> whether to use the explicit feedback ALS variant or one adapted for an implicit feedback data</p></li><li style="list-style-type: disc"><p>
<span class="emphasis"><em>alpha</em></span> is a<a id="id378" class="indexterm"></a> parameter applicable to the implicit feedback variant of ALS that governs the baseline confidence in preference observations</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note28"></a>Note</h3><p>The standard approach to matrix factorization-based collaborative filtering treats the entries in the user-item matrix as explicit preferences given by the user to the item. However, it is common in many real-world use cases to only have access to implicit feedback (for example, views, clicks, purchases, likes, shares, and so on). Essentially, instead of trying to model the matrix of ratings directly, this approach treats the data as a combination of binary preferences and confidence values. The ratings are then related to the level of confidence in observed user preferences rather than the explicit ratings given to the items.</p><p>A detailed MLlib guide <a id="id379" class="indexterm"></a>to collaborating filtering can be founded at <a class="ulink" href="http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html" target="_blank">http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html</a>.</p></div></li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec95"></a>Preparing coding</h3></div></div></div><p>In our data, each<a id="id380" class="indexterm"></a> row consists of a user, a movie, and a rating. Here, we will use the default <code class="literal">ALS.train()</code> method with the ratings assumed as explicit. The recommendations are evaluated by measuring the Mean Squared Error of rating prediction. Take a look at the following code:</p><div class="informalexample"><pre class="programlisting"># Build the recommendation model using Alternating Least Squares
rank = 10
numIterations = 10
model = ALS.train(ratings, rank, numIterations)

# Evaluate the model on training data
testdata = ratings.map(lambda p: (p[0], p[1]))
predictions = model.predictAll(testdata).map(lambda r: ((r[0], r[1]), r[2]))
ratesAndPreds = ratings.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)
MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()
print("Mean Squared Error = " + str(MSE))</pre></div><p>If the rating matrix<a id="id381" class="indexterm"></a> is derived from another source of information, you can use the <code class="literal">trainImplicit</code> method to get better results, as follows:</p><div class="informalexample"><pre class="programlisting"># Build the recommendation model using Alternating Least Squares based on implicit ratings
model = ALS.trainImplicit(ratings, rank, numIterations, alpha=0.01)</pre></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec59"></a>Data treatment with SPSS</h2></div></div><hr /></div><p>There <a id="id382" class="indexterm"></a>are always many common data and feature issues to work with for any machine learning project, including this movie recommendation project for which we can use SPSS Modeler.</p><p>In comparison with other projects in this book, the data structure here is relatively simple; however, one special issue for the data to be used for this project is about missing values because some users do not rate some movies. To deal with this, SPSS Modeler has a few super nodes to deal with the issue. In other words, we need to develop a special SPSS Modeler Stream, which include nodes for missing value treatments. After this job, we need to separate the data into parts to train and test.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec96"></a>Missing data nodes on SPSS modeler</h3></div></div></div><p>To deal with<a id="id383" class="indexterm"></a> missing values and build a special data stream, we need to start with a Type Node with some Super Nodes to handle missing values to be filled with imputed values.</p><p>Specifically, you can do this from the Data Audit report, which allows you to specify options for specific fields as appropriate and then generate a Super Node that imputes values using a number of methods. This is the most flexible method and it also allows you to specify the handling for a large number of fields in a single node.</p><p>The following is a screenshot of SPSS Modeler Stream for missing data treatment:</p><div class="mediaobject"><img src="graphics/B04883_07_07.jpg" /></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note29"></a>Note</h3><p>For more details about handling missing values with SPSS Modeler 17.0, refer to <a class="link" href="#" linkend="ch07">Chapter 7</a> of the Modeler 17.0 guide at <a class="ulink" href="http://public.dhe.ibm.com/software/analytics/spss/documentation/modeler/17.0/en/ModelerUsersGuide.pdf" target="_blank">ftp://public.dhe.ibm.com/software/analytics/spss/documentation/modeler/17.0/en/ModelerUsersGuide.pdf</a>.</p></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec60"></a>Model estimation</h2></div></div><hr /></div><p>For this project, our <a id="id384" class="indexterm"></a>strategy for model estimation is to employ a complete SPSS Modeler stream developed from previous sections and then use SPSS Analytics Server for Spark Implementation. Our stream consists of SPSS Modeler Nodes for data treatment, as described in section, <span class="emphasis"><em>Data treatment</em></span> and also model training Nodes with MLlib codes described in section, <span class="emphasis"><em>Methods for recommendation development</em></span> as we prepared our SPSS Modeler to be ready to use MLlib in section, <span class="emphasis"><em>Spark for a recommendation engine</em></span>.</p><p>As noted before, our <a id="id385" class="indexterm"></a>IBM SPSS Modeler nodes created from <span class="strong"><strong>Custom Dialog Builder</strong></span> depend on the Spark environment and will only run against IBM SPSS Analytic Server. SPSS Analytics Server is a tool to manage all the computing for model estimations and we have to employ IBM SPSS Analytic Server to implement the model estimation for this project, which makes everything easy for us. However, we also need to arrange for SPSS on the Spark system to run models for each movie category and also for each customer segment for us.</p><p>For more information about IBM SPSS Analytic Server, take a look at IBM Knowledge Center.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec97"></a>SPSS on Spark – the SPSS Analytics server</h3></div></div></div><p>IBM SPSS <a id="id386" class="indexterm"></a>Modeler 17.1 and Analytic Server 2.1 offer<a id="id387" class="indexterm"></a> easy integration with Apache Spark, which allows us to implement the data and modeling streams built so far.</p><div class="mediaobject"><img src="graphics/B04883_07_08.jpg" /></div><p>For <a id="id388" class="indexterm"></a>more <a id="id389" class="indexterm"></a>information about SPSS Analytic Server V 2.1, refer to its administrative guide at:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<a class="ulink" href="http://public.dhe.ibm.com/software/analytics/spss/documentation/analyticserver/2.1/English/IBM_SPSS_Analytic_Server_2.1_Administrators_Guide.pdf" target="_blank">ftp://public.dhe.ibm.com/software/analytics/spss/documentation/analyticserver/2.1/English/IBM_SPSS_Analytic_Server_2.1_Administrators_Guide.pdf</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://www-01.ibm.com/support/knowledgecenter/SSWLVY_2.1.0/analytic_server/knowledge_center/product_landing.dita" target="_blank">http://www-01.ibm.com/support/knowledgecenter/SSWLVY_2.1.0/analytic_server/knowledge_center/product_landing.dita</a>
</p></li></ul></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec61"></a>Model evaluation</h2></div></div><hr /></div><p>In the last<a id="id390" class="indexterm"></a> section, we completed our model estimation. Now it is the time for us to evaluate these estimated models to see whether they fit our client's criteria so that we can either move to results explanation or go back to some previous stage to refine our predictive models.</p><p>As mentioned earlier for this project, using MLlib codes, our recommendations are evaluated by measuring the Mean Squared Error of rating predictions. However, most users may want to perform more evaluations with their favored measurements.</p><p>In practise, the <a id="id391" class="indexterm"></a>model estimation results from SPSS Modeler may be exported for evaluation with other tools, such as R, as some users may wish. Within SPSS Modeler, we can create a Modeler Node against the test data to evaluate our results.</p><p>One of the most commonly used ways to evaluate is to measure the correlation between the predicted and actual ratings for our test dataset of movie users.</p><p>Another commonly used error index with <span class="emphasis"><em>Memory-Based algorithms</em></span> can be calculated through the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>For each user <span class="emphasis"><em>a</em></span> in the test set:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Split <span class="emphasis"><em>a</em></span>'s votes into observed (I) and predict (P).</p></li><li><p>Measure the average absolute deviation between predicted and actual votes in P.</p></li><li><p>Predict the votes in P and form a ranked list.</p></li><li><p>Score the list by its expected utility (Ra) by assuming (a) the utility of the kth item in the list is <code class="literal">max(va,j-d,0)</code>, where d is the <span class="emphasis"><em>default vote</em></span> (b), the probability of reaching the <span class="emphasis"><em>k</em></span> rank drops exponentially in <span class="emphasis"><em>k</em></span>.</p></li></ol></div></li><li><p>Average <span class="emphasis"><em>Ra</em></span> over all test users.</p></li></ol></div><p>On SPSS Modeler, once the model gets built, you can:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Attach a <span class="strong"><strong>Table</strong></span> node to explore your results.</p></li><li><p>Use the <span class="strong"><strong>Analysis</strong></span> node to create a coincidence matrix showing the pattern of matches between each predicted field and its target field. Run the <span class="strong"><strong>Analysis</strong></span> node to see the results.</p><div class="mediaobject"><img src="graphics/B04883_07_09.jpg" /></div></li></ol></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec62"></a>Recommendation deployment</h2></div></div><hr /></div><p>The <a id="id392" class="indexterm"></a>way of implementing the machine learning results for this project as required by the customer is to use them to make new movie recommendations when new movies come in or new users come in. One example of this typical use is to make movie recommendations for new users, which is what we will discuss in this section.</p><p>To make recommendations for a new user, we need to learn this new user's taste by asking the user to rate a few movies, for which we need to select a small set of movies that received the most ratings from users in our movie dataset.</p><p>Once we have the data of new users, then we can apply the trained model for new predictions, which can be obtained via the following code:</p><div class="informalexample"><pre class="programlisting">class MatrixFactorizationModel(object):
    def predictAll(self, usersProducts):
        # ...
        return RDD(self._java_model.predict(usersProductsJRDD._jrdd),
                   self._context, RatingDeserializer())</pre></div><p>After we get all the predictions, we can list the top recommendations, and we will see an output that will be similar to the following:</p><div class="informalexample"><pre class="programlisting">Movies recommended for you:

 1: Saving Private Ryan (1998)
 2: Star Wars: Episode IV - A New Hope (1977)
 3: Braveheart (1995)
   ……</pre></div><p>If we are <a id="id393" class="indexterm"></a>working within IBM SPSS Modeler, we can just add a new Node with the data imported to complete the prediction.</p><p>Also, IBM® SPSS® Modeler provides a number of mechanisms to export the entire machine learning workflow to external applications so that the work completed here can be used to your advantage outside of IBM SPSS Modeler as well.</p><p>The IBM SPSS Modeler streams can also be used in conjunction with:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>IBM SPSS Modeler Advantage </p></li><li style="list-style-type: disc"><p>Applications that can import and export files in the PMML format</p></li></ul></div><p>IBM SPSS Modeler can import and export PMML, making it possible to share models with other applications that support this format, such as IBM SPSS Statistics. To do so, you need to:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Right-click on a model nugget on the models palette. (Alternatively, double-click on a model nugget on the canvas and select the <span class="strong"><strong>File</strong></span> menu).</p></li><li><p>On the menu, click on <span class="strong"><strong>Export PMML</strong></span>.</p></li><li><p>In the <span class="strong"><strong>Export</strong></span> (or <span class="strong"><strong>Save</strong></span>) dialog box, specify a target directory and a unique name for the model.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note30"></a>Note</h3><p>For more details about handling missing values with SPSS Modeler 17.0, refer to <a class="link" href="#" linkend="ch07">Chapter 7</a> of the Modeler 17.0 guide at <a class="ulink" href="ftp://public.dhe.ibm.com/software/analytics/spss/documentation/modeler/17.0/en/ModelerUsersGuide.pdf" target="_blank">ftp://public.dhe.ibm.com/software/analytics/spss/documentation/modeler/17.0/en/ModelerUsersGuide.pdf</a>.</p></div><div class="mediaobject"><img src="graphics/B04883_07_10.jpg" /></div></li></ol></div><p>To use it, we<a id="id394" class="indexterm"></a> can:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Deliver analytical results as customer interactions occur through integration with business user systems—combining the information gathered during the interaction with historical data to determine the next best action</p></li><li style="list-style-type: disc"><p>Deploy streams created in SPSS Modeler to be executed in an operational environment</p></li><li style="list-style-type: disc"><p>Incorporate features that ensure scalability, reliability, and security</p></li><li style="list-style-type: disc"><p>Integrate with the existing authentication systems for authentication and single sign-on capabilities</p></li><li style="list-style-type: disc"><p>Support application server clustering and virtualization for a more effective use of resources</p></li><li style="list-style-type: disc"><p>Create a unified platform that can increase the impact of your analytics investment with IBM SPSS Collaboration and Deployment Services for System z. This version combines the predictive analytics power of IBM SPSS Modeler and IBM SPSS Analytical Decision Management with the security, high availability, and reliability of the System z platform.</p></li></ul></div><p>For more information on utilizing IBM SPSS Collaboration and Deployment Services, go to <a class="ulink" href="http://www-01.ibm.com/support/docview.wss?uid=swg27043649" target="_blank">http://www-01.ibm.com/support/docview.wss?uid=swg27043649</a>.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec63"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we switched our focus to SPSS on Spark with which we processed data on Spark and then built a model for movie recommendations; using this, we produced movie recommendations for individual users.</p><p>Specifically, we first selected collaborative filtering as our method as per business needs after we prepared Spark computing with SPSS and loaded in preprocessed data. Second, we worked on data preparation with SPSS Modeler. Third, we implemented model estimation using SPSS Analytic Server. Fourth, we evaluated these estimated models by assessing error ratios. And finally, we deployed our machine learning results with some examples of recommending movies for individual users.</p><p>After this chapter, you will have gained a full understanding of how Apache Spark can be utilized to make your work easier and faster in conducting supervised machine learning and also gained a deeper understanding of developing recommendation engines. At the same time, you will have learned how SPSS and Spark can work well together.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch08"></a>Chapter 8. Learning Analytics on Spark</h2></div></div></div><p>To continue our machine learning on Spark, we will further extend our application to serve the educational sector in this chapter and the government sector in next chapter. Specifically, in this chapter, we will extend our application to serve learning organizations, such as universities and training institutions, for which we will apply machine learning to improve the learning analytics for a real case of predicting student attrition. In the next chapter, we will utilize our Apache Spark machine learning to serve city governments, for which we will demonstrate our application with a real use case of predicting service requests.</p><p>By following the structures and processes established in previous chapters, in this chapter, we will first review machine learning methods and related computing for the real case of predicting student attrition, and we will then discuss how Apache Spark comes in to make them easy. At the same time, by working on this real-life student attrition prediction example, we will illustrate our machine learning process of predicting attritions step by step with Big Data in the following sections:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Spark for attrition prediction</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Processing Big Data fast and easy with Spark</p></li></ul></div></li><li style="list-style-type: disc"><p>Methods for attrition prediction</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Regression and decision trees</p></li></ul></div></li><li style="list-style-type: disc"><p>Feature preparation</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Feature extraction and data merging</p></li></ul></div></li><li style="list-style-type: disc"><p>Model estimation</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Distributed model estimation</p></li></ul></div></li><li style="list-style-type: disc"><p>Model evaluation</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Confusion matrix and false positive ratio</p></li></ul></div></li><li style="list-style-type: disc"><p>Results explanation</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Significant features and impacts</p></li></ul></div></li><li style="list-style-type: disc"><p>Model deployment</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Rules and scoring</p></li></ul></div></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec64"></a>Spark for attrition prediction</h2></div></div><hr /></div><p>In this section, we will <a id="id395" class="indexterm"></a>start with a real use case and then describe how to prepare Apache Spark for this attrition prediction project.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec98"></a>The use case</h3></div></div></div><p>NIY University is a <a id="id396" class="indexterm"></a>private university and wants to improve its student retention using predictive modeling with Big Data. According to ACT's research (refer to <a class="ulink" href="http://www.act.org/research/policymakers/pdf/retain_2015.pdf" target="_blank">http://www.act.org/research/policymakers/pdf/retain_2015.pdf</a>), the average retention rate for <a id="id397" class="indexterm"></a>American colleges was only about 68% in 2015, and it is even lower for two-year public colleges at 54.7% and for private two-year colleges at 63.4%. That is, about 32% of students left school before graduation, and the attrition is even at greater for two-year public colleges at 45.3% and for two-year private colleges at 36.6%. As student attrition costs both colleges and students a lot, using Big Data to predict students' attrition and designing interventions to prevent them has a lot of value.</p><p>The university has a lot of information about student demographics and the past test scores of its students. At the same time, the university also collected its students' online behavior data on university websites as well as some social media data along with some data of campus social activity. The university especially collected a lot of data on their learning management systems as it uses MOODLE as the main learning platform. The goal of this project is to build a model for the university to identity students at risk, understand how some of their interventions affect students' academic performance, and then work on student retention.</p><p>To sum up, for this project, we have a target variable of student performance measured by test scores as well as a categorical variable of attrition along with a lot of data on demographics, behavior, performance, and interventions.</p><p>Through some preliminary analysis, the university understands some of their data challenges as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The data is not ready to use, especially the web log data, and some of the learning management system data needs to be developed into useful features ready for machine learning</p></li><li style="list-style-type: disc"><p>Students with various backgrounds major in various subjects with various career goals, for which attrition patterns are very different from each other</p></li></ul></div><p>To deal with the challenges mentioned here, for this real project, we will utilize some feature development techniques plus some distributed computing techniques discussed in the previous chapter, for <a id="id398" class="indexterm"></a>which we will specially focus our effort on organizing our computations with some notebooks and then implementing them in an integrated environment to distribute computing.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec99"></a>Spark computing</h3></div></div></div><p>After learning about <a id="id399" class="indexterm"></a>Spark computing in the previous seven chapters, you must be very familiar with setting up Spark computing projects by now, for which there <a id="id400" class="indexterm"></a>are a few options that include the Databricks platform, IBM DataScientistWorkbench, SPSS on Spark, and Apache Spark with MLlib alone.</p><p>Either one of the preceding mentioned approaches should work well for this learning analytics project. Therefore, in the following section, we will touch on using one of the four approaches but will focus our efforts more on utilizing the Zeppelin notebook as this approach of using the Zeppelin notebook was only briefly discussed in <a class="link" href="#" linkend="ch05">Chapter 5</a>, <span class="emphasis"><em>Risk Scoring on Spark</em></span>. The Zeppelin <a id="id401" class="indexterm"></a>notebook is widely utilized, and it is similar to the Jupyter notebook used in IBM DataScientistWorkbench. Both Zeppelin and Jupyter have a similar coding style, embed images, and run different programming languages.</p><p>The Jupyter notebook is more mature in terms of abilities and utility, but its Scala version is weak. With Zeppelin, it's easier to mix languages in the same notebook. You can do some SQL and Scala, then mark down to document it all together. You can also easily convert your notebook into a presentation style to maybe present it to the management or use it in dashboards.</p><p>Also, for practical use, you may take the code developed here in this chapter, put them on a different notebook, and then implement the notebook with any other approaches, as mentioned in the preceding paragraph, so that you will not be limited by our Zeppelin with Spark approach.</p><div class="mediaobject"><img src="graphics/B04883_08_01.jpg" /><div class="caption"><p>Data uploading</p></div></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note31"></a>Note</h3><p>For more information <a id="id402" class="indexterm"></a>about setting up the Zeppelin notebook, visit <a class="ulink" href="http://sparktutorials.net/setup-your-zeppelin-notebook-for-data-science-in-apache-spark" target="_blank">http://sparktutorials.net/setup-your-zeppelin-notebook-for-data-science-in-apache-spark</a> or <a class="ulink" href="http://hortonworks.com/blog/introduction-to-data-science-with-apache-spark/" target="_blank">http://hortonworks.com/blog/introduction-to-data-science-with-apache-spark/</a>.</p></div><p>The following <a id="id403" class="indexterm"></a>screenshot shows how the Zeppelin starting page looks:</p><div class="mediaobject"><img src="graphics/B04883_08_02.jpg" /></div><p>Users can click on <span class="strong"><strong>Create new note</strong></span>, the first line under <span class="strong"><strong>Notebook</strong></span> in the left-hand side column, to start. Then a <a id="id404" class="indexterm"></a>box will open to allow users to type in the notebook's name and then click on <span class="strong"><strong>Create Note</strong></span> to create a new notebook:</p><div class="mediaobject"><img src="graphics/B04883_08_03.jpg" /></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec65"></a>Methods of attrition prediction</h2></div></div><hr /></div><p>In the previous <a id="id405" class="indexterm"></a>section, we described our use case of predicting student attrition and also prepared our Spark computing platform. In this section, we need to perform the task of mapping our use case to machine learning methods, which is to select our analytical methods or predictive models (equations) for this attrition prediction project.</p><p>To model and predict student attrition, the most suitable models include logistic regression and decision tree, as both of them yield good results. Some researchers use neural network and SVM models, but the results are no better than logistic regression. Therefore, for this exercise, we will focus our efforts on logistic regression and decision trees, as well as random forest as an extension of decision tree, and then use model evaluation to determine which one is the best.</p><p>As always, once we finalize our decision regarding analytical methods or models, we need to prepare for coding.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec100"></a>Regression models</h3></div></div></div><p>Regression was <a id="id406" class="indexterm"></a>used in the previous chapters; especially <a id="id407" class="indexterm"></a>in <a class="link" href="#" linkend="ch06">Chapter 6</a>, <span class="emphasis"><em>Churn Prediction on Spark</em></span>, we used logistic regression with good results. As predicting student attrition has a lot in common <a id="id408" class="indexterm"></a>with the work of predicting <a id="id409" class="indexterm"></a>customer churn, we will reuse a lot of the work presented in <a class="link" href="#" linkend="ch06">Chapter 6</a>, <span class="emphasis"><em>Churn Prediction on Spark</em></span>.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl3sec12"></a>About regression</h4></div></div></div><p>There are two kinds of regression modeling that are suitable for attrition prediction, similar to churn prediction. One is <a id="id410" class="indexterm"></a>linear regression, and another is <a id="id411" class="indexterm"></a>logistic regression. For this project, logistic regression is more suitable as we have a target variable about whether the <a id="id412" class="indexterm"></a>student left; we even have the target variable <a id="id413" class="indexterm"></a>of student performance. Logistic regression is an alternative method to modeling discrete choice using maximum likelihood estimation based on the logistic function as opposed to ordinary least squares (linear probability models). A major advantage of logistic regression for dichotomous dependent variables is that it overcomes the inherent heteroskedasticity (that is, nonconstant variance) associated with linear probability models, which is often a special concern for our student data.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl3sec13"></a>Preparing for coding</h4></div></div></div><p>As before, in <a id="id414" class="indexterm"></a>MLlib, for logistic regression, we will use the following:</p><div class="informalexample"><pre class="programlisting">val model = new LogisticRegressionWithSGD()
.setNumClasses(2)</pre></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec101"></a>Decision trees</h3></div></div></div><p>As discussed briefly in <a class="link" href="#" linkend="ch06">Chapter 6</a>, <span class="emphasis"><em>Churn Prediction on Spark</em></span>, in comparison to regression, decision tree is <a id="id415" class="indexterm"></a>easy to use, robust with missing data, and easy to interpret. Here, our reason to use decision tree is mainly due to its <a id="id416" class="indexterm"></a>robustness with missing data, as missing data is a big issue with this real use case. Also, decision tree models produce good charts that clearly express the impact of various features on leading a student to leave, so it is very useful for result interpretation and intervention design.</p><p>Random forest comes from a set of trees, often hundreds of trees, with good functions to produce scores and rank independent variables by their impact on the target variable. For these two reasons, we will also use random forest for this case.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl3sec14"></a>Preparing for coding</h4></div></div></div><p>As before, within <a id="id417" class="indexterm"></a>MLlib, we can use the following code:</p><div class="informalexample"><pre class="programlisting">val numClasses = 2
val categoricalFeaturesInfo = Map[Int, Int]()
val impurity = "gini"
val maxDepth = 6
val maxBins = 32
val model = DecisionTree.trainClassifier(trainingData, numClasses,
  categoricalFeaturesInfo, impurity, maxDepth, maxBins)</pre></div><p>We need to expand our work to random forest, so with MLlib, we will use the following code for this:</p><div class="informalexample"><pre class="programlisting">// To train a RandomForest model.
val treeStrategy = Strategy.defaultStrategy("Classification")
val numTrees = 300
val featureSubsetStrategy = "auto" // Let the algorithm choose.
val model = RandomForest.trainClassifier(trainingData,
  treeStrategy, numTrees, featureSubsetStrategy, seed = 12345)</pre></div></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec66"></a>Feature preparation</h2></div></div><hr /></div><p>In the <span class="emphasis"><em>Feature extraction</em></span> section of <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Data Preparation for Spark ML</em></span>, we reviewed a few <a id="id418" class="indexterm"></a>methods for feature extraction as well as their implementation on Apache Spark. All the techniques discussed there can be applied to our datasets here, especially the ones of utilizing time series to create new features.</p><p>As mentioned earlier, for this project, we have a target categorical variable of student attrition and a lot of data on demographics, behavior, performance, as well as interventions. The demographic data is almost ready to be used but needs to be merged with the following table for a partial list of the features:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>FEATURE NAME</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Description</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>
<code class="literal">ACT</code>
</p>
</td><td style="" align="left" valign="top">
<p>These are the average ACT scores</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">AGE</code>
</p>
</td><td style="" align="left" valign="top">
<p>This is the age</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">UNEMPLOYMENT</code>
</p>
</td><td style="" align="left" valign="top">
<p>This is the student's county unemployment rate</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">FIRST_GENERATION</code>
</p>
</td><td style="" align="left" valign="top">
<p>This is a first-generation student indicator using the "Y/N" options</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">HS_GPA</code>
</p>
</td><td style="" align="left" valign="top">
<p>This is the high school GPA</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">PUBLIC_CODE</code>
</p>
</td><td style="" align="left" valign="top">
<p>This is an indicator of the type of high school</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">REP_RACE</code>
</p>
</td><td style="" align="left" valign="top">
<p>This is the student's reported race/ethnicity</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">DISTANCE</code>
</p>
</td><td style="" align="left" valign="top">
<p>This is the distance of the student's home from campus</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">SEX</code>
</p>
</td><td style="" align="left" valign="top">
<p>This is the student's gender</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">STARBUCKS</code>
</p>
</td><td style="" align="left" valign="top">
<p>This is the number of Starbucks located in the student's county</p>
</td></tr></tbody></table></div><p>Many log files about students' web behavior are also available for this project, for which we will use techniques similar to what discussed in the <span class="emphasis"><em>Feature preparation</em></span> section of <a class="link" href="#" linkend="ch04">Chapter 4</a>, <span class="emphasis"><em>Fraud Detection on Spark</em></span>.</p><p>For this project, our focus on feature preparation is to extract more features from the MOODLE learning management system as this is the main and unique data source for learning analytics, which cover many rich characteristics of the students' learning. They often include students' clicks, timing, and total hours spent on each learning activity along with statistics on access to reading materials, syllabus, assignments, submission timing, and so on.</p><p>All the methods and procedures discussed here for Moodle can also be applied to other learning management systems, such as Sakai and Blackboard. However, for data about student behavior, especially for these behavior characteristics that need to be measured with the data points from Moodle, a lot of work is needed to get them organized, make them meaningful, and then <a id="id419" class="indexterm"></a>merge them into the main dataset; all this will covered in this chapter.</p><div class="mediaobject"><img src="graphics/B04883_08_04.jpg" /></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec102"></a>Feature development</h3></div></div></div><p>In the previous <a id="id420" class="indexterm"></a>chapters, we used SparkSQL, MLlib, and R for feature extraction. Here, for this project, we can use all of them with SparkSQL and MLlib as the most effective tools.</p><p>For your convenience, a complete <a id="id421" class="indexterm"></a>guide to MLlib feature extraction can be found at <a class="ulink" href="http://spark.apache.org/docs/latest/mllib-feature-extraction.html" target="_blank">http://spark.apache.org/docs/latest/mllib-feature-extraction.html</a>.</p><p>For this project, extracting useful features from web log files adds a lot of value. However, the main challenge actually stays with organizing existing datasets and then developing new features from them. Especially for data exported from Moodle, some of it is easily organized, such as that of student performance and classroom participation. However, even with performance features, if we introduce the time dimension into our feature development, the performance changes over time to become something useful. Along with this logic, many new features can be developed.</p><p>Besides the preceding features, there are also some important time-related items, such as the specific time when the students submitted their homework, whether it was the middle of the night or afternoon time or days or hours before the due time, for which some categorical features can be created. Time intervals between periods of participation are also of significance to form some new features.</p><p>Some social network analysis tools are available to extract features from the learning data to measure student interaction with teachers as well as with classmates to form new features.</p><p>With all the preceding <a id="id422" class="indexterm"></a>points taken into consideration, for this use case, we can develop more than 200 features that are of potentially high value to our modeling.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec103"></a>Feature selection</h3></div></div></div><p>Hundreds of features <a id="id423" class="indexterm"></a>in hand to use will enable us to obtain good prediction models. However, feature selection is definitely needed, as discussed in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>A Holistic View on Spark</em></span>, partially for a good explanation and also to avoid overfitting.</p><p>For feature selection, we will adopt a good strategy that we tested in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>A Holistic View on Spark</em></span>, which is to take three steps to complete our feature selection. First, we will perform <span class="strong"><strong>principal components analysis</strong></span> (<span class="strong"><strong>PCA</strong></span>). Second, we will use our subject knowledge to aid the grouping of features. Then, finally, we will apply machine learning feature selection to filter out redundant or irrelevant features.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl3sec15"></a>Principal components analysis</h4></div></div></div><p>If you are using <a id="id424" class="indexterm"></a>MLlib for PCA, visit <a class="ulink" href="http://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html#principal-component-analysis-pca" target="_blank">http://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html#principal-component-analysis-pca</a>. This link has a few example codes that users may adopt and modify to run PCA on Spark. For more information on <a id="id425" class="indexterm"></a>MLlib, visit <a class="ulink" href="https://spark.apache.org/docs/1.2.1/mllib-dimensionality-reduction.html" target="_blank">https://spark.apache.org/docs/1.2.1/mllib-dimensionality-reduction.html</a>.</p><p>To use R, there are at least five functions to perform PCA, which are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">prcomp() (stats)</code>
</p></li><li style="list-style-type: disc"><p>
<code class="literal">princomp() (stats)</code>
</p></li><li style="list-style-type: disc"><p>
<code class="literal">PCA() (FactoMineR)</code>
</p></li><li style="list-style-type: disc"><p>
<code class="literal">dudi.pca() (ade4)</code>
</p></li><li style="list-style-type: disc"><p>
<code class="literal">acp() (amap)</code>
</p></li></ul></div><p>The <code class="literal">prcomp</code> and <code class="literal">princomp</code> functions from the basic package stats are commonly used, for which we also have good functions for the result summary and plots. Therefore, we will use these <a id="id426" class="indexterm"></a>two functions.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch08lvl4sec01"></a>Subject knowledge aid</h5></div></div></div><p>As is always <a id="id427" class="indexterm"></a>the case, if some subject knowledge can be used, feature reduction results can be improved greatly.</p><p>For our example, some concepts used by previous student attrition research are good to start with. They include the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Academic performance</p></li><li style="list-style-type: disc"><p>Financial status</p></li><li style="list-style-type: disc"><p>Emotional encouragement from personal social networks</p></li><li style="list-style-type: disc"><p>Emotional encouragement in school</p></li><li style="list-style-type: disc"><p>Personal adjustment</p></li><li style="list-style-type: disc"><p>Study patterns</p></li></ul></div><p>As an exercise, we will group all the developed features into six groups according to whether they are indicators measuring one of the preceding six concepts. Then, we will perform PCA six times, one for each data category. For example, for academic performance, we need to perform PCA on 53 features or variables to identify factors or dimensions that can fully represent the information we have about academic performance.</p><p>At the end of this PCA exercise, we obtained two to four features for each category, as summarized in the following table:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Category</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Number of Factors</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Factor Names</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>Academic Performance</p>
</td><td style="" align="left" valign="top">
<p>4</p>
</td><td style="" align="left" valign="top">
<p>AF1, AF2, AF3, AF4</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Financial Status</p>
</td><td style="" align="left" valign="top">
<p>2</p>
</td><td style="" align="left" valign="top">
<p>F1, F2</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Emotional Encouragement 1</p>
</td><td style="" align="left" valign="top">
<p>2</p>
</td><td style="" align="left" valign="top">
<p>EE1_1, EE!_2</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Emotional Encouragement 2</p>
</td><td style="" align="left" valign="top">
<p>2</p>
</td><td style="" align="left" valign="top">
<p>EE2_1, EE2_2</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Personal Adjustment</p>
</td><td style="" align="left" valign="top">
<p>3</p>
</td><td style="" align="left" valign="top">
<p>PA1, PA2, PA3</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Study Patterns</p>
</td><td style="" align="left" valign="top">
<p>3</p>
</td><td style="" align="left" valign="top">
<p>SP1, SP2, SP3</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Total</p>
</td><td style="" align="left" valign="top">
<p> 16</p>
</td><td style="" align="left" valign="top"> </td></tr></tbody></table></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl3sec16"></a>ML feature selection</h4></div></div></div><p>In MLlib, we can use the <code class="literal">ChiSqSelector</code> algorithm as follows:</p><div class="informalexample"><pre class="programlisting">// Create ChiSqSelector that will select top 25 of 240 features
val selector = new ChiSqSelector(25)
// Create ChiSqSelector model (selecting features)
val transformer = selector.fit(TrainingData)</pre></div><p>In R, we can use some R packages to make the computation easy. Among the available packages, CARET is one of the commonly used packages.</p></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec67"></a>Model estimation</h2></div></div><hr /></div><p>Once the feature sets <a id="id428" class="indexterm"></a>get finalized, in our last section, what follows is the estimating of parameters of the selected models, for which we can use MLlib on the Zeppelin notebook.</p><p>Similar to what we did before, for the best modeling, we need to arrange distributed computing, especially for this case, with various student segments for various study subjects. For this distributed computing part, readers may refer to previous chapters as we will not repeat them here.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec104"></a>Spark implementation with the Zeppelin notebook</h3></div></div></div><p>With <a id="id429" class="indexterm"></a>MLlib for SCALA <a id="id430" class="indexterm"></a>code for random forest, we will use the following code:</p><div class="informalexample"><pre class="programlisting">// Train a RandomForest model.
val treeStrategy = Strategy.defaultStrategy("Classification")
val numTrees = 300
val featureSubsetStrategy = "auto" // Let the algorithm choose.
val model = RandomForest.trainClassifier(trainingData,
  treeStrategy, numTrees, featureSubsetStrategy, seed = 12345)</pre></div><p>For decision tree, we will execute the following code:</p><div class="informalexample"><pre class="programlisting">val model = DecisionTree.trainClassifier(trainingData, numClasses,
  categoricalFeaturesInfo, impurity, maxDepth, maxBins)</pre></div><p>In MLlib, for linear regression, we will run the following code:</p><div class="informalexample"><pre class="programlisting">val numIterations = 90
val model = LinearRegressionWithSGD.train(TrainingData, numIterations)</pre></div><p>For logistic regression, we will use the following code:</p><div class="informalexample"><pre class="programlisting">val model = new LogisticRegressionWithSGD()
.setNumClasses(2)</pre></div><p>To get all them implemented, we need to first input all the preceding codes into our Zeppelin notebook and then complete their computing over there.</p><p>In other words, we need to input the codes described before into the Zeppelin notebook, as follows:</p><div class="mediaobject"><img src="graphics/B04883_08_05.jpg" /></div><p>Then we <a id="id431" class="indexterm"></a>can press <span class="emphasis"><em>Shift</em></span> + <span class="emphasis"><em>Enter</em></span> <a id="id432" class="indexterm"></a>to run these commands and then obtain results similar to the following screenshot:</p><div class="mediaobject"><img src="graphics/B04883_08_06.jpg" /></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec68"></a>Model evaluation</h2></div></div><hr /></div><p>In the previous <a id="id433" class="indexterm"></a>section, we completed our model estimation. Now it is the time for us to evaluate these estimated models to check whether they fit our client's criteria so that we can either move to the explanation of results or go back to some previous stages to refine our predictive models.</p><p>To perform our model evaluation, in this section, we will use a confusion matrix and error ratio numbers. To calculate them, we need to use our test data rather than training data.</p><p>Here are the two common error types in student attrition prediction:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>False negative (Type I error)</strong></span>: This involves failing to identify a student who has a <a id="id434" class="indexterm"></a>high propensity to leave.</p><p>From a practical perspective, this is the least desirable error as the student is very likely to leave and the university lost its chance to act to keep the students, thus adversely affecting its income and also the students' future career.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>False positive (Type II error)</strong></span>: This involves classifying a good, satisfied student as one <a id="id435" class="indexterm"></a>likely to leave.</p><p>From a practical perspective, this may be acceptable as it does not impact the income or students' future career, but it will create confusion and may waste some of the university's resources as the university will act or even offer some special assistance to save these students.</p></li></ul></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec105"></a>A quick evaluation</h3></div></div></div><p>As discussed before, MLlib has <a id="id436" class="indexterm"></a>algorithms to return a confusion matrix and even false positive numbers.</p><p>MLlib has <code class="literal">confusionMatrix</code> and <code class="literal">numFalseNegatives()</code> to use.</p><p>The following code calculates error ratios:</p><div class="informalexample"><pre class="programlisting">// Evaluate model on test instances and compute test error
val testErr = testData.map { point =&gt;
  val prediction = model.predict(point.features)
  if (point.label == prediction) 1.0 else 0.0
}.mean()
println("Test Error = " + testErr)
println("Learned Random Forest:n" + model.toDebugString)</pre></div><p>The following code may be used to obtain evaluation metrics for the estimated models:</p><div class="informalexample"><pre class="programlisting">// Get evaluation metrics.
val metrics = new MulticlassMetrics(predictionAndLabels)
val precision = metrics.precision
println("Precision = " + precision)</pre></div><p>To visualize the performance of our classifiers, we can use the ROCR R package. For more info about using <a id="id437" class="indexterm"></a>ROCR, readers may visit <a class="ulink" href="https://rocr.bioinf.mpi-sb.mpg.de/" target="_blank">https://rocr.bioinf.mpi-sb.mpg.de/</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec106"></a>The confusion matrix and error ratios</h3></div></div></div><p>Any <a id="id438" class="indexterm"></a>predictive algorithm going into production <a id="id439" class="indexterm"></a>will have to be the one with the least Type I error.</p><p>In our case, we used multiple algorithms on a <code class="literal">Test</code> dataset to predict student attrition. Shown in the following screenshot are the results from the top two performing algorithms:</p><div class="mediaobject"><img src="graphics/B04883_08_07.jpg" /></div><p>To implement the model evaluation, we need to adopt the same method used in the <span class="emphasis"><em>Model estimation</em></span> section; that is, we need to input all the codes into our Zeppelin notebook and then run the model evaluation part of the code to obtain tables similar to the following one:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " colspan="4" align="left" valign="bottom">
<p>Predicted</p>
</th></tr></thead><tbody><tr><td style="" rowspan="3" align="left" valign="top">
<p>Real</p>
</td><td style="" align="left" valign="top"> </td><td style="" align="left" valign="top">
<p>0</p>
</td><td style="" align="left" valign="top">
<p>1-attrition</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>0</p>
</td><td style="" align="left" valign="top">
<p>4527</p>
</td><td style="" align="left" valign="top">
<p>733</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>1-attrition</p>
</td><td style="" align="left" valign="top">
<p>57</p>
</td><td style="" align="left" valign="top">
<p>342</p>
</td></tr></tbody></table></div><p>399 students at risk</p><p>85.34 % accuracy (4257+342)/(4257+342+57+733)</p><p>With the <a id="id440" class="indexterm"></a>preceding evaluation, we can <a id="id441" class="indexterm"></a>compare models and select the acceptable ones.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec69"></a>Results explanation</h2></div></div><hr /></div><p>After we have passed our <a id="id442" class="indexterm"></a>model evaluation stage and decided to select the estimated and evaluated model as our final model, our next task is to interpret results to the university leaders and technicians.</p><p>In terms of explaining the machine learning results, the university is particularly interested in, firstly, understanding how their designed interventions affect student attrition, and, secondly, among the common reasons of finances, academic performance, social/emotional encouragement, and personal adjustment, which has the biggest impact.</p><p>We will work on results explanation with our focus on big influencing variables in the following sections.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec107"></a>Calculating the impact of interventions</h3></div></div></div><p>The following <a id="id443" class="indexterm"></a>summarizes some of the result samples briefly, for which we can use some functions from <code class="literal">randomForest</code> and decision tree to produce.</p><p>With Spark 1.5, you can use the following code to obtain a vector of feature importance:</p><div class="informalexample"><pre class="programlisting">val importances: Vector = model.featureImportances</pre></div><p>With the <code class="literal">randomForest</code> package in R, a simple code of <code class="literal">estimatedModel$importance</code> will return a ranking of variables by their importance in determining attrition.</p><p>The table for impact assessment for interventions is as follows:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Feature</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Impacts</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>Teacher interaction</p>
</td><td style="" align="left" valign="top">
<p>1</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Financial aid</p>
</td><td style="" align="left" valign="top">
<p>2</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Study grouping</p>
</td><td style="" align="left" valign="top">
<p>3</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>…</p>
</td><td style="" align="left" valign="top"> </td></tr></tbody></table></div><p>Here, to obtain variable importance through the <code class="literal">randomForest</code> functions, we need a full model estimated with all the data complete. So, it does not really solve our problems.</p><p>What learning organizations really need is to actually use a partial set of available features to estimate a model with limited variables and then assess how good this partial model is, which is to <a id="id444" class="indexterm"></a>say how good the attrition catching and false positive ratios are. To complete this task, Apache Spark's advantage of fast computing is utilized, which helps us get results.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec108"></a>Calculating the impact of main causes</h3></div></div></div><p>As we briefly <a id="id445" class="indexterm"></a>discussed in the <span class="emphasis"><em>Feature preparation</em></span> section, the main predictors selected can be summarized with the following table:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Category</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Number of factors</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Factor names</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>Academic performance</p>
</td><td style="" align="left" valign="top">
<p>4</p>
</td><td style="" align="left" valign="top">
<p>AF1, AF2, AF3, AF4</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Financial status</p>
</td><td style="" align="left" valign="top">
<p>2</p>
</td><td style="" align="left" valign="top">
<p>F1, F2</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Emotional encouragement 1</p>
</td><td style="" align="left" valign="top">
<p>2</p>
</td><td style="" align="left" valign="top">
<p>EE1_1, EE!_2</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Emotional encouragement 2</p>
</td><td style="" align="left" valign="top">
<p>2</p>
</td><td style="" align="left" valign="top">
<p>EE2_1, EE2_2</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Personal adjustment</p>
</td><td style="" align="left" valign="top">
<p>3</p>
</td><td style="" align="left" valign="top">
<p>PA1, PA2, PA3</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Study patterns</p>
</td><td style="" align="left" valign="top">
<p>3</p>
</td><td style="" align="left" valign="top">
<p>SP1, SP2, SP3</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Total</p>
</td><td style="" align="left" valign="top">
<p>16</p>
</td><td style="" align="left" valign="top"> </td></tr></tbody></table></div><p>The university leaders are interested in learning how these features cause attrition, for which we can perform what was described in the previous section. That is, we need to apply the code used to obtain feature importance to the preceding features to rank their importance.</p><p>As for logistic regression results, we can also apply the <span class="emphasis"><em>Prob(Yi=1) = exp(BXi)/(1+exp(BXi))</em></span> equation to obtain the impact of each feature at a certain point.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec70"></a>Deployment</h2></div></div><hr /></div><p>As discussed <a id="id446" class="indexterm"></a>before, MLlib supports model export to <span class="strong"><strong>Predictive Model Markup </strong></span><a id="id447" class="indexterm"></a>
<span class="strong"><strong>Language</strong></span> (<span class="strong"><strong>PMML</strong></span>). Therefore, we export some developed models to PMML for this project as some other departments of the university are interested in our analytical results and use other systems such as SPSS.</p><p>However, for practical purposes, the users of this project are more interested in rule-based decision making to use some of our insights and also in score-based decision making to reduce student attrition.</p><p>Specifically, as for this project, the client is interested in applying our results to, firstly, decide which interventions to use for a combination of course adjustments or counseling services with a special student segment, and, secondly, when the university needs to start some interventions as per the student attrition score.</p><p>Therefore, we need to <a id="id448" class="indexterm"></a>turn some of our results into rules and also produce a student attrition risk score for this university.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec109"></a>Rules</h3></div></div></div><p>All the algorithms <a id="id449" class="indexterm"></a>either in MLlib or R can produce trees directly so that users may use these trees to derive rules directly.</p><p>Also, as discussed before, for R results, there are several tools to help extract rules from developed predictive models.</p><p>For the decision tree model developed, we should use the <code class="literal">rpart.utils</code> R package, which can extract rules and export them in various formats, such as RODBC.</p><p>The <code class="literal">rpart.rules.table(model1)</code> returns an unpivoted table of variable values (factor levels) associated with each branch, that is, sub rules to be used.</p><p>However, for this project, partially due to the issue of data incompleteness, it is better for us to use some insight into deriving rules directly. That is, we should use the insight discussed in last section. For example, we can do the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>If academic performance is decreasing dramatically, we can contactthe teacher</p></li><li style="list-style-type: disc"><p>If the student's social network score is below a certain level and academic performance is also changing dramatically (even now at low scores), some actions are needed</p></li></ul></div><p>From an analytical perspective, one of the main issues here is to minimize the false positive while catching enough attritions.</p><p>The university had a high false positive ratio from using their past rules, and as a result of this, too many alerts were sent out, adding a big burden for manual inspection. Therefore, by taking advantage of <a id="id450" class="indexterm"></a>Spark's fast computing, we carefully produced rules, and for each rule, we supplied false positive ratios that helped the university use these rules as well as provide useful feedback.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec110"></a>Scoring</h3></div></div></div><p>From coefficients of our <a id="id451" class="indexterm"></a>predictive models, we can derive a probability score for attrition, but this takes some work.</p><p>Using the following MLlib code, we can obtain probability scores quickly:</p><div class="informalexample"><pre class="programlisting">// Compute raw scores on the test set.
val predictionAndLabels = test.map { case LabeledPoint(label, features) =&gt;
  val prediction = model.predict(features)
  (prediction, label)
}</pre></div><p>The preceding code returns labels, but for binary classification, you can use the <code class="literal">LogisticRegressionModel.clearThreshold</code> method. After it is called, <code class="literal">predict</code> will return raw scores:</p><div class="mediaobject"><img src="graphics/B04883_08_08.jpg" /></div><p>Unlike the labels mentioned before, these are in the [0, 1] range and can be interpreted as probabilities.</p><p>Using R, <code class="literal">model$predicted</code> will return the case class as <code class="literal">ATTRITION</code> or <code class="literal">NOT</code>. However, <code class="literal">prob=predict(model,x,type="prob")</code> will produce a probability value, which can be used directly as a score.</p><p>However, in order to use the score, we need to select a cutting out score. For example, we can choose to take action when the attrition probability score is over 80.</p><p>Different score cutting points will produce different false positive ratios and also the ratios of catching possible attrition, for which the users need to make a decision about how to balance the results.</p><p>By taking advantage of Spark's fast computing, results can be calculated fast, which allows the university to select a cutting point instantly and make changes whenever needed.</p><p>Another way to deal with <a id="id452" class="indexterm"></a>this issue is to use the <code class="literal">OptimalCutpoints</code> R package.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec71"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we extended our machine learning on Spark to serve learning analytics, for which we completed a step-by-step process of processing big data obtained from learning management systems and other sources for a rapid development of student attrition prediction models on Apache Spark. With the machine learning results obtained, we developed rules and scores to be used by NIY University for interventions to reduce student attrition.</p><p>Specifically, we first selected a supervised machine learning approach with a focus on logistic regression and decision trees as per the special needs of this university and the nature of the project, and after this, we prepared Spark computing and loaded in the preprocessed data. Secondly, we worked on feature development and selection. Thirdly, we estimated model coefficients with the Zeppeline notebook on Spark. Next, we evaluated these estimated models using a confusion matrix and error ratios. Then, we interpreted our machine learning results to the university leaders and technicians. Finally, we deployed our machine learning results with some special effort on scoring students as per attrition probabilities, but we also used insight to develop rules.</p><p>This process is similar to the process used in the previous chapters for commercial applications, such as churn modeling. However, in working for educational applications, we made some special considerations for feature development and result explanation.</p><p>After reading this chapter, you should have gained a complete understanding of how Apache Spark can be utilized to make our work easier and faster in conducting supervised machine learning to serve educational institutions and, specially, develop student attrition prediction models. At the same time, you gained a good understanding of how fast computing can be turned into analytical capabilities for educational organizations.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch09"></a>Chapter 9. City Analytics on Spark</h2></div></div></div><p>Following the strategy adopted in <a class="link" href="#" linkend="ch08">Chapter 8</a>, <span class="emphasis"><em>Learning Analytics on Spark</em></span>, in this chapter, we will further extend our Spark machine learning application to smart city analytics, for which we will apply machine learning to open data for city analytics. In other words, we will extend the benefit of machine learning on Spark to serving city governments.</p><p>Specifically in this chapter, we will first review machine learning methods and related computing for a service request forecasting project and will then discuss how Apache Spark comes in to make them easy. At the same time, with this real-life service forecasting example, we will illustrate step by step our machine learning process of predicting service requests with big data.</p><p>Here, we will use the service forecasting project for the purpose of illustrating our technologies and processes. That is, what is described in this chapter is not limited to service request forecasting but can be easily applied to other city analytics projects, such as water usage analytics. Actually, they can be applied to various kinds of machine learning on various kinds of open data, including open data provided by universities and federal agencies, such as<a id="id453" class="indexterm"></a> that from the well-known LIGO project for gravitational wave detection and studies (for more information on this, refer to <a class="ulink" href="http://www.ligo.org/" target="_blank">http://www.ligo.org/</a> and <a class="ulink" href="http://www.researchmethods.org/AlexLiu_CalTech_Jan21.pdf" target="_blank">http://www.researchmethods.org/AlexLiu_CalTech_Jan21.pdf</a>).</p><p>In this chapter, we will cover the following topics:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Spark for service forecasting</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Easy computing with Spark</p></li></ul></div></li><li style="list-style-type: disc"><p>Methods of service forecasting</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Regression and time series</p></li></ul></div></li><li style="list-style-type: disc"><p>Data and feature preparation</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Data merging and feature selection</p></li></ul></div></li><li style="list-style-type: disc"><p>Model estimation</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Model estimation</p></li></ul></div></li><li style="list-style-type: disc"><p>Model evaluation</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>RMSE</p></li><li style="list-style-type: disc"><p>Results explanation</p></li><li style="list-style-type: disc"><p>Significant features and trends</p></li></ul></div></li><li style="list-style-type: disc"><p>Model deployment</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Rules and scoring</p></li></ul></div></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec72"></a>Spark for service forecasting</h2></div></div><hr /></div><p>In this section, we will<a id="id454" class="indexterm"></a> describe a real use case of predicting service requests in detail and then describe how to prepare Apache Spark computing <a id="id455" class="indexterm"></a>for this real-life project.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec111"></a>The use case</h3></div></div></div><p>In the United States, and <a id="id456" class="indexterm"></a>worldwide, more and more cities have made their collected data open to the public. As a result, city governments and many other organizations have performed machine learning on these open datasets with good insight into improving decision making and a lot of positive impact, for example, in New York and Chicago. Using large amount of open data is becoming a trend now. For example, using big data to measure cities is becoming a research trend, as we can note from <a class="ulink" href="http://files.meetup.com/11744342/CITY_RANKING_Oct7.pdf" target="_blank">http://files.meetup.com/11744342/CITY_RANKING_Oct7.pdf</a>.</p><p>Using data analytics<a id="id457" class="indexterm"></a> for cities has a wide impact as more than half of us live in urban centers now, and the percentage is still increasing. Therefore, what you will learn in this chapter will enable data scientists to create a huge positive impact.</p><p>Among all the open city datasets, 311 datasets are about service requests from citizens to the city government and are all publicly available, as listed on the following websites for the cities of New York, Los Angeles, Houston, and San Francisco:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<a class="ulink" href="https://nycopendata.socrata.com/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9" target="_blank">https://nycopendata.socrata.com/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="https://data.lacity.org/dataset/Clean-311/6y5f-2byv" target="_blank">https://data.lacity.org/dataset/Clean-311/6y5f-2byv</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://data.ohouston.org/dataset/city-of-houston-311-service-requests" target="_blank">http://data.ohouston.org/dataset/city-of-houston-311-service-requests</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="https://data.sfgov.org/City-Infrastructure/Case-Data-from-San-Francisco-311-SF311-/vw6y-z8j6" target="_blank">https://data.sfgov.org/City-Infrastructure/Case-Data-from-San-Francisco-311-SF311-/vw6y-z8j6</a>
</p></li></ul></div><p>These datasets are so rich in detail that many cities are interested in using them to forecast future<a id="id458" class="indexterm"></a> requests and measure effectiveness. One of our collaborators is tasked to use this data in combination with others to predict service needs for several cities, including Los Angeles and Houston, so that these cities can better allocate their resources accordingly.</p><p>Through some preliminary data analysis, the research group understands some of their data challenges as the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Data quality is not as good as expected; for example, there are a lot of missing cases</p></li><li style="list-style-type: disc"><p>Data accuracy is another issue to deal with</p></li><li style="list-style-type: disc"><p>Data exists in different silos that need to be merged together</p></li></ul></div><p>To deal with the challenges mentioned earlier, for this real project, we utilized some techniques presented in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Data Preparation for Spark ML</em></span>, to merge all the datasets together and also our Apache Spark technology to treat missing cases to create clean datasets for each city.</p><p>As a summary, the following table gives a brief description of these preprocessed datasets:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom"> </th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Time Period</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p># Requests</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>% Closed</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Top Agent</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>
<span class="strong"><strong>NYC</strong></span>
</p>
</td><td style="" align="left" valign="top">
<p>Sep 2012 ~ Jan 2014</p>
</td><td style="" align="left" valign="top">
<p>2,138,736</p>
</td><td style="" align="left" valign="top">
<p>75.3%</p>
</td><td style="" align="left" valign="top">
<p>HPD</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<span class="strong"><strong>SFO</strong></span>
</p>
</td><td style="" align="left" valign="top">
<p>July 2008 ~ Jan 2014</p>
</td><td style="" align="left" valign="top">
<p>910,573</p>
</td><td style="" align="left" valign="top">
<p>95.3%</p>
</td><td style="" align="left" valign="top">
<p>DPW</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<span class="strong"><strong>Los Angeles</strong></span>
</p>
</td><td style="" align="left" valign="top">
<p>Jan 2011 ~ June 30 2014</p>
</td><td style="" align="left" valign="top">
<p>2,713,630</p>
</td><td style="" align="left" valign="top">
<p>?</p>
</td><td style="" align="left" valign="top">
<p>LADBS</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<span class="strong"><strong>Houston</strong></span>
</p>
</td><td style="" align="left" valign="top">
<p>2012</p>
</td><td style="" align="left" valign="top">
<p>296,019</p>
</td><td style="" align="left" valign="top">
<p>98.2%</p>
</td><td style="" align="left" valign="top">
<p>PWE</p>
</td></tr></tbody></table></div><p>As we can note from the preceding table, we only used a part of the available open data, mainly for the reason of data completeness. In other words, we only used data from the period when we have enough datasets to merge and also the data quality for service requests is reasonable as per our initial preprocessing data.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec112"></a>Spark computing</h3></div></div></div><p>To set up Apache Spark for this project of service request forecasting, we will adopt a strategy similar to<a id="id459" class="indexterm"></a> that we used in <a class="link" href="#" linkend="ch08">Chapter 8</a>, <span class="emphasis"><em>Learning Analytics on Spark</em></span>, for the purpose of enforcing our learning. In other words, for the Apache Spark computing part, readers may use this chapter to review what was learned in <a class="link" href="#" linkend="ch08">Chapter 8</a>, <span class="emphasis"><em>Learning Analytics on Spark</em></span>, as well as what was learned in chapters 1 through 7.</p><p>As discussed in the Spark computing section of <a class="link" href="#" linkend="ch08">Chapter 8</a>, <span class="emphasis"><em>Learning Analytics on Spark</em></span>, you may choose one of the following approaches for our kind of projects:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Spark on Databrick's platform</p></li><li style="list-style-type: disc"><p>Spark on IBM DataScientistWorkbench</p></li><li style="list-style-type: disc"><p>SPSS on Spark</p></li><li style="list-style-type: disc"><p>Apache Spark with MLlib alone</p></li></ul></div><p>You learned all the <a id="id460" class="indexterm"></a>details of utilizing them in the previous chapters—that is, from <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>A Holistic View on Spark</em></span> to <a class="link" href="#" linkend="ch07">Chapter 7</a>, <span class="emphasis"><em>Recommendations on Spark</em></span>.</p><p>Either one of the four approaches mentioned before should work very well for this city analytics project, as described. Specifically, you may take the codes as developed in this chapter, put them into a separate notebook, and then implement the notebook with an approach mentioned previously.</p><p>With the strategy described here, similar to what we did in <a class="link" href="#" linkend="ch08">Chapter 8</a>, <span class="emphasis"><em>Learning Analytics on Spark</em></span>, we will touch on using one of the four approaches in the following section; however, will spend more effort on utilizing a Zeppelin notebook for users to learn more about the Zeppelin approach and to review the technologies described in <a class="link" href="#" linkend="ch08">Chapter 8</a>, <span class="emphasis"><em>Learning Analytics on Spark</em></span>.</p><p>To work with a Zeppelin notework approach, similarly to <a class="link" href="#" linkend="ch08">Chapter 8</a>, <span class="emphasis"><em>Learning Analytics on Spark</em></span>, we will start with the following page:</p><div class="mediaobject"><img src="graphics/B04883_09_01.jpg" /></div><p>Users can click on <span class="strong"><strong>Create new note</strong></span>, which is the first line under <span class="strong"><strong>Notebook</strong></span> on the left-hand side column, to start organizing code into the notebook.</p><p>Then, a box<a id="id461" class="indexterm"></a> will open to allow users to type in the notebook's name, and after typing in the name, users can click on <span class="strong"><strong>Create Note</strong></span> to create a new notebook:</p><div class="mediaobject"><img src="graphics/B04883_09_02.jpg" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec113"></a>Methods of service forecasting</h3></div></div></div><p>In the previous section, we described our use case of using open data to forecast service requests and also <a id="id462" class="indexterm"></a>prepared our Spark computing platform with Zeppelin notebooks as the focus. In our following 4E framework, as the next step in machine learning, we need to complete the task of mapping our use case to machine learning methods; that is, we need to select our analytical methods or predictive models (equations) for this project of predicting service requests with Big Data on Spark.</p><p>To model and predict service requests, there are many suitable models, including regression, decision tree, and time series. For this exercise, we will use both regression and time series modeling as time is a significant part of our data, and then, we will use evaluation to determine which one, or whether a combination, of them is the best. However, as regression is already utilized many times in the previous chapters and time series modeling may still be new to some of our readers, we will spend more time on describing and discussing the time series modeling methods.</p><p>For our clients of this project—some branches of the city government and civic organizations—the only concerns are whether the service request number will exceed certain levels<a id="id463" class="indexterm"></a> because problems will follow if so. For this problem, decision tree and random forest are the right methods. However, as an exercise for learning, our focus here will still be on regression and time series modeling because decision tree and random forest are covered many times in our previous chapters. From this method selection discussion, you will understand that we often need to employ a few modeling methods in order to meet clients' needs as well as to achieve best results.</p><p>As always, once we finalize our decision for analytical methods or models, we need to prepare the related dependent variable and also prepare for coding.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec114"></a>Regression models</h3></div></div></div><p>So far, you<a id="id464" class="indexterm"></a> must know that regression is among the most commonly used methods of prediction and has been utilized for various<a id="id465" class="indexterm"></a> projects so far.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl3sec17"></a>About regression</h4></div></div></div><p>As we discussed, there are two kinds of regression modeling that are suitable for various kinds of predictions: one is linear<a id="id466" class="indexterm"></a> regression and the other is<a id="id467" class="indexterm"></a> logistic regression. For this project, linear regression can be used when we take daily service request volume as our target variable, while logistic regression can be used if we want to predict whether or not a certain type of service is requested at a certain location during a certain time period.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl3sec18"></a>Preparing for coding</h4></div></div></div><p>For your <a id="id468" class="indexterm"></a>convenience in MLlib, we have the following code to be used for linear regression:</p><div class="informalexample"><pre class="programlisting">val numIterations = 90
val model = LinearRegressionWithSGD.train(TrainingData, numIterations)</pre></div><p>For logistic regression, we can use the following code:</p><div class="informalexample"><pre class="programlisting">val model = new LogisticRegressionWithSGD()
  .setNumClasses(2)</pre></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec115"></a>Time series modeling</h3></div></div></div><p>Our data for this project is of a<a id="id469" class="indexterm"></a> time series nature. Generally speaking, a time series is a sequence of data points that consists of the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Successive measurements made over a time interval</p></li><li style="list-style-type: disc"><p>A continuous time interval</p></li></ul></div><p>The distance in this time interval between any two consecutive data point is the same. For example, we have parking service requests made on a daily basis so that we have data with the following pattern:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /><col align="left" /><col align="left" /><col align="left" /><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Day 1</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Day 2</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Day 3</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Day 4</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Day 5</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Day 6</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Day 7</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Day 8</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>……</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>20 requests</p>
</td><td style="" align="left" valign="top">
<p>31 requests</p>
</td><td style="" align="left" valign="top">
<p>19 requests</p>
</td><td style="" align="left" valign="top">
<p>35 requests</p>
</td><td style="" align="left" valign="top">
<p>22 requests</p>
</td><td style="" align="left" valign="top">
<p>39 requests</p>
</td><td style="" align="left" valign="top">
<p>13 requests</p>
</td><td style="" align="left" valign="top">
<p>28 requests</p>
</td><td style="" align="left" valign="top">
<p>……</p>
</td></tr></tbody></table></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl3sec19"></a>About time series</h4></div></div></div><p>There are many models specially created to model time series data, such as the ARIMA model, for which algorithms are readily available in R or SPSS.</p><p>There are also<a id="id470" class="indexterm"></a> many introductory materials available to discuss using R to complete time series modeling; some of them are at <a class="ulink" href="http://www.stats.uwo.ca/faculty/aim/tsar/tsar.pdf" target="_blank">http://www.stats.uwo.ca/faculty/aim/tsar/tsar.pdf</a> or <a class="ulink" href="http://www.statoek.wiso.uni-goettingen.de/veranstaltungen/zeitreihen/sommer03/ts_r_intro.pdf" target="_blank">http://www.statoek.wiso.uni-goettingen.de/veranstaltungen/zeitreihen/sommer03/ts_r_intro.pdf</a>.</p><p>For our time series data of daily service requests, such as SFO data from 2008 to 2014, we plan to use two models: the <span class="strong"><strong>autoregressive moving average</strong></span> (<span class="strong"><strong>ARMA</strong></span>) <a id="id471" class="indexterm"></a>and <span class="strong"><strong>autoregressive integrated moving average</strong></span> (<span class="strong"><strong>ARIMA</strong></span>) models. Here, the ARMA model provides a parsimonious description of a (weekly) stationary stochastic process in terms of two polynomials: one for <a id="id472" class="indexterm"></a>the autoregression and the second for the moving average. The ARIMA model is a generalization of the ARMA model.</p><p>Both the ARMA and ARIMA models can provide a good forecast of future service requests. Whether the ARMA or ARIMA model is better will depend on our model evaluation using RMSE.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl3sec20"></a>Preparing for coding</h4></div></div></div><p>R has many <a id="id473" class="indexterm"></a>packages for time series modeling, such as the timeSeries or ts package.</p><p>When estimating the ARIMA model, we need to use the arima function with a code such as the following:</p><div class="informalexample"><pre class="programlisting"> fit1&lt;-arima(data1,order=c(1,0,1))</pre></div><p>Here, we used <code class="literal">c(1,0,1)</code> to specify orders for the ARIMA model.</p><p>As for MLlib, the algorithms for time series modeling are still in development. However, some libraries are being developed to facilitate time series modeling on Spark, such as the <code class="literal">spark-ts</code> library developed by Cloudera.</p><p>This library <a id="id474" class="indexterm"></a>allows users to preprocess data and then build some simple models as well as evaluate them. The code can be developed in Scala. However, as it is still in development, it is far behind what R can provide.</p><p>For an example<a id="id475" class="indexterm"></a> of using the <code class="literal">spark-ts</code> library for time series data modeling, go to <a class="ulink" href="http://blog.cloudera.com/blog/2015/12/spark-ts-a-new-library-for-analyzing-time-series-data-with-apache-spark/" target="_blank">http://blog.cloudera.com/blog/2015/12/spark-ts-a-new-library-for-analyzing-time-series-data-with-apache-spark/</a>.</p></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec73"></a>Data and feature preparation</h2></div></div><hr /></div><p>In the <span class="emphasis"><em>Feature extraction</em></span> section of <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Data Preparation for Spark ML</em></span>, we reviewed a few methods of feature extraction and discussed their implementation in Apache Spark. All the techniques<a id="id476" class="indexterm"></a> discussed there can be applied to our data here.</p><p>Besides<a id="id477" class="indexterm"></a> feature development, for this project, we will also need to spend a lot of effort in merging various datasets together to obtain more features.</p><p>Therefore, for this project, we actually need to conduct feature development, then data merging, and then feature selection, which is to utilize all the techniques discussed in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Data Preparation for Spark ML</em></span> and <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>A Holistic View on Spark</em></span>.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec116"></a>Data merging</h3></div></div></div><p>To obtain features for predicting, we need to add some external datasets, including weather data from<a id="id478" class="indexterm"></a> National Weather Service Forecast Office, events as well as calendar data from the Open Data portal, and socio-economic data for each zip code block from census data source.</p><p>In the, <span class="emphasis"><em>Joining data</em></span> section of <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Data Preparation for Spark ML</em></span>, we described methods to join data together with Spark SQL and other tools. All the techniques described there as well as the ones about identity matching and data cleaning techniques described in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Data Preparation for Spark ML</em></span>, can be used in this chapter.</p><p>As for this data merging task, the main focus includes, firstly, merging data on date per day, and, secondly, merging data on location per zip code. That is, first we will reorganize all the 311 requests' data into one dataset with features per day, which is to obtain the number of requests per day and other daily features. Then, the second task is similar; we will reorganize all the 311 requests' data into another dataset with features per location (here, per zip code), to obtain features such as the number of service requests per zip code. To learn how to reorganize datasets, readers may refer to the <span class="emphasis"><em>Data reorganizing</em></span> section in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Data Preparation for Spark ML</em></span>.</p><p>After we have created the two datasets mentioned previously, we will merge the first dataset with weather and calendar data and the second dataset with census data.</p><p>After merging with events data and calendar data, we will obtain new features for "whether holiday", special events, weekdays versus weekend, and others.</p><p>After merging <a id="id479" class="indexterm"></a>with weather data, we will obtain new features for rainy, snowy, average temperature, temperature range of the day, and other variables.</p><p>On the location side, we will work on the zip code level so that after merging with census data, we will obtain some new features about employment, income level, race, and others.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec117"></a>Feature selection</h3></div></div></div><p>Taking the New <a id="id480" class="indexterm"></a>York city 311 data as an example, we have more than 50 features in the data, which include information about the time requests that were made, locations for services, government agencies to whom the services request, the types of services requested, and the processing time for requests as well as the results of these requests.</p><p>After we merged location-related datasets and time-related datasets as described in the previous section, we will have more than 100 features ready to be used.</p><p>As for the feature selection for this project, we could follow what was used in <a class="link" href="#" linkend="ch08">Chapter 8</a>, <span class="emphasis"><em>Learning Analytics on Spark</em></span>, which is to utilize <span class="strong"><strong>principal component analysis</strong></span> (<span class="strong"><strong>PCA</strong></span>) and subject knowledge to group features and then apply machine learning for final feature selection. However, as an exercise, we will not repeat what was learned but will try something different. That is, we will let the machine learning algorithm pick up the features most useful in prediction.</p><p>In MLlib, we can <a id="id481" class="indexterm"></a>use the <code class="literal">ChiSqSelector</code> algorithm as follows:</p><div class="informalexample"><pre class="programlisting">// Create ChiSqSelector that will select top 25 of 400 features
val selector = new ChiSqSelector(25)
// Create ChiSqSelector model (selecting features)
val transformer = selector.fit(TrainingData)</pre></div><p>In R, we can use some R packages to make computation easy. Among the available packages, CARET is one of the commonly used packages.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec74"></a>Model estimation</h2></div></div><hr /></div><p>Once the feature sets get finalized in our last section, what follows is an estimation of all the parameters of the selected models, for which we adopted the approach of using MLlib on the Zeppeline notebook for this project and R notebooks in the Databricks environment because we need to estimate some regression and time series models.</p><p>Similarly to before, for the best modeling, we need to arrange distributed computing, especially for this<a id="id482" class="indexterm"></a> case with various kinds of services. In other words, we will estimate models to predict the daily volume of each kind of service request, which is for heating, construction-related, noise-related, parking-related, and other service requests.</p><p>In order to complete this task of estimating models for various service types, we need to group all the services into a set of service types. However, for this exercise, we just selected 50 top service types and then conducted parallel computing for model estimation for all these 50 services.</p><p>For this distributed computing part, readers may refer to the model estimation parts in previous chapters, as we will not repeat all the details here. Overall, as we discussed in the <span class="emphasis"><em>Methods of service forecasting</em></span> section of this chapter, we will focus on using two methods: regression and time series modeling. From what we have learned so far, for regression, we can complete the model estimation with Zeppelin notebooks on Spark using MLlib algorithms. As for time series modeling, it is better to use R so that we can implement them in the Databricks or IBM DataScientistWorkbench environment, for which we should refer to <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>A Holistic View on Spark</em></span> and <a class="link" href="#" linkend="ch05">Chapter 5</a>, <span class="emphasis"><em>Risk Scoring on Spark</em></span>.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec118"></a>Spark implementation with the Zeppelin notebook</h3></div></div></div><p>As discussed, to <a id="id483" class="indexterm"></a>utilize regression to predict the daily volume of service requests, we have the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Location-related <a id="id484" class="indexterm"></a>features, such as employment ratios</p></li><li style="list-style-type: disc"><p>Weather<a id="id485" class="indexterm"></a> features</p></li><li style="list-style-type: disc"><p>Event-related features</p></li></ul></div><p>In the <span class="emphasis"><em>Data and feature preparation</em></span> section, we had the data prepared; now, we need to divide them into a training and a test set. So, we can use the training set for model estimation.</p><p>In MLlib, for linear regression, we will use the following code:</p><div class="informalexample"><pre class="programlisting">val numIterations = 90
val model = LinearRegressionWithSGD.train(TrainingData, numIterations)</pre></div><p>For logistic regresssion, we will use the following code:</p><div class="informalexample"><pre class="programlisting">val model = new LogisticRegressionWithSGD()
  .setNumClasses(2)</pre></div><p>We need to input<a id="id486" class="indexterm"></a> the preceding<a id="id487" class="indexterm"></a> code into the Zeppelin<a id="id488" class="indexterm"></a> notebook as follows:</p><div class="mediaobject"><img src="graphics/B04883_09_03.jpg" /></div><p>Then we can press <span class="emphasis"><em>Shift</em></span> + <span class="emphasis"><em>Enter</em></span> to run the commands to obtain results, as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/B04883_09_04.jpg" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec119"></a>Spark implementation with the R notebook</h3></div></div></div><p>For time series<a id="id489" class="indexterm"></a> modeling, as discussed, we will use R notebooks within the Databricks environment similarly to what we did in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>A Holistic View on Spark</em></span>.</p><p>To do so, we can use the <a id="id490" class="indexterm"></a>Databricks environment's Job feature. Specifically, within the Databricks environment, we can go to <span class="strong"><strong>Jobs</strong></span> and create jobs, as<a id="id491" class="indexterm"></a> shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/B04883_09_05.jpg" /></div><p>Then users can select notebooks to run, specify clusters, and schedule jobs. Once scheduled, users can also monitor the running and then collect results back.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec75"></a>Model evaluation</h2></div></div><hr /></div><p>In the last section, we completed our model estimation. Now, it is the time for us to evaluate these estimated models to check whether they fit the city's criterions so that we can either move<a id="id492" class="indexterm"></a> to the results explanation or go back to some previous stages to refine our predictive models.</p><p>To perform our model evaluation, in this section, we will mainly use <span class="strong"><strong>root mean square error</strong></span> (<span class="strong"><strong>RMSE</strong></span>) to assess our models for both the regression and time series models. While other measures, such as MSE, can also be used to assess models, as an exercise, we will focus on RMSE as the processes of using other measures are similar.</p><p>When working on this real-life project, as mentioned in the <span class="emphasis"><em>Methods of service forecasting</em></span> section of this chapter, we also used decision tree and random forest models, for which we should use a confusion matrix and error ratios to evaluate. Here, we will not discuss these model evaluation methods as they are used a few times in the previous chapters, such as in <a class="link" href="#" linkend="ch04">Chapter 4</a>, <span class="emphasis"><em>Fraud Detection on Spark</em></span>.</p><p>Similarly to model estimation, to calculate RMSE, we need to use MLlib for regression modeling to be implemented with Zeppeline notebooks on Spark. For time series modeling, we will use R notebooks to be implemented in the Databricks environment of Spark.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec120"></a>RMSE calculation with MLlib</h3></div></div></div><p>In MLlib, we can<a id="id493" class="indexterm"></a> use the following code to<a id="id494" class="indexterm"></a> calculate RMSE:</p><div class="informalexample"><pre class="programlisting">val valuesAndPreds = test.map { point =&gt;
  val prediction = new_model.predict(point.features)
  val r = (point.label, prediction)
  r
}
val residuals = valuesAndPreds.map {case (v, p) =&gt; math.pow((v - p), 2)}
val MSE = residuals.mean();
val RMSE = math.pow(MSE, 0.5)</pre></div><p>Besides the preceding, MLlib<a id="id495" class="indexterm"></a> also has some functions in the <code class="literal">RegressionMetrics</code> and <code class="literal">RankingMetrics</code> classes for us to use for RMSE calculation.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec121"></a>RMSE calculation with R</h3></div></div></div><p>In R, the <code class="literal">forecast</code> package<a id="id496" class="indexterm"></a> has an <code class="literal">accuracy</code> function<a id="id497" class="indexterm"></a> that can be used to calculate forecasting accuracy as well as RMSE. Take a look at the<a id="id498" class="indexterm"></a> following:</p><div class="informalexample"><pre class="programlisting">accuracy(f, x, test=NULL, d=NULL, D=NULL)</pre></div><p>The measures calculated are:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>ME</strong></span> (<span class="strong"><strong>mean error</strong></span>)</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>RMSE</strong></span> (<span class="strong"><strong>root mean square error</strong></span>)</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>MAE</strong></span> (<span class="strong"><strong>mean absolute error</strong></span>)</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>MPE</strong></span> (<span class="strong"><strong>mean percentage error</strong></span>)</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>MAPE</strong></span> (<span class="strong"><strong>mean absolute percentage error</strong></span>)</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>MASE</strong></span> (<span class="strong"><strong>mean absolute scaled error</strong></span>)</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>ACF1</strong></span> (<span class="strong"><strong>autocorrelation of errors at lag 1</strong></span>)</p></li></ul></div><p>To perform a complete evaluation, what we did is to calculate RMSE for all the models we estimated. Then, we compared and picked up the ones with smaller RMSE.</p><p>For more information on <a id="id499" class="indexterm"></a>using the <code class="literal">forecast</code> R package, refer to the following URL:</p><p>
<a class="ulink" href="https://cran.r-project.org/web/packages/forecast/forecast.pdf" target="_blank">https://cran.r-project.org/web/packages/forecast/forecast.pdf</a>
</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec76"></a>Explanations of the results</h2></div></div><hr /></div><p>As before, after we have passed our model evaluation stage and decided to select the estimated and evaluated model as our final model, our next task is to interpret the results for the city management and technicians.</p><p>In terms of explaining the<a id="id500" class="indexterm"></a> machine learning results, the city is particularly interested in understanding what factors influence the service request number and how service requests change over time.</p><p>So, to serve the city governments and other interested civic organizations, we need to set our focus on further deriving results about big influencing variables and time series trends with our final models. Then, we need to work on interpretations as well as some visualizations as R provides many good visualization solutions.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec122"></a>Biggest influencers</h3></div></div></div><p>In terms of finding <a id="id501" class="indexterm"></a>out the features with the largest impact on the target feature, as you learned from the previous chapters, the random forest method is a good solution. Therefore, once our Zeppelin notebook is up, we should utilize some algorithms for <code class="literal">randomForest</code>, for which we, of course, need to recode our target feature into a binary one. Then, as we noted in previous chapters, such as in <a class="link" href="#" linkend="ch08">Chapter 8</a>, <span class="emphasis"><em>Learning Analytics on Spark</em></span>, the <code class="literal">randomForest</code> algorithm can give us a list of all the features as per their impacts on the target variable along with nice visualization graphs.</p><p>However, for this project, as we have a nice target feature variable with continuous values, the linear regression results also give us the needed insights directly. In other words, the features with larger coefficients in linear regression have a larger impact on the target feature. Another way of assessing predictors is to use the associated R squared, for which we usually go deep when we conduct feature selection. In other words, this task of finding out the biggest influencers may be performed together with the feature selection work, as described in <span class="emphasis"><em>Data and feature preparation</em></span> section of this chapter.</p><p>As per our results, overall, we will find events and holidays as big influencers, followed by weekday versus weekend, and finally followed by weather:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /></colgroup><tbody><tr><td style="" align="left" valign="top">
<p>1</p>
</td><td style="" align="left" valign="top">
<p>Events</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>2</p>
</td><td style="" align="left" valign="top">
<p>Holidays</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>3</p>
</td><td style="" align="left" valign="top">
<p>Weekend</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>4</p>
</td><td style="" align="left" valign="top">
<p>Weather</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>...</p>
</td><td style="" align="left" valign="top">
<p>...</p>
</td></tr></tbody></table></div><p>Both the linear regression models and ARIMA models confirm similar results.</p><p>As discussed before, in <a class="link" href="#" linkend="ch05">Chapter 5</a>, <span class="emphasis"><em>Risk Scoring on Spark</em></span>, R has many special packages for us to assess the predictive features and also visualize them, for which readers are encouraged to explore more.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec123"></a>Visualizing trends</h3></div></div></div><p>Users of our results, both from city governments and public user communities, are very interested in future trends; so, one of our important tasks is to visualize the trends as per our history data <a id="id502" class="indexterm"></a>and forecasts.</p><p>Consider this <a id="id503" class="indexterm"></a>example: we need to create a graph of the changing trends of heating-related service requests and noise-related service requests in NYC in 2013.</p><p>To produce this graph, we can use some simple code, as follows:</p><div class="informalexample"><pre class="programlisting">plot(Cmonth, HEATING_mean_month, main="% Heating and Noise Complains by Month", xlab="Month", ylab="% Complains", col="red")
points(Cmonth, Noise_mean, col="blue", pch=6)</pre></div><p>The following screenshot shows the output of the preceding code:</p><div class="mediaobject"><img src="graphics/B04883_09_06.jpg" /></div><p>The preceding graph produced by our R notebook illustrates the changes over time in 2013 for, firstly, the noise-related service requests as denoted by blue triangles, and, secondly, heating-related <a id="id504" class="indexterm"></a>service requests as denoted by red circles.</p><p>This graph shows a clear seasonal trend as there are a lot more requests for heating-related services in winter<a id="id505" class="indexterm"></a> than in summer. As shown by this graph, there are a lot more noise-related service requests in summer than in winter.</p><p>Here's another example: a graph of sanitation requests from July 2010 to January 2015 in Los Angeles:</p><div class="mediaobject"><img src="graphics/B04883_09_07.jpg" /></div><p>The preceding graph, also produced by our R notebook, shows a significant impact of seasons, but the trends<a id="id506" class="indexterm"></a> may not be easily interpreted.</p><p>Let's take a look at yet another example: mapping out the high water usage areas per zip code in Los Angeles between 2012 and 2013.</p><p>In recent years, Los Angeles has been experiencing a water shortage problem, so the city government and several citizen groups are very interested in understanding the overall water usage as well as the impact of some of their interventions.</p><p>The following graph, again produced by our R notebook, identifies the high water usage per zip code zones denoted by blue dots:</p><div class="mediaobject"><img src="graphics/B04883_09_08.jpg" /></div><p>For the preceding <a id="id507" class="indexterm"></a>visualization, we used the following code:</p><div class="informalexample"><pre class="programlisting">library(maps)
library(mapdata)
library(maptools)
library(scales)

map("worldHires", "usa", xlim=c(-119, -117), ylim=c(33.50, 34.50), col="gray95", fill=TRUE
l&lt;-abs(long)

long1&lt;--l
points(long1, lat, pch=19, col="red", cex=1)
points(long1[FY.12.13 &gt; mean(FY.12.13)], lat[FY.12.13 &gt; mean(FY.12.13)], pch=19, col="blue", cex=1)</pre></div><p>As we can note here, we used a few packages, including <code class="literal">mapdata</code>, <code class="literal">maptools</code>, and <code class="literal">scales</code> to produce this graph.</p><p>So, as we experienced so far, R has many packages for visualization as well as forecasting, especially for time series data modeling. However, more may come soon for Spark MLlib deployment</p><p>One of the main <a id="id508" class="indexterm"></a>purposes for this project is to produce good predictive models for the city to forecast the future service request volume on a daily basis per zip code using our developed regression models. This kind of forecasting is of value to various departments who use various kinds of software systems for decision making. As discussed before, MLlib<a id="id509" class="indexterm"></a> supports model export to <span class="strong"><strong>Predictive Model Markup Language</strong></span> (<span class="strong"><strong>PMML</strong></span>). Therefore, we will export some developed models to PMML for this project.</p><p>In practice, the users of this project are more interested in rule-based decision making to use some of our insights and also in score-based decision making to evaluate their regional units.</p><p>Specifically, as for this project, the client is interested in applying our results to, firstly, decide when an alert may be sent out if the number of service requests forecasted is very high, for which rules should be established, and, secondly, develop scores and use these scores to rank regions, such as by zip code zones, so that the city could use these rankings to measure performance as well as to plan for the future.</p><p>Besides the preceding, clients are also interested in forecasting the services in general using time series models, for which R actually has a package called <code class="literal">forecast</code> that is ready to be used:</p><div class="informalexample"><pre class="programlisting">forecast(fit)
plot(forecast(fit))</pre></div><p>To sum it up, for our special tasks, we need to turn some of our results into rules and to produce some performance scores for the client.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl3sec21"></a>The rules of sending out alerts</h4></div></div></div><p>As discussed before, for R results, there are several tools to help extracting rules out from developed predictive models.</p><p>For the decision<a id="id510" class="indexterm"></a> tree model developed to model, whether or not the number of service requests exceeds a certain level, we should use the <code class="literal">rpart.utils</code> R package, which can extract rules and export them in various formats, such as RODBC.</p><p>The <code class="literal">rpart.rules.table(model1)*</code> package returns an unpivoted table of variable values (factor levels) associated with each branch.</p><p>However, for this project, partially due to the issue of data incompleteness, we need to use some insights to derive rules directly. That is, we need to use the insights discussed in the last section. For example, we can do the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>If a special event is to be held, our prediction will show certain service requests going up dramatically, and an alert will be sent out</p></li><li style="list-style-type: disc"><p>If weather conditions change for a certain area, some special service request will go up so that an alert needs to be sent out</p></li></ul></div><p>From an analytical perspective, we face the same issue here to minimize false alerts while ensuring adequate warning.</p><p>The city government had a high false alert ratio from their past rules, and as a result of this, too many alerts were sent out that became a burden and also caused a lot of resource wasting.</p><p>Therefore, by taking advantage of Spark's fast computing, we carefully produced rules, and for each rule, we supplied false positive ratios that helped the company utilize the rules.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl3sec22"></a>Scores to rank city zones</h4></div></div></div><p>With our regression<a id="id511" class="indexterm"></a> and time series modeling in place, we have two ways to forecast the number of service requests for each zip code zone at any specific time in the future.</p><p>For time series modeling, as discussed, we can have the R package of <code class="literal">forecast</code> and use the following code:</p><div class="informalexample"><pre class="programlisting">forecast(fit)
plot(forecast(fit))</pre></div><p>With the regression models, we can use the estimated regression equations to perform the forecasting directly. Alternatively, we can use the following code:</p><div class="informalexample"><pre class="programlisting">forecast(fit, newdata=data.frame(City=30))</pre></div><p>Once we have the forecasted service request number in hand, one way of creating scores is using the request number divided by the maximum number.</p><p>As long as we <a id="id512" class="indexterm"></a>obtain the scores, we can classify all the zip code zones into several categories and also illustrate them on a map to identify special zones for attention, such as in the following graph:</p><div class="mediaobject"><img src="graphics/B04883_09_09.jpg" /></div></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec77"></a>Summary</h2></div></div><hr /></div><p>In this chapter, using a service request forecasting project, we went through a step-by-step process of utilizing big data to serve city governments as well as related civic organizations, from which we processed open data on Apache Spark and then built several models, including regression and time series ARIMA models to predict service demands. With this, we then developed rules for alerts and scores for zip code zone ranking to help cities prepare resources to measure effectiveness and also rank communities.</p><p>Specifically, we first selected a supervised machine learning approach with a focus on time series modeling per use case needs after we prepared Spark computing and loaded in preprocessed data. Secondly, we worked on data and feature preparation by merging a few datasets together and selecting a core set of features from hundreds of features. Thirdly, we estimated model coefficients using the Zeppelin notebook with MLlib and the R notebook on Databricks. Next, we evaluated these estimated models mainly using RMSE. Then, we interpreted our machine learning results with graphs to show trends and tables to show the biggest predictors. Finally, we deployed our machine learning results with a focus on scoring but also used insights to develop rules.</p><p>The preceding process is similar to the process as described in the previous chapters. However, in this chapter, we worked on times series modeling, which is a new method. This enabled us to be able to deal with the data with time dimensions and develop insight over time.</p><p>After this chapter, readers should have gained a better understanding of how Apache Spark could be utilized not only for commercial use but also for public use to serve cities and universities.</p><p>As this is our last chapter, readers may use this real-life project of service request forecasting to review all the modeling methods, such as regression and decision tree, as well as all the Spark computing platforms, such as the Zeppelin notebook with Spark and the R notebook for the Databricks environment. For this purpose, we discussed more methods and platforms in this chapter than in the earlier ones.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch10"></a>Chapter 10. Learning Telco Data on Spark</h2></div></div></div><p>With a new approach different from the approaches in the previous chapters, in this chapter and the next chapter, we will start with a set of huge amount of data and then let the data lead us. In other words, we will apply Spark machine learning to certain types of big datasets, and then the data needs and new insights will guide our machine learning to result in useful and actionable insights, by taking advantage of the processes made easily and fast with Apache Spark. As for this chapter, we will work on telco data, and then, for the next chapter, we will work on open data made available by various levels of governments.</p><p>By following a similar process adopted in the previous chapters, in this chapter, we will first review machine learning methods and related Spark computing to use Telco Data to learn more about customer behavior insights. We will then discuss how Apache Spark comes in to make them easy as before. At the same time, with this real-life use case of customer behavior insight discovery, we will also follow our 4E process of working on equations selection, estimation, evaluation, and explanation to illustrate our step-by-step machine learning process of segmenting customers and scoring customers with this big Telco Data.</p><p>However, as you are expected to have gained knowledge of Spark computing and the related tools, including R and SPSS, at this stage, we will jump around the 4Es as the machine learning needs. We will also not be limited by working only on one project or on one model. Specifically for this chapter, we will work to discover insights, score customers, and then build predictive models of the newly developed scores to go deeper in solving clients' problems.</p><p>Here, we will use a real-life project to illustrate our technologies and processes, with a computing focus on customer scoring and explanation of the scores. However, what is described here is not limited to score customers, but can also be easily applied to other machine learning projects, such as marketing effectiveness or service quality studies. We will cover the following topics in this chapter:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Spark for Telco Data learning</p></li><li style="list-style-type: disc"><p>Methods to learn from Telco Data</p></li><li style="list-style-type: disc"><p>Data and feature development</p></li><li style="list-style-type: disc"><p>Model estimation</p></li><li style="list-style-type: disc"><p>Model evaluation</p></li><li style="list-style-type: disc"><p>Results explanation</p></li><li style="list-style-type: disc"><p>Model deployment</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec78"></a>Spark for using Telco Data</h2></div></div><hr /></div><p>In this section, we will start with a real-life use case of learning from Telco Data and then describe how to <a id="id513" class="indexterm"></a>prepare Apache Spark computing for this real-life project of Telco Data machine learning.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec124"></a>The use case</h3></div></div></div><p>Telco companies in the United States and also in other regions have huge amounts of data in their hands now. Many telco companies have started considering this data as their most <a id="id514" class="indexterm"></a>valuable asset. They have started utilizing the data not only for their own data-driven decision making, but also for their clients. Specifically, some telco companies started using Big Data analytics to differentiate their offerings and target customers more effectively, thereby generating greater customer loyalty as well as taking advantage of new innovative business models. They also used data to increase operational efficiencies and improve effectiveness of customer-experience management. To serve their corporate clients, some telco companies have used the data to help segment customers in better ways to increase marketing effectiveness.</p><p>As for this exercise, the telco corp. VRS provided us with a big dataset to start with. The dataset contains call data and other basic information about their millions of subscribers.</p><p>However, the raw data is just a collection of many codes, with things such as 1bbddf1… to represent IDs and 73de6rd… to represent location. So, we would need to utilize some subject knowledge to make use of them and then develop new features from the raw data.</p><p>The telco company is<a id="id515" class="indexterm"></a> interested in learning any useful insights from the data. So, we were asked to explore any insights that could be learned from the data and then help them build some models to predict customer churn, Call Center calls, and purchase propensities, if possible. Once these scores get built, it is also helpful for the client to understand what affects these scores. So, the project is a very practical one. It is data driven and problems driven. We have strong interests to showcase our Apache Spark technologies, but the client is only interested in how the Apache Spark technologies can help discover new and useful insights faster and better.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec125"></a>Spark computing</h3></div></div></div><p>As discussed in the <span class="emphasis"><em>Spark computing</em></span> section of <a class="link" href="#" linkend="ch08">Chapter 8</a>, <span class="emphasis"><em>Learning Analytics on Spark</em></span>, you may choose one of the following approaches for our kind of projects: Spark on Databrick's platform, Spark on IBM Data Scientist Workbench, SPSS on Spark, or Apache Spark with MLlib alone. You have learned about all the details of utilizing them in the previous chapters, mainly from <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>A Holistic View on Spark</em></span> to <a class="link" href="#" linkend="ch07">Chapter 7</a>, <span class="emphasis"><em>Recommendations on Spark</em></span>.</p><p>Either one of the previously mentioned four approaches should work very well for this project of learning from <a id="id516" class="indexterm"></a>Telco Data. Especially, you may take the codes as developed in this chapter and put them into a separate notebook. You can then implement the notebook with an approach, as mentioned earlier. Using notebooks is preferred for all the approaches, except for SPSS on Spark.</p><p>As an exercise and also for the best to fit our data and project goals, we will focus on both the third approach and the fourth approach, which are to use SPSS on Spark and to utilize Apache Spark with MLlib. However, we will also use R notebook on Databrick's platform, as moving cleaned datasets around is not a difficult task.</p><p>To use SPSS on Spark, we will need IBM SPSS Modeler 17.1 and IBM SPSS Analytics Server 2.1, which has a good integration with Apache Spark. The following screenshot shows<a id="id517" class="indexterm"></a> the SPSS Modeler for MLlib node creation:</p><div class="mediaobject"><img src="graphics/B04883_10_01.jpg" /></div><p>Through this good integration, data science users of SPSS Modeler can now create new Modeler nodes to exploit MLlib algorithms and share them so that we can combine the third and fourth approaches to implement them.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec79"></a>Methods for learning from Telco Data</h2></div></div><hr /></div><p>In the previous section, we described our use case of learning customer insights from big Telco Data <a id="id518" class="indexterm"></a>with a new dynamic approach and also prepared our Spark computing platform with <a id="id519" class="indexterm"></a>SPSS on Spark and MLlib as the focus. By following the same process adopted in the previous chapters, as the next step for our machine learning, we now need to complete the task of mapping our use case to machine learning methods. We need to select our analytical methods or predictive models (equations) for this project of scoring customers with big data on Spark. Even during our machine learning, we may need to jump back to this stage. We shall still fully complete this stage and prepare all the needed knowledge and codes for jumping back here easily during our machine learning process.</p><p>As for the modeling part of our learning, that is, to model customer behavior with Big Data, which is either to depart or to call services, or to purchase for our case here, there are many suitable methods, including regression and decision tree. For this exercise, we will use both regression and decision tree. However, as both regression and decision tree have already been utilized many times in the previous chapters, we will spend more time to ensemble the estimated models for this chapter.</p><p>However, besides <a id="id520" class="indexterm"></a><a id="id521" class="indexterm"></a>developing <a id="id522" class="indexterm"></a>models for predictions, we are also asked for some exploratory work in which we can use some machine learning for descriptive statistics and insight visualizations.</p><p>As always, once we finalize our decision for analytical methods or models, we will need to prepare for coding, in MLlib, in SPSS, and in R, for this project.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec126"></a>Descriptive statistics and visualization</h3></div></div></div><p>As one of our main tasks for this project is to explore the available data for useful insights, we will need to use <a id="id523" class="indexterm"></a>descriptive statistics <a id="id524" class="indexterm"></a>and visualization tools from R, SPSS, or MLlib. One of the most basic methods for data exploration is to build cross-tabulation tables, for which we can use the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The table function in R</p></li><li style="list-style-type: disc"><p>CROSSTAB in SPSS</p></li><li style="list-style-type: disc"><p>colStats in MLlib</p></li></ul></div><p>As for visualizations, R has better plot functions for us to use from its ggplots package.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec127"></a>Linear and logistic regression models</h3></div></div></div><p>For this project, in terms of creating predictions, we have two target variables: a binary variable about customer churn and a numeric variable about the number of calls made to the <a id="id525" class="indexterm"></a>Call Center. As an exercise, we can build linear regression to predict Call Center calls and <a id="id526" class="indexterm"></a>logistic regression models for customer churn.</p><p>So far, you must know very well that regression is most commonly used for prediction and has been utilized for various projects in this book so far.</p><p>For your convenience, in MLlib, for linear regression, we can use the following code:</p><div class="informalexample"><pre class="programlisting">val numIterations = 90
val model = LinearRegressionWithSGD.train(TrainingData, numIterations)</pre></div><p>For logistic regression, we can use a line of different code as follows:</p><div class="informalexample"><pre class="programlisting">val model = new LogisticRegressionWithSGD()
  .setNumClasses(2)</pre></div><p>In R, we can use<a id="id527" class="indexterm"></a> the <code class="literal">glm</code> function <a id="id528" class="indexterm"></a>of generalized linear regression to implement logistic regression, and the <code class="literal">lm</code> function to implement linear regression, while SPSS Modeler has a regression node for us to use.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec128"></a>Decision tree and random forest</h3></div></div></div><p>Both decision tree and <a id="id529" class="indexterm"></a>random forest aim to model ways of classifying cases into departed <a id="id530" class="indexterm"></a>or not departed for our use case, in sequence with the results to be illustrated by trees.</p><p>Specifically, decision tree modeling uses a sequence of branching operations based on comparisons of some quantities such as number of calls and quality of services to illustrate the impact of predictors, which is relatively ease of use, robustness with missing data, and ease of interpretability, in comparison to regression modeling. Robustness with missing data has a big advantage for this use case, as data incompleteness is one of the biggest issues here.</p><p>Random forest comes from a set of trees, with good functions to produce scores and to rank predictors by their impact on target variables, which will be useful for us to help identify interventions to reduce subscriber churn. However, mean results of hundreds of trees somehow cover details so that a decision tree explanation can still be very intuitive and valuable. It is considered better than random forest.</p><p>Just like we did earlier, within MLlib, we can use the following code:</p><div class="informalexample"><pre class="programlisting">val numClasses = 2
val categoricalFeaturesInfo = Map[Int, Int]()
val impurity = "gini"
val maxDepth = 6
val maxBins = 32
val model = DecisionTree.trainClassifier(trainingData, numClasses, categoricalFeaturesInfo, impurity, maxDepth, maxBins)</pre></div><p>For random forest and in MLlib, we can use the following code:</p><div class="informalexample"><pre class="programlisting">// To train a RandomForest model.
val treeStrategy = Strategy.defaultStrategy("Classification")
val numTrees = 300
val featureSubsetStrategy = "auto" // Let the algorithm choose.
val model = RandomForest.trainClassifier(trainingData,
  treeStrategy, numTrees, featureSubsetStrategy, seed = 12345)</pre></div><p>More guidance about coding<a id="id531" class="indexterm"></a> for decision tree can be found at <a class="ulink" href="http://spark.apache.org/docs/latest/mllib-decision-tree.html" target="_blank">http://spark.apache.org/docs/latest/mllib-decision-tree.html</a> and for <a id="id532" class="indexterm"></a>random forest at <a class="ulink" href="http://spark.apache.org/docs/latest/mllib-ensembles.html" target="_blank">http://spark.apache.org/docs/latest/mllib-ensembles.html</a>.</p><p>In R, we used<a id="id533" class="indexterm"></a> the R package <a id="id534" class="indexterm"></a>of <code class="literal">randomForest</code> and <code class="literal">rpart</code> to implement random forest and decision tree modeling with code similar to the following:</p><div class="informalexample"><pre class="programlisting">library(randomForest)
library(rpart)

Model2 &lt;- randomForest(default ~ ., data=train, importance=TRUE, ntree=2000)
Model3 &lt;- rpart(default ~ ., data=train)</pre></div><p>For SPSS, the SPSS Modeler has a tree node and a random forest extension for us to use.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec80"></a>Data and feature development</h2></div></div><hr /></div><p>In the <span class="emphasis"><em>Feature extraction</em></span> section of <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Data Preparation for Spark ML</em></span>, we have reviewed a few methods for feature extraction and discussed their implementation on Apache Spark. All the <a id="id535" class="indexterm"></a>techniques discussed there will be applied to our datasets here.</p><p>Besides feature development, for this project, we will also need to spend a lot of effort to merge various datasets together to obtain more features.</p><p>Therefore, for this project, we actually need to conduct feature development, then data merging and reorganizing, and then feature selection, which is to utilize all the techniques discussed in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Data Preparation for Spark ML</em></span>, and also in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>A Holistic View on Spark</em></span>. A lot of work has been completed to produce several good datasets for this big project, with the techniques described earlier.</p><p>As an exercise, we will focus on some of the key tasks, which are to reorganize data per day, then merge datasets, and finally conduct feature selection to obtain a good set of features for machine learning.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec129"></a>Data reorganizing</h3></div></div></div><p>To obtain more and <a id="id536" class="indexterm"></a>good features to predict and use the data to serve the clients of the telco company, we need to add some external datasets, including customer purchase data and some open data.</p><p>In the <span class="emphasis"><em>Joining data</em></span> section of <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Data Preparation for Spark ML</em></span>, we have described methods to join data with Spark SQL and other tools. All the techniques described there as well as the ones about identity matching and data cleaning techniques described in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Data Preparation for Spark ML</em></span> could be used here.</p><p>As for this data-reorganizing task, the main focus in here includes (1) to aggregate data on date per day and (2) to aggregate data on location per zip code as well as per location types. That is, first, we reorganize all data into one dataset with features per day, which is to obtain the number of calls per day and other daily features. Then, the second task is similar, but to reorganize all the data into another dataset with features per location, here per zip code. It means to obtain features such as number of calls per zip code. About how to reorganize datasets, you may refer to the <span class="emphasis"><em>Data reorganizing</em></span> section of <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Data Preparation for Spark ML</em></span>.</p><p>Specifically, all the tools to be used have good functions for data aggregation.</p><p>SPSS has a function of <code class="literal">aggregate</code>, for which we just need to specify the date or location as a break and specify the sum or mean as the function to create new data.</p><p>R also has a function of <code class="literal">aggregate</code>, for which we will need to use <code class="literal">by</code> to specify the date as a break and then use <code class="literal">FUN</code> to specify the function to create new data.</p><p>After we have these two datasets created, we can merge the first dataset with customer data.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec130"></a>Feature development and selection</h3></div></div></div><p>After we merged location-related datasets and time-related datasets, as described in the previous section, we have more than 100 features ready to be used.</p><p>As for the <a id="id537" class="indexterm"></a>feature selection for this project, we could follow what we used for <a class="link" href="#" linkend="ch08">Chapter 8</a>, <span class="emphasis"><em>Learning Analytics on Spark</em></span>, which is to utilize (<span class="strong"><strong>Principal Component Analysis</strong></span> (<span class="strong"><strong>PCA</strong></span>) and <a id="id538" class="indexterm"></a>also to use subject knowledge to group features, and then apply machine learning for final feature selection. However, as an exercise, you will not repeat what you learned. We will try something different. That is, we will let the machine learning algorithms pick up the features most useful in prediction.</p><p>In MLlib, we can use the <code class="literal">ChiSqSelector</code> algorithm as follows:</p><div class="informalexample"><pre class="programlisting">// Create ChiSqSelector that will select top 25 of 400 features
val selector = new ChiSqSelector(25)
// Create ChiSqSelector model (selecting features)
val transformer = selector.fit(TrainingData)</pre></div><p>In R, we can use <a id="id539" class="indexterm"></a>some R packages to make computation easy. Among the available packages, CARET is one of the commonly used packages.</p><p>With all the work described in the preceding sections, <span class="emphasis"><em>Data reorganizing</em></span> and <span class="emphasis"><em>Feature development and selection</em></span>, is done, we end with a dataset with the following features to be used:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Basic info</strong></span>: <code class="literal">location – state</code>, <code class="literal">account service length</code>, <code class="literal">area code</code>, <code class="literal">phone number</code>, <code class="literal">phone mftr</code>, <code class="literal">international call plan</code>, and <code class="literal">voice mail plan</code>
</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Usage info</strong></span>: <code class="literal">number vmail messages daily</code>, <code class="literal">total day minutes</code>, <code class="literal">total day calls</code>, <code class="literal">total calls dropped</code>, <code class="literal">total day charge</code>, <code class="literal">total eve minutes</code>, <code class="literal">total eve calls</code>, <code class="literal">total eve charge</code>, <code class="literal">total night minutes</code>, <code class="literal">total night calls</code>, <code class="literal">total night charge</code>, <code class="literal">total intl minutes</code>, <code class="literal">total intl calls</code>, t<code class="literal">otal intl charge</code>, <code class="literal">number Call Center calls</code>, and <code class="literal">call locations</code>
</p></li></ul></div><p>Especially for our study, we also have a special feature that is about whether or not the subscriber churned, which will be the essential target variable for our core supervised machine learning. In the preceding list of features, the second to the last is <code class="literal">number Call Center calls</code>, which will be also used as a target variable for some of our supervised machine learning.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec81"></a>Model estimation</h2></div></div><hr /></div><p>Once feature sets get finalized in our last section, what follows is to estimate all the parameters of the <a id="id540" class="indexterm"></a>selected models, for which we have adopted an approach of using SPSS on Spark and also R notebooks in the Databricks environment, plus MLlib directly on Spark. However, for the purpose of organizing workflows better, we focused our effort on organizing all the codes into R notebooks and also coding SPSS Modeler nodes.</p><p>For this project, as mentioned earlier, we will also conduct some exploratory analysis for descriptive statistics and for visualization, for which we can take the MLlib codes and get them implemented directly. Also, with R codes, we obtained quick and good results.</p><p>For the best modelling, we need to arrange distributed computing, especially for this case, with various locations in combination with various customer segments.</p><p>For this distributed computing part, you need to refer to previous chapters, and we will use SPSS Analytics Server with Apache Spark as well as Databrick's environment.</p><p>As we discussed in the <span class="emphasis"><em>Methods for service forecasting</em></span> section, we will use two methods, regression <a id="id541" class="indexterm"></a>and decision tree, for the supervised machine learning part. From what you learned so far, for regression, you can complete the model estimation either with SPSS or with R. As for some modelling with random forest, it is better to use R so that you can implement the methods with R codes in the Databricks environment, for which we should refer to <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>A Holistic View on Spark</em></span> and <a class="link" href="#" linkend="ch05">Chapter 5</a>, <span class="emphasis"><em>Risk Scoring on Spark</em></span>.</p><p>With the feature list we have from the previous section, we have our target variables of whether or not the subscriber departed and also another target variable of Call Center calls. When modelling subscriber churn, the variable of Call Center calls will also be used as one of the predictors.</p><p>For decision tree modelling, the following code in R is what is needed, as we need some R packages:</p><div class="informalexample"><pre class="programlisting">library(languageR)
library(rms)
library(Party)

data.controls &lt;- cforest_unbiased(ntree=1000, mtry=3)
set.seed(47)
data.cforest &lt;- cforest(CustomerChurn ~ x + y + z…, data = mob_churn, controls=data.controls)</pre></div><p>Besides, we used R in the Databricks environment. At the same time, we used the SPSS Modeler to estimate these predictive models, for which we need to use the SPSS Analytics Server. The following screenshot shows the SPSS Modeler with nodes developed:</p><div class="mediaobject"><img src="graphics/B04883_10_02.jpg" /></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec131"></a>SPSS on Spark – SPSS Analytics Server</h3></div></div></div><p>The IBM SPSS Modeler 17.1 and Analytic Server 2.1 offer easy integration with Spark, which allows <a id="id542" class="indexterm"></a>us to easily<a id="id543" class="indexterm"></a> implement the data and modeling streams built so far:</p><div class="mediaobject"><img src="graphics/B04883_10_03.jpg" /></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec82"></a>Model evaluation</h2></div></div><hr /></div><p>In the last section, we summarized what is needed to complete our model estimation for our supervised machine learning. Now it is time for us to evaluate these estimated models to see if they fit the client's criterions so that we can either move to the results explanation stage or go back to some previous stages to refine our predictive models.</p><p>To perform our <a id="id544" class="indexterm"></a>model evaluation, in this section, we will need to use <span class="strong"><strong>Root Mean Square Error</strong></span> (<span class="strong"><strong>RMSE</strong></span>) to <a id="id545" class="indexterm"></a>assess our linear regression models of predicting Call Center calls, and use confusion matrix to assess our logistic regression model of predicting customer churn, for which the following numbers are often preferred:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>True Positive (TP)</strong></span>: Label is positive and prediction is also positive</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>True Negative (TN)</strong></span>: Label is negative and prediction is also negative</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>False Positive (FP)</strong></span>: Label is negative but prediction is positive</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>False Negative (FN)</strong></span>: Label is positive but prediction is negative</p></li></ul></div><p>Here, positive means the subscriber departed, and negative means the subscriber stayed.</p><p>The preceding four numbers are the building blocks for most classifier-evaluation metrics. A fundamental point in considering classifier evaluation is that pure accuracy (that is, was the prediction correct or incorrect) is not necessarily a best one, as a dataset could be highly unbalanced. For example, if a model is designed to predict fraud from a dataset where 95 percent of the data points are not fraud and 5 percent of the data points are fraud, then a naive classifier that predicts all as not fraud, regardless of the input, will be 95 percent accurate. For our case, the churn ratio is also not very high. For this reason, metrics for precision (positive predictive value) and recall (sensitivity) are typically used because they take into account the type of error. In other words, some balance between precision and recall is needed, which can be captured by combining the two into a single metric called the F-measure that will be calculated here with MLlib.</p><p>Just like for model estimation, to calculate RMSEs and to produce a confusion matrix, we need to use MLlib for regression modelling to be implemented with Apache Spark, which, as an exercise, can also be implemented with SPSS on Spark, when we take the MLlib codes to form a SPSS Modeler node. For logistic regression modelling, we will use R notebooks to be implemented in the Databricks environment for Apache Spark. In practice, we tried both MLlib and R for both calculating RMSE and calculating error ratios, because one of the main purposes for this project is to go beyond the limits of our tools of R and MLlib.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec132"></a>RMSE calculations with MLlib</h3></div></div></div><p>In MLlib, we <a id="id546" class="indexterm"></a>can use the following codes to calculate RMSE:</p><div class="informalexample"><pre class="programlisting">val valuesAndPreds = test.map { point =&gt;
  val prediction = new_model.predict(point.features)
  val r = (point.label, prediction)
  r
}
val residuals = valuesAndPreds.map {case (v, p) =&gt; math.pow((v - p), 2)}
val MSE = residuals.mean();
val RMSE = math.pow(MSE, 0.5)</pre></div><p>Besides the preceding code, MLlib also has some functions in the <code class="literal">RegressionMetrics</code> and <code class="literal">RankingMetrics</code> classes for us to use for the RMSE calculation.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec133"></a>RMSE calculations with R</h3></div></div></div><p>In R, the <code class="literal">forecast</code> package has an <code class="literal">accuracy</code> function that can be used to calculate forecasting <a id="id547" class="indexterm"></a>accuracy as well as RMSEs:</p><div class="informalexample"><pre class="programlisting">accuracy(f, x, test=NULL, d=NULL, D=NULL)</pre></div><p>The measures calculated are:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>ME</strong></span> (<span class="strong"><strong>Mean Error</strong></span>)</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>RMSE</strong></span> (<span class="strong"><strong>Root Mean Squared Error</strong></span>)</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>MAE</strong></span> (<span class="strong"><strong>Mean Absolute Error</strong></span>)</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>MPE</strong></span> (<span class="strong"><strong>Mean Percentage Error</strong></span>)</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>MAPE</strong></span> (<span class="strong"><strong>Mean Absolute Percentage Error</strong></span>)</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>MASE</strong></span> (<span class="strong"><strong>Mean Absolute Scaled Error</strong></span>)</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>ACF1</strong></span> (<span class="strong"><strong>Autocorrelation of errors at lag 1</strong></span>)</p></li></ul></div><p>To perform a complete evaluation, we need to calculate RMSEs for all the models we estimated. Then, we compare and pick up the ones with smaller RMSEs.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec134"></a>Confusion matrix and error ratios with MLlib and R</h3></div></div></div><p>In MLlib, we can <a id="id548" class="indexterm"></a>use the <a id="id549" class="indexterm"></a>following code to calculate the error ratios:</p><div class="informalexample"><pre class="programlisting">// F-measure
val f1Score = metrics.fMeasureByThreshold
f1Score.foreach { case (t, f) =&gt;
  println(s"Threshold: $t, F-score: $f, Beta = 1")
}

val beta = 0.5
val fScore = metrics.fMeasureByThreshold(beta)
f1Score.foreach { case (t, f) =&gt;
  println(s"Threshold: $t, F-score: $f, Beta = 0.5")
}</pre></div><p>In R, we have the following code to produce a confusion matrix, which can be included in our R notebook for implementation:</p><div class="informalexample"><pre class="programlisting">model$confusion</pre></div><p>With what is described in the preceding paragraph, we selected our final linear regression models for Call Center calls prediction and our final logistic regression models for subscriber churns.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec83"></a>Results explanation</h2></div></div><hr /></div><p>After we passed our model-evaluation stage and decided to select the estimated and evaluated model as our final models, our next task is to interpret the results to the telco company and their clients.</p><p>In terms of explaining the <a id="id550" class="indexterm"></a>machine learning results, the telco company is particularly interested in understanding what influences the Call Center call volume as well as what impacts the subscriber churn. Of course, they are also open to other special insights.</p><p>We will work on these tasks, with our focus on big influencing features and some special insights.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec135"></a>Descriptive statistics and visualizations</h3></div></div></div><p>With R or SPSS on Spark, as well as MLlib in place, one advantage is to obtain analytical results fast. So, quickly, we have obtained the following insights as summarized by the following tables.</p><p>For subscriber <a id="id551" class="indexterm"></a>churn, we <a id="id552" class="indexterm"></a>have the following two tables that summarize the subscriber churn ratios, per their phone manufactures and per our market segments of six main categories. Producing some customer segmentations is another task performed for the telco company, but we will not discuss this in detail as per the limitation of this book. You may just consider this as one of the taken features. The following table shows the subscriber churn ratios per phone manufactures:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>:MFTR</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Churn Rate</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>A</p>
</td><td style="" align="left" valign="top">
<p>.09</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>H</p>
</td><td style="" align="left" valign="top">
<p>.11</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>L</p>
</td><td style="" align="left" valign="top">
<p>.11</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>M</p>
</td><td style="" align="left" valign="top">
<p>.12</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>N</p>
</td><td style="" align="left" valign="top">
<p>.08</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>R</p>
</td><td style="" align="left" valign="top">
<p>.10</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>S</p>
</td><td style="" align="left" valign="top">
<p>.10</p>
</td></tr></tbody></table></div><p>The following table shows the subscriber churn ratios per market segment:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Segment</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Churn Rate</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>DG1</p>
</td><td style="" align="left" valign="top">
<p>.09</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>DG2</p>
</td><td style="" align="left" valign="top">
<p>.05</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>HB</p>
</td><td style="" align="left" valign="top">
<p>.13</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>NS</p>
</td><td style="" align="left" valign="top">
<p>.10</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>NP</p>
</td><td style="" align="left" valign="top">
<p>.10</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>UN</p>
</td><td style="" align="left" valign="top">
<p>.10</p>
</td></tr></tbody></table></div><p>For Call Center calls, we have the following two tables that summarize the average calls made by subscribers, per their phone manufactures and per our market segments. The<a id="id553" class="indexterm"></a> following<a id="id554" class="indexterm"></a> table shows the average calls made by the subscribers per phone manufactures:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>MFTR</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Average Call Center Calls</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>A</p>
</td><td style="" align="left" valign="top">
<p>1.26</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>H</p>
</td><td style="" align="left" valign="top">
<p>1.11</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>L</p>
</td><td style="" align="left" valign="top">
<p>0.89</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>M</p>
</td><td style="" align="left" valign="top">
<p>1.03</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>N</p>
</td><td style="" align="left" valign="top">
<p>0.88</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>R</p>
</td><td style="" align="left" valign="top">
<p>1.30</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>S</p>
</td><td style="" align="left" valign="top">
<p>1.00</p>
</td></tr></tbody></table></div><p>The following table shows the average calls made by the subscribers per market segment:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Segment</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Average Call Center Calls</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>DG1</p>
</td><td style="" align="left" valign="top">
<p>1.13</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>DG2</p>
</td><td style="" align="left" valign="top">
<p>2.86</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>HB</p>
</td><td style="" align="left" valign="top">
<p>0.50</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>NS</p>
</td><td style="" align="left" valign="top">
<p>2.31</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>NP</p>
</td><td style="" align="left" valign="top">
<p>1.12</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>UN</p>
</td><td style="" align="left" valign="top">
<p>1.52</p>
</td></tr></tbody></table></div><p>Furthermore, our results also map out stores per churn rate or per Call Center calls. Here is an example:</p><div class="mediaobject"><img src="graphics/B04883_10_04.jpg" /></div><p>For this <a id="id555" class="indexterm"></a>mapping <a id="id556" class="indexterm"></a>task, we have used the R code.</p><p>The following is an example of R codes used to visualize store distribution:</p><div class="informalexample"><pre class="programlisting">library(maps)
library(mapdata)
library(maptools)
library(scales)

map("worldHires", "usa", xlim=c(-120, -70), ylim=c(25, 55), col="gray95", fill=TRUE)
points(lon, lat, pch=19, col="red", cex=1)</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec136"></a>Biggest influencers</h3></div></div></div><p>In terms of finding out the <a id="id557" class="indexterm"></a>features with the largest impact on the target features of subscriber churn and Call Center calls, once our Spark computing is up, we can easily utilize algorithms for randomForest. Then, as we saw in <a class="link" href="#" linkend="ch08">Chapter 8</a>, <span class="emphasis"><em>Learning Analytics on Spark</em></span>, the randomForest algorithm can give us a list of all the features per their impact on the target variable and with nice visualization graphs.</p><p>However, for this project, with Call Center calls as a good target feature variable with continuous values, the linear regression results give us the insights directly. In other words, the features with larger coefficients in the linear regression have a larger impact on the target feature. Another way of assessing predictors is to use the associated R squared, which we also used when we conducted feature selection. In other words, this task may be performed together with the feature selection work as described in the <span class="emphasis"><em>Data and feature development</em></span> section.</p><p>However, for impact on subscriber churns, we have used randomForest results, for which we have the following list of the five largest predictors in order:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Call Center calls</p></li><li style="list-style-type: disc"><p>Quality of services</p></li><li style="list-style-type: disc"><p>Usage</p></li><li style="list-style-type: disc"><p>Manufacturer</p></li><li style="list-style-type: disc"><p>Customer segments</p></li></ul></div><p>With the preceding results, it is easy and also not surprising to see the impact of Call Center calls as the biggest, which also indicates to the telco company about where they need to intervene to reduce subscriber churns.</p><p>For Call Center calls, we have the following list of the four largest predictors in order:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Quality of services</p></li><li style="list-style-type: disc"><p>Usage</p></li><li style="list-style-type: disc"><p>Manufacturer</p></li><li style="list-style-type: disc"><p>Segments</p></li></ul></div><p>Per the preceding results, the main drivers of Call Center calls are service quality and call usage, with actually the interaction of these two that needs to be further explored.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec137"></a>Special insights</h3></div></div></div><p>As we see from the <a id="id558" class="indexterm"></a>preceding section, quality of services has a very big impact on both customer churn and also on Call Center calls.</p><p>Therefore, the client is very interested in learning more about the relationship between QoS and churn, for which we use R to visualize their relationship. We found that there is more customer churn in the middle values of QoS.</p><p>This result may reflect a non-linear relationship between the two, and in our opinion, this calls for more data on the location's social and economic characteristics and about competition for us to explore the relationship deeper.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec138"></a>Visualizing trends</h3></div></div></div><p>With our Spark computing in place, a lot of visualization can be produced, especially with R in<a id="id559" class="indexterm"></a> place. The following image is one example. Here, data transforming success ratios over a year has been plotted to show service quality changes over the course of the year:</p><div class="mediaobject"><img src="graphics/B04883_10_05.jpg" /></div><p>The following image shows the SMS success rate in 2012:</p><div class="mediaobject"><img src="graphics/B04883_10_06.jpg" /></div><p>For this <a id="id560" class="indexterm"></a>work, we used the following R code:</p><div class="informalexample"><pre class="programlisting">library(lubridate)
Rtime&lt;-ymd(day1)
plot(Rtime[event_type == "SMS"],
event_success_mean[event_type == "SMS"], col="red",
main="SMS Success Rate in 2012",
xlab="Sep to Dec 2012", ylab="Average SMS Success Rate"
lines(Rtime[event_type == "SMS"],
event_success_mean[event_type == "SMS"],
col="red", main="SMS Success Rate in 2012",
xlab="Sep to Dec 2012", ylab="Average SMS Success Rate")
plot(Rtime[event_type == "Data"],
event_success_mean[event_type == "Data"],
col="red", main="Data Success Rate in 2012",
xlab="Sep to Dec 2012", ylab="Average Data Success Rate")
lines(Rtime[event_type == "Data"],
event_success_mean[event_type == "Data"], col="red")</pre></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec84"></a>Model deployment</h2></div></div><hr /></div><p>One of the main purposes of this project is to produce good predictive models for the telco company to forecast Call Center calls on a daily basis and also to understand or even reduce subscriber churn, besides producing insights for some of the clients of this telco company. As discussed earlier, MLlib supports model export to <span class="strong"><strong>Predictive Model Markup Language</strong></span> (<span class="strong"><strong>PMML</strong></span>). Therefore, we export some developed models to PMML for this <a id="id561" class="indexterm"></a>project so that the telco company can take them to integrate with their existing analytical and <a id="id562" class="indexterm"></a>decision-making platforms.</p><p>In practice, the users for this project, executives of the telco company, are more interested in rule-based decision making to use some of our insights and also in score-based decision making to impact subscriber churns.</p><p>Specifically, as for this project, the client is interested in applying our results to (1) decide when an alert may be sent out if the number of service requests as forecasted will be very high, for which rules should be established, and (2) develop scores and use these scores to rank stores so that the company could use these rankings to measure performance as well as to intervene to get stores to work in reducing subscriber churns, besides changing the Call Center behavior.</p><p>To sum up, as for our special tasks, we need to turn some of our results into some rules and also need to produce some good scores for the telco company. To serve the clients of this telco company, we are asked to produce scores per purchasing propensity and to produce some better customer segmentations.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec139"></a>Rules to send out alerts</h3></div></div></div><p>As discussed <a id="id563" class="indexterm"></a>earlier, for analytical results produced with R, there are several tools to help extract rules from these developed predictive models with R.</p><p>For the decision tree model developed to model whether or not a subscriber left, we can use the <code class="literal">rpart.utils</code> R package, which will extract rules and export them in various formats such as RODBC. Specifically, <code class="literal">rpart.rules.table(model1)</code> will return an unpivoted table of variable values (factor levels) associated with each branch.</p><p>However, for this <a id="id564" class="indexterm"></a>project, we will also utilize some insights to derive rules directly. That is, we need to use the insights discussed in the last section, <span class="emphasis"><em>Results explanation</em></span>, as we have obtained very rich results, as described in the section.</p><p>From an analytical perspective, we face the same issue here: to minimize false alerts while ensuring adequate warning. In other words, if too many alerts were sent out, it will become a big burden and also will cause a lot of resource wasting, plus a lot of confusion.</p><p>Therefore, by taking advantage of Spark's fast computing, we carefully produced rules, and for each rule, we supplied false positive ratios that helped the telco company utilize these rules. Actually, for this stage of the work, some subject knowledge from the company experts has been used, and a good interaction is the key to ensure success.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec140"></a>Scores subscribers for churn and for Call Center calls</h3></div></div></div><p>With our linear regression and<a id="id565" class="indexterm"></a> logistic regression models in place, producing scores is easy.</p><p>For this project, we have used the churn probability as a score and then also used the predicted number of Call Center calls divided by the maximum calls as another score.</p><p>For MLlib, we used code similar to the following:</p><div class="informalexample"><pre class="programlisting">// Compute raw scores on the test set.
val predictionAndLabels = test.map { case LabeledPoint(label, features) =&gt; val prediction = model.predict(features)
  (prediction, label)
}</pre></div><p>In R and SPSS, we also have easy methods to produce scores, for which you may refer to our previous chapters.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec141"></a>Scores subscribers for purchase propensity</h3></div></div></div><p>After we merged our cleaned data with clients' transaction data, we used the same approach to develop models to <a id="id566" class="indexterm"></a>predict purchases and other customer actions, as we do for predicting Call Center calls and churn, but with a different set of predictors.</p><p>With the predictive models developed, we can use MLlib, R, or SPSS to score new data, but for this project, we used SPSS nodes to do so.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec85"></a>Summary</h2></div></div><hr /></div><p>This chapter constitutes an extension of what was described and discussed in the previous chapters (<a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>A Holistic View on Spark</em></span> to <a class="link" href="#" linkend="ch09">Chapter 9</a>, <span class="emphasis"><em>City Analytics on Spark</em></span>). Here, we took an approach driven by data and analytical needs rather than driven by predefined projects. We also developed some predictive models to score subscribers on customer churn, on Call Center calling probabilities, and even on purchasing propensity.</p><p>In this chapter, using a real-life project of learning from telco data, we have gone through a step-by-step process of utilizing big data to serve the telco company as well as their clients, from which we processed a large amount of data on Apache Spark. We then built several models, including regression and decision tree, to predict customer churn and Call Center calls and also purchasing, with which we then developed rules for alerts and also developed scores to help the telco company and its clients. At the same time, we completed some exploratory analytics by taking advantage of the Apache Spark fast computation.</p><p>Specifically, we first selected two supervised machine learning approaches after we prepared Spark computing and loaded in preprocessed data. Second, we worked on data and feature preparation by merging a few datasets together and further developing features. We then selected a core set of features for model building. Third, we estimated model coefficients by directly using MLlib and R notebooks on Databricks as well as SPSS. Fourth, we evaluated these estimated models, mainly using RMSEs and error ratios. Then, we interpreted our machine learning results with a focus on special insights and biggest predictors. Finally, we deployed our machine learning results by developing a few scores, but also used insights to develop rules for sending alerts.</p><p>This process is similar to the ones used in previous chapters. However, here, we take a more dynamic approach that we have used descriptive statistics and visualization for data exploratory work, and then work between SPSS and R and MLlib dynamically, as well as jump between the 4Es as needed.</p><p>After reading this chapter, you would have gained a better understanding about how Apache Spark could be used with MLlib, R, and SPSS to perform productive machine learning.</p><p>Specially, after reading this chapter, you will reach a new level of utilizing machine learning in a dynamic way to solve problems. That is, users are not limited to linearly progressing step by step to get a project done, but will go back and forth to achieve optimal results. They will also jump between MLlib, SPSS, R, and other tools to achieve the best analytical solutions.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch11"></a>Chapter 11. Modeling Open Data on Spark</h2></div></div></div><p>Following what we did in <a class="link" href="#" linkend="ch10">Chapter 10</a>, <span class="emphasis"><em>Learning Telco Data on Spark</em></span>, in this chapter, we will further extend our Apache Spark machine learning to a project of learning from open data. In <a class="link" href="#" linkend="ch09">Chapter 9</a>, <span class="emphasis"><em>City Analytics on Spark</em></span>, we already applied machine learning to open data, where we built models to predict service requests. Here, we will further move up into a new level where we will explore machine learning approaches of turning more open data into useful insights, as well as building models to score school districts or schools for academic achievements, technologies, and others. After that, we will build predictive models to explain what impacts the ranking and scoring of these districts.</p><p>To follow the good structure established early, in this chapter, we will still first review machine learning methods and related computing for this real-life project of learning from open data. We will then set up Apache Spark computing. At the same time, with our real-life learning examples, we will further illustrate our step-by-step machine learning process with big data. However, far beyond this, we will further demonstrate the benefits of our dynamic approaches as taken in the last chapter, <a class="link" href="#" linkend="ch10">Chapter 10</a>, <span class="emphasis"><em>Learning Telco Data on Spark</em></span>, which will allow us to generate results quickly. We will then quickly adjust ourselves to go deep in machine learning to generate even more insights. In other words, as you are expected to have already gained a much better knowledge of Spark computing and the related tools, including R and SPSS, at this stage, we will jump around the 4Es as needed. Also, we will not be limited by working only on one project or on one model or a specific process. Therefore, especially for this chapter, we will just work as needed to discover insights, score districts, and then build predictive models of the newly developed scores so that we can solve clients' problems better.</p><p>Here, we aim to illustrate our technologies and processes using these real-life projects of learning from open data. However, what is described in this chapter is not limited to district scoring and ranking, but can also be easily applied to other scoring and ranking projects, such as to score and rank corporations or countries. Also, they actually can be applied to various kinds of machine learning on various kinds of open datasets. In this chapter, we will cover the following topics:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Spark for learning from open data</p></li><li style="list-style-type: disc"><p>Methods for scoring and ranking</p></li><li style="list-style-type: disc"><p>Data and feature preparation</p></li><li style="list-style-type: disc"><p>Model estimation</p></li><li style="list-style-type: disc"><p>Model evaluation</p></li><li style="list-style-type: disc"><p>Results explanation</p></li><li style="list-style-type: disc"><p>Model deployment</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch11lvl1sec86"></a>Spark for learning from open data</h2></div></div><hr /></div><p>In this section, we will describe <a id="id567" class="indexterm"></a>our real-life use case of learning from open data, and then describe how to prepare Apache Spark computing for our real-life projects.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec142"></a>The use case</h3></div></div></div><p>As discussed in <a class="link" href="#" linkend="ch09">Chapter 9</a>, <span class="emphasis"><em>City Analytics on Spark</em></span>, in the United States and worldwide, more and more governments at various levels have made their collected data openly available to the public. As a result of expanding analytics of open data, many governmental and social organizations have used these open datasets to improve service to citizens, with a lot of good results recorded, such as in <a class="ulink" href="https://www.socrata.com/video/added-value-open-datas-internal-use-case/" target="_blank">https://www.socrata.com/video/added-value-open-datas-internal-use-case/</a>. Using data analytics for cities has a huge impact as more than half of us live in urban centers now, and this urban residence percentage is higher and higher every year.</p><p>Especially, using big data to<a id="id568" class="indexterm"></a> measure communities is also favored by researchers and practitioners, as we can see at <a class="ulink" href="http://files.meetup.com/11744342/CITY_RANKING_Oct7.pdf" target="_blank">http://files.meetup.com/11744342/CITY_RANKING_Oct7.pdf</a>. Many cities have policy initiatives to measure communities or even smaller units such as streets with good results and data available for public use, such as that from Los Angeles at <a class="ulink" href="http://lahub.maps.arcgis.com/apps/MapJournal/index.html?appid=7279dc87ea9e416d9f90bf844505a54a" target="_blank">http://lahub.maps.arcgis.com/apps/MapJournal/index.html?appid=7279dc87ea9e416d9f90bf844505a54a</a>. Using available open data and computing tools just to create some measurements and rankings may be easy. However, creating an accurate and object ranking of certain properties of some communities is not an easy task. Here, we are asked to use available open data, in combination with other datasets, such as census data and social media data, to improve rankings of communities, with a focus on school districts or schools.</p><p>At the same time, we are also asked to explore the available open data and try to model it with available machine learning tools on Spark. In other words, for this project, besides developing a good score to measure and rank communities, we are also asked for special insights to be developed from our dynamic machine learning. Once the ranking is<a id="id569" class="indexterm"></a> ready, we are even asked to explore the rankings with more machine learning, which makes this project really dynamic, as aided by the ease and speed of Spark computing.</p><p>However, everything starts from data, as we found out in <a class="link" href="#" linkend="ch09">Chapter 9</a>, <span class="emphasis"><em>City Analytics on Spark</em></span>. The datasets are not as good as we expected, and they have the following issues for us to deal with:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Data quality is not as good as expected. For example, there are a lot of missing cases.</p></li><li style="list-style-type: disc"><p>Data accuracy is another issue to deal with.</p></li><li style="list-style-type: disc"><p>Data exists in different silos, which need to be merged together.</p></li></ul></div><p>Therefore, we will still need to perform a big task of data cleaning and feature preparation. We are lucky that we already have a good process from data to equation, estimation, evaluation, and explanation.</p><p>For this work of learning from open data, as we took a dynamic approach, the research team became interested in educational data and gradually turned our focus to the work of ranking school districts with open data.</p><p>With regard to this subject, we found some open data about schools at <a class="ulink" href="https://www.ed-data.k12.ca.us/Pages/Home.aspx" target="_blank">https://www.ed-data.k12.ca.us/Pages/Home.aspx</a>.</p><p>The state government of California also has some open data at <a class="ulink" href="http://data.ca.gov/category/by-data-format/data-files/" target="_blank">http://data.ca.gov/category/by-data-format/data-files/</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec143"></a>Spark computing</h3></div></div></div><p>As discussed earlier, like <a id="id570" class="indexterm"></a>in the <span class="emphasis"><em>Spark computing</em></span> section of <a class="link" href="#" linkend="ch08">Chapter 8</a>, <span class="emphasis"><em>Learning Analytics on Spark</em></span>, you may choose one of the following approaches for our kind of projects:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Spark on Databrick's platform</p></li><li style="list-style-type: disc"><p>Spark on IBM DataScientistWorkbench</p></li><li style="list-style-type: disc"><p>SPSS on Spark</p></li><li style="list-style-type: disc"><p>Apache Spark with MLlib alone</p></li></ul></div><p>You have already learned in detail about utilizing them one by one in the previous chapters, mainly from <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>A Holistic View on Spark</em></span> to <a class="link" href="#" linkend="ch07">Chapter 7</a>, <span class="emphasis"><em>Recommendations on Spark</em></span>.</p><p>Either one of the preceding four approaches should work very well for our projects of learning from open data here. Specially, you may also take the codes as developed in this chapter, and put them into a separate notebook. You can then implement the notebook with one of the preceding approaches.</p><p>As an exercise and also <a id="id571" class="indexterm"></a>for the best to fit our big amount of open data and project goals of fast ranking, we will need to work with the fourth approach, which is to utilize Apache Spark with MLlib. However, we will also need to use R programming a lot for better visualization and reporting, so that we will utilize our first approach of Spark in Databrick's platform as well. At the same time, to take advantage of some good PCA algorithms in SPSS and to easily develop related workflows, we will also need to use SPSS on Spark to practice a special dynamic approach of utilizing Apache Spark. Finally, to meet the needs of creating many data-cleaning workflows, we will also need to use the DataScientistWorkBench platform, with which we can use OpenRefine.</p><p>Let's review the approaches mentioned previously in brief to get us really prepared.</p><p>As we discussed in the <span class="emphasis"><em>Spark computing for machine learning</em></span> section of <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Spark for Machine Learning</em></span>, Apache Spark has a unified platform that consists of the Spark core engine and four libraries, which are Spark SQL, Spark Streaming, MLlib, and GraphX:</p><div class="mediaobject"><img src="graphics/B04883_11_01.jpg" /></div><p>As MLlib is Apache Spark's built-in machine learning library, it is relatively easy to set up and scale for our project of learning from open data.</p><p>To work within the Databricks environment, we need to perform the following steps to set up clusters:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>First, you need to <a id="id572" class="indexterm"></a>go to the main menu and click on <span class="strong"><strong>Clusters</strong></span>. Then, a window will open up for you to write a name for the cluster. You can select a Spark version and then specify the number of workers:</p><div class="mediaobject"><img src="graphics/B04883_11_02.jpg" /></div></li><li><p>Once clusters have been created, we can go to the main menu, click on the down arrow on the right-hand side of <span class="strong"><strong>Tables</strong></span>, and then choose <span class="strong"><strong>Create Tables</strong></span> to import our open datasets that are cleaned and prepared, as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/B04883_11_03.jpg" /></div></li></ol></div><p>To utilize <a id="id573" class="indexterm"></a>IBM Data Scientist Workbench, we need to go to <a class="ulink" href="https://datascientistworkbench.com/" target="_blank">https://datascientistworkbench.com/</a>:</p><div class="mediaobject"><img src="graphics/B04883_11_04.jpg" /></div><p>As shown in the <a id="id574" class="indexterm"></a>preceding screenshot, Data Scientist Workbench has Apache Spark installed and also has a data cleaning system, OpenRefine, integrated so that our data preparation work can be made easier and more organized:</p><div class="mediaobject"><img src="graphics/B04883_11_05.jpg" /></div><p>For this project, we will use Data Scientist Workbench for data cleaning and also a little for R notebook creation, as well as Apache Spark implementation. For this setup, some of the Apache Spark techniques described in the previous chapters should apply.</p><p>With regards<a id="id575" class="indexterm"></a> to SPSS on Spark, we will use IBM SPSS Modeler 17.1 and IBM SPSS Analytics Server 2.1, which has a good integration with Apache Spark.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec144"></a>Methods for scoring and ranking</h3></div></div></div><p>In the previous section, we described our use case of learning from open data, with a focus on using<a id="id576" class="indexterm"></a> open data to score and rank communities, and also prepared our Spark computing platform with R notebooks, SPSS workflows, and MLlib codes to use. As the next step <a id="id577" class="indexterm"></a>for our machine learning per our 4E framework, we need to complete a task of mapping our use case to machine learning methods, which is to select our analytical methods or predictive models (equations) for this project of scoring and ranking with open data on Spark.</p><p>To turn data into insights, we will need to explore many methods, for which our dynamic approach should work well. To develop scores and rankings, it is not a difficult task with our available analytical tools and fast computing. However, to obtain objective and accurate raking is not an easy job. One approach to achieve this is to ensemble many rankings together, as it will improve results dramatically per past research. Visit <a class="ulink" href="http://www.researchmethods.org/Ranking-indicators" target="_blank">http://www.researchmethods.org/Ranking-indicators</a> and <a class="ulink" href="http://www.researchmethods.org/InnovativeAnalysisSociety" target="_blank">http://www.researchmethods.org/InnovativeAnalysisSociety</a> for more information.</p><p>Therefore, for this project, we will take a dynamic approach, with which we will explore our open data, with methods including cluster analysis and principal component analysis. We will then use this knowledge to build a few rankings and scores. After that, we will ensemble results to improve rankings and scores. Finally, we will develop models to explain the impact of various features on these rankings and scores. However, as we are taking a dynamic approach, we will jump between these stages to achieve optimal results. As always, once we finalize our decision for analytical methods or models, we will need to prepare the related dependent variable and also prepare for coding.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec145"></a>Cluster analysis</h3></div></div></div><p>Both Spark MLlib and R have algorithms <a id="id578" class="indexterm"></a>available for cluster analysis:</p><div class="informalexample"><pre class="programlisting">// Cluster the data into two classes using KMeans
val numClusters = 2
val numIterations = 20
val clusters = KMeans.train(parsedData, numClusters, numIterations)

// Evaluate clustering by computing Within Set Sum of Squared Errors
val WSSSE = clusters.computeCost(parsedData)
println("Within Set Sum of Squared Errors = " + WSSSE)</pre></div><p>In R, we can <a id="id579" class="indexterm"></a>use some R codes. Here is an example:</p><div class="informalexample"><pre class="programlisting"># K-Means Cluster Analysisfit &lt;- kmeans(schooldata, 5) # 5 cluster solution
# get cluster means aggregate(schooldata,by=list(fit$cluster),FUN=mean)</pre></div><p>For more about <a id="id580" class="indexterm"></a>cluster analysis with Spark MLlib, go to <a class="ulink" href="http://spark.apache.org/docs/latest/mllib-clustering.html" target="_blank">http://spark.apache.org/docs/latest/mllib-clustering.html</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec146"></a>Principal component analysis</h3></div></div></div><p>Both Spark MLlib <a id="id581" class="indexterm"></a>and R have <a id="id582" class="indexterm"></a>algorithms available for <span class="strong"><strong>principal component analysis</strong></span> (<span class="strong"><strong>PCA</strong></span>):</p><div class="informalexample"><pre class="programlisting">// Compute the top 10 principal components.
val pc: Matrix = mat.computePrincipalComponents(10) // Principal components are stored in a local dense matrix.

// Project the rows to the linear space spanned by the top 10 principal components.
val projected: RowMatrix = mat.multiply(pc)</pre></div><p>In R, we can use the <code class="literal">prcomp</code> function from the <code class="literal">stats</code> package.</p><p>For more on PCA with <a id="id583" class="indexterm"></a>Spark MLlib, go to <a class="ulink" href="http://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html" target="_blank">http://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html</a>.</p><p>Besides cluster analysis and PCA, we will also use regression modelling and decision tree modelling to help us understand more about how communities fall into one category or one rank rather than others.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec147"></a>Regression models</h3></div></div></div><p>So far, you must know that <a id="id584" class="indexterm"></a>regression is among the most commonly used models for prediction, and has been utilized for various projects so far.</p><p>As we discussed, there are two kinds of regression modeling that are suitable for various kinds of predictions. One is linear regression and another is logistic regression. For this project, linear regression can be used when we take daily service request volume as our target variable, while logistic regression can be used if we want to predict whether or not a certain type of service is requested in a certain location at a certain time period.</p><p>For your <a id="id585" class="indexterm"></a>convenience, in MLlib, for linear regression, we have the following code to be used:</p><div class="informalexample"><pre class="programlisting">val numIterations = 90
val model = LinearRegressionWithSGD.train(TrainingData, numIterations)</pre></div><p>For logistic regresssion, we can use these following code:</p><div class="informalexample"><pre class="programlisting">val model = new LogisticRegressionWithSGD()
.setNumClasses(2)</pre></div><p>In R, as we did earlier, we will use the GLM and LM functions for linear regression modeling and logistic regression modeling.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec148"></a>Score resembling</h3></div></div></div><p>Once the scores <a id="id586" class="indexterm"></a>get developed, one of the easy ways of resembling them is to construct a linear combination with special weighting for each. The weights can be developed with subject knowledge or with machine learning.</p><p>Besides the preceding things, there are also a few R packages for score ensemble.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch11lvl1sec87"></a>Data and feature preparation</h2></div></div><hr /></div><p>Everyone who has<a id="id587" class="indexterm"></a> worked with open data will agree that a huge amount of time is needed to clean datasets, with a lot of work to be completed to take care of data accuracy and data incompleteness.</p><p>Also, one main task is to merge all the datasets together, as we have separate datasets for crime, education, resource usage, request demand, and transportation from the open datasets. We also have datasets from some separate sources, including census.</p><p>In the <span class="emphasis"><em>Feature extraction</em></span> section of <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Data Preparation for Spark ML</em></span>, we reviewed a few methods for feature extraction and discussed their implementation on Apache Spark. All the techniques discussed there can be applied to our data here.</p><p>Besides data merging, we will also need to spend a lot of time on feature development, as we need features to develop our models to obtain insights for this project.</p><p>Therefore, for <a id="id588" class="indexterm"></a>this project, we actually need to conduct data merging, and then feature development and selection, which is to utilize all the techniques discussed in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Data Preparation for Spark ML</em></span>, and also in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>A Holistic View on Spark</em></span>.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec149"></a>Data cleaning</h3></div></div></div><p>To obtain good <a id="id589" class="indexterm"></a>datasets to use, a lot of work needs to be completed to clean our data, especially in taking care of the data accuracy issue and missing values.</p><p>Due to the big demand for data cleaning, we have adopted a few special approaches and actually also a dynamic approach for us to use a few tools in cleaning the datasets and then combine them for our machine learning. Specially, we have also used OpenRefine as discussed in <a class="link" href="#" linkend="ch05">Chapter 5</a>, <span class="emphasis"><em>Risk Scoring on Spark</em></span>. OpenRefine, formerly <span class="emphasis"><em>Google Refine</em></span>, is an open source application for data cleaning.</p><p>Some of our team members have also used OpenRefine directly. For more information on for using <a id="id590" class="indexterm"></a>OpenRefine directly, go to <a class="ulink" href="http://openrefine.org/" target="_blank">http://openrefine.org/</a>.</p><p>To use OpenRefine on <a id="id591" class="indexterm"></a>Data Scientist WorkBench, first go to <a class="ulink" href="https://datascientistworkbench.com/" target="_blank">https://datascientistworkbench.com/</a>.</p><p>After login, we will see the following screenshot:</p><div class="mediaobject"><img src="graphics/B04883_11_06.jpg" /></div><p>Then, click on the <span class="strong"><strong>OpenRefine</strong></span> button in the top-right corner:</p><div class="mediaobject"><img src="graphics/B04883_11_07.jpg" /></div><p>From here, we can <a id="id592" class="indexterm"></a>import datasets from your computer or from a URL address.</p><p>Then, we can create an OpenRefine project to do data cleaning and preparation. After that, we can export the prepared data or send the data to a notebook by drag and drop.</p><p>For this project, we especially used OpenRefine for identity matching (reconciliation), duplicates deleting, and then a little bit of dataset merging.</p><p>Besides using OpenRefine, some of our members have cleaned sample data. They have then programmed the procedures for distributed computing for data cleaning, especially for taking care of some data mistakes.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec150"></a>Data merging</h3></div></div></div><p>In the <span class="emphasis"><em>Joining data</em></span> section of <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Data Preparation for Spark ML</em></span>, we described methods to join<a id="id593" class="indexterm"></a> data together with Spark SQL and other tools. All the data techniques described in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Data Preparation for Spark ML</em></span>, as well as the ones about identity matching and data cleaning techniques will be used.</p><p>As for this data merging task, the main focus is to merge data on location per zip code and also per school districts. That is, first, we need to work on identity analytics to ensure that we have good IDs for matching.</p><p>Then, we merge datasets.</p><p>After that, we reorganize datasets into a format suitable for the methods we selected in the previous section.</p><p>For information<a id="id594" class="indexterm"></a> about how to reorganize datasets, you may refer to the <span class="emphasis"><em>Data reorganizing</em></span> section of <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Data Preparation for Spark ML</em></span>.</p><p>Specifically, we start with simple data at <a class="ulink" href="https://www.ed-data.k12.ca.us/Pages/Home.aspx" target="_blank">https://www.ed-data.k12.ca.us/Pages/Home.aspx</a>.</p><p>Then, we merge a few datasets, such as weather data, census data, and city educational datasets, into it.</p><p>After that, we reorganize all the data to obtain features per school district and per academic term.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec151"></a>Feature development</h3></div></div></div><p>As an exercise, we have also used some social media data and worked to develop features from it.</p><p>One easy feature<a id="id595" class="indexterm"></a> for social media is the social influence score for the principal of the school, which I suspect is not very useful. However, to obtain the social influence scores for all the students or for all the teachers is difficult.</p><p>As for the web data, we obtained some log data for each school's website. Using some similar methods to those used in <a class="link" href="#" linkend="ch04">Chapter 4</a>, <span class="emphasis"><em>Fraud Detection on Spark</em></span>, we extracted some features from the web log data. Specifically, to parse them and to make sense of them, we used some subject knowledge. With that, our team worked manually with some sample data. Then, they used the patterns discovered to develop codes in R to parse and turn extracted information into features. These features include the number of clicks, time between clicks, type of clicks, and other features, which were used to construct interaction features for the schools.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec152"></a>Feature selection</h3></div></div></div><p>After the work mentioned in the preceding section, we have more than 100 features ready to be used.</p><p>As for the feature<a id="id596" class="indexterm"></a><a id="id597" class="indexterm"></a> selection for this project, we could follow what we used for <a class="link" href="#" linkend="ch08">Chapter 8</a>, <span class="emphasis"><em>Learning Analytics on Spark</em></span>, which was to utilize PCA and also to use subject knowledge to group features, and then apply machine learning for its final feature selection. However, as an exercise, you will not repeat what you learned, but will try something different. That is, we will let the machine learning algorithms pick up the features most useful in prediction.</p><p>In MLlib, we can use the <code class="literal">ChiSqSelector</code> algorithm as follows:</p><div class="informalexample"><pre class="programlisting">// Create ChiSqSelector that will select top 25 of 400 features
val selector = new ChiSqSelector(25)
// Create ChiSqSelector model (selecting features)
val transformer = selector.fit(TrainingData)</pre></div><p>In R, we can use<a id="id598" class="indexterm"></a> some R packages to make computation easy. Among the available packages, CARET is one of the commonly used packages.</p><p>After this, we will end with a large amount of data with the following list of our sample features:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>School name</p></li><li style="list-style-type: disc"><p>School ID</p></li><li style="list-style-type: disc"><p>Graduation ratio</p></li><li style="list-style-type: disc"><p>Dropout ratio</p></li><li style="list-style-type: disc"><p>Average score from state exam 1</p></li><li style="list-style-type: disc"><p>Average score from state exam 2</p></li><li style="list-style-type: disc"><p>Social media participation score</p></li><li style="list-style-type: disc"><p>Web interactions</p></li><li style="list-style-type: disc"><p>Parent participation</p></li><li style="list-style-type: disc"><p>Outdoor activities</p></li><li style="list-style-type: disc"><p>Mobility</p></li><li style="list-style-type: disc"><p>Technology usage</p></li><li style="list-style-type: disc"><p>College connection</p></li></ul></div><p>Besides this, we have also obtained a dataset with a school district as a unit, for which school averages were calculated as each district has more than one school.</p><p>So, besides the preceding features, we also have data for school district for the following features:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Economics</p></li><li style="list-style-type: disc"><p>Crime</p></li><li style="list-style-type: disc"><p>Business</p></li></ul></div><p>We have all the data from 2000 to 2015.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch11lvl1sec88"></a>Model estimation</h2></div></div><hr /></div><p>Once the feature sets get finalized in our last section, what follows is to estimate all the parameters of <a id="id599" class="indexterm"></a>the selected models, for which we have adopted a dynamic approach of using SPSS on Spark, R notebooks in the Databricks environment, and MLlib directly on Spark. For the purpose of organizing workflows better, we focused our effort on organizing all the codes into R notebooks and also coding SPSS Modeler nodes.</p><p>For this project, as mentioned earlier, we need to conduct some exploratory analysis for descriptive statistics and for visualization. For this, we can take the MLlib codes and implement them directly. Also, with R codes, we obtained quick and good results.</p><p>For the best modelling, we need to arrange distributed computing, especially for this case, with various locations in combination with various customer segments per parents. In the United States, there are 13,506 school districts in 50 states. The difference between states is quite big. For this distributed computing part, you need to refer to the previous chapters. We will use SPSS Analytics Server with Apache Spark as well as the Databricks environment.</p><p>As we discussed in the <span class="emphasis"><em>Spark for learning from open data</em></span> section, we mainly use regression for the supervised machine learning part. From what you have learned so far, for regression, we can complete the model estimation either with SPSS or with R. In order to implement them in the Databricks environment, we should refer to <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>A Holistic View on Spark</em></span> and <a class="link" href="#" linkend="ch05">Chapter 5</a>, <span class="emphasis"><em>Risk Scoring on Spark</em></span>.</p><div class="mediaobject"><img src="graphics/B04883_11_08.jpg" /></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec153"></a>SPSS on Spark – SPSS Analytics Server</h3></div></div></div><p>The IBM SPSS Modeler 17.1 and Analytic Server 2.1 offer easy integration with Spark, which allows us to<a id="id600" class="indexterm"></a> implement the data and modeling streams built so far easily.</p><p>Besides using SPSS Modeler to estimate these predictive models, for which we need to use SPSS Analytics Server, we have also used R notebook in the Databricks environment and Data Scientist WorkBench.</p><p>With this, as an example, we have obtained a cluster analysis plot as follows:</p><div class="mediaobject"><img src="graphics/B04883_11_09.jpg" /></div><p>As an <a id="id601" class="indexterm"></a>example, with R, we have obtained a PCA plot as follows:</p><div class="mediaobject"><img src="graphics/B04883_11_10.jpg" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec154"></a>Model evaluation</h3></div></div></div><p>In the previous section, we completed our model estimation as well as some exploratory work. Now it<a id="id602" class="indexterm"></a> is time for us to evaluate these estimated models to see if they fit our criteria so that we can either move to our next stage for results explanation or go back to some previous stages to refine our predictive models.</p><p>To perform our model evaluation, in this section, we have conducted evaluations for cluster analysis and also for PCA. However, our focus is still on assessing predictive models, the regression models with rankings as our target variables. For this task, we will mainly use <a id="id603" class="indexterm"></a>
<span class="strong"><strong>Root Mean Square Error</strong></span> (<span class="strong"><strong>RMSE</strong></span>) to assess our models, as  it is good for assessing regression models.</p><p>Just like we did for model estimation, to calculate RMSEs, we need to use MLlib for regression modeling on Spark. At the same time, we will also use R notebooks to be implemented in the Databricks environment for Spark. Of course, we also used an analytical server for SPSS, as we have adopted a dynamic approach here.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec155"></a>RMSE calculations with MLlib</h3></div></div></div><p>As used with good<a id="id604" class="indexterm"></a> results in the past, for MLlib, we can use the following code to calculate RMSE:</p><div class="informalexample"><pre class="programlisting">val valuesAndPreds = test.map { point =&gt;
  val prediction = new_model.predict(point.features)
  val r = (point.label, prediction)
  r
}
val residuals = valuesAndPreds.map {case (v, p) =&gt; math.pow((v - p), 2)}
val MSE = residuals.mean();
val RMSE = math.pow(MSE, 0.5)</pre></div><p>Besides the preceding code, MLlib also has some functions in the <code class="literal">RegressionMetrics</code> and <code class="literal">RankingMetrics</code> classes for us to use for the RMSE calculation.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec156"></a>RMSE calculations with R</h3></div></div></div><p>In R, the <code class="literal">forecast</code> package has an <code class="literal">accuracy</code> function that can be used to calculate forecasting <a id="id605" class="indexterm"></a>accuracy as well as RMSEs:</p><div class="informalexample"><pre class="programlisting">accuracy(f, x, test=NULL, d=NULL, D=NULL)</pre></div><p>The measures calculated also include the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>ME</strong></span> (<span class="strong"><strong>Mean Error</strong></span>)</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>RMSE</strong></span> (<span class="strong"><strong>Root Mean Squared Error</strong></span>)</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>MAE</strong></span> (<span class="strong"><strong>Mean Absolute Error</strong></span>)</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>MPE</strong></span> (<span class="strong"><strong>Mean Percentage Error</strong></span>)</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>MAPE</strong></span> (<span class="strong"><strong>Mean Absolute Percentage Error</strong></span>)</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>MASE</strong></span> (<span class="strong"><strong>Mean Absolute Scaled Error</strong></span>)</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>ACF1</strong></span> (<span class="strong"><strong>Autocorrelation of errors at lag 1</strong></span>)</p></li></ul></div><p>To perform a complete evaluation, we calculated RMSEs for all the models we estimated. Then, we compared and picked up the ones with smaller RMSEs.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch11lvl1sec89"></a>Results explanation</h2></div></div><hr /></div><p>Per our 4Es framework<a id="id606" class="indexterm"></a> used for this book, after we passed our model evaluation stage and selected the estimated and evaluated models as our final models, our next task for this project is to interpret the results to our clients.</p><p>In terms of explaining the machine learning results, the users of our project are particularly interested in understanding what influences the known rankings that are widely used. Also, they are interested in how new rankings are different from others and how the new rankings can be used.</p><p>So, we will work on their requests, but will not cover all of them as the purpose here is mainly to exhibit technologies. Also, for the confidentiality issue and also space limitations, we will not go into the details too much, but will focus more on utilizing our technologies for better explanations.</p><p>Overall, the interpretation is straightforward here, which include the following three tasks:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Present a list of top-ranked schools and school districts</p></li><li style="list-style-type: disc"><p>Compare various lists</p></li><li style="list-style-type: disc"><p>Explain the impact of factors such as parent involvement and economy on the rankings</p></li></ul></div><p>One of the main achievements of this project is for us to obtain a better and more accurate ranking with our ensemble methods as well as good analytics, but it is very challenging to explain it the users, and it is also beyond the scope of this book here.</p><p>Another big improvement achieved here is the capability for us to quickly produce rankings per various requirements, such as to rank per academic performance or per future employment or per graduation rate, which is interesting to users, but seems still take time for adoption. However, users understand the benefits of fast-producing rankings, as made possible using Apache Spark.</p><p>So, as a result, we have delivered a few lists, and reported on ranking comparison and on factors influencing rankings.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec157"></a>Comparing ranks</h3></div></div></div><p>R has some packages<a id="id607" class="indexterm"></a> that help us analyze and compare rankings, such as pmr and Rmallow. However, for this project, the users preferred simple comparison, such as a direct comparison of the top 10 schools and the top 10 school districts, which made our explanation a little easier.</p><p>Another task of the explanatory works is to compare our list to others, such as the one at <a class="ulink" href="http://www.school-ratings.com/schoolRatings.php?zipOrCity=91030" target="_blank">http://www.school-ratings.com/schoolRatings.php?zipOrCity=91030</a>, or the one provided by the LA Times at <a class="ulink" href="http://schools.latimes.com/" target="_blank">http://schools.latimes.com/</a>, or the one by SchoolIE. They claimed to be using big data to evaluate schools from many perspectives, rather than by one angle, at <a class="ulink" href="http://www.schoolie.com/" target="_blank">http://www.schoolie.com/</a>.</p><p>As a result, we found ours to be closer to the one created by SchoolIE.</p><p>R has some algorithms to compute similarity or distance between rankings, which we explored, but have not used to serve the clients. This is because we adopted an approach with simple comparison that our clients preferred, and it is still very effective.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec158"></a>Biggest influencers</h3></div></div></div><p>As people are interested in how some schools are on top and other schools are not, our results about the <a id="id608" class="indexterm"></a>biggest predictors are of great interest.</p><p>For this part, we use results from our estimated predictive models of regression, for which we have used our own rankings as the target variable, and also some well-known rankings such as those provided by the US News and World Report and those by some state organizations.</p><p>For this task, we have just used the coefficients in our linear regression models to tell us which one has a bigger impact. We also used the <code class="literal">RandomForest</code> function to rank features per their impact on moving schools into the top 100. In other words, we split the list into "top 100" and "the rest." We then ran the decision tree modeling and random forest modeling on it, and then used the Random Forest's feature <code class="literal">importance</code> function to obtain a list of features as ordered by their impact on the target variable of whether the school is in top 100. In R, we need to use the function of <code class="literal">importance</code> in R's <code class="literal">randomForest</code> package.</p><p>Per our results, the economic status of the community, parents' involvements, and college connections are among the factors having the biggest impact for some coast schools. However, technology use has not had as much impact as expected.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch11lvl1sec90"></a>Deployment</h2></div></div><hr /></div><p>In the past, rankings were mostly reviewed as a reference by users. With this project, we found we are also in a position to assist our users in integrating our results with their decision-making tools, to help them utilize rankings better and also make their lives easier. For this, producing rules from rankings and also making scores behind rankings easily accessible became very important.</p><p>Because of the preceding <a id="id609" class="indexterm"></a>reason, our deployment is still on to develop a set of rules and also to make all the scores available for decision makers, which include schools and some parents. Specifically, the main task of sending out a rule to alert users when some ranking changes dramatically, especially when a ranking drops down dramatically. Users of this project also have a need to obtain all the scores and rankings for their management performance.</p><p>Another purpose for this project is to produce good predictive models for the users to forecast possible changes of school rankings as per population changes using our developed regression models.</p><p>All the three needs for rankings, scores, and forecasting, mentioned in the preceding paragraph, are of value to various kinds of users who use various kinds of software systems for decision making. So, we need a bridge such as <span class="strong"><strong>Predictive Model Markup Language</strong></span> (<span class="strong"><strong>PMML</strong></span>), which is adopted as the standard by many systems. As discussed <a id="id610" class="indexterm"></a>before, MLlib supports model export to PMML. Therefore, we export some developed models to PMML for this project.</p><p>In practice, the users for this project are more interested in rule-based decision making to use some of our insights and also in score-based decision making to evaluate their regional units' performance. Specifically, for this project, the client is interested in applying our results to (1) decide when an alert may be sent out if rankings have been changed or ranking changes will likely occur in the future, for which rules should be established, and to (2) develop scores and use them scores to measure performance as well as to plan for the future.</p><p>Besides this, clients are also interested in forecasting the attendance and other requests per ranking changes, for which R actually has a package called <code class="literal">forecast</code> that is ready to be used for this purpose:</p><div class="informalexample"><pre class="programlisting">forecast(fit)
plot(forecast(fit))</pre></div><p>To sum up, for our special tasks, we need to turn some of our results into some rules and also need to produce some performance scores for the client.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec159"></a>Rules for sending out alerts</h3></div></div></div><p>As discussed earlier, for R results, there are several tools to help extract rules from developed <a id="id611" class="indexterm"></a>predictive models.</p><p>For the decision tree model developed to model whether or not a service request level exceeds a certain level, we should use the <code class="literal">rpart.utils</code> R package, which can extract rules and export them in various formats such as RODBC.</p><p>The <code class="literal">rpart.rules.table(model1)  *</code> package returns an unpivoted table of variable values (factor levels) associated with each branch.</p><p>However, for this project, partially due to the data incompleteness issue, we will need to utilize some insights to derive rules directly. That is, we need to use the insights discussed in the last section. For example, we can do the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>If big mobility occurred and also parents' involvement dropped, our prediction shows rankings will go down dramatically and so an alert will be sent out</p></li></ul></div><p>From an analytical perspective, we face the same issue here, to minimize false alerts, while ensuring adequate warning.</p><p>Therefore, by taking advantage of Spark's fast computing, we carefully produced rules, and for each rule, we supplied false positive ratios that helped the client utilize the rules.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec160"></a>Scores for ranking school districts</h3></div></div></div><p>With our regression<a id="id612" class="indexterm"></a> modeling in place, we have two ways to forecast the ranking change at a specific time.</p><p>One is to use the estimated regression equations to do forecasting directly. Alternatively, we can use the following code:</p><div class="informalexample"><pre class="programlisting">forecast(fit, newdata=data.frame(City=30))</pre></div><p>As long as we have obtained the scores, we can classify all the districts or schools into several categories and also illustrate them on a map to identify special zones for attention, such as <a id="id613" class="indexterm"></a>the graphs as produced by R:</p><div class="mediaobject"><img src="graphics/B04883_11_11.jpg" /></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch11lvl1sec91"></a>Summary</h2></div></div><hr /></div><p>The work presented in this chapter is a further extension of <a class="link" href="#" linkend="ch10">Chapter 10</a>, <span class="emphasis"><em>Learning Telco Data on Spark</em></span>, as well as <a class="link" href="#" linkend="ch09">Chapter 9</a>, <span class="emphasis"><em>City Analytics on Spark</em></span>. It is a very special extension of <a class="link" href="#" linkend="ch09">Chapter 9</a>, <span class="emphasis"><em>City Analytics on Spark</em></span>, as both chapters are using open datasets. It is also an extension of <a class="link" href="#" linkend="ch10">Chapter 10</a>, <span class="emphasis"><em>Learning Telco Data on Spark</em></span>, as both chapters take a dynamic approach so that readers can take advantage of all the learned techniques to achieve better machine learning results and also to develop the best analytical solutions. Therefore, this chapter may be used as a review chapter, as well as a special chapter for you to synthesize all the knowledge learned.</p><p>In this chapter, with a real-life project of learning from open data, we have repeated the same step-by-step RM4Es process as used in the previous chapters, from which we processed open data on Apache Spark and then selected models (equations). For each model, we estimated their coefficients and then evaluated these models against model performance metrics. Finally, with the models estimated, we explained our results in detail. With this real-life project of learning from open data, we further demonstrated the benefit of utilizing our RM4Es framework to organize our machine learning processes.</p><p>A little different from the previous chapters, specifically, we first selected a dynamic machine learning approach with cluster analysis and PCA plus regression. Then, we prepared Spark computing and loaded in preprocessed data. Second, we worked on data and feature preparation using cleaned open datasets, by reorganizing a few datasets together, and by selecting a core set of features. Especially, in dealing with open datasets, a lot more work is needed to clean the data and reorganize it, as demonstrated here, which should be a special learning for anyone using open data. Third, we developed measurements and estimated predictive model coefficients using MLlib, R, and SPSS on Spark. Fourth, we evaluated these estimated models, mainly using RMSEs. Then, we interpreted our machine learning results with lists and ranking comparisons, as well as the biggest predictors for top rankings. Finally, we deployed our machine learning results with a focus on scoring and rules.</p><p>The preceding process is similar to the process described in the previous chapters. However, in this chapter, we focused our effort on a dynamic approach, which give you opportunities to combine what you have learned so far for the best analytical results. Especially, for this project, we explored the datasets and built several measurements and rankings of districts, with which we then developed rules for alerts and scores for performance, to help schools and parents for their decision making and performance management.</p><p>After reading this chapter, you would be completely ready to utilize Apache Spark for dynamic machine learning so that you can quickly develop actionable insights from a large amount of open data. By now, users will have mastered our process, our framework, and various approaches. Rather than being limited by any of them, users will be able to fully take advantage of all of them or any combination of them for optimal machine learning results.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">

    <div id="backindex">
      <h1 class="title">Index</h1>
      <h2>A</h2>
      <ul>
        <li>accumulators<ul><li>about / <a href="#ch01lvl1sec12" title="The Spark computing framework" class="link">The Spark computing framework</a></li></ul></li>
        <li>ACT<ul><li>URL / <a href="#ch08lvl1sec64" title="The use case" class="link">The use case</a></li></ul></li>
        <li>Alternating Least Squares (ALS) algorithm / <a href="#ch07lvl1sec58" title="Collaborative filtering" class="link">Collaborative filtering</a></li>
        <li>Apache Spark<ul><li>URL / <a href="#ch06lvl1sec49" title="Spark computing" class="link">Spark computing</a></li></ul></li>
        <li>Apache Spark Notebooks<ul><li>about / <a href="#ch05lvl1sec41" title="Apache Spark notebooks" class="link">Apache Spark notebooks</a></li></ul></li>
        <li>attrition prediction<ul><li>about / <a href="#ch08lvl1sec64" title="Spark for attrition prediction" class="link">Spark for attrition prediction</a></li><li>use case / <a href="#ch08lvl1sec64" title="The use case" class="link">The use case</a></li><li>Spark computing / <a href="#ch08lvl1sec64" title="Spark computing" class="link">Spark computing</a></li><li>Spark conputing / <a href="#ch08lvl1sec64" title="Spark computing" class="link">Spark computing</a></li></ul></li>
        <li>attrition prediction, methods<ul><li>about / <a href="#ch08lvl1sec65" title="Methods of attrition prediction" class="link">Methods of attrition prediction</a></li><li>regression models / <a href="#ch08lvl1sec65" title="Regression models" class="link">Regression models</a></li><li>decisiom trees / <a href="#ch08lvl1sec65" title="Decision trees" class="link">Decision trees</a></li></ul></li>
        <li>automation<ul><li>about / <a href="#ch02lvl1sec23" title="Repeatability and automation" class="link">Repeatability and automation</a></li><li>datasets preprocessing, workflows / <a href="#ch02lvl1sec23" title="Dataset preprocessing workflows" class="link">Dataset preprocessing workflows</a></li></ul></li>
        <li>autoregressive-moving average (ARMA) / <a href="#ch09lvl1sec72" title="About time series" class="link">About time series</a></li>
        <li>autoregressive integrated moving average (ARIMA) / <a href="#ch09lvl1sec72" title="About time series" class="link">About time series</a></li>
      </ul>
      <h2>B</h2>
      <ul>
        <li>Berkeley Data Analytics Stack (BDAS)<ul><li>about / <a href="#ch02lvl1sec18" title="Data cleaning in Spark" class="link">Data cleaning in Spark</a></li></ul></li>
        <li>broadcast variables<ul><li>about / <a href="#ch01lvl1sec12" title="The Spark computing framework" class="link">The Spark computing framework</a></li></ul></li>
      </ul>
      <h2>C</h2>
      <ul>
        <li>churn prediction<ul><li>with Spark / <a href="#ch06lvl1sec49" title="Spark for churn prediction" class="link">Spark for churn prediction</a></li><li>use case / <a href="#ch06lvl1sec49" title="The use case" class="link">The use case</a></li><li>parallel computing / <a href="#ch06lvl1sec49" title="Spark computing" class="link">Spark computing</a></li><li>feature preparation / <a href="#ch06lvl1sec51" title="Feature preparation" class="link">Feature preparation</a></li><li>model estimation / <a href="#ch06lvl1sec52" title="Model estimation" class="link">Model estimation</a></li><li>Spark implementation, with MLlib / <a href="#ch06lvl1sec52" title="Spark implementation with MLlib" class="link">Spark implementation with MLlib</a></li><li>model evaluation / <a href="#ch06lvl1sec53" title="Model evaluation" class="link">Model evaluation</a></li><li>results, explaining / <a href="#ch06lvl1sec54" title="Results explanation" class="link">Results explanation</a></li><li>impact of interventions, calculating / <a href="#ch06lvl1sec54" title="Calculating the impact of interventions" class="link">Calculating the impact of interventions</a></li><li>deployment / <a href="#ch06lvl1sec55" title="Deployment" class="link">Deployment</a></li><li>scoring / <a href="#ch06lvl1sec55" title="Scoring" class="link">Scoring</a></li><li>intervention recommendations / <a href="#ch06lvl1sec55" title="Intervention recommendations" class="link">Intervention recommendations</a></li></ul></li>
        <li>churn prediction, feature preparation<ul><li>feature extraction / <a href="#ch06lvl1sec51" title="Feature extraction" class="link">Feature extraction</a></li><li>feature selection / <a href="#ch06lvl1sec51" title="Feature selection" class="link">Feature selection</a></li></ul></li>
        <li>churn prediction, methods<ul><li>about / <a href="#ch06lvl1sec50" title="Methods for churn prediction" class="link">Methods for churn prediction</a></li><li>regression models / <a href="#ch06lvl1sec50" title="Regression models" class="link">Regression models</a></li><li>decision trees / <a href="#ch06lvl1sec50" title="Decision trees and Random forest" class="link">Decision trees and Random forest</a></li><li>Random Forest / <a href="#ch06lvl1sec50" title="Decision trees and Random forest" class="link">Decision trees and Random forest</a></li></ul></li>
        <li>cluster analysis<ul><li>reference link / <a href="#ch11lvl1sec86" title="Cluster analysis" class="link">Cluster analysis</a></li></ul></li>
        <li>confusion matrix<ul><li>about / <a href="#ch04lvl1sec37" title="Confusion matrix and false positive ratios" class="link">Confusion matrix and false positive ratios</a></li><li>and error ratios / <a href="#ch08lvl1sec68" title="The confusion matrix and error ratios" class="link">The confusion matrix and error ratios</a></li></ul></li>
        <li>Cross Industry Standard Process for Data Mining (CRISP-DM)<ul><li>about / <a href="#ch01lvl1sec13" title="ML as a step-by-step workflow" class="link">ML as a step-by-step workflow</a></li></ul></li>
      </ul>
      <h2>D</h2>
      <ul>
        <li>data<ul><li>preparing / <a href="#ch09lvl1sec73" title="Data and feature preparation" class="link">Data and feature preparation</a></li><li>merging / <a href="#ch09lvl1sec73" title="Data merging" class="link">Data merging</a></li></ul></li>
        <li>data and feature preparation<ul><li>about / <a href="#ch05lvl1sec43" title="Data and feature preparation" class="link">Data and feature preparation</a></li><li>OpenRefine, using / <a href="#ch05lvl1sec43" title="OpenRefine" class="link">OpenRefine</a></li></ul></li>
        <li>Databricks notebook<ul><li>about / <a href="#ch01lvl1sec15" title="Spark notebooks" class="link">Spark notebooks</a></li><li>URL / <a href="#ch01lvl1sec15" title="Spark notebooks" class="link">Spark notebooks</a></li></ul></li>
        <li>DataBricks Workspace<ul><li>URL / <a href="#ch02lvl1sec17" title="Exploring and visualizing datasets" class="link">Exploring and visualizing datasets</a></li></ul></li>
        <li>data cleaning<ul><li>about / <a href="#ch02lvl1sec18" title="Data cleaning" class="link">Data cleaning</a></li><li>data incompleteness, dealing with / <a href="#ch02lvl1sec18" title="Dealing with data incompleteness" class="link">Dealing with data incompleteness</a></li><li>in Spark / <a href="#ch02lvl1sec18" title="Data cleaning in Spark" class="link">Data cleaning in Spark</a></li><li>with SampleClean / <a href="#ch02lvl1sec18" title="Data cleaning made easy" class="link">Data cleaning made easy</a></li></ul></li>
        <li>DataFrame<ul><li>about / <a href="#ch02lvl1sec21" title="Dataset joining and its tool – the Spark SQL" class="link">Dataset joining and its tool – the Spark SQL</a></li></ul></li>
        <li>dataframe API<ul><li>for R / <a href="#ch01lvl1sec12" title="Dataframes API for R" class="link">Dataframes API for R</a></li><li>URL / <a href="#ch01lvl1sec12" title="Dataframes API for R" class="link">Dataframes API for R</a></li></ul></li>
        <li>DataScientistWorkbench<ul><li>about / <a href="#ch05lvl1sec41" title="Apache Spark notebooks" class="link">Apache Spark notebooks</a></li></ul></li>
        <li>Data Scientist WorkBench<ul><li>URL / <a href="#ch11lvl1sec87" title="Data cleaning" class="link">Data cleaning</a></li></ul></li>
        <li>dataset reorganization<ul><li>about / <a href="#ch02lvl1sec20" title="Dataset reorganizing" class="link">Dataset reorganizing</a></li><li>tasks / <a href="#ch02lvl1sec20" title="Dataset reorganizing tasks" class="link">Dataset reorganizing tasks</a></li><li>with Spark SQL / <a href="#ch02lvl1sec20" title="Dataset reorganizing with Spark SQL" class="link">Dataset reorganizing with Spark SQL</a></li><li>with R / <a href="#ch02lvl1sec20" title="Dataset reorganizing with R on Spark" class="link">Dataset reorganizing with R on Spark</a></li></ul></li>
        <li>datasets<ul><li>loading / <a href="#ch02lvl1sec17" title="Accessing and loading datasets" class="link">Accessing and loading datasets</a>, <a href="#ch02lvl1sec17" title="Loading datasets into Spark" class="link">Loading datasets into Spark</a></li><li>accessing / <a href="#ch02lvl1sec17" title="Accessing publicly available datasets" class="link">Accessing publicly available datasets</a></li><li>references / <a href="#ch02lvl1sec17" title="Accessing publicly available datasets" class="link">Accessing publicly available datasets</a></li><li>exploring / <a href="#ch02lvl1sec17" title="Exploring and visualizing datasets" class="link">Exploring and visualizing datasets</a></li><li>visualizing / <a href="#ch02lvl1sec17" title="Exploring and visualizing datasets" class="link">Exploring and visualizing datasets</a></li><li>joining / <a href="#ch02lvl1sec21" title="Dataset joining" class="link">Dataset joining</a></li><li>joining, with Spark SQL / <a href="#ch02lvl1sec21" title="Dataset joining and its tool – the Spark SQL" class="link">Dataset joining and its tool – the Spark SQL</a></li><li>joining, in Spark / <a href="#ch02lvl1sec21" title="Dataset joining in Spark" class="link">Dataset joining in Spark</a>, <a href="#ch02lvl1sec21" title="Dataset joining with the R data table package" class="link">Dataset joining with the R data table package</a></li></ul></li>
        <li>datasets preprocessing<ul><li>workflows / <a href="#ch02lvl1sec23" title="Dataset preprocessing workflows" class="link">Dataset preprocessing workflows</a></li><li>with Spark pipeline / <a href="#ch02lvl1sec23" title="Spark pipelines for dataset preprocessing" class="link">Spark pipelines for dataset preprocessing</a></li><li>automation / <a href="#ch02lvl1sec23" title="Dataset preprocessing automation" class="link">Dataset preprocessing automation</a></li></ul></li>
        <li>data treatment, with SPSS<ul><li>about / <a href="#ch07lvl1sec59" title="Data treatment with SPSS" class="link">Data treatment with SPSS</a></li><li>data nodes, missing on SPSS modeler / <a href="#ch07lvl1sec59" title="Missing data nodes on SPSS modeler" class="link">Missing data nodes on SPSS modeler</a></li></ul></li>
        <li>decision tree<ul><li>URL / <a href="#ch10lvl1sec79" title="Decision tree and random forest" class="link">Decision tree and random forest</a></li></ul></li>
        <li>decision trees<ul><li>about / <a href="#ch04lvl1sec34" title="Decision trees" class="link">Decision trees</a>, <a href="#ch08lvl1sec65" title="Decision trees" class="link">Decision trees</a></li><li>for churn prediction / <a href="#ch06lvl1sec50" title="Decision trees and Random forest" class="link">Decision trees and Random forest</a></li><li>URL / <a href="#ch06lvl1sec50" title="Decision trees and Random forest" class="link">Decision trees and Random forest</a></li><li>code, preparing for / <a href="#ch08lvl1sec65" title="Preparing for coding" class="link">Preparing for coding</a></li></ul></li>
        <li>deployment<ul><li>about / <a href="#ch08lvl1sec70" title="Deployment" class="link">Deployment</a></li><li>rules / <a href="#ch08lvl1sec70" title="Rules" class="link">Rules</a></li></ul></li>
        <li>deployment, holistic view<ul><li>about / <a href="#ch03lvl1sec31" title="Deployment" class="link">Deployment</a></li><li>dashboard / <a href="#ch03lvl1sec31" title="Dashboard" class="link">Dashboard</a></li><li>rules / <a href="#ch03lvl1sec31" title="Rules" class="link">Rules</a></li></ul></li>
        <li>deployment, open data<ul><li>about / <a href="#ch11lvl1sec90" title="Deployment" class="link">Deployment</a></li></ul></li>
        <li>deployment, risk scoring<ul><li>about / <a href="#ch05lvl1sec47" title="Deployment" class="link">Deployment</a></li><li>scoring / <a href="#ch05lvl1sec47" title="Scoring" class="link">Scoring</a></li></ul></li>
        <li>Directed Acyclic Graph (DAG)<ul><li>about / <a href="#ch01lvl1sec08" title="Spark advantages" class="link">Spark advantages</a>, <a href="#ch01lvl1sec14" title="ML workflow examples" class="link">ML workflow examples</a></li></ul></li>
        <li>distributed computing<ul><li>about / <a href="#ch04lvl1sec33" title="Distributed computing" class="link">Distributed computing</a></li></ul></li>
      </ul>
      <h2>E</h2>
      <ul>
        <li>entity resolution<ul><li>about / <a href="#ch02lvl1sec19" title="Entity resolution" class="link">Entity resolution</a></li><li>short string comparison / <a href="#ch02lvl1sec19" title="Short string comparison" class="link">Short string comparison</a></li><li>long string comparison / <a href="#ch02lvl1sec19" title="Long string comparison" class="link">Long string comparison</a></li><li>record deduplication / <a href="#ch02lvl1sec19" title="Record deduplication" class="link">Record deduplication</a></li></ul></li>
      </ul>
      <h2>F</h2>
      <ul>
        <li>False Negative (Type I Error) / <a href="#ch08lvl1sec68" title="Model evaluation" class="link">Model evaluation</a></li>
        <li>False Positive (FP) error rate / <a href="#ch05lvl1sec45" title="ROC" class="link">ROC</a></li>
        <li>False Positive (Type II Error) / <a href="#ch08lvl1sec68" title="Model evaluation" class="link">Model evaluation</a></li>
        <li>false positive ratios<ul><li>about / <a href="#ch04lvl1sec37" title="Confusion matrix and false positive ratios" class="link">Confusion matrix and false positive ratios</a></li></ul></li>
        <li>feature<ul><li>preparing / <a href="#ch09lvl1sec73" title="Data and feature preparation" class="link">Data and feature preparation</a></li><li>selecting / <a href="#ch09lvl1sec73" title="Feature selection" class="link">Feature selection</a></li></ul></li>
        <li>feature development, Telco Data<ul><li>about / <a href="#ch10lvl1sec80" title="Data and feature development" class="link">Data and feature development</a></li><li>data, reorganizing / <a href="#ch10lvl1sec80" title="Data reorganizing" class="link">Data reorganizing</a></li><li>feature selection / <a href="#ch10lvl1sec80" title="Feature development and selection" class="link">Feature development and selection</a></li></ul></li>
        <li>feature extraction<ul><li>about / <a href="#ch02lvl1sec22" title="Feature extraction" class="link">Feature extraction</a></li><li>challenges / <a href="#ch02lvl1sec22" title="Feature development challenges" class="link">Feature development challenges</a></li><li>with Spark MLlib / <a href="#ch02lvl1sec22" title="Feature development with Spark MLlib" class="link">Feature development with Spark MLlib</a></li><li>with R / <a href="#ch02lvl1sec22" title="Feature development with R" class="link">Feature development with R</a></li><li>preparation / <a href="#ch04lvl1sec35" title="Feature preparation" class="link">Feature preparation</a></li><li>from LogFile / <a href="#ch04lvl1sec35" title="Feature extraction from LogFile" class="link">Feature extraction from LogFile</a></li><li>data, merging / <a href="#ch04lvl1sec35" title="Data merging" class="link">Data merging</a></li></ul></li>
        <li>feature preparation<ul><li>about / <a href="#ch08lvl1sec66" title="Feature preparation" class="link">Feature preparation</a></li><li>feature development / <a href="#ch08lvl1sec66" title="Feature development" class="link">Feature development</a></li><li>feature selection / <a href="#ch08lvl1sec66" title="Feature selection" class="link">Feature selection</a></li></ul></li>
        <li>feature preparation, holistic view<ul><li>about / <a href="#ch03lvl1sec27" title="Feature preparation" class="link">Feature preparation</a></li><li>PCA / <a href="#ch03lvl1sec27" title="PCA" class="link">PCA</a></li><li>grouping by category / <a href="#ch03lvl1sec27" title="Grouping by category to use subject knowledge" class="link">Grouping by category to use subject knowledge</a></li><li>feature selection / <a href="#ch03lvl1sec27" title="Feature selection" class="link">Feature selection</a></li></ul></li>
        <li>feature preparation, open data<ul><li>about / <a href="#ch11lvl1sec87" title="Data and feature preparation" class="link">Data and feature preparation</a></li><li>data, cleaning / <a href="#ch11lvl1sec87" title="Data cleaning" class="link">Data cleaning</a></li><li>data, merging / <a href="#ch11lvl1sec87" title="Data merging" class="link">Data merging</a></li><li>feature development / <a href="#ch11lvl1sec87" title="Feature development" class="link">Feature development</a></li><li>feature selection / <a href="#ch11lvl1sec87" title="Feature selection" class="link">Feature selection</a></li></ul></li>
        <li>FORECAST R package<ul><li>reference link / <a href="#ch09lvl1sec75" title="RMSE calculation with R" class="link">RMSE calculation with R</a></li></ul></li>
        <li>fraud detection<ul><li>about / <a href="#ch04lvl1sec33" title="Spark for fraud detection" class="link">Spark for fraud detection</a></li><li>use case / <a href="#ch04lvl1sec33" title="The use case" class="link">The use case</a></li><li>distributed computing / <a href="#ch04lvl1sec33" title="Distributed computing" class="link">Distributed computing</a></li><li>methods / <a href="#ch04lvl1sec34" title="Methods for fraud detection" class="link">Methods for fraud detection</a></li><li>Random forest / <a href="#ch04lvl1sec34" title="Random forest" class="link">Random forest</a></li><li>decision trees / <a href="#ch04lvl1sec34" title="Decision trees" class="link">Decision trees</a></li><li>deploying / <a href="#ch04lvl1sec39" title="Deploying fraud detection" class="link">Deploying fraud detection</a></li><li>rules / <a href="#ch04lvl1sec39" title="Rules" class="link">Rules</a></li><li>scoring / <a href="#ch04lvl1sec39" title="Scoring" class="link">Scoring</a></li></ul></li>
      </ul>
      <h2>G</h2>
      <ul>
        <li>GraphX<ul><li>about / <a href="#ch01lvl1sec08" title="Spark overview" class="link">Spark overview</a></li></ul></li>
      </ul>
      <h2>H</h2>
      <ul>
        <li>holistic view, Spark<ul><li>about / <a href="#ch03lvl1sec25" title="Spark for a holistic view" class="link">Spark for a holistic view</a></li><li>use case / <a href="#ch03lvl1sec25" title="The use case" class="link">The use case</a></li><li>fast and easy computing / <a href="#ch03lvl1sec25" title="Fast and easy computing" class="link">Fast and easy computing</a></li><li>methods / <a href="#ch03lvl1sec26" title="Methods for a holistic view" class="link">Methods for a holistic view</a></li></ul></li>
      </ul>
      <h2>I</h2>
      <ul>
        <li>IBM Data Scientist Workbench<ul><li>reference / <a href="#ch05lvl1sec41" title="Apache Spark notebooks" class="link">Apache Spark notebooks</a></li><li>URL / <a href="#ch11lvl1sec86" title="Spark computing" class="link">Spark computing</a></li></ul></li>
        <li>IBM Predictive Extensions<ul><li>installing / <a href="#ch07lvl1sec57" title="SPSS on Spark" class="link">SPSS on Spark</a></li></ul></li>
        <li>IBM SystemML<ul><li>URL / <a href="#ch01lvl1sec11" title="Other ML libraries" class="link">Other ML libraries</a></li></ul></li>
        <li>identity matching<ul><li>about / <a href="#ch02lvl1sec19" title="Identity matching" class="link">Identity matching</a></li><li>identity issues / <a href="#ch02lvl1sec19" title="Identity issues" class="link">Identity issues</a></li><li>on Spark / <a href="#ch02lvl1sec19" title="Identity matching on Spark" class="link">Identity matching on Spark</a></li><li>entity resolution / <a href="#ch02lvl1sec19" title="Entity resolution" class="link">Entity resolution</a></li><li>with SampleClean / <a href="#ch02lvl1sec19" title="Identity matching made better" class="link">Identity matching made better</a></li><li>crowdsourced deduplication / <a href="#ch02lvl1sec19" title="Crowdsourced deduplication" class="link">Crowdsourced deduplication</a></li><li>crowd, configuring / <a href="#ch02lvl1sec19" title="Configuring the crowd" class="link">Configuring the crowd</a></li><li>crowd, using / <a href="#ch02lvl1sec19" title="Using the crowd" class="link">Using the crowd</a></li></ul></li>
      </ul>
      <h2>J</h2>
      <ul>
        <li>Jupyter notebook<ul><li>reference / <a href="#ch05lvl1sec41" title="Apache Spark notebooks" class="link">Apache Spark notebooks</a></li></ul></li>
      </ul>
      <h2>K</h2>
      <ul>
        <li>Knitr package<ul><li>installing / <a href="#ch01lvl1sec15" title="Step 2: Installing the Knitr package" class="link">Step 2: Installing the Knitr package</a></li></ul></li>
        <li>Kolmogorov-Smirnov (KS) / <a href="#ch05lvl1sec45" title="Kolmogorov-Smirnov" class="link">Kolmogorov-Smirnov</a></li>
      </ul>
      <h2>L</h2>
      <ul>
        <li>Last Observation Carried Forward (LOCF)<ul><li>about / <a href="#ch02lvl1sec21" title="Dataset joining with the R data table package" class="link">Dataset joining with the R data table package</a></li></ul></li>
        <li>linear regression<ul><li>about / <a href="#ch06lvl1sec50" title="Regression models" class="link">Regression models</a></li></ul> / <a href="#ch08lvl1sec65" title="About regression" class="link">About regression</a></li>
        <li>LogFile<ul><li>feature extraction / <a href="#ch04lvl1sec35" title="Feature extraction from LogFile" class="link">Feature extraction from LogFile</a></li></ul></li>
        <li>logistic regression<ul><li>about / <a href="#ch06lvl1sec50" title="Regression models" class="link">Regression models</a></li></ul></li>
        <li>logistic regression  / <a href="#ch08lvl1sec65" title="About regression" class="link">About regression</a></li>
      </ul>
      <h2>M</h2>
      <ul>
        <li>machine learning<ul><li>Spark, computing / <a href="#ch01lvl1sec09" title="Spark computing for machine learning" class="link">Spark computing for machine learning</a></li></ul></li>
        <li>machine learning (ML)<ul><li>notebook approach / <a href="#ch01lvl1sec15" title="Notebook approach for ML" class="link">Notebook approach for ML</a></li></ul></li>
        <li>machine learning algorithms<ul><li>about / <a href="#ch01lvl1sec10" title="Machine learning algorithms" class="link">Machine learning algorithms</a></li></ul></li>
        <li>machine learning methods, Telco Data<ul><li>about / <a href="#ch10lvl1sec79" title="Methods for learning from Telco Data" class="link">Methods for learning from Telco Data</a></li><li>descriptive statistics / <a href="#ch10lvl1sec79" title="Descriptive statistics and visualization" class="link">Descriptive statistics and visualization</a></li><li>visualization / <a href="#ch10lvl1sec79" title="Descriptive statistics and visualization" class="link">Descriptive statistics and visualization</a></li><li>linear regression model / <a href="#ch10lvl1sec79" title="Linear and logistic regression models" class="link">Linear and logistic regression models</a></li><li>logistic regression model / <a href="#ch10lvl1sec79" title="Linear and logistic regression models" class="link">Linear and logistic regression models</a></li><li>random forest / <a href="#ch10lvl1sec79" title="Decision tree and random forest" class="link">Decision tree and random forest</a></li><li>decision tree / <a href="#ch10lvl1sec79" title="Decision tree and random forest" class="link">Decision tree and random forest</a></li></ul></li>
        <li>methods, for holistic view<ul><li>about / <a href="#ch03lvl1sec26" title="Methods for a holistic view" class="link">Methods for a holistic view</a></li><li>regression modeling / <a href="#ch03lvl1sec26" title="Regression modeling" class="link">Regression modeling</a></li><li>SEM approach / <a href="#ch03lvl1sec26" title="The SEM approach" class="link">The SEM approach</a></li><li>decision trees / <a href="#ch03lvl1sec26" title="Decision trees" class="link">Decision trees</a></li></ul></li>
        <li>methods, for recommendation<ul><li>about / <a href="#ch07lvl1sec58" title="Methods for recommendation" class="link">Methods for recommendation</a></li><li>collaborative filtering / <a href="#ch07lvl1sec58" title="Collaborative filtering" class="link">Collaborative filtering</a></li><li>coding, preparing / <a href="#ch07lvl1sec58" title="Preparing coding" class="link">Preparing coding</a></li></ul></li>
        <li>methods, for risk scoring<ul><li>logistic regression / <a href="#ch05lvl1sec42" title="Logistic regression" class="link">Logistic regression</a></li><li>coding, preparing in R / <a href="#ch05lvl1sec42" title="Preparing coding in R" class="link">Preparing coding in R</a></li><li>Random Forest / <a href="#ch05lvl1sec42" title="Random forest and decision trees" class="link">Random forest and decision trees</a></li><li>decision trees / <a href="#ch05lvl1sec42" title="Random forest and decision trees" class="link">Random forest and decision trees</a></li><li>coding, preparing / <a href="#ch05lvl1sec42" title="Preparing coding" class="link">Preparing coding</a></li></ul></li>
        <li>ML frameworks<ul><li>about / <a href="#ch01lvl1sec12" title="ML frameworks, RM4Es and Spark computing" class="link">ML frameworks, RM4Es and Spark computing</a>, <a href="#ch01lvl1sec12" title="ML frameworks" class="link">ML frameworks</a></li></ul></li>
        <li>MLlib<ul><li>about / <a href="#ch01lvl1sec11" title="MLlib" class="link">MLlib</a>, <a href="#ch08lvl1sec66" title="Feature development" class="link">Feature development</a></li><li>URL / <a href="#ch01lvl1sec11" title="MLlib" class="link">MLlib</a>, <a href="#ch08lvl1sec66" title="Principal components analysis" class="link">Principal components analysis</a></li><li>SystemML / <a href="#ch01lvl1sec11" title="Other ML libraries" class="link">Other ML libraries</a></li><li>implementing, for model estimation / <a href="#ch04lvl1sec36" title="MLlib implementation" class="link">MLlib implementation</a></li><li>URL, for feature selection / <a href="#ch06lvl1sec51" title="Feature selection" class="link">Feature selection</a></li><li>used, for RMSE calculation / <a href="#ch09lvl1sec75" title="RMSE calculation with MLlib" class="link">RMSE calculation with MLlib</a></li></ul></li>
        <li>Mllib<ul><li>URL / <a href="#ch03lvl1sec27" title="PCA" class="link">PCA</a></li></ul></li>
        <li>MLlib, parameters<ul><li>numBlocks / <a href="#ch07lvl1sec58" title="Collaborative filtering" class="link">Collaborative filtering</a></li><li>rank / <a href="#ch07lvl1sec58" title="Collaborative filtering" class="link">Collaborative filtering</a></li><li>iterations / <a href="#ch07lvl1sec58" title="Collaborative filtering" class="link">Collaborative filtering</a></li><li>lambda / <a href="#ch07lvl1sec58" title="Collaborative filtering" class="link">Collaborative filtering</a></li><li>implicitPrefs / <a href="#ch07lvl1sec58" title="Collaborative filtering" class="link">Collaborative filtering</a></li><li>alpha / <a href="#ch07lvl1sec58" title="Collaborative filtering" class="link">Collaborative filtering</a></li></ul></li>
        <li>MLlib - PMML model export<ul><li>URL / <a href="#ch06lvl1sec55" title="Deployment" class="link">Deployment</a></li></ul></li>
        <li>MLlib feature extraction<ul><li>URL / <a href="#ch06lvl1sec51" title="Feature extraction" class="link">Feature extraction</a></li></ul></li>
        <li>MLlib guide<ul><li>reference / <a href="#ch07lvl1sec58" title="Collaborative filtering" class="link">Collaborative filtering</a></li></ul></li>
        <li>ML workflows<ul><li>about / <a href="#ch01lvl1sec13" title="ML workflows and Spark pipelines" class="link">ML workflows and Spark pipelines</a>, <a href="#ch01lvl1sec13" title="ML as a step-by-step workflow" class="link">ML as a step-by-step workflow</a></li><li>examples / <a href="#ch01lvl1sec14" title="ML workflow examples" class="link">ML workflow examples</a></li></ul></li>
        <li>model deployment, Telco Data<ul><li>about / <a href="#ch10lvl1sec84" title="Model deployment" class="link">Model deployment</a></li><li>alerts, sending / <a href="#ch10lvl1sec84" title="Rules to send out alerts" class="link">Rules to send out alerts</a></li><li>scores, producing / <a href="#ch10lvl1sec84" title="Scores subscribers for churn and for Call Center calls" class="link">Scores subscribers for churn and for Call Center calls</a></li><li>purchase propensity, predicting / <a href="#ch10lvl1sec84" title="Scores subscribers for purchase propensity" class="link">Scores subscribers for purchase propensity</a></li></ul></li>
        <li>model estimation<ul><li>about / <a href="#ch04lvl1sec36" title="Model estimation" class="link">Model estimation</a>, <a href="#ch08lvl1sec67" title="Model estimation" class="link">Model estimation</a>, <a href="#ch09lvl1sec74" title="Model estimation" class="link">Model estimation</a></li><li>MLlib, implementing / <a href="#ch04lvl1sec36" title="MLlib implementation" class="link">MLlib implementation</a></li><li>R notebooks, implementing / <a href="#ch04lvl1sec36" title="R notebooks implementation" class="link">R notebooks implementation</a></li><li>Spark implementation, Zeppelin notebook used / <a href="#ch08lvl1sec67" title="Spark implementation with the Zeppelin notebook" class="link">Spark implementation with the Zeppelin notebook</a></li><li>Spark implementation, with Zeppelin notebook / <a href="#ch09lvl1sec74" title="Spark implementation with the Zeppelin notebook" class="link">Spark implementation with the Zeppelin notebook</a></li><li>Spark implementation, with R notebook / <a href="#ch09lvl1sec74" title="Spark implementation with the R notebook" class="link">Spark implementation with the R notebook</a></li></ul></li>
        <li>model estimation, holistic view<ul><li>about / <a href="#ch03lvl1sec28" title="Model estimation" class="link">Model estimation</a></li><li>MLlib implementation / <a href="#ch03lvl1sec28" title="MLlib implementation" class="link">MLlib implementation</a></li><li>R notebooks implementation / <a href="#ch03lvl1sec28" title="The R notebooks' implementation" class="link">The R notebooks' implementation</a></li></ul></li>
        <li>model estimation, open data<ul><li>about / <a href="#ch11lvl1sec88" title="Model estimation" class="link">Model estimation</a></li><li>SPSS Analytics Server / <a href="#ch11lvl1sec88" title="SPSS on Spark – SPSS Analytics Server" class="link">SPSS on Spark – SPSS Analytics Server</a></li><li>model evaluation / <a href="#ch11lvl1sec88" title="Model evaluation" class="link">Model evaluation</a></li><li>RMSE, calculating with MLlib / <a href="#ch11lvl1sec88" title="RMSE calculations with MLlib" class="link">RMSE calculations with MLlib</a></li><li>RMSE, calculating with R / <a href="#ch11lvl1sec88" title="RMSE calculations with R" class="link">RMSE calculations with R</a></li></ul></li>
        <li>model estimation, recommendation<ul><li>about / <a href="#ch07lvl1sec60" title="Model estimation" class="link">Model estimation</a></li><li>SPSS on Spark / <a href="#ch07lvl1sec60" title="SPSS on Spark – the SPSS Analytics server" class="link">SPSS on Spark – the SPSS Analytics server</a></li></ul></li>
        <li>model estimation, risk scoring<ul><li>about / <a href="#ch05lvl1sec44" title="Model estimation" class="link">Model estimation</a></li><li>DataScientistWorkbench for R Notebooks / <a href="#ch05lvl1sec44" title="The DataScientistWorkbench for R notebooks" class="link">The DataScientistWorkbench for R notebooks</a></li><li>R Notebooks implementation / <a href="#ch05lvl1sec44" title="R notebooks implementation" class="link">R notebooks implementation</a></li></ul></li>
        <li>model estimation, Telco Data<ul><li>about / <a href="#ch10lvl1sec81" title="Model estimation" class="link">Model estimation</a></li><li>SPSS Analytics Server / <a href="#ch10lvl1sec81" title="SPSS on Spark – SPSS Analytics Server" class="link">SPSS on Spark – SPSS Analytics Server</a></li></ul></li>
        <li>model evaluation<ul><li>about / <a href="#ch04lvl1sec37" title="Model evaluation" class="link">Model evaluation</a>, <a href="#ch08lvl1sec68" title="Model evaluation" class="link">Model evaluation</a>, <a href="#ch08lvl1sec68" title="A quick evaluation" class="link">A quick evaluation</a>, <a href="#ch09lvl1sec75" title="Model evaluation" class="link">Model evaluation</a></li><li>performing / <a href="#ch04lvl1sec37" title="A quick evaluation" class="link">A quick evaluation</a></li><li>confusion matrix / <a href="#ch04lvl1sec37" title="Confusion matrix and false positive ratios" class="link">Confusion matrix and false positive ratios</a></li><li>false positive ratios / <a href="#ch04lvl1sec37" title="Confusion matrix and false positive ratios" class="link">Confusion matrix and false positive ratios</a></li><li>confusion matrix and error ratios / <a href="#ch08lvl1sec68" title="The confusion matrix and error ratios" class="link">The confusion matrix and error ratios</a></li><li>RMSE calculation, with MLlib / <a href="#ch09lvl1sec75" title="RMSE calculation with MLlib" class="link">RMSE calculation with MLlib</a></li><li>RMSE calculation, with R / <a href="#ch09lvl1sec75" title="RMSE calculation with R" class="link">RMSE calculation with R</a></li></ul></li>
        <li>model evaluation, holistic view<ul><li>about / <a href="#ch03lvl1sec29" title="Model evaluation" class="link">Model evaluation</a></li><li>quick evaluations / <a href="#ch03lvl1sec29" title="Quick evaluations" class="link">Quick evaluations</a></li><li>RMSE / <a href="#ch03lvl1sec29" title="RMSE" class="link">RMSE</a></li><li>ROC curves / <a href="#ch03lvl1sec29" title="ROC curves" class="link">ROC curves</a></li></ul></li>
        <li>model evaluation, recommendation<ul><li>about / <a href="#ch07lvl1sec61" title="Model evaluation" class="link">Model evaluation</a></li></ul></li>
        <li>model evaluation, risk scoring<ul><li>about / <a href="#ch05lvl1sec45" title="Model evaluation" class="link">Model evaluation</a></li><li>confusion matrix / <a href="#ch05lvl1sec45" title="Confusion matrix" class="link">Confusion matrix</a></li><li>ROC / <a href="#ch05lvl1sec45" title="ROC" class="link">ROC</a></li><li>Kolmogorov-Smirnov (KS) / <a href="#ch05lvl1sec45" title="Kolmogorov-Smirnov" class="link">Kolmogorov-Smirnov</a></li></ul></li>
        <li>model evaluation, Telco Data<ul><li>about / <a href="#ch10lvl1sec82" title="Model evaluation" class="link">Model evaluation</a></li><li>RMSE, calculating with MLlib / <a href="#ch10lvl1sec82" title="RMSE calculations with MLlib" class="link">RMSE calculations with MLlib</a></li><li>RMSE, calculating with R / <a href="#ch10lvl1sec82" title="RMSE calculations with R" class="link">RMSE calculations with R</a></li><li>error ratios, calculating with 
MLlib / <a href="#ch10lvl1sec82" title="Confusion matrix and error ratios with MLlib and R" class="link">Confusion matrix and error ratios with MLlib and R</a></li><li>confusion matrix, calculating with R / <a href="#ch10lvl1sec82" title="Confusion matrix and error ratios with MLlib and R" class="link">Confusion matrix and error ratios with MLlib and R</a></li></ul></li>
      </ul>
      <h2>N</h2>
      <ul>
        <li>notebook approach<ul><li>for machine learning (ML) / <a href="#ch01lvl1sec15" title="Notebook approach for ML" class="link">Notebook approach for ML</a></li></ul></li>
      </ul>
      <h2>O</h2>
      <ul>
        <li>open data<ul><li>use case / <a href="#ch11lvl1sec86" title="Spark for learning from open data" class="link">Spark for learning from open data</a>, <a href="#ch11lvl1sec86" title="The use case" class="link">The use case</a></li><li>reference link / <a href="#ch11lvl1sec86" title="The use case" class="link">The use case</a></li><li>Spark, computing / <a href="#ch11lvl1sec86" title="Spark computing" class="link">Spark computing</a></li><li>scoring / <a href="#ch11lvl1sec86" title="Methods for scoring and ranking" class="link">Methods for scoring and ranking</a></li><li>ranking / <a href="#ch11lvl1sec86" title="Methods for scoring and ranking" class="link">Methods for scoring and ranking</a></li><li>cluster analysis / <a href="#ch11lvl1sec86" title="Cluster analysis" class="link">Cluster analysis</a></li><li>principal component analysis (PCA) / <a href="#ch11lvl1sec86" title="Principal component analysis" class="link">Principal component analysis</a></li><li>regression models / <a href="#ch11lvl1sec86" title="Regression models" class="link">Regression models</a></li><li>score, resembling / <a href="#ch11lvl1sec86" title="Score resembling" class="link">Score resembling</a></li></ul></li>
        <li>OpenRefine<ul><li>about / <a href="#ch05lvl1sec43" title="OpenRefine" class="link">OpenRefine</a></li><li>URL / <a href="#ch11lvl1sec87" title="Data cleaning" class="link">Data cleaning</a></li></ul></li>
      </ul>
      <h2>P</h2>
      <ul>
        <li>PCA<ul><li>about / <a href="#ch03lvl1sec27" title="PCA" class="link">PCA</a></li><li>reference / <a href="#ch03lvl1sec27" title="PCA" class="link">PCA</a></li></ul></li>
        <li>PipelineStages<ul><li>about / <a href="#ch01lvl1sec14" title="ML workflow examples" class="link">ML workflow examples</a></li></ul></li>
        <li>Predictive Model Markup Language (PMML)<ul><li>about / <a href="#ch03lvl1sec31" title="Deployment" class="link">Deployment</a>, <a href="#ch04lvl1sec39" title="Deploying fraud detection" class="link">Deploying fraud detection</a>, <a href="#ch06lvl1sec55" title="Deployment" class="link">Deployment</a>, <a href="#ch10lvl1sec84" title="Model deployment" class="link">Model deployment</a>, <a href="#ch11lvl1sec90" title="Deployment" class="link">Deployment</a></li></ul> / <a href="#ch08lvl1sec70" title="Deployment" class="link">Deployment</a>, <a href="#ch09lvl1sec76" title="Visualizing trends" class="link">Visualizing trends</a></li>
        <li>Principal Component Analysis (PCA) / <a href="#ch09lvl1sec73" title="Feature selection" class="link">Feature selection</a><ul><li>about / <a href="#ch10lvl1sec80" title="Feature development and selection" class="link">Feature development and selection</a></li></ul></li>
        <li>principal component analysis (PCA)<ul><li>about / <a href="#ch11lvl1sec86" title="Principal component analysis" class="link">Principal component analysis</a></li><li>URL / <a href="#ch11lvl1sec86" title="Principal component analysis" class="link">Principal component analysis</a></li></ul></li>
        <li>Principal components analysis (PCA)<ul><li>about / <a href="#ch08lvl1sec66" title="Principal components analysis" class="link">Principal components analysis</a></li><li>Subject knowledge aid / <a href="#ch08lvl1sec66" title="Subject knowledge aid" class="link">Subject knowledge aid</a></li></ul></li>
      </ul>
      <h2>R</h2>
      <ul>
        <li>R<ul><li>dataframe API / <a href="#ch01lvl1sec12" title="Dataframes API for R" class="link">Dataframes API for R</a></li><li>dataset reorganization / <a href="#ch02lvl1sec20" title="Dataset reorganizing with R on Spark" class="link">Dataset reorganizing with R on Spark</a></li><li>feature extraction / <a href="#ch02lvl1sec22" title="Feature development with R" class="link">Feature development with R</a></li><li>used, for RMSE calculation / <a href="#ch09lvl1sec75" title="RMSE calculation with R" class="link">RMSE calculation with R</a></li></ul></li>
        <li>Random forest<ul><li>about / <a href="#ch04lvl1sec34" title="Random forest" class="link">Random forest</a></li><li>reference link / <a href="#ch04lvl1sec34" title="Random forest" class="link">Random forest</a></li></ul></li>
        <li>Random Forest<ul><li>for churn prediction / <a href="#ch06lvl1sec50" title="Decision trees and Random forest" class="link">Decision trees and Random forest</a></li><li>URL / <a href="#ch06lvl1sec50" title="Decision trees and Random forest" class="link">Decision trees and Random forest</a></li></ul></li>
        <li>random forest<ul><li>URL / <a href="#ch10lvl1sec79" title="Decision tree and random forest" class="link">Decision tree and random forest</a></li></ul></li>
        <li>Receiver Operating Characteristic curve (ROC) / <a href="#ch05lvl1sec45" title="ROC" class="link">ROC</a></li>
        <li>recommendation deployment<ul><li>about / <a href="#ch07lvl1sec62" title="Recommendation deployment" class="link">Recommendation deployment</a></li></ul></li>
        <li>recommendations, on Spark<ul><li>Spark, for recommendation engine / <a href="#ch07lvl1sec57" title="Apache Spark for a recommendation engine" class="link">Apache Spark for a recommendation engine</a></li></ul></li>
        <li>regression models<ul><li>for churn prediction / <a href="#ch06lvl1sec50" title="Regression models" class="link">Regression models</a></li><li>linear regression / <a href="#ch06lvl1sec50" title="Regression models" class="link">Regression models</a>, <a href="#ch08lvl1sec65" title="About regression" class="link">About regression</a>, <a href="#ch09lvl1sec72" title="About regression" class="link">About regression</a></li><li>logistic regression / <a href="#ch06lvl1sec50" title="Regression models" class="link">Regression models</a>, <a href="#ch08lvl1sec65" title="About regression" class="link">About regression</a>, <a href="#ch09lvl1sec72" title="About regression" class="link">About regression</a></li><li>about / <a href="#ch08lvl1sec65" title="Regression models" class="link">Regression models</a>, <a href="#ch09lvl1sec72" title="Regression models" class="link">Regression models</a></li><li>code, preparing for / <a href="#ch08lvl1sec65" title="Preparing for coding" class="link">Preparing for coding</a></li><li>coding, preparation steps / <a href="#ch09lvl1sec72" title="Preparing for coding" class="link">Preparing for coding</a></li></ul></li>
        <li>repeatability<ul><li>about / <a href="#ch02lvl1sec23" title="Repeatability and automation" class="link">Repeatability and automation</a></li></ul></li>
        <li>ReporteRs R package<ul><li>URL / <a href="#ch02lvl1sec22" title="Feature development with R" class="link">Feature development with R</a></li></ul></li>
        <li>Research Methods Four Elements (RM4Es)<ul><li>about / <a href="#ch01lvl1sec12" title="RM4Es" class="link">RM4Es</a></li><li>Equation / <a href="#ch01lvl1sec12" title="RM4Es" class="link">RM4Es</a></li><li>Estimation / <a href="#ch01lvl1sec12" title="RM4Es" class="link">RM4Es</a></li><li>Evaluation / <a href="#ch01lvl1sec12" title="RM4Es" class="link">RM4Es</a></li><li>Explanation / <a href="#ch01lvl1sec12" title="RM4Es" class="link">RM4Es</a></li></ul></li>
        <li>Resilient Distributed Dataset (RDD)<ul><li>about / <a href="#ch01lvl1sec08" title="Spark advantages" class="link">Spark advantages</a>, <a href="#ch01lvl1sec12" title="Spark RDD" class="link">Spark RDD</a></li></ul></li>
        <li>results<ul><li>about / <a href="#ch08lvl1sec69" title="Results explanation" class="link">Results explanation</a></li><li>interventions impact, calculating / <a href="#ch08lvl1sec69" title="Calculating the impact of interventions" class="link">Calculating the impact of interventions</a></li><li>main causes impact, calculating / <a href="#ch08lvl1sec69" title="Calculating the impact of main causes" class="link">Calculating the impact of main causes</a></li><li>scoring / <a href="#ch08lvl1sec70" title="Scoring" class="link">Scoring</a></li><li>explanation / <a href="#ch09lvl1sec76" title="Explanations of the results" class="link">Explanations of the results</a></li><li>biggest influencers / <a href="#ch09lvl1sec76" title="Biggest influencers" class="link">Biggest influencers</a></li><li>trends, visualizing / <a href="#ch09lvl1sec76" title="Visualizing trends" class="link">Visualizing trends</a></li></ul></li>
        <li>results, open data<ul><li>about / <a href="#ch11lvl1sec89" title="Results explanation" class="link">Results explanation</a></li><li>ranks, comparing / <a href="#ch11lvl1sec89" title="Comparing ranks" class="link">Comparing ranks</a></li><li>impacts, predicting / <a href="#ch11lvl1sec89" title="Biggest influencers" class="link">Biggest influencers</a></li><li>alerts, sending / <a href="#ch11lvl1sec90" title="Rules for sending out alerts" class="link">Rules for sending out alerts</a></li><li>school districts, ranking / <a href="#ch11lvl1sec90" title="Scores for ranking school districts" class="link">Scores for ranking school districts</a></li></ul></li>
        <li>results, Telco Data<ul><li>about / <a href="#ch10lvl1sec83" title="Results explanation" class="link">Results explanation</a></li><li>descriptive statistics / <a href="#ch10lvl1sec83" title="Descriptive statistics and visualizations" class="link">Descriptive statistics and visualizations</a></li><li>visualizations / <a href="#ch10lvl1sec83" title="Descriptive statistics and visualizations" class="link">Descriptive statistics and visualizations</a></li><li>impacts, analizing / <a href="#ch10lvl1sec83" title="Biggest influencers" class="link">Biggest influencers</a></li><li>insights / <a href="#ch10lvl1sec83" title="Special insights" class="link">Special insights</a></li><li>trends, visualizing / <a href="#ch10lvl1sec83" title="Visualizing trends" class="link">Visualizing trends</a></li></ul></li>
        <li>results explanation<ul><li>about / <a href="#ch04lvl1sec38" title="Results explanation" class="link">Results explanation</a></li><li>influencing variables / <a href="#ch04lvl1sec38" title="Big influencers and their impacts" class="link">Big influencers and their impacts</a></li></ul></li>
        <li>results explanation, holistic view<ul><li>about / <a href="#ch03lvl1sec30" title="Results explanation" class="link">Results explanation</a></li><li>impacts assessments / <a href="#ch03lvl1sec30" title="Impact assessments" class="link">Impact assessments</a></li></ul></li>
        <li>results explanation, risk scoring<ul><li>about / <a href="#ch05lvl1sec46" title="Results explanation" class="link">Results explanation</a></li><li>big influencers / <a href="#ch05lvl1sec46" title="Big influencers and their impacts" class="link">Big influencers and their impacts</a></li></ul></li>
        <li>risk scoring<ul><li>methods / <a href="#ch05lvl1sec42" title="Methods of risk scoring" class="link">Methods of risk scoring</a></li></ul></li>
        <li>R Markdown<ul><li>about / <a href="#ch01lvl1sec15" title="Notebook approach for ML" class="link">Notebook approach for ML</a></li><li>R studio, downloading / <a href="#ch01lvl1sec15" title="Step 1: Getting the software ready" class="link">Step 1: Getting the software ready</a></li><li>Knitr package, installing / <a href="#ch01lvl1sec15" title="Step 2: Installing the Knitr package" class="link">Step 2: Installing the Knitr package</a></li><li>report, creating / <a href="#ch01lvl1sec15" title="Step 3: Creating a simple report" class="link">Step 3: Creating a simple report</a></li></ul></li>
        <li>RMSE (Root-Mean-Square Error)<ul><li>about / <a href="#ch03lvl1sec29" title="Model evaluation" class="link">Model evaluation</a>, <a href="#ch03lvl1sec29" title="RMSE" class="link">RMSE</a></li><li>example  / <a href="#ch03lvl1sec29" title="RMSE" class="link">RMSE</a></li></ul></li>
        <li>RMSE calculation<ul><li>with MLlib / <a href="#ch09lvl1sec75" title="RMSE calculation with MLlib" class="link">RMSE calculation with MLlib</a></li><li>with R / <a href="#ch09lvl1sec75" title="RMSE calculation with R" class="link">RMSE calculation with R</a></li></ul></li>
        <li>R notebook<ul><li>references / <a href="#ch05lvl1sec41" title="Apache Spark notebooks" class="link">Apache Spark notebooks</a></li><li>used, for Spark implementation / <a href="#ch09lvl1sec74" title="Spark implementation with the R notebook" class="link">Spark implementation with the R notebook</a></li></ul></li>
        <li>R notebooks<ul><li>implementing, for model estimation / <a href="#ch04lvl1sec36" title="R notebooks implementation" class="link">R notebooks implementation</a></li></ul></li>
        <li>R Notebooks implementation<ul><li>about / <a href="#ch05lvl1sec44" title="R notebooks implementation" class="link">R notebooks implementation</a></li><li>logistic regression / <a href="#ch05lvl1sec44" title="R notebooks implementation" class="link">R notebooks implementation</a></li><li>Random Forest / <a href="#ch05lvl1sec44" title="R notebooks implementation" class="link">R notebooks implementation</a></li><li>decision tree / <a href="#ch05lvl1sec44" title="R notebooks implementation" class="link">R notebooks implementation</a></li></ul></li>
        <li>ROC (Receiver Operating Characteristic)<ul><li>about / <a href="#ch03lvl1sec29" title="Model evaluation" class="link">Model evaluation</a></li></ul></li>
        <li>ROCR<ul><li>URL / <a href="#ch04lvl1sec37" title="A quick evaluation" class="link">A quick evaluation</a></li></ul></li>
        <li>Root Mean Square Error (RMSE)<ul><li>about / <a href="#ch10lvl1sec82" title="Model evaluation" class="link">Model evaluation</a>, <a href="#ch11lvl1sec88" title="Model evaluation" class="link">Model evaluation</a></li></ul></li>
        <li>R package PMML<ul><li>reference / <a href="#ch03lvl1sec31" title="Deployment" class="link">Deployment</a></li></ul></li>
        <li>R studio<ul><li>URL / <a href="#ch01lvl1sec15" title="Step 1: Getting the software ready" class="link">Step 1: Getting the software ready</a></li></ul></li>
      </ul>
      <h2>S</h2>
      <ul>
        <li>SampleClean<ul><li>used, for data cleaning / <a href="#ch02lvl1sec18" title="Data cleaning made easy" class="link">Data cleaning made easy</a></li><li>URL / <a href="#ch02lvl1sec18" title="Data cleaning made easy" class="link">Data cleaning made easy</a>, <a href="#ch02lvl1sec19" title="Record deduplication" class="link">Record deduplication</a></li><li>used, for identity matching / <a href="#ch02lvl1sec19" title="Identity matching made better" class="link">Identity matching made better</a></li></ul></li>
        <li>service forecasting, Spark used<ul><li>about / <a href="#ch09lvl1sec72" title="Spark for service forecasting" class="link">Spark for service forecasting</a></li><li>use case / <a href="#ch09lvl1sec72" title="The use case" class="link">The use case</a></li><li>use case, reference links / <a href="#ch09lvl1sec72" title="The use case" class="link">The use case</a></li><li>computing / <a href="#ch09lvl1sec72" title="Spark computing" class="link">Spark computing</a></li><li>methods / <a href="#ch09lvl1sec72" title="Methods of service forecasting" class="link">Methods of service forecasting</a></li><li>regression models / <a href="#ch09lvl1sec72" title="Regression models" class="link">Regression models</a></li></ul></li>
        <li>shared variables<ul><li>broadcast variables / <a href="#ch01lvl1sec12" title="The Spark computing framework" class="link">The Spark computing framework</a></li><li>accumulators / <a href="#ch01lvl1sec12" title="The Spark computing framework" class="link">The Spark computing framework</a></li></ul></li>
        <li>Spark<ul><li>overview / <a href="#ch01lvl1sec08" title="Spark overview and Spark advantages" class="link">Spark overview and Spark advantages</a>, <a href="#ch01lvl1sec08" title="Spark overview" class="link">Spark overview</a></li><li>advantages / <a href="#ch01lvl1sec08" title="Spark overview and Spark advantages" class="link">Spark overview and Spark advantages</a>, <a href="#ch01lvl1sec08" title="Spark advantages" class="link">Spark advantages</a></li><li>URL / <a href="#ch01lvl1sec08" title="Spark overview" class="link">Spark overview</a></li><li>URL, for documentation / <a href="#ch01lvl1sec08" title="Spark overview" class="link">Spark overview</a></li><li>reference link / <a href="#ch01lvl1sec08" title="Spark advantages" class="link">Spark advantages</a></li><li>computing, for machine learning / <a href="#ch01lvl1sec09" title="Spark computing for machine learning" class="link">Spark computing for machine learning</a></li><li>holistic view / <a href="#ch03lvl1sec25" title="Spark for a holistic view" class="link">Spark for a holistic view</a></li><li>used, for service forecasting / <a href="#ch09lvl1sec72" title="Spark for service forecasting" class="link">Spark for service forecasting</a></li></ul></li>
        <li>Spark, for recommendation engine<ul><li>use case / <a href="#ch07lvl1sec57" title="The use case" class="link">The use case</a></li><li>SPSS on Spark / <a href="#ch07lvl1sec57" title="SPSS on Spark" class="link">SPSS on Spark</a></li></ul></li>
        <li>Spark, for risk scoring<ul><li>about / <a href="#ch05lvl1sec41" title="Spark for risk scoring" class="link">Spark for risk scoring</a></li><li>use case / <a href="#ch05lvl1sec41" title="The use case" class="link">The use case</a></li><li>Apache Spark Notebooks / <a href="#ch05lvl1sec41" title="Apache Spark notebooks" class="link">Apache Spark notebooks</a></li></ul></li>
        <li>spark-ts library<ul><li>reference link / <a href="#ch09lvl1sec72" title="Preparing for coding" class="link">Preparing for coding</a></li></ul></li>
        <li>Spark computing<ul><li>about / <a href="#ch08lvl1sec64" title="Spark computing" class="link">Spark computing</a></li></ul></li>
        <li>Spark computing framework<ul><li>about / <a href="#ch01lvl1sec12" title="The Spark computing framework" class="link">The Spark computing framework</a></li></ul></li>
        <li>Spark dataframe<ul><li>about / <a href="#ch01lvl1sec12" title="Spark dataframes" class="link">Spark dataframes</a></li><li>URL / <a href="#ch01lvl1sec12" title="Spark dataframes" class="link">Spark dataframes</a></li></ul></li>
        <li>Spark DataSource API<ul><li>URL / <a href="#ch02lvl1sec17" title="Loading datasets into Spark" class="link">Loading datasets into Spark</a></li></ul></li>
        <li>Spark implementation<ul><li>Zeppelin notebook, using / <a href="#ch09lvl1sec74" title="Spark implementation with the Zeppelin notebook" class="link">Spark implementation with the Zeppelin notebook</a></li><li>R notebook, using / <a href="#ch09lvl1sec74" title="Spark implementation with the R notebook" class="link">Spark implementation with the R notebook</a></li></ul></li>
        <li>Spark MLlib<ul><li>feature extraction / <a href="#ch02lvl1sec22" title="Feature development with Spark MLlib" class="link">Feature development with Spark MLlib</a></li><li>URL / <a href="#ch02lvl1sec22" title="Feature development with Spark MLlib" class="link">Feature development with Spark MLlib</a></li></ul></li>
        <li>Spark notebooks<ul><li>about / <a href="#ch01lvl1sec15" title="Spark notebooks" class="link">Spark notebooks</a></li><li>notebook approach, for machine learning (ML) / <a href="#ch01lvl1sec15" title="Notebook approach for ML" class="link">Notebook approach for ML</a></li><li>Databricks notebook / <a href="#ch01lvl1sec15" title="Spark notebooks" class="link">Spark notebooks</a></li></ul></li>
        <li>Spark pipeline<ul><li>about / <a href="#ch01lvl1sec13" title="ML workflows and Spark pipelines" class="link">ML workflows and Spark pipelines</a></li><li>URL / <a href="#ch01lvl1sec14" title="ML workflow examples" class="link">ML workflow examples</a></li><li>used, for datasets preprocessing / <a href="#ch02lvl1sec23" title="Spark pipelines for dataset preprocessing" class="link">Spark pipelines for dataset preprocessing</a></li></ul></li>
        <li>Spark RDD<ul><li>about / <a href="#ch01lvl1sec12" title="Spark RDD" class="link">Spark RDD</a></li><li>URL / <a href="#ch01lvl1sec12" title="Spark RDD" class="link">Spark RDD</a></li></ul></li>
        <li>SparkSQL<ul><li>about / <a href="#ch04lvl1sec35" title="Feature extraction from LogFile" class="link">Feature extraction from LogFile</a></li></ul></li>
        <li>Spark SQL<ul><li>used, for dataset reorganization / <a href="#ch02lvl1sec20" title="Dataset reorganizing with Spark SQL" class="link">Dataset reorganizing with Spark SQL</a></li><li>URL / <a href="#ch02lvl1sec20" title="Dataset reorganizing with Spark SQL" class="link">Dataset reorganizing with Spark SQL</a>, <a href="#ch02lvl1sec21" title="Dataset joining in Spark" class="link">Dataset joining in Spark</a></li><li>datasets, joining / <a href="#ch02lvl1sec21" title="Dataset joining and its tool – the Spark SQL" class="link">Dataset joining and its tool – the Spark SQL</a></li></ul></li>
        <li>SPSS Analytics Server<ul><li>about / <a href="#ch10lvl1sec81" title="SPSS on Spark – SPSS Analytics Server" class="link">SPSS on Spark – SPSS Analytics Server</a></li></ul></li>
        <li>SPSS Analytics server / <a href="#ch07lvl1sec60" title="SPSS on Spark – the SPSS Analytics server" class="link">SPSS on Spark – the SPSS Analytics server</a></li>
        <li>SPSS on Spark / <a href="#ch07lvl1sec57" title="SPSS on Spark" class="link">SPSS on Spark</a></li>
        <li>SQLContext<ul><li>about / <a href="#ch02lvl1sec21" title="Dataset joining and its tool – the Spark SQL" class="link">Dataset joining and its tool – the Spark SQL</a></li></ul></li>
        <li>Structural Equation Modeling (SEM) / <a href="#ch03lvl1sec26" title="The SEM approach" class="link">The SEM approach</a></li>
        <li>SystemML<ul><li>about / <a href="#ch01lvl1sec11" title="Other ML libraries" class="link">Other ML libraries</a></li></ul></li>
      </ul>
      <h2>T</h2>
      <ul>
        <li>Telco Data<ul><li>using / <a href="#ch10lvl1sec78" title="Spark for using Telco Data" class="link">Spark for using Telco Data</a></li><li>use case / <a href="#ch10lvl1sec78" title="The use case" class="link">The use case</a></li><li>Spark, computing for / <a href="#ch10lvl1sec78" title="Spark computing" class="link">Spark computing</a></li><li>machine learning methods / <a href="#ch10lvl1sec79" title="Methods for learning from Telco Data" class="link">Methods for learning from Telco Data</a></li></ul></li>
        <li>time series modeling<ul><li>about / <a href="#ch09lvl1sec72" title="Time series modeling" class="link">Time series modeling</a></li><li>reference link / <a href="#ch09lvl1sec72" title="About time series" class="link">About time series</a></li><li>coding, preparation steps / <a href="#ch09lvl1sec72" title="Preparing for coding" class="link">Preparing for coding</a></li></ul></li>
        <li>trends, visualizing<ul><li>about / <a href="#ch09lvl1sec76" title="Visualizing trends" class="link">Visualizing trends</a></li><li>sending out alerts, rules / <a href="#ch09lvl1sec76" title="The rules of sending out alerts" class="link">The rules of sending out alerts</a></li><li>city zones, ranking scores / <a href="#ch09lvl1sec76" title="Scores to rank city zones" class="link">Scores to rank city zones</a></li></ul></li>
        <li>True Positive (TP) error rate / <a href="#ch05lvl1sec45" title="ROC" class="link">ROC</a></li>
      </ul>
      <h2>Z</h2>
      <ul>
        <li>Zeppelin<ul><li>URL / <a href="#ch04lvl1sec33" title="Distributed computing" class="link">Distributed computing</a></li></ul></li>
        <li>Zeppeline notebook / <a href="#ch08lvl1sec64" title="Spark computing" class="link">Spark computing</a></li>
        <li>Zeppelin notebook<ul><li>URL / <a href="#ch08lvl1sec64" title="Spark computing" class="link">Spark computing</a></li><li>used, for implementing notebook / <a href="#ch08lvl1sec67" title="Spark implementation with the Zeppelin notebook" class="link">Spark implementation with the Zeppelin notebook</a></li><li>used, for Spark implementation / <a href="#ch09lvl1sec74" title="Spark implementation with the Zeppelin notebook" class="link">Spark implementation with the Zeppelin notebook</a></li></ul></li>
        <li>Zepperlin / <a href="#ch05lvl1sec41" title="Apache Spark notebooks" class="link">Apache Spark notebooks</a></li>
      </ul>
    </div>
  </div></div></div>
</div></div></div></body></html>
