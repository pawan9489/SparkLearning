<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="">

    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0"> <!--320-->
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no, user-scalable=no">

    <link rel="icon" href="../../mapt/images/favicon.ico">

    <link rel="stylesheet" href="../../mapt/css/font-awesome.css">
    <link rel="stylesheet" href="../../mapt/css/google-fonts.css">
    <link rel="stylesheet" href="../../mapt/css/devicon.css">

    <link rel="stylesheet" href="../../mapt/css/bootstrap.css">
    <link rel="stylesheet" href="../../mapt/css/bootstrap-xl.css">
    <link rel="stylesheet" href="../../mapt/css/magnific-popup.css">
    <link rel="stylesheet" href="../../mapt/css/prism.css">
    <link rel="stylesheet" href="../../mapt/css/hljs-github.css">

    <link rel="stylesheet" href="../../mapt/css/mapt.css">
    <link rel="stylesheet" href="../../mapt/css/custom.css">

    <script src="../../mapt/js/jquery.js"></script>
    <script src="../../mapt/js/bootstrap.js"></script>
    <script src="../../mapt/js/jquery.magnific-popup.js"></script>
    <script src="../../mapt/js/highlight.min.js"></script>

    <script src="../../mapt/js/custom.js"></script>
    
    <title>Learning Real-time Processing with Spark Streaming</title>
</head>

<body class="home-body">
    <div id="wrapper">
        <div id="sidebar-wrapper">    
            <ul class="sidebar-nav">
                <div class="list-group" id="sidebar-nav" role="tablist">
                    <li>
                        <a href="../../index.html" class="sidenav-menu-holder back-btn" id="back-link">
                            <span class="sidenav-menu">Book List</span>
                            <span class="pull-left mr5"><i class="fa fa-chevron-left"></i></span>
                        </a>
                    </li>
                    
                    <li class="book-info copyright">
                        <span class="info text-nowrap"><span class="copyleft">&copy;</span><span><strong>RuTracker</strong>.org</span></span>
                    </li>          
                    <li class="book-info copyright">
                        <span class="info text-nowrap">Pub date: <strong>28 Sep 2015</strong></span>
                    </li>         
                    <li class="book-info">
                        <span class="info text-nowrap">Price: â‚¬<strong>28.99</strong></span>
                        <span class="info text-nowrap">ISBN: <strong>9781783987665</strong></span>
                    </li>     
            
                    <li>
                        <a href="graphics/cover.jpg" class="sidenav-menu-holder cover-img">
                            <img src="graphics/cover.jpg" class="cover-image">
                        </a>
                    </li>        
            
                    <div class="book_navigation">
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse1">
                                <div class="section-name">1: Installing and Configuring Spark and Spark Streaming</div>
                            </a>
                        </li>
                        <div id="collapse1" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="1" class="sub-nav">
                                <a href="#ch01">
                                    <div class="section-name">Chapter 1: Installing and Configuring Spark and Spark Streaming</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec08" class="sub-nav">
                                <a href="#ch01lvl1sec08">                    
                                    <div class="section-name">Installation of Spark</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec09" class="sub-nav">
                                <a href="#ch01lvl1sec09">                    
                                    <div class="section-name">Configuring and running the Spark cluster</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec10" class="sub-nav">
                                <a href="#ch01lvl1sec10">                    
                                    <div class="section-name">Your first Spark program</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec11" class="sub-nav">
                                <a href="#ch01lvl1sec11">                    
                                    <div class="section-name">Tools and utilities for administrators/developers</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec12" class="sub-nav">
                                <a href="#ch01lvl1sec12">                    
                                    <div class="section-name">Troubleshooting</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec13" class="sub-nav">
                                <a href="#ch01lvl1sec13">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse2">
                                <div class="section-name">2: Architecture and Components of Spark and Spark Streaming</div>
                            </a>
                        </li>
                        <div id="collapse2" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="2" class="sub-nav">
                                <a href="#ch02">
                                    <div class="section-name">Chapter 2: Architecture and Components of Spark and Spark Streaming</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec14" class="sub-nav">
                                <a href="#ch02lvl1sec14">                    
                                    <div class="section-name">Batch versus real-time data processing</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec15" class="sub-nav">
                                <a href="#ch02lvl1sec15">                    
                                    <div class="section-name">Architecture of Spark</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec16" class="sub-nav">
                                <a href="#ch02lvl1sec16">                    
                                    <div class="section-name">Architecture of Spark Streaming</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec17" class="sub-nav">
                                <a href="#ch02lvl1sec17">                    
                                    <div class="section-name">Your first Spark Streaming program</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec18" class="sub-nav">
                                <a href="#ch02lvl1sec18">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse3">
                                <div class="section-name">3: Processing Distributed Log Files in Real Time</div>
                            </a>
                        </li>
                        <div id="collapse3" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="3" class="sub-nav">
                                <a href="#ch03">
                                    <div class="section-name">Chapter 3: Processing Distributed Log Files in Real Time</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec19" class="sub-nav">
                                <a href="#ch03lvl1sec19">                    
                                    <div class="section-name">Spark packaging structure and client APIs</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec20" class="sub-nav">
                                <a href="#ch03lvl1sec20">                    
                                    <div class="section-name">Resilient distributed datasets and discretized streams</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec21" class="sub-nav">
                                <a href="#ch03lvl1sec21">                    
                                    <div class="section-name">Data loading from distributed and varied sources</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec22" class="sub-nav">
                                <a href="#ch03lvl1sec22">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse4">
                                <div class="section-name">4: Applying Transformations to Streaming Data</div>
                            </a>
                        </li>
                        <div id="collapse4" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="4" class="sub-nav">
                                <a href="#ch04">
                                    <div class="section-name">Chapter 4: Applying Transformations to Streaming Data</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec23" class="sub-nav">
                                <a href="#ch04lvl1sec23">                    
                                    <div class="section-name">Understanding and applying transformation functions</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec24" class="sub-nav">
                                <a href="#ch04lvl1sec24">                    
                                    <div class="section-name">Performance tuning</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec25" class="sub-nav">
                                <a href="#ch04lvl1sec25">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse5">
                                <div class="section-name">5: Persisting Log Analysis Data</div>
                            </a>
                        </li>
                        <div id="collapse5" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="5" class="sub-nav">
                                <a href="#ch05">
                                    <div class="section-name">Chapter 5: Persisting Log Analysis Data</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec26" class="sub-nav">
                                <a href="#ch05lvl1sec26">                    
                                    <div class="section-name">Output operations in Spark Streaming</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec27" class="sub-nav">
                                <a href="#ch05lvl1sec27">                    
                                    <div class="section-name">Integration with Cassandra</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec28" class="sub-nav">
                                <a href="#ch05lvl1sec28">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse6">
                                <div class="section-name">6: Integration with Advanced Spark Libraries</div>
                            </a>
                        </li>
                        <div id="collapse6" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="6" class="sub-nav">
                                <a href="#ch06">
                                    <div class="section-name">Chapter 6: Integration with Advanced Spark Libraries</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec29" class="sub-nav">
                                <a href="#ch06lvl1sec29">                    
                                    <div class="section-name">Querying streaming data in real time</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec30" class="sub-nav">
                                <a href="#ch06lvl1sec30">                    
                                    <div class="section-name">Graph analysis Spark GraphX</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec31" class="sub-nav">
                                <a href="#ch06lvl1sec31">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse7">
                                <div class="section-name">7: Deploying in Production</div>
                            </a>
                        </li>
                        <div id="collapse7" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="7" class="sub-nav">
                                <a href="#ch07">
                                    <div class="section-name">Chapter 7: Deploying in Production</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec32" class="sub-nav">
                                <a href="#ch07lvl1sec32">                    
                                    <div class="section-name">Spark deployment models</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec33" class="sub-nav">
                                <a href="#ch07lvl1sec33">                    
                                    <div class="section-name">High availability and fault tolerance</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec34" class="sub-nav">
                                <a href="#ch07lvl1sec34">                    
                                    <div class="section-name">Monitoring streaming jobs</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec35" class="sub-nav">
                                <a href="#ch07lvl1sec35">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapsebackindex">
                                <div class="section-name">Appendix A: Index</div>
                            </a>
                        </li>
                        <div id="collapsebackindex" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="backindex" class="sub-nav">
                                <a href="#backindex">
                                    <div class="section-name">Chapter Appendix A: Index</div>
                                </a>
                            </li>
                        </div>
                    </div>
                </div>
            </ul>
        </div>
        
        <div id="page-content-wrapper" class="book-page">
            <a href="#" id="menu-toggle" class="toggle-nav"><i class="fa fa-bars fa-2x mr5"></i></a>
            
            <a href="#" id="back_to_top" class="back-to-top"><img src="../../mapt/images/kopimi.svg"></a>
            
            <div class="col-sm-12 col-xl-offset-2 col-xl-8 col-lg-offset-1 col-lg-10">
                <div class="btn-group pull-right mt15 mb30" role="group">
                    <a href="#home" class="btn btn-default">
                        <i class="fa fa-share fa-lg no-text-padding"></i>
                        <span class="hidden-xs ml5">Book Home</span>
                    </a>
                    <button class="btn btn-default" data-nid="23069" id="code-download">
                        <i class="fa fa-file fa-lg"></i>
                        <span class="hidden-xs ml5">Download Code Files</span>
                    </button>
                </div>
            </div>
            <div class="clearfix"></div>
            
            <div id="book-wrapper" class="container-fluid">
                <div class="col-sm-12 col-xl-offset-2 col-xl-8 col-lg-offset-1 col-lg-10" id="home">
                    <h2 class="product-title">Learning Real-time Processing with Spark Streaming</h2>
                    <hr>
                    <div class="row">
                        <div class="col-sm-12">
                            <h5 class="mt10">By Sumit Gupta</h5>
                            <div>
                                <p class="mb20"><b>Building scalable and fault-tolerant streaming applications made easy with Spark streaming</b></p>
                                <a href="#ch01" class="btn btn-info btn-lg pull-right hidden-xs">
                                    Start Reading <i class="fa fa-chevron-right ml5"></i>
                                </a>
                                <a href="#ch01" class="btn btn-info btn-lg btn-block mt20 mb20 visible-xs">
                                    Start Reading <i class="fa fa-chevron-right ml5"></i>
                                </a>
                                <div class="clearfix"></div>
                                <div class="col-sm-12">
                                    <ul id="myTabs" class="nav nav-tabs nav-justified hidden-xs mt20" role="tablist">
                                        <li class="active">
                                            <a href="#info" role="tab" id="info-tab" data-toggle="tab">
                                                <h5>Info</h5>
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#content" role="tab" id="content-tab" data-toggle="tab">
                                                <h5>Contents</h5>
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#author" role="tab" id="author-tab" data-toggle="tab">
                                                <h5>Author</h5>
                                            </a>
                                        </li>
                                    </ul>
                
                                    <ul id="myTabsMobile" class="nav nav-pills text-center nav-stacked visible-xs mb60" role="tablist">
                                        <li class="active">
                                            <a href="#info" role="tab" id="info-tab-responsive" data-toggle="tab">
                                                Info
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#content" role="tab" id="content-tab-responsive" data-toggle="tab">
                                                Contents
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#author" role="tab" id="author-tab-responsive" data-toggle="tab">
                                                Author
                                            </a>
                                        </li>
                                    </ul>
                
                                    <div id="myTabContent" class="tab-content pt30">
                                    
                                        <div role="tabpanel" class="tab-pane active fade in" id="info">
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">Features</h5>
                                            </div>
                                            <div class="hidden-xs">
                                                <h5>Features</h5>
                                            </div>
                                            <hr>
                                            <div>
                                                <ul>
                <li>Process live data streams more efficiently with better fault recovery using Spark Streaming</li>
                <li>Implement and deploy real-time log file analysis</li>
                <li>Learn about integration with Advance Spark Libraries â€“ GraphX, Spark SQL, and MLib.</li>
                </ul>
                                            </div>
                                            <br>
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">Learning</h5>
                                            </div>
                                            <div class="hidden-xs">
                                                <h5>Learning</h5>
                                            </div>
                                            <hr>
                                            <div>
                                                <ul>
                <li>Install and configure Spark and Spark Streaming to execute applications</li>
                <li>Explore the architecture and components of Spark and Spark Streaming to use it as a base for other libraries</li>
                <li>Process distributed log files in real-time to load data from distributed sources</li>
                <li>Apply transformations on streaming data to use its functions</li>
                <li>Integrate Apache Spark with the various advance libraries like MLib and GraphX</li>
                <li>Apply production deployment scenarios to deploy your application</li>
                </ul>
                                            </div>
                                            <br>
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">About</h5>
                                            </div>
                                            <div class="hidden-xs">
                                                <h5>About</h5>
                                            </div>
                                            <hr>
                                            <div>
                                                <p>Using practical examples with easy-to-follow steps, this book will teach you how to build real-time applications with Spark Streaming.</p>
                <p>Starting with installing and setting the required environment, you will write and execute your first program for Spark Streaming. This will be followed by exploring the architecture and components of Spark Streaming along with an overview of libraries/functions exposed by Spark. Next you will be taught about various client APIs for coding in Spark by using the use-case of distributed log file processing. You will then apply various functions to transform and enrich streaming data. Next you will learn how to cache and persist datasets. Moving on you will integrate Apache Spark with various other libraries/components of Spark like Mlib, GraphX, and Spark SQL. Finally, you will learn about deploying your application and cover the different scenarios ranging from standalone mode to distributed mode using Mesos, Yarn, and private data centers or on cloud infrastructure.</p>
                                            </div>
                                        </div>
                                        
                                        <div role="tabpanel" class="tab-pane fade in" id="content">
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">Contents</h5>
                                                <hr>
                                            </div>
                                            <ul>
                                                <div>
                                                    <li data-chapter="1">
                                                        <div class="section-name">1: Installing and Configuring Spark and Spark Streaming</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="1" class="chapter-section">
                                                                    <a href="#ch01">        
                                                                        <div class="section-name">Chapter 1: Installing and Configuring Spark and Spark Streaming</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec08" class="chapter-section">
                                                                    <a href="#ch01lvl1sec08">                    
                                                                        <div class="section-name">Installation of Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec09" class="chapter-section">
                                                                    <a href="#ch01lvl1sec09">                    
                                                                        <div class="section-name">Configuring and running the Spark cluster</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec10" class="chapter-section">
                                                                    <a href="#ch01lvl1sec10">                    
                                                                        <div class="section-name">Your first Spark program</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec11" class="chapter-section">
                                                                    <a href="#ch01lvl1sec11">                    
                                                                        <div class="section-name">Tools and utilities for administrators/developers</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec12" class="chapter-section">
                                                                    <a href="#ch01lvl1sec12">                    
                                                                        <div class="section-name">Troubleshooting</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec13" class="chapter-section">
                                                                    <a href="#ch01lvl1sec13">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="2">
                                                        <div class="section-name">2: Architecture and Components of Spark and Spark Streaming</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="2" class="chapter-section">
                                                                    <a href="#ch02">        
                                                                        <div class="section-name">Chapter 2: Architecture and Components of Spark and Spark Streaming</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec14" class="chapter-section">
                                                                    <a href="#ch02lvl1sec14">                    
                                                                        <div class="section-name">Batch versus real-time data processing</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec15" class="chapter-section">
                                                                    <a href="#ch02lvl1sec15">                    
                                                                        <div class="section-name">Architecture of Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec16" class="chapter-section">
                                                                    <a href="#ch02lvl1sec16">                    
                                                                        <div class="section-name">Architecture of Spark Streaming</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec17" class="chapter-section">
                                                                    <a href="#ch02lvl1sec17">                    
                                                                        <div class="section-name">Your first Spark Streaming program</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec18" class="chapter-section">
                                                                    <a href="#ch02lvl1sec18">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="3">
                                                        <div class="section-name">3: Processing Distributed Log Files in Real Time</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="3" class="chapter-section">
                                                                    <a href="#ch03">        
                                                                        <div class="section-name">Chapter 3: Processing Distributed Log Files in Real Time</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec19" class="chapter-section">
                                                                    <a href="#ch03lvl1sec19">                    
                                                                        <div class="section-name">Spark packaging structure and client APIs</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec20" class="chapter-section">
                                                                    <a href="#ch03lvl1sec20">                    
                                                                        <div class="section-name">Resilient distributed datasets and discretized streams</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec21" class="chapter-section">
                                                                    <a href="#ch03lvl1sec21">                    
                                                                        <div class="section-name">Data loading from distributed and varied sources</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec22" class="chapter-section">
                                                                    <a href="#ch03lvl1sec22">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="4">
                                                        <div class="section-name">4: Applying Transformations to Streaming Data</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="4" class="chapter-section">
                                                                    <a href="#ch04">        
                                                                        <div class="section-name">Chapter 4: Applying Transformations to Streaming Data</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec23" class="chapter-section">
                                                                    <a href="#ch04lvl1sec23">                    
                                                                        <div class="section-name">Understanding and applying transformation functions</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec24" class="chapter-section">
                                                                    <a href="#ch04lvl1sec24">                    
                                                                        <div class="section-name">Performance tuning</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec25" class="chapter-section">
                                                                    <a href="#ch04lvl1sec25">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="5">
                                                        <div class="section-name">5: Persisting Log Analysis Data</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="5" class="chapter-section">
                                                                    <a href="#ch05">        
                                                                        <div class="section-name">Chapter 5: Persisting Log Analysis Data</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec26" class="chapter-section">
                                                                    <a href="#ch05lvl1sec26">                    
                                                                        <div class="section-name">Output operations in Spark Streaming</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec27" class="chapter-section">
                                                                    <a href="#ch05lvl1sec27">                    
                                                                        <div class="section-name">Integration with Cassandra</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec28" class="chapter-section">
                                                                    <a href="#ch05lvl1sec28">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="6">
                                                        <div class="section-name">6: Integration with Advanced Spark Libraries</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="6" class="chapter-section">
                                                                    <a href="#ch06">        
                                                                        <div class="section-name">Chapter 6: Integration with Advanced Spark Libraries</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec29" class="chapter-section">
                                                                    <a href="#ch06lvl1sec29">                    
                                                                        <div class="section-name">Querying streaming data in real time</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec30" class="chapter-section">
                                                                    <a href="#ch06lvl1sec30">                    
                                                                        <div class="section-name">Graph analysis Spark GraphX</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec31" class="chapter-section">
                                                                    <a href="#ch06lvl1sec31">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="7">
                                                        <div class="section-name">7: Deploying in Production</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="7" class="chapter-section">
                                                                    <a href="#ch07">        
                                                                        <div class="section-name">Chapter 7: Deploying in Production</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec32" class="chapter-section">
                                                                    <a href="#ch07lvl1sec32">                    
                                                                        <div class="section-name">Spark deployment models</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec33" class="chapter-section">
                                                                    <a href="#ch07lvl1sec33">                    
                                                                        <div class="section-name">High availability and fault tolerance</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec34" class="chapter-section">
                                                                    <a href="#ch07lvl1sec34">                    
                                                                        <div class="section-name">Monitoring streaming jobs</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec35" class="chapter-section">
                                                                    <a href="#ch07lvl1sec35">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="backindex">
                                                        <div class="section-name">Appendix A: Index</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="backindex" class="chapter-section">
                                                                    <a href="#backindex">
                                                                        <div class="section-name">Chapter Appendix A: Index</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                            </ul>
                                        </div>
                                        
                                        <div role="tabpanel" class="tab-pane fade" id="author">
                                            <div class="visible-xs">
                                                <h4 class="mobile-title">About the Author</h4>
                                                <hr>
                                            </div>
                                            <p><strong>Sumit Gupta</strong></p>
                                            <div>
                                                <p>Sumit Gupta is a seasoned professional, innovator, and technology evangelist with over 100 man months of experience in architecting, managing, and delivering enterprise solutions revolving around a variety of business domains, such as hospitality, healthcare, risk management, insurance, and more. He is passionate about technology and overall he has 15 years of hands-on experience in the software industry. He has been using Big Data and cloud technologies over the past 4 to 5 years to solve complex business problems.</p>
                <p>Sumit also authored Neo4j Essentials, Building Web Applications with Python and Neo4j, and Learning Real-time Processing with Spark Streaming, all with Packt Publishing.</p>
                <p>You can find him on LinkedIn at: <a href="https://in.linkedin.com/in/sumit1001" target="_blank">https://in.linkedin.com/in/sumit1001</a>.</p>
                <p>You can find more information on him at: <a href="https://www.packtpub.com/books/info/authors/sumit-gupta" target="_blank">https://www.packtpub.com/books/info/authors/sumit-gupta</a>.</p>
                                            </div>
                                        </div>
                                        
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    
                    <div class="next-wrapper">
                        <div class="row ns">
                            <hr />
                            <span class="hidden-xs">
                                <h4 class="pull-left">
                                    <strong>Up Next: </strong><span class="section-title"></span>
                                </h4>
                                <a href="#" class="btn btn-primary pull-right btn-lg">
                                    Next Section
                                </a>
                            </span>
                            <span class="visible-xs">
                                <a href="#" class="btn btn-primary btn-block btn-lg">
                                    Next Section
                                </a>
                            </span>
                        </div>
                        <div class="row ns">
                            <hr>
                        </div>
                    </div>
                </div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch01"></a>ChapterÂ 1.Â Installing and Configuring Spark and Spark Streaming</h2></div></div></div><p>Apache Spark (<a class="ulink" href="http://spark.apache.org/" target="_blank">http://spark.apache.org/</a>) is a general-purpose, open source cluster computing<a id="id0" class="indexterm"></a> framework developed in the AMPLab in UC Berkeley in 2009.</p><p>The emergence of Spark has not only opened new data processing possibilities for a variety of business use cases but at the same time introduced a unified platform for performing various batch and real-time operations using a common framework. Depending on the user and business needs, data can be consumed or processed every second (even less) or maybe every day, which is in harmony with the needs of enterprises.</p><p>Spark, being<a id="id1" class="indexterm"></a> a general-purpose distributed <a id="id2" class="indexterm"></a>processing framework, enables <span class="strong"><strong>Rapid Application Development</strong></span> (<span class="strong"><strong>RAD</strong></span>) and at the same time it also allows the reusability of code across batch and streaming applications. One of the most enticing features of Spark is that you can code on your desktop or laptop and it can also be deployed on top of several other cluster managers <a id="id3" class="indexterm"></a>provided by Apache Mesos (<a class="ulink" href="http://mesos.apache.org/" target="_blank">http://mesos.apache.org/</a>) or <a id="id4" class="indexterm"></a>Apache Hadoop YARN (<a class="ulink" href="https://hadoop.apache.org/" target="_blank">https://hadoop.apache.org/</a>) without any changes.</p><p>We will talk more about Spark and its features in the subsequent chapters but let's move ahead and prepare (install and configure) our environment for development on Apache Spark.</p><p>This chapter will help you understand the paradigm, applicability, aspects and characteristics of Apache Spark and Spark Streaming. It will also guide you through the installation process and running your first program using the Spark framework. At the end of this chapter, your work environment will be fully functional and ready to explore and develop applications using the Spark framework. This chapter will cover the following points:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Installation of Spark</p></li><li style="list-style-type: disc"><p>Configuring and running a Spark cluster</p></li><li style="list-style-type: disc"><p>Your first Spark program</p></li><li style="list-style-type: disc"><p>Tools and utilities for administrators</p></li><li style="list-style-type: disc"><p>Troubleshooting</p></li></ul></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec08"></a>Installation of Spark</h2></div></div><hr /></div><p>In <a id="id5" class="indexterm"></a>this section we will discuss the various aspects of Spark installation and its dependent components.</p><p>Spark supports a variety of hardware and software platforms. It can be deployed on commodity hardware and also supports deployments on high-end servers. Spark clusters can be provisioned either on cloud or on-premises. Though there is no single configuration or standards which can guide us through the requirement of Spark but still we can define "must to have" versus "good to have" and the rest varies on the requirements imposed by the use cases.</p><p>We will discuss deployment aspects more in <a class="link" href="#" linkend="ch07">Chapter 7</a>, <span class="emphasis"><em>Deploying in Production</em></span>, but let's move forward and understand the hardware and software requirements for developing applications on Spark.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec07"></a>Hardware requirements</h3></div></div></div><p>In this <a id="id6" class="indexterm"></a>section we will discuss the hardware required for batch and real-time applications developed on Spark.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec01"></a>CPU</h4></div></div></div><p>Spark <a id="id7" class="indexterm"></a>provides data processing in batch and real-time and both kinds of workloads are CPU-intensive. In large scale deployments, there has to be perfect management and utilization of computing resources. Spark solves this challenge by reducing the sharing or context switching between the threads. The objective is to provide sufficient computing resources to each thread, so that it can run independently and produce results in a timely manner. The following are the recommended requirements for CPU for each machine which will be part of a Spark cluster:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Must have: Dual core (2 cores)</p></li><li style="list-style-type: disc"><p>Good to have: 16 cores</p></li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec02"></a>RAM</h4></div></div></div><p>Real-time data processing or low latency jobs mandate that all reads/writes happen from memory itself. Any reads/writes happening from disk may impact performance.</p><p>Spark <a id="id8" class="indexterm"></a>provides the optimal performance for memory intensive jobs by caching datasets within the memory itself, so that the data can be directly read or processed from memory itself and there are very few or no reads from disks.</p><p>The general rule for memory is "the more, the better" but it depends on your use case and application. Spark<a id="id9" class="indexterm"></a> is implemented in Scala (<a class="ulink" href="http://www.scala-lang.org/" target="_blank">http://www.scala-lang.org/</a>), which requires JVM as a runtime environment for deploying Spark and, as is true for other Java-based applications, the same is applicable for Spark. We need to provide optimum memory for optimal performance. As a general rule, we should allocate only 75 percent of available memory to our Spark application and the rest should be left for the OS and other system processes. Considering all aspects and constraints exposed by the Java garbage collector, the following are the memory requirements:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Must have: 8 GB</p></li><li style="list-style-type: disc"><p>Good to have: 24 GB</p></li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec03"></a>Disk</h4></div></div></div><p>Everything <a id="id10" class="indexterm"></a>cannot be fitted into memory and eventually you need a persistent storage area (disk) for storing the data which cannot be fitted into the memory. Spark automatically spills the datasets that do not fit in memory either to the disk or re-computes on the fly when needed. Again the exact size of disks depends on the data size of your application but we would recommend the following specifications:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Must have: SATA drives with 15k RPM, with minimum capacity of 1-2 TBs each.</p></li><li style="list-style-type: disc"><p>Good to<a id="id11" class="indexterm"></a> have: Non-RAID architectureâ€”deploying <span class="strong"><strong>Just Bunch of Disks</strong></span> (<span class="strong"><strong>JBOD</strong></span>) without any data redundancy capabilities. SSDs are preferred for higher throughput and better response times.</p></li></ul></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note02"></a>Note</h3><p>Refer<a id="id12" class="indexterm"></a> to the following link for non-RAID architectures:</p><p>
<a class="ulink" href="http://en.wikipedia.org/wiki/Non-RAID_drive_architectures" target="_blank">http://en.wikipedia.org/wiki/Non-RAID_drive_architectures</a>
</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec04"></a>Network</h4></div></div></div><p>Movement<a id="id13" class="indexterm"></a> of datasets from one node to another is an intrinsic feature of any distributed computing framework. If your network is slow then it will eventually impact the performance of your job.</p><p>Spark provides a "scale out" architecture for when an application runs of multiple nodes and is network-bound for large computations which span over multiple machines. Here are the recommended specifications for the network bandwidth allocated between the nodes of a Spark cluster.</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Must have: 1 Gbps</p></li><li style="list-style-type: disc"><p>Good to have: 10 Gbps</p></li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec05"></a>Operating system</h4></div></div></div><p>Spark<a id="id14" class="indexterm"></a> follows the principle of code once and deploy anywhere. Since Spark is coded in Scala, Spark jobs can be deployed on a large number of operating systems. The following are the various flavors of OS recommended for Spark deployment:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Production: Linux, HP UX</p></li><li style="list-style-type: disc"><p>Development: Windows XP/7/8, Mac OS X, Linux</p></li></ul></div><p>In this section we have discussed the hardware prerequisites for setting up the Spark cluster. Let's move forward and discuss the software requirements for developing, building and deploying our Spark applications.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec08"></a>Software requirements</h3></div></div></div><p>In this <a id="id15" class="indexterm"></a>section we will talk about the software required <a id="id16" class="indexterm"></a>for developing and deploying Spark-based applications.</p><p>Spark Core is coded in Scala but it offers several development APIs in different languages such as Scala, Java and Python, so that you can choose your preferred weapon for coding. The dependent software may vary based on the programming languages but still there are common sets of software for configuring the Spark cluster and then language-specific software for developing Spark jobs in specific programming languages. Spark also supports<a id="id17" class="indexterm"></a> deployment and development on Windows<a id="id18" class="indexterm"></a> and Linux but, for brevity, we will discuss the installation steps only for the Linux-based operating systems specifically for Java and Scala.</p><p>Let's install all the required software which we need for developing Spark-based applications in Scala and Java.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec06"></a>Spark</h4></div></div></div><p>Perform<a id="id19" class="indexterm"></a> the following steps to install Spark:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Download <a id="id20" class="indexterm"></a>Spark compressed tarball from <a class="ulink" href="http://d3kbcqa49mib13.cloudfront.net/spark-1.3.0-bin-hadoop2.4.tgz" target="_blank">http://d3kbcqa49mib13.cloudfront.net/spark-1.3.0-bin-hadoop2.4.tgz</a>.</p></li><li><p>Create a new directory <code class="literal">Spark-1.3.0</code> on your local file system and extract Spark tarball into this directory.</p></li><li><p>Execute the following command on your Linux shell for setting <code class="literal">SPARK_HOME</code> as an environment variable:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>export SPARK_HOME=&lt;Path of Spark install Dir&gt;</strong></span>
</pre></div></li><li><p>Now browse your directory <code class="literal">SPARK_HOME</code> and it should be similar to the following illustration:</p><div class="mediaobject"><img src="graphics/B01793_01_01.jpg" /></div></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec07"></a>Java</h4></div></div></div><p>Perform <a id="id21" class="indexterm"></a>the following steps for installing Java:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Download<a id="id22" class="indexterm"></a> and install Oracle Java 7 from <a class="ulink" href="http://www.oracle.com/technetwork/java/javase/install-linux-self-extracting-138783.html" target="_blank">http://www.oracle.com/technetwork/java/javase/install-linux-self-extracting-138783.html</a>.</p></li><li><p>Execute<a id="id23" class="indexterm"></a> the following command on your Linux shell for setting <code class="literal">JAVA_HOME</code> as an environment variable:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>export JAVA_HOME=&lt;Path of Java install Dir&gt;</strong></span>
</pre></div></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec08"></a>Scala</h4></div></div></div><p>Perform<a id="id24" class="indexterm"></a> the following steps for installing Scala:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Download <a id="id25" class="indexterm"></a>Scala 2.10.5 compressed tarball from <a class="ulink" href="http://downloads.typesafe.com/scala/2.10.5/scala-2.10.5.tgz?_ga=1.7758962.1104547853.1428884173" target="_blank">http://downloads.typesafe.com/scala/2.10.5/scala-2.10.5.tgz?_ga=1.7758962.1104547853.1428884173</a>.</p></li><li><p>Create a new directory <code class="literal">Scala-2.10.5</code> on your local filesystem and extract Scala tarball into this directory.</p></li><li><p>Execute the following commands on your Linux shell for setting <code class="literal">SCALA_HOME</code> as an environment variable and add the Scala compiler in the system <code class="literal">$PATH</code>.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>export SCALA_HOME=&lt;Path of Scala install Dir&gt;</strong></span>
<span class="strong"><strong>export PATH = $PATH:$SCALA_HOME/bin</strong></span>
</pre></div></li><li><p>Next, execute<a id="id26" class="indexterm"></a> the following command to ensure that Scala runtime and Scala compiler is available and version is 2.10.x:</p><div class="mediaobject"><img src="graphics/B01793_01_02.jpg" /></div></li></ol></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note03"></a>Note</h3><p>Spark 1.3.0 is packaged and supports the 2.10.5 version of Scala, so it is advisable to use the same version to avoid any runtime exceptions due to mismatch of libraries.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec09"></a>Eclipse</h4></div></div></div><p>Perform the following steps to install Eclipse:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Based<a id="id27" class="indexterm"></a> on your hardware<a id="id28" class="indexterm"></a> configuration, download Eclipse Luna (4.4) from <a class="ulink" href="http://www.eclipse.org/downloads/packages/eclipse-ide-java-ee-developers/lunasr2" target="_blank">http://www.eclipse.org/downloads/packages/eclipse-ide-java-ee-developers/lunasr2</a>:</p><div class="mediaobject"><img src="graphics/B01793_01_03.jpg" /></div></li><li><p>Next, install <a id="id29" class="indexterm"></a>IDE for Scala in Eclipse itself, so that we can write and compile our Scala code inside Eclipse (<a class="ulink" href="http://scala-ide.org/download/current.html" target="_blank">http://scala-ide.org/download/current.html</a>).</p></li></ol></div><p>And we are done with the installation of all the required software!</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec09"></a>Installing Spark extensions â€“ Spark Streaming</h3></div></div></div><p>Core<a id="id30" class="indexterm"></a> Spark packages provide the functionality of distributed processing of datasets in batches. It is often referred to as <span class="strong"><strong>batch processing</strong></span>. At the same<a id="id31" class="indexterm"></a> time Spark also provides extensions like MLlib, GraphX, and <a id="id32" class="indexterm"></a>so on for other desired functionalities.</p><p>Spark Streaming is one such extension for processing and streaming data and it is packaged with Spark itself. We do not have to install anything separate to install Spark Streaming. Spark Streaming is an API which is packaged and integrated with Spark itself. In subsequent chapters we will discuss the Spark Streaming API and its usages more.</p><p>In the previous section we have installed all the required software. Let's move forward and configure our Spark cluster for the execution of Spark jobs.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec09"></a>Configuring and running the Spark cluster</h2></div></div><hr /></div><p>In this <a id="id33" class="indexterm"></a>section, we will configure our Spark cluster so that we can <a id="id34" class="indexterm"></a>deploy and execute our Spark application.</p><p>Spark essentially enables the distributed execution of a given piece of code. Though we will talk about Spark architecture in the next chapter, let's briefly talk about the major components which need to be configured for setting up the Spark cluster.</p><p>The following are the high-level components involved in setting up the Spark cluster:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Driver</strong></span>: It is<a id="id35" class="indexterm"></a> the client program which defines <code class="literal">SparkContext</code>. It connects to the cluster manager and requests resources for further execution of the jobs in distributed mode.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Cluster manager</strong></span> / <span class="strong"><strong>Spark master</strong></span>: Cluster <a id="id36" class="indexterm"></a>manager manages and allocates the required system resources to the Spark jobs. Furthermore, it coordinates and keeps track of the live/dead nodes in a cluster. It enables the execution of jobs submitted by the driver on worker nodes (also called Spark workers) and finally tracks and shows the status of various jobs running by the worker nodes.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Spark worker</strong></span>: Worker actually <a id="id37" class="indexterm"></a>executes the business logic submitted by the driver. Spark workers are abstracted from the Spark driver and are allocated to the driver by the cluster manager dynamically.</p></li></ul></div><p>The following diagram shows the high-level components of Spark and the way they work in combination for the execution of the submitted jobs:</p><div class="mediaobject"><img src="graphics/B01793_01_04.jpg" /></div><p>Now we <a id="id38" class="indexterm"></a>know about the different components of the Spark cluster, let's <a id="id39" class="indexterm"></a>move forward and set up these different components and bring up the Spark cluster.</p><p>Spark supports three different deployment models for configuring the cluster and different components of Spark in production and other environments:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Standalone mode</strong></span>: The core Spark<a id="id40" class="indexterm"></a> distribution contains the required APIs to create an independent, distributed and fault-tolerant cluster without any external or third-party libraries or dependencies</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note04"></a>Note</h3><p>Standalone mode should not confused with local mode. In local mode Spark jobs can be executed on a local machine without any special cluster setup, just passing <code class="literal">local[N]</code> as the master URL, where <span class="emphasis"><em>N</em></span> is the number of parallel threads.</p></div></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Apache Mesos</strong></span> (<a class="ulink" href="http://mesos.apache.org/" target="_blank">http://mesos.apache.org/</a>): This is <a id="id41" class="indexterm"></a>a<a id="id42" class="indexterm"></a> distributed general computing framework which abstracts out system resources like CPU and memory and enables the distributed execution of the <a id="id43" class="indexterm"></a>submitted jobs</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Hadoop YARN</strong></span> (<a class="ulink" href="http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html" target="_blank">http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html</a>): Spark can also utilize the resource manager <a id="id44" class="indexterm"></a>of Hadoop-2 for acquiring <a id="id45" class="indexterm"></a>the cluster resources and scheduling the Spark jobs</p></li></ul></div><p>We will <a id="id46" class="indexterm"></a>discuss the deployment models in detail in <a class="link" href="#" linkend="ch07">Chapter 7</a>, <span class="emphasis"><em>Deploying in Production</em></span>, but here we will discuss the bare minimum configuration <a id="id47" class="indexterm"></a>and the steps for configuring our Spark cluster using <span class="strong"><strong>standalone mode</strong></span>, so that we can quickly move forwards towards the next section in<a id="id48" class="indexterm"></a> which we will write and execute our first Spark program.</p><p>Perform the following<a id="id49" class="indexterm"></a> steps to bring up an independent cluster using Spark binaries:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>The first step in setting up the Spark cluster is to bring up the master node which will track and allocate the systems resource. Open your Linux shell and execute the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$SPARK_HOME/sbin/start-master.sh</strong></span>
</pre></div><p>The preceding command will bring up your master node and it will also enable a UIâ€”Spark UI for monitoring the nodes/jobs in Spark clusterâ€”<code class="literal">http://&lt;host&gt;:8080/. "&lt;host&gt;"</code> is the domain name of the machine on which the master is running.</p></li><li><p>Next, let's bring up our worker nodes, which will execute our Spark jobs. Execute the following command on the same Linux shell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$SPARK_HOME/bin/spark-class org.apache.spark.deploy.worker.Worker &lt;Spark-Master&gt; &amp;</strong></span>
</pre></div></li><li><p>In the preceding command, replace the <code class="literal">&lt;Spark-Master&gt;</code> with the Spark URL shown at the top of the Spark UI, just besides Spark master at. The preceding command will start the Spark worker process in the background and the same will also be reported in the Spark UI.</p><div class="mediaobject"><img src="graphics/B01793_01_05.jpg" /></div></li></ol></div><p>The <a id="id50" class="indexterm"></a>Spark UI shown in the preceding illustration reports the<a id="id51" class="indexterm"></a> following statuses in three different sections:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Workers</strong></span>: Reports the health of a worker nodeâ€”alive or dead and also provides drill-down to query the status and details logs of the various jobs executed by that specific worker node</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Running Applications</strong></span>: Shows the applications which are currently being executed in the cluster and also provides drill-down and enables viewing of application logs</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Completed Application</strong></span>: Same functionality as <span class="strong"><strong>Running Applications,</strong></span> the only difference being that it shows the jobs which are finished</p></li></ul></div><p>And we are done!</p><p>Our Spark cluster is up and running and ready to execute our Spark jobs with one worker node.</p><p>Let's move forward and write our first Spark application in Scala and Java and further execute it on our newly created cluster.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip02"></a>Tip</h3><p>
<span class="strong"><strong>Downloading the example code</strong></span>
</p><p>You can download the example code files from your account at <a class="ulink" href="http://www.packtpub.com" target="_blank">http://www.packtpub.com</a> for all the Packt Publishing books you have purchased. If you purchased this book elsewhere, you can visit <a class="ulink" href="http://www.packtpub.com/support" target="_blank">http://www.packtpub.com/support</a> and register to have the files e-mailed directly to you.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec10"></a>Your first Spark program</h2></div></div><hr /></div><p>In this<a id="id52" class="indexterm"></a> section we will discuss the basic terminology used in Spark and then we will code and deploy our first Spark application using Scala and Java.</p><p>Now as we have configured our Spark cluster, we are ready to code and deploy our Spark jobs but, before moving forward, let's talk about a few important components of Spark:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>RDD</strong></span>: Spark works on the concept of <span class="strong"><strong>RDD</strong></span> (<span class="strong"><strong>Resilient Distributed Datasets</strong></span>). All <a id="id53" class="indexterm"></a>data which needs to be processed <a id="id54" class="indexterm"></a>in Spark needs to be converted into <a id="id55" class="indexterm"></a>RDD and then it is loaded into the Spark cluster for further processing. RDD is a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a fault-tolerant manner. Spark provides various ways to create RDDs such as RDDs using Hadoop input formats. Raw text or binary files can also be converted into RDDs.</p><p>We will talk more about RDDs in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Processing Distributed Log Files in Real Time</em></span>, but the preceding description should be sufficient to understand subsequent examples.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>SparkContext</strong></span>: SparkContext is the key to access all features exposed by the Spark<a id="id56" class="indexterm"></a> framework. It is the main entry<a id="id57" class="indexterm"></a> point for any application for creating connections to a Spark cluster, access methods for creating RDDs and so on. The only constraint with SparkContext is that there cannot be more than one SparkContext in a given JVM but multiple contexts can be created in different JVMs. Eventually<a id="id58" class="indexterm"></a> this constraint may be removed in future releases of Spark (<a class="ulink" href="https://issues.apache.org/jira/browse/SPARK-2243" target="_blank">https://issues.apache.org/jira/browse/SPARK-2243</a>).</p></li></ul></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec10"></a>Coding Spark jobs in Scala</h3></div></div></div><p>In this <a id="id59" class="indexterm"></a>section we will code our Spark jobs in Scala. This will be our first Spark job so we will keep it simple and count the number of lines in a given text file.</p><p>Perform the following steps to code the Spark example in Scala which counts the number of lines given in a text file:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Open Eclipse and create a Scala project called <code class="literal">Spark-Examples</code>.</p></li><li><p>Expand your newly created project and modify the version of <span class="strong"><strong>Scala library container</strong></span> to 2.10. This is done to ensure that the version of Scala libraries used by Spark and that custom deployed are same.</p><div class="mediaobject"><img src="graphics/B01793_01_06.jpg" /></div></li><li><p>Open <a id="id60" class="indexterm"></a>the properties of your project <code class="literal">Spark-Examples</code> and add the dependencies for the all libraries packaged with the Spark distribution, which can be found at <code class="literal">$SPARK_HOME/lib</code>.</p></li><li><p>Next, create a Scala package <code class="literal">chapter.one</code> and within this package define a new Scala object by the name of <code class="literal">ScalaFirstSparkExample</code>.</p></li><li><p>Define a <code class="literal">main</code> method in the Scala object and also import <code class="literal">SparkConf</code> and <code class="literal">SparkContext</code>:</p><div class="informalexample"><pre class="programlisting">package chapter.one

import org.apache.spark.{SparkConf, SparkContext}

object ScalaFirstSparkExample {
  
  def main(args: Array[String]){
    //Scala Main Method
  }
}</pre></div></li><li><p>Now, add the following code to the <code class="literal">main</code> method of <code class="literal">ScalaFirstSparkExample</code>:</p><div class="informalexample"><pre class="programlisting">    println("Creating Spark Configuration")
    //Create an Object of Spark Configuration
    val conf = new SparkConf()
    //Set the logical and user defined Name of this Application
    conf.setAppName("My First Spark Scala Application")
    //Define the URL of the Spark Master. 
    //Useful only if you are executing Scala App directly //from the console.
    //We will comment it for now but will use later
    //conf.setMaster("spark://ip-10-237-224-94:7077")

    println("Creating Spark Context")
    //Create a Spark Context and provide previously created 
    //Object of SparkConf as an reference. 
    val ctx = new SparkContext(conf)

    println("Loading the Dataset and will further process it")
    
    //Defining and Loading the Text file from the local //file system or HDFS 
    //and converting it into RDD.
    //SparkContext.textFile(..) - It uses the Hadoop's //TextInputFormat and file is 
    //broken by New line Character.
    //Refer to http://hadoop.apache.org/docs/r2.6.0/api/org/apache/hadoop/mapred/TextInputFormat.html
    //The Second Argument is the Partitions which specify //the parallelism. 
    //It should be equal or more then number of Cores in //the cluster.
    val file = System.getenv("SPARK_HOME")+"/README.md";
    val logData = ctx.textFile(file, 2)

    //Invoking Filter operation on the RDD.
    //And counting the number of lines in the Data loaded //in RDD.
    //Simply returning true as "TextInputFormat" have //already divided the data by "\n"
    //So each RDD will have only 1 line.
    val numLines = logData.filter(line =&gt; true).count()
    
    //Finally Printing the Number of lines.
    println("Number of Lines in the Dataset " + numLines)</pre></div><p>And<a id="id61" class="indexterm"></a> we are done! Our first Spark program is ready for execution.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note05"></a>Note</h3><p>Follow the comments provided before each line to understand the code. The same style has been used for all other code examples given in this book.</p></div></li><li><p>Now from Eclipse itself, export your project as a <code class="literal">.jar</code> file, name it <code class="literal">Spark-Examples.jar</code> and save this <code class="literal">.jar</code> file in the root of <code class="literal">$SPARK_HOME</code>.</p><div class="mediaobject"><img src="graphics/B01793_01-07.jpg" /></div></li><li><p>Next, open your Linux console, browse to <code class="literal">$SPARK_HOME</code>, and execute the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$SPARK_HOME/bin/spark-submit --class chapter.one.ScalaFirstSparkExample --master spark://ip-10-180-61-254:7077 Spark-Examples.jar</strong></span>
</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note06"></a>Note</h3><p>We will talk about <code class="literal">spark-submit</code> at length in the next section but ensure that the value given to parameter <code class="literal">--master</code> is the same as it is shown on your Spark UI.</p></div></li><li><p>As soon as you click on <span class="emphasis"><em>Enter</em></span> and execute the preceding command you will see lot of activity (log messages) on the console and finally you will see the output of your job at the end:</p><div class="mediaobject"><img src="graphics/B01793_01_08.jpg" /></div></li></ol></div><p>Wow! Isn't that simple! All credit goes to Scala and Spark.</p><p>As we<a id="id62" class="indexterm"></a> move forward and discuss Spark more, you would appreciate the ease of coding and simplicity provided by Scala and Spark for creating, deploying and running jobs in a distributed framework.</p><p>Your completed job will also be available for viewing at the Spark UI:</p><div class="mediaobject"><img src="graphics/B01793_01_09.jpg" /></div><p>The preceding image shows the status of our first Scala job on the UI. Now let's move forward and use Java to develop our Spark job.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec11"></a>Coding Spark jobs in Java</h3></div></div></div><p>Perform<a id="id63" class="indexterm"></a> the following steps to write your first Spark example in Java which counts the number of lines given in a text file:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Open your <code class="literal">Spark-Example</code> project created in the previous section and create a new Java file called <code class="literal">JavaFirstSparkExample</code> in the package <code class="literal">chapter.one</code>.</p></li><li><p>Define <a id="id64" class="indexterm"></a>a <code class="literal">main</code> method in <code class="literal">JavaFirstSparkExample</code> and also import <code class="literal">SparkConf</code> and <code class="literal">SparkContext</code>.</p><div class="informalexample"><pre class="programlisting">package chapter.one

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function;

public class JavaFirstSparkExample {
  
  public static void main(String args[]){
    //Java Main Method
  }
}</pre></div><p>Now add the following code to the main method of <code class="literal">JavaFirstSparkExample</code>:</p><div class="informalexample"><pre class="programlisting">    System.out.println("Creating Spark Configuration");
    // Create an Object of Spark Configuration
    SparkConf javaConf = new SparkConf();
    // Set the logical and user defined Name of this Application
    javaConf.setAppName("My First Spark Java Application");
    // Define the URL of the Spark Master
    //Useful only if you are executing Scala App directly //from the console.
    //We will comment it for now but will use later
    //conf.setMaster("spark://ip-10-237-224-94:7077");

    System.out.println("Creating Spark Context");
    // Create a Spark Context and provide previously created 
    //Objectx of SparkConf as an reference.
    JavaSparkContext javaCtx = new JavaSparkContext(javaConf);
    System.out.println("Loading the Dataset and will further process it");

    //Defining and Loading the Text file from the local //filesystem or HDFS 
    //and converting it into RDD.
    //SparkContext.textFile(..) - It uses the Hadoop's         
    //TextInputFormat and file is 
    //broken by New line Character.
    //Refer to 
    //http://hadoop.apache.org/docs/r2.6.0/api/org/apache//hadoop/mapred/TextInputFormat.html
    //The Second Argument is the Partitions which specify //the parallelism. 
    //It should be equal or more then number of Cores in 
    //the cluster.
    String file = System.getenv("SPARK_HOME")+"/README.md";
    JavaRDD&lt;String&gt; logData = javaCtx.textFile(file);

    //Invoking Filter operation on the RDD.
    //And counting the number of lines in the Data loaded //in RDD.
    //Simply returning true as "TextInputFormat" have //already divided the data by "\n"
    //So each RDD will have only 1 line.
    long numLines = logData.filter(new Function&lt;String, Boolean&gt;() {
      public Boolean call(String s) {
        return true;
      }
    }).count();

    //Finally Printing the Number of lines
    System.out.println("Number of Lines in the Dataset "+numLines);

    javaCtx.close();</pre></div></li><li><p>Next, compile<a id="id65" class="indexterm"></a> the preceding <code class="literal">JavaFirstSparkExample</code> class from Eclipse itself and perform steps 7, 8 and 9 of the previous section in which we executed the Spark Scala example.</p></li></ol></div><p>And we are done! Analyze the output on the console it should be same as we saw while running the Spark application in Scala.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip03"></a>Tip</h3><p>We can also execute our Spark jobs locally where we can set the master URL in our Spark jobs to <code class="literal">SparkConf().setMaster("local[2]")</code> and can execute it as a normal Scala program. Ensure that Spark libraries (<code class="literal">$SPARK_HOME/lib/*.jar</code>) are in classpath while running the Spark jobs.</p></div><p>In this<a id="id66" class="indexterm"></a> section we have introduced the basic terminology used in the context of Spark and also written our first Spark program in Java/Scala and further executed the same on the Spark cluster. Let's move forward and see more details about the tools and utilities packaged with the Spark distribution and how they can help us in managing our cluster and jobs.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec11"></a>Tools and utilities for administrators/developers</h2></div></div><hr /></div><p>In this <a id="id67" class="indexterm"></a>section we will discuss the common tools and utilities<a id="id68" class="indexterm"></a> available with core Spark packages which can help administrators or developers in managing and monitoring Spark clusters and jobs.</p><p>Spark is packaged into 4-5 different folders and each folder has its own significance. Let's move forward and explore a few of these folders and their significance to developers and administrators.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec12"></a>Cluster management</h3></div></div></div><p>Cluster<a id="id69" class="indexterm"></a> management is a process of managing and configuring various services, or sets of services provided by a distributed software to form a farm or group of nodes to serve the needs of the user and act as a single unit. It includes various activities like adding and replacing and configuring nodes, scheduling and monitoring jobs or nodes and many more. In this section we will talk about the various utilities available with Spark, which are useful in configuring our Spark cluster:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">$SPARK_HOME/sbin</code>: This folder contains <a id="id70" class="indexterm"></a>all the scripts which help administrators in starting and stopping the Spark cluster. For example: <code class="literal">stop-all.sh</code> stops all the services with respect to the Spark cluster and <code class="literal">start-all.sh</code> starts all services (master/slaves) and brings up the complete cluster but, before we use these scripts, we need to create a <code class="literal">slaves</code> file in the <code class="literal">$SPARK_HOME/conf</code> folder which will contain the name of all the independent machines on which we wish to execute the Spark workers.</p><p>All the slave nodes should be accessible from master node and <code class="literal">password less ssh</code> should be configured on all the machines (<a class="ulink" href="http://tinyurl.com/l8kp6v3" target="_blank">http://tinyurl.com/l8kp6v3</a>).</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip04"></a>Tip</h3><p>If <code class="literal">password less ssh</code> doesn't work then you can specify <code class="literal">SPARK_SSH_FOREGROUND</code> as an environment variable and the scripts will provide <a id="id71" class="indexterm"></a>you the option to specify the password for each slave in same order as it is mentioned in the <code class="literal">conf/slaves</code> file.</p></div></li><li style="list-style-type: disc"><p>
<code class="literal">$SPARK_HOME/conf</code>: This folder contains all the templates for configuring the <a id="id72" class="indexterm"></a>Spark cluster. The Spark cluster uses the default configurations but <a id="id73" class="indexterm"></a>developers and administrators can customize them by adding specific configurations and removing <code class="literal">.template</code> from the filenames. Let's see the usage of different configuration files:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">slaves.template</code>: It is used to define the domain name of the hosts which are entrusted to host Spark workers.</p></li><li style="list-style-type: disc"><p>
<code class="literal">log4.properties.template</code>: Defines the logging information, which is by default in <code class="literal">INFO</code> mode. We can customize and provide fine-grained loggers.</p></li><li style="list-style-type: disc"><p>
<code class="literal">spark-defaults.conf.template</code>: Defines the default Spark configurations used when executing the <code class="literal">$SPARK_HOME/spark-submit</code> command (see the next section for <code class="literal">spark-submit</code>).</p></li><li style="list-style-type: disc"><p>
<code class="literal">spark-env.sh.template</code>: Defines the environment variables used by Spark driver/master and worker processes.</p></li><li style="list-style-type: disc"><p>
<code class="literal">metrics.properties.template</code>: This file is used for monitoring purposes where we can configure different metrics provided by the master/worker or driver processes.</p></li><li style="list-style-type: disc"><p>
<code class="literal">fairscheduler.xml.template</code>: Defines the type and mode of scheduler for the Spark jobs.</p></li></ul></div></li></ul></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note07"></a>Note</h3><p>Refer to <a class="ulink" href="https://spark.apache.org/docs/latest/configuration.html" target="_blank">https://spark.apache.org/docs/latest/configuration.html</a> for <a id="id74" class="indexterm"></a>complete configuration parameters for the Spark master and worker.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec13"></a>Submitting Spark jobs</h3></div></div></div><p>In this section we will talk about the utilities for submitting our jobs or client programs to our Spark <a id="id75" class="indexterm"></a>cluster.</p><p>The <code class="literal">$SPARK_HOME/bin</code> folder contains utilities for running or submitting the Spark jobs to the Spark cluster. We have already seen the usage of <code class="literal">spark-class</code> and <code class="literal">spark-submit</code>. <code class="literal">spark-class</code> represents the base driver for running any custom Scala or Java code on the Spark cluster while <code class="literal">spark-submit</code> provides additional features like launching applications on YARN/Mesos, querying job status, killing jobs, and so on.</p><p>Another utility which is worth mentioning is <code class="literal">spark-shell</code>. This utility creates a SparkContext and provides a console where you can write and directly submit your Spark jobs in Scala. Here is the exact syntax for <code class="literal">spark-shell</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$SPARK_HOME/bin/spark-shell â€“master &lt;url of master&gt;</strong></span>
</pre></div><p>
<code class="literal">spark-shell</code> is helpful in debugging Spark jobs where developers want to write and check the output of each line interactively.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec12"></a>Troubleshooting</h2></div></div><hr /></div><p>In this<a id="id76" class="indexterm"></a> section we will talk about tips and tricks which are helpful when solving the most common errors encountered while working with Spark.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec14"></a>Configuring port numbers</h3></div></div></div><p>Spark<a id="id77" class="indexterm"></a> binds various network ports for communication and exposing information to developers and administrators. There may be instances where the default ports used by Spark may not be available or may be blocked by the network firewall which in turn will result in modifying the default Spark ports for master/worker or driver.</p><p>Here is<a id="id78" class="indexterm"></a> the list of all ports utilized by Spark and their associated parameters, which need to be configured for any changes <a class="ulink" href="http://spark.apache.org/docs/latest/security.html#configuring-ports-for-network-security" target="_blank">http://spark.apache.org/docs/latest/security.html#configuring-ports-for-network-security</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec15"></a>Classpath issues â€“ class not found exception</h3></div></div></div><p>Spark<a id="id79" class="indexterm"></a> runs in a distributed model as does the job. So if your Spark job is dependent upon external libraries, then ensure that you package them into a single JAR file and place it in a common location or the default classpath of all worker nodes or define the path of the JAR file within <code class="literal">SparkConf</code> itself. It should look <a id="id80" class="indexterm"></a>something like this:</p><div class="informalexample"><pre class="programlisting">val sparkConf = new SparkConf().setAppName("myapp").setJars(&lt;path of Jar file&gt;))</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec16"></a>Other common exceptions</h3></div></div></div><p>In this<a id="id81" class="indexterm"></a> section we will talk about few of the common errors/issues/exceptions encountered by developers when they set up Spark or execute Spark jobs.</p><p>Setting up Spark clusters and executing Spark jobs is a seamless process but, no matter what we do, there may be errors or exceptions which we see while working with Spark. The following are a few such exceptions and resolutions which should help users in troubleshooting:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Too many open files</strong></span>: Increase the <code class="literal">ulimit</code> on your Linux OS by executing <code class="literal">sudo ulimit â€“n 20000</code>.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Version of Scala</strong></span>: Spark 1.3.0 supports Scala 2.10, so if you have multiple versions of Scala deployed on your box, then ensure that all versions are same, that is, Scala 2.10.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Out of memory on workers in standalone mode</strong></span>: Configure <code class="literal">SPARK_WORKER_MEMORY in "$SPARK_HOME/conf/spark-env.sh</code>. By default it provides <code class="literal">total memory - 1G</code> to workers but, at the same time, you should analyze and ensure that you are not loading or caching too much data on worker nodes.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Out of memory in applications executed on worker nodes</strong></span>: Configure <code class="literal">spark.executor.memory</code> in your <code class="literal">SparkConf</code>, like this:</p><div class="informalexample"><pre class="programlisting">val sparkConf = new SparkConf().setAppName("myapp") .set("spark.executor.memory", "1g")</pre></div></li></ul></div><p>The <a id="id82" class="indexterm"></a>preceding tips will help you solve basic issues in setting up Spark clusters but, as you move ahead, there could be more complex issues which are beyond the basic setup and for all those issues please post your queries at <a class="ulink" href="http://stackoverflow.com/questions/tagged/apache-spark" target="_blank">http://stackoverflow.com/questions/tagged/apache-spark</a> or mail at <code class="email">&lt;<a class="email" href="mailto:user@spark.apache.org">user@spark.apache.org</a>&gt;</code>.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec13"></a>Summary</h2></div></div><hr /></div><p>Throughout this chapter, we have gone through the various concepts and installation procedures of Spark and its various other components. We have also written our first Spark job in Scala and Java and executed the same in distributed mode. At the end we also discussed solutions for fixing common problems encountered during the setup of the Spark cluster.</p><p>In the next chapter, we will talk about the architecture of Spark and Spark Streaming and will also write and execute Spark Streaming examples.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch02"></a>ChapterÂ 2.Â Architecture and Components of Spark and Spark Streaming</h2></div></div></div><p>Apache Hadoop brought a revolution in data processing and storage space when it enabled fault-tolerant and distributed processing of large data (TBs/PBs) over commodity machines. Developed <a id="id83" class="indexterm"></a>on the MapReduce programing model (<a class="ulink" href="http://en.wikipedia.org/wiki/MapReduce" target="_blank">http://en.wikipedia.org/wiki/MapReduce</a>), Hadoop provided a low<a id="id84" class="indexterm"></a> cost solution and reliable batch processing (<a class="ulink" href="http://en.wikipedia.org/wiki/Batch_processing" target="_blank">http://en.wikipedia.org/wiki/Batch_processing</a>) of large data.</p><p>Hadoop was<a id="id85" class="indexterm"></a> a perfect fit for most of the varied and complex use cases but there was still a large set of use cases like real-time data processing and computations, iterative data processing (machine learning), and graph processing which were not possible with Hadoop and were still a distant dream for architects and developers, mainly due to two reasons:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Excessive and intensive use of disks for all intermediate stages</p></li><li style="list-style-type: disc"><p>Only provides map and reduce operations and no other operations like joining/flattening, and grouping of datasets.</p></li></ul></div><p>And that's where Apache Spark was introduced as a general-purpose data processing engine which can be deployed on variety of infrastructures like YARN, Mesos, and standalone too.</p><p>Spark takes MapReduce to the next level with enabled in-memory data storage and near real-time data processing. At the same time, it introduced lot of new operators for performing different kinds of operations such as joins, merging, grouping and many more. Spark introduced a new framework where applications directly run in the systems memory (RAM) and are 100 times faster and 10 times faster even when running on disk as compared to the same applications running on Hadoop cluster.</p><p>The architecture of Spark is quite different from other computing frameworks where it not only supports a variety of programming languages such as Java, Scala, Python and R but also provides libraries and extensions built upon Spark Core APIs:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>SQL for structured data processing</p></li><li style="list-style-type: disc"><p>MLlib for iterative data processingâ€”machine learning</p></li><li style="list-style-type: disc"><p>GraphX for graph processing</p></li><li style="list-style-type: disc"><p>Spark Streamingâ€”real-time data processing of streaming data</p></li></ul></div><p>This chapter will help you understand the overall architecture and various components of Spark and Spark Streaming. It will also help you to develop your first Spark Streaming example in Scala and Java. At the end of this chapter, you will be able to comprehend and appreciate the architecture of Spark and will be ready to take a deep dive and explore some real world use cases of Spark Streaming.</p><p>This chapter will cover the following points:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Batch versus real-time data processing</p></li><li style="list-style-type: disc"><p>Architecture of Spark</p></li><li style="list-style-type: disc"><p>Architecture of Spark Streaming</p></li><li style="list-style-type: disc"><p>Your first Spark Streaming program</p></li></ul></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec14"></a>Batch versus real-time data processing</h2></div></div><hr /></div><p>In<a id="id86" class="indexterm"></a> this section we will talk about the <a id="id87" class="indexterm"></a>complexities involved in working with batch and real-time data processing. We will also discuss factors differentiating data processing of large datasets into batch or real time. This will help us in understanding and appreciating the Spark architecture defined in subsequent sections.</p><p>Let's move forward and understand batch and real-time data processing.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec17"></a>Batch processing</h3></div></div></div><p>Batch <a id="id88" class="indexterm"></a>processing is a process of defining a series of jobs which are connected with each other or executed after one other in a sequence or in parallel; finally, the output of all the jobs is consolidated to produce the final output. In batch processing, input data is collected in batches over a period of time and the output of one batch can be the input of another. It is also called non-continuous processing of data which is comprised of a collection of large data files (GBs/TBs/PBs) and fast response time is not critical to the business.</p><p>Every batch job is associated with the batch window, which is refereed as the time window for processing the jobs and a period of less intensive online activity, as prescribed by production enterprise constraints.</p><p>In most<a id="id89" class="indexterm"></a> cases, batch jobs are scheduled and run at predefined intervals or at a specific time or on a certain day in a month or year.</p><p>A few <a id="id90" class="indexterm"></a>examples of batch jobs are:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Log analysis</strong></span>: In this application, logs are collected over a period of time (per day, week <a id="id91" class="indexterm"></a>or month) and analysis is performed for deriving various <span class="strong"><strong>KPI</strong></span>â€”<span class="strong"><strong>key performance indicators</strong></span> for the underlying system (<a class="ulink" href="http://en.wikipedia.org/wiki/Log_analysis" target="_blank">http://en.wikipedia.org/wiki/Log_analysis</a>)</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Billing applications</strong></span>: Calculating the usage of a service provided by the vendor over a period of time and generating billing information, for example, credit companies producing billing information at the end of each month</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Backups</strong></span>: A series of jobs running at non-critical business hours and taking the backup of the critical systems</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Data warehouses</strong></span>: Consolidating business information in aggregated and static views like weekly/month/yearly reports</p></li></ul></div><p>And the list goes onâ€¦</p><p>The preceding examples may foster a perception that batch jobs are not critical in nature, which is an incomplete and false statement.</p><p>Batch jobs are critical to business though instant response is not expected from the batch jobs. For example, an offer recommendation job which analyses past data and generates recommendations for a user for upcoming offers can be executed every night and compute the data and store it some persistent storage. But it needs to finish the computation within the stipulated time and, if doesn't, users would not see updated offers/recommendations, which in turn will impact the business.</p><p>Let's talk briefly<a id="id92" class="indexterm"></a> about the complexity involved in batch processing systems:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Large data</strong></span>: Data <a id="id93" class="indexterm"></a>is really huge and requires a good amount of computational resources to produce results in a fixed time window.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Scalability</strong></span>: This need to have a scale-out architecture so that it can meet the demands<a id="id94" class="indexterm"></a> of growing data just by adding some more computational resources without re-architecting the complete solution.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Distributed processing</strong></span>: There is a limit to the computational resources (CPUs/RAM) that can be added to a single machine and eventually, as your data<a id="id95" class="indexterm"></a> grows, your batch jobs <a id="id96" class="indexterm"></a>may run out of resources. Batch jobs should support scale-out architecture: <a class="ulink" href="http://en.wikipedia.org/wiki/Scalability#Horizontal_and_vertical_scaling" target="_blank">http://en.wikipedia.org/wiki/Scalability#Horizontal_and_vertical_scaling</a> where they should support distributed processing and computations of data over the cluster of servers.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Fault tolerant</strong></span>: Failures do happen and, especially when your job is running over<a id="id97" class="indexterm"></a> multiple severs, there could be failures. There <a id="id98" class="indexterm"></a>could be hardware or network failure or many other external factors which are not under the control of the user/system. For example, a few nodes in the cluster went down while they were processing the batch jobs. So your system should be fault-tolerant in such a manner that it should resume the processing of the failed steps in a job and should not reprocess everything.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Enterprise constraints</strong></span>: Enterprises in today's world impose constraints like SLAs where<a id="id99" class="indexterm"></a> the batch jobs needs to be completed in a stipulated time so that the system resources can be used for other purposes. There are many more such enterprise constraints like job scheduling, reprocessing, clustering, persistence and so on.</p></li></ul></div><p>The preceding descriptions should be sufficient to understand that designing or architecting batch applications is not simple. It requires knowledge to develop a distributed, scalable and performance-efficient system for processing batch jobs efficiently and effectively.</p><p>We will talk shortly about how Spark handles all these complexities but, before that, let's move forward and discuss real-time data processing and its complexities.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec18"></a>Real-time data processing</h3></div></div></div><p>Real-time <a id="id100" class="indexterm"></a>data processing is receiving constantly changing data, such as information relating to air-traffic control, travel booking systems, and so on, and processes it sufficiently rapidly to be able to control the source of the data. The response time for processing data in real time is instant and expected to be in milliseconds (sometimes microseconds too).</p><p>A system is said to be a real-time data processing system only when it produces the logically correct results within the given time frame (milliseconds) and if they miss their SLAs or deadlines then there are consequences.</p><p>Real-time systems<a id="id101" class="indexterm"></a> frequently use the term <span class="strong"><strong>latency,</strong></span> which is the time interval between the stimulation and response, or in our world it is the difference between the time the data was received and the response generated. The lesser, the better.</p><p>Real-time data <a id="id102" class="indexterm"></a>processing is often also referred to as near real-time because of the latency introduced or the relaxation in the SLAs in producing the desired results.</p><p>Here are a <a id="id103" class="indexterm"></a>few examples of real-time systems which receive data in near or real-time, process it and send back the results:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Bank ATMs: Receive an input from the user and instantly reflect the transactions (withdrawal or any other request) to the centralized account.</p></li><li style="list-style-type: disc"><p>Real-time monitoring: Capturing and analyzing data emitted from various data sources like sensors, logs live feeds, and so on in real time.</p></li><li style="list-style-type: disc"><p>Real-time <a id="id104" class="indexterm"></a>business intelligence: Process <a id="id105" class="indexterm"></a>of delivering <span class="strong"><strong>business intelligence</strong></span> (<span class="strong"><strong>BI</strong></span>) or information about business operations as they occur. For more information, refer to <a class="ulink" href="https://en.wikipedia.org/wiki/Real-time_business_intelligence" target="_blank">https://en.wikipedia.org/wiki/Real-time_business_intelligence</a>.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Operational intelligence</strong></span> (<span class="strong"><strong>OI</strong></span>): It uses real-time data processing and CEP (<a class="ulink" href="http://en.wikipedia.org/wiki/Complex_event_processing" target="_blank">http://en.wikipedia.org/wiki/Complex_event_processing</a>) to gain <a id="id106" class="indexterm"></a>insight into operations by running <a id="id107" class="indexterm"></a>query analyses against live feeds and event data. For more information, refer to <a class="ulink" href="http://en.wikipedia.org/wiki/Operational_intelligence" target="_blank">http://en.wikipedia.org/wiki/Operational_intelligence</a>.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Point of Sale (POS) systems</strong></span>: Update inventory, provide inventory history, and <a id="id108" class="indexterm"></a>sales of a particular item allowing an organization to run payments in real time.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Assembly lines</strong></span>: Process <a id="id109" class="indexterm"></a>data in real time to reduce time, cost and errors. Errors are instantly captured and appropriate actions are taken without any delays which could otherwise have produced low quality or faulty products.</p></li></ul></div><p>And the list goes onâ€¦</p><p>Let's talk briefly about the complexity involved in real-time data processing systems:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>System responsiveness</strong></span>: The un-said expectations from real-time data processing <a id="id110" class="indexterm"></a>systems are that they should process data as it arrives within milliseconds or microseconds, and should not introduce any delays in producing results.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Fault-tolerant</strong></span>: Failures do happen but real-time systems cannot afford to lose a single event.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Scalable</strong></span>: This need to have a scale-out architecture so that it can meet the demands of growing data by adding more computational resources without re-architecting the complete solution.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>In memory</strong></span>: Real-time systems cannot afford to read/write from disks, so data processing needs to be handled within the memory itself. So the systems should ensure sufficient memory for storing the input data in the system's memory.</p></li></ul></div><p>The <a id="id111" class="indexterm"></a>preceding descriptions should be sufficient to understand that, like batch processing, designing or architecting, real-time data processing system is not so simple.</p><p>There were <a id="id112" class="indexterm"></a>systems like Apache Hadoop (<a class="ulink" href="https://hadoop.apache.org/" target="_blank">https://hadoop.apache.org/</a>) or Storm (<a class="ulink" href="https://storm.apache.org/" target="_blank">https://storm.apache.org/</a>) used either for batch or real time but not for both. It was acceptable but both systems offered a different programing paradigm which was difficult to maintain. It also added complexities for application developers when they had to follow two different architectural styles of designing systems for batch and real time, which ideally should have been absorbed and handled by the underlying framework itself.</p><p>As we always say, necessity is the mother of invention, and that's what happened. Apache Spark was developed as a next generation framework and a one-stop solution for all use cases irrespective of the fact of whether they needed to be processed in batches or real time.</p><p>Let's move towards the next section and dive into the architecture of Spark and Spark Streaming and see how they handle the complexities of batch processing and real-time data processing systems.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec15"></a>Architecture of Spark</h2></div></div><hr /></div><p>In this <a id="id113" class="indexterm"></a>section we will discuss the need for the Spark framework in comparison to Hadoop and then we will also talk about the architecture of Spark which is also referred as its Core Spark Framework.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec19"></a>Spark versus Hadoop</h3></div></div></div><p>Apache<a id="id114" class="indexterm"></a> Spark is an open source cluster computing framework which seemed to be similar to Apache Hadoop but actually it is superior to Hadoop. Hadoop performed well for the majority of large scale and distributed data processing over commodity boxes but it failed in two scenarios:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Iterative and interactive computations and workloads: For example, machine learning algorithms which reuse intermediate or working datasets across multiple parallel operations.</p></li><li style="list-style-type: disc"><p>Real-time data processing: Hadoop was mainly built for batch processing where it lacks in-memory data processing capabilities which are necessary for real-time data processing.</p></li></ul></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note08"></a>Note</h3><p>Refer to <a class="ulink" href="https://www.typesafe.com/blog/how-spark-beats-mapreduce-event-streaming-iterative-algorithms-and-elasticity" target="_blank">https://www.typesafe.com/blog/how-spark-beats-mapreduce-event-streaming-iterative-algorithms-and-elasticity</a> for more information on the pitfalls of Hadoop for iterative and streaming computations.</p></div><p>The <a id="id115" class="indexterm"></a>preceding scenarios were the motivating factors for a system which could efficiently and effectively support iterative as well as interactive data processing.</p><p>The creators of Apache Spark realized that they not only needed to retain the benefits of Apache Hadoop like batch computations, scalability, fault tolerance and distributed data processing but it also needed a new architecture which avoided expensive reads from the disks during various map/reduce stages and supported in-memory processing of distributed data over the cluster of nodes.</p><p>Apache Spark does exactly the same thing and introduced a new layer abstraction of distributed datasets which is partitioned over the set of machines (cluster) and can be cached<a id="id116" class="indexterm"></a> in the memory to reduce the latency, which they named <span class="strong"><strong>RDD</strong></span> (<span class="strong"><strong>Resilient Distributed Datasets</strong></span>).</p><p>RDD, by definition, is an immutable (read-only) collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost.</p><p>Before moving forward it is important to mention that Spark is capable of performing in-memory operations but it can work on-disk at the same time too.</p><p>Spark is designed to be a general execution engine and whenever the data does not fit in memory, Spark interacts with the underlying data storage layer to transfer data to the permanent storage and free up memory.</p><p>Let's move forward and discuss the architecture of Spark and its approach of implementing<a id="id117" class="indexterm"></a> in-memory and disk-based distributed data processing over clusters of nodes.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec20"></a>Layered architecture â€“ Spark</h3></div></div></div><p>Spark<a id="id118" class="indexterm"></a> provides a well-defined and layered architecture where all its layers and components are loosely coupled, and integration with external components and extensions is performed using well-defined contracts. Spark architecture also provides the flexibility to define custom libraries and extensions by extending its core APIs.</p><p>Here is the high-level architecture of Spark and its various layers which also show the deployment stack of a typical Spark production deployment:</p><div class="mediaobject"><img src="graphics/B01793_02_01.jpg" /></div><p>The preceding illustration shows the high-level architecture of Spark which is divided into the following layers:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Data storage layer</strong></span>: This layer is responsible for providing the persistent storage <a id="id119" class="indexterm"></a>area to Spark applications so that Spark workers can dump data whenever the memory is not sufficient. Spark is extensible and capable of using any kind of filesystem. RDDs, which hold the data, are agnostic to the underlying storage layer and can persist the data in various persistent storage areas like local filesystems, HDFS or any other NoSQL database like HBase, Cassandra, MongoDB, S3, Elasticsearch.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Resource manager APIs</strong></span>: These APIs are used to allocate the available resources (across the cluster) to the client jobs and once the job is finished these <a id="id120" class="indexterm"></a>resources are reclaimed.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Spark Core libraries</strong></span>: The Spark Core library contains APIs providing the Spark<a id="id121" class="indexterm"></a> general execution engine which runs on<a id="id122" class="indexterm"></a> the Spark platform, providing in-memory distributed data processing and a generalized execution model to support a wide variety of applications and languages.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Spark extensions/libraries</strong></span>: These are additional functionalities which are developed<a id="id123" class="indexterm"></a> by extending the Spark Core APIs to support different use cases. For example, Spark Streaming is one such extension which is developed for performing computations over real-time and streaming data.</p></li></ul></div><p>Let's re-define and arrange the components which we defined in the <span class="emphasis"><em>Configuring and running the Spark cluster</em></span> section of <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Installing and Configuring Spark and Spark Streaming</em></span>, and analyze how it fits into the different layers of the Spark architecture.</p><div class="mediaobject"><img src="graphics/B01793_02_02.jpg" /></div><p>The preceding illustration shows the interaction between the different layers of the Spark architecture. We are using the core Spark libraries, which provide a general execution engine for in-memory distributed data processing.</p><p>We will<a id="id124" class="indexterm"></a> discuss one of the most popular Spark extensions for real-time data processing of streaming data in the next sectionâ€”Spark Streaming. The rest of the chapters will strenuously examine Spark Streaming using real-world examples and we will also introduce its other extensions.</p><p>Let's move forward and talk about the architecture of Spark Streaming and write our first program which consumes the streaming data and does computations in real time.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec16"></a>Architecture of Spark Streaming</h2></div></div><hr /></div><p>In the <a id="id125" class="indexterm"></a>previous section we discussed Spark, highlighting the position of Spark Streaming in the overall architecture of Spark. In this section we will discuss the various components and architecture of Spark Streaming.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec21"></a>What is Spark Streaming?</h3></div></div></div><p>Spark<a id="id126" class="indexterm"></a> Streaming is an interesting and powerful extension of <a id="id127" class="indexterm"></a>Spark which provides the processing of streaming data<a id="id128" class="indexterm"></a> or fast moving data (<a class="ulink" href="http://en.wikipedia.org/wiki/Stream_(computing)" target="_blank">http://en.wikipedia.org/wiki/Stream_(computing)</a>) in near real-time.</p><p>There are many applications and use cases like spam filtering, intrusion detection, and clickstream data analysis which generates live data in milliseconds and further to make it more complex it needs to be analyzed at the same time and produce results.</p><p>Spark already provided a general execution engine for in-memory data processing, so Spark Streaming was introduced as an extension of Spark which leveraged the same Spark API and provides the processing upon live/streaming data in near real-time. It also integrates with a variety of popular data sources including HDFS, Apache Flume, Apache Kafka, Twitter and TCP Sockets.</p><p>Let's move forward and understand the architecture of Spark Streaming.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec22"></a>High-level architecture â€“ Spark Streaming</h3></div></div></div><p>Spark<a id="id129" class="indexterm"></a> Streaming implemented the concept of micro-batching where the live/streaming data is divided into a series of deterministic micro-batches and each batch is processed as an individual record and the further output of each batch is sent to the user-defined output streams and can be further persisted into HDFS, NoSQL or can be used to create live dashboards. We will talk more about persistence in <a class="link" href="#" linkend="ch05">Chapter 5</a>, <span class="emphasis"><em>Persisting Log Analysis Data</em></span>.</p><p>The size of each batch is governed by the acceptable latency stated by the underlying use case. It can be a few milliseconds or seconds. For example, it collects all Twitter feeds every 100 millisecond and processes them as one single batch. So the system will create a series of multiple batches each containing the feeds received within 100 milliseconds and will submit them to Spark for further processing as a continuous process.</p><div class="mediaobject"><img src="graphics/B01793_02_03.jpg" /></div><p>The preceding illustration shows the high-level architecture of Spark Streaming.</p><p>Let's discuss in detail the various components defined in the preceding architecture:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Input data streams</strong></span>: These are input data sources which are generating data at a very high velocity (every second, millisecond or even less) and sending it in the form of a continuous stream of data. Spark provides certain connectors<a id="id130" class="indexterm"></a> to connect to these <a id="id131" class="indexterm"></a>incoming data streams and capture the data for further processing, which are further categorized into basic and advanced data sources:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Basic data sources</strong></span>: The data connectors or APIs for these data sources are in-built and packaged with core Spark Streaming bundles. We do not need any external libraries for these connectors.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Advanced data sources</strong></span>: The data connectors or APIs for advanced sources are not part of core Spark Streaming bundles. They require linking or the inclusion of external libraries as dependencies in your build scripts or may be directly downloaded from the community website at <a class="ulink" href="http://spark-packages.org/?q=tags%3A%22Streaming%22" target="_blank">http://spark-packages.org/?q=tags%3A%22Streaming%22</a> and include in the classpath of your application. You can also extend and build your own custom <a id="id132" class="indexterm"></a>receivers/extensions. Refer to <a class="ulink" href="http://spark.apache.org/docs/latest/streaming-custom-receivers.html" target="_blank">http://spark.apache.org/docs/latest/streaming-custom-receivers.html</a> for building custom receivers.</p></li></ul></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note09"></a>Note</h3><p>Refer to the link for a list of all advanced data at <a class="ulink" href="https://spark.apache.org/docs/1.3.0/streaming-programming-guide.html#linking" target="_blank">https://spark.apache.org/docs/1.3.0/streaming-programming-guide.html#linking</a>.</p></div></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Spark Streaming</strong></span>: The API or extension which provides the consumption of streaming<a id="id133" class="indexterm"></a> data at a predefined interval and converts the streaming data into series batches which are further sent to the Spark Core engine for processing.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Batch</strong></span>: Batches <a id="id134" class="indexterm"></a>are nothing more than a series of RDDs. Spark Streaming provides another layer of abstraction over a series of RDDs known as <span class="strong"><strong>DStreams</strong></span> (<span class="strong"><strong>Discretized streams</strong></span>). DStreams hold the reference of a series of RDDs which are based on the data directly provided by input streams or processed data streams generated by transforming the input streams. All operations on DStreams are applied directly to the underlying RDDs. We will discuss DStreams and its associated operations more in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Processing Distributed Log Files in Real Time</em></span>.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Spark Core engine</strong></span>: The core engine of Spark which receives the input in the form <a id="id135" class="indexterm"></a>of RDD and further processes and finally sends it to the associated output streams for storage.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Output data streams</strong></span>: The output of each processed batch is directly sent to the <a id="id136" class="indexterm"></a>output streams for further actions. These output streams can be of varied types, ranging from a raw file system, NoSQL, Queues or web sockets for visualizing the streaming data.</p></li></ul></div><p>Let's <a id="id137" class="indexterm"></a>see this working and move on to the next section where we will write our first Spark Streaming program in Scala and Java.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec17"></a>Your first Spark Streaming program</h2></div></div><hr /></div><p>In this <a id="id138" class="indexterm"></a>section we will code and deploy our first Spark Streaming job in Scala and Java.</p><p>Our streaming job will connect to a specific port number and will receive or capture data at regular intervals. It will evaluate and calculate the count of distinct words in the data received. Lastly, it will print this on the console.</p><p>The Spark Streaming example shown in subsequent sections is comprised of two distinct parts:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Spark Streaming job</strong></span>: The Spark Streaming job, which contains the actual application logic and will be executed on the Spark cluster</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Client application</strong></span>: The client application which opens a specific port and will write some data to that port at regular intervals</p></li></ul></div><p>Let's move forward and see the code for the Spark Streaming job in Scala and Java and then we will also see the client application. Finally, we will deploy everything on the cluster and will see the results on the console.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec23"></a>Coding Spark Streaming jobs in Scala</h3></div></div></div><p>Let's<a id="id139" class="indexterm"></a> extend our Scala project <code class="literal">Spark-Examples</code> which we created in <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Installing and Configuring Spark and Spark Streaming</em></span>, perform the following steps and code a Spark Streaming example in Scala which counts the number of distinct words received from the client:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Create a Scala package <code class="literal">chapter.two</code> and, within this package, define a new Scala object called <code class="literal">ScalaFirstStreamingExample</code>.</p></li><li><p>Define a main method in the Scala object and also import the packages as shown in the following example:</p><div class="informalexample"><pre class="programlisting">package chapter.two

import org.apache.spark.SparkConf
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming._
import org.apache.spark.storage.StorageLevel._
import org.apache.spark.rdd.RDD
import org.apache.spark.streaming.dstream.DStream
import org.apache.spark.streaming.dstream.ForEachDStream

object ScalaFirstStreamingExample {
  
  def main(args: Array[String]){
    //Scala Main Method
  }
}</pre></div></li><li><p>Now add the following code to<a id="id140" class="indexterm"></a> the main method of <code class="literal">ScalaFirstStreamingExample</code>:</p><div class="informalexample"><pre class="programlisting">    println("Creating Spark Configuration")
    //Create an Object of Spark Configuration
    val conf = new SparkConf()
    //Set the logical and user defined Name of this Application
    conf.setAppName("My First Spark Streaming Application")
    
    println("Retreiving Streaming Context from Spark Conf")
    //Retrieving Streaming Context from SparkConf Object.
    //Second parameter is the time interval at which //streaming data will be divided into batches  
    val streamCtx = new StreamingContext(conf, Seconds(2))

    //Define the type of Stream. Here we are using TCP //Socket as text stream, 
    //It will keep watching for the incoming data from a //specific machine (localhost) and port (9087) 
    //Once the data is retrieved it will be saved in the //memory and in case memory
    //is not sufficient, then it will store it on the Disk
    //It will further read the Data and convert it into DStream
    val lines = streamCtx.socketTextStream("localhost", 9087, MEMORY_AND_DISK_SER_2)
    
    //Apply the Split() function to all elements of DStream 
    //which will further generate multiple new records from //each record in Source Stream
    //And then use flatmap to consolidate all records and //create a new DStream.
    val words = lines.flatMap(x =&gt; x.split(" "))
    
    //Now, we will count these words by applying a using map()
    //map() helps in applying a given function to each //element in an RDD. 
    val pairs = words.map(word =&gt; (word, 1))
    
    //Further we will aggregate the value of each key by //using/applying the given function.
    val wordCounts = pairs.reduceByKey(_ + _)
    
    //Lastly we will print all Values
    //wordCounts.print(20)
    
    myPrint(wordCounts,streamCtx)
    //Most important statement which will initiate the //Streaming Context
    streamCtx.start();
    //Wait till the execution is completed.
    streamCtx.awaitTermination();  </pre></div></li><li><p>Now define one more<a id="id141" class="indexterm"></a> function <code class="literal">printValues(â€¦)</code> in your <code class="literal">ScalaFirstStreamingExample</code>:</p><div class="informalexample"><pre class="programlisting">/**
   * Simple Print function, for printing all elements of RDD
   */
def printValues(stream:DStream[(String,Int)],streamCtx: StreamingContext){
    stream.foreachRDD(foreachFunc)
    def foreachFunc = (rdd: RDD[(String,Int)]) =&gt; {
      val array = rdd.collect()
      println("---------Start Printing Results----------")
      for(res&lt;-array){
        println(res)
      }
      println("---------Finished Printing Results----------")
    }
  }

//Most important statement which will initiate the //Streaming Context
streamCtx.start();
//Wait till the execution is completed.
streamCtx.awaitTermination();</pre></div></li></ol></div><p>And we are done! Our first streaming job <a id="id142" class="indexterm"></a>in Scala is ready but before we deploy this on the Spark cluster, let's see the same implementation in Java and also code the client application which will send the data to our streaming job.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec24"></a>Coding Spark Streaming jobs in Java</h3></div></div></div><p>Perform<a id="id143" class="indexterm"></a> the following steps to code the Spark Streaming example in Java which counts the number of words received on a specific port number:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Create a Java file <code class="literal">JavaFirstStreamingExample</code> in the package <code class="literal">chapter.two</code> of <code class="literal">Spark-Examples</code>.</p></li><li><p>Define a main method in the Java class and also import the packages, as shown in the following example:</p><div class="informalexample"><pre class="programlisting">package chapter.two;

import java.util.Arrays;

import org.apache.spark.*;
import org.apache.spark.api.java.function.*;
import org.apache.spark.storage.StorageLevel;
import org.apache.spark.streaming.*;
import org.apache.spark.streaming.api.java.*;

import scala.Tuple2;

public class JavaFirstStreamingExample {
  
  public static void main(String[] s){
    //Java Main Method
}</pre></div></li><li><p>Now add the following code to the main method of <code class="literal">JavaFirstStreamingExample</code>:</p><div class="informalexample"><pre class="programlisting">    System.out.println("Creating Spark Configuration");
    //Create an Object of Spark Configuration
    SparkConf conf = new SparkConf();
    //Set the logical and user defined Name of this Application
    conf.setAppName("My First Spark Streaming Application");
    //Define the URL of the Spark Master. 
    //Useful only if you are executing Scala App directly //from the console.
    //We will comment it for now but will use later
    //conf.setMaster("spark://ip-10-237-224-94:7077")
    
    System.out.println("Retreiving Streaming Context from Spark Conf");
    //Retrieving Streaming Context from SparkConf Object.
    //Second parameter is the time interval at which //streaming data will be divided into batches  
    JavaStreamingContext streamCtx = new JavaStreamingContext(conf, Durations.seconds(2));

    //Define the type of Stream. Here we are using TCP //Socket as text stream, 
    //It will keep watching for the incoming data from a //specific machine (localhost) and port (9087) 
    //Once the data is retrieved it will be saved in the //memory and in case memory
    //is not sufficient, then it will store it on the Disk.  
    //It will further read the Data and convert it into DStream
    JavaReceiverInputDStream&lt;String&gt; lines = streamCtx.socketTextStream("localhost", 9087,StorageLevel.MEMORY_AND_DISK_SER_2());
    
    //Apply the x.split() function to all elements of //JavaReceiverInputDStream 
    //which will further generate multiple new records from //each record in Source Stream
    //And then use flatmap to consolidate all records and //create a new JavaDStream.
    JavaDStream&lt;String&gt; words = lines.flatMap( new FlatMapFunction&lt;String, String&gt;() {
        @Override public Iterable&lt;String&gt; call(String x) {
            return Arrays.asList(x.split(" "));
        }
    });
    
    
    //Now, we will count these words by applying a using //mapToPair()
    //mapToPair() helps in applying a given function to //each element in an RDD
    //And further will return the Scala Tuple with "word" //as key and value as "count".
    JavaPairDStream&lt;String, Integer&gt; pairs = words.mapToPair(
        new PairFunction&lt;String, String, Integer&gt;() {
            @Override
            public Tuple2&lt;String, Integer&gt; call(String s) throws Exception {
                return new Tuple2&lt;String, Integer&gt;(s, 1);
            }
    });
    
    
    //Further we will aggregate the value of each key by //using/applying the given function.
    JavaPairDStream&lt;String, Integer&gt; wordCounts = pairs.reduceByKey(
        new Function2&lt;Integer, Integer, Integer&gt;() {
        @Override public Integer call(Integer i1, Integer i2) throws Exception {
            return i1 + i2;
        }
    });
   
    
    //Lastly we will print First 10 Words.
    //We can also implement custom print method for //printing all values,
    //as we did in Scala example.
    wordCounts.print(10);
    //Most important statement which will initiate the //Streaming Context
    streamCtx.start();
    //Wait till the execution is completed.
    streamCtx.awaitTermination();  </pre></div></li></ol></div><p>And we are done! Our first streaming job <a id="id144" class="indexterm"></a>in Java is ready. Let's move on to the next section where we will create our client application which will send the data to our streaming job.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec25"></a>The client application</h3></div></div></div><p>The client <a id="id145" class="indexterm"></a>application will allow the user to type messages on the console and capture data. Once data is captured, it will immediately send it to the specific port number (socket) where our streaming job is waiting for the data.</p><p>Perform the following steps to create the client application:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Extend the Eclipse project <code class="literal">Spark-Examples</code> and create a Java class <code class="literal">ClientApp.java</code> in the package <code class="literal">chapter.two</code>.</p></li><li><p>Define the main method and add imports as shown in the following code:</p><div class="informalexample"><pre class="programlisting">package chapter.two;

import java.net.*;
import java.io.*;
public class ClientApp {

    public static void main(String[] args) {
      //Main Method
}</pre></div></li><li><p>Next add the following piece of code in the main method:</p><div class="informalexample"><pre class="programlisting">try{
    System.out.println("Defining new Socket");
    ServerSocket soc = new ServerSocket(9087);
    System.out.println("Waiting for Incoming Connection");
    Socket clientSocket = soc.accept();

    System.out.println("Connection Received");
    OutputStream outputStream = clientSocket.getOutputStream();
    //Keep Reading the data in a Infinite loop and send it //over to the Socket.
    while(true){
        PrintWriter out = new PrintWriter(outputStream, true);
        BufferedReader read = new BufferedReader(new InputStreamReader(System.in));
        System.out.println("Waiting for user to input some data");
        String data = read.readLine();
        System.out.println("Data received and now writing it to Socket");
        out.println(data);

    }

}catch(Exception e ){
    e.printStackTrace();
}</pre></div></li></ol></div><p>And<a id="id146" class="indexterm"></a> we are done with our client application!</p><p>Now let's move on to the final step where we will package and deploy our Spark Streaming job on the Spark cluster.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec26"></a>Packaging and deploying a Spark Streaming job</h3></div></div></div><p>The <a id="id147" class="indexterm"></a>packaging and deployment<a id="id148" class="indexterm"></a> of Spark Streaming jobs on the Spark cluster is very similar to the process followed for deploying standard Spark batch jobs but with one additional step, in which we need to ensure that our data stream providing the data is up and running before our job is deployed.</p><p>Perform the following steps for packaging and deploying a streaming job:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>We will export your project as a <code class="literal">.jar</code> file, name it <code class="literal">Spark-Examples.jar</code> and save this <code class="literal">.jar</code> file in the root of <code class="literal">$SPARK_HOME</code>. Execute the following command on your Linux console, from the location where you compiled your <code class="literal">Spark-Examples</code> project:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>jar -cf $SPARK_HOME/Spark-Examples.jar *</strong></span>
</pre></div></li><li><p>Next open your Linux console, browse to <code class="literal">$SPARK_HOME</code>, and execute the following command for initializing your client application:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -classpath Spark-Examples.jar chapter.two.ClientApp</strong></span>
</pre></div><p>Once your client app is up and running it will wait till someone (in our case it will be the streaming job) is there to receive the data. Let's refer to this console as <code class="literal">ClientConsole</code>:</p><div class="mediaobject"><img src="graphics/B01793_02-04.jpg" /></div></li><li><p>Next open <a id="id149" class="indexterm"></a>another Linux console, browse <a id="id150" class="indexterm"></a>to <code class="literal">$SPARK_HOME</code>, and execute the following command for deploying your Spark Streaming job:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$SPARK_HOME/bin/spark-submit --class chapter.two.ScalaFirstStreamingExample --master &lt;SPARK-MASTER-URL&gt; Spark-Examples.jar</strong></span>
</pre></div></li><li><p>Let's refer to this console as <code class="literal">SparkJobConsole</code>.</p></li><li><p>As soon as you click on <span class="emphasis"><em>Enter</em></span> and execute the preceding command you should see lot of activity (log messages) on the <code class="literal">SparkJobConsole</code> and your streaming job is ready to receive the data from your <code class="literal">ClientApp</code> every two seconds. Whatever you type in your <code class="literal">ClientConsole</code>, you will see the count of those words appearing in your <code class="literal">SparkJobConsole</code>.</p></li><li><p>For example, type some text in your <code class="literal">ClientConsole</code> window, as shown in the following illustration:</p><div class="mediaobject"><img src="graphics/B01793_02-05.jpg" /></div></li><li><p>You will see the count of the distinct words in the text sent by our <code class="literal">ClientApp</code> on the <code class="literal">SparkJobConsole</code>:</p><div class="mediaobject"><img src="graphics/B01793_02-06.jpg" /></div><p>The preceding example<a id="id151" class="indexterm"></a> shows <a id="id152" class="indexterm"></a>a stateless computation or a count of words received from the user every two seconds.</p></li><li><p>You will be able to see the status of your streaming job on Spark Master UI under the <span class="strong"><strong>Running Applications</strong></span> tab:</p><div class="mediaobject"><img src="graphics/B01793_02-07.jpg" /></div></li></ol></div><p>Wow! Isn't that simple and easy! That is the real power of Sparkâ€”one framework for all your batch and streaming jobs.</p><p>In this section, we have developed and deployed our first Spark Streaming job, which counts the number of distinct words from the text sent by client applications over a socket in real time.</p><p>Let's move on to the next chapter where we will see the complex real world examples of Spark Streaming.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec18"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we have discussed the challenges and programming paradigms for batch and real-time data processing. We also discussed the need for frameworks like Spark and its differences with preexisting frameworks like Hadoop. At the end, we developed and deployed our first Spark Streaming program.</p><p>In the next chapter, we will talk about Spark Client APIs and some of its integral components. We will also introduce a real-world use case for distributed data processing in real time.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch03"></a>ChapterÂ 3.Â Processing Distributed Log Files in Real Time</h2></div></div></div><p>In today's world, large amounts of valuable data are stored in repositories distributed across large-scale networks which can be accessed over the Web. The key challenge is to provide a distributed, efficient, scalable, extensible fault-tolerant system to manipulate this data easily, safely, and with high performance.</p><p>There is no doubt that systems like Hadoop brought revolution and provided a framework for processing data at Internet scaleâ€”large and distributed data in TBs/PBs. But it was primarily meant for data-intensive operations, where it processed data over clusters of nodes/machines and devoted most of its processing time to I/O and manipulation of data.</p><p>Apache Spark took distributed computing to the next level when it introduced a new architectural paradigm for data-intensive and computer-intensive operations and also provided extensions like Spark Streaming for processing streaming data in real-time or near real-time.</p><p>Spark Streaming is<a id="id153" class="indexterm"></a> an interesting extension to the core Spark APIs. It provides the distributed processing (loading, manipulating and persisting) of a continuous stream of data while retaining all the other core features of Spark.</p><p>Additionally, it provides high-level operators in Java, Scala, and Python for running ad hoc operations on the streaming data enabling developers to build powerful interactive applications.</p><p>This chapter will discuss the Spark Streaming APIs for performing various data loading operations. We will deep dive into the core components of Spark and Spark Streaming and, at the end, we will also discuss real-life use cases for distributed log file analysis in real time.</p><p>This chapter will cover the following points:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Spark packaging structure and client APIsâ€”Java and Scala</p></li><li style="list-style-type: disc"><p>Resilient distributed datasets and discretized streams</p></li><li style="list-style-type: disc"><p>Data loading from distributed and varied sources</p></li></ul></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec19"></a>Spark packaging structure and client APIs</h2></div></div><hr /></div><p>In this <a id="id154" class="indexterm"></a>section, we will discuss Spark's packaging structure <a id="id155" class="indexterm"></a>and various client APIs provided by Spark for data loading, manipulation and finally caching in memory or persisting to external storage. We will also talk about the Spark packaging structure which will help us to understand the purpose and functionalities provided by the various packages and classes.</p><p>Spark is<a id="id156" class="indexterm"></a> written in Scala (<a class="ulink" href="http://www.scala-lang.org/" target="_blank">http://www.scala-lang.org/</a>) but for interoperability it also provides the equivalent APIs in Java and Python.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note10"></a>Note</h3><p>For<a id="id157" class="indexterm"></a> brevity we will only talk about the Scala and Java APIs and, for Python APIs, users can refer to <a class="ulink" href="https://spark.apache.org/docs/1.3.0/api/python/index.html" target="_blank">https://spark.apache.org/docs/1.3.0/api/python/index.html</a>.</p></div><p>As discussed in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Architecture and Components of Spark and Spark Streaming</em></span>, Spark provides layered architecture and, at a high level, Spark is divided into two different modules:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Spark Core</strong></span>: It <a id="id158" class="indexterm"></a>contains the core packages and classes for various basic operations like task scheduling/distribution, tracking, I/O operations, memory management, and many more. Spark Core also defines the RDD which are immutable objects and represent the main programming extensions.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Spark libraries/extensions</strong></span>: They contain the various modules built over the Spark <a id="id159" class="indexterm"></a>Core APIs for providing specialized features. As of Spark 1.3.0, the following extensions are available and packaged with the Spark standard distribution:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Spark Streaming</strong></span>: Spark Streaming is used for the consuming, transforming, processing and storing of high-throughput live data streams<a id="id160" class="indexterm"></a> in fault-tolerant manner. It is also known as a real-time or near-real-time extension of Spark for processing streaming data.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Spark SQL</strong></span>: This is a module for<a id="id161" class="indexterm"></a> structured data processing. Spark SQL provides another layer of abstraction known as DataFrames which is merely a distributed collection of data organized into<a id="id162" class="indexterm"></a> columns referred <a id="id163" class="indexterm"></a>by the user defined names. DataFrame are equivalent to the tables in relational <a id="id164" class="indexterm"></a>databases or frames in Python (<a class="ulink" href="http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.html" target="_blank">http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.html</a>) or R (<a class="ulink" href="http://www.r-tutor.com/r-introduction/data-frame" target="_blank">http://www.r-tutor.com/r-introduction/data-frame</a>).</p><p>We <a id="id165" class="indexterm"></a>will read more about Spark SQL in <a class="link" href="#" linkend="ch06">Chapter 6</a>, <span class="emphasis"><em>Integration with Advanced Spark Libraries</em></span>.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Spark MLlib</strong></span>: This <a id="id166" class="indexterm"></a>provides machine learning (<a class="ulink" href="http://en.wikipedia.org/wiki/Machine_learning algorithms" target="_blank">http://en.wikipedia.org/wiki/Machine_learning algorithms</a>) for statistical <a id="id167" class="indexterm"></a>analysis including algorithms classification, regression, clustering, collaborative filtering and dimensionality <a id="id168" class="indexterm"></a>reduction. Spark MLlib provides high-level APIs for defining the sequence of stages or the series of steps executed in order, which is known as machine learning pipelines.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Spark GraphX</strong></span>: Another extension of Spark which exposes APIs for defining and manipulating data using graphs (<a class="ulink" href="http://en.wikipedia.org/wiki/Graph_(mathematics)" target="_blank">http://en.wikipedia.org/wiki/Graph_(mathematics)</a>). This <a id="id169" class="indexterm"></a>extends the RDDs<a id="id170" class="indexterm"></a> and provides an abstraction for working with directed multigraphs with properties attached to each vertex and edge. It also provides various graph algorithms like PageRank, connected components and triangle counting. GraphX not only provides the functionality of graphs but also retains the core feature of Spark where all graph computations are executed<a id="id171" class="indexterm"></a> in parallel which is referred as <span class="strong"><strong>graph-parallel computations</strong></span>.</p><p>We will talk more about Spark GraphX in <a class="link" href="#" linkend="ch06">Chapter 6</a>, <span class="emphasis"><em>Integration with Advanced Spark Libraries</em></span>.</p></li></ul></div></li></ul></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note11"></a>Note</h3><p>Spark has also defined one more extension, Spark Bagel, but it will soon be replaced by GraphX.</p></div><p>Let's dive into each of the Spark modules and discuss the important components, packages <a id="id172" class="indexterm"></a>and<a id="id173" class="indexterm"></a> their associated functionalities.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec27"></a>Spark Core</h3></div></div></div><p>Spark Core is<a id="id174" class="indexterm"></a> the heart of Spark and provides two basic components, SparkContext and Spark Config. Both of these components are used by each and every standard or customized Spark job or Spark library and extension. <span class="strong"><strong>Context</strong></span> and <span class="strong"><strong>Config</strong></span> is not a new term/concept and more or less it has now become a standard Architectural Pattern. By definition â€“ a Context is an entry point of the application which provides the access to various resources/features exposed by the framework while the Config contains the application configurations, which helps in defining the environment of the application.</p><p>Let's move on to the nitty-gritty of the Scala and Java APIs exposed by Spark Core.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec10"></a>SparkContext and Spark Config â€“ Scala APIs</h4></div></div></div><p>
<code class="literal">org.apache.spark.SparkContext</code> is the first statement in any Spark job which<a id="id175" class="indexterm"></a> defines the SparkContext and then further <a id="id176" class="indexterm"></a>defines the business logic. SparkContext is the entry point for accessing any of the Spark features which we may want to use or leverage, for example, connecting to the Spark cluster, submitting jobs, and so on. Even the references to all Spark extensions are provided by SparkContext. There can be only one SparkContext per JVM which needs to be stopped if we want to create a new one. SparkContext is immutable, which means it cannot be changed or modified once it is started.</p><p>In order to create the SparkContext you need to provide the reference of <code class="literal">org.apache.spark.SparkConf</code> which holds the values of all <code class="literal">spark.*</code> environment variables defined as Java system properties. There are default values associated with each and every configuration variable, which can be overwritten by explicitly defining it in your Spark job. For example, let's assume that our job is dependent on some custom JAR files and we want to distribute these custom JAR files to the worker nodes before our job is executed, which if not done may produce failures. It can be achieved by the following code snippet:</p><div class="informalexample"><pre class="programlisting">val conf = new SparkConf();
val jars = Seq("$SPARK_HOME/lib/a.jar","$SPARK_HOME/lib/b.jar")
conf.setJars(jars)
val context = new SparkContext(conf)
â€¦â€¦â€¦
â€¦â€¦</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note12"></a>Note</h3><p>Refer <a id="id177" class="indexterm"></a>to <a class="ulink" href="https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.SparkContext" target="_blank">https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.SparkContext</a> and <a class="ulink" href="https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.SparkConf" target="_blank">https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.SparkConf</a> for complete list of methods exposed by <code class="literal">SparkContext</code> and<a id="id178" class="indexterm"></a> <code class="literal">SparkConf</code>.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec11"></a>SparkContext and Spark Config â€“ Java APIs</h4></div></div></div><p>Though<a id="id179" class="indexterm"></a> Scala code is eventually converted into class <a id="id180" class="indexterm"></a>files and executed on JVM itself, there are some APIs which are only defined in Scala and do not have an equivalent in Java. So for all those Scala objects, Spark provides compatible and equivalent Java APIs for interoperability in the <code class="literal">org.apache.spark.api.java.*</code> package. For example, <code class="literal">org.apache.spark.api.java.JavaSparkContext</code> is defined as a Java-friendly version of <code class="literal">SparkContext</code> which provides the same functionality but utilizes the Java primitives instead of Scala.</p><p>We need to remember that there are only certain Scala functions which do not have an equivalent in Java like higher order functions (a function within a function), so we will use Java implementations only for those functions and the rest will still use the Scala APIs. For example, <code class="literal">org.apache.spark.SparkConf</code> does not have an equivalent Java API, as it already defines Java-friendly or compatible operations.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note13"></a>Note</h3><p>Further in the book, we will use the Scala APIs for all our examples.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec12"></a>RDD â€“ Scala APIs</h4></div></div></div><p>
<code class="literal">org.apache.spark.rdd.RDD.scala</code> is another important component of Spark which represents the distributed collection of datasets. It also exposes various operations which can be<a id="id181" class="indexterm"></a> executed in parallel over the cluster. SparkContext exposes various methods to load the data from HDFS or the local filesystem or Scala collections and finally create an RDD on which various operations such as <code class="literal">map</code>, <code class="literal">filter</code>, <code class="literal">join</code>, and <code class="literal">persist</code> can be invoked.</p><p>RDD also defines some useful child classes within the <code class="literal">org.apache.spark.rdd.*</code> package like <code class="literal">PairRDDFunctions</code> for working with key/value pairs, <code class="literal">SequenceFileRDDFunctions</code> for working with Hadoop sequence files and <code class="literal">DoubleRDDFunctions</code> for working with RDDs of doubles.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note14"></a>Note</h3><p>Refer to <a class="ulink" href="https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.rdd.RDD" target="_blank">https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.rdd.RDD</a> for a complete list of methods exposed by <code class="literal">RDD.scala</code> and<a id="id182" class="indexterm"></a> refer to <a class="ulink" href="https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.rdd.package" target="_blank">https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.rdd.package</a> for various types of RDDs provided by Spark.</p></div><p>We <a id="id183" class="indexterm"></a>discussed a few examples in <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Installing and Configuring Spark and Spark Streaming</em></span> and <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Architecture and Components of Spark and Spark Streaming</em></span> which load data from the local filesystem and create an RDD. Let us see one more example for creating RDDs from the data in relational databases where we will use <code class="literal">org.apache.spark.rdd.JDBCRDD.scala</code>:</p><div class="informalexample"><pre class="programlisting">//Define the Configuration
val conf = new SparkConf();
//Define Context
val ctx = new SparkContext(conf)

//Define JDBC RDD
val rdd = new JdbcRDD(
ctx,
() =&gt; { DriverManager.getConnection("jdbc:derby:temp/Jdbc-RDDExample") },
      "SELECT EMP_ID,Name FROM EMP WHERE Age &gt; = ? AND ID &lt;= ?",20, 30, 3,
      (r: ResultSet) =&gt; { r.getInt(1); r.getString(2) } ).cache()

//Print only first Column in the ResultSet
System.out.println(rdd.first)</pre></div><p>The preceding example assumes that you have the Apache Derby database with an <code class="literal">EMP</code> table with columnsâ€”ID, name, age and the ID of all employees within the age group of 20 to 30 and finally print the first record.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note15"></a>Note</h3><p>For other<a id="id184" class="indexterm"></a> available operations, please refer to <code class="literal">JdbcRDD.scala</code> (<a class="ulink" href="https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.rdd.JdbcRDD" target="_blank">https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.rdd.JdbcRDD</a>).</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec13"></a>RDD â€“ Java APIs</h4></div></div></div><p>
<code class="literal">org.apache.spark.api.java.JavaRDD</code> and <code class="literal">org.apache.spark.api.java.JavaRDDLike</code> are top two classes defined in Scala for interoperability with the Java APIs.</p><p>It also<a id="id185" class="indexterm"></a> defines <code class="literal">JavaDoubleRDD</code>, <code class="literal">JavaHadoopRDD</code>, <code class="literal">JavaNewHadoopRDD</code> and <code class="literal">JavaPairRDD</code> in <code class="literal">org.apache.spark.api.java.package</code>.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note16"></a>Note</h3><p>Refer<a id="id186" class="indexterm"></a> to <a class="ulink" href="https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.api.java.package" target="_blank">https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.api.java.package</a> for further details on the Java-specific RDD APIs.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec14"></a>Other Spark Core packages</h4></div></div></div><p>Spark provides many other classes and distributes them as part of Spark Core packages. Let's discuss <a id="id187" class="indexterm"></a>the important ones which we will be using frequently while developing Spark jobs:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">org.apache.spark</code>: It is the core package of the Spark API which contains functionality for creating/distributing/submitting Spark jobs on the cluster.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note17"></a>Note</h3><p>For<a id="id188" class="indexterm"></a> more information, refer to <a class="ulink" href="https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.package" target="_blank">https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.package</a>.</p></div></li><li style="list-style-type: disc"><p>
<code class="literal">org.apache.spark.annotation</code>: Contains the annotations which are used within the Spark API. This is the internal Spark package and you may not use the annotations defined in this package while developing your custom Spark jobs. The three main annotations defined within this package are:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">DeveloperAPI</code>: All those methods which are marked with <code class="literal">DeveloperAPI</code> are for advance usage where users are free to extend and modify the default functionality. These methods may be changed or removed in the next minor or major releases.</p></li><li style="list-style-type: disc"><p>
<code class="literal">Experimental</code>: All those methods or classes which are officially not adopted by Spark but are introduced temporarily in a specific release are marked as <code class="literal">Experimental</code>. These methods may be changed or removed in the next minor or major releases.</p></li><li style="list-style-type: disc"><p>
<code class="literal">AlphaComponent</code>: All those methods or classes which are still in the testing phase and are not recommended for production use are marked as <code class="literal">AlphaComponent</code>. These methods may be changed or removed in the next minor or major releases.</p></li></ul></div></li><li style="list-style-type: disc"><p>
<code class="literal">org.apache.spark.broadcast</code>: Provides the API for sharing the read-only variables <a id="id189" class="indexterm"></a>across the Spark jobs. Once the variables are defined and broadcast, they cannot be changed. This is one of the important packages which are frequently used by developers in their custom Spark jobs. Broadcasting the variables and data across the cluster is a complex task and we need to ensure that an efficient mechanism is used so that it improves the overall performance of the Spark job and does not become an overhead. Spark provides two different types of implementations of broadcastsâ€”<code class="literal">HttpBroadcast</code> and <code class="literal">TorrentBroadcast</code>. <code class="literal">HttpBroadcast</code> leverages the HTTP server as a broadcast mechanism. In this mechanism the broadcast data is fetched from the driver, through a HTTP Server running at the driver itself and further stored in the executor Block Manager for faster accesses. <code class="literal">TorrentBroadcast</code>, which is also the default implementation of the broadcast, maintains its own Block Manager. The first request to access the data makes the call to its own Block Manager and, if not found, the data is fetched in chunks from the executor or driver. Torrent Broadcast works on the principle of BitTorrent and ensures that the driver is not the bottleneck in fetching the shared variables and data.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note18"></a>Note</h3><p>Spark <a id="id190" class="indexterm"></a>also provides accumulators which work like broadcast but provide updatable variables shared across the Spark jobs but with some limitations. Please<a id="id191" class="indexterm"></a> refer to <a class="ulink" href="https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.Accumulator" target="_blank">https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.Accumulator</a>.</p></div></li><li style="list-style-type: disc"><p>
<code class="literal">org.apache.spark.io</code>: Provides implementation of various compression libraries which can be used at block storage level. This whole package is marked as <code class="literal">DeveloperAPI</code>, so developers can extend and provide their own implementations. By default, it provides three implementationsâ€”LZ4, LZF and Snappy. Again developers may not use it directly but if required, it can provide custom implementations, for performance-tuning the compression/decompression mechanism.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note19"></a>Note</h3><p>For<a id="id192" class="indexterm"></a> more details, refer to <a class="ulink" href="https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.io.package" target="_blank">https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.io.package</a>.</p></div></li><li style="list-style-type: disc"><p>
<code class="literal">org.apache.spark.scheduler</code>: This provides various scheduler libraries which help in job scheduling, tracking and monitoring. It defines the <span class="strong"><strong>Directed Acyclic Graph</strong></span> (<span class="strong"><strong>DAG</strong></span>) scheduler <a class="ulink" href="http://en.wikipedia.org/wiki/Directed_acyclic_graph" target="_blank">http://en.wikipedia.org/wiki/Directed_acyclic_graph</a>. Spark DAG <a id="id193" class="indexterm"></a>scheduler defines the stage-oriented scheduling <a id="id194" class="indexterm"></a>where it keeps track <a id="id195" class="indexterm"></a>of the completion of each RDD and the output of each stage and then computes DAG which is further submitted to the underlying <code class="literal">org.apache.spark.scheduler.TaskScheduler</code> that runs them on the cluster.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note20"></a>Note</h3><p>Refer <a id="id196" class="indexterm"></a>to <a class="ulink" href="https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.scheduler.package" target="_blank">https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.scheduler.package</a>.</p></div></li><li style="list-style-type: disc"><p>
<code class="literal">org.apache.spark.serializer</code>: Defines the APIs for serialization and deserialization used in data shuffling and RDD.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note21"></a>Note</h3><p>Refer to <a class="ulink" href="https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.serializer.package" target="_blank">https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.serializer.package</a>.</p></div></li><li style="list-style-type: disc"><p>
<code class="literal">org.apache.spark.storage</code>: Provides APIs for structuring, managing and finally persisting the data stored with RDD within blocks. It also keeps tracks of data and ensures it is either stored in memory or, if the memory is full, it is flushed to disk.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note22"></a>Note</h3><p>Refer to <a class="ulink" href="https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.storage.package" target="_blank">https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.storage.package</a>.</p></div></li><li style="list-style-type: disc"><p>
<code class="literal">org.apache.spark.ui</code>: Contains <a id="id197" class="indexterm"></a>the classes containing the data which needs to be displayed on Spark UI.</p></li><li style="list-style-type: disc"><p>
<code class="literal">org.apache.spark.util</code>: Utility classes for performing common functions across the Spark APIs. For example, it defines <code class="literal">MutablePair</code> which can be used as an alternative to Scala's <code class="literal">Tuple2</code> with the difference that <code class="literal">MutablePair</code> is updatable while Scala's <code class="literal">Tuple2</code> is not. It helps in optimizing<a id="id198" class="indexterm"></a> memory and minimizing object allocations.</p></li></ul></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec28"></a>Spark libraries and extensions</h3></div></div></div><p>Spark<a id="id199" class="indexterm"></a> extensions are the auxiliary features developed by extending <a id="id200" class="indexterm"></a>Core Spark APIs. Spark extensions are segregated from the Core Spark API and packaged into their own packages.</p><p>Let's see the packaging structure of each of the Spark extensions and discuss a few of the important classes provided by each of the Spark extensions.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec15"></a>Spark Streaming</h4></div></div></div><p>All Spark<a id="id201" class="indexterm"></a> Streaming classes are packaged in the <code class="literal">org.apache.spark.streaming.*</code> package. Spark Streaming defines two critical classes, <code class="literal">StreamingContext.scala</code> and <code class="literal">DStream.scala</code>. Let's examine the functions and roles <a id="id202" class="indexterm"></a>performed by these classes:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">org.apache.spark.streaming.StreamingContext</code>: It is similar to <code class="literal">SparkContext</code> but provides an entry point to Spark Streaming functionality. It defines methods to create the objects of <code class="literal">DStream.scala</code> from input sources. It also provides functions to start and stop the Spark Streaming jobs.</p></li><li style="list-style-type: disc"><p>
<code class="literal">org.apache.spark.streaming.dstream.DStream.scala</code>: DStream, or discretized streams, provides the basic abstraction of Spark Streaming. It provides the sequence of RDDs created from the live data or transforming the existing DStreams. This class defines the global operations which can be performed on all DStreams and a few specific operations which can be applied on specific types of DStreams.</p></li></ul></div><p>Spark<a id="id203" class="indexterm"></a> Streaming also defines various sub packages for providing implementation for various types of input receivers:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">org.apache.spark.streaming.flume.*</code>: Provides classes for consuming input data<a id="id204" class="indexterm"></a> from Fume (<a class="ulink" href="https://flume.apache.org/" target="_blank">https://flume.apache.org/</a>).</p></li><li style="list-style-type: disc"><p>
<code class="literal">org.apache.spark.streaming.kafka.*</code>: Provides classes for consuming input <a id="id205" class="indexterm"></a>data from Kafka (<a class="ulink" href="http://kafka.apache.org/" target="_blank">http://kafka.apache.org/</a>).</p></li><li style="list-style-type: disc"><p>
<code class="literal">org.apache.spark.streaming.mqtt.*</code>: Provides classes for consuming input data<a id="id206" class="indexterm"></a> from MQTT (<a class="ulink" href="http://mqtt.org" target="_blank">http://mqtt.org</a>).</p></li><li style="list-style-type: disc"><p>
<code class="literal">org.apache.spark.streaming.kinesis.*</code>: Provides classes for consuming<a id="id207" class="indexterm"></a> input data from Amazon Kinesis (<a class="ulink" href="http://aws.amazon.com/kinesis/" target="_blank">http://aws.amazon.com/kinesis/</a>).</p></li><li style="list-style-type: disc"><p>
<code class="literal">org.apache.spark.streaming.twitter.*</code>: Provides classes for consuming <a id="id208" class="indexterm"></a>input data from Twitter feeds.</p></li><li style="list-style-type: disc"><p>
<code class="literal">org.apache.spark.streaming.zeromq.*</code>: Provides classes for consuming<a id="id209" class="indexterm"></a> input data from Zero MQ (<a class="ulink" href="http://zeromq.org/" target="_blank">http://zeromq.org/</a>).</p></li></ul></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note23"></a>Note</h3><p>For more <a id="id210" class="indexterm"></a>information on Spark Streaming APIs, refer to <a class="ulink" href="https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.streaming.package" target="_blank">https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.streaming.package</a>.</p><p>Spark Streaming also define compatible Java classes in the <code class="literal">org.apache.spark.streaming.api.java.*</code> package for interoperability with Java APIs.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec16"></a>Spark MLlib</h4></div></div></div><p>Spark <a id="id211" class="indexterm"></a>provides a scalable machine learning library consisting of common machine learning algorithms and utilities. It includes various standard algorithms<a id="id212" class="indexterm"></a> for classification (<a class="ulink" href="http://en.wikipedia.org/wiki/Statistical_classification" target="_blank">http://en.wikipedia.org/wiki/Statistical_classification</a>), regression (<a class="ulink" href="http://en.wikipedia.org/wiki/Regression_analysis" target="_blank">http://en.wikipedia.org/wiki/Regression_analysis</a>), clustering (<a class="ulink" href="http://en.wikipedia.org/wiki/Cluster_analysis" target="_blank">http://en.wikipedia.org/wiki/Cluster_analysis</a>), and collaborative filtering (<a class="ulink" href="http://en.wikipedia.org/wiki/Recommender_system#Collaborative_filtering" target="_blank">http://en.wikipedia.org/wiki/Recommender_system#Collaborative_filtering</a>).</p><p>All Spark <a id="id213" class="indexterm"></a>machine learning libraries are packaged in the <code class="literal">org.apache.spark.mllib.*</code> package. Let's see some of the main packages provided by the <code class="literal">org.apache.spark.mllib.*</code> package:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">org.apache.spark.mllib.classificaiton.*</code>: Provides implementation of various <a id="id214" class="indexterm"></a>machine learning algorithms for classification like Naive Bayes (<a class="ulink" href="http://en.wikipedia.org/wiki/Naive_Bayes_classifier" target="_blank">http://en.wikipedia.org/wiki/Naive_Bayes_classifier</a>), support vector<a id="id215" class="indexterm"></a> machines (<code class="literal">http://en.wikipedia.org/wiki/Support_vector_machine</code>) and many more. Refer to <a class="ulink" href="https://spark.apache.org/docs/latest/mllib-classification-regression.html" target="_blank">https://spark.apache.org/docs/latest/mllib-classification-regression.html</a> for more information on supported classification<a id="id216" class="indexterm"></a> models.</p></li><li style="list-style-type: disc"><p>
<code class="literal">org.apache.spark.mllib.recommendation.*</code>: Provides classes for collaborative filtering which are generally used to build recommended systems. For <a id="id217" class="indexterm"></a>more<a id="id218" class="indexterm"></a> details, refer to <a class="ulink" href="https://spark.apache.org/docs/latest/mllib-collaborative-filtering.html" target="_blank">https://spark.apache.org/docs/latest/mllib-collaborative-filtering.html</a>.</p></li><li style="list-style-type: disc"><p>
<code class="literal">org.apache.spark.mllib.clustering.*</code>: Provides implementation of various<a id="id219" class="indexterm"></a> clustering algorithms. For more information, please refer to <a class="ulink" href="https://spark.apache.org/docs/latest/mllib-clustering.html" target="_blank">https://spark.apache.org/docs/latest/mllib-clustering.html</a>.</p></li></ul></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note24"></a>Note</h3><p>Spark 1.2 also<a id="id220" class="indexterm"></a> introduced a new package called <code class="literal">org.apache.spark.ml</code>, which is an alpha component and aims to provide a uniform set of high-level APIs for creating and tuning practical machine learning pipelines. For more information, refer to <a class="ulink" href="https://spark.apache.org/docs/latest/ml-guide.html" target="_blank">https://spark.apache.org/docs/latest/ml-guide.html</a>.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec17"></a>Spark SQL</h4></div></div></div><p>Spark SQL <a id="id221" class="indexterm"></a>provides the processing of structured data and facilitates the<a id="id222" class="indexterm"></a> execution of relational queries which are expressed in structured query language. (<a class="ulink" href="http://en.wikipedia.org/wiki/SQL" target="_blank">http://en.wikipedia.org/wiki/SQL</a>).</p><p>All Spark SQL classes are packaged in <code class="literal">org.apcahe.spark.sql.*</code> and its subpackages. Let us discuss a few of the important Spark SQL packages:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">org.apache.spark.sql.*</code>: Topmost package which contains all classes and <a id="id223" class="indexterm"></a>subpackages for the implementation of Spark SQL.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note25"></a>Note</h3><p>For more information, refer to <a class="ulink" href="https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.sql.package" target="_blank">https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.sql.package</a>.</p></div></li><li style="list-style-type: disc"><p>
<code class="literal">org.apache.spark.sql.hive.*</code>: Provides the implementation for executing Apache Hive queries over Spark.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note26"></a>Note</h3><p>For more information, refer to <a class="ulink" href="https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.sql.hive.package" target="_blank">https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.sql.hive.package</a>.</p></div></li><li style="list-style-type: disc"><p>
<code class="literal">org.apache.spark.sql.sources.*</code>: Provides classes and APIs for adding different kinds of data sources to Spark SQL. Spark SQL provides a unified interface for loading and persisting data from various data sources like CSV, JSON, JDBC, Parquet, Hive, AWS S3, Amazon Redshift, HBase, MongoDB, Elasticsearch, Solr, and many more. It also provides the flexibility for extending<a id="id224" class="indexterm"></a> and adding new data sources. A few of the data source APIs for loading the data from Hive, Parquet and JDBC (MySQL and PostgreSQL) are distributed with the standard Spark distribution while for others we need to download from the vendor or Spark community website and manually configure.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note27"></a>Note</h3><p>For more<a id="id225" class="indexterm"></a> information, refer to <a class="ulink" href="https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.sql.sources.package" target="_blank">https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.sql.sources.package</a>.</p><p>Databricks<a id="id226" class="indexterm"></a> provide the list of third party packages and extensions for various other data sources (<a class="ulink" href="http://spark-packages.org/" target="_blank">http://spark-packages.org/</a>) with an understanding that support will be directly provided by the vendor and not by Spark or Databricks.</p></div></li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec18"></a>Spark GraphX</h4></div></div></div><p>Spark GraphX is<a id="id227" class="indexterm"></a> another extension which provides loading, structuring <a id="id228" class="indexterm"></a>and distributed processing of data in the form of graphs: <a class="ulink" href="http://en.wikipedia.org/wiki/Graph_(abstract_data_type)" target="_blank">http://en.wikipedia.org/wiki/Graph_(abstract_data_type)</a>.</p><p>GraphX supports the <span class="strong"><strong>property graph data model</strong></span> which is a combination of vertices and edges. The graph provides basic operations to access and manipulate the data associated with vertices and edges as well as the underlying structure. Like Spark RDDs, the graph is a functional data structure in which mutating operations return new graphs.</p><p>Let's discuss<a id="id229" class="indexterm"></a> a few of the important Spark GraphX packages:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">org.apache.spark.graphx.*</code>: Topmost package which contains all classes and <a id="id230" class="indexterm"></a>subpackages for the implementation of GraphX. For more information, refer to <a class="ulink" href="https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.graphx.package" target="_blank">https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.graphx.package</a>.</p></li><li style="list-style-type: disc"><p>
<code class="literal">org.apache.spark.graphx.impl.*</code>: It contains the implementations for a few of the abstract classes defined in <code class="literal">org.apache.spark.graphx package</code> like <code class="literal">org.apache.spark.graphx.VertexRDD</code>, <code class="literal">org.apache.spark.graphx.EdgeRDD</code> or <code class="literal">org.apache.spark.graphx.Graph</code>. For more information, refer to <a class="ulink" href="https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.graphx.impl.package" target="_blank">https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.graphx.impl.package</a>.</p></li><li style="list-style-type: disc"><p>
<code class="literal">org.apache.spark.graphx.lib.*</code>: It contains the various utility classes, analytical <a id="id231" class="indexterm"></a>functions and algorithms for processing<a id="id232" class="indexterm"></a> graphs<a id="id233" class="indexterm"></a> like PageRank (<a class="ulink" href="http://en.wikipedia.org/wiki/PageRank" target="_blank">http://en.wikipedia.org/wiki/PageRank</a>), SVD (<a class="ulink" href="http://en.wikipedia.org/wiki/Singular_value_decomposition" target="_blank">http://en.wikipedia.org/wiki/Singular_value_decomposition</a>) and shortest <a id="id234" class="indexterm"></a>path (<a class="ulink" href="http://en.wikipedia.org/wiki/Shortest_path_problem" target="_blank">http://en.wikipedia.org/wiki/Shortest_path_problem</a>). For more information, refer to <a class="ulink" href="https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.graphx.lib.package" target="_blank">https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.graphx.lib.package</a>.</p></li></ul></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note28"></a>Note</h3><p>Refer to <a class="ulink" href="https://spark.apache.org/docs/latest/graphx-programming-guide.html" target="_blank">https://spark.apache.org/docs/latest/graphx-programming-guide.html</a> for more information on GraphX.</p></div><p>In this section we discussed the important Spark APIs/classes and packages provided by Spark and its various libraries/extensions. Let's move on and discuss in detail the architecture of RDD and DStreams which are a core abstraction layer for providing the distributed processing of the datasets.</p></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec20"></a>Resilient distributed datasets and discretized streams</h2></div></div><hr /></div><p>In this<a id="id235" class="indexterm"></a> section we will discuss the architecture, motivation and other important concepts related to resilient distributed datasets. We <a id="id236" class="indexterm"></a>will also talk about the implementation methodology adopted by Spark libraries/extensions like Spark Streaming for extending and exposing resilient distributed datasets.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec29"></a>Resilient distributed datasets</h3></div></div></div><p>
<span class="strong"><strong>Resilient distributed datasets</strong></span> (<span class="strong"><strong>RDD</strong></span>) is an independent concept which was developed in the University of California, Berkeley and was first implemented in systems like Spark to show its real usage and power. RDD is a core component of Spark. It provides in-memory representation of immutable datasets for parallel and distributed processing. RDD are more of abstraction (an agnostic of the underlying data store) providing the core functionality of in-memory data representation for storing and retrieving data objects which can be further extended to capture various data structures like graph or relational structures or streaming data. Let's move forward and talk about the nitty-gritty of RDD.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec19"></a>Motivation behind RDD</h4></div></div></div><p>Frameworks like Hadoop and MapReduce are widely adopted for parallel and distributed data processing. There is no doubt that these frameworks introduce a new paradigm for distributed <a id="id237" class="indexterm"></a>data processing and that too in fault-tolerant manner (without losing a single byte). But it does have some limitations. It is not suited for the problem statements where we need iterative data processing as in recursive functions or machine learning algorithms where data needs to be in-memory for the computations.</p><p>For all those scenarios, a new paradigmâ€”RDD was introduced which contains all the features of Hadoop like systems like distributed processing, fault-tolerant, and so on but essentially keeps data in the memory providing distributed in-memory data processing over a cluster of nodes.</p><p>Let us move<a id="id238" class="indexterm"></a> forward and discuss the important features of RDDs.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec20"></a>Fault tolerance</h4></div></div></div><p>One of <a id="id239" class="indexterm"></a>the main challenges of RDD was to provide an efficient fault-tolerance mechanism for the datasets which are already loaded and processed in the memory. Though this not a new concept and is already being implemented in various distributed processing systems like Hadoop, key-value stores and so on but the architecture used by these systems is not good enough for in-memory distributed data processing. Frameworks like Hadoop, provided fault tolerance by maintaining multiple copies of the same dataset over the various nodes in the cluster or maintain the log of updates happening over the original datasets and applying the same over the machines/nodes. This process is good for disk-based systems but the same mechanism is inefficient for data-intensive workloads or memory-based systems because first they require copying large amounts of data over the cluster network, whose bandwidth is far lower than that of RAM, and second they incur substantial storage overheads.</p><p>RDD introduced a new concept for fault tolerance and provided a coarse-grained interface based on transformations. Now, instead of replicating data or keeping logs of updates, RDDs keep track of the transformations (like map, reduce, join, and so on) applied to the particular dataset, which<a id="id240" class="indexterm"></a> is also called a <span class="strong"><strong>data lineage</strong></span>.</p><p>This allows an efficient mechanism for fault tolerance where, in event of loss of any partition, RDD has enough information to derive the same partition by applying the same set of transformations on the original dataset. Moreover this computation is parallel and involves processing on multiple nodes, so the recomputation is very fast and efficient too, in contrast<a id="id241" class="indexterm"></a> to costly replication used by other distributed data processing frameworks.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec21"></a>Transformations and actions</h4></div></div></div><p>RDDs<a id="id242" class="indexterm"></a> are read-only collections of records which are partitioned <a id="id243" class="indexterm"></a>over the cluster of nodes. They are created by applying deterministic operations over the distributed datasets residing in any persistence storage system like disks, Queues, NoSQL, RDBMS, and so on. These deterministic operations are known as <span class="strong"><strong>transformations</strong></span>. Transformations by design are lazy and are not applied instantly unless the results are requested by the driver program. The operations which apply the<a id="id244" class="indexterm"></a> transformation and return the value to the drivers are known as <span class="strong"><strong>actions</strong></span>.</p><p>For example, invoking a <code class="literal">map(â€¦)</code> function is an transformation which does not yield results but invoking <code class="literal">reduce(â€¦)</code> is an action which yields results and returns the output to the driver.</p><p>A few of the frequently used transformation functions provided by RDDs are:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">map(func)</code>: Applies <a id="id245" class="indexterm"></a>the provided function to each element of the source dataset and generates a new distributed RDD.</p></li><li style="list-style-type: disc"><p>
<code class="literal">filter(func)</code>: Applies the provided function to all the elements of the source datasets and generates a new RDD containing only those elements for which <code class="literal">func</code> returned <code class="literal">true</code>.</p></li><li style="list-style-type: disc"><p>
<code class="literal">union(otherDataset)</code>: Joins the datasets and returns a new RDD containing all the elements of an RDD on which <code class="literal">union</code> was invoked and the elements (RDD) were passed as an argument.</p></li><li style="list-style-type: disc"><p>
<code class="literal">sortByKey([asc/desc],[numOfTasks])</code>: When called on a dataset of key/value, it returns a dataset of key/value sorted by keys in ascending or descending order, as specified in the first argument. By default, it is ascending.</p></li></ul></div><p>All of the preceding transformation functions return a new/fresh RDD. Remember, RDDs are immutable and, once created cannot be changed.</p><p>A few <a id="id246" class="indexterm"></a>of the frequently used action functions provided by RDDs are:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">reduce(func)</code>: Aggregates all the elements of the datasets by applying <code class="literal">func</code>. The provided <code class="literal">func</code> should be commutative and associative so that it can be computed correctly in parallel.</p></li><li style="list-style-type: disc"><p>
<code class="literal">collect()</code>: Returns all the elements of the datasets as an array to the driver program.</p></li><li style="list-style-type: disc"><p>
<code class="literal">first()</code>: Returns <a id="id247" class="indexterm"></a>the first element of the RDD or dataset.</p></li><li style="list-style-type: disc"><p>
<code class="literal">take(n)</code>: Returns<a id="id248" class="indexterm"></a> the first <span class="emphasis"><em>n</em></span> elements of the RDD or dataset.</p></li></ul></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note29"></a>Note</h3><p>Refer<a id="id249" class="indexterm"></a> to <a class="ulink" href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD" target="_blank">https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD</a> for a complete list of operations provided by RDD.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec22"></a>RDD storage</h4></div></div></div><p>RDDs by <a id="id250" class="indexterm"></a>design are distributed and partitioned over clusters of nodes which are kept in the system's memory but, at the same time, it provides operations which can be used to store RDD on disks or to an external system. The native integration for storing RDD in a filesystem and HDFS is provided by Spark Core packages while it is also extended by the community and various other vendors to provide storage of RDD in external storage systems like MongoDB, DataStax, Elasticsearch, and so on.</p><p>The following are a few of the functions which can be used for storing RDDs:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">saveAsTextFile(path)</code>: Writes the elements of RDD to a text file in a local filesystem or HDFS or any other mapped or mounted network drive.</p></li><li style="list-style-type: disc"><p>
<code class="literal">saveAsSequenceFile (path)</code>: Writes the elements of RDD as a Hadoop Sequence file in a local filesystem or HDFS or any other mapped or mounted network drive. This is available on RDDs of key-value pairs that implement Hadoop's writable interface.</p></li><li style="list-style-type: disc"><p>
<code class="literal">saveAsObjectFile(path)</code>: Write the elements of the dataset to a given path using Java serialization mechanism, which can then be loaded using <code class="literal">SparkContext.objectFile(path)</code>.</p></li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec23"></a>RDD persistence</h4></div></div></div><p>Persistence <a id="id251" class="indexterm"></a>in RDD is also called caching of RDD. It can simply be done by invoking <code class="literal">&lt;RDD&gt;.persist(StorageLevel)</code> or <code class="literal">&lt;RDD&gt;.cache()</code>. By default, RDD is persisted in memory (default for <code class="literal">cache()</code>) but it also provides the persistence on disks or any other external systems which are defined by the provided function <code class="literal">persist</code> and its<a id="id252" class="indexterm"></a> parameter of class <code class="literal">StorageLevel</code> (<a class="ulink" href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.storage.StorageLevel$" target="_blank">https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.storage.StorageLevel$</a>).</p><p>
<code class="literal">StorageLevel</code> is annotated as <code class="literal">DeveloperApi()</code> which can be extended to provide the custom implementation of persistence.</p><p>Caching or persistence is a key tool for iterative algorithms and fast interactive use. Whenever persist is invoked on RDD, each node stores its associated partitions and computes in memory and further reuses them in other actions on the computed datasets. This in turn enables the future actions to be much faster.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec24"></a>Shuffling in RDD</h4></div></div></div><p>
<span class="strong"><strong>Shuffling</strong></span> redistributes data across clusters so that it is grouped differently<a id="id253" class="indexterm"></a> across the partitions. It is a costly operation as it involves <a id="id254" class="indexterm"></a>copying data across executors and nodes and then creating new partitions and it then distributes them across the cluster.</p><p>There are certain transformation operations defined in <code class="literal">org.apache.spark.rdd.RDD</code> and <code class="literal">org.apache.spark.rdd.PairRDDFunctions</code> which trigger shuffling and repartition. These <a id="id255" class="indexterm"></a>operations include:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">RDD.repartition(â€¦)</code>: Repartitions the existing dataset across the cluster of nodes</p></li><li style="list-style-type: disc"><p>
<code class="literal">RDD.coalesce(â€¦)</code>: Repartitions the existing dataset into a smaller number of given partitions</p></li><li style="list-style-type: disc"><p>All operations which ends by <code class="literal">ByKey</code> (except count operations) like <code class="literal">PairRDDFunctions.reducebyKey()</code> or <code class="literal">groupByKey</code>
</p></li><li style="list-style-type: disc"><p>All join operations like <code class="literal">PairRDDFunctions.join(â€¦)</code> or <code class="literal">PairRDDFunctions.cogroup(â€¦)</code> operations</p></li></ul></div><p>Shuffling is a costly and time-consuming operation as it requires the recomputation of datasets which may span partitions, invoking operations like <code class="literal">groupByKey(â€¦.)</code> for grouping, or for performing aggregation like sum or average. It is a real challenge because not all values for a single key necessarily reside on the same partition/node or machine, but they need to be collocated to compute the results. During computations, a single task will operate on a single partition but, to organize all the data for a single <code class="literal">groupByKey</code> task to execute, it will read from all partitions and find all the values for all keys, and then bring all the values across the partitions to compute the final result for each key.</p><p>The shuffle is an expensive operation since it involves disk I/O, data serialization, and network I/O but there are certain configurations which can help in tuning and performance <a id="id256" class="indexterm"></a>optimizations.</p><p>Refer to <a class="ulink" href="https://spark.apache.org/docs/latest/configuration.html#shuffle-behavior" target="_blank">https://spark.apache.org/docs/latest/configuration.html#shuffle-behavior</a> for a complete list of parameters which can be used to optimize the shuffle operations.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note30"></a>Note</h3><p>Refer to <a class="ulink" href="https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf" target="_blank">https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf</a> for<a id="id257" class="indexterm"></a> more information on RDDs.</p></div><p>In this section we have talked about the core and intrinsic features of RDD. Now as we have enough information on RDDs, let's move on and discuss the way it has been extended by Spark extensions like Spark Streaming.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec30"></a>Discretized streams</h3></div></div></div><p>Discretized <a id="id258" class="indexterm"></a>streams are also called DStreams and are a new stream processing model in which computations are structured as a series of stateless, deterministic batch computations at small time intervals.</p><p>This new stream processing model not only enables powerful recovery mechanisms (similar to those in batch systems) but it also out-performs replication and upstream backup.</p><p>DStreams leveraged the concepts of resilient distributed datasets and created a series of RDDs (of the same type) as one single DStream which is processed and computed at a user-defined time interval. DStreams can be created either from input data streams which are connected to data sources like filesystems, sockets, and so on. DStreams can also be created by applying high-level operations on other DStreams. There can be multiple DStreams per Spark Streaming context and each DStream contains a series of RDDs. Each RDD is the snapshot of the data received at a particular point in time from the receiver. The duration or interval after which data needs to be converted or computed into RDDs is defined as the second parameter of <code class="literal">StreamingContext</code>:</p><div class="informalexample"><pre class="programlisting">val streamCtx = new StreamingContext(conf, Seconds(2))</pre></div><p>For example, the data from Twitter feeds is collected every 100 milliseconds but it is processed every second:</p><div class="informalexample"><pre class="programlisting">val streamCtx = new StreamingContext(conf, Seconds(1))</pre></div><p>So the DStream of this feed would look similar to the following illustration:</p><div class="mediaobject"><img src="graphics/B01793_03_01.jpg" /></div><p>DStreams <a id="id259" class="indexterm"></a>are primarily an execution strategy, breaking down computation into steps. They implement <a id="id260" class="indexterm"></a>and use most of the standard operations which are used in other streaming systems like "sliding window" or "incremental processing". The real benefit of DStreams was a Unified Architecture and design for processing Streams and, at the same time, it also exposed Stream operations over other interfaces like SQL.</p><p>DStreams support two types of operations:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Transformations</strong></span>: Supports all transformation operations as supported by RDD<a id="id261" class="indexterm"></a> like <code class="literal">map()</code>, <code class="literal">flatmap()</code>, and so on and creates a new DStream from the processed data. Transformations are applied separately on the RDD at each time interval.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Output operations</strong></span>: These operations help to save the final output to external systems<a id="id262" class="indexterm"></a> or the same external cache. Again it is similar to the output operations supported by RDD.</p></li></ul></div><p>Apart from all the standard operations, DStreams support two additional operations:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Windowing</strong></span>: Windowing is a special type of operation which is provided only<a id="id263" class="indexterm"></a> by DStreams and groups all the records from a sliding window of past time intervals into one RDD. We will talk more about windowing functions in <a class="link" href="#" linkend="ch04">Chapter 4</a>, <span class="emphasis"><em>Applying Transformations to Streaming Data</em></span>.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Incremental aggregation</strong></span> / <span class="strong"><strong>stateful processing</strong></span>: This<a id="id264" class="indexterm"></a> provides the functionality for a common use case where we may need to compute an aggregate like a count or max over a sliding window. DStreams provide several variants of an incremental operation. All methods in <code class="literal">DStream.scala</code> prefixed with <code class="literal">Window</code> provide incremental aggregations like <code class="literal">countByWindow</code>, <code class="literal">reduceByWindow</code>, and so on.</p></li></ul></div><p>Refer to <a class="ulink" href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.dstream.DStream" target="_blank">https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.dstream.DStream</a> for more information on the operations supported <a id="id265" class="indexterm"></a>by the DStream API.</p><p>In this<a id="id266" class="indexterm"></a> section we have talked about the core and intrinsic features of discretized streams for stream processing in Spark. Now let us move forward and discuss the use case where we will use all these concepts and load the streaming data from multiple and varied data sources.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec21"></a>Data loading from distributed and varied sources</h2></div></div><hr /></div><p>In large<a id="id267" class="indexterm"></a> enterprises, log file analysis is one of the popular use cases. Architects/business analysts and all other stake holders always want to analyze the logs of various activities like events, security, access, and so on and uncover the hidden patterns. For example, the web logs from a popular user interfacing application (a website or portal) can easily provide you with the following information:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Most popular pages: Frequently visited pages</p></li><li style="list-style-type: disc"><p>Type of browsers or user agent used by consumers to visit the website</p></li><li style="list-style-type: disc"><p>Origin of the user: Users' referrer</p></li><li style="list-style-type: disc"><p>Final status of the user request (HTTP status codes): Successful (200), broken links (404), redirection (301), and many more</p></li></ul></div><p>Consider another example where enterprises are always concerned about unauthorized access to their network and shared resources. Network administrators used to spend a considerable amount of time in analyzing the access logs or event logs where they were constantly looking for any unauthorized access or suspicious activities with respect to their critical, important and confidential network resources.</p><p>There are many more use cases where log analysis can provide insights into existing applications or network resources which not only help in improving the overall user experience (in<a id="id268" class="indexterm"></a> the case of web applications) but also protect from outside<a id="id269" class="indexterm"></a> threats like hacking (<a class="ulink" href="http://en.wikipedia.org/wiki/Hacker_(computer_security)" target="_blank">http://en.wikipedia.org/wiki/Hacker_(computer_security)</a>) or spoofing (<a class="ulink" href="http://en.wikipedia.org/wiki/Website_spoofing" target="_blank">http://en.wikipedia.org/wiki/Website_spoofing</a>).</p><p>It becomes more complex when all this information needs to be consumed and analyzed in real time. For example analyzing the network access logs at end of the day does not make sense because the hackers might have completed their job by that time.</p><p>Let's <a id="id270" class="indexterm"></a>explore one such use case for web log analysis where an online system (a website or any portal) is deployed on a couple of servers (clustered deployment) serving the user request from multiple nodes or machines and log files are also generated on each of these nodes, which would look like similar to the following illustration:</p><div class="mediaobject"><img src="graphics/B01793_03_02.jpg" /></div><p>The preceding illustration shows a typical deployment of an online and user interfacing application.</p><p>The first challenge with this kind of distributed system is that the weblogs are generated on multiple servers, though logically they are from the same application. In order to solve this challenge, we will use Spark Streaming and its integration with Flume to read the streaming log files from multiple sources and load it in our Spark Streaming application.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note31"></a>Note</h3><p>For more<a id="id271" class="indexterm"></a> information on Flume, refer to <a class="ulink" href="https://flume.apache.org/" target="_blank">https://flume.apache.org/</a>.</p></div><p>Let's discuss the architecture and core components of Flume and then we will set up Flume and write our custom Spark Streaming application and integrate it with Flume.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec31"></a>Flume architecture</h3></div></div></div><p>Flume is a reliable and highly available service for efficiently and effectively consuming streaming<a id="id272" class="indexterm"></a> data in near real-time (with negligible latency of milliseconds) from different data sources. It is highly customizable and extendable, robust and provides fault tolerance with tunable reliability mechanisms. Flume is slowly and gradually becoming a de facto standard for performing <span class="strong"><strong>ETL</strong></span> (<span class="strong"><strong>Extract, Transform, Load</strong></span>) for events or data received in real or near real-time. Let's discuss<a id="id273" class="indexterm"></a> the architecture of Flume and the components it uses for performing ETL.</p><p>Flume works in a distributed mode in which it defines following four components:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Source</strong></span>: Source <a id="id274" class="indexterm"></a>is a component which reads or extracts (<span class="strong"><strong>Extract</strong></span> of ETL Process) the data from a configurable data source. Flume defines a few standard components which can be configured to read the data from predefined data sources like databases, filesystems, sockets, and so on. Every chunk of data read by the Flume source is known as an <span class="strong"><strong>event</strong></span>, which is further delivered to a channel.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note32"></a>Note</h3><p>Refer to <a class="ulink" href="https://flume.apache.org/FlumeUserGuide.html#flume-sources" target="_blank">https://flume.apache.org/FlumeUserGuide.html#flume-sources</a> for standard Flume sources and a developer guide and <a class="ulink" href="https://flume.apache.org/FlumeDeveloperGuide.html#source" target="_blank">https://flume.apache.org/FlumeDeveloperGuide.html#source</a> for <a id="id275" class="indexterm"></a>developing custom sources.</p></div></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Channel</strong></span>: Channels are the staging area where events are stored, so that they can be <a id="id276" class="indexterm"></a>consumed further by Flume sinks. It decouples the sources from the sink so that the sink is not dependent upon the source. There could be scenarios where the source could extract the events or data faster than the events and data consumption capacity of the sink. Channels help the source and the sink to work independently at their own capacity. Channels can be reliable or non-reliable. For example, Flume defines standard channels like memory, JDBC, file, and so on, where the memory channel is unreliable while the file and JDBC are reliable channels.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note33"></a>Note</h3><p>Refer to <a class="ulink" href="https://flume.apache.org/FlumeUserGuide.html#flume-channels" target="_blank">https://flume.apache.org/FlumeUserGuide.html#flume-channels</a>, for more information on the standard channels provided by Flume.</p></div></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Interceptor</strong></span>: Interceptor is another component which modifies, enhances or transforms (<span class="strong"><strong>Transform</strong></span> of ETL process) data before it is consumed by the sink. An interceptor can modify or even drop events based on any criteria chosen by <a id="id277" class="indexterm"></a>the developer or the interceptor. Flume<a id="id278" class="indexterm"></a> supports the chaining of interceptors. This is made possible by specifying the list of interceptor builder class names in the configuration.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note34"></a>Note</h3><p>Refer to <a class="ulink" href="https://flume.apache.org/FlumeUserGuide.html#flume-interceptors" target="_blank">https://flume.apache.org/FlumeUserGuide.html#flume-interceptors</a> for<a id="id279" class="indexterm"></a> standard Flume interceptors.</p></div></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Sink</strong></span>: Sink is<a id="id280" class="indexterm"></a> another component which retrieves or loads (<span class="strong"><strong>Load</strong></span> of ETL process) data from channels and then can optionally process or transform and finally deliver it to the intended recipients or consumer or another system like Kafka or Avro or store it to a persistent storage area like files or HDFS, and so on. Similar to the source, Flume also provides standard sinks like HDFS, file, and so on.</p><p>Sink completes our pure ETL pipeline where we have the Flume source for extracting, the Flume interceptor for transformation and the Flume sink for loading events into various systems or sending data to a bus or a processing system like Spark or Storm.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note35"></a>Note</h3><p>Refer to <a class="ulink" href="https://flume.apache.org/FlumeUserGuide.html#flume-sinks" target="_blank">https://flume.apache.org/FlumeUserGuide.html#flume-sinks</a> for<a id="id281" class="indexterm"></a> standard Flume sinks and a developer guide and <a class="ulink" href="https://flume.apache.org/FlumeDeveloperGuide.html#sink" target="_blank">https://flume.apache.org/FlumeDeveloperGuide.html#sink</a> for <a id="id282" class="indexterm"></a>developing a custom sink.</p></div></li></ul></div><p>All the four Flume components (source, channel, interceptor and sink) work in conjunction and the high-level architecture is same as shown in the following illustration:</p><div class="mediaobject"><img src="graphics/B01793_03_03.jpg" /></div><p>In the <a id="id283" class="indexterm"></a>next section we will configure Flume for consuming data from web logs.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec32"></a>Installing and configuring Flume</h3></div></div></div><p>Perform <a id="id284" class="indexterm"></a>the <a id="id285" class="indexterm"></a>following steps to configure Flume:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Download<a id="id286" class="indexterm"></a> Flume from <a class="ulink" href="http://www.apache.org/dyn/closer.cgi/flume/1.6.0/apache-flume-1.6.0-bin.tar.gz" target="_blank">http://www.apache.org/dyn/closer.cgi/flume/1.6.0/apache-flume-1.6.0-bin.tar.gz</a> and extract it using the following command on the same machine or server where your web log files are generated:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>tar â€“ zxvf apache-flume-1.6.0-bin.tar.gz</strong></span>
</pre></div></li><li><p>Execute the following command to define the <code class="literal">FLUME_HOME</code> environment variable:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>export FLUME_HOME=&lt;path of extracted Flume binaries&gt;</strong></span>
</pre></div></li><li><p>Next, browse the <code class="literal">FLUME_HOME/conf</code> directory and create a new config file named <code class="literal">spark-flume.conf</code>.</p></li><li><p>Edit the <code class="literal">spark-flume.conf</code> file and add following content:</p><div class="informalexample"><pre class="programlisting">#Defining Agent-1 and the logical names of the Source/ Channel and Sink
a1.sources = src-1
a1.channels = c1
a1.sinks = spark

#Defining Agent-2 and the logical names of the Source/ Channel and Sink
a2.sources = src-2
a2.channels = c2
a2.sinks = spark1

#Configuring Source for Agent-1
#Here we are defining a source which will execute a custom Linux Command "tail" to get the Data from configured web log file
a1.sources.src-1.type = exec
#Name of the Log File with the full path
a1.sources.src-1.command = tail -f /home/servers/node-1/appserver-1/logs/debug.log
#Define the Channel which will be used by Source to deliver the messages.
a1.sources.src-1.channels = c1

#Defining and providing Configuration of Channel for Agent-1
#Memory channel is not a reliable channel.
a1.channels.c1.type = memory
a1.channels.c1.capacity = 2000
a1.channels.c1.transactionCapacity = 100

#Configuring Sink for Agent-1
a1.sinks.spark.type = spark
#This is the Custom Sink which will be used to integrate with our Spark Application
a1.sinks.spark.type = org.apache.spark.streaming.flume.sink.SparkSink
#Name of the host where this Sink is running
a1.sinks.spark.hostname = localhost
#Custom port where our Spark-Application will connect and consume the event
a1.sinks.spark.port = 4949
#Define the Channel which will be used by Sink to receive the messages.
a1.sinks.spark.channel = c1

#Configuring Source for Agent-2
#Here we are defining a source which will execute a custom Linux Command "tail" to get the Data from configured web log file
a2.sources.src-2.type = exec
#Name of the Log File with the full path
a2.sources.src-2.command = tail -f /home/servers/node-1/appserver-2/logs/debug.log
#Define the Channel which will be used by Source to deliver the messages.
a2.sources.src-2.channels = c2

#Defining and providing Configuration of Channel for Agent-2
a2.channels.c2.type = memory
a2.channels.c2.capacity = 2000
a2.channels.c2.transactionCapacity = 100

#Configuring Sink for Agent-2
a2.sinks.spark1.type = spark
#This is the Custom Sink which will be used to integrate with our Spark Application
a2.sinks.spark1.type = org.apache.spark.streaming.flume.sink.SparkSink
#Name of the host where this Sink is running
a2.sinks.spark1.hostname = localhost
#Custom port where our Spark-Application will connect and consume the event
a2.sinks.spark1.port = 4950
#Define the Channel which will be used by Sink to receive the messages.
a2.sinks.spark1.channel = c2</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note36"></a>Note</h3><p>Follow the comments provided in the configuration to understand the purpose of each property. The same style is used later in this chapter and book.</p></div><p>In <a id="id287" class="indexterm"></a>the <a id="id288" class="indexterm"></a>preceding Flume configuration we defined two agents (<code class="literal">a1</code> and <code class="literal">a2</code>) which monitor the log files from two different app servers hosted on the same machine. Each agent is configured with its own source, channel and sink. We have defined the standard source and channel (provided by Flume) but we have configured a custom sink which will help us in integration with our Spark application.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note37"></a>Note</h3><p>Exec is not a reliable source and is not recommended when we need strong event delivery semantics or guaranteed delivery. It is better to develop a custom source using Flume SDK. It is recommended to have a cleanup script with an <code class="literal">exec</code> command which runs at regular intervals to check process tables for the <code class="literal">tail -f </code>command whose parent PID is 1 and kill them manually as they are dead processes.</p></div></li><li><p>Next, let's<a id="id289" class="indexterm"></a> download the following additional libraries for<a id="id290" class="indexterm"></a> our custom sink and add them to <code class="literal">$FLUME_HOME/lib/</code>:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<a class="ulink" href="http://central.maven.org/maven2/org/apache/spark/spark-streaming-flume_2.10/1.3.0/spark-streaming-flume_2.10-1.3.0.jar" target="_blank">http://central.maven.org/maven2/org/apache/spark/spark-streaming-flume_2.10/1.3.0/spark-streaming-flume_2.10-1.3.0.jar</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://central.maven.org/maven2/org/apache/spark/spark-streaming-flume-sink_2.10/1.3.0/spark-streaming-flume-sink_2.10-1.3.0.jar" target="_blank">http://central.maven.org/maven2/org/apache/spark/spark-streaming-flume-sink_2.10/1.3.0/spark-streaming-flume-sink_2.10-1.3.0.jar</a>
</p></li></ul></div></li><li><p>Our<a id="id291" class="indexterm"></a> custom sink also has a dependency upon the Spark Core libraries, so copy <code class="literal">$SPARK_HOME/lib/spark-assembly-1.3.0-hadoop2.4.0.jar</code> to <code class="literal">$FLUME_HOME/lib/</code>.</p><p>Our Flume configuration is completed and finally we are ready to execute our Flume agents.</p></li><li><p>Browse <code class="literal">$FLUME_HOME</code> and execute the following commands for running both the agents (<code class="literal">a1</code> and <code class="literal">a2</code>):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>./bin/flume-ng agent --conf conf --conf-file conf/spark-flume.conf --name a1 &amp;</strong></span>
<span class="strong"><strong>./bin/flume-ng agent --conf conf --conf-file conf/spark-flume.conf --name a2 &amp;</strong></span>
</pre></div></li><li><p>If everything works as planned than your Flume is up and running and agents are consuming events from the app server log files and delivering to the memory channel.</p></li><li><p>If you see any exceptions, check your configuration, especially the path of log file given in <code class="literal">a1.sources.src-1.command</code> and <code class="literal">a2.sources.src-2.command</code> and ensure that it does exists and is accessible to your Flume agents.</p></li><li><p>Next let's write our custom Spark application for consuming the messages from both Flume agents and then print them on the console.</p></li></ol></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note38"></a>Note</h3><p>In the absence of real log files you can write a custom Scala/Java class with which can write logs or text in the configured log files or you can download publicly available log files from <a class="ulink" href="http://www.monitorware.com/en/logsamples/download/apache-samples.rar" target="_blank">http://www.monitorware.com/en/logsamples/download/apache-samples.rar</a>.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec33"></a>Configuring Spark to consume Flume events</h3></div></div></div><p>Perform<a id="id292" class="indexterm"></a> the following steps to configure <a id="id293" class="indexterm"></a>your Spark application to receive the events generated by the Flume agents:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Download <a id="id294" class="indexterm"></a>the following JAR files and place them in the <code class="literal">$SPARK_HOME/lib/</code> directory:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<a class="ulink" href="http://central.maven.org/maven2/org/apache/spark/spark-streaming-flume_2.10/1.3.0/spark-streaming-flume_2.10-1.3.0.jar" target="_blank">http://central.maven.org/maven2/org/apache/spark/spark-streaming-flume_2.10/1.3.0/spark-streaming-flume_2.10-1.3.0.jar</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://central.maven.org/maven2/org/apache/spark/spark-streaming-flume-sink_2.10/1.3.0/spark-streaming-flume-sink_2.10-1.3.0.jar" target="_blank">http://central.maven.org/maven2/org/apache/spark/spark-streaming-flume-sink_2.10/1.3.0/spark-streaming-flume-sink_2.10-1.3.0.jar</a>
</p></li></ul></div></li><li><p>The preceding JAR files are the same as those we downloaded and copied in our <code class="literal">$FLUME_HOME/lib/</code> directory in the previous <span class="emphasis"><em>Installing and configuring Flume</em></span> section.</p></li><li><p>Rename <code class="literal">$SPARK_HOME/conf/spark-defaults.conf.template</code> as <code class="literal">$SPARK_HOME/conf/spark-defaults.conf</code>.</p></li><li><p>Edit your <code class="literal">$SPARK_HOME/conf/spark-defaults.conf</code> file and add the following entries at the end of the file:</p><div class="informalexample"><pre class="programlisting">spark.driver.extraClassPath=$SPARK_HOME/lib/spark-streaming-flume_2.10-1.3.0.jar:$FLUME_HOME/lib/flume-ng-sdk-1.5.2.jar:$SPARK_HOME/lib/spark-streaming-flume-sink_2.10-1.3.0.jar
spark.executor.extraClassPath=$SPARK_HOME/lib/spark-streaming-flume_2.10-1.3.0.jar:$FLUME_HOME/lib/flume-ng-sdk-1.5.2.jar:$SPARK_HOME/lib/spark-streaming-flume-sink_2.10-1.3.0.jar</pre></div></li><li><p>The preceding entries ensure that all Spark dependencies for consuming events from Flume agents are in the classpath of our Spark driver and at the same time they are also available to Spark executors which will be executing our Spark application.</p></li><li><p>Next, we<a id="id295" class="indexterm"></a> will extend our<a id="id296" class="indexterm"></a> Scala project <code class="literal">Spark-Examples</code> which we created in <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Installing and Configuring Spark and Spark Streaming</em></span> and add a new package <code class="literal">chapter.three</code> and within this package define a new Scala object by the name of <code class="literal">ScalaLoadDistributedEvents.scala</code>.</p></li><li><p>Define a main method in the Scala object and import the packages, as shown in the following example:</p><div class="informalexample"><pre class="programlisting">package chapter.three
import org.apache.spark.SparkConf
import org.apache.spark.streaming._
import org.apache.spark.streaming.flume._
import org.apache.spark.storage.StorageLevel
import org.apache.spark.rdd._
import org.apache.spark.streaming.dstream._
import java.net.InetSocketAddress
import java.io.ObjectOutputStream
import java.io.ObjectOutput
import java.io.ByteArrayOutputStream

object ScalaLoadDistributedEvents {
  
   def main(args:Array[String]){
    
    println("Creating Spark Configuration")
    //Create an Object of Spark Configuration
    val conf = new SparkConf()
    //Set the logical and user defined Name of this //Application
    conf.setAppName("Streaming Data Loading Application")
    
    println("Retreiving Streaming Context from Spark Conf")
    //Retrieving Streaming Context from SparkConf Object.
    //Second parameter is the time interval at which //streaming data will be divided into batches
    val streamCtx = new StreamingContext(conf, Seconds(2))
    
    //Create an Array of InetSocketaddress containing the  //Host and the Port of the machines
    //where Flume Sink is delivering the Events
    //Basically it is the value of following properties //defined in Flume Config: -
    //1. a1.sinks.spark.hostname
    //2. a1.sinks.spark.port
    //3. a2.sinks.spark1.hostname
    //4. a2.sinks.spark1.port
    var addresses = new Array[InetSocketAddress](2);
    
    addresses(0) = new InetSocketAddress("localhost",4949)
    addresses(1) = new InetSocketAddress("localhost",4950)
    
    //Create a Flume Polling Stream which will poll the //Sink the get the events
    //from sinks every 2 seconds.
    //Last 2 parameters of this method are important as the
    //1.maxBatchSize = It is the maximum number of events //to be pulled from the Spark sink
    //in a single RPC call.
    //2.parallelism - The Number of concurrent requests //this stream should send to the sink.
    //for more information refer to
    //https://spark.apache.org/docs/1.1.0/api/java/org/apache/spark/streaming/flume/FlumeUtils.html
    val flumeStream = FlumeUtils.createPollingStream(streamCtx,addresses,StorageLevel.MEMORY_AND_DISK_SER_2,1000,1)
   
    //Define Output Stream Connected to Console for //printing the results
    val outputStream = new ObjectOutputStream(Console.out)
    //Invoking custom Print Method for writing Events to //Console
    printValues(flumeStream,streamCtx, outputStream)
    
    //Most important statement which will initiate the //Streaming Context
    streamCtx.start();
    //Wait till the execution is completed.
    streamCtx.awaitTermination();
  }</pre></div></li><li><p>Now define one more function <code class="literal">printValues(â€¦)</code> in <code class="literal">ScalaLoadDistributedEvents</code>:</p><div class="informalexample"><pre class="programlisting">/**
   * Simple Print function, for printing all elements of RDD
   */
  def printValues(stream:DStream[SparkFlumeEvent],streamCtx: StreamingContext, outputStream: ObjectOutput){
    stream.foreachRDD(foreachFunc)
    //SparkFlumeEvent is the wrapper classes containing all //the events captured by the Stream
    def foreachFunc = (rdd: RDD[SparkFlumeEvent]) =&gt; {
      val array = rdd.collect()
      println("---------Start Printing Results----------")
      println("Total size of Events= " +array.size)
      for(flumeEvent&lt;-array){
        //This is to get the AvroFlumeEvent from //SparkFlumeEvent
        //for printing the Original Data
        val payLoad = flumeEvent.event.getBody()
        //Printing the actual events captured by the Stream
        println(new String(payLoad.array()))
      }
      println("---------Finished Printing Results--------")
    } }</pre></div></li></ol></div><p>And <a id="id297" class="indexterm"></a>we are done! Our Spark Streaming <a id="id298" class="indexterm"></a>job is ready to be deployed and executed. Let's move on to the next section where we will perform the final step and deploy our Spark Streaming application.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec34"></a>Packaging and deploying a Spark Streaming job</h3></div></div></div><p>Perform<a id="id299" class="indexterm"></a> the following steps to deploy and run your <a id="id300" class="indexterm"></a>Spark Streaming job:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Export your project as a JAR file and name it <code class="literal">Spark-Examples.jar</code> and save this JAR file in the root of <code class="literal">$SPARK_HOME</code>.</p></li><li><p>Next, open your Linux console and browse to <code class="literal">$SPARK_HOME</code> and execute the following command to deploy your Spark Streaming job:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$SPARK_HOME/bin/spark-submit --class chapter.three.ScalaLoadDistributedEvents --master &lt;SPARK-MASTER-URL&gt; Spark-Examples.jar</strong></span>
</pre></div></li><li><p>As <a id="id301" class="indexterm"></a>soon as you click on <span class="emphasis"><em>Enter</em></span> and execute<a id="id302" class="indexterm"></a> the preceding command you should see that all your web logs are being consumed and printed on the console.</p></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec35"></a>Overall architecture of distributed log file processing</h3></div></div></div><p>To<a id="id303" class="indexterm"></a> summarize the overall use case, here is what we have developed and deployed:</p><div class="mediaobject"><img src="graphics/B01793_03_04.jpg" /></div><p>The preceding illustration defines the various components and the overall architecture of our distributed log processing use case. We will continue using the same architecture in subsequent chapters and will exploit and leverage various other features of Spark Streaming.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note39"></a>Note</h3><p>Refer<a id="id304" class="indexterm"></a> to <a class="ulink" href="https://spark.apache.org/docs/1.2.0/streaming-flume-integration.html" target="_blank">https://spark.apache.org/docs/1.2.0/streaming-flume-integration.html</a> for more information on Spark and Flume integration.</p></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec22"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we have discussed in detail the packages and core components and classes of Spark and its extensions. We have also discussed resilient distributed datasets and discretized streams and finally integrated and configured Spark Streaming with Flume and executed our distributed log file processing use case.</p><p>In the next chapter, we will discuss and apply various transformation functions over our streaming data and discuss the performance-tuning aspects of our Spark Streaming application/cluster.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch04"></a>ChapterÂ 4.Â Applying Transformations to Streaming Data</h2></div></div></div><p>Data is of no use if it cannot be transformed and some meaningful analysis is derived from the overall process!</p><p>Data analysis is <a id="id305" class="indexterm"></a>a multi-step process which includes inspecting, transforming and finally modeling, with the important goal of discovering useful information which is further considered and applied in arriving at critical business decisions.</p><p>In simple terms, transformation is a process where a series of rules or functions is applied to extracted data, so that it can be loaded to the end target for further analysis.</p><p>An important activity in transformation is data cleansing, which aims to process and prepare only proper and relevant data, which can be analyzed and interpreted by the system.</p><p>Transformation is very subjective and, depending on the end goal, can perform a variety of functions. Aggregations (<code class="literal">sum</code>, <code class="literal">avg</code>, <code class="literal">min</code>, and <code class="literal">max</code>), sorting, joining data from multiple sources, disaggregations, deriving new values, and so on are some examples of the common functions involved in the transformation process.</p><p>In this chapter we will talk about the different transformation functions in Spark Streaming and the process of applying these transformation functions on the distributed streaming data loaded from data sources.</p><p>This chapter will cover the following points:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Understanding and applying transformation functions</p></li><li style="list-style-type: disc"><p>Performance tuning</p></li></ul></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec23"></a>Understanding and applying transformation functions</h2></div></div><hr /></div><p>Spark<a id="id306" class="indexterm"></a> Streaming provides various high-level transformation <a id="id307" class="indexterm"></a>functions which can be used to perform operations such as simple aggregations, counts but also including complex windowing functions over streaming data.</p><p>Let's reuse the code snippets written in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Processing Distributed Log Files in Real Time</em></span>, with a different set of log data which will help us to better understand the way these functions are applied to the streaming data.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec36"></a>Simulating log streaming</h3></div></div></div><p>Perform<a id="id308" class="indexterm"></a> the following steps for generating Apache access logs and simulate your environment to provide the logs in real time:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Download<a id="id309" class="indexterm"></a> sample Apache access logs from the following location <a class="ulink" href="http://www.monitorware.com/en/logsamples/download/apache-samples.rar" target="_blank">http://www.monitorware.com/en/logsamples/download/apache-samples.rar</a>.</p></li><li><p>Extract the downloaded <code class="literal">apache_samples.rar</code> and its subarchives into a folder. Let's refer to this folder as <code class="literal">$LOG_FOLDER</code>.</p></li><li><p>Next, we will write the program which will read the Apache <code class="literal">access_log</code> at certain intervals and will extract each row and put the log data into the log files monitored by our Flume agents.</p></li><li><p>Create a package called <code class="literal">chapter.four</code> and a Java class <code class="literal">SampleLogGenerator.java</code> and add the following code to it:</p><div class="informalexample"><pre class="programlisting">package chapter.four;

import java.io.BufferedReader;
import java.io.File;
import java.io.FileOutputStream;
import java.io.FileReader;

public class SampleLogGenerator {

  public static void main(String[] args) {
    try {

      if (args.length != 2) {
        System.out.println("Usage - java SampleLogGenerator &lt;Location of Log File to be read&gt; &lt;location of the log file in which logs needs to be updated&gt;");
        System.exit(0);
      }
      String location = args[0];

      File f = new File(location);
      FileOutputStream writer = new FileOutputStream(f);

      File read = new File(args[1]);
      BufferedReader reader = new BufferedReader(new FileReader(read));

      for (;;) {

        writer.write((reader.readLine()+"\n").getBytes());
        writer.flush();
        Thread.sleep(500);
      }

    } catch (Exception e) {
      e.printStackTrace();
    }

  }

}</pre></div></li><li><p>Next, edit <code class="literal">$FLUME_HOME/conf/spark-flume.conf</code> and change the location of <a id="id310" class="indexterm"></a>property of <code class="literal">a1.sources.src-1.command</code> to <code class="literal">/home/servers/node-1/appserver-1/logs/access.log</code>.</p></li><li><p>Next, we will compile the preceding Java program and execute it by providing the following two runtime parameters:</p><div class="informalexample"><pre class="programlisting">java chapter.four.SampleLogGenerator  $LOG_FOLDER/access_log/access_log /home/servers/node-1/appserver-1/logs/access.log</pre></div></li><li><p>Now you will see that the logs are being generated in <code class="literal">/home/servers/node-1/appserver-1/logs/access.log</code> which simulates the real environment where logs are being generated for every request.</p></li><li><p>Next, kill all your Flume agents if you have any running (by executing <code class="literal">kill -9 &lt;PID&gt;</code>) and browse <code class="literal">$FLUME_HOME</code> and execute the following command to bring up only one Flume agent for consuming the data and delivering it to the Spark sink:</p><div class="informalexample"><pre class="programlisting">./bin/flume-ng agent --conf conf --conf-file conf/spark-flume.conf --name a1</pre></div></li></ol></div><p>We are <a id="id311" class="indexterm"></a>done with our setup. Now the logs are being generated in real time and we need to consume them and perform further analysis.</p><p>Let us move to the next section where we will discuss the transformation operations provided by Spark Streaming.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec37"></a>Functional operations</h3></div></div></div><p>Discretized <a id="id312" class="indexterm"></a>streams or DStream are nothing but a series of RDD which are represented by <code class="literal">org.apache.spark.streaming.dstream.DStream.scala</code> and <code class="literal">org.apache.spark.streaming.dstream.PairDStreamFunctions.scala</code>. It defines various higher-order functions like map and reduce that accepts and apply a given function to each element of the RDD and produces a new RDD. As per Wikipedia, higher-order functions are those functions that either accept a function as input or output a function. It is a common concept, defined in functional programming languages like Clojure, Lisp, Erlang, Haskell and now in Java-8 too.</p><p>The <a id="id313" class="indexterm"></a>following are the different higher-order functions<a id="id314" class="indexterm"></a> and operations provided by DStream:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">flatMap(flatMapFunc)</code>: DStream[U]â€”Similar to <code class="literal">map(â€¦)</code> but, before returning, it flattens the results and then returns the final result set.</p></li><li style="list-style-type: disc"><p>
<code class="literal">forEachRDD(forEachFunc)</code>: Applies the given function to all RDDs in a given stream. It is a special type of function and it is worth noting that the given function is applied to all RDDs on the driver node itself but there could be actions defined in the RDD and all those actions are performed over the cluster. <code class="literal">forEach</code> is categorized as an output operator and, by default, output operations are executed one-at-a-time, sequentially in the order they are defined in the application.</p></li><li style="list-style-type: disc"><p>
<code class="literal">filter(filterFunc)</code>: DStream[T]â€”Applies the provided function to all the <a id="id315" class="indexterm"></a>elements of RDD and generates the RDD <a id="id316" class="indexterm"></a>only for those elements which return <code class="literal">TRUE</code>.</p></li><li style="list-style-type: disc"><p>
<code class="literal">map(mapFunc)</code>: DStream[U]â€”Applies a given function <code class="literal">mapFunc</code> to all elements of RDD and generates a new RDD.</p></li><li style="list-style-type: disc"><p>
<code class="literal">mapPartitions(mapPartFunc, preservePartitioning)</code>: Return a new DStream in <a id="id317" class="indexterm"></a>which each RDD is generated by applying <code class="literal">mapPartitions()</code> to each RDD in the invoking DStream. Applying <code class="literal">mapPartitions()</code> to an RDD applies the given function to each partition of the RDD.</p></li></ul></div><p>Let's continue our log streaming example and see a few of the preceding functions in action.</p><p>Perform the following steps and create a custom Spark program for consumption and analysis of logs generated by our Flume agents:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>We will extend our <code class="literal">Spark-Examples</code> project and create a new Scala class <code class="literal">ScalaLogAnalyzer.scala</code> in the package, <code class="literal">chapter.four</code>.</p></li><li><p>Edit <code class="literal">ScalaLogAnalyzer.scala</code> and add the following piece of code:</p><div class="informalexample"><pre class="programlisting">package chapter.four

import java.util.regex.Pattern
import java.util.regex.Matcher

class ScalaLogAnalyzer extends Serializable{
  
  /**
   * Transform the Apache log files and convert them into a Map of Key/Value pair
   */
  def tansfromLogData(logLine: String):Map[String,String] ={
    //Pattern which will extract the relevant data from Apache Access Log Files
     val LOG_ENTRY_PATTERN = """^(\S+) (\S+) (\S+) \[([\w:/]+\s[+\-]\d{4})\] "(\S+) (\S+) (\S+)" (\d{3}) (\S+)""";
     val PATTERN = Pattern.compile(LOG_ENTRY_PATTERN);
     val matcher = PATTERN.matcher(logLine);
     
    //Matching the pattern for the each line of the Apache access Log file 
    if (!matcher.find()) {
      System.out.println("Cannot parse logline" + logLine);
    }
    //Finally create a Key/Value pair of extracted data and return to calling program
    createDataMap(matcher);
    
  }

  /**
   * Create a Map of the data which is extracted by applying Regular expression.
   */
  def createDataMap(m:Matcher):Map[String,String] = {
    return Map[String, String](
      ("IP" -&gt; m.group(1)),
      ("client" -&gt; m.group(2)),
      ("user" -&gt; m.group(3)),
      ("date" -&gt; m.group(4)),
      ("method" -&gt; m.group(5)),
      ("request" -&gt; m.group(6)),
      ("protocol" -&gt; m.group(7)),
      ("respCode" -&gt; m.group(8)),
      ("size" -&gt; m.group(9))
  )}</pre></div><p>The <a id="id318" class="indexterm"></a>preceding class applies the regular expression (pattern) and transforms a given line of Apache access logs into a map of key/value pairs. This is required so that we can capture each distinct section of the Apache access log statements and then perform further analysis.</p></li><li><p>Next, we will define another Scala object in the package <code class="literal">chapter.four</code> and name it <code class="literal">ScalaTransformLogEvents.scala</code>, which will consume the log events and apply the different functions to perform analysis.</p></li><li><p>Edit <code class="literal">ScalaTransformLogEvents.scala</code> and add the following piece of code which consumes the logs and converts them into key/value pairs:</p><div class="informalexample"><pre class="programlisting">package chapter.four

import org.apache.spark.SparkConf
import org.apache.spark.streaming._
import org.apache.spark.streaming.flume._
import org.apache.spark.storage.StorageLevel
import org.apache.spark.rdd._
import org.apache.spark.streaming.dstream._
import java.net.InetSocketAddress
import java.io.ObjectOutputStream
import java.io.ObjectOutput
import java.io.ByteArrayOutputStream

object ScalaTransformLogEvents {
  

  def main(args:Array[String]){
     
    /** Start Common piece of code for all kinds of Transform operations*/    
    println("Creating Spark Configuration")
    val conf = new SparkConf()
    conf.setAppName("Apache Log Transformer")
    println("Retreiving Streaming Context from Spark Conf")
    val streamCtx = new StreamingContext(conf, Seconds(10))
    
    var addresses = new Array[InetSocketAddress](1);
    addresses(0) = new InetSocketAddress("localhost",4949)
    
    val flumeStream = FlumeUtils.createPollingStream(streamCtx,addresses,StorageLevel.MEMORY_AND_DISK_SER_2,1000,1)
        
    //Utility class for Transforming Log Data
    val transformLog = new ScalaLogAnalyzer()
    //Invoking Flatmap operation to flatening the results and convert them into Key/Value pairs
    val newDstream = flumeStream.flatMap { x =&gt; transformLog.tansfromLogData(new String(x.event.getBody().array())) }
   
    /** End Common piece of code for all kinds of Transform operations*/
    
    /**Start - Transformation Functions */
    executeTransformations(newDstream,streamCtx)  
    /**End - Transformation Functions */
    
    streamCtx.start()
    streamCtx.awaitTermination()  
  }

   /**
    * Define and execute all Transformations to the log data
    */
  def executeTransformations(dStream:DStream[(String,String)],streamCtx: StreamingContext){ }
}</pre></div></li></ol></div><p>Most<a id="id319" class="indexterm"></a> of the preceding piece of code is familiar to us as we have written and executed the same piece of the code in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Processing Distributed Log Files in Real Time</em></span>, except for the statement <code class="literal">flumeStream.flatMapâ€¦..</code>.</p><p>This statement is actually flattening the results and leverages our <code class="literal">Utility</code> class <code class="literal">ScalaLogAnalyzer</code> for converting each line of log data received into a <code class="literal">Map</code> of key/value pairs, where keys are static and values are extracted from the log files by applying the pattern. Finally, the <code class="literal">Map</code> containing the key/value pair is wrapped into the RDD for further analysis. The values in <code class="literal">Map</code> should look similar to the following illustration:</p><div class="informalexample"><pre class="programlisting">("method", "GET"),
("request","/twiki/bin/edit/Main/Double_bounce_sender?topicparent=Main.ConfigurationVariables"),
("size","12846"),
("date","07/Mar/2004:16:05:49-0800"),
("IP","64.242.88.10")
â€¦â€¦â€¦</pre></div><p>Next we have defined a new function <code class="literal">executeTransformations(â€¦)</code> which accepts the flattened RDD and will define all transformation operations.</p><p>Going forward, we will only define the structure of <code class="literal">executeTransformations()</code> and leave everything else the same.</p><p>Now let's enhance our <code class="literal">executeTransformations()</code> method and add some transformation functions to solve some of the real-world problem statements. We will do this in the form of questions and answers, where we will take up different scenarios and then provide the <a id="id320" class="indexterm"></a>structure of our function to solve the given problem statement.</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>For <code class="literal">EachFunction</code>: Let's focus on the following problem scenario to understand <a id="id321" class="indexterm"></a>the implementation of <code class="literal">forEachFunction</code>:</p><p>
<span class="strong"><strong>Scenario</strong></span>: How do you print all the values of the transformed data?</p><p>
<span class="strong"><strong>Solution</strong></span>: Let's define one more function <code class="literal">printLogValue(â€¦)</code>, which will use the <code class="literal">forEachRDD</code> function of <code class="literal">DStream.Scala</code> for printing all key/values of the transformed log data on the console.</p><div class="informalexample"><pre class="programlisting">  def printLogValues(stream:DStream[(String,String)],streamCtx: StreamingContext){
    //Implementing ForEach function for printing all the data in provided DStream
    stream.foreachRDD(foreachFunc)
    //Define the forEachFunction and also print the values on Console
    def foreachFunc = (rdd: RDD[(String,String)]) =&gt; {
   
      //collect() method fetches the data from all partitions and "collects" at driver node. 
      //So in case data is too huge than driver may crash. 
      //In production environments we persist this RDD data into HDFS or use the rdd.take(n) method.

      val array = rdd.collect()
      println("---------Start Printing Results----------")
      for(dataMap&lt;-array.array){
        print(dataMap._1,"-----",dataMap._2)
      }
      println("---------Finished Printing Results----------")
    }
  }</pre></div><p>In the preceding implementation, we have used <code class="literal">forEachRDD</code> to unwrap all the values given in the stream of RDDs and then print the same on<a id="id322" class="indexterm"></a> the console.</p><p>We can then invoke this method from our <code class="literal">executeTransformation(â€¦)</code> method which should look something like this:</p><div class="informalexample"><pre class="programlisting">   /**
    * Define and execute all Transformations to the log data
    */
  def executeTransformations(dStream:DStream[(String,String)],streamCtx: StreamingContext){
    //Start - Print all attributes of the Apache Access Log
    printLogValues(dStream,streamCtx)
    //End - Print all attributes of the Apache Access Log
}</pre></div></li><li style="list-style-type: disc"><p>
<code class="literal">filter(filterFunc)</code>
</p><p>
<span class="strong"><strong>Scenario</strong></span>: How do you count the total number <code class="literal">GET</code> requests?</p><p>
<span class="strong"><strong>Solution</strong></span>: This is simple if<a id="id323" class="indexterm"></a> we only have to get the requests whose type is <code class="literal">GET</code> and then <a id="id324" class="indexterm"></a>count them. This can be done by adding the following piece of code to our <code class="literal">executeTransformation</code> method:</p><div class="informalexample"><pre class="programlisting">dStream.filter(x=&gt; x._1.equals("method") &amp;&amp; x._2.contains("GET")).count().print()</pre></div><p>The preceding statement filters and retrieves only those elements in the RDD where the value of an element is <code class="literal">GET</code> and then further applies the <code class="literal">count()</code> function and finally prints the total count in the DStream.</p></li><li style="list-style-type: disc"><p>
<code class="literal">map(mapFunc)</code>
</p><p>
<span class="strong"><strong>Scenario</strong></span>: How<a id="id325" class="indexterm"></a> do you count the total number of distinct requests for requested URLs?</p><p>
<span class="strong"><strong>Solution</strong></span>: The solution is again simple. We will filter the DStream and retrieve all the keys containing the "request" as a key and then define a map function to build a new key/value pair of URLs, where key will be<a id="id326" class="indexterm"></a> the actual URL and the value will be the count. Finally, we will use the <code class="literal">reduceByKey</code> operation to count all the distinct URLs and print the same on the console.</p><div class="informalexample"><pre class="programlisting">val newStream = dStream.filter(x=&gt;x._1.contains("request")).map(x=&gt;(x._2,1))
newStream.reduceByKey(_+_).print(100)</pre></div></li></ul></div><p>Finally<a id="id327" class="indexterm"></a> to execute the preceding program, perform the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Export your project as a JAR file and name it as <code class="literal">Spark-Examples.jar</code> and save this JAR file in the root of <code class="literal">$SPARK_HOME</code>.</p></li><li><p>Next, open your Linux console and browse to <code class="literal">$SPARK_HOME</code> and execute the following command to deploy your Spark Streaming job:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$SPARK_HOME/bin/spark-submit --class chapter.four.ScalaTransformLogEvents --master &lt;SPARK-MASTER-URL&gt; Spark-Examples.jar</strong></span>
</pre></div></li></ol></div><p>As soon as you click on <span class="emphasis"><em>Enter</em></span> and execute the preceding command you should see that all your transformations are working and the results are printed on the console.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note40"></a>Note</h3><p>Refer <a id="id328" class="indexterm"></a>to <a class="ulink" href="https://en.wikipedia.org/wiki/Higher-order_function" target="_blank">https://en.wikipedia.org/wiki/Higher-order_function</a> to know more about high-order functions.</p></div><p>In this section, we have discussed various higher-order and functional operations and their usage in Spark Streaming. Let's move on and discuss transform and windowing operations on streams.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec38"></a>Transform operations</h3></div></div></div><p>Transform <a id="id329" class="indexterm"></a>operations, <code class="literal">def transform(â€¦.)</code>, are special types of operation which can be applied to any RDD in DStreams and perform any RDD-to-RDD operations. This method is also used to merge the two Spark worlds, the batch and the streaming one. You can create RDDs using batch processes and merge with RDDs created using Spark Streaming. It even helps in code reusability across Spark batch and streaming, where you may have written functions in your Spark batch applications which we now want to use in our Spark Streaming application.</p><p>For example, let's assume that in order to compute the same log data using a Spark batch application you have written a function similar to the following one:</p><div class="informalexample"><pre class="programlisting">val functionCountRequestType = (rdd:RDD[(String,String)]) =&gt; {
    rdd.filter(f=&gt;f._1.contains("method"))
.map(x=&gt;(x._2,1))
.reduceByKey(_+_)
}</pre></div><p>The preceding<a id="id330" class="indexterm"></a> function takes up an RDD and computes the count of requests (<code class="literal">GET</code> or <code class="literal">POST</code>) being served by the server.</p><p>Now we will add the preceding function to our <code class="literal">ScalaTransformLogEvents</code> class and, in the <code class="literal">main</code> method, we will write something like this:</p><div class="informalexample"><pre class="programlisting">val transformedRDD = dStream.transform(functionCountRequestType)</pre></div><p>Next, we will invoke the <code class="literal">updateStateByKey</code> operation to keep a running count of the requests for all the time. <code class="literal">updateStateByKey</code> creates and returns a new "state" of DStream and the keys defined in the DStream.</p><p>The new state of each key is generated by applying the given function on the previous state of the key and the new values of each key. It is also important to enable the "checkpoint" while using <code class="literal">updateStateByKey</code> so that our streaming application is resilient to failures unrelated to the application logic and the state of the keys at different time intervals are not lost.</p><p>Let's move forward and define one more function which defines how the state of a key should be updated, whether it should be adding, deriving average or anything else:</p><div class="informalexample"><pre class="programlisting">val functionTotalCount = (values: Seq[Int], state: Option[Int])=&gt;{
    Option(values.sum + state.sum)    
}</pre></div><p>In the preceding function, the new state is derived by adding the new and the old state of the key.</p><p>Finally, we will enable "checkpoint", invoke <code class="literal">updateStateByKey</code>, and print the values either using <code class="literal">forEach</code> or the utility function, <code class="literal">print()</code>:</p><div class="informalexample"><pre class="programlisting">streamCtx.checkpoint("checkpointDir")
transformedRDD.updateStateByKey(functionTotalCount).print(100)</pre></div><p>In this section we have discussed the <code class="literal">transform</code> functions which encourage reusability of the code where Spark Streaming jobs can use the functions defined in Spark batch jobs.</p><p>Let's <a id="id331" class="indexterm"></a>move on to the next section and talk about windowing and sliding window operations.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec39"></a>Windowing operations</h3></div></div></div><p>In real-time <a id="id332" class="indexterm"></a>streaming, it is important to define the scope and size of data which needs to be analyzed and queried.</p><p>Micro-batching provides <a id="id333" class="indexterm"></a>some flexibility where we can accumulate events and then start our computing jobs but in micro-batching all batches of data are independent and only contain new events as they appear or are received by the streams, they do not add new events to the older events without changing the batch size.</p><p>
<span class="strong"><strong>Windowing functions</strong></span> provide exactly the same functionality where they define the scope of the data which needs to be analyzed, as in the last 10 minutes, 20 minutes, and so on, and further slide this window by a configured interval like one or two minutes.</p><div class="mediaobject"><img src="graphics/B01793_04-01.jpg" /></div><p>Let's consider the preceding illustration where the interval of the window is two seconds and the sliding window is one second, which means that, at the end of every one second interval, our DStream will contain the data for last two seconds.</p><p>DStreams provide a variety of windowed operations. Let's see a few of the windowed operations provided by DStream.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note41"></a>Note</h3><p>For further <a id="id334" class="indexterm"></a>details, you can refer to DStream API <a class="ulink" href="https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.streaming.dstream.DStream" target="_blank">https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.streaming.dstream.DStream</a> where all operations which end in "window" provide windowed computations.</p></div><p>Let's extend our <code class="literal">ScalaTransformLogEvents</code> example and perform the following steps to implement a few of the windowing functions:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Open and edit <code class="literal">ScalaTransformLogEvents</code> and the following new method, which defines<a id="id335" class="indexterm"></a> the windowing operations:</p><div class="informalexample"><pre class="programlisting">    /**
    * Window and Sliding Windows
    */
   
   def executeWindowingOperations(dStream:DStream[(String,String)],streamCtx: StreamingContext){
     
     //This Provide the Aggregated Count of all response Codes
     println("Printing count of Response Code using windowing Operation")
     val wStream = dStream.window(Seconds(40),Seconds(20))
     val respCodeStream = wStream.filter(x=&gt;x._1.contains("respCode")).map(x=&gt;(x._2,1))
     respCodeStream.reduceByKey(_+_).print(100)
     
     //This provide the Aggregated count of all response Codes by using WIndow operation in Reduce method
     println("Printing count of Response Code using reducebyKeyAndWindow Operation")
     val respCodeStream_1 = dStream.filter(x=&gt;x._1.contains("respCode")).map(x=&gt;(x._2,1))
     respCodeStream_1.reduceByKeyAndWindow((x:Int,y:Int)=&gt;x+y,Seconds(40),Seconds(20)).print(100)

     //This will apply and print groupByKeyAndWindow in the Sliding Window
     println("Applying and Printing groupByKeyAndWindow in a Sliding Window")
     val respCodeStream_2 = dStream.filter(x=&gt;x._1.contains("respCode")).map(x=&gt;(x._2,1))
     respCodeStream_2.groupByKeyAndWindow(Seconds(40),Seconds(20)).print(100)
     
   }</pre></div><p>In the preceding function we have defined three variations of the windowing operations:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">window(WindowDuration, SlideDuration</code>: This operation works directly on the stream and provides all elements of DStream within the given duration.</p></li><li style="list-style-type: disc"><p>
<code class="literal">reduceByKeyAndWindow(reduceFunc, WindowDuration, SlideDuration</code>: This operation works with the <span class="strong"><strong>reduce</strong></span> function where it<a id="id336" class="indexterm"></a> applies the <span class="strong"><strong>reduce</strong></span> function only on the RDDs in the sliding window.</p></li><li style="list-style-type: disc"><p>
<code class="literal">groupByKeyAndWindow(WindowDuration, SlideDuration)</code>: Applies the <code class="literal">groupBy</code> operation on the available keys within the <a id="id337" class="indexterm"></a>duration as specified by <code class="literal">SlidingDuration</code>.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip05"></a>Tip</h3><p>It is important to note that the duration for windows and sliding windows should be multiples of batching intervals defined for DStream. In our case we have defined 10 seconds, so all our window operations should contain multiples of 10.</p></div></li></ul></div></li><li><p>Next, at the end of the <code class="literal">executeTransformation</code> function, we will invoke this new method and write the following mentioned piece of code:</p><div class="informalexample"><pre class="programlisting">//Start - Windowing Operation
executeWindowingOperations(dStream,streamCtx)
//End - Windowing Operation</pre></div></li></ol></div><p>And we are done!</p><p>In order to execute the preceding code, perform the same steps as in the previous section <span class="emphasis"><em>Functional operations</em></span>.</p><p>In this section, we discussed a few of the windows operations exposed by Spark Streaming. Let's move on to the next section where we will talk about the various configurations and parameters available for tuning our Spark application.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec24"></a>Performance tuning</h2></div></div><hr /></div><p>Spark <a id="id338" class="indexterm"></a>provides various configuration parameters which if used efficiently can significantly improve the overall performance of your Spark Streaming job. Let's look at a few of the features which can help us in tuning our Spark jobs.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec40"></a>Partitioning and parallelism</h3></div></div></div><p>Spark<a id="id339" class="indexterm"></a> Streaming jobs collect and buffer data at regular intervals (batch intervals) which is further divided into various stages of execution to<a id="id340" class="indexterm"></a> form the execution pipeline. Each byte in <a id="id341" class="indexterm"></a>the dataset is represented by RDD and the execution pipeline is called a <span class="strong"><strong>Direct Acyclic Graph</strong></span> (<span class="strong"><strong>DAG</strong></span>).</p><p>The dataset involved in each stage of the execution pipeline is further stored in the data blocks of equal sizes which is nothing more than the partitions represented by the RDD.</p><p>Lastly, for each partition, we have exactly one task allocated or executed.</p><p>So the parallelism of your job directly depends on the number of partitions configured for your jobs, which can be controlled by defining <code class="literal">spark.default.parallelism</code> in <code class="literal">$SPARK_HOME/conf/spark-defaults.conf</code>. It needs to be configured correctly so that you get a sufficient amount of parallelism for your Spark jobs. The general rule is to configure parallelism by at least twice the number of total cores in the cluster but that is a bare minimum value which gives us a starting point and it may vary for different workloads.</p><p>Unless specified by the RDDs, Spark by default uses <code class="literal">org.apache.spark.HashPartitioner</code> and the default value for the maximum number of partitions will be the same as the number of partitions in the largest upstream RDD.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note42"></a>Note</h3><p>Refer to <a class="ulink" href="http://www.bigsynapse.com/spark-input-output" target="_blank">http://www.bigsynapse.com/spark-input-output</a> for more<a id="id342" class="indexterm"></a> details on understanding partitioning and parallelism.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec41"></a>Serialization</h3></div></div></div><p>Serialization is <a id="id343" class="indexterm"></a>another area of focus for performance tuning Spark jobs. Serialization<a id="id344" class="indexterm"></a> and deserialization are required for not only shuffling data between worker nodes but also when serializing RDDs to disk. By default, Spark utilizes the Java serialization mechanism which is compatible with most file formats but is also slow.</p><p>We can switch to Kryo Serialization (<a class="ulink" href="https://github.com/EsotericSoftware/kryo" target="_blank">https://github.com/EsotericSoftware/kryo</a>), which is very compact and faster than Java <a id="id345" class="indexterm"></a>serialization. Though it does not support all serializable types it is still much faster than the Java serialization mechanism for all the compatible file formats. We can configure our jobs to use <code class="literal">KyroSerializer</code> by configuring <code class="literal">spark.seralizer</code> in our <code class="literal">SparkConf</code> object.</p><div class="informalexample"><pre class="programlisting">conf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")</pre></div><p>
<code class="literal">KyroSerializer</code> by <a id="id346" class="indexterm"></a>default stores the full class names with their associated objects in Spark executors memory which again is a waste of memory so, to optimize, it is advisable to register all required classes in advance with <code class="literal">KyroSerializer</code> so that all objects are mapped to Class IDs and not with full class names. This can be done by defining explicit registrations of all required classes using <code class="literal">SparkConf.registerKryoClasses(â€¦.)</code>.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note43"></a>Note</h3><p>Refer to<a id="id347" class="indexterm"></a> the Kyro documentation (<a class="ulink" href="https://github.com/EsotericSoftware/kryo" target="_blank">https://github.com/EsotericSoftware/kryo</a>) for more optimization parameters and compatible file formats.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec42"></a>Spark memory tuning</h3></div></div></div><p>Spark is a<a id="id348" class="indexterm"></a> JVM-based execution framework, so<a id="id349" class="indexterm"></a> tuning the JVMs for the right kind of workloads can also significantly improve the overall response time of our Spark Streaming jobs. To being with, there are a couple of areas where we should focus.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl3sec25"></a>Garbage collection</h4></div></div></div><p>As a first <a id="id350" class="indexterm"></a>step, we need uncover the current GC behavior and statistics and, for that, we should configure following parameters in <code class="literal">$SPARK_HOME/conf/spark-defaults.conf</code>:</p><div class="informalexample"><pre class="programlisting">spark.executor.extraJavaOptions = -XX:+PrintFlagsFinal -XX:+PrintReferenceGC -Xloggc:$JAVA_HOME/jvm.log -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintAdaptiveSizePolicy</pre></div><p>Now your GC details are printed and available in <code class="literal">$JAVA_HOME/jvm.log</code>. We can further analyze the behavior of our JVM and then apply optimization techniques.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note44"></a>Note</h3><p>Refer to <a class="ulink" href="https://databricks.com/blog/2015/05/28/tuning-java-garbage-collection-for-spark-applications.html" target="_blank">https://databricks.com/blog/2015/05/28/tuning-java-garbage-collection-for-spark-applications.html</a> for more <a id="id351" class="indexterm"></a>details on various optimization techniques and tuning GC for Spark applications.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl3sec26"></a>Object sizes</h4></div></div></div><p>Optimizing<a id="id352" class="indexterm"></a> the size of the objects which are stored in the memory can also improve the overall performance of your application. Here are a few tips which can help us to improve the memory consumed by our objects:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Avoid using wrapper objects or pointer bases, data structures, or nested data structures with a lot of small objects.</p></li><li><p>Use arrays of objects or primitive types in your data structures. You can also use <a id="id353" class="indexterm"></a>the <code class="literal">fastutil</code> (<a class="ulink" href="http://fastutil.di.unimi.it/" target="_blank">http://fastutil.di.unimi.it/</a>) library which provides faster and optimized collection classes.</p></li><li><p>Avoid strings or custom objects; instead use numeric IDs for the objects.</p></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl3sec27"></a>Executor memory and caching RDDs</h4></div></div></div><p>Another <a id="id354" class="indexterm"></a>option is to configure the memory <a id="id355" class="indexterm"></a>of the Spark executors and also decide the appropriate size given in the memory to our Spark jobs for caching the RDDs.</p><p>Executor memory can be configured by defining the <code class="literal">spark.executor.memory</code> property in the <code class="literal">SparkConf</code> object or <code class="literal">$SPARK_HOME/conf/spark-defaults.conf</code> or we can also define it while submitting our jobs:</p><div class="informalexample"><pre class="programlisting">val conf = new SparkConf().set("spark.executor.memory", "1g")</pre></div><p>Or:</p><div class="informalexample"><pre class="programlisting">$SPARK_HOME/bin/spark-submit --executor-memory 1g â€¦..</pre></div><p>The Spark framework, by default, takes up 60 percent of the executor's configured memory for caching RDDs, which leaves only 40 percent as available memory for the execution of your Spark jobs. This may not be sufficient and if you see full GCs or slowness in your tasks or may be experiencing out of memory, then you can reduce the cache size by configuring <code class="literal">spark.storage.memoryFraction</code> in your <code class="literal">SparkConf</code> object:</p><div class="informalexample"><pre class="programlisting">val conf = new SparkConf().set("spark.storage.memoryFraction","0.4")</pre></div><p>The preceding statement reduces the memory allocated for caching RDDs to 40 percent.</p><p>Finally, we<a id="id356" class="indexterm"></a> can also think about using off-heap caching solutions which do not use a JVM like Tachyon (<a class="ulink" href="http://tachyon-project.org/Running-Spark-on-Tachyon.html" target="_blank">http://tachyon-project.org/Running-Spark-on-Tachyon.html</a>).</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note45"></a>Note</h3><p>For more<a id="id357" class="indexterm"></a> details on performance features, refer to <a class="ulink" href="https://spark.apache.org/docs/1.3.0/tuning.html" target="_blank">https://spark.apache.org/docs/1.3.0/tuning.html</a> and <a class="ulink" href="https://spark.apache.org/docs/1.3.0/configuration.html" target="_blank">https://spark.apache.org/docs/1.3.0/configuration.html</a> for various <a id="id358" class="indexterm"></a>available configuration parameters.</p></div><p>In this<a id="id359" class="indexterm"></a> section we have talked about various<a id="id360" class="indexterm"></a> aspects of tuning our Spark jobs but no matter how much we discuss it, performance is always tricky and there will always be new discoveries which may require expert advice. So, for all that expert advice, post <a id="id361" class="indexterm"></a>your queries to the Spark Community (<a class="ulink" href="https://spark.apache.org/community.html" target="_blank">https://spark.apache.org/community.html</a>).</p></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec25"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we have discussed in detail the various available transformations, starting from aggregations and also including advance functions like windowing and sliding windows. We also discussed various aspects of Spark Streaming which should be considered while performance tuning our Spark Streaming jobs.</p><p>In the next chapter, we will discuss persisting log analysis data and integration with various other NoSQL databases.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch05"></a>ChapterÂ 5.Â Persisting Log Analysis Data</h2></div></div></div><p>Systems like Hadoop and Spark have reduced the overall cost of solutions and infrastructure. They have revolutionized the industry with low cost but robust and stable frameworks which are scalable/extendable/customizable and can process a variety of data formats.</p><p>There <a id="id362" class="indexterm"></a>are a bunch of enterprise products for performing <span class="strong"><strong>ETL</strong></span> (<span class="strong"><strong>Extraction</strong></span>, <span class="strong"><strong>Transformation</strong></span>, and <span class="strong"><strong>Loading</strong></span>) operations but, when we have to deal with the huge amount of data or varied data sources or produce reports in an hour, these tools are not useful. As a result, architects and developers have been evaluating and implementing Hadoop and Spark-like systems just for ETL use cases where the source or raw data is processed or transformed and finally stored in external systems like Oracle or Teradata or simply generate the reports to dump on the filesystem.</p><p>The benefit<a id="id363" class="indexterm"></a> of using Hadoop or Spark as an ETL framework is that it<a id="id364" class="indexterm"></a> provides a low cost solution where all kinds of data processing is performed on low cost and commodity hardware and eventually reduces the overall size of raw data from TBs to GBs/MBs and finally stores the processed data back into Oracle or Teradata for further analysis. Hadoop or Spark-like frameworks can be scaled out vertically, and adding nodes to the existing cluster can be done in no time without<a id="id365" class="indexterm"></a> bringing down the whole cluster and also providing no <span class="strong"><strong>SPOF</strong></span> (<span class="strong"><strong>single point of failure</strong></span>).</p><p>The real advantage here is that systems like Oracle or Teradata have to store only transformed data which is less than 50 percent of the overall size of the raw data, which eventually saves on the overall cost of the hardware and software licenses for Oracle and Teradata.</p><p>We talked about extraction and transformation in previous chapters. In this chapter we will discuss the operations provided by Spark for writing data (loading) to external systems. We will also talk about the integration of Spark with various popular NoSQL databases.</p><p>This chapter will cover the following points:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Output operation in Spark Streaming</p></li><li style="list-style-type: disc"><p>Integration with NoSQL DBâ€”Cassandra</p></li></ul></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec26"></a>Output operations in Spark Streaming</h2></div></div><hr /></div><p>Spark Streaming provides various standard and customizable output operations over DStreams <a id="id366" class="indexterm"></a>which can read the data from RDDs and can either save it to Hadoop or a text file, or print on the console. Let's see these output operations provided by Spark Streaming:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">print()</code>: This is the one of the basic functions for printing elements of RDDs stored within DStreams. It executes on the driver node and, by default, prints the first 10 elements of every batch of data in DStreams on the driver console. There is also an overloaded function <code class="literal">print(numElements:Int)</code> which provides the flexibility to print more than 10 elements.</p></li><li style="list-style-type: disc"><p>
<code class="literal">saveAsHadoopFiles(â€¦)</code>: This operation provides the integration with the Hadoop/HDFS where it persist the elements of the DStream in <span class="strong"><strong>HDFS</strong></span> (<span class="strong"><strong>Hadoop Distributed File System</strong></span>). This method works with the old MapReduce <a id="id367" class="indexterm"></a>API which was available with the Hadoop = &lt;0.20 version and can be invoked only on the DStreams containing a key/value pair. It is exposed by <code class="literal">org.apache.spark.streaming.dstream.PairDStreamFunctions.scala</code>, so it is not available in the standard DStreams.</p></li><li style="list-style-type: disc"><p>
<code class="literal">saveAsNewAPIHadoopFiles(â€¦)</code>: Similar to <code class="literal">saveAsHadoopFiles(â€¦)</code> with the difference that it leverages a new MapReduce API which is available with the Hadoop &gt; 0.20 version.</p></li><li style="list-style-type: disc"><p>
<code class="literal">saveAsTextFiles(â€¦)</code>: Persists the elements of the invoking DStream as a text file on the local filesystem or any other provided location.</p></li><li style="list-style-type: disc"><p>
<code class="literal">saveAsObjectFiles(â€¦)</code>: Persists the elements of the invoking DStream as a sequence file at the provided location on the HDFS/filesystem.</p></li><li style="list-style-type: disc"><p>
<code class="literal">forEachRDD(..)</code>: <code class="literal">forEachRDD</code> is a powerful, generic and special type of operation which can be extended and customized for integration and persisting data in any of the external systems which may not available as part of the standard Spark distribution, for example, persisting data in RDBMS, submitting data to REST services or MQ systems, and many more. It is similar to <code class="literal">forEach()</code>, where it takes up an arbitrary function as an argument which is further applied over each RDD in DStreams. This output operation is executed on RDDs which are available over the worker nodes. For example, let's assume that we need to submit the elements of each RDD into a messaging queue, so our implementation would look something like this:</p><div class="informalexample"><pre class="programlisting">dStream.forEachRDD{ //Line-1
//assuming that this method provides the connection to underlying MQ infrastructure
val conn = getConnection(â€¦) // Line-2
   rdd=&gt; rdd.forEach{ // Line-3
   //Function to create messages and post to Queues/
   //Topics
   createTextMessages().post(conn) // Line-4
   }
}</pre></div></li></ul></div><p>Although logically<a id="id368" class="indexterm"></a> the preceding code will work and we will be able to post data to queues and queues/topics, the preceding methods have overheads and sometimes may produce exceptions because of the following reasons:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The <code class="literal">getConnection()</code> function in the preceding code is executed over the driver while the rest of the function will be executed over the worker nodes, which in turn means that the connection object needs to be serialized and will be sent to the worker nodes, which is not a good idea.</p><div class="mediaobject"><img src="graphics/B01793_05-01.jpg" /></div><p>The <a id="id369" class="indexterm"></a>preceding illustration shows the process of creating a connection at driver nodes and then serializing the connection object and sending it to physical or worker nodes.</p><p>A better approach is to create connection on the worker nodes themselves and move the code for creating the connection just before <code class="literal">Line-4</code>.</p></li><li style="list-style-type: disc"><p>Assuming that we created a connection just before <code class="literal">Line-4</code>, again this is not the best approach as now we are creating a connection for each record in the RDD. So, in order to optimize it, we can implement <code class="literal">forEachPartition()</code> and then create connections for each partition, instead of each record. Our final implementation would look something like this:</p><div class="informalexample"><pre class="programlisting">dStream.forEachRDD{ //Line-1
  rdd=&gt; forEachPartition{ //Line-2
  val conn = getConnection(â€¦) // Line-3
  partition=&gt; partition.forEach(record =&gt; createTextMessages().post(conn)) //Line-4
  }
  }</pre></div><p>The following illustration shows the best way to create a connection to external systems like MQ for each partition:</p><div class="mediaobject"><img src="graphics/B01793_05-02.jpg" /></div></li></ul></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip06"></a>Tip</h3><p>Java API for DStreams, <code class="literal">org.apache.spark.streaming.api.java.JavaDStream</code>, can also be used to invoke all of the preceding output operations.</p></div><p>Let's <a id="id370" class="indexterm"></a>see a working piece of code which will leverage the output operations and persists data in filesystems and Hadoop.</p><p>Perform the following steps to set up Hadoop and HDFS which we will be using for persisting our log file data:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Download<a id="id371" class="indexterm"></a> Hadoop 2.4.0 distribution from <a class="ulink" href="https://archive.apache.org/dist/hadoop/common/hadoop-2.4.0/hadoop-2.4.0.tar.gz" target="_blank">https://archive.apache.org/dist/hadoop/common/hadoop-2.4.0/hadoop-2.4.0.tar.gz</a> and extract the archive to any folder of your choice on the same machine where you configured Spark.</p></li><li><p>Open the Linux shell and execute:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>export HADOOP_PREFIX=&lt;path of your directory where we extracted Hadoop&gt;</strong></span>
</pre></div></li><li><p>Follow the steps defined at <a class="ulink" href="http://hadoop.apache.org/docs/r2.5.2/hadoop-project-dist/hadoop-common/SingleCluster.html" target="_blank">http://hadoop.apache.org/docs/r2.5.2/hadoop-project-dist/hadoop-common/SingleCluster.html</a> for single node setup. After completing<a id="id372" class="indexterm"></a> the "prerequisites" defined in the given link, you can follow the setup instructions defined either for "pseudo-distributed mode" or "fully-distributed mode". For our needs "pseudo-distributed mode" will work but that does not stop you from trying the latter one.</p></li><li><p>Once <a id="id373" class="indexterm"></a>you have completed the setup, open your Linux shell and execute the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$HADOOP_PREFIX/bin/hdfs namenode â€“format</strong></span>
<span class="strong"><strong>$HADOOP_PREFIX/sbin/start-dfs.sh</strong></span>
</pre></div></li><li><p>The first command will format <code class="literal">namenode</code> and make the filesystem ready for use and with the second command we are starting the minimum required Hadoop services which will include namenode and secondary namenode.</p></li><li><p>Next, let's execute these commands to create a directory structure in HDFS where we will store our data:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$HADOOP_PREFIX/bin/hdfs dfs -mkdir /spark</strong></span>
<span class="strong"><strong>$HADOOP_PREFIX/bin/hdfs dfs -mkdir /spark/streaming</strong></span>
<span class="strong"><strong>$HADOOP_PREFIX/bin/hdfs dfs -mkdir /spark/streaming/oldApi</strong></span>
<span class="strong"><strong>$HADOOP_PREFIX/bin/hdfs dfs -mkdir /spark/streaming/newApi</strong></span>
<span class="strong"><strong>$HADOOP_PREFIX/bin/hdfs dfs â€“mkdir /spark/streaming/sequenceFiles</strong></span>
</pre></div></li></ol></div><p>If everything goes right and there are no exceptions then open your browser and browse to <code class="literal">http://localhost:50070/explorer.html#/</code> and you will be able to see the empty directories created by the preceding commands, as shown in the following illustration:</p><div class="mediaobject"><img src="graphics/B01793_05-03.jpg" /></div><p>The preceding image shows the HDFS filesystem explorer where we can browse, view and download any of the files created by the users using HDFS APIs.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note46"></a>Note</h3><p>Refer <a id="id374" class="indexterm"></a>to <a class="ulink" href="http://hadoop.apache.org/" target="_blank">http://hadoop.apache.org/</a> for more information on Hadoop and HDFS.</p></div><p>Perform<a id="id375" class="indexterm"></a> the following steps to implement output operations over DStreams and persist data in the HDFS and local filesystem:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Open your Linux shell and create a directory by executing the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>mkdir $SPARK_HOME/outputDir</strong></span>
</pre></div></li><li><p>Next, we will extend our <code class="literal">Spark-Examples</code> project and create a package called <code class="literal">chapter.five</code> and a Scala object <code class="literal">ScalaPersistLogData</code> and add the following piece of code:</p><div class="informalexample"><pre class="programlisting">package chapter.five

import org.apache.spark.SparkConf
import org.apache.spark.streaming._
import org.apache.spark.streaming.flume._
import org.apache.spark.storage.StorageLevel
import org.apache.spark.rdd._
import org.apache.spark.streaming.dstream._
import java.net.InetSocketAddress
import java.io.ObjectOutputStream
import java.io.ObjectOutput
import java.io.ByteArrayOutputStream
import org.apache.spark.SparkContext
import chapter.four.ScalaLogAnalyzer
import org.apache.hadoop.io._
import org.apache.hadoop.mapreduce._
import org.apache.hadoop.mapred.JobConf
import org.apache.hadoop._

object ScalaPersistLogData {

   def main(args:Array[String]){
     
    /** Start Common piece of code for all kinds of Output Operations*/    
    println("Creating Spark Configuration")
    val conf = new SparkConf()
    conf.setAppName("Apache Log Persister")
    println("Retreiving Streaming Context from Spark Conf")
    val streamCtx = new StreamingContext(conf, Seconds(10))
    
    var addresses = new Array[InetSocketAddress](1);
    addresses(0) = new InetSocketAddress("localhost",4949)
    
    val flumeStream = FlumeUtils.createPollingStream(streamCtx,addresses,StorageLevel.MEMORY_AND_DISK_SER_2,1000,1)
        
    //Utility class for Transforming Log Data
    val transformLog = new ScalaLogAnalyzer()
    //Invoking Flatmap operation for flattening the results //and convert them into Key/Value pairs
    val newDstream = flumeStream.flatMap { x =&gt; transformLog.tansformLogData(new String(x.event.getBody().array())) }
   
    /** End Common piece of code for all kinds of Output Operations*/
    
    /**Start - Output Operations */
    persistsDstreams(newDstream,streamCtx)
    /**End - Output Operations */
    
    streamCtx.start();
    streamCtx.awaitTermination();
  }</pre></div><p>The <a id="id376" class="indexterm"></a>main method of our new class, <code class="literal">ScalaPersistLogData</code>, remains almost the same, where we have defined the common piece of code which consumes the data from Spark Sink. Refer to the <span class="emphasis"><em>Installing and configuring Flume</em></span> section of <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Processing Distributed Log Files in Real Time</em></span>, for setting up Spark Sink.</p><p>But at the end of the <code class="literal">main()</code> method we have invoked a new method <code class="literal">persistsDstreams(â€¦)</code> which will contain all our output operations.</p></li><li><p>Next, we <a id="id377" class="indexterm"></a>will define a new method <code class="literal">persistsDstreams(â€¦)</code> and add the following piece of code to <code class="literal">ScalaPersistLogData.scala</code>:</p><div class="informalexample"><pre class="programlisting">/**
    * Define and execute all Output Operations over DStreams
    */
  def persistsDstreams(dStream:DStream[(String,String)],streamCtx: StreamingContext){
    
   //Writing Data as Text Files on Local File system.
   //This method takes 2 arguments: -
   //1."prefix" of file, which would be appended with Time //(in milliseconds) by Spark API's 
   //2."suffix" of the file
   //The final format will be //"&lt;prefix&gt;&lt;Milliseconds&gt;&lt;suffix&gt;"
   dStream.saveAsTextFiles("/home/ec2-user/softwares/spark-1.3.0-bin-hadoop2.4/outputDir/data-", "")
    
   //Creating an Object of Hadoop Config with default Values
   val hConf = new JobConf(new org.apache.hadoop.conf.Configuration())
   
   //Defining the TextOutputFormat using old API's //available with =&lt;0.20 
   val oldClassOutput = classOf[org.apache.hadoop.mapred.TextOutputFormat[Text,Text]]
   //Invoking Output operation to save data in HDFS using //old API's 
   //This method accepts following Parameters: -
   //1."prefix" of file, which would be appended with Time //(in milliseconds) by Spark API's 
   //2."suffix" of the file
   //3.Key - Class which can work with the Key  
   //4.Value - Class which can work with the Key
   //5.OutputFormat - Class needed for writing the Output //in a specific Format
   //6.HadoopConfig - Object of Hadoop Config
   dStream.saveAsHadoopFiles("hdfs://localhost:9000/spark/streaming/oldApi/data-", "", classOf[Text], classOf[Text], oldClassOutput ,hConf )
   
   //Defining the TextOutputFormat using new API's available with &gt;0.20
   val newTextOutputFormat = classOf[org.apache.hadoop.mapreduce.lib.output.TextOutputFormat[Text, Text]]
   
   //Invoking Output operation to save data in HDFS using new API's 
   //This method accepts same set of parameters as "saveAsHadoopFiles"
   dStream.saveAsNewAPIHadoopFiles("hdfs://localhost:9000/spark/streaming/newApi/data-", "", classOf[Text], classOf[Text], newTextOutputFormat ,hConf )
   
   //Defining saveAsObject for saving data in form of //Hadoop Sequence Files
   dStream.saveAsObjectFiles("hdfs://localhost:9000/spark/streaming/sequenceFiles/data-")
    
   //Using forEachRDD for printing the data for each Partition
   dStream.foreachRDD( 
        rdd =&gt; rdd.foreachPartition(
        data=&gt; data.foreach(
            //Printing the Values which can be replaced by //custom code 
            //for storing data in any other external //System.
            tup =&gt; System.out.println("Key = "+tup._1+", Value = "+tup._2)
            )   
        )    
    )
   }</pre></div></li></ol></div><p>And <a id="id378" class="indexterm"></a>we are done! Now let's execute this preceding piece of code and see the output in the HDFS and filesystem.</p><p>Perform the following steps to execute the preceding piece of code:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Assuming that your Spark job from <a class="link" href="#" linkend="ch04">Chapter 4</a>, <span class="emphasis"><em>Applying Transformations to Streaming Data</em></span> is still running, stop it by pressing <span class="emphasis"><em>Ctrl</em></span> + <span class="emphasis"><em>C</em></span>.</p></li><li><p>Next, export <a id="id379" class="indexterm"></a>your project as a JAR file, name it <code class="literal">Spark-Examples.jar</code> and save it in the root of <code class="literal">$SPARK_HOME</code>.</p></li><li><p>Next, open your Linux console and navigate to <code class="literal">$SPARK_HOME</code> and execute the following command to deploy your Spark Streaming job:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$SPARK_HOME/bin/spark-submit --class chapter.five.ScalaPersistLogData --master &lt;SPARK-MASTER-URL&gt; Spark-Examples.jar</strong></span>
</pre></div></li></ol></div><p>As soon as you execute the preceding command and hit <span class="emphasis"><em>Enter</em></span>, you will see that all your transformed data is persisted into different directories in HDFS and the filesystem which should look something similar to the following illustration:</p><div class="mediaobject"><img src="graphics/B01793_05-04.jpg" /></div><p>The preceding illustration shows the directories and files created in HDFS by the <code class="literal">saveAsNewAPIHadoopFile(â€¦)</code> method. You can further click on any of the directories and open the contents of the files.</p><div class="mediaobject"><img src="graphics/B01793_05-05.jpg" /></div><p>The preceding illustration shows the directories and files created in HDFS by the <code class="literal">saveAsHadoopFile(â€¦)</code> method. You can further click on any of the directories and open the contents of the files created in the directories.</p><p>The same structure would be created by <code class="literal">saveAsObjectFiles(â€¦)</code> in HDFS in the <code class="literal">sequenceFiles</code> folder and <code class="literal">saveAsTextFiles(â€¦)</code> in <code class="literal">$SPARK_HOME/outputDir/</code>.</p><p>Apart from<a id="id380" class="indexterm"></a> the preceding illustration you will also see the data printed on the console and that is done by the <code class="literal">forEachRDD(func)</code> method.</p><p>In this section we have discussed various output operations exposed by the DStreams API. We also configured and set up Hadoop/HDFS and finally implemented output operations for storing log data in the various directories on the filesystem and HDFS.</p><p>Let's move on to the section where we will talk about Spark integration with NoSQL systems like Cassandra for storing and retrieving our log data.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec27"></a>Integration with Cassandra</h2></div></div><hr /></div><p>Apache Cassandra, <a class="ulink" href="http://cassandra.apache.org/" target="_blank">http://cassandra.apache.org/</a>, is a massively distributed database for<a id="id381" class="indexterm"></a> handling large data across data centers. It is <a id="id382" class="indexterm"></a>a linearly scalable NoSQL (non-relational) open source database which offers high availability with ease of operation. It also offers a low cost solution for commodity hardware or cloud infrastructure but with proven fault tolerance.</p><p>The integration of a NoSQL database like Cassandra with Spark Streaming not only provides the flexibility to downstream systems like web applications, portals or mobile apps for consumption of the processed data according to convenience or requirements, but can also be used as the data source for Spark for further deep analytics.</p><p>DataStax, <a class="ulink" href="http://www.datastax.com" target="_blank">www.datastax.com</a>, the <a id="id383" class="indexterm"></a>company which offers a commercial <a id="id384" class="indexterm"></a>product DataStax Enterprise (built on top of Apache Cassandra) realized the opportunity and provided an excellent open source driver/API for integration between Spark and Cassandra. The Spark-Cassandra driver can be used in our Spark applications where we can directly load the data from Cassandra and use it our Spark applications for further analysis. This driver also provides server-side filters which only load selected or relevant datasets so that our Spark application only works with the portion of data of interest, saving resources (network, memory, CPU). This driver provides all APIs for performing CRUD operations over Cassandra and it also provides specific operations on DStreams so that you do not have to define <code class="literal">forEach(â€¦)</code>, for example, it defines <code class="literal">DStream.saveToCassandra(â€¦)</code> for storing the data of each RDD in the Cassandra tables.</p><p>Let's <a id="id385" class="indexterm"></a>extend our Spark project, <code class="literal">Spark-Examples</code>, and write Spark Streaming jobs for consuming streaming web logs and then further persisting the same data in Cassandra.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec43"></a>Installing and configuring Apache Cassandra</h3></div></div></div><p>Perform <a id="id386" class="indexterm"></a>the following steps to install and configure Apache<a id="id387" class="indexterm"></a> Cassandra:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Download and extract Apache Cassandra 2.1.7 from <a class="ulink" href="http://www.apache.org/dyn/closer.lua/cassandra/2.1.7/apache-cassandra-2.1.7-bin.tar.gz" target="_blank">http://www.apache.org/dyn/closer.lua/cassandra/2.1.7/apache-cassandra-2.1.7-bin.tar.gz</a> on the same machine where we installed our <a id="id388" class="indexterm"></a>Spark and Flume software.</p></li><li><p>Execute the following command on your Linux console and define the environment variable <code class="literal">CASSANDRA_HOME</code> as the environment variable which will point to the directory where we have extracted the downloaded archive file:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>export CASSANDRA_HOME = &lt;location of the Archive&gt;</strong></span>
</pre></div></li><li><p>Next, on the same console, execute the following command to bring up your Cassandra database with the default configuration:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$CASSANDRA_HOME/bin/cassandra</strong></span>
</pre></div><p>The preceding command will bring up your Cassandra database which is now ready to serve the user request but, before that, let's create a keyspace and tables where we will store our data.</p></li><li><p>Execute <a id="id389" class="indexterm"></a>the following command to open the Cassandra <span class="strong"><strong>CQL</strong></span> (<span class="strong"><strong>Cassandra Query Language</strong></span>) console:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$CASSANDRA_HOME/bin/cqlsh</strong></span>
</pre></div><p>CQLSH is <a id="id390" class="indexterm"></a>the command line utility which provides SQL-like syntax to perform CRUD operations on Cassandra databases.</p></li><li><p>Next, execute the following CQL commands on your CQLSH to create a keyspace and table in your Cassandra database:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>CREATE KEYSPACE logdata WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1 };</strong></span>
<span class="strong"><strong>CREATE TABLE logdata.apachelogdata(ip text PRIMARY KEY, client text,user text,date text,method text,request text,protocol text,respcode text,size text);</strong></span>
<span class="strong"><strong>select * from logdata.apachelogdata;</strong></span>
</pre></div><p>The<a id="id391" class="indexterm"></a> following illustration shows the<a id="id392" class="indexterm"></a> output of the CQL commands which creates the keyspace <code class="literal">logdata</code> and table <code class="literal">apachelogdata</code> within the keyspace and also executes the <code class="literal">select</code> query:</p><div class="mediaobject"><img src="graphics/B01793_05-06.jpg" /></div></li></ol></div><p>Our Cassandra setup is complete and now we will code and configure our Spark application to insert data in the <code class="literal">logdata.apachelogdata</code> table.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec44"></a>Configuring Spark for integration with Cassandra</h3></div></div></div><p>Perform<a id="id393" class="indexterm"></a> the following steps for configuring and integrating Spark platform with Cassandra:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Download the following JAR files and<a id="id394" class="indexterm"></a> store them in the <code class="literal">$CASSANDRA_HOME/lib/</code>.</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Spark-Cassandra connector: <a class="ulink" href="http://central.maven.org/maven2/com/datastax/spark/spark-cassandra-connector_2.10/1.3.0-M1/spark-cassandra-connector_2.10-1.3.0-M1.jar" target="_blank">http://central.maven.org/maven2/com/datastax/spark/spark-cassandra-connector_2.10/1.3.0-M1/spark-cassandra-connector_2.10-1.3.0-M1.jar</a>
</p></li><li style="list-style-type: disc"><p>Cassandra <a id="id395" class="indexterm"></a>Core <a id="id396" class="indexterm"></a>driver: <a class="ulink" href="http://search.maven.org/remotecontent?filepath=com/datastax/cassandra/cassandra-driver-core/2.1.5/cassandra-driver-core-2.1.5.jar" target="_blank">http://search.maven.org/remotecontent?filepath=com/datastax/cassandra/cassandra-driver-core/2.1.5/cassandra-driver-core-2.1.5.jar</a>
</p></li><li style="list-style-type: disc"><p>Spark-Cassandra Java library: <a class="ulink" href="http://search.maven.org/remotecontent?filepath=com/datastax/spark/spark-cassandra-connector-java_2.10/1.3.0-M1/spark-cassandra-connector-java_2.10-1.3.0-M1.jar" target="_blank">http://search.maven.org/remotecontent?filepath=com/datastax/spark/spark-cassandra-connector-java_2.10/1.3.0-M1/spark-cassandra-connector-java_2.10-1.3.0-M1.jar</a>
</p></li><li style="list-style-type: disc"><p>Other<a id="id397" class="indexterm"></a> libraries:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<a class="ulink" href="http://central.maven.org/maven2/org/joda/joda-convert/1.2/joda-convert-1.2.jar" target="_blank">http://central.maven.org/maven2/org/joda/joda-convert/1.2/joda-convert-1.2.jar</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://central.maven.org/maven2/joda-time/joda-time/2.3/joda-time-2.3.jar" target="_blank">http://central.maven.org/maven2/joda-time/joda-time/2.3/joda-time-2.3.jar</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://central.maven.org/maven2/com/twitter/jsr166e/1.1.0/jsr166e-1.1.0.jar" target="_blank">http://central.maven.org/maven2/com/twitter/jsr166e/1.1.0/jsr166e-1.1.0.jar</a>
</p></li></ul></div></li></ul></div></li><li><p>Next, the <a id="id398" class="indexterm"></a>your <code class="literal">$SPARK_HOME/conf/spark-default.conf</code> file and append the path given for <code class="literal">spark.executor.extraClassPath</code> and <code class="literal">spark.driver.extraClassPath</code> with the physical path of the libraries downloaded in the previous step.</p></li><li><p>Next, append and add the dependencies of the following Cassandra library for the <code class="literal">spark.executor.extraClassPath</code> and <code class="literal">spark.driver.extraClassPath</code> variables:</p><div class="informalexample"><pre class="programlisting">$CASSANDRA_HOME/lib/apache-cassandra-2.1.7.jar
$CASSANDRA_HOME/lib/apache-cassandra-clientutil-2.1.7.jar
$CASSANDRA_HOME/lib/apache-cassandra-thrift-2.1.7.jar
$CASSANDRA_HOME/lib/cassandra-driver-internal-only-2.5.1.zip
$CASSANDRA_HOME/lib/thrift-server-0.3.7.jar:
$CASSANDRA_HOME/lib/guava-16.0.jar</pre></div></li><li><p>Next, save the <code class="literal">spark-default.conf</code> file and restart Spark master and worker to check everything works as expected without any exceptions or errors.</p></li></ol></div><p>We are done with the configurations. Let's now write the Spark Streaming code for capturing log analysis data to further store in Cassandra.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec45"></a>Coding Spark jobs for persisting streaming web logs in Cassandra</h3></div></div></div><p>Let's extend <a id="id399" class="indexterm"></a>our <code class="literal">Spark-Examples</code> project and perform the following steps to create a Spark job for persisting web logs in Cassandra:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Open the <code class="literal">Spark-Examples</code> project and edit <code class="literal">chapter.four.ScalaLogAnalyzer</code> and add the following piece of code just before the closing bracket of the class <code class="literal">chapter.four.ScalaLogAnalyzer</code>:</p><div class="informalexample"><pre class="programlisting">def tansformLogDataIntoSeq(logLine: String):Seq[(String,String,String,String,String,String,String,String,String)] ={
    //Pattern which will extract the relevant data from Apache Access Log Files """^(\S+) (\S+) (\S+) \[([\w:/]+\s[+\-]\d{4})\] "(\S+) (\S+) (\S+)" (\d{3}) (\S+)""";
    val PATTERN = Pattern.compile(LOG_ENTRY_PATTERN);
    val matcher = PATTERN.matcher(logLine);
    
    //Matching the pattern for the each line of the Apache access Log file 
    if (!matcher.find()) {
      System.out.println("Cannot parse logline" + logLine);
    }
    //Finally create a Key/Value pair of extracted data and //return to calling program
    createSeq(matcher);
    
}
  
def createSeq(m:Matcher):Seq[(String,String,String,String,String,String,String,String,String)] = {
    Seq((m.group(1),m.group(2), m.group(3), m.group(4), m.group(5), m.group(6), m.group(7) , m.group(8), m.group(9)))
}</pre></div><p>The preceding piece of code defines a utility function for converting log log data into iterable collections which we will further use and insert in Cassandra.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note47"></a>Note</h3><p>For <a id="id400" class="indexterm"></a>more information on Seq, refer to <a class="ulink" href="http://www.scala-lang.org/api/2.11.5/index.html#scala.collection.Seq" target="_blank">http://www.scala-lang.org/api/2.11.5/index.html#scala.collection.Seq</a>.</p></div></li><li><p>Within the package <code class="literal">chapter.five</code>, create <a id="id401" class="indexterm"></a>a new Scala object <code class="literal">ScalaPersistInCassandra</code> and add the following piece of code:</p><div class="informalexample"><pre class="programlisting">package chapter.five

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.streaming.StreamingContext
import java.net.InetSocketAddress
import org.apache.spark.streaming.flume.FlumeUtils
import org.apache.spark.streaming.Seconds
import org.apache.spark.storage.StorageLevel
import com.datastax.spark.connector.streaming._
import com.datastax.spark.connector._
import chapter.four.ScalaLogAnalyzer
import com.datastax.spark.connector.SomeColumns

object ScalaPersistInCassandra {
 
  def main(args:Array[String]){
  
    /** Start Common piece of code for all kinds of Output Operations*/    
    println("Creating Spark Configuration")
    val conf = new SparkConf()
    conf.setAppName("Apache Log Persister in Cassandra")
    //Cassandra Host Name
    println("Setting Cassandra Host Name for getting Connection")
    conf.set("spark.cassandra.connection.host", "localhost")
    
    println("Retreiving Streaming Context from Spark Conf")
    val streamCtx = new StreamingContext(conf, Seconds(10))
    var addresses = new Array[InetSocketAddress](1);
    addresses(0) = new InetSocketAddress("localhost",4949)
    val flumeStream = FlumeUtils.createPollingStream(streamCtx,addresses,StorageLevel.MEMORY_AND_DISK_SER_2,1000,1)
    //Utility class for Transforming Log Data
    val transformLog = new ScalaLogAnalyzer()
    //Invoking Flatmap operation to flattening the results and convert them into Key/Value pairs
    val newDstream = flumeStream.flatMap { x =&gt; transformLog.tansformLogDataIntoSeq(new String(x.event.getBody().array())) }
    /** End Common piece of code for all kinds of Output Operations*/
      
    //Define Keyspace
    val keyspaceName ="logdata"
    //Define Table
    val csTableName="apachelogdata"
    //Invoke saveToCassandra to persist DStream to Cassandra CF
    newDstream 
    .saveToCassandra(keyspaceName, csTableName, SomeColumns("ip","client","user","date","method","request","protocol","respcode","size"))
    
    streamCtx.start();
    streamCtx.awaitTermination();  
    
  }
}</pre></div></li></ol></div><p>And <a id="id402" class="indexterm"></a>we are done! Now perform the following steps to execute the preceding piece of code and see the output in Cassandra.</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Assuming that your Spark cluster is down, start the Spark master, slave and Flume agent. If required, also run your log simulator so that your agent receives the log data.</p></li><li><p>Next, export your project as a JAR file, name it <code class="literal">Spark-Examples.jar</code> and save it JAR file in the root of <code class="literal">$SPARK_HOME</code>.</p></li><li><p>Next, open the Linux console and browse to <code class="literal">$SPARK_HOME</code> and execute the following command to deploy your Spark Streaming job:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$SPARK_HOME/bin/spark-submit --class chapter.five.ScalaPersistInCassandra --master &lt;SPARK-MASTER-URL&gt; Spark-Examples.jar.</strong></span>
</pre></div></li><li><p>As soon as the preceding statement has executed, hit <span class="emphasis"><em>Enter</em></span> and your Apache log<a id="id403" class="indexterm"></a> data is persisted in Cassandra which can be further queried by executing the following command on CQLSH:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>select * from LogData.ApacheLogData;</strong></span>
</pre></div><p>The preceding illustration shows the output of the <code class="literal">select</code> command on the CQLSH:</p><div class="mediaobject"><img src="graphics/B01793_05-07.jpg" /></div></li></ol></div><p>Let's perform the following steps to read and filter data from the Cassandra table based on certain criteria in our Spark Streaming job. We also print the top 10 <code class="literal">GET</code> requests based on the write time to the Cassandra table.</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Open and edit <code class="literal">ScalaPersistInCassandra.scala</code> and add a new method by the name of <code class="literal">readAndPrintData</code>, shown as follows:</p><div class="informalexample"><pre class="programlisting">def readAndPrintData(streamCtx: StreamingContext){}</pre></div></li><li><p>Next, invoke this method from the main method just after the initialization of <code class="literal">StreamingContext</code>:</p><div class="informalexample"><pre class="programlisting">println("Retreiving Streaming Context from Spark Conf")
val streamCtx = new StreamingContext(conf, Seconds(10))
//First read the existing data from Cassandra
readAndPrintData(streamCtx)</pre></div></li><li><p>Now, edit <code class="literal">readAndPrintData(streamCtx)</code> and add the following piece of code for printing a specific column of the Cassandra table and also printing all rows:</p><div class="informalexample"><pre class="programlisting">//Reading data from Cassandra and Printing on Console
println("Start - Printing the data from Cassandra.. ")
println("Start - Print All IP's .................. ")
//Prints the first Column (IP) of the table
//Get the reference of the apachelogdata table from the Context
//which further returns the Object of CassandraTableScanRDD
val csRDD = streamCtx.cassandraTable("logdata", "apachelogdata").collect()
//Now using forEach print only the ip column using the getString() method
csRDD.foreach ( x =&gt; println("IP = " + x.getString("ip")))
println("End - Print All IP's ....................... ")
println("Start - Print All Rows ..................... ")
//Use the Same RDD and print complete Rows just by
//invoking toString() method
csRDD.foreach ( x =&gt; println("Cassandra Row = " + x.toString()))
println("End - Print All Rows ....................... ")
println("End - Printing the data from Cassandra...... ")</pre></div><p>The preceding piece of code is executed before your job is launched and will print the data on your console or driver where your job is running.</p></li><li><p>Next <a id="id404" class="indexterm"></a>we will see the usage of server-side filters but, before that, let's execute the following set of commands on the Linux console to open CQLSH and create an index on the column <code class="literal">method</code>.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$CASSANDRA_HOME/bin/cqlsh</strong></span>
<span class="strong"><strong>CREATE INDEX method_name ON logdata.apachelogdata ( method);</strong></span>
</pre></div><p>In Cassandra, server-side filters only work with indexed columns or we have to include extra parameters (<code class="literal">ALLOW FILTERING</code>) at the end of the CQL query. The advantage of using server-side filters is that the database performs the filtering and provides the relevant data which is more efficient than filtering data at the Spark cluster. In the next steps, we will use the column <code class="literal">method</code> as our filtering criteria.</p></li><li><p>Edit the <code class="literal">readAndPrintData(streamCtx)</code> method and just before the closing braces add the following piece of code:</p><div class="informalexample"><pre class="programlisting">println("Start - Print only Filetered Rows .......... ")
//Get the RDD and select the column to be printed and use //where clause
//to specify the condition.
//Here we are selecting only "ip" column where "method=GET"
val csFilterRDD = streamCtx.cassandraTable("logdata", "apachelogdata").select("ip").where("method=?","GET")
//Finally print the ip column by using foreach loop.
csFilterRDD.collect().foreach( x =&gt; println("IP = " + x.getString("ip")))
println("End - Print only Filetered Rows ......... ")</pre></div></li><li><p>Next, edit the <code class="literal">readAndPrintData(streamCtx)</code> method and just before the closing braces add the <a id="id405" class="indexterm"></a>following piece of code for printing the top 10 <code class="literal">GET</code> requests based on the "write" time to the Cassandra table:</p><div class="informalexample"><pre class="programlisting">println("Start - Print Top 10 GET request")
//we are using the *writetime* method of CQL which gives //time(microseconds) of record written in Cassandra
 val csTimeRDD = streamCtx.cassandraTable("logdata", "apachelogdata").
select("ip","method","date","method".writeTime.as("time")).where("method=?","GET")
csTimeRDD.collect().
sortBy(x =&gt; calculateDate(x.getLong("time"))).
reverse.take(10).foreach(        
      x =&gt; 
println(x.getString("ip") + " - " + x.getString("date")+" - "+ x.getString("method")+" - "+calculateDate(x.getLong("time")) ))
println("End - Print Top 10 Latest request ")</pre></div></li></ol></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note48"></a>Note</h3><p>The preceding piece of code needs to be used with caution. The ideal way to use it in a production environment with large datasets would be to use server-side filters and perform sorting and ordering in Cassandra itself.</p></div><p>We also need to add a convenient method outside <code class="literal">readAndPrintData(â€¦)</code> which will convert microseconds into <code class="literal">java.util.Date</code>:</p><div class="informalexample"><pre class="programlisting">import java.util.Date
import java.util.Calendar

  /**
   * Converting Microseconds to Date
   */
  def calculateDate(data:Long): Date = {
    val cal = Calendar.getInstance
    cal.setTimeInMillis(data/1000)
    cal.getTime
  }</pre></div><p>The preceding method not <a id="id406" class="indexterm"></a>only prints the top 10 <code class="literal">GET</code> requests based on the "write" time to the Cassandra table but it also helps us in knowing the latency between the records produced by the webserver and the time when it was written in the Cassandra server, which we can easily get by computing the difference between the "write" time and the "date" in the Cassandra table.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note49"></a>Note</h3><p>In the production environment it is advised to separate reads from writes, where writes can be streamed in separate job and reads can be scheduled as a recurring Spark job executed after a certain time interval, say every 10 seconds, so that the user can see the top 10 records being refreshed and updated after a certain interval.</p></div><p>We are done with all our changes and now let's compile and run the Spark job again to see that, as per expectations, data from the Cassandra table is printed on the console.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note50"></a>Note</h3><p>Refer<a id="id407" class="indexterm"></a> to the official documentation <a class="ulink" href="https://github.com/datastax/spark-cassandra-connector/" target="_blank">https://github.com/datastax/spark-cassandra-connector/</a> for more information on the available APIs and functionality supported by the Spark-Cassandra connector.</p></div><p>In this section we have discussed the integration of Spark and Cassandra. We also talked about a few of the features like server-side filters exposed by the connector.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec28"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we have discussed in detail various output operations. We have further extended our output operation and shown fully fledged integration with Apache Cassandra using the Spark-Cassandra connector.</p><p>In the next chapter, we will discuss the integration of Spark Streaming with other advance Spark libraries.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch06"></a>ChapterÂ 6.Â Integration with Advanced Spark Libraries</h2></div></div></div><p>No single software in today's world can fulfill the varied, versatile, and complex demands or needs of enterprises and to be honest neither should it!</p><p>Software is made to fulfill specific needs arising out of the enterprises at a particular point in time which may change in future due to many other factors. These factors may or may not be controlled like government policies, business/market dynamics, and many more.</p><p>Considering all these factors, integration and interoperability of any software system with internal/external systems/software is pivotal in fulfilling the enterprise needs. Integration and interoperability are categorized as non-functional requirements, which are always implicit and may or may not be explicitly stated by the end users.</p><p>Over the period of time, architects have realized the importance of these implicit requirements in modern enterprises and now all the enterprise architectures provide due diligence and provisions in the fulfillment of these requirements. Even the enterprise architecture frameworks such as <span class="strong"><strong>The Open Group Architecture Framework</strong></span> (<span class="strong"><strong>TOGAF</strong></span>) defines the specific set of procedures and guidelines for defining and establishing the interoperability and integration requirements of modern enterprises.</p><p>Spark <a id="id408" class="indexterm"></a>community realized the importance of both these factors and provided a versatile and scalable framework with certain hooks for integration and interoperability with the different systems/libraries for example data consumed and processed via Spark streams can also be loaded into the structured (table: rows/columns) format and can be further queried using SQL. Even the data can be stored in form of Hive tables in HDFS as persistent tables that will exist even after our Spark program has restarted.</p><p>In this chapter, we will discuss about the integration of Spark Streaming with various other advanced Spark libraries such as Spark SQL and Spark GraphX.</p><p>This chapter will cover the following points:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Querying streaming data in real time: Spark SQL</p></li><li style="list-style-type: disc"><p>Graph analysis: Spark GraphX</p></li></ul></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec29"></a>Querying streaming data in real time</h2></div></div><hr /></div><p>Spark<a id="id409" class="indexterm"></a> Streaming is developed on the principle of integration and interoperability where it doesn't only provide a framework for consuming data in near real time from varied data sources but at the same time, it also provides the integration with Spark SQL where existing DStreams can be converted into structured data format for querying using standard SQL constructs.</p><p>There are many such use cases where SQL on streaming data is a much needed feature, for example, in our distributed log analysis use case, we may need to combine the precomputed datasets with the streaming data for performing exploratory analysis using interactive SQL queries, which is difficult to implement only with streaming operators as they are not designed for introducing new datasets and perform ad hoc queries.</p><p>Moreover, SQL's success at expressing complex data transformations derives from the fact that it is based on a set of very powerful data processing primitives that do filtering, merging, correlation, and aggregation, which is not available in the low-level programming languages such as Java/C++ and may result in long development cycles and high maintenance costs.</p><p>Let's move forward and first understand few things about Spark SQL and then, we will also see the process of converting the existing DStreams into the structured formats.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec46"></a>Understanding Spark SQL</h3></div></div></div><p>Spark SQL is <a id="id410" class="indexterm"></a>one of the modules developed over Spark framework for processing structured data, which is stored in the form of rows and columns. At a very high level, it is similar to the data residing in RDBMS in form rows and columns and then SQL queries are executed for performing analysis, but Spark SQL is much more versatile and flexible as compared to RDBMS. Spark SQL provides distributed processing of SQL queries and can be compared to frameworks like Hive, Impala, or Drill. Here<a id="id411" class="indexterm"></a> are a few notable features of Spark SQL:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>It is capable of loading data from a variety of data sources, such as text files, JSON, Hive, HDFS, Parquet format and of course RDBMS too, so that we can consume, join and process datasets from different and varied data sources</p></li><li style="list-style-type: disc"><p>It<a id="id412" class="indexterm"></a> supports static and dynamic schema definition for the data loaded from various sources, which helps in defining schema for known data structures/types and also for those datasets where the columns and their types are not known until runtime</p></li><li style="list-style-type: disc"><p>It can work as a distributed query engine using thrift JDBC/ODBC server or command-line interface where end users or applications can interact with Spark SQL directly to run SQL queries</p></li><li style="list-style-type: disc"><p>It<a id="id413" class="indexterm"></a> provides integration with Spark Streaming where DStreams can be transformed into structured format and further SQL queries can be executed</p></li><li style="list-style-type: disc"><p>It is capable of caching tables using an in-memory columnar format for faster reads and in-memory data processing</p></li><li style="list-style-type: disc"><p>It supports Schema evolution so that new columns can be added/deleted to the existing schema and Spark SQL still maintains the compatibility between all the versions of the schema</p></li></ul></div><p>Spark <a id="id414" class="indexterm"></a>SQL defines the higher level of programming abstraction called <span class="strong"><strong>DataFrames</strong></span>, which is also an extension to the existing RDD API.</p><p>DataFrames are the distributed collection of the objects in form the rows and named columns, which is similar to tables in the RDBMS, but with much richer functionality containing all the previously defined features. The DataFrame API is inspired by the concepts<a id="id415" class="indexterm"></a> of DataFrames in R (<a class="ulink" href="http://www.r-tutor.com/r-introduction/data-frame" target="_blank">http://www.r-tutor.com/r-introduction/data-frame</a>) and Python (<a class="ulink" href="http://pandas.pydata.org/pandas-docs/stable/dsintro.html#dataframe" target="_blank">http://pandas.pydata.org/pandas-docs/stable/dsintro.html#dataframe</a>).</p><p>Let's move ahead and understand how Spark SQL works with the help of an example:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>As a first step, let's create sample JSON data with basic information about the company's departments such as <code class="literal">Name</code>, <code class="literal">Employees</code>, and so on and save this data into a <code class="literal">company.json</code> file. The JSON file would look similar to this:</p><div class="informalexample"><pre class="programlisting">[
   {  
      "Name":"DEPT_A",
      "No_Of_Emp":10,
      "No_Of_Supervisors":2
   },
   {  
      "Name":"DEPT_B",
      "No_Of_Emp":12,
      "No_Of_Supervisors":2
   },
   {  
      "Name":"DEPT_C",
      "No_Of_Emp":14,
      "No_Of_Supervisors":3
   },
   {  
      "Name":"DEPT_D",
      "No_Of_Emp":10,
      "No_Of_Supervisors":1
   },
   {  
      "Name":"DEPT_E",
      "No_Of_Emp":20,
      "No_Of_Supervisors":5
   }
]</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note51"></a>Note</h3><p>You<a id="id416" class="indexterm"></a> can use any online JSON editor such as <a class="ulink" href="http://codebeautify.org/online-json-editor" target="_blank">http://codebeautify.org/online-json-editor</a> to see and edit the data defined in the preceding JSON code.</p></div></li><li><p>Next, let's <a id="id417" class="indexterm"></a>extend our <code class="literal">Spark-Examples</code> project and create a new package named <code class="literal">chapter.six</code>. Within this new package, create a new Scala object and name it <code class="literal">ScalaFirstSparkSQL.scala</code>.</p></li><li><p>Next, add the following <code class="literal">import</code> statements just below the package declaration:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.sql._
import org.apache.spark.sql.functions._</pre></div></li><li><p>Further, in your main method, add the following set of statements to create <code class="literal">SQLContext</code> from <code class="literal">SparkContext</code>:</p><div class="informalexample"><pre class="programlisting">//Creating Spark Configuration
val conf = new SparkConf()
//Setting Application/Job Name
conf.setAppName("My First Spark SQL")
//Define Spark Context which we will use to initialize our //SQL Context 
val sparkCtx = new SparkContext(conf)
//Creating SQL Context
val sqlCtx = new SQLContext(sparkCtx)</pre></div><p>The <code class="literal">SQLContext</code> class or any of its descendants, such as <code class="literal">HiveContext</code> for working with Hive tables or <code class="literal">CassandraSQLContext</code> for working with Cassandra tables, is the main entry point for accessing all the functionalities of Spark SQL. It allows the creation of DataFrames and also provides functionality to fire SQL queries over DataFrames.</p></li><li><p>Next, we<a id="id418" class="indexterm"></a> will define the following code to load the JSON file (<code class="literal">company.json</code>) using the <code class="literal">SQLContext</code> class and further we will also create a data frame:</p><div class="informalexample"><pre class="programlisting">//Define path of your JSON File (company.json) which needs //to be processed 
val path = "/home/softwares/spark/data/company.json";
//Use SQLCOntext and Load the JSON file.
//This will return the DataFrame which can be further //Queried using SQL queries.
val dataFrame = sqlCtx.jsonFile(path)</pre></div></li></ol></div><p>In the preceding piece of code, we are using the <code class="literal">jsonFile(â€¦)</code> method for loading the JSON data. There are other utility methods defined by the <code class="literal">SQLContext</code> class for reading raw data from filesystem, creating DataFrames from the existing RDD, and many more.</p><p>Spark SQL supports two different methods for converting the existing RDDs into DataFrames. The first method uses reflection to infer the schema of an RDD from the given data. This approach leads to more concise code and helps in instances where we already know the schema while writing the Spark application. We have used the same approach in our example.</p><p>The second method is through a programmatic interface that allows to construct a schema and then apply it to an existing RDD and finally generate a data frame. This method is more verbose, but provides flexibility and helps in those instances where columns and datatypes are not known until the data is received at runtime.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note52"></a>Note</h3><p>For a <a id="id419" class="indexterm"></a>complete list of method exposed by <code class="literal">SQLContext</code>, refer to <a class="ulink" href="https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.sql.SQLContext" target="_blank">https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.sql.SQLContext</a>.</p></div><p>Once <a id="id420" class="indexterm"></a>DataFrame is created, we need to register the DataFrame as a temporary table within the SQL context so that we can execute the SQL queries over the registered table. Let's add the following piece of code for registering our DataFrame with our SQL context and name it <code class="literal">company</code>:</p><div class="informalexample"><pre class="programlisting">//Register the data as a temporary table within SQL Context
//Temporary table is destroyed as soon as SQL Context is //destroyed. 
dataFrame.registerTempTable("company");</pre></div><p>We are done! Our JSON data is automatically organized into the table (rows/column) and is ready to accept the SQL queries. Even the datatypes are also inferred from the type of data entered within the JSON file itself.</p><p>Now we will start executing the SQL queries on our table, but before this, let's see the schema being created/defined by the <code class="literal">SQLContext</code> class:</p><div class="informalexample"><pre class="programlisting">//Printing the Schema of the Data loaded in the Data Frame
dataFrame.printSchema();</pre></div><p>The execution of the preceding statement will provide results similar to the following screenshot:</p><div class="mediaobject"><img src="graphics/B01793_06-01.jpg" /></div><p>The preceding screenshot shows the schema of the JSON data loaded by Spark SQL. Pretty simple and straightforward, isn't it? Spark SQL has automatically created our schema based on the data defined in our <code class="literal">company.json</code> file. It has also even defined the datatype of each of the columns.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note53"></a>Note</h3><p>We<a id="id421" class="indexterm"></a> can also define the schema using reflection (<a class="ulink" href="https://spark.apache.org/docs/1.3.0/sql-programming-guide.html#inferring-the-schema-using-reflection" target="_blank">https://spark.apache.org/docs/1.3.0/sql-programming-guide.html#inferring-the-schema-using-reflection</a>) or programmatically (<a class="ulink" href="https://spark.apache.org/docs/1.3.0/sql-programming-guide.html#inferring-the-schema-using-reflection" target="_blank">https://spark.apache.org/docs/1.3.0/sql-programming-guide.html#inferring-the-schema-using-reflection</a>).</p></div><p>Next, let's execute some SQL queries to see the data stored in the DataFrame, so the first SQL <a id="id422" class="indexterm"></a>would be to print all the records:</p><div class="informalexample"><pre class="programlisting">    //Executing SQL Queries to Print all records in the DataFrame 
    println("Printing All records")
    sqlCtx.sql("Select * from company").collect().foreach(print)</pre></div><p>The execution of the preceding statement will produce the following results on the console where the driver is executed:</p><div class="mediaobject"><img src="graphics/B01793_06-02.jpg" /></div><p>Next, let's also select only few columns instead of all the records and print the same on the console:</p><div class="informalexample"><pre class="programlisting">   //Executing SQL Queries to Print Name and Employees 
   //in each Department
   println("\n Printing Number of Employees in All Departments")
   sqlCtx.sql("Select Name, No_Of_Emp from company").collect().foreach(println)</pre></div><p>The execution of the preceding statement will produce the following results on the console where the driver is executed:</p><div class="mediaobject"><img src="graphics/B01793_06-03.jpg" /></div><p>Finally, let's do some aggregation and count the total number of employees across the departments:</p><div class="informalexample"><pre class="programlisting">    //Using the aggregate function (agg) to print the 
    //total number of employees in the Company 
    println("\n Printing Total Number of Employees in Company_X")
    val allRec = sqlCtx.sql("Select * from company").agg(Map("No_Of_Emp"-&gt;"sum"))
    allRec.collect.foreach ( println )</pre></div><p>In the <a id="id423" class="indexterm"></a>preceding piece of code, we are using the <code class="literal">agg(â€¦)</code> function and performing the sum of all the employees across the departments, where <code class="literal">sum</code> can be replaced by <code class="literal">avg</code>, <code class="literal">max</code>, <code class="literal">min</code>, or <code class="literal">count</code>.</p><p>The execution of the preceding statement will produce the following results on the console where driver is executed:</p><div class="mediaobject"><img src="graphics/B01793_06-04.jpg" /></div><p>The preceding images shows the results of executing the aggregation on our <code class="literal">company.json</code> data.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note54"></a>Note</h3><p>For<a id="id424" class="indexterm"></a> further information on the available functions for performing aggregation, refer to the DataFrame API at <a class="ulink" href="https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.sql.DataFrame" target="_blank">https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.sql.DataFrame</a>.</p></div><p>As a last step, we will stop our Spark SQL context by invoking <code class="literal">stop()</code> function on <code class="literal">SparkContext</code>â€”<code class="literal">sparkCtx.stop()</code>.This is required to let your application notify the master or resource manager to release all the resources allocated to the Spark job. It also ensures the graceful shutdown of the job and avoids any resource leakage that may happen otherwise. Also, as of now there can be only one Spark context active per JVM, and we need to <code class="literal">stop()</code> the active <code class="literal">SparkContext</code> before creating a new one.</p><p>In this section, we have seen the step-by-step process of using Spark SQL as a standalone program. Though we have considered the JSON files as an example, but we can also leverage <a id="id425" class="indexterm"></a>Spark <a id="id426" class="indexterm"></a>SQL with Cassandra (<a class="ulink" href="https://github.com/datastax/spark-cassandra-connector/blob/master/doc/2_loading.md" target="_blank">https://github.com/datastax/spark-cassandra-connector/blob/master/doc/2_loading.md</a>), MongoDB (<a class="ulink" href="https://github.com/Stratio/spark-mongodb" target="_blank">https://github.com/Stratio/spark-mongodb</a>), or Elasticsearch (<a class="ulink" href="http://chapeau.freevariable.com/2015/04/elasticsearch-and-spark-1-dot-3.html" target="_blank">http://chapeau.freevariable.com/2015/04/elasticsearch-and-spark-1-dot-3.html</a>). Let's<a id="id427" class="indexterm"></a> move forward toward our next section where we will talk about integrating Spark SQL with Spark Streaming.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec47"></a>Integrating Spark SQL with streams</h3></div></div></div><p>Let's<a id="id428" class="indexterm"></a> continue our distributed log processing example and integrate the same with Spark SQL. We will capture the streaming data using Flume and then further perform aggregations using Spark SQL. Refer to the<span class="emphasis"><em> Data loading from distributed and varied sources</em></span> section of <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Processing Distributed Log Files in Real Time</em></span>, for more details around the use case.</p><p>We will enhance our distributed log files processing use case and will leverage Spark SQL for analyzing our Apache log data received/captured with Spark streams in a particular "streaming window". We will first convert our log files into a structured format (DataFrames) and then execute the SQL queries over the structured data for counting the number of distinct type of requests received in a window and finally printing the same on console. Performing data analysis with SQL queries is always preferred because firstly, it provides easy to use functions for aggregations, filtering, merging, and correlation, which is missing in low-level languages such as C or Java; secondly, SQL is easy to learn, adaptable, and widely accepted for performing data analysis as compared to any other programming language.</p><p>Here is the overall architecture of our distributed log processing use case after we have integrated Spark Streaming with Spark SQL:</p><div class="mediaobject"><img src="graphics/B01793_06-05.jpg" /></div><p>Now<a id="id429" class="indexterm"></a> that we have implemented Spark Streaming, let's move ahead and perform the following steps for integrating Spark SQL with Spark Streaming:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Extend the <code class="literal">Spark-Examples</code> project and create a Scala object (<code class="literal">ScalaQueryingStreams.scala</code> in package <code class="literal">chapter.six</code>).</p></li><li><p>Next, add the following import statements just below the package declaration:</p><div class="informalexample"><pre class="programlisting">import java.net.InetSocketAddress
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.annotation.Experimental
import org.apache.spark.sql.SQLContext
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.Seconds
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.flume.FlumeUtils
import chapter.four.ScalaLogAnalyzer</pre></div></li><li><p>Download<a id="id430" class="indexterm"></a> the utility JAR file for converting streams into JSON file from <a class="ulink" href="http://central.maven.org/maven2/com/googlecode/json-simple/json-simple/1.1.1/json-simple-1.1.1.jar" target="_blank">http://central.maven.org/maven2/com/googlecode/json-simple/json-simple/1.1.1/json-simple-1.1.1.jar</a>. Save it at <code class="literal">$SPARK_HOME/lib/json-simple-1.1.1.jar</code> and also add it in your project classpath.</p></li><li><p>Next, edit <code class="literal">chapter.four.ScalaLogAnalyzer</code> and define a new method, <code class="literal">tansformLogDataIntoJSON(â€¦)</code>. This new method will parse and convert the<a id="id431" class="indexterm"></a> streaming data into JSON string:</p><div class="informalexample"><pre class="programlisting">  /**
  * Transform the Apache log files and convert them into JSON Format
  */
  def tansformLogDataIntoJSON(logLine: String): String = {
    //Pattern which will extract the relevant data from //Apache Access Log Files
    val LOG_ENTRY_PATTERN = " ""^(\S+) (\S+) (\S+) \[([\w:/]+\s[+\-]\d{4})\] "(\S+) (\S+) (\S+)" (\d{3}) (\S+)"""
    val PATTERN = Pattern.compile(LOG_ENTRY_PATTERN)
    val matcher = PATTERN.matcher(logLine)

    //Matching the pattern for the each line of the Apache //access Log file 
    if (!matcher.find()) {
      System.out.println("Cannot parse logline" + logLine)
    }
    
    //Creating the JSON Formatted String from the Map
    import scala.collection.JavaConversions._
    val obj = new JSONObject(mapAsJavaMap(createDataMap(matcher)))
    val json = obj.toJSONString()
    
    println("JSON DATA new One - ", json )
    return json
  }</pre></div></li><li><p>Next, edit <code class="literal">ScalaQueryingStreams</code> and add the following piece of code:</p><div class="informalexample"><pre class="programlisting">  def main(args: Array[String]) {

    //Creating Spark Configuration
    val conf = new SparkConf()
    conf.setAppName("Integrating Spark SQL")
    //Define Spark Context which we will use to initialize //our SQL Context 
    val sparkCtx = new SparkContext(conf)
    //Retrieving Streaming Context from Spark Context
    val streamCtx = new StreamingContext(sparkCtx, Seconds(10))
    
    //Defining Host for receiving data from Flume Sink    
    var addresses = new Array[InetSocketAddress](1);
    addresses(0) = new InetSocketAddress("localhost", 4949)
    //Creating Flume Polling Stream
    val flumeStream = FlumeUtils.createPollingStream (streamCtx, addresses, StorageLevel.MEMORY_AND_DISK_SER_2, 1000, 1)

    //Utility class for Transforming Log Data
    val transformLog = new ScalaLogAnalyzer()

    //Invoking map() operation to convert the log data into //RDD of JSON Formatted String
    val newDstream = flumeStream.map { x =&gt; transformLog.tansformLogDataIntoJSON(new String(x.event.getBody().array())) }

    //Defining Window Operation, So that we can execute SQL //Query on data received within a particular Window
    val wStream = newDstream.window(Seconds(40), Seconds(20))

    //Creating SQL DataFrame for each of the RDD's
    wStream.foreachRDD { rdd =&gt;
      //Getting the SQL Context from Utility Method which 
      //provides Singleton Instance of SQL Context
      val sqlCtx = getInstance(sparkCtx)
      //Converting JSON RDD into the SQL DataFrame by using //jsonRDD() function
      val df = sqlCtx.jsonRDD(rdd)
      //creating and Registering the Temporary table for //the Converting DataFrame into table for further //Querying
      df.registerTempTable("apacheLogData")
      
      //Print the Schema
      println("Here is the Schema of your Log Files............")
      df.printSchema()
      //Executing the Query to get the total count of //different HTTP Response Code in the Data Frame
      val logDataFrame = sqlCtx.sql("select method, count(*) as total from apacheLogData group by method")
      //Finally printing the results on the Console
      println("Total Number of Requests.............. ")
      logDataFrame.show()
    }

    streamCtx.start();
    streamCtx.awaitTermination();
  }

  //Defining Singleton SQLContext variable
  @transient private var instance: SQLContext = null

  //Lazy initialization of SQL Context
  def getInstance(sparkContext: SparkContext): SQLContext = synchronized {
    if (instance == null) {
      instance = new SQLContext(sparkContext)
    }
    instance
  }</pre></div></li></ol></div><p>We <a id="id432" class="indexterm"></a>are done! Our Spark streams are converted into Spark SQL DataFrames and we have also written the SQL queries for querying the data.</p><p>Now let's move forward and perform the following steps for executing the preceding piece of code:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Edit <code class="literal">$SPARK_HOME/conf/spark-defaults.conf</code> and append the value of the <code class="literal">spark.driver.extraClassPath</code> and <code class="literal">spark.executor.extraClassPath</code> parameters with the location of your JSON jar file, that is, <code class="literal">$SPARK_HOME/lib/json-simple-1.1.1.jar</code>.</p></li><li><p>Assuming your Spark cluster is down, let's bring up our Spark cluster by performing the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Bring up our Spark master by executing the following command on your Linux Console:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$SPARK_HOME/sbin/start-master.sh</strong></span>
</pre></div></li><li><p>Next, bring <a id="id433" class="indexterm"></a>up our Spark worker by executing the following command on your Linux console:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$SPARK_HOME/bin/spark-class org.apache.spark.deploy.worker.Worker &lt;Spark Master URL&gt; &amp;</strong></span>
</pre></div></li><li><p>Execute the following command to bring up our Flume agent:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$FLUME_HOME/bin/flume-ng agent --conf conf --conf-file $FLUME_HOME/conf/spark-flume.conf --name a1 &amp;</strong></span>
</pre></div></li><li><p>Compile and export our <code class="literal">Spark-Examples</code> project and create a JAR file named <code class="literal">Spark-Examples.jar</code>.</p></li><li><p>Execute the following command to simulate the log generation in real time from the folder where our <code class="literal">Spark-Examples.jar</code> file is saved:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -classpath Spark-Examples.jar chapter.four.SampleLogGenerator &lt;path of file for saving log file&gt; &lt;path of "access_log" file&gt; &amp; </strong></span>
</pre></div></li></ol></div><p>Our Spark environment is ready. Next, we will move forward and execute our Spark job for consuming events and transform them into DataFrames.</p></li><li><p>Execute the following command on Linux console for executing our Spark job:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$SPARK_HOME/bin/spark-submit --class chapter.six.ScalaQueryingStreams --master &lt;Spark-master-URL&gt; Spark-Examples.jar</strong></span>
</pre></div><p>As soon as we execute the preceding command, the logs would be consumed and we would get the results on the console that would look similar to the following screenshot:</p><div class="mediaobject"><img src="graphics/B01793_06-06.jpg" /></div></li></ol></div><p>The<a id="id434" class="indexterm"></a> preceding screenshot shows the Schema of our log data captured in each stream of events and at the same time, it also prints the total number of unique request being served by the web server from which the log files are analyzed.</p><p>In this section, we have discussed about the integration of Spark streams with Spark SQL. We captured the streaming log data and converted them into Spark SQL DataFrames and then further executed the queries for getting the distinct requests.</p><p>Let's move forward and see the integration of Spark Streaming with Spark GraphX.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec30"></a>Graph analysis â€“ Spark GraphX</h2></div></div><hr /></div><p>In<a id="id435" class="indexterm"></a> today's world, nothing exists in isolation; everything is connected with each other and graphs are one of most efficient and widely used methodologies for representing data in the form of nodes and the relationships between them.</p><p>Graphs and graph analysis have been used by data scientists and researchers to uncover new, interesting, and hidden patterns in datasets which would otherwise be difficult to see with the naked eye.</p><p>The<a id="id436" class="indexterm"></a> social networks, network management, genealogy, public transport links, and road maps are all examples of such complex use cases that require a generic data structure to elegantly and efficiently represent the data in form of relationships, and at the same time, making them easy to query in a highly-accessible manner.</p><p>Though<a id="id437" class="indexterm"></a> there is no universal accepted definition of graphs, but as per Wikipedia (<a class="ulink" href="http://en.wikipedia.org/wiki/Graph_(abstract_data_type)" target="_blank">http://en.wikipedia.org/wiki/Graph_(abstract_data_type)</a>):</p><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>"A graph data structure consists of a finite (and possibly mutable) set of nodes or vertices, together with a set of ordered pairs of these nodes (or, in some cases, a set of unordered pairs). These pairs are known as edges or arcs. As in mathematics, an edge (x, y) is said to point or go from x to y. The nodes may be part of the graph structure, or may be external entities represented by integer indices or references."</em></span></p></blockquote></div><p>Graph analysis or graph data structure is not a new terminology, and there have been open<a id="id438" class="indexterm"></a> source <a id="id439" class="indexterm"></a>systems such as Apache Giraph (<a class="ulink" href="http://giraph.apache.org/" target="_blank">http://giraph.apache.org/</a>), GraphLab (<a class="ulink" href="https://en.wikipedia.org/wiki/GraphLab" target="_blank">https://en.wikipedia.org/wiki/GraphLab</a>), and Google's Pregel (<a class="ulink" href="http://kowshik.github.io/JPregel/pregel_paper.pdf" target="_blank">http://kowshik.github.io/JPregel/pregel_paper.pdf</a>), which exposes the specialized <a id="id440" class="indexterm"></a>APIs for simplifying graph programming. However, all these systems do not provide efficient, fault tolerant and parallel or distributed computations/transformations of graphs.</p><p>Spark introduced a unified approach for storing, processing, and transforming the data in form of graphs over its distributed in-memory data processing platform, which is known as GraphX.</p><p>GraphX is developed <a id="id441" class="indexterm"></a>on the concepts of <span class="strong"><strong>property graph model</strong></span> where data is represented in form of vertices and edges:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Vertices</strong></span>: These<a id="id442" class="indexterm"></a> are the also known as <span class="strong"><strong>nodes</strong></span> that represent <a id="id443" class="indexterm"></a>the entity and contain the attributes of that entity</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Edges</strong></span>: These <a id="id444" class="indexterm"></a>are used to define the relationships or connection between vertices/nodes in the form of lines, defining the direction of relationships</p></li></ul></div><p>Let's take an example of school where we have subjects, teachers and students and consider the following relationships between the three:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>A school provides specialization in three subjectsâ€”Science, Math, and English</p></li><li style="list-style-type: disc"><p>There are three specialized teachers for teaching these subjects:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Mary teaches Science</p></li><li style="list-style-type: disc"><p>Leena teaches English</p></li><li style="list-style-type: disc"><p>Kate teaches Math</p></li></ul></div></li><li style="list-style-type: disc"><p>There<a id="id445" class="indexterm"></a> are two students enrolled into each subject:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Science: Joseph and Adam</p></li><li style="list-style-type: disc"><p>Math: Ram and Jessica</p></li><li style="list-style-type: disc"><p>English: Brooks and Camily</p></li></ul></div></li></ul></div><p>Representing it graphically in the form of graphs would look something similar to the following illustration:</p><div class="mediaobject"><img src="graphics/B01793_06-07.jpg" /></div><p>The preceding image illustrates the visual representation of subject versus student versus teacher<a id="id446" class="indexterm"></a> relationship.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec48"></a>Introduction to the GraphX API</h3></div></div></div><p>In this<a id="id447" class="indexterm"></a> section, we will discuss about the usage of the various APIs exposed by Spark GraphX.</p><p>Let's move forward and extend the example of students/teacher/subjects and define the same model using Spark GraphX for understanding the various APIs exposed by Spark GraphX.</p><p>Perform the following steps for defining the Graph model using the Spark GraphX API:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Let's extend our <code class="literal">Spark-Examples</code> project and create a new package, <code class="literal">chapter.six</code>, and under this package, create a new Scala object and name it <code class="literal">ScalaSparkGraphx.scala</code>.</p></li><li><p>Edit <code class="literal">ScalaSparkGraphx.scala</code> and add the following packages just below the package declaration:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.graphx._
import org.apache.spark.graphx.Graph.graphToGraphOps
import org.apache.spark.rdd.RDD</pre></div></li><li><p>Next, add a <code class="literal">main</code> method and add the following piece of code within it:</p><div class="informalexample"><pre class="programlisting">def main(args: Array[String]) {

  //Creating Spark Configuration
  val conf = new SparkConf()
  conf.setAppName("My First Spark GraphX ")
  //Define Spark Context
  val sparkCtx = new SparkContext(conf)

  //Define Vertices/Nodes for Subjects
  //"parallelize()" is used to distribute a local Scala //collection to form an RDD.
  //It acts lazily, i.e. 
  //if a mutable collection is altered after invoking //"parallelize" but before
  //any invocation to the RDD operation then your resultant //RDD will contain modified collection.
  val subjects: RDD[(VertexId, (String))] = sparkCtx.parallelize(Array((1L, ("English")), (2L, ("Math")),(3L, ("Science"))))

  //Define Vertices/Nodes for Teachers
  val teachers: RDD[(VertexId, (String))] = sparkCtx.parallelize(Array((4L, ("Leena")), (5L, ("Kate")),(6L, ("Mary"))))
  //Define Vertices/Nodes for Students
  val students: RDD[(VertexId, (String))] = sparkCtx.parallelize(Array((7L, ("Adam")), (8L, ("Joseph")),(9L, ("Jessica")),(10L, ("Ram")),(11L, ("brooks")),(12L, ("Camily"))))
  //Join all Vertices and create 1 Vertex
  val vertices = subjects.union(teachers).union(students)

  //Define Edges/Relationships between Subject versus //Teachers
  val subjectsVSteachers: RDD[Edge[String]] = sparkCtx.parallelize(Array(Edge(4L,1L, "teaches"), Edge(5L,2L, "teaches"),Edge(6L, 3L, "teaches")))

  //Define Edges/Relationships between Subject vs Students
  val subjectsVSstudents: RDD[Edge[String]] = sparkCtx.parallelize(
  Array(Edge(7L, 3L, "Enrolled"), Edge(8L, 3L, "Enrolled"),Edge(9L, 2L, "Enrolled"),Edge(10L, 2L, "Enrolled"),Edge(11L, 1L, "Enrolled"),
  Edge(12L, 1L, "Enrolled"))) 
  //Join all Edges and create 1 Edge  
  val edges = subjectsVSteachers.union(subjectsVSstudents)
  //Define Object of Graph
  val graph = Graph(vertices, edges)

  //Print total number of Vertices and Edges
  println("Total vertices = " + graph.vertices.count()+", Total Edges = "+graph.edges.count())

  //Print Students and Teachers associated with Subject = //Math
  println("Students and Teachers associated with Math....")
  //EdgeTriplet represents an edge along with the vertex //attributes of its neighboring vertices.triplets
  graph.triplets.filter(f=&gt;f.dstAttr.equalsIgnoreCase("Math")).collect().foreach(println)

  sparkCtx.stop()
}</pre></div><p>We are done! Let's move ahead and execute the preceding piece of code and see the results on Spark console.</p></li><li><p>Compile<a id="id448" class="indexterm"></a> your Scala code, export it as JAR file, and name it <code class="literal">Spark-Examples.jar</code>.</p></li><li><p>Assuming your Spark cluster is up and running, open your Linux console and browse <code class="literal">$SPARK_HOME</code> and execute the following command for deploying your Spark Streaming job:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$SPARK_HOME/bin/spark-submit --class chapter.six.ScalaSparkGraphx --master &lt;SPARK-MASTER-URL&gt; Spark-Examples.jar</strong></span>
</pre></div><p>As soon as you click on <span class="emphasis"><em>Enter</em></span> and execute the preceding command, you will see that your graph is created; the queries will be executed on your graph data model and will produce the results, which would look something similar to the following screenshot showing the output of the queries:</p><div class="mediaobject"><img src="graphics/B01793_06-08.jpg" /></div></li></ol></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note55"></a>Note</h3><p>For<a id="id449" class="indexterm"></a> more information on Spark GraphX, refer to <a class="ulink" href="https://spark.apache.org/docs/1.3.0/graphx-programming-guide.html" target="_blank">https://spark.apache.org/docs/1.3.0/graphx-programming-guide.html</a>.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec49"></a>Integration with Spark Streaming</h3></div></div></div><p>Spark<a id="id450" class="indexterm"></a> GraphX does not provide any direct integration with Spark Streaming, but GraphX, at high level, extends the Spark RDD by introducing the resilient distributed property graph: a directed multigraph with properties attached to each vertex and edge.</p><p>Same goes for DStreams too, where they are nothing more than a continuous sequence of RDDs (of the same type), representing a continuous stream of data. So, the integration between streaming and GraphX is possible via RDDs. In DStream, RDDs can be in transformed into vertices and edges and further, they can be directly used to create the Graphs for every chunk of data received or for the data received within a window.</p><p>For example, consider our distributed log processing/analysis example, which was introduced in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Processing Distributed Log Files in Real Time</em></span>, where we consume the events from distributed sources using Spark-Flume sink. We can further extend this example where we can create the graph of each chunk of data received from the Flume streams in a window where each line of event/log can be unwrapped/parsed and all the attributes can be converted into vertices and further, all the vertices can be connected to one of the attributes (let's say, "IP"), resulting in the edges.</p><p>This is exactly what we will be doing where we will create the graph of all the requests received in a particular time window (as defined by our streams) from distinct IP addresses. This will not only help us to analyze the various requests received from different users/IPs, but at the same time, we can also analyze the popular pages by the number of time they have been requested by the various users in a specific time window.</p><p>The following illustration shows the high-level architecture of our distributed log processing example, which is extended and converted into Graphs using the Spark GraphX APIs:</p><div class="mediaobject"><img src="graphics/B01793_06-09.jpg" /></div><p>Graphs<a id="id451" class="indexterm"></a> are really helpful for analyzing the hidden patterns and trends that are not possible to uncover with the naked eye, not even with SQL. Even in our distributed log processing example, we can uncover the hidden patterns or changing trends by evaluating the URLs requested by the various users over the period of time. This would tell us the changing behaviors of our prospective users or also the increase or decline in activities of the various users visiting our website.</p><p>Let's move forward and perform the following steps for extending our distributed log processing example and convert chunk of log events into graphs:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Extend our <code class="literal">Spark-Examples</code> project and create a Scala object: <code class="literal">ScalaCreateStreamingGraphs.scala</code> in package <code class="literal">chapter.six</code>.</p></li><li><p>Next, edit <code class="literal">ScalaCreateStreamingGraphs.scala</code> and add the following <code class="literal">import</code> statements:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark._
import org.apache.spark.streaming._
import org.apache.spark.graphx._
import org.apache.spark.rdd.RDD
import org.apache.spark.streaming.flume._
import org.apache.spark.storage.StorageLevel
import org.apache.spark.rdd._
import org.apache.spark.streaming.dstream._
import java.net.InetSocketAddress
import chapter.four.ScalaLogAnalyzer</pre></div></li><li><p>Next, open and edit <code class="literal">chapter.four.ScalaLogAnalyzer</code> and define the utility <a id="id452" class="indexterm"></a>method, <code class="literal">transformIntoGraph(â€¦)</code>, which process the array of Flume events and creates vertices and edges:</p><div class="informalexample"><pre class="programlisting">  /**
  * Utility method for transforming Flume Events into Sequence of Vertices and Edges  
  */
  def transformIntoGraph(eventArr: Array[SparkFlumeEvent]): Tuple2[Set[(VertexId, (String))],Set[Edge[String]]] = {
    println("Start Transformation........")
    //Defining mutable Sets for holding the Vertices and //Edges
    val verticesSet: scala.collection.mutable.Set[(VertexId,String)] = scala.collection.mutable.Set()
    val edgesSet: scala.collection.mutable.Set[Edge[String]] = scala.collection.mutable.Set()

    //Creating Map of IP and Vertices ID, 
    //so that we create Edges to the same IP 
    var ipMap:Map[String,Long] = Map()

    //Looping over the Array of Flume Events
    for(event&lt;-eventArr){
      //Get the Line of Log and Transform into //Attribute Map
      val eventAttrMap = tansfromLogData(new String(event.event.getBody().array()))

      //Using Random function for defining Unique Vertices //ID's
      //Creating Vertices for IP
      //Creating new or Getting existing VertexID for //IP coming from Events
      val ip_verticeID:Long = if(ipMap.contains(eventAttrMap.get("IP").get)){
        ipMap.get(eventAttrMap.get("IP").get).get
        
      }
      else{
        //Using Random function for defining Unique //Vertex ID's
        val id = Random.nextLong()
        //Add to the Map
        ipMap+= (eventAttrMap.get("IP").get -&gt; id)
        //Return the Value
        id
      }
      //Add Vertex for IP
      verticesSet+=((ip_verticeID,"IP="+eventAttrMap.get("IP")))
      //Creating Vertex for Request
      val request_verticeID = Random.nextLong()
      verticesSet+=((request_verticeID,"Request="+eventAttrMap.get("request")))
      //Creating Vertice for Date
      val date_verticeID = Random.nextLong()
      verticesSet+=((date_verticeID,"Date="+eventAttrMap.get("date")))
      //Creating Vertice for Method      
      val method_verticeID = Random.nextLong()
      verticesSet+=((method_verticeID,"Method="+eventAttrMap.get("method")))
      //Creating Vertice for Response Code      
      val respCode_verticeID = Random.nextLong()
      verticesSet+=((respCode_verticeID,"ResponseCode="+eventAttrMap.get("respCode")))
      
      //Defining Edges. All parameters are //in relation to the User IP 
      edgesSet.+=(Edge(ip_verticeID,request_verticeID,"Request")).+=(Edge(ip_verticeID,date_verticeID,"date"))
      edgesSet.+=(Edge(ip_verticeID,method_verticeID,"methodType")).+=(Edge(ip_verticeID,respCode_verticeID,"responseCode")
        }
        println("End Transformation........")
    
        //Finally Return the Tuple of 2 Set containing Vertices and Edges
        return  (verticesSet,edgesSet)
 
        }</pre></div></li><li><p>Edit <code class="literal">ScalaCreateStreamingGraphs.scala</code> and within the <code class="literal">main</code> method, add<a id="id453" class="indexterm"></a> the following piece of code:</p><div class="informalexample"><pre class="programlisting">def main(args: Array[String]) {

  //Creating Spark Configuration
  val conf = new SparkConf()
  conf.setAppName("Integrating Spark Streaming with GraphX")
  //Define Spark Context which we will use to initialize //our SQL Context 
  val sparkCtx = new SparkContext(conf)
  //Retrieving Streaming Context from Spark Context
  val streamCtx = new StreamingContext(sparkCtx, Seconds(10))
  //Address of Flume Sink
  var addresses = new Array[InetSocketAddress](1);
  addresses(0) = new InetSocketAddress("localhost",4949)
  //Creating a Stream
  val flumeStream = FlumeUtils.createPollingStream(streamCtx,addresses,StorageLevel.MEMORY_AND_DISK_SER_2,1000,1)
  //Define Window Function to collect Event for a certain //Duration    
  val newDstream = flumeStream.window(Seconds(40),Seconds(20))
  
  //Define Utility class for Transforming Log Data
  val transformLog = new ScalaLogAnalyzer()
  //Create Graphs for each RDD
  val graphStream = newDstream.foreachRDD { x =&gt; 
    //Invoke utility Method for Transforming Events into //Graphs Vertices and Edges
    //Wrapped in a Mutable Seq
    val tuple = transformLog.transformIntoGraph(x.collect())
    println("Creating Graphs Now..................")
    //Define Vertices 
    val vertices:RDD[(VertexId, (String))] = sparkCtx.parallelize(tuple._1.toSeq)
    //Define Edges
    val edges:RDD[Edge[String]] = sparkCtx.parallelize(tuple._2.toSeq)
    //Create or Initialize Graph
    val graph = Graph(vertices,edges)
    //Print total number of Vertices and Edges in the Graph
    println("Total vertices = " + graph.vertices.count()+", Total Edges = "+graph.edges.count())
    //Printing All Vertices in the Graph
    graph.vertices.collect().iterator.foreach(f=&gt;println("Vertex-ID = "+f._1+", Vertex-Name = "+f._2))
    //Printing Requests from Distinct IP's in this Window
    println("Printing Requests from Distinct IP's in this Window")
    println("Here is the Count = "+graph.vertices.filter ( x =&gt; x._2.startsWith("IP")).count())
    println("Here is the Distinct IP's = ")
    graph.vertices.filter ( x =&gt; x._2.startsWith("IP")).collect.foreach(ip=&gt;println(ip._2))
   
    //Printing count of Distinct URL requested in this Window
    println("Printing count of Distinct URL requested in this Window")
    val filteredRDD = graph.vertices.filter ( x =&gt; x._2.startsWith("Request=")).map(x =&gt; (x._2, 1)).reduceByKey(_+_)
    filteredRDD.collect.foreach(url=&gt;println(url._1+" = " + url._2))
  }
  
  streamCtx.start();
  streamCtx.awaitTermination();  
}</pre></div></li></ol></div><p>Our<a id="id454" class="indexterm"></a> integration is completed. Now, for every chunk of data received, the preceding piece of code will create a graph that can be further analyzed or queried. Now, for running the example, first we have to bring up our cluster and follow the same steps as we did in the previous section, <span class="emphasis"><em>Integrating Spark SQL with streams</em></span>. Once the cluster is up and running we can execute the following command on our Linux console:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$SPARK_HOME/bin/spark-submit --class chapter.six.ScalaCreateStreamingGraphs --master &lt;Spark-master-URL&gt; Spark-Examples.jar</strong></span>
</pre></div><p>As soon as we execute this command, the logs would be consumed and we would get the results on the console that would look similar to the following screenshot:</p><div class="mediaobject"><img src="graphics/B01793_06-10.jpg" /></div><p>The <a id="id455" class="indexterm"></a>preceding screenshot shows the number of vertices and edges created after parsing the log data and at the same time, it prints <code class="literal">Vertex-ID</code> and <code class="literal">Vertex-Name</code> on the console. It also shows the request received from distinct IP addresses and the count of distinct URLs.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note56"></a>Note</h3><p>We can extend our example, where we can persist the vertices and edges in Spark memory or on some persistent storage area such as HDFS across the streaming time windows and further join them with the data received in the subsequent time windows to create one single graph for each user/IP address.</p></div><p>In this section, we discussed about the integration of Spark streams with Spark GraphX. We captured the streaming log data and converted them into graphs using the Spark GraphX APIs and then further executed the few basic queries.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec31"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we discussed about the integration of advanced spark libraries like Spark SQL and Spark GraphX with Spark Streaming. We also extended our distributed log analysis example and applied the Spark SQL and GraphX APIs over the distributed events received in near real time.</p><p>In the next chapter, we will discuss the various deployment aspects of Spark and Spark Streaming.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch07"></a>ChapterÂ 7.Â Deploying in Production</h2></div></div></div><p>If you can't get software into the hands of your users, then what is its value? Zero, isn't it?</p><p>That's exactly the reason that every type of software at the end of the day needs to be deployed in production, and that's where the real challenge is!</p><p>Production deployment is a complex endeavor, which is one of the most critical aspects of any <span class="strong"><strong>software development life cycle</strong></span> (<span class="strong"><strong>SDLC</strong></span>). Development teams are constantly striving<a id="id456" class="indexterm"></a> to deploy software in such a way that various enterprise concerns such as maintenance, backups, restores, and disaster recovery are easier to perform and flexible enough to scale and accommodate the future needs. Architects/developers envision their production environment/deployments from day-1 (sometimes, even before) of the project kick-off.</p><p>Though application software or frameworks do provide various deployment options, which one to use and how to use will largely depend upon the usage of software by the end users.</p><p>For example, you may have strict SLAs for consuming/processing near real-time feeds in comparison to your batch processes. So, your deployments will be configured and optimized in such a manner that consumption or processing of your near real-time feeds will always take precedence over your batch processes.</p><p>We should also remember that the production deployments are evolving and would change over the period of time due to many reasons, such as new versions, innovation in hardware, change in user behavior, business dynamics, and others.</p><p>The same is true for Spark and Spark-based applications, but lately, there have been a lot of rumors about Spark and its production readiness and if this was not enough, we must have heard people categorizing Spark and Mesos as one and the same. Funny, isn't it?</p><p>That's true that the idea of Spark was originated while working on Apache Mesos at the University of California, Berkeley, but both are not the same. Apache Mesos is an open source cluster manager that provides efficient resource isolation and sharing across the distributed applications or frameworks, while Apache Spark is an open source cluster computing framework that requires an efficient cluster manager such as Apache Mesos or YARN and a distributed storage System such as HDFS. In a nutshell, Apache Mesos and Spark are not the same, they are different!</p><p>As far as production readiness is concerned, there are more than 80 companies already using Spark <a id="id457" class="indexterm"></a>in production and the list is growing day by day. You can find more about it at <a class="ulink" href="https://cwiki.apache.org/confluence/display/SPARK/Powered+By+Spark" target="_blank">https://cwiki.apache.org/confluence/display/SPARK/Powered+By+Spark</a>.</p><p>Let's move forward and read more about the deployment aspects and the concerns of Spark and Spark-based applications.</p><p>This chapter will cover the following points:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Spark deployment models</p></li><li style="list-style-type: disc"><p>High availability and fault tolerance</p></li><li style="list-style-type: disc"><p>Monitoring streaming jobs</p></li></ul></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec32"></a>Spark deployment models</h2></div></div><hr /></div><p>Spark is <a id="id458" class="indexterm"></a>developed as a framework that can be further deployed on the various distributed cluster computing frameworks such as Hadoop/YARN, Apache Mesos, and standalone too.</p><p>Spark specifies the integration hooks that can be extended and integrated in such a manner that our Spark applications can leverage the cluster manager of other distributed cluster computing frameworks that may provide efficient resource isolation and sharing across distributed applications.</p><div class="mediaobject"><img src="graphics/B01793_07_01.jpg" /></div><p>The preceding illustration shows different deployment options available for deploying Spark and Spark-based applications.</p><p>In the <a id="id459" class="indexterm"></a>previous chapters, we executed our Spark and Spark Streaming jobs in standalone cluster mode provided by Spark itself.</p><p>Let's move forward and discuss the deployment steps for deploying our Spark applications on other cluster computing frameworks such as Apache Mesos and YARN.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec50"></a>Deploying on Apache Mesos</h3></div></div></div><p>Apache Mesos (<a class="ulink" href="http://mesos.apache.org/" target="_blank">http://mesos.apache.org/</a>) is a cluster manager that provides efficient<a id="id460" class="indexterm"></a> resource isolation <a id="id461" class="indexterm"></a>and sharing across distributed applications or frameworks. It <a id="id462" class="indexterm"></a>can run Hadoop, MPI, Hypertable, Spark, and other frameworks on a dynamically shared pool of nodes.</p><p>Apache Mesos and Spark are closely related to each other (but they are not same). The story started back in 2009 when Mesos was ready and there were thoughts going on about the ideas/framework that can be developed on top of Mesos, and that's exactly how Spark was born.</p><p>The objective was to showcase how easy it was to build a framework from scratch in Mesos and at the same time, the target was to support interactive and iterative computations like machine learning and also provide ad hoc querying.</p><p>Mesos completely abstracts out compute resources such as CPU, memory, and others from the machines (physical or virtual) and enables the fault tolerant and elastic distribution of the compute resources. Mesos maintains a pool of compute resources that are allocated to the applications as per the demand/request.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl3sec28"></a>Installing and configuring Apache Mesos</h4></div></div></div><p>Let's move<a id="id463" class="indexterm"></a> ahead and install Apache Mesos and its dependent<a id="id464" class="indexterm"></a> components. Then, we will also deploy and execute our Spark examples on the same cluster.</p><p>Perform the following steps for installing Apache Mesos and its dependent components on Linux:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Execute<a id="id465" class="indexterm"></a> the following commands to install Apache Maven (<a class="ulink" href="https://maven.apache.org/" target="_blank">https://maven.apache.org/</a>):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>wget http://mirror.olnevhost.net/pub/apache/maven/maven-3/3.0.5/binaries/apache-maven-3.0.5-bin.tar.gz</strong></span>
<span class="strong"><strong>tar xvf apache-maven-3.0.5-bin.tar.gz</strong></span>
<span class="strong"><strong>export M2_HOME=&lt;path of installation Directory&gt;</strong></span>
<span class="strong"><strong>export M2=$M2_HOME/bin</strong></span>
<span class="strong"><strong>export PATH=$M2:$PATH</strong></span>
</pre></div></li><li><p>Next, execute the following Linux commands for installing Apache Mesos dependencies:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo yum install  -y autoconf libtool</strong></span>
<span class="strong"><strong>sudo yum install -y build-essential python-dev python-boto libcurl4-nss-dev libsasl2-dev maven libapr1-dev libsvn-dev</strong></span>
<span class="strong"><strong>sudo yum groupinstall -y "Development Tools"</strong></span>
<span class="strong"><strong>sudo yum install -y python-devel  zlib-devel libcurl-devel openssl-devel cyrus-sasl-devel cyrus-sasl-md5 apr-devel subversion-devel apr-util-devel</strong></span>
</pre></div></li><li><p>Now, execute the following commands on your Linux console for downloading and extracting Apache Mesos; let's refer the directory, in which we will extract Mesos, as <code class="literal">&lt;MESOS_HOME&gt;</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>wget http://www.apache.org/dist/mesos/0.22.1/mesos-0.22.1.tar.gz</strong></span>
<span class="strong"><strong>tar -zxf mesos-0.22.1.tar.gz</strong></span>
</pre></div></li><li><p>Then, execute the following Linux commands for compiling the Mesos libraries and creating a build:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>mkdir &lt;MESOS_HOME&gt;/build</strong></span>
<span class="strong"><strong>cd &lt;MESOS_HOME&gt;/build</strong></span>
<span class="strong"><strong>../configure</strong></span>
<span class="strong"><strong>Make</strong></span>
</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note57"></a>Note</h3><p>Replace <code class="literal">&lt;MESOS_HOME&gt;</code> with the actual path of the directory in all the instances.</p></div><p>We are done with the installation. Our Apache Mesos binaries are compiled and are ready to be used.</p></li><li><p>In <code class="literal">&lt;MESOS_HOME&gt;/build/</code>, execute the following command on Linux console to start the Mesos master:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>mkdir workdir</strong></span>
<span class="strong"><strong>./bin/mesos-master.sh --cluster='Spark Cluster' --work_dir=&lt;MESOS_HOME&gt;/build/workdir &amp;</strong></span>
</pre></div></li><li><p>Next, bring <a id="id466" class="indexterm"></a>up the Mesos slave by executing<a id="id467" class="indexterm"></a> the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>/bin/mesos-slave.sh --master=&lt;IP of the server&gt;:5050</strong></span>
</pre></div><p>The <code class="literal">&lt;IP of the server&gt;</code> is the IP of your local machine that is binded by Mesos master. In case you have more than one network interfaces, you can also explicitly specify the IP by providing the <code class="literal">--ip=&lt;IP-Address&gt;</code> option while bringing up your master. Alternatively, you can also see this IP on the master UI (<code class="literal">http://&lt;host-name&gt;:5050</code>).</p><p>As soon as you execute the preceding command, your Apache Mesos cluster is up and running with one master and one slave and is ready to accept the request. We can browse the Mesos master at <code class="literal">http://&lt;host-name&gt;:5050</code>. It would look similar to the following screenshot:</p><div class="mediaobject"><img src="graphics/B01793_07-02.jpg" /></div><p>The preceding screenshot shows the Mesos master UI. It is also known as admin console or monitoring console where we can monitor all the active and completed jobs/tasks and of course the health <a id="id468" class="indexterm"></a>of the cluster can also be monitored <a id="id469" class="indexterm"></a>from the same UI.</p></li></ol></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note58"></a>Note</h3><p>For<a id="id470" class="indexterm"></a> more information on Mesos, refer to the official documentation at <a class="ulink" href="http://mesos.apache.org/documentation/latest/" target="_blank">http://mesos.apache.org/documentation/latest/</a>.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl3sec29"></a>Integrating and executing Spark applications on Apache Mesos</h4></div></div></div><p>Perform <a id="id471" class="indexterm"></a>the following steps for integrating <a id="id472" class="indexterm"></a>and submitting Spark applications on Apache Mesos cluster:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Execute the following command on your Linux console for setting the environment variable for Mesos native library, <code class="literal">libmesos.so</code>, which will be used by Java runtime for executing our Spark jobs:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>export MESOS_NATIVE_JAVA_LIBRARY=&lt;Path &gt;</strong></span>
</pre></div><p>By default, <code class="literal">libmesos.so</code> can be found at <code class="literal">/usr/local/lib/libmesos.so</code>, but if it is not found at the default location, you can find it at <code class="literal">&lt;MESOS_HOME&gt;/build/src/.libs/libmesos.so</code>.</p></li><li><p>Next, open <code class="literal">&lt;SPARK_HOME&gt;/conf/spark-defaults.conf</code> and define the variable, <code class="literal">spark.executor.uri=</code>. The value of this variable will be the location of the Spark binaries, either accessed via <code class="literal">http://</code> or <code class="literal">hdfs://</code> (Hadoop) or <code class="literal">s3://</code> (Amazon S3 at <a class="ulink" href="http://aws.amazon.com/s3/" target="_blank">http://aws.amazon.com/s3/</a>).</p></li><li><p>This <a id="id473" class="indexterm"></a>variable is required for Mesos slave nodes. These nodes require Spark binaries for executing Spark jobs. Our Spark jobs will fail in case we do not provide the correct URL of the Spark binaries. Since <code class="literal">http</code> is the simplest one, we will directly reference the Spark binaries available at the Spark website. Define the following variable and its value in the <code class="literal">&lt;SPARK_HOME&gt;/conf/spark-defaults.conf</code> file:</p><div class="informalexample"><pre class="programlisting">spark.executor.uri= http://d3kbcqa49mib13.cloudfront.net/spark-1.3.0-bin-hadoop2.4.tgz</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note59"></a>Note</h3><p>In production systems, it is recommended to upload Spark binaries on HDFS, which should be in same network/subnet as of Mesos slave nodes.</p></div></li><li><p>Finally, we <a id="id474" class="indexterm"></a>will execute<a id="id475" class="indexterm"></a> our Spark job in the same manner as we did it before using <code class="literal">spark-submit</code>, where everything remains the same except the URL of the <code class="literal">--master</code> parameter needs to be changed to the master node of the Mesos cluster. For example, the samples from the <span class="emphasis"><em>Your first Spark program</em></span> section of <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Installing and Configuring Spark and Spark Streaming</em></span>, can be submitted to Mesos cluster by executing the following command on our Linux console from the location where we have saved/exported our <code class="literal">Spark-Examples</code> project:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$SPARK_HOME/bin/spark-submit --class chapter.one.ScalaFirstSparkExample --master mesos://&lt;Host-Name&gt;:5050 Spark-Examples.jar</strong></span>
</pre></div></li><li><p>As soon as we execute the preceding command, we will see the logs coming on the console, and at the same time, the <span class="strong"><strong>Framework</strong></span> tab of the Mesos monitoring console will also tell us the status of our job that would be similar to the following screenshot:</p><div class="mediaobject"><img src="graphics/B01793_07-03.jpg" /></div><p>The preceding screenshot <a id="id476" class="indexterm"></a>shows the output <a id="id477" class="indexterm"></a>of our Spark job on the console. The following illustrations shows the Mesos UI with the status of our Spark job; the Spark Streaming jobs/applications can also be executed using the same process without any changes:</p><div class="mediaobject"><img src="graphics/B01793_07-04.jpg" /></div></li></ol></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note60"></a>Note</h3><p>In Spark 1.3, Spark driver can only be executed (using <code class="literal">spark-submit</code>) in the client mode, but in the recently released Spark 1.4.0 (June 2015), it can also be executed <a id="id478" class="indexterm"></a>in the cluster mode. The output can be seen on Mesos UI at <a class="ulink" href="https://spark.apache.org/docs/1.4.0/running-on-mesos.html#cluster-mode" target="_blank">https://spark.apache.org/docs/1.4.0/running-on-mesos.html#cluster-mode</a>.</p></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec51"></a>Deploying on Hadoop or YARN</h3></div></div></div><p>Hadoop 2.0 <a id="id479" class="indexterm"></a>aka YARN was a complete <a id="id480" class="indexterm"></a>change in the architecture. It <a id="id481" class="indexterm"></a>was introduced as a generic cluster computing <a id="id482" class="indexterm"></a>framework, entrusted with the responsibility for allocating and managing the resources required to execute the varied jobs or applications. It<a id="id483" class="indexterm"></a> introduced new <a id="id484" class="indexterm"></a>daemon <a id="id485" class="indexterm"></a>services such as <span class="strong"><strong>resource manager</strong></span> (<span class="strong"><strong>RM</strong></span>), <span class="strong"><strong>node manager</strong></span> (<span class="strong"><strong>NM</strong></span>), and <span class="strong"><strong>application master</strong></span> (<span class="strong"><strong>AM</strong></span>), which are responsible for managing cluster resources, individual nodes, and respective applications.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note61"></a>Note</h3><p>For <a id="id486" class="indexterm"></a>more information on the architecture of YARN, please refer to <a class="ulink" href="http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html" target="_blank">http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html</a>.</p></div><p>YARN also introduced specific interfaces/guidelines for application developers where they can implement/follow and submit or execute their custom applications on the YARN cluster.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note62"></a>Note</h3><p>For <a id="id487" class="indexterm"></a>more information on executing custom applications on YARN, refer to <a class="ulink" href="http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/WritingYarnApplications.html" target="_blank">http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/WritingYarnApplications.html</a>.</p></div><p>The Spark framework implements the interfaces exposed by YARN and provides the flexibility of executing the Spark applications on YARN. Spark applications can be executed in two different modes in YARN:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>YARN client mode</strong></span>: In this mode, the Spark driver executes the client machine (the machine used for submitting the job), and the YARN application <a id="id488" class="indexterm"></a>master is just used for requesting the resources from YARN. The behavior of Spark application is the same as we have experienced in Mesos or standalone mode where all our logs and sysouts (<code class="literal">println</code>) are printed on the same console which is used for submitting the job.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>YARN cluster mode</strong></span>: In this mode, the Spark driver runs inside the YARN application<a id="id489" class="indexterm"></a> master process, which is further managed by YARN on the cluster, and the client can go away just after submitting the application. Now as our Spark driver is executed on the YARN cluster, our application logs/sysouts (<code class="literal">println</code>) are also written in the log files maintained by YARN and not on the machine that is used to submit our Spark job.</p></li></ul></div><p>Let's<a id="id490" class="indexterm"></a> move forward and execute <a id="id491" class="indexterm"></a>our Spark jobs on YARN. Perform the following steps for integrating and submitting Spark applications on YARN:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Download<a id="id492" class="indexterm"></a> Hadoop 2.4 from <a class="ulink" href="https://archive.apache.org/dist/hadoop/common/hadoop-2.4.0/hadoop-2.4.0.tar.gz" target="_blank">https://archive.apache.org/dist/hadoop/common/hadoop-2.4.0/hadoop-2.4.0.tar.gz</a>.</p></li><li><p>Extract TAR file to any of the directories and execute the following command on your Linux console:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>export HADOOP_HOME = &lt;path of the directory where Hadoop is extracted &gt;</strong></span>
<span class="strong"><strong>export HADOOP_CONF_DIR = $HADOOP_HOME /etc/hadoop</strong></span>
</pre></div></li><li><p>Next, we will set up Hadoop and YARN cluster. Hadoop/YARN can be set up in three different modes:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Standalone mode</strong></span>: To set up YARN<a id="id493" class="indexterm"></a> in this <a id="id494" class="indexterm"></a>mode, perform the steps provided at <a class="ulink" href="http://hadoop.apache.org/docs/r2.4.1/hadoop-project-dist/hadoop-common/SingleCluster.html#Standalone_Operation" target="_blank">http://hadoop.apache.org/docs/r2.4.1/hadoop-project-dist/hadoop-common/SingleCluster.html#Standalone_Operation</a>.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Pseudo-distributed mode</strong></span>: For this <a id="id495" class="indexterm"></a>mode, perform the steps provided <a id="id496" class="indexterm"></a>at <a class="ulink" href="http://hadoop.apache.org/docs/r2.4.1/hadoop-project-dist/hadoop-common/SingleCluster.html#Pseudo-Distributed_Operation" target="_blank">http://hadoop.apache.org/docs/r2.4.1/hadoop-project-dist/hadoop-common/SingleCluster.html#Pseudo-Distributed_Operation</a>.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Fully distributed mode</strong></span>: To set up<a id="id497" class="indexterm"></a> YARN <a id="id498" class="indexterm"></a>in this mode, perform the steps at <a class="ulink" href="http://hadoop.apache.org/docs/r2.4.1/hadoop-project-dist/hadoop-common/SingleCluster.html#Fully-Distributed_Operation" target="_blank">http://hadoop.apache.org/docs/r2.4.1/hadoop-project-dist/hadoop-common/SingleCluster.html#Fully-Distributed_Operation</a>.</p></li></ul></div><p>Once<a id="id499" class="indexterm"></a> our cluster is up and running in any <a id="id500" class="indexterm"></a>of the specified modes, ensure that the following daemons/services are running in our Hadoop/YARN cluster:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Resource and node manager</strong></span>: Open a new browser window and browse <code class="literal">http://&lt;HOST-NAME/IP&gt;:8088/cluster</code>. This should show the UI of the resource manager.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Name and data node</strong></span>: Open a new browser window and browse <code class="literal">http://&lt;HOST-NAME/IP&gt;:50070/dfshealth.html</code>. This should show the UI of the name node that will provide options to browse and view Hadoop Distributed File System, data nodes, and various other Hadoop configurations.</p></li></ul></div></li><li><p>Next, we<a id="id501" class="indexterm"></a> will execute<a id="id502" class="indexterm"></a> our first Spark Streaming example developed in the <span class="emphasis"><em>Your first Spark Streaming program</em></span> section on YARN in client mode in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Architecture and Components of Spark and Spark Streaming</em></span>. Follow the same steps as defined in the example, but for submitting our Spark Streaming job, use the following command where we have replaced <code class="literal">&lt;SPARK-MASTER-URL&gt;</code> with <code class="literal">yarn-client</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$SPARK_HOME/bin/spark-submit --class chapter.two.ScalaFirstStreamingExample --master yarn-client Spark-Examples.jar</strong></span>
</pre></div></li><li><p>Although there is no change in the result or outcome of the preceding command, now we are leveraging YARN for our job execution and we can see the same in the ResourceManager UI by browsing <code class="literal">http://&lt;HOST-NAME/IP&gt;:8088/cluster</code>. This would be similar to the following screenshot:</p><div class="mediaobject"><img src="graphics/B01793_07-05.jpg" /></div><p>The preceding illustration shows the ResourceManager UI where we can see the detailed status, logs, and so on of all applications executed by YARN. You can click on <span class="strong"><strong>Tracking UI</strong></span> or <span class="strong"><strong>ID</strong></span> of the application for browsing the logs.</p></li><li><p>Next, for<a id="id503" class="indexterm"></a> executing the<a id="id504" class="indexterm"></a> same example (which we did in the previous step) in the YARN cluster mode, we need to just change <code class="literal">yarn-client</code> to <code class="literal">yarn-cluster</code> in our <code class="literal">spark-submit</code> command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$SPARK_HOME/bin/spark-submit --class chapter.two.ScalaFirstStreamingExample --master yarn-cluster Spark-Examples.jar</strong></span>
</pre></div></li><li><p>As <code class="literal">yarn-cluster</code> would execute our Spark drive in cluster mode, so all the logs printed on the console will be visible in the YARN logs and not on the console that we used to submit the job. You can browse the logs and see the results by clicking on the ID of the application on the ResourceManager UI and then on logs. The following screenshot shows the details of our Spark Streaming job on the YARN ResourceManager UI:</p><div class="mediaobject"><img src="graphics/B01793_07-06.jpg" /></div></li></ol></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note63"></a>Note</h3><p>The path of all the static or dynamic files that is processed by our Spark Streaming job should be available on HDFS. YARN will not read files from the local filesystem. For more information on deploying Spark and Spark Streaming application<a id="id505" class="indexterm"></a> on YARN and Mesos, refer to <a class="ulink" href="https://spark.apache.org/docs/1.3.0/running-on-yarn.html" target="_blank">https://spark.apache.org/docs/1.3.0/running-on-yarn.html</a> and <a class="ulink" href="https://spark.apache.org/docs/1.3.0/running-on-mesos.html" target="_blank">https://spark.apache.org/docs/1.3.0/running-on-mesos.html</a>.</p></div><p>In this<a id="id506" class="indexterm"></a> section, we discussed the steps<a id="id507" class="indexterm"></a> involved in deploying our<a id="id508" class="indexterm"></a> Spark Streaming applications in various cluster computing<a id="id509" class="indexterm"></a> frameworks. Let's move toward the next section where we will discuss about the process and options available for deploying highly available and fault tolerant Spark Streaming applications.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec33"></a>High availability and fault tolerance</h2></div></div><hr /></div><p>In this<a id="id510" class="indexterm"></a> section, we will talk about the high availability and fault<a id="id511" class="indexterm"></a> tolerance features of Spark and Spark Streaming in various <a id="id512" class="indexterm"></a>kind of deployment models.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec52"></a>High availability in the standalone mode</h3></div></div></div><p>In the <a id="id513" class="indexterm"></a>standalone mode, Spark cluster, by default, is resilient to the failure of worker nodes/processes. As soon as the worker goes down, the master chooses another available worker and schedules the jobs, but what if the master itself goes down? Will it be the single point of failure? No!</p><p>Spark<a id="id514" class="indexterm"></a> Standalone mode leverages ZooKeeper (<a class="ulink" href="http://zookeeper.apache.org/doc/trunk/zookeeperStarted.html" target="_blank">http://zookeeper.apache.org/doc/trunk/zookeeperStarted.html</a>) for leader election and provides the flexibility to create multiple/backup masters, which automatically takes up the role of master as soon as the current master goes down. In any case and at any point of time, there would be one and only one master serving the user requests.</p><p>In order<a id="id515" class="indexterm"></a> to enable the HA mode, configure <code class="literal">SPARK_DAEMON_JAVA_OPTS</code> in <code class="literal">$SPARK_HOME/conf/spark-env.sh</code> and add/append the following properties:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">spark.deploy.recoveryMode</code>: The default value is <code class="literal">NONE</code>. Set it to <code class="literal">ZOOKEEPER</code> to enable the HA mode.</p></li><li style="list-style-type: disc"><p>
<code class="literal">spark.deploy.zookeeper.url</code>: Provide the comma-separated <code class="literal">IP:PORT</code> of nodes where we have configured the ZooKeeper cluster. For example, <code class="literal">192.168.1.10:2181</code>, <code class="literal">192.168.1.11:2181</code>.</p></li><li style="list-style-type: disc"><p>
<code class="literal">spark.deploy.zookeeper.dir</code>: The directory in ZooKeeper to store recovery state (the default value is <code class="literal">spark</code>).</p></li></ul></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note64"></a>Note</h3><p>Ensure that we configure masters correctly with ZooKeeper instances; incorrect configurations may lead to undesirable results where masters may be unable to discover each other and every node will assume itself as master and start scheduling the jobs on worker nodes.</p></div><p>Once our master nodes are configured with ZooKeeper and our cluster is up and running, the client applications or worker process need not to worry about the current master node. They can specify the comma-separated list of all the configured master nodes in SparkContext so that our applications is registered with all the configured master nodes (<code class="literal">spark://host:port,host1:port1â€¦</code>), and in case one of the master nodes goes down, the configuration remains valid then too.</p><p>Once <a id="id516" class="indexterm"></a>the application registers itself with any of the active masters, the failovers are seamless and automatic; the application does not have to do anything.</p><p>The way it works is that the registration details of all applications is stored within the ZooKeeper and in case the current master goes down, the new master reads all the configurations including details about the registered applications from ZooKeeper and informs all the previously registered applications and workers about the change in leadership. So, the applications now do not have to worry about the current master nodes, and at the same time, it also provides the flexibility where we can bring up master nodes at any <a id="id517" class="indexterm"></a>point of time without changing anything in the applications. </p><p>The following illustration shows the overall HA architecture process for the standalone deployment mode:</p><div class="mediaobject"><img src="graphics/B01793_07_07.jpg" /></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note65"></a>Note</h3><p>For <a id="id518" class="indexterm"></a>more information on HA in the standalone mode, refer to <a class="ulink" href="https://spark.apache.org/docs/1.3.0/spark-standalone.html" target="_blank">https://spark.apache.org/docs/1.3.0/spark-standalone.html</a>.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec53"></a>High availability in Mesos or YARN</h3></div></div></div><p>High<a id="id519" class="indexterm"></a> availability in Apache Mesos and YARN works in similar<a id="id520" class="indexterm"></a> manner as it does in the standalone mode. The overall architecture of achieving high availability also remains the same where Mesos/YARN also leverages ZooKeeper and provides the flexibility of having backup master/resource manager nodes, which can be automatically elected as leader by ZooKeeper in case of any failures with the current master node.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note66"></a>Note</h3><p>For <a id="id521" class="indexterm"></a>more information on HA in Apache Mesos and YARN, refer to <a class="ulink" href="http://mesos.apache.org/documentation/latest/high-availability/" target="_blank">http://mesos.apache.org/documentation/latest/high-availability/</a> and <a class="ulink" href="http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html" target="_blank">http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html</a>.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec54"></a>Fault tolerance</h3></div></div></div><p>Fault tolerance is<a id="id522" class="indexterm"></a> one of the critical architectures and design principles that enables the smooth functioning of any software system without any data loss, which may happen due to the malfunctioning of any component of the system.</p><p>As per<a id="id523" class="indexterm"></a> Wikipedia (<a class="ulink" href="https://en.wikipedia.org/wiki/Fault_tolerance" target="_blank">https://en.wikipedia.org/wiki/Fault_tolerance</a>):</p><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>"Fault tolerance is the property that enables a system to continue operating properly in the event of the failure of (or one or more faults within) some of its components."</em></span></p></blockquote></div><p>There can be various types of faults within a system, for example, hardware failures, server crashes, software bugs, physical damages, or other flaws introduced to the system from outside/external sources. The objective of any fault tolerant system is to identify and design system in such a manner that it is capable enough to gracefully handle the identified or unidentified possibility of errors that may occur in future.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note67"></a>Note</h3><p>For more <a id="id524" class="indexterm"></a>information on fault tolerance in distributed systems, refer to <a class="ulink" href="https://en.wikipedia.org/wiki/Fault-tolerant_computer_system" target="_blank">https://en.wikipedia.org/wiki/Fault-tolerant_computer_system</a> and <a class="ulink" href="http://www-itec.uni-klu.ac.at/~laszlo/courses/DistSys_BP/FaultTolerance.pdf" target="_blank">http://www-itec.uni-klu.ac.at/~laszlo/courses/DistSys_BP/FaultTolerance.pdf</a>.</p></div><p>Let's move forward and discuss the various intrinsic properties and features of Spark/Spark Streaming that provides fault tolerance.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl3sec30"></a>Fault tolerance in Spark Streaming</h4></div></div></div><p>Processing <a id="id525" class="indexterm"></a>of streaming data in near real-time involves various components where we need to ensure that appropriate fault tolerance is already available or can be applied. Let's discuss the each of these components and their resiliency toward the various failures:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>DStreams</strong></span>: DStreams are nothing more than a wrapper around the continuous <a id="id526" class="indexterm"></a>series of <span class="strong"><strong>RDDs</strong></span> (<span class="strong"><strong>Resilient Distributed Datasets</strong></span>). RDDs<a id="id527" class="indexterm"></a> themselves are immutable, deterministically recomputable distributed dataset. In event of loss of any partition of RDD due to the failure in worker nodes, the lost partition can be recomputed by applying the lineage of operations on the original dataset.</p><p>All RDD transformations are deterministic, so irrespective of any failures in the cluster, the final RDD always remains the same. This is the default behavior of RDD that also enables DStreams to be resilient towards the failures.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Data receivers</strong></span>: Unlike Spark batch processes that work on fault tolerant file <a id="id528" class="indexterm"></a>systems such as S3 and HDFS, Spark Streaming receives most of the streaming data (except while reading from filesystem) over the network from near real-time streams where we have to consume and process the data at the same time.</p><p>Once the data is successfully consumed and converted into DStreams there is no worry but while we consume and process the data, there is a possibility of failure happening at multiple levels. Spark provides the following guidelines for handling failures at the time of receiving or consuming data from network streams:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Enabling replication</strong></span>: The data received by the receivers can be replicated to the<a id="id529" class="indexterm"></a> multiple spark executors on the worker nodes. We can enable replication by using appropriate <code class="literal">StorageLevels</code> (ending with <code class="literal">_2</code> such as <code class="literal">MEMORY_ONLY_2</code>, <code class="literal">MEMORY_AND_DISK_2</code>, and more). Another benefit of replication is that the executors continue running the tasks on the RDD without waiting to recompute a lost partition.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Enabling write-ahead logs</strong></span>: There<a id="id530" class="indexterm"></a> could be instances where data is received but not replicated because it is still getting processed/transformed by the executors, and in case any failures occur at this time, there can be a complete loss of data.</p><p>Spark provided write-ahead logs, where the input data received through receivers will be first saved to write-ahead logs and then only it will be processed by the executors. Write-ahead logs are nothing but the journal which is recommended to be maintained on the distributed <a id="id531" class="indexterm"></a>and reliable file system <a id="id532" class="indexterm"></a>like HDFS or S3. This also facilitates the recovery of data even after driver failures. It can be enabled by configuring <code class="literal">spark.streaming.receiver.writeAheadLog.enable=true</code> in our <code class="literal">SparkConf</code>. Needless to say that we have to pay some price, that is performance, for using this feature, but again it is the trade-off between the various non-functional requirements (performance versus fault tolerance).</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note68"></a>Note</h3><p>Disable replication while we enable write-ahead logs as replication will be provided by the underlying filesystem anyways.</p></div></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Reliable receivers</strong></span>: We should also think about using the sources and receivers<a id="id533" class="indexterm"></a> that supports acknowledgments (known <a id="id534" class="indexterm"></a>as <span class="strong"><strong>acking</strong></span>) so that the data is reliably received and written to write-ahead logs such as Kafka, Flume, and others. For<a id="id535" class="indexterm"></a> more info on receiver's reliability, refer to <a class="ulink" href="https://spark.apache.org/docs/1.3.0/streaming-custom-receivers.html" target="_blank">https://spark.apache.org/docs/1.3.0/streaming-custom-receivers.html</a>.</p></li></ul></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note69"></a>Note</h3><p>For<a id="id536" class="indexterm"></a> more information on fault tolerance in Spark Streaming, refer to <a class="ulink" href="https://spark.apache.org/docs/1.4.0/streaming-programming-guide.html#fault-tolerance-semantics" target="_blank">https://spark.apache.org/docs/1.4.0/streaming-programming-guide.html#fault-tolerance-semantics</a> and <a class="ulink" href="https://databricks.com/blog/2015/01/15/improved-driver-fault-tolerance-and-zero-data-loss-in-spark-streaming.html" target="_blank">https://databricks.com/blog/2015/01/15/improved-driver-fault-tolerance-and-zero-data-loss-in-spark-streaming.html</a>
</p></div></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Spark driver</strong></span>: The Spark driver is another single point of failure. In the scenarios <a id="id537" class="indexterm"></a>where the system running the Spark driver crashes, our whole job is also halted. This is a serious concern but it can be addressed by implementing/enabling the following guidelines/features:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Deploy modes</strong></span>: The <code class="literal">spark-submit</code> script is the most common script <a id="id538" class="indexterm"></a>for submitting the jobs/application and launch <code class="literal">SparkDriver</code>. By default, <code class="literal">SparkDriver</code> runs/executes on the same machine which is used to launch the <code class="literal">spark-submit</code> script. Launching scripts on client machines is unsafe and does not survive failures. The <code class="literal">spark-submit</code> script provides an option to specify <code class="literal">--deploy-mode</code> where we can specify to deploy our driver on the worker nodes (cluster) or on external machine in the client mode (default mode). For example, we can execute our streaming job developed in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Architecture and Components of Spark and Spark Streaming</em></span>, by specifying the following command on the standalone Spark cluster:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$SPARK_HOME/bin/spark-submit --class chapter.two.ScalaFirstStreamingExample --master &lt;MASTER-IP&gt; --deploy-mode cluster Spark-Examples.jar</strong></span>
</pre></div></li><li style="list-style-type: disc"><p>For YARN, we can just specify <code class="literal">yarn-cluster</code> against <code class="literal">--master</code> and jobs are launched in the YARN cluster:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$SPARK_HOME/bin/spark-submit --class chapter.two.ScalaFirstStreamingExample --master yarn-cluster Spark-Examples.jar</strong></span>
</pre></div></li><li style="list-style-type: disc"><p>In the cluster mode, our driver is launched from one of the worker nodes in the cluster itself that makes it much reliable and can definitely survive failures as cluster itself is highly available and resilient to failures.</p></li></ul></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note70"></a>Note</h3><p>In Mesos, the launching driver in cluster mode is only available in Spark 1.4.x.</p></div></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Automatic restart</strong></span>: In cluster mode, we can also add one more parameter, <code class="literal">--supervise</code>, to<a id="id539" class="indexterm"></a> our <code class="literal">spark-submit</code> script, which will automatically restart our driver in case it exits with a non-zero code or may be failure of the node executing the driver. For example, we can execute our streaming jobs developed in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Architecture and Components of Spark and Spark Streaming</em></span>, by specifying the following <a id="id540" class="indexterm"></a>command on the standalone Spark cluster:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$SPARK_HOME/bin/spark-submit --class chapter.two.ScalaFirstStreamingExample --master &lt;MASTER-IP&gt; --deploy-mode cluster --supervise Spark-Examples.jar</strong></span>
</pre></div></li></ul></div><p>For YARN, refer <a id="id541" class="indexterm"></a>to <a class="ulink" href="http://hadoop.apache.org/docs/r2.4.1/hadoop-yarn/hadoop-yarn-site/ResourceManagerRestart.html" target="_blank">http://hadoop.apache.org/docs/r2.4.1/hadoop-yarn/hadoop-yarn-site/ResourceManagerRestart.html</a>, and for Mesos, we can use <a class="ulink" href="https://github.com/mesosphere/marathon" target="_blank">https://github.com/mesosphere/marathon</a> for automatic restart of jobs.</p><p>In <a id="id542" class="indexterm"></a>this section, we have discussed about the various features provided by Spark for ensuring that our real-time jobs are resilient to failures and can be executed 24 x 7 without minimum or no manual intervention. Let's move forward and understand the security and monitoring aspects of the Spark Streaming applications that is another important aspect of enterprise deployments.</p></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec34"></a>Monitoring streaming jobs</h2></div></div><hr /></div><p>Spark <a id="id543" class="indexterm"></a>provides different ways to monitor our Spark Streaming job. Though monitoring also depends upon the underlying cluster manager, at the same time, Spark itself produces a lot of information about our jobs that can be captured, analyzed, and further help to monitor and tune our jobs:</p><div class="mediaobject"><img src="graphics/B01793_07_08.jpg" /></div><p>The <a id="id544" class="indexterm"></a>preceding illustration shows the various monitoring options available with Spark, by default. At high level, the monitoring options can be categorized into two different categories, either of which can be leveraged to monitor our streaming jobs deployed in the standalone mode.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec55"></a>Application or job UI</h3></div></div></div><p>Spark<a id="id545" class="indexterm"></a> hosts a Web-UI over HTTP (by default, on 8080) on <a id="id546" class="indexterm"></a>the master node, which shows the status and history of all the active or completed jobs and the list of worker nodes available/ready for accepting the jobs.</p><div class="mediaobject"><img src="graphics/B01793_07-09.jpg" /></div><p>The<a id="id547" class="indexterm"></a> preceding screenshot shows the Web-UI hosted on the <a id="id548" class="indexterm"></a>master node and the details about workers and applications.</p><p>We can click on <span class="strong"><strong>Worker id</strong></span> of any of the <span class="strong"><strong>Workers</strong></span> section to see the list of in-process and completed executors by that specific worker.</p><div class="mediaobject"><img src="graphics/B01793_07-10.jpg" /></div><p>The preceding screenshot shows the details presented by the Web-UI of the specific node/worker.</p><p>Each SparkContext also launches a separate WEB-UI, which provides useful details and statistics about the jobs running on the Spark cluster binded to that specific SparkContext. By default, this UI is available at <code class="literal">http://&lt;IP-MasterNode&gt;:4040/</code> and can be <a id="id549" class="indexterm"></a>accessed by clicking on any of the jobs listed in <a id="id550" class="indexterm"></a>the <span class="strong"><strong>Running Applications</strong></span> tab on the Web-UI hosted by the master node. </p><p>In case there are multiple SparkContext running/active at a time on the Spark hosts, they are bind to the successive ports (4041/4042/ and so on).</p><div class="mediaobject"><img src="graphics/B01793_07-11.jpg" /></div><p>The preceding screenshot shows the WEB-UI presented by each of the active SparkContext within the cluster. Basically, it divulges the following information about each and every job which is currently running on the cluster:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Jobs</strong></span>: This gives the list of active and completed Spark jobs.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Stages</strong></span>: This shows the list of scheduler stages and tasks being executed by the executors.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Storage</strong></span>: This gives a summary of RDD sizes (in memory and disk), memory usage, and cached partitions.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Environment</strong></span>: This shows various configured runtime configurations and properties.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Executors</strong></span>: This gives information about the executors running on that machine and its further details such as the memory used and completed tasks. We can also browse logs and at the same time take thread dumps for analyzing any deadlocks.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Streaming</strong></span>: This shows the statistics, summary, and performance details from the last 100 batches executed by that specific Spark Streaming context. It provides details about the receiver and batch processing statistics such as processing time and scheduling delays for the last 100 batches.</p></li></ul></div><p>All this<a id="id551" class="indexterm"></a> information is only available while the SparkContext/application is active. As soon as SparkContext is killed, all this information is lost. In <a id="id552" class="indexterm"></a>case we need to still see these details even after the job is completed, we need to enable the event logging and configure the following parameters in our <code class="literal">$SPARK_HOME/conf/spark-default.conf</code> file:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The <code class="literal">spark.eventLog.enabled=true</code> parameter enables the event logging of applications jobs</p></li><li style="list-style-type: disc"><p>The <code class="literal">spark.eventLog.dir=hdfs://localhost:9000/jobHistory</code> parameter defines the location of these logs on HDFS</p></li></ul></div><p>Once we have enabled the event logging, we can bring up our history server by configuring the following parameter in our <code class="literal">Spark-default.conf</code> file:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">spark.history.fs.logDirectory=hdfs://localhost:9000/jobHistory</code>: This parameter defines the location of the directory where event logs are stored</p></li></ul></div><p>Finally, we will bring up our history server by executing the following command on our Linux console:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$SPARK_HOME/sbin/start-history-server.sh</strong></span>
</pre></div><p>Our history server, by default, is hosted on port 18080 and provides similar details to what we have seen in Web-UI for the running/live SparkContext, with the only difference that it shows only completed or incompleted jobs. Incompleted jobs are those that are forcefully halted by the admin and have not been gracefully shut down by invoking <code class="literal">stop()</code> on <code class="literal">SparkContext</code> or <code class="literal">StreamingContext</code>.</p><div class="mediaobject"><img src="graphics/B01793_07-12.jpg" /></div><p>The preceding screenshot shows the UI of the <span class="strong"><strong>History Server</strong></span>. We can click on any of the completed or<a id="id553" class="indexterm"></a> incompleted jobs and can analyze the same<a id="id554" class="indexterm"></a> details as we have seen for live/in-process jobs being executed by our Spark cluster.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec56"></a>Integration with other monitoring tools</h3></div></div></div><p>Spark, by <a id="id555" class="indexterm"></a>default, also provides integration with other monitoring tools such as Ganglia and Graphite. It provides a flexible, extendable, and <a id="id556" class="indexterm"></a>configurable metrics system based on the Coda Hale metrics library (<a class="ulink" href="https://github.com/dropwizard/metrics" target="_blank">https://github.com/dropwizard/metrics</a>).</p><p>Let's follow these steps to enable JMX visualization for viewing the statistics exposed by the Spark executors:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Rename <code class="literal">$SPARK_HOME/conf/metrics.properties.template</code> to <code class="literal">$SPARK_HOME/conf/metrics.properties</code>.</p></li><li><p>Open and edit <code class="literal">$SPARK_HOME/conf/metrics.properties</code> and uncomment the following properties:</p><div class="informalexample"><pre class="programlisting">*.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink
master.source.jvm.class=org.apache.spark.metrics.source.JvmSource
worker.source.jvm.class=org.apache.spark.metrics.source.JvmSource
driver.source.jvm.class=org.apache.spark.metrics.source.JvmSource
executor.source.jvm.class=org.apache.spark.metrics.source.JvmSource</pre></div></li><li><p>Open and edit the <code class="literal">$SPARK_HOME/conf/spark-default.conf</code> file and add the following properties for enabling the remote connection to JMX:</p><div class="informalexample"><pre class="programlisting">spark.executor.extraJavaOptions=-Dcom.sun.management.jmxremote.port=9999 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=&lt;Host-Name&gt;</pre></div></li><li><p>We are done! Now execute your streaming jobs and open any JMX client such <a id="id557" class="indexterm"></a>as JConsole, VisualVM, and others and use port <code class="literal">&lt;Host-Name&gt;:9999</code> to connect and see various statistics about your streaming job. The following screenshot has the JConsole UI that shows the various metrics/statistics exposed by Spark JMX sink:</p><div class="mediaobject"><img src="graphics/B01793_07-13.jpg" /></div></li></ol></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note71"></a>Note</h3><p>For<a id="id558" class="indexterm"></a> more information about monitoring in Spark, refer to <a class="ulink" href="https://spark.apache.org/docs/1.3.0/monitoring.html" target="_blank">https://spark.apache.org/docs/1.3.0/monitoring.html</a>.</p></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec35"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we discussed in detail about various deployment models supported by Spark and Spark Streaming. We also discussed various other critical and important aspects of production deployments such as high availability, fault tolerance, and monitoring.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">

    <div id="backindex">
      <h1 class="title">Index</h1>
      <h2>A</h2>
      <ul>
        <li>accumulators<ul><li>about / <a href="#ch03lvl1sec19" title="Other Spark Core packages" class="link">Other Spark Core packages</a></li><li>URL / <a href="#ch03lvl1sec19" title="Other Spark Core packages" class="link">Other Spark Core packages</a></li></ul></li>
        <li>acking<ul><li>about / <a href="#ch07lvl1sec33" title="Fault tolerance in Spark Streaming" class="link">Fault tolerance in Spark Streaming</a></li></ul></li>
        <li>actions<ul><li>about / <a href="#ch03lvl1sec20" title="Transformations and actions" class="link">Transformations and actions</a></li></ul></li>
        <li>administrators/developers<ul><li>tools / <a href="#ch01lvl1sec11" title="Tools and utilities for administrators/developers" class="link">Tools and utilities for administrators/developers</a></li><li>utilities / <a href="#ch01lvl1sec11" title="Tools and utilities for administrators/developers" class="link">Tools and utilities for administrators/developers</a></li></ul></li>
        <li>algorithms, for classification<ul><li>references / <a href="#ch03lvl1sec19" title="Spark MLlib" class="link">Spark MLlib</a></li></ul></li>
        <li>Amazon Kinesis<ul><li>URL / <a href="#ch03lvl1sec19" title="Spark Streaming" class="link">Spark Streaming</a></li></ul></li>
        <li>Amazon S3<ul><li>URL / <a href="#ch07lvl1sec32" title="Integrating and executing Spark applications on Apache Mesos" class="link">Integrating and executing Spark applications on Apache Mesos</a></li></ul></li>
        <li>Apache access logs<ul><li>URL / <a href="#ch04lvl1sec23" title="Simulating log streaming" class="link">Simulating log streaming</a></li></ul></li>
        <li>Apache Cassandra<ul><li>integration with / <a href="#ch05lvl1sec27" title="Integration with Cassandra" class="link">Integration with Cassandra</a></li><li>URL / <a href="#ch05lvl1sec27" title="Integration with Cassandra" class="link">Integration with Cassandra</a></li><li>installing / <a href="#ch05lvl1sec27" title="Installing and configuring Apache Cassandra" class="link">Installing and configuring Apache Cassandra</a></li><li>configuring / <a href="#ch05lvl1sec27" title="Installing and configuring Apache Cassandra" class="link">Installing and configuring Apache Cassandra</a></li></ul></li>
        <li>Apache Cassandra 2.1.7<ul><li>URL / <a href="#ch05lvl1sec27" title="Installing and configuring Apache Cassandra" class="link">Installing and configuring Apache Cassandra</a></li></ul></li>
        <li>Apache Giraph<ul><li>URL / <a href="#ch06lvl1sec30" title="Graph analysis â€“ Spark GraphX" class="link">Graph analysis â€“ Spark GraphX</a></li></ul></li>
        <li>Apache Hadoop<ul><li>URL / <a href="#ch02lvl1sec14" title="Real-time data processing" class="link">Real-time data processing</a></li></ul></li>
        <li>Apache Maven<ul><li>URL / <a href="#ch07lvl1sec32" title="Installing and configuring Apache Mesos" class="link">Installing and configuring Apache Mesos</a></li></ul></li>
        <li>Apache Mesos<ul><li>URL / <a href="#ch01lvl1sec09" title="Configuring and running the Spark cluster" class="link">Configuring and running the Spark cluster</a>, <a href="#ch07lvl1sec32" title="Deploying on Apache Mesos" class="link">Deploying on Apache Mesos</a></li><li>deploying on / <a href="#ch07lvl1sec32" title="Deploying on Apache Mesos" class="link">Deploying on Apache Mesos</a></li><li>installing / <a href="#ch07lvl1sec32" title="Installing and configuring Apache Mesos" class="link">Installing and configuring Apache Mesos</a></li><li>configuring / <a href="#ch07lvl1sec32" title="Installing and configuring Apache Mesos" class="link">Installing and configuring Apache Mesos</a></li><li>Spark applications, integrating on / <a href="#ch07lvl1sec32" title="Integrating and executing Spark applications on Apache Mesos" class="link">Integrating and executing Spark applications on Apache Mesos</a></li><li>Spark applications, executing on / <a href="#ch07lvl1sec32" title="Integrating and executing Spark applications on Apache Mesos" class="link">Integrating and executing Spark applications on Apache Mesos</a></li></ul></li>
        <li>application master (AM)<ul><li>about / <a href="#ch07lvl1sec32" title="Deploying on Hadoop or YARN" class="link">Deploying on Hadoop or YARN</a></li></ul></li>
        <li>architecture, YARN<ul><li>URL / <a href="#ch07lvl1sec32" title="Deploying on Hadoop or YARN" class="link">Deploying on Hadoop or YARN</a></li></ul></li>
        <li>assembly lines<ul><li>about / <a href="#ch02lvl1sec14" title="Real-time data processing" class="link">Real-time data processing</a></li></ul></li>
      </ul>
      <h2>B</h2>
      <ul>
        <li>batch jobs<ul><li>examples / <a href="#ch02lvl1sec14" title="Batch processing" class="link">Batch processing</a></li><li>references / <a href="#ch02lvl1sec14" title="Batch processing" class="link">Batch processing</a></li></ul></li>
        <li>batch processing<ul><li>about / <a href="#ch01lvl1sec08" title="Installing Spark extensions â€“ Spark Streaming" class="link">Installing Spark extensions â€“ Spark Streaming</a></li><li>versus real-time data processing / <a href="#ch02lvl1sec14" title="Batch versus real-time data processing" class="link">Batch versus real-time data processing</a></li><li>defining / <a href="#ch02lvl1sec14" title="Batch processing" class="link">Batch processing</a></li></ul></li>
        <li>batch processing systems<ul><li>complexity / <a href="#ch02lvl1sec14" title="Batch processing" class="link">Batch processing</a></li><li>Large data / <a href="#ch02lvl1sec14" title="Batch processing" class="link">Batch processing</a></li><li>Scalability / <a href="#ch02lvl1sec14" title="Batch processing" class="link">Batch processing</a></li><li>Distributed processing / <a href="#ch02lvl1sec14" title="Batch processing" class="link">Batch processing</a></li><li>Fault tolerant / <a href="#ch02lvl1sec14" title="Batch processing" class="link">Batch processing</a></li><li>Enterprise constraints / <a href="#ch02lvl1sec14" title="Batch processing" class="link">Batch processing</a></li></ul></li>
        <li>business intelligence (BI)<ul><li>about / <a href="#ch02lvl1sec14" title="Real-time data processing" class="link">Real-time data processing</a></li><li>URL / <a href="#ch02lvl1sec14" title="Real-time data processing" class="link">Real-time data processing</a></li></ul></li>
      </ul>
      <h2>C</h2>
      <ul>
        <li>Cassandra Core driver<ul><li>URL / <a href="#ch05lvl1sec27" title="Configuring Spark for integration with Cassandra" class="link">Configuring Spark for integration with Cassandra</a></li></ul></li>
        <li>classification models<ul><li>URL / <a href="#ch03lvl1sec19" title="Spark MLlib" class="link">Spark MLlib</a></li></ul></li>
        <li>clustering algorithms<ul><li>URL / <a href="#ch03lvl1sec19" title="Spark MLlib" class="link">Spark MLlib</a></li></ul></li>
        <li>cluster management<ul><li>about / <a href="#ch01lvl1sec11" title="Cluster management" class="link">Cluster management</a></li><li>$SPARK_HOME/sbin / <a href="#ch01lvl1sec11" title="Cluster management" class="link">Cluster management</a></li><li>$SPARK_HOME/conf / <a href="#ch01lvl1sec11" title="Cluster management" class="link">Cluster management</a></li></ul></li>
        <li>Coda Hale metrics library<ul><li>URL / <a href="#ch07lvl1sec34" title="Integration with other monitoring tools" class="link">Integration with other monitoring tools</a></li></ul></li>
        <li>collaborative filtering<ul><li>URL / <a href="#ch03lvl1sec19" title="Spark MLlib" class="link">Spark MLlib</a></li></ul></li>
        <li>components, Flume<ul><li>Source / <a href="#ch03lvl1sec21" title="Flume architecture" class="link">Flume architecture</a></li><li>Channel / <a href="#ch03lvl1sec21" title="Flume architecture" class="link">Flume architecture</a></li><li>Interceptor / <a href="#ch03lvl1sec21" title="Flume architecture" class="link">Flume architecture</a></li><li>Sink / <a href="#ch03lvl1sec21" title="Flume architecture" class="link">Flume architecture</a></li></ul></li>
        <li>components, Spark<ul><li>RDD / <a href="#ch01lvl1sec10" title="Your first Spark program" class="link">Your first Spark program</a></li><li>SparkContext / <a href="#ch01lvl1sec10" title="Your first Spark program" class="link">Your first Spark program</a></li></ul></li>
        <li>components, Spark cluster<ul><li>Driver / <a href="#ch01lvl1sec09" title="Configuring and running the Spark cluster" class="link">Configuring and running the Spark cluster</a></li><li>Cluster manager / Spark master / <a href="#ch01lvl1sec09" title="Configuring and running the Spark cluster" class="link">Configuring and running the Spark cluster</a></li><li>Spark worker / <a href="#ch01lvl1sec09" title="Configuring and running the Spark cluster" class="link">Configuring and running the Spark cluster</a></li></ul></li>
        <li>components, Spark Streaming<ul><li>Input data streams / <a href="#ch02lvl1sec16" title="High-level architecture â€“ Spark Streaming" class="link">High-level architecture â€“ Spark Streaming</a></li><li>Spark Streaming / <a href="#ch02lvl1sec16" title="High-level architecture â€“ Spark Streaming" class="link">High-level architecture â€“ Spark Streaming</a></li><li>Batch / <a href="#ch02lvl1sec16" title="High-level architecture â€“ Spark Streaming" class="link">High-level architecture â€“ Spark Streaming</a></li><li>Spark core engine / <a href="#ch02lvl1sec16" title="High-level architecture â€“ Spark Streaming" class="link">High-level architecture â€“ Spark Streaming</a></li><li>Output data streams / <a href="#ch02lvl1sec16" title="High-level architecture â€“ Spark Streaming" class="link">High-level architecture â€“ Spark Streaming</a></li><li>DStreams / <a href="#ch07lvl1sec33" title="Fault tolerance in Spark Streaming" class="link">Fault tolerance in Spark Streaming</a></li><li>data receivers / <a href="#ch07lvl1sec33" title="Fault tolerance in Spark Streaming" class="link">Fault tolerance in Spark Streaming</a></li></ul></li>
        <li>configuration parameters<ul><li>URL / <a href="#ch04lvl1sec24" title="Executor memory and caching RDDs" class="link">Executor memory and caching RDDs</a></li></ul></li>
        <li>CQL (Cassandra Query Language)<ul><li>about / <a href="#ch05lvl1sec27" title="Installing and configuring Apache Cassandra" class="link">Installing and configuring Apache Cassandra</a></li></ul></li>
        <li>CQLSH<ul><li>about / <a href="#ch05lvl1sec27" title="Installing and configuring Apache Cassandra" class="link">Installing and configuring Apache Cassandra</a></li></ul></li>
        <li>Curator<ul><li>URL / <a href="#ch07lvl1sec33" title="High availability and fault tolerance" class="link">High availability and fault tolerance</a></li></ul></li>
        <li>custom applications, on YARN<ul><li>URL / <a href="#ch07lvl1sec32" title="Deploying on Hadoop or YARN" class="link">Deploying on Hadoop or YARN</a></li></ul></li>
        <li>custom sink<ul><li>URL / <a href="#ch03lvl1sec21" title="Flume architecture" class="link">Flume architecture</a></li></ul></li>
      </ul>
      <h2>D</h2>
      <ul>
        <li>data, loading<ul><li>about / <a href="#ch03lvl1sec21" title="Data loading from distributed and varied sources" class="link">Data loading from distributed and varied sources</a></li><li>Flume architecture / <a href="#ch03lvl1sec21" title="Flume architecture" class="link">Flume architecture</a></li></ul></li>
        <li>databricks<ul><li>about / <a href="#ch03lvl1sec19" title="Spark SQL" class="link">Spark SQL</a></li></ul></li>
        <li>Data Frame API<ul><li>URL / <a href="#ch06lvl1sec29" title="Understanding Spark SQL" class="link">Understanding Spark SQL</a></li></ul></li>
        <li>data frames<ul><li>references / <a href="#ch03lvl1sec19" title="Spark packaging structure and client APIs" class="link">Spark packaging structure and client APIs</a></li></ul></li>
        <li>DataFrames<ul><li>about / <a href="#ch06lvl1sec29" title="Understanding Spark SQL" class="link">Understanding Spark SQL</a></li><li>URL / <a href="#ch06lvl1sec29" title="Understanding Spark SQL" class="link">Understanding Spark SQL</a></li></ul></li>
        <li>data lineage<ul><li>about / <a href="#ch03lvl1sec20" title="Fault tolerance" class="link">Fault tolerance</a></li></ul></li>
        <li>DataStax<ul><li>URL / <a href="#ch05lvl1sec27" title="Integration with Cassandra" class="link">Integration with Cassandra</a></li><li>about / <a href="#ch05lvl1sec27" title="Integration with Cassandra" class="link">Integration with Cassandra</a></li></ul></li>
        <li>deployment models, Spark<ul><li>Standalone mode / <a href="#ch01lvl1sec09" title="Configuring and running the Spark cluster" class="link">Configuring and running the Spark cluster</a></li><li>Apache Mesos / <a href="#ch01lvl1sec09" title="Configuring and running the Spark cluster" class="link">Configuring and running the Spark cluster</a></li><li>Hadoop YARN / <a href="#ch01lvl1sec09" title="Configuring and running the Spark cluster" class="link">Configuring and running the Spark cluster</a></li></ul></li>
        <li>Direct Acyclic Graph (DAG)<ul><li>about / <a href="#ch04lvl1sec24" title="Partitioning and parallelism" class="link">Partitioning and parallelism</a></li></ul></li>
        <li>Directed Acyclic Graph (DAG)<ul><li>about / <a href="#ch03lvl1sec19" title="Other Spark Core packages" class="link">Other Spark Core packages</a></li><li>URL / <a href="#ch03lvl1sec19" title="Other Spark Core packages" class="link">Other Spark Core packages</a></li><li>references / <a href="#ch03lvl1sec19" title="Other Spark Core packages" class="link">Other Spark Core packages</a></li></ul></li>
        <li>discretized streams<ul><li>about / <a href="#ch03lvl1sec20" title="Resilient distributed datasets and discretized streams" class="link">Resilient distributed datasets and discretized streams</a>, <a href="#ch03lvl1sec20" title="Discretized streams" class="link">Discretized streams</a></li></ul></li>
        <li>distributed log file processing<ul><li>architecture, defining / <a href="#ch03lvl1sec21" title="Overall architecture of distributed log file processing" class="link">Overall architecture of distributed log file processing</a></li></ul></li>
        <li>DStream<ul><li>higher-order functions / <a href="#ch04lvl1sec23" title="Functional operations" class="link">Functional operations</a></li><li>operations / <a href="#ch04lvl1sec23" title="Functional operations" class="link">Functional operations</a></li></ul></li>
        <li>DStream API<ul><li>URL / <a href="#ch03lvl1sec20" title="Discretized streams" class="link">Discretized streams</a>, <a href="#ch04lvl1sec23" title="Windowing operations" class="link">Windowing operations</a></li></ul></li>
        <li>DStreams<ul><li>about / <a href="#ch03lvl1sec20" title="Discretized streams" class="link">Discretized streams</a></li></ul></li>
      </ul>
      <h2>E</h2>
      <ul>
        <li>Eclipse Luna (4.4)<ul><li>URL / <a href="#ch01lvl1sec08" title="Eclipse" class="link">Eclipse</a></li></ul></li>
        <li>edges<ul><li>about / <a href="#ch06lvl1sec30" title="Graph analysis â€“ Spark GraphX" class="link">Graph analysis â€“ Spark GraphX</a></li></ul></li>
        <li>Elasticsearch<ul><li>URL / <a href="#ch06lvl1sec29" title="Understanding Spark SQL" class="link">Understanding Spark SQL</a></li></ul></li>
        <li>ETL (Extract, Transform, Load)<ul><li>about / <a href="#ch03lvl1sec21" title="Flume architecture" class="link">Flume architecture</a></li></ul></li>
        <li>Event Streaming, Iterative Algorithms and Elasticity<ul><li>URL / <a href="#ch02lvl1sec15" title="Spark versus Hadoop" class="link">Spark versus Hadoop</a></li></ul></li>
      </ul>
      <h2>F</h2>
      <ul>
        <li>fastutil<ul><li>URL / <a href="#ch04lvl1sec24" title="Object sizes" class="link">Object sizes</a></li></ul></li>
        <li>fault tolerance<ul><li>about / <a href="#ch07lvl1sec33" title="High availability and fault tolerance" class="link">High availability and fault tolerance</a>, <a href="#ch07lvl1sec33" title="Fault tolerance" class="link">Fault tolerance</a></li><li>URL / <a href="#ch07lvl1sec33" title="Fault tolerance" class="link">Fault tolerance</a></li><li>references / <a href="#ch07lvl1sec33" title="Fault tolerance" class="link">Fault tolerance</a></li><li>in Spark Streaming / <a href="#ch07lvl1sec33" title="Fault tolerance in Spark Streaming" class="link">Fault tolerance in Spark Streaming</a></li></ul></li>
        <li>fault tolerance, in Spark Streaming<ul><li>URL / <a href="#ch07lvl1sec33" title="Fault tolerance in Spark Streaming" class="link">Fault tolerance in Spark Streaming</a></li></ul></li>
        <li>filter(filterFunc)<ul><li>about / <a href="#ch04lvl1sec23" title="Functional operations" class="link">Functional operations</a></li></ul></li>
        <li>Flume<ul><li>URL / <a href="#ch03lvl1sec21" title="Data loading from distributed and varied sources" class="link">Data loading from distributed and varied sources</a>, <a href="#ch03lvl1sec21" title="Installing and configuring Flume" class="link">Installing and configuring Flume</a></li><li>references / <a href="#ch03lvl1sec21" title="Flume architecture" class="link">Flume architecture</a>, <a href="#ch03lvl1sec21" title="Installing and configuring Flume" class="link">Installing and configuring Flume</a>, <a href="#ch03lvl1sec21" title="Configuring Spark to consume Flume events" class="link">Configuring Spark to consume Flume events</a></li><li>installing / <a href="#ch03lvl1sec21" title="Installing and configuring Flume" class="link">Installing and configuring Flume</a></li><li>configuring / <a href="#ch03lvl1sec21" title="Installing and configuring Flume" class="link">Installing and configuring Flume</a></li></ul></li>
        <li>Flume events<ul><li>consuming, via Spark configuration / <a href="#ch03lvl1sec21" title="Configuring Spark to consume Flume events" class="link">Configuring Spark to consume Flume events</a></li></ul></li>
        <li>Flume interceptors<ul><li>URL / <a href="#ch03lvl1sec21" title="Flume architecture" class="link">Flume architecture</a></li></ul></li>
        <li>Flume sinks<ul><li>URL / <a href="#ch03lvl1sec21" title="Flume architecture" class="link">Flume architecture</a></li></ul></li>
        <li>ForEachFunction<ul><li>about / <a href="#ch04lvl1sec23" title="Functional operations" class="link">Functional operations</a></li></ul></li>
        <li>fully distributed mode, Hadoop/YARN<ul><li>URL / <a href="#ch07lvl1sec32" title="Deploying on Hadoop or YARN" class="link">Deploying on Hadoop or YARN</a></li></ul></li>
        <li>Fume<ul><li>URL / <a href="#ch03lvl1sec19" title="Spark Streaming" class="link">Spark Streaming</a></li></ul></li>
      </ul>
      <h2>G</h2>
      <ul>
        <li>graph-parallel computations<ul><li>about / <a href="#ch03lvl1sec19" title="Spark packaging structure and client APIs" class="link">Spark packaging structure and client APIs</a></li></ul></li>
        <li>GraphLab<ul><li>URL / <a href="#ch06lvl1sec30" title="Graph analysis â€“ Spark GraphX" class="link">Graph analysis â€“ Spark GraphX</a></li></ul></li>
        <li>Graphs<ul><li>URL / <a href="#ch06lvl1sec30" title="Graph analysis â€“ Spark GraphX" class="link">Graph analysis â€“ Spark GraphX</a></li></ul></li>
        <li>GraphX API<ul><li>defining / <a href="#ch06lvl1sec30" title="Introduction to the GraphX API" class="link">Introduction to the GraphX API</a></li></ul></li>
        <li>guidelines, Spark<ul><li>replication, enabling / <a href="#ch07lvl1sec33" title="Fault tolerance in Spark Streaming" class="link">Fault tolerance in Spark Streaming</a></li><li>write-ahead logs, enabling / <a href="#ch07lvl1sec33" title="Fault tolerance in Spark Streaming" class="link">Fault tolerance in Spark Streaming</a></li><li>reliable receivers / <a href="#ch07lvl1sec33" title="Fault tolerance in Spark Streaming" class="link">Fault tolerance in Spark Streaming</a></li><li>Spark driver / <a href="#ch07lvl1sec33" title="Fault tolerance in Spark Streaming" class="link">Fault tolerance in Spark Streaming</a></li><li>automatic restart / <a href="#ch07lvl1sec33" title="Fault tolerance in Spark Streaming" class="link">Fault tolerance in Spark Streaming</a></li></ul></li>
      </ul>
      <h2>H</h2>
      <ul>
        <li>HA, in standalone mode<ul><li>URL / <a href="#ch07lvl1sec33" title="High availability in the standalone mode" class="link">High availability in the standalone mode</a></li></ul></li>
        <li>hacking<ul><li>URL / <a href="#ch03lvl1sec21" title="Data loading from distributed and varied sources" class="link">Data loading from distributed and varied sources</a></li></ul></li>
        <li>Hadoop<ul><li>deploying on / <a href="#ch07lvl1sec32" title="Deploying on Hadoop or YARN" class="link">Deploying on Hadoop or YARN</a></li></ul></li>
        <li>Hadoop 2.4<ul><li>URL / <a href="#ch07lvl1sec32" title="Deploying on Hadoop or YARN" class="link">Deploying on Hadoop or YARN</a></li></ul></li>
        <li>Hadoop 2.4.0 distribution<ul><li>URL / <a href="#ch05lvl1sec26" title="Output operations in Spark Streaming" class="link">Output operations in Spark Streaming</a></li><li>references / <a href="#ch05lvl1sec26" title="Output operations in Spark Streaming" class="link">Output operations in Spark Streaming</a></li></ul></li>
        <li>Hadoop and HDFS<ul><li>URL / <a href="#ch05lvl1sec26" title="Output operations in Spark Streaming" class="link">Output operations in Spark Streaming</a></li></ul></li>
        <li>Hadoop YARN<ul><li>URL / <a href="#ch01lvl1sec09" title="Configuring and running the Spark cluster" class="link">Configuring and running the Spark cluster</a></li></ul></li>
        <li>HA in Apache Mesos and YARN<ul><li>URL / <a href="#ch07lvl1sec33" title="High availability in Mesos or YARN" class="link">High availability in Mesos or YARN</a></li></ul></li>
        <li>HA mode<ul><li>enabling / <a href="#ch07lvl1sec33" title="High availability in the standalone mode" class="link">High availability in the standalone mode</a></li></ul></li>
        <li>hardware requirements, Spark<ul><li>CPU / <a href="#ch01lvl1sec08" title="CPU" class="link">CPU</a></li><li>RAM / <a href="#ch01lvl1sec08" title="RAM" class="link">RAM</a></li><li>disk / <a href="#ch01lvl1sec08" title="Disk" class="link">Disk</a></li><li>network / <a href="#ch01lvl1sec08" title="Network" class="link">Network</a></li><li>operating system / <a href="#ch01lvl1sec08" title="Operating system" class="link">Operating system</a></li></ul></li>
        <li>HDFS (Hadoop Distributed File System)<ul><li>about / <a href="#ch05lvl1sec26" title="Output operations in Spark Streaming" class="link">Output operations in Spark Streaming</a></li></ul></li>
        <li>high-order functions<ul><li>URL / <a href="#ch04lvl1sec23" title="Functional operations" class="link">Functional operations</a></li></ul></li>
        <li>high availability<ul><li>about / <a href="#ch07lvl1sec33" title="High availability and fault tolerance" class="link">High availability and fault tolerance</a></li><li>in standalone mode / <a href="#ch07lvl1sec33" title="High availability in the standalone mode" class="link">High availability in the standalone mode</a></li><li>in Mesos / <a href="#ch07lvl1sec33" title="High availability in Mesos or YARN" class="link">High availability in Mesos or YARN</a></li><li>in YARN / <a href="#ch07lvl1sec33" title="High availability in Mesos or YARN" class="link">High availability in Mesos or YARN</a></li></ul></li>
      </ul>
      <h2>J</h2>
      <ul>
        <li>Java-specific RDD classes<ul><li>URL / <a href="#ch03lvl1sec19" title="RDD â€“ Java APIs" class="link">RDD â€“ Java APIs</a></li></ul></li>
        <li>Java APIs<ul><li>defining, in SparkConf / <a href="#ch03lvl1sec19" title="SparkContext and Spark Config â€“ Java APIs" class="link">SparkContext and Spark Config â€“ Java APIs</a></li><li>defining, in SparkContext / <a href="#ch03lvl1sec19" title="SparkContext and Spark Config â€“ Java APIs" class="link">SparkContext and Spark Config â€“ Java APIs</a></li></ul></li>
        <li>JdbcRDD.scala<ul><li>URL / <a href="#ch03lvl1sec19" title="RDD â€“ Scala APIs" class="link">RDD â€“ Scala APIs</a></li></ul></li>
        <li>JSON editor<ul><li>URL / <a href="#ch06lvl1sec29" title="Understanding Spark SQL" class="link">Understanding Spark SQL</a></li></ul></li>
        <li>Just Bunch of Disks (JBOD)<ul><li>about / <a href="#ch01lvl1sec08" title="Disk" class="link">Disk</a></li></ul></li>
      </ul>
      <h2>K</h2>
      <ul>
        <li>Kafka<ul><li>URL / <a href="#ch03lvl1sec19" title="Spark Streaming" class="link">Spark Streaming</a></li></ul></li>
        <li>key performance indicators (KPI)<ul><li>URL / <a href="#ch02lvl1sec14" title="Batch processing" class="link">Batch processing</a></li></ul></li>
        <li>Kryo Serialization<ul><li>URL / <a href="#ch04lvl1sec24" title="Serialization" class="link">Serialization</a></li></ul></li>
        <li>Kyro documentation<ul><li>URL / <a href="#ch04lvl1sec24" title="Serialization" class="link">Serialization</a></li></ul></li>
      </ul>
      <h2>L</h2>
      <ul>
        <li>latency<ul><li>about / <a href="#ch02lvl1sec14" title="Real-time data processing" class="link">Real-time data processing</a></li></ul></li>
        <li>libraries<ul><li>URL / <a href="#ch05lvl1sec27" title="Configuring Spark for integration with Cassandra" class="link">Configuring Spark for integration with Cassandra</a></li></ul></li>
      </ul>
      <h2>M</h2>
      <ul>
        <li>map(mapFunc)<ul><li>about / <a href="#ch04lvl1sec23" title="Functional operations" class="link">Functional operations</a></li></ul></li>
        <li>Mesos<ul><li>URL / <a href="#ch07lvl1sec32" title="Installing and configuring Apache Mesos" class="link">Installing and configuring Apache Mesos</a></li></ul></li>
        <li>Mesos UI<ul><li>URL / <a href="#ch07lvl1sec32" title="Integrating and executing Spark applications on Apache Mesos" class="link">Integrating and executing Spark applications on Apache Mesos</a></li></ul></li>
        <li>micro-batching<ul><li>about / <a href="#ch04lvl1sec23" title="Windowing operations" class="link">Windowing operations</a></li></ul></li>
        <li>modes, Hadoop/YARN<ul><li>standalone mode / <a href="#ch07lvl1sec32" title="Deploying on Hadoop or YARN" class="link">Deploying on Hadoop or YARN</a></li><li>pseudo-distributed mode / <a href="#ch07lvl1sec32" title="Deploying on Hadoop or YARN" class="link">Deploying on Hadoop or YARN</a></li><li>fully distributed mode / <a href="#ch07lvl1sec32" title="Deploying on Hadoop or YARN" class="link">Deploying on Hadoop or YARN</a></li></ul></li>
        <li>MongoDB<ul><li>URL / <a href="#ch06lvl1sec29" title="Understanding Spark SQL" class="link">Understanding Spark SQL</a></li></ul></li>
        <li>monitoring, in Spark<ul><li>URL / <a href="#ch07lvl1sec34" title="Integration with other monitoring tools" class="link">Integration with other monitoring tools</a></li></ul></li>
        <li>MQTT<ul><li>URL / <a href="#ch03lvl1sec19" title="Spark Streaming" class="link">Spark Streaming</a></li></ul></li>
      </ul>
      <h2>N</h2>
      <ul>
        <li>Naive Bayes<ul><li>URL / <a href="#ch03lvl1sec19" title="Spark MLlib" class="link">Spark MLlib</a></li></ul></li>
        <li>node manager (NM)<ul><li>about / <a href="#ch07lvl1sec32" title="Deploying on Hadoop or YARN" class="link">Deploying on Hadoop or YARN</a></li></ul></li>
        <li>nodes<ul><li>about / <a href="#ch06lvl1sec30" title="Graph analysis â€“ Spark GraphX" class="link">Graph analysis â€“ Spark GraphX</a></li></ul></li>
        <li>non-RAID architectures<ul><li>URL / <a href="#ch01lvl1sec08" title="Disk" class="link">Disk</a></li></ul></li>
      </ul>
      <h2>O</h2>
      <ul>
        <li>Operational intelligence (OI)<ul><li>URL / <a href="#ch02lvl1sec14" title="Real-time data processing" class="link">Real-time data processing</a></li><li>about / <a href="#ch02lvl1sec14" title="Real-time data processing" class="link">Real-time data processing</a></li></ul></li>
        <li>operations, DStreams<ul><li>transformations / <a href="#ch03lvl1sec20" title="Discretized streams" class="link">Discretized streams</a></li><li>output operations / <a href="#ch03lvl1sec20" title="Discretized streams" class="link">Discretized streams</a></li><li>windowing / <a href="#ch03lvl1sec20" title="Discretized streams" class="link">Discretized streams</a></li><li>incremental aggregation / stateful processing / <a href="#ch03lvl1sec20" title="Discretized streams" class="link">Discretized streams</a></li></ul></li>
        <li>optimization techniques<ul><li>URL / <a href="#ch04lvl1sec24" title="Garbage collection" class="link">Garbage collection</a></li></ul></li>
        <li>Oracle Java 7<ul><li>URL / <a href="#ch01lvl1sec08" title="Java" class="link">Java</a></li></ul></li>
      </ul>
      <h2>P</h2>
      <ul>
        <li>PageRank<ul><li>URL / <a href="#ch03lvl1sec19" title="Spark GraphX" class="link">Spark GraphX</a></li></ul></li>
        <li>partitioning and parallelism<ul><li>URL / <a href="#ch04lvl1sec24" title="Partitioning and parallelism" class="link">Partitioning and parallelism</a></li></ul></li>
        <li>performance features<ul><li>URL / <a href="#ch04lvl1sec24" title="Executor memory and caching RDDs" class="link">Executor memory and caching RDDs</a></li></ul></li>
        <li>performance tuning<ul><li>about / <a href="#ch04lvl1sec24" title="Performance tuning" class="link">Performance tuning</a></li><li>partitioning / <a href="#ch04lvl1sec24" title="Partitioning and parallelism" class="link">Partitioning and parallelism</a></li><li>parallelism / <a href="#ch04lvl1sec24" title="Partitioning and parallelism" class="link">Partitioning and parallelism</a></li><li>serialization / <a href="#ch04lvl1sec24" title="Serialization" class="link">Serialization</a></li><li>Spark memory tuning / <a href="#ch04lvl1sec24" title="Spark memory tuning" class="link">Spark memory tuning</a></li></ul></li>
        <li>Point of Sale (POS) systems<ul><li>about / <a href="#ch02lvl1sec14" title="Real-time data processing" class="link">Real-time data processing</a></li></ul></li>
        <li>Pregel, Google<ul><li>URL / <a href="#ch06lvl1sec30" title="Graph analysis â€“ Spark GraphX" class="link">Graph analysis â€“ Spark GraphX</a></li></ul></li>
        <li>property graph model<ul><li>about / <a href="#ch06lvl1sec30" title="Graph analysis â€“ Spark GraphX" class="link">Graph analysis â€“ Spark GraphX</a></li></ul></li>
        <li>pseudo-distributed mode, Hadoop/YARN<ul><li>URL / <a href="#ch07lvl1sec32" title="Deploying on Hadoop or YARN" class="link">Deploying on Hadoop or YARN</a></li></ul></li>
        <li>Python APIs<ul><li>URL / <a href="#ch03lvl1sec19" title="Spark packaging structure and client APIs" class="link">Spark packaging structure and client APIs</a></li></ul></li>
      </ul>
      <h2>R</h2>
      <ul>
        <li>RDD<ul><li>about / <a href="#ch03lvl1sec20" title="Resilient distributed datasets and discretized streams" class="link">Resilient distributed datasets and discretized streams</a></li><li>defining / <a href="#ch03lvl1sec20" title="Motivation behind RDD" class="link">Motivation behind RDD</a></li><li>features / <a href="#ch03lvl1sec20" title="Motivation behind RDD" class="link">Motivation behind RDD</a></li><li>fault-tolerance / <a href="#ch03lvl1sec20" title="Fault tolerance" class="link">Fault tolerance</a></li><li>transformations / <a href="#ch03lvl1sec20" title="Transformations and actions" class="link">Transformations and actions</a></li><li>actions / <a href="#ch03lvl1sec20" title="Transformations and actions" class="link">Transformations and actions</a></li><li>functions / <a href="#ch03lvl1sec20" title="Transformations and actions" class="link">Transformations and actions</a></li><li>action functions / <a href="#ch03lvl1sec20" title="Transformations and actions" class="link">Transformations and actions</a></li><li>references / <a href="#ch03lvl1sec20" title="Transformations and actions" class="link">Transformations and actions</a></li><li>storage / <a href="#ch03lvl1sec20" title="RDD storage" class="link">RDD storage</a></li><li>persistence / <a href="#ch03lvl1sec20" title="RDD persistence" class="link">RDD persistence</a></li><li>shuffling / <a href="#ch03lvl1sec20" title="Shuffling in RDD" class="link">Shuffling in RDD</a></li></ul></li>
        <li>RDD (Resilient Distributed Datasets)<ul><li>about / <a href="#ch01lvl1sec10" title="Your first Spark program" class="link">Your first Spark program</a>, <a href="#ch02lvl1sec15" title="Spark versus Hadoop" class="link">Spark versus Hadoop</a></li></ul></li>
        <li>RDD, Java APIs<ul><li>defining / <a href="#ch03lvl1sec19" title="RDD â€“ Java APIs" class="link">RDD â€“ Java APIs</a></li></ul></li>
        <li>RDD, Scala APIs<ul><li>defining / <a href="#ch03lvl1sec19" title="RDD â€“ Scala APIs" class="link">RDD â€“ Scala APIs</a></li><li>URL / <a href="#ch03lvl1sec19" title="RDD â€“ Scala APIs" class="link">RDD â€“ Scala APIs</a></li></ul></li>
        <li>RDDs (Resilient Distributed Datasets)<ul><li>about / <a href="#ch07lvl1sec33" title="Fault tolerance in Spark Streaming" class="link">Fault tolerance in Spark Streaming</a></li></ul></li>
        <li>real-time data processing<ul><li>versus batch processing / <a href="#ch02lvl1sec14" title="Batch versus real-time data processing" class="link">Batch versus real-time data processing</a></li><li>defining / <a href="#ch02lvl1sec14" title="Real-time data processing" class="link">Real-time data processing</a></li></ul></li>
        <li>real-time data processing systems<ul><li>limitations / <a href="#ch02lvl1sec14" title="Real-time data processing" class="link">Real-time data processing</a></li></ul></li>
        <li>real-time systems<ul><li>examples / <a href="#ch02lvl1sec14" title="Real-time data processing" class="link">Real-time data processing</a></li></ul></li>
        <li>receiver's reliability<ul><li>URL / <a href="#ch07lvl1sec33" title="Fault tolerance in Spark Streaming" class="link">Fault tolerance in Spark Streaming</a></li></ul></li>
        <li>reduce function<ul><li>about / <a href="#ch04lvl1sec23" title="Windowing operations" class="link">Windowing operations</a></li></ul></li>
        <li>resource manager (RM)<ul><li>about / <a href="#ch07lvl1sec32" title="Deploying on Hadoop or YARN" class="link">Deploying on Hadoop or YARN</a></li></ul></li>
      </ul>
      <h2>S</h2>
      <ul>
        <li>Scala<ul><li>URL / <a href="#ch01lvl1sec08" title="RAM" class="link">RAM</a>, <a href="#ch03lvl1sec19" title="Spark packaging structure and client APIs" class="link">Spark packaging structure and client APIs</a></li></ul></li>
        <li>Scala 2.10.5 compressed tarball<ul><li>URL / <a href="#ch01lvl1sec08" title="Scala" class="link">Scala</a></li></ul></li>
        <li>Scala APIs<ul><li>defining, in SparkContext / <a href="#ch03lvl1sec19" title="SparkContext and Spark Config â€“ Scala APIs" class="link">SparkContext and Spark Config â€“ Scala APIs</a></li><li>defining, in SparkConf / <a href="#ch03lvl1sec19" title="SparkContext and Spark Config â€“ Scala APIs" class="link">SparkContext and Spark Config â€“ Scala APIs</a></li></ul></li>
        <li>Scala code<ul><li>URL / <a href="#ch01lvl1sec08" title="Eclipse" class="link">Eclipse</a></li></ul></li>
        <li>schema, using reflection<ul><li>URL / <a href="#ch06lvl1sec29" title="Understanding Spark SQL" class="link">Understanding Spark SQL</a></li></ul></li>
        <li>Seq<ul><li>URL / <a href="#ch05lvl1sec27" title="Coding Spark jobs for persisting streaming web logs in Cassandra" class="link">Coding Spark jobs for persisting streaming web logs in Cassandra</a></li></ul></li>
        <li>serialization<ul><li>about / <a href="#ch04lvl1sec24" title="Serialization" class="link">Serialization</a></li></ul></li>
        <li>shortest path<ul><li>URL / <a href="#ch03lvl1sec19" title="Spark GraphX" class="link">Spark GraphX</a></li></ul></li>
        <li>shuffling<ul><li>about / <a href="#ch03lvl1sec20" title="Shuffling in RDD" class="link">Shuffling in RDD</a></li><li>operations / <a href="#ch03lvl1sec20" title="Shuffling in RDD" class="link">Shuffling in RDD</a></li><li>URL / <a href="#ch03lvl1sec20" title="Shuffling in RDD" class="link">Shuffling in RDD</a></li></ul></li>
        <li>slave nodes<ul><li>references / <a href="#ch01lvl1sec11" title="Cluster management" class="link">Cluster management</a></li></ul></li>
        <li>software requirements, Spark<ul><li>defining / <a href="#ch01lvl1sec08" title="Software requirements" class="link">Software requirements</a></li><li>Spark, installing / <a href="#ch01lvl1sec08" title="Spark" class="link">Spark</a></li><li>Java, installing / <a href="#ch01lvl1sec08" title="Java" class="link">Java</a></li><li>Scala, installing / <a href="#ch01lvl1sec08" title="Scala" class="link">Scala</a></li><li>Eclipse, installing / <a href="#ch01lvl1sec08" title="Eclipse" class="link">Eclipse</a></li></ul></li>
        <li>Spark<ul><li>installing / <a href="#ch01lvl1sec08" title="Installation of Spark" class="link">Installation of Spark</a></li><li>hardware requirements / <a href="#ch01lvl1sec08" title="Hardware requirements" class="link">Hardware requirements</a></li><li>software requirements / <a href="#ch01lvl1sec08" title="Software requirements" class="link">Software requirements</a></li><li>references / <a href="#ch01lvl1sec12" title="Configuring port numbers" class="link">Configuring port numbers</a>, <a href="#ch03lvl1sec19" title="Other Spark Core packages" class="link">Other Spark Core packages</a></li><li>architecture / <a href="#ch02lvl1sec15" title="Architecture of Spark" class="link">Architecture of Spark</a></li><li>versus Hadoop / <a href="#ch02lvl1sec15" title="Spark versus Hadoop" class="link">Spark versus Hadoop</a></li><li>layered architecture / <a href="#ch02lvl1sec15" title="Layered architecture â€“ Spark" class="link">Layered architecture â€“ Spark</a></li><li>Data storage layer / <a href="#ch02lvl1sec15" title="Layered architecture â€“ Spark" class="link">Layered architecture â€“ Spark</a></li><li>Resource manager APIs / <a href="#ch02lvl1sec15" title="Layered architecture â€“ Spark" class="link">Layered architecture â€“ Spark</a></li><li>core libraries / <a href="#ch02lvl1sec15" title="Layered architecture â€“ Spark" class="link">Layered architecture â€“ Spark</a></li><li>extensions/libraries / <a href="#ch02lvl1sec15" title="Layered architecture â€“ Spark" class="link">Layered architecture â€“ Spark</a></li><li>packaging structure / <a href="#ch03lvl1sec19" title="Spark packaging structure and client APIs" class="link">Spark packaging structure and client APIs</a></li><li>client APIs / <a href="#ch03lvl1sec19" title="Spark packaging structure and client APIs" class="link">Spark packaging structure and client APIs</a></li><li>Core / <a href="#ch03lvl1sec19" title="Spark packaging structure and client APIs" class="link">Spark packaging structure and client APIs</a></li><li>libraries/extensions / <a href="#ch03lvl1sec19" title="Spark packaging structure and client APIs" class="link">Spark packaging structure and client APIs</a></li><li>libraries / <a href="#ch03lvl1sec19" title="Spark libraries and extensions" class="link">Spark libraries and extensions</a></li><li>extensions / <a href="#ch03lvl1sec19" title="Spark libraries and extensions" class="link">Spark libraries and extensions</a></li><li>configuring, to consume Flume events / <a href="#ch03lvl1sec21" title="Configuring Spark to consume Flume events" class="link">Configuring Spark to consume Flume events</a></li><li>configuring, for integration with Cassandra / <a href="#ch05lvl1sec27" title="Configuring Spark for integration with Cassandra" class="link">Configuring Spark for integration with Cassandra</a></li></ul></li>
        <li>Spark-Cassandra connector<ul><li>URL / <a href="#ch05lvl1sec27" title="Configuring Spark for integration with Cassandra" class="link">Configuring Spark for integration with Cassandra</a>, <a href="#ch05lvl1sec27" title="Coding Spark jobs for persisting streaming web logs in Cassandra" class="link">Coding Spark jobs for persisting streaming web logs in Cassandra</a></li></ul></li>
        <li>Spark-Cassandra Java library<ul><li>URL / <a href="#ch05lvl1sec27" title="Configuring Spark for integration with Cassandra" class="link">Configuring Spark for integration with Cassandra</a></li></ul></li>
        <li>Spark 1.2<ul><li>URL / <a href="#ch03lvl1sec19" title="Spark MLlib" class="link">Spark MLlib</a></li></ul></li>
        <li>Spark and Flume integration<ul><li>URL / <a href="#ch03lvl1sec21" title="Overall architecture of distributed log file processing" class="link">Overall architecture of distributed log file processing</a></li></ul></li>
        <li>Spark applications<ul><li>YARN client mode / <a href="#ch07lvl1sec32" title="Deploying on Hadoop or YARN" class="link">Deploying on Hadoop or YARN</a></li><li>YARN cluster mode / <a href="#ch07lvl1sec32" title="Deploying on Hadoop or YARN" class="link">Deploying on Hadoop or YARN</a></li></ul></li>
        <li>Spark binaries<ul><li>using / <a href="#ch01lvl1sec09" title="Configuring and running the Spark cluster" class="link">Configuring and running the Spark cluster</a></li></ul></li>
        <li>Spark cluster<ul><li>configuring / <a href="#ch01lvl1sec09" title="Configuring and running the Spark cluster" class="link">Configuring and running the Spark cluster</a></li><li>running / <a href="#ch01lvl1sec09" title="Configuring and running the Spark cluster" class="link">Configuring and running the Spark cluster</a></li></ul></li>
        <li>Spark clusters<ul><li>setting up, URL / <a href="#ch01lvl1sec12" title="Other common exceptions" class="link">Other common exceptions</a></li></ul></li>
        <li>Spark Community<ul><li>URL / <a href="#ch04lvl1sec24" title="Executor memory and caching RDDs" class="link">Executor memory and caching RDDs</a></li></ul></li>
        <li>Spark compressed tarball<ul><li>URL / <a href="#ch01lvl1sec08" title="Spark" class="link">Spark</a></li></ul></li>
        <li>SparkConf<ul><li>URL / <a href="#ch03lvl1sec19" title="SparkContext and Spark Config â€“ Scala APIs" class="link">SparkContext and Spark Config â€“ Scala APIs</a></li></ul></li>
        <li>SparkContext<ul><li>about / <a href="#ch01lvl1sec10" title="Your first Spark program" class="link">Your first Spark program</a></li><li>URL / <a href="#ch01lvl1sec10" title="Your first Spark program" class="link">Your first Spark program</a>, <a href="#ch03lvl1sec19" title="SparkContext and Spark Config â€“ Scala APIs" class="link">SparkContext and Spark Config â€“ Scala APIs</a></li></ul></li>
        <li>Spark Core<ul><li>defining / <a href="#ch03lvl1sec19" title="Spark Core" class="link">Spark Core</a></li><li>packages / <a href="#ch03lvl1sec19" title="Other Spark Core packages" class="link">Other Spark Core packages</a></li></ul></li>
        <li>Spark Core packages<ul><li>URL / <a href="#ch03lvl1sec19" title="Other Spark Core packages" class="link">Other Spark Core packages</a></li></ul></li>
        <li>Spark deployment models<ul><li>defining / <a href="#ch07lvl1sec32" title="Spark deployment models" class="link">Spark deployment models</a></li><li>deploying, on Apache Mesos / <a href="#ch07lvl1sec32" title="Deploying on Apache Mesos" class="link">Deploying on Apache Mesos</a></li><li>deploying, on Hadoop / <a href="#ch07lvl1sec32" title="Deploying on Hadoop or YARN" class="link">Deploying on Hadoop or YARN</a></li><li>deploying, on YARN / <a href="#ch07lvl1sec32" title="Deploying on Hadoop or YARN" class="link">Deploying on Hadoop or YARN</a></li></ul></li>
        <li>Spark extensions<ul><li>installing / <a href="#ch01lvl1sec08" title="Installing Spark extensions â€“ Spark Streaming" class="link">Installing Spark extensions â€“ Spark Streaming</a></li></ul></li>
        <li>Spark GraphX<ul><li>URL / <a href="#ch03lvl1sec19" title="Spark packaging structure and client APIs" class="link">Spark packaging structure and client APIs</a>, <a href="#ch03lvl1sec19" title="Spark GraphX" class="link">Spark GraphX</a>, <a href="#ch06lvl1sec30" title="Introduction to the GraphX API" class="link">Introduction to the GraphX API</a></li><li>about / <a href="#ch03lvl1sec19" title="Spark GraphX" class="link">Spark GraphX</a></li><li>packages / <a href="#ch03lvl1sec19" title="Spark GraphX" class="link">Spark GraphX</a></li><li>packages, references / <a href="#ch03lvl1sec19" title="Spark GraphX" class="link">Spark GraphX</a></li><li>used, for graph analysis / <a href="#ch06lvl1sec30" title="Graph analysis â€“ Spark GraphX" class="link">Graph analysis â€“ Spark GraphX</a></li><li>integrating, with Spark Streaming / <a href="#ch06lvl1sec30" title="Integration with Spark Streaming" class="link">Integration with Spark Streaming</a></li></ul></li>
        <li>Spark jobs<ul><li>submitting / <a href="#ch01lvl1sec11" title="Submitting Spark jobs" class="link">Submitting Spark jobs</a></li><li>coding, for streaming web logs in Cassandra / <a href="#ch05lvl1sec27" title="Coding Spark jobs for persisting streaming web logs in Cassandra" class="link">Coding Spark jobs for persisting streaming web logs in Cassandra</a></li></ul></li>
        <li>Spark libraries/extensions<ul><li>Spark Streaming / <a href="#ch03lvl1sec19" title="Spark packaging structure and client APIs" class="link">Spark packaging structure and client APIs</a></li><li>Spark SQL / <a href="#ch03lvl1sec19" title="Spark packaging structure and client APIs" class="link">Spark packaging structure and client APIs</a></li><li>Spark MLlib / <a href="#ch03lvl1sec19" title="Spark packaging structure and client APIs" class="link">Spark packaging structure and client APIs</a></li><li>Spark GraphX / <a href="#ch03lvl1sec19" title="Spark packaging structure and client APIs" class="link">Spark packaging structure and client APIs</a></li></ul></li>
        <li>Spark master and worker<ul><li>URL / <a href="#ch01lvl1sec11" title="Cluster management" class="link">Cluster management</a></li></ul></li>
        <li>Spark memory tuning<ul><li>about / <a href="#ch04lvl1sec24" title="Spark memory tuning" class="link">Spark memory tuning</a></li><li>garbage collection / <a href="#ch04lvl1sec24" title="Garbage collection" class="link">Garbage collection</a></li><li>object sizes / <a href="#ch04lvl1sec24" title="Object sizes" class="link">Object sizes</a></li><li>executor memory / <a href="#ch04lvl1sec24" title="Executor memory and caching RDDs" class="link">Executor memory and caching RDDs</a></li><li>RDDs, caching / <a href="#ch04lvl1sec24" title="Executor memory and caching RDDs" class="link">Executor memory and caching RDDs</a></li></ul></li>
        <li>Spark MLlib<ul><li>URL / <a href="#ch03lvl1sec19" title="Spark packaging structure and client APIs" class="link">Spark packaging structure and client APIs</a></li><li>about / <a href="#ch03lvl1sec19" title="Spark MLlib" class="link">Spark MLlib</a></li></ul></li>
        <li>Spark program<ul><li>defining / <a href="#ch01lvl1sec10" title="Your first Spark program" class="link">Your first Spark program</a></li><li>Spark jobs, coding in Scala / <a href="#ch01lvl1sec10" title="Coding Spark jobs in Scala" class="link">Coding Spark jobs in Scala</a></li><li>Spark jobs, coding in Java / <a href="#ch01lvl1sec10" title="Coding Spark jobs in Java" class="link">Coding Spark jobs in Java</a></li></ul></li>
        <li>Spark SQL<ul><li>about / <a href="#ch03lvl1sec19" title="Spark SQL" class="link">Spark SQL</a>, <a href="#ch06lvl1sec29" title="Understanding Spark SQL" class="link">Understanding Spark SQL</a></li><li>URL / <a href="#ch03lvl1sec19" title="Spark SQL" class="link">Spark SQL</a></li><li>features / <a href="#ch06lvl1sec29" title="Understanding Spark SQL" class="link">Understanding Spark SQL</a></li><li>integrating, with streams / <a href="#ch06lvl1sec29" title="Integrating Spark SQL with streams" class="link">Integrating Spark SQL with streams</a></li></ul></li>
        <li>Spark SQL, with Cassandra<ul><li>URL / <a href="#ch06lvl1sec29" title="Understanding Spark SQL" class="link">Understanding Spark SQL</a></li></ul></li>
        <li>Spark SQL packages<ul><li>references / <a href="#ch03lvl1sec19" title="Spark SQL" class="link">Spark SQL</a></li></ul></li>
        <li>Spark Streaming<ul><li>about / <a href="#ch01lvl1sec08" title="Installing Spark extensions â€“ Spark Streaming" class="link">Installing Spark extensions â€“ Spark Streaming</a>, <a href="#ch03lvl1sec19" title="Spark Streaming" class="link">Spark Streaming</a></li><li>architecture / <a href="#ch02lvl1sec16" title="Architecture of Spark Streaming" class="link">Architecture of Spark Streaming</a></li><li>defining / <a href="#ch02lvl1sec16" title="What is Spark Streaming?" class="link">What is Spark Streaming?</a></li><li>URL / <a href="#ch02lvl1sec16" title="What is Spark Streaming?" class="link">What is Spark Streaming?</a></li><li>high-level architecture / <a href="#ch02lvl1sec16" title="High-level architecture â€“ Spark Streaming" class="link">High-level architecture â€“ Spark Streaming</a></li><li>functions / <a href="#ch03lvl1sec19" title="Spark Streaming" class="link">Spark Streaming</a></li><li>sub packages, defining / <a href="#ch03lvl1sec19" title="Spark Streaming" class="link">Spark Streaming</a></li><li>output operations / <a href="#ch05lvl1sec26" title="Output operations in Spark Streaming" class="link">Output operations in Spark Streaming</a></li></ul></li>
        <li>Spark Streaming APIs<ul><li>URL / <a href="#ch03lvl1sec19" title="Spark Streaming" class="link">Spark Streaming</a></li></ul></li>
        <li>Spark Streaming job<ul><li>packaging / <a href="#ch03lvl1sec21" title="Packaging and deploying a Spark Streaming job" class="link">Packaging and deploying a Spark Streaming job</a></li><li>deploying / <a href="#ch03lvl1sec21" title="Packaging and deploying a Spark Streaming job" class="link">Packaging and deploying a Spark Streaming job</a></li></ul></li>
        <li>Spark Streaming program<ul><li>defining / <a href="#ch02lvl1sec17" title="Your first Spark Streaming program" class="link">Your first Spark Streaming program</a></li><li>Spark Streaming jobs, coding in Scala / <a href="#ch02lvl1sec17" title="Coding Spark Streaming jobs in Scala" class="link">Coding Spark Streaming jobs in Scala</a></li><li>Spark Streaming jobs, coding in Java / <a href="#ch02lvl1sec17" title="Coding Spark Streaming jobs in Java" class="link">Coding Spark Streaming jobs in Java</a></li><li>client application / <a href="#ch02lvl1sec17" title="The client application" class="link">The client application</a></li><li>Spark Streaming job, packaging / <a href="#ch02lvl1sec17" title="Packaging and deploying a Spark Streaming job" class="link">Packaging and deploying a Spark Streaming job</a></li><li>Spark Streaming job, deploying / <a href="#ch02lvl1sec17" title="Packaging and deploying a Spark Streaming job" class="link">Packaging and deploying a Spark Streaming job</a></li></ul></li>
        <li>spoofing<ul><li>URL / <a href="#ch03lvl1sec21" title="Data loading from distributed and varied sources" class="link">Data loading from distributed and varied sources</a></li></ul></li>
        <li>SQLContext<ul><li>URL / <a href="#ch06lvl1sec29" title="Understanding Spark SQL" class="link">Understanding Spark SQL</a></li></ul></li>
        <li>standalone mode<ul><li>about / <a href="#ch01lvl1sec09" title="Configuring and running the Spark cluster" class="link">Configuring and running the Spark cluster</a></li></ul></li>
        <li>standalone mode, Hadoop/YARN<ul><li>URL / <a href="#ch07lvl1sec32" title="Deploying on Hadoop or YARN" class="link">Deploying on Hadoop or YARN</a></li></ul></li>
        <li>StorageLevel class<ul><li>URL / <a href="#ch03lvl1sec20" title="RDD persistence" class="link">RDD persistence</a></li></ul></li>
        <li>streaming data<ul><li>querying, in real time / <a href="#ch06lvl1sec29" title="Querying streaming data in real time" class="link">Querying streaming data in real time</a></li></ul></li>
        <li>streaming jobs<ul><li>monitoring / <a href="#ch07lvl1sec34" title="Monitoring streaming jobs" class="link">Monitoring streaming jobs</a></li><li>application / <a href="#ch07lvl1sec34" title="Application or job UI" class="link">Application or job UI</a></li><li>job UI / <a href="#ch07lvl1sec34" title="Application or job UI" class="link">Application or job UI</a></li><li>integration, with other monitoring tool / <a href="#ch07lvl1sec34" title="Integration with other monitoring tools" class="link">Integration with other monitoring tools</a></li></ul></li>
        <li>support vector machines<ul><li>URL / <a href="#ch03lvl1sec19" title="Spark MLlib" class="link">Spark MLlib</a></li></ul></li>
        <li>SVD<ul><li>URL / <a href="#ch03lvl1sec19" title="Spark GraphX" class="link">Spark GraphX</a></li></ul></li>
      </ul>
      <h2>T</h2>
      <ul>
        <li>Tachyon<ul><li>URL / <a href="#ch04lvl1sec24" title="Executor memory and caching RDDs" class="link">Executor memory and caching RDDs</a></li></ul></li>
        <li>transformation functions<ul><li>defining / <a href="#ch04lvl1sec23" title="Understanding and applying transformation functions" class="link">Understanding and applying transformation functions</a></li><li>applying / <a href="#ch04lvl1sec23" title="Understanding and applying transformation functions" class="link">Understanding and applying transformation functions</a></li><li>log streaming, simulating / <a href="#ch04lvl1sec23" title="Simulating log streaming" class="link">Simulating log streaming</a></li><li>functional operations / <a href="#ch04lvl1sec23" title="Functional operations" class="link">Functional operations</a></li><li>transform operations / <a href="#ch04lvl1sec23" title="Transform operations" class="link">Transform operations</a></li><li>windowing operations / <a href="#ch04lvl1sec23" title="Windowing operations" class="link">Windowing operations</a></li></ul></li>
        <li>troubleshooting<ul><li>about / <a href="#ch01lvl1sec12" title="Troubleshooting" class="link">Troubleshooting</a></li><li>PORT numbers, configuring / <a href="#ch01lvl1sec12" title="Configuring port numbers" class="link">Configuring port numbers</a></li><li>classpath issues / <a href="#ch01lvl1sec12" title="Classpath issues â€“ class not found exception" class="link">Classpath issues â€“ class not found exception</a></li><li>common exceptions / <a href="#ch01lvl1sec12" title="Other common exceptions" class="link">Other common exceptions</a></li></ul></li>
      </ul>
      <h2>U</h2>
      <ul>
        <li>utility JAR file<ul><li>URL / <a href="#ch06lvl1sec29" title="Integrating Spark SQL with streams" class="link">Integrating Spark SQL with streams</a></li></ul></li>
      </ul>
      <h2>V</h2>
      <ul>
        <li>vertices<ul><li>about / <a href="#ch06lvl1sec30" title="Graph analysis â€“ Spark GraphX" class="link">Graph analysis â€“ Spark GraphX</a></li></ul></li>
      </ul>
      <h2>Y</h2>
      <ul>
        <li>YARN<ul><li>deploying on / <a href="#ch07lvl1sec32" title="Deploying on Hadoop or YARN" class="link">Deploying on Hadoop or YARN</a></li><li>URL / <a href="#ch07lvl1sec33" title="Fault tolerance in Spark Streaming" class="link">Fault tolerance in Spark Streaming</a></li></ul></li>
        <li>YARN and Mesos<ul><li>references / <a href="#ch07lvl1sec32" title="Deploying on Hadoop or YARN" class="link">Deploying on Hadoop or YARN</a></li></ul></li>
      </ul>
      <h2>Z</h2>
      <ul>
        <li>Zero MQ<ul><li>URL / <a href="#ch03lvl1sec19" title="Spark Streaming" class="link">Spark Streaming</a></li></ul></li>
        <li>ZooKeeper<ul><li>URL / <a href="#ch07lvl1sec33" title="High availability in the standalone mode" class="link">High availability in the standalone mode</a></li></ul></li>
      </ul>
    </div>
  </div></div></div>
</div></div></div></body></html>
