<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="">

    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0"> <!--320-->
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no, user-scalable=no">

    <link rel="icon" href="../../mapt/images/favicon.ico">

    <link rel="stylesheet" href="../../mapt/css/font-awesome.css">
    <link rel="stylesheet" href="../../mapt/css/google-fonts.css">
    <link rel="stylesheet" href="../../mapt/css/devicon.css">

    <link rel="stylesheet" href="../../mapt/css/bootstrap.css">
    <link rel="stylesheet" href="../../mapt/css/bootstrap-xl.css">
    <link rel="stylesheet" href="../../mapt/css/magnific-popup.css">
    <link rel="stylesheet" href="../../mapt/css/prism.css">
    <link rel="stylesheet" href="../../mapt/css/hljs-github.css">

    <link rel="stylesheet" href="../../mapt/css/mapt.css">
    <link rel="stylesheet" href="../../mapt/css/custom.css">

    <script src="../../mapt/js/jquery.js"></script>
    <script src="../../mapt/js/bootstrap.js"></script>
    <script src="../../mapt/js/jquery.magnific-popup.js"></script>
    <script src="../../mapt/js/highlight.min.js"></script>

    <script src="../../mapt/js/custom.js"></script>
    
    <title>Fast Data Processing with Spark - Second Edition</title>
</head>

<body class="home-body">
    <div id="wrapper">
        <div id="sidebar-wrapper">    
            <ul class="sidebar-nav">
                <div class="list-group" id="sidebar-nav" role="tablist">
                    <li>
                        <a href="../../index.html" class="sidenav-menu-holder back-btn" id="back-link">
                            <span class="sidenav-menu">Book List</span>
                            <span class="pull-left mr5"><i class="fa fa-chevron-left"></i></span>
                        </a>
                    </li>
                    
                    <li class="book-info copyright">
                        <span class="info text-nowrap"><span class="copyleft">&copy;</span><span><strong>RuTracker</strong>.org</span></span>
                    </li>          
                    <li class="book-info copyright">
                        <span class="info text-nowrap">Pub date: <strong>31 Mar 2015</strong></span>
                    </li>         
                    <li class="book-info">
                        <span class="info text-nowrap">Price: €<strong>19.99</strong></span>
                        <span class="info text-nowrap">ISBN: <strong>9781784392574</strong></span>
                    </li>     
            
                    <li>
                        <a href="graphics/cover.jpg" class="sidenav-menu-holder cover-img">
                            <img src="graphics/cover.jpg" class="cover-image">
                        </a>
                    </li>        
            
                    <div class="book_navigation">
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse1">
                                <div class="section-name">1: Installing Spark and Setting up your Cluster</div>
                            </a>
                        </li>
                        <div id="collapse1" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="1" class="sub-nav">
                                <a href="#ch01">
                                    <div class="section-name">Chapter 1: Installing Spark and Setting up your Cluster</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec08" class="sub-nav">
                                <a href="#ch01lvl1sec08">                    
                                    <div class="section-name">Directory organization and convention</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec09" class="sub-nav">
                                <a href="#ch01lvl1sec09">                    
                                    <div class="section-name">Installing prebuilt distribution</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec10" class="sub-nav">
                                <a href="#ch01lvl1sec10">                    
                                    <div class="section-name">Building Spark from source</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec11" class="sub-nav">
                                <a href="#ch01lvl1sec11">                    
                                    <div class="section-name">Spark topology</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec12" class="sub-nav">
                                <a href="#ch01lvl1sec12">                    
                                    <div class="section-name">A single machine</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec13" class="sub-nav">
                                <a href="#ch01lvl1sec13">                    
                                    <div class="section-name">Running Spark on EC2</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec14" class="sub-nav">
                                <a href="#ch01lvl1sec14">                    
                                    <div class="section-name">Deploying Spark with Chef (Opscode)</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec15" class="sub-nav">
                                <a href="#ch01lvl1sec15">                    
                                    <div class="section-name">Deploying Spark on Mesos</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec16" class="sub-nav">
                                <a href="#ch01lvl1sec16">                    
                                    <div class="section-name">Spark on YARN</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec17" class="sub-nav">
                                <a href="#ch01lvl1sec17">                    
                                    <div class="section-name">Spark Standalone mode</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec18" class="sub-nav">
                                <a href="#ch01lvl1sec18">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse2">
                                <div class="section-name">2: Using the Spark Shell</div>
                            </a>
                        </li>
                        <div id="collapse2" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="2" class="sub-nav">
                                <a href="#ch02">
                                    <div class="section-name">Chapter 2: Using the Spark Shell</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec19" class="sub-nav">
                                <a href="#ch02lvl1sec19">                    
                                    <div class="section-name">Loading a simple text file</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec20" class="sub-nav">
                                <a href="#ch02lvl1sec20">                    
                                    <div class="section-name">Using the Spark shell to run logistic regression</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec21" class="sub-nav">
                                <a href="#ch02lvl1sec21">                    
                                    <div class="section-name">Interactively loading data from S3</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec22" class="sub-nav">
                                <a href="#ch02lvl1sec22">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse3">
                                <div class="section-name">3: Building and Running a Spark Application</div>
                            </a>
                        </li>
                        <div id="collapse3" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="3" class="sub-nav">
                                <a href="#ch03">
                                    <div class="section-name">Chapter 3: Building and Running a Spark Application</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec23" class="sub-nav">
                                <a href="#ch03lvl1sec23">                    
                                    <div class="section-name">Building your Spark project with sbt</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec24" class="sub-nav">
                                <a href="#ch03lvl1sec24">                    
                                    <div class="section-name">Building your Spark job with Maven</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec25" class="sub-nav">
                                <a href="#ch03lvl1sec25">                    
                                    <div class="section-name">Building your Spark job with something else</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec26" class="sub-nav">
                                <a href="#ch03lvl1sec26">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse4">
                                <div class="section-name">4: Creating a SparkContext</div>
                            </a>
                        </li>
                        <div id="collapse4" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="4" class="sub-nav">
                                <a href="#ch04">
                                    <div class="section-name">Chapter 4: Creating a SparkContext</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec27" class="sub-nav">
                                <a href="#ch04lvl1sec27">                    
                                    <div class="section-name">Scala</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec28" class="sub-nav">
                                <a href="#ch04lvl1sec28">                    
                                    <div class="section-name">Java</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec29" class="sub-nav">
                                <a href="#ch04lvl1sec29">                    
                                    <div class="section-name">SparkContext metadata</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec30" class="sub-nav">
                                <a href="#ch04lvl1sec30">                    
                                    <div class="section-name">Shared Java and Scala APIs</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec31" class="sub-nav">
                                <a href="#ch04lvl1sec31">                    
                                    <div class="section-name">Python</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec32" class="sub-nav">
                                <a href="#ch04lvl1sec32">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse5">
                                <div class="section-name">5: Loading and Saving Data in Spark</div>
                            </a>
                        </li>
                        <div id="collapse5" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="5" class="sub-nav">
                                <a href="#ch05">
                                    <div class="section-name">Chapter 5: Loading and Saving Data in Spark</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec33" class="sub-nav">
                                <a href="#ch05lvl1sec33">                    
                                    <div class="section-name">RDDs</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec34" class="sub-nav">
                                <a href="#ch05lvl1sec34">                    
                                    <div class="section-name">Loading data into an RDD</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec35" class="sub-nav">
                                <a href="#ch05lvl1sec35">                    
                                    <div class="section-name">Saving your data</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec36" class="sub-nav">
                                <a href="#ch05lvl1sec36">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse6">
                                <div class="section-name">6: Manipulating your RDD</div>
                            </a>
                        </li>
                        <div id="collapse6" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="6" class="sub-nav">
                                <a href="#ch06">
                                    <div class="section-name">Chapter 6: Manipulating your RDD</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec37" class="sub-nav">
                                <a href="#ch06lvl1sec37">                    
                                    <div class="section-name">Manipulating your RDD in Scala and Java</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec38" class="sub-nav">
                                <a href="#ch06lvl1sec38">                    
                                    <div class="section-name">Manipulating your RDD in Python</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec39" class="sub-nav">
                                <a href="#ch06lvl1sec39">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse7">
                                <div class="section-name">7: Spark SQL</div>
                            </a>
                        </li>
                        <div id="collapse7" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="7" class="sub-nav">
                                <a href="#ch07">
                                    <div class="section-name">Chapter 7: Spark SQL</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec40" class="sub-nav">
                                <a href="#ch07lvl1sec40">                    
                                    <div class="section-name">The Spark SQL architecture</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec41" class="sub-nav">
                                <a href="#ch07lvl1sec41">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse8">
                                <div class="section-name">8: Spark with Big Data</div>
                            </a>
                        </li>
                        <div id="collapse8" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="8" class="sub-nav">
                                <a href="#ch08">
                                    <div class="section-name">Chapter 8: Spark with Big Data</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec42" class="sub-nav">
                                <a href="#ch08lvl1sec42">                    
                                    <div class="section-name">Parquet an efficient and interoperable big data format</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec43" class="sub-nav">
                                <a href="#ch08lvl1sec43">                    
                                    <div class="section-name">Querying Parquet files with Impala</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec44" class="sub-nav">
                                <a href="#ch08lvl1sec44">                    
                                    <div class="section-name">HBase</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec45" class="sub-nav">
                                <a href="#ch08lvl1sec45">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse9">
                                <div class="section-name">9: Machine Learning Using Spark MLlib</div>
                            </a>
                        </li>
                        <div id="collapse9" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="9" class="sub-nav">
                                <a href="#ch09">
                                    <div class="section-name">Chapter 9: Machine Learning Using Spark MLlib</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec46" class="sub-nav">
                                <a href="#ch09lvl1sec46">                    
                                    <div class="section-name">The Spark machine learning algorithm table</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec47" class="sub-nav">
                                <a href="#ch09lvl1sec47">                    
                                    <div class="section-name">Spark MLlib examples</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec48" class="sub-nav">
                                <a href="#ch09lvl1sec48">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse10">
                                <div class="section-name">10: Testing</div>
                            </a>
                        </li>
                        <div id="collapse10" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="10" class="sub-nav">
                                <a href="#ch10">
                                    <div class="section-name">Chapter 10: Testing</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec49" class="sub-nav">
                                <a href="#ch10lvl1sec49">                    
                                    <div class="section-name">Testing in Java and Scala</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec50" class="sub-nav">
                                <a href="#ch10lvl1sec50">                    
                                    <div class="section-name">Testing in Python</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec51" class="sub-nav">
                                <a href="#ch10lvl1sec51">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse11">
                                <div class="section-name">11: Tips and Tricks</div>
                            </a>
                        </li>
                        <div id="collapse11" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="11" class="sub-nav">
                                <a href="#ch11">
                                    <div class="section-name">Chapter 11: Tips and Tricks</div>
                                </a>
                            </li>
                            <li data-chapter="11" data-section-id="ch11lvl1sec52" class="sub-nav">
                                <a href="#ch11lvl1sec52">                    
                                    <div class="section-name">Where to find logs</div>
                                </a>
                            </li>
                            <li data-chapter="11" data-section-id="ch11lvl1sec53" class="sub-nav">
                                <a href="#ch11lvl1sec53">                    
                                    <div class="section-name">Concurrency limitations</div>
                                </a>
                            </li>
                            <li data-chapter="11" data-section-id="ch11lvl1sec54" class="sub-nav">
                                <a href="#ch11lvl1sec54">                    
                                    <div class="section-name">Using Spark with other languages</div>
                                </a>
                            </li>
                            <li data-chapter="11" data-section-id="ch11lvl1sec55" class="sub-nav">
                                <a href="#ch11lvl1sec55">                    
                                    <div class="section-name">A quick note on security</div>
                                </a>
                            </li>
                            <li data-chapter="11" data-section-id="ch11lvl1sec56" class="sub-nav">
                                <a href="#ch11lvl1sec56">                    
                                    <div class="section-name">Community developed packages</div>
                                </a>
                            </li>
                            <li data-chapter="11" data-section-id="ch11lvl1sec57" class="sub-nav">
                                <a href="#ch11lvl1sec57">                    
                                    <div class="section-name">Mailing lists</div>
                                </a>
                            </li>
                            <li data-chapter="11" data-section-id="ch11lvl1sec58" class="sub-nav">
                                <a href="#ch11lvl1sec58">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapsebackindex">
                                <div class="section-name">Appendix A: Index</div>
                            </a>
                        </li>
                        <div id="collapsebackindex" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="backindex" class="sub-nav">
                                <a href="#backindex">
                                    <div class="section-name">Chapter Appendix A: Index</div>
                                </a>
                            </li>
                        </div>
                    </div>
                </div>
            </ul>
        </div>
        
        <div id="page-content-wrapper" class="book-page">
            <a href="#" id="menu-toggle" class="toggle-nav"><i class="fa fa-bars fa-2x mr5"></i></a>
            
            <a href="#" id="back_to_top" class="back-to-top"><img src="../../mapt/images/kopimi.svg"></a>
            
            <div class="col-sm-12 col-xl-offset-2 col-xl-8 col-lg-offset-1 col-lg-10">
                <div class="btn-group pull-right mt15 mb30" role="group">
                    <a href="#home" class="btn btn-default">
                        <i class="fa fa-share fa-lg no-text-padding"></i>
                        <span class="hidden-xs ml5">Book Home</span>
                    </a>
                    <button class="btn btn-default" data-nid="20817" id="code-download">
                        <i class="fa fa-file fa-lg"></i>
                        <span class="hidden-xs ml5">Download Code Files</span>
                    </button>
                </div>
            </div>
            <div class="clearfix"></div>
            
            <div id="book-wrapper" class="container-fluid">
                <div class="col-sm-12 col-xl-offset-2 col-xl-8 col-lg-offset-1 col-lg-10" id="home">
                    <h2 class="product-title">Fast Data Processing with Spark - Second Edition</h2>
                    <hr>
                    <div class="row">
                        <div class="col-sm-12">
                            <h5 class="mt10">By Krishna Sankar, Holden Karau</h5>
                            <div>
                                <p class="mb20"><b>Perform real-time analytics using Spark in a fast, distributed, and scalable way</b></p>
                                <a href="#ch01" class="btn btn-info btn-lg pull-right hidden-xs">
                                    Start Reading <i class="fa fa-chevron-right ml5"></i>
                                </a>
                                <a href="#ch01" class="btn btn-info btn-lg btn-block mt20 mb20 visible-xs">
                                    Start Reading <i class="fa fa-chevron-right ml5"></i>
                                </a>
                                <div class="clearfix"></div>
                                <div class="col-sm-12">
                                    <ul id="myTabs" class="nav nav-tabs nav-justified hidden-xs mt20" role="tablist">
                                        <li class="active">
                                            <a href="#info" role="tab" id="info-tab" data-toggle="tab">
                                                <h5>Info</h5>
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#content" role="tab" id="content-tab" data-toggle="tab">
                                                <h5>Contents</h5>
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#author" role="tab" id="author-tab" data-toggle="tab">
                                                <h5>Author</h5>
                                            </a>
                                        </li>
                                    </ul>
                
                                    <ul id="myTabsMobile" class="nav nav-pills text-center nav-stacked visible-xs mb60" role="tablist">
                                        <li class="active">
                                            <a href="#info" role="tab" id="info-tab-responsive" data-toggle="tab">
                                                Info
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#content" role="tab" id="content-tab-responsive" data-toggle="tab">
                                                Contents
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#author" role="tab" id="author-tab-responsive" data-toggle="tab">
                                                Author
                                            </a>
                                        </li>
                                    </ul>
                
                                    <div id="myTabContent" class="tab-content pt30">
                                    
                                        <div role="tabpanel" class="tab-pane active fade in" id="info">
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">Features</h5>
                                            </div>
                                            <div class="hidden-xs">
                                                <h5>Features</h5>
                                            </div>
                                            <hr>
                                            <div>
                                                <ul>
                <li>Develop a machine learning system with Spark’s MLlib and scalable algorithms</li>
                <li>Deploy Spark jobs to various clusters such as Mesos, EC2, Chef, YARN, EMR, and so on</li>
                <li>This is a step-by-step tutorial that unleashes the power of Spark and its latest features</li>
                </ul>
                                            </div>
                                            <br>
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">Learning</h5>
                                            </div>
                                            <div class="hidden-xs">
                                                <h5>Learning</h5>
                                            </div>
                                            <hr>
                                            <div>
                                                <ul>
                <li>Install and set up Spark on your cluster</li>
                <li>Prototype distributed applications with Spark's interactive shell</li>
                <li>Learn different ways to interact with Spark's distributed representation of data (RDDs)</li>
                <li>Query Spark with a SQL-like query syntax</li>
                <li>Effectively test your distributed software</li>
                <li>Recognize how Spark works with big data</li>
                <li>Implement machine learning systems with highly scalable algorithms</li>
                </ul>
                                            </div>
                                            <br>
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">About</h5>
                                            </div>
                                            <div class="hidden-xs">
                                                <h5>About</h5>
                                            </div>
                                            <hr>
                                            <div>
                                                <p>Spark is a framework used for writing fast, distributed programs. Spark solves similar problems as Hadoop MapReduce does, but with a fast in-memory approach and a clean functional style API. With its ability to integrate with Hadoop and built-in tools for interactive query analysis (Spark SQL), large-scale graph processing and analysis (GraphX), and real-time analysis (Spark Streaming), it can be interactively used to quickly process and query big datasets.</p>
                <p>Fast Data Processing with Spark - Second Edition covers how to write distributed programs with Spark. The book will guide you through every step required to write effective distributed programs from setting up your cluster and interactively exploring the API to developing analytics applications and tuning them for your purposes.</p>
                                            </div>
                                        </div>
                                        
                                        <div role="tabpanel" class="tab-pane fade in" id="content">
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">Contents</h5>
                                                <hr>
                                            </div>
                                            <ul>
                                                <div>
                                                    <li data-chapter="1">
                                                        <div class="section-name">1: Installing Spark and Setting up your Cluster</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="1" class="chapter-section">
                                                                    <a href="#ch01">        
                                                                        <div class="section-name">Chapter 1: Installing Spark and Setting up your Cluster</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec08" class="chapter-section">
                                                                    <a href="#ch01lvl1sec08">                    
                                                                        <div class="section-name">Directory organization and convention</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec09" class="chapter-section">
                                                                    <a href="#ch01lvl1sec09">                    
                                                                        <div class="section-name">Installing prebuilt distribution</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec10" class="chapter-section">
                                                                    <a href="#ch01lvl1sec10">                    
                                                                        <div class="section-name">Building Spark from source</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec11" class="chapter-section">
                                                                    <a href="#ch01lvl1sec11">                    
                                                                        <div class="section-name">Spark topology</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec12" class="chapter-section">
                                                                    <a href="#ch01lvl1sec12">                    
                                                                        <div class="section-name">A single machine</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec13" class="chapter-section">
                                                                    <a href="#ch01lvl1sec13">                    
                                                                        <div class="section-name">Running Spark on EC2</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec14" class="chapter-section">
                                                                    <a href="#ch01lvl1sec14">                    
                                                                        <div class="section-name">Deploying Spark with Chef (Opscode)</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec15" class="chapter-section">
                                                                    <a href="#ch01lvl1sec15">                    
                                                                        <div class="section-name">Deploying Spark on Mesos</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec16" class="chapter-section">
                                                                    <a href="#ch01lvl1sec16">                    
                                                                        <div class="section-name">Spark on YARN</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec17" class="chapter-section">
                                                                    <a href="#ch01lvl1sec17">                    
                                                                        <div class="section-name">Spark Standalone mode</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec18" class="chapter-section">
                                                                    <a href="#ch01lvl1sec18">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="2">
                                                        <div class="section-name">2: Using the Spark Shell</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="2" class="chapter-section">
                                                                    <a href="#ch02">        
                                                                        <div class="section-name">Chapter 2: Using the Spark Shell</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec19" class="chapter-section">
                                                                    <a href="#ch02lvl1sec19">                    
                                                                        <div class="section-name">Loading a simple text file</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec20" class="chapter-section">
                                                                    <a href="#ch02lvl1sec20">                    
                                                                        <div class="section-name">Using the Spark shell to run logistic regression</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec21" class="chapter-section">
                                                                    <a href="#ch02lvl1sec21">                    
                                                                        <div class="section-name">Interactively loading data from S3</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec22" class="chapter-section">
                                                                    <a href="#ch02lvl1sec22">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="3">
                                                        <div class="section-name">3: Building and Running a Spark Application</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="3" class="chapter-section">
                                                                    <a href="#ch03">        
                                                                        <div class="section-name">Chapter 3: Building and Running a Spark Application</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec23" class="chapter-section">
                                                                    <a href="#ch03lvl1sec23">                    
                                                                        <div class="section-name">Building your Spark project with sbt</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec24" class="chapter-section">
                                                                    <a href="#ch03lvl1sec24">                    
                                                                        <div class="section-name">Building your Spark job with Maven</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec25" class="chapter-section">
                                                                    <a href="#ch03lvl1sec25">                    
                                                                        <div class="section-name">Building your Spark job with something else</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec26" class="chapter-section">
                                                                    <a href="#ch03lvl1sec26">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="4">
                                                        <div class="section-name">4: Creating a SparkContext</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="4" class="chapter-section">
                                                                    <a href="#ch04">        
                                                                        <div class="section-name">Chapter 4: Creating a SparkContext</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec27" class="chapter-section">
                                                                    <a href="#ch04lvl1sec27">                    
                                                                        <div class="section-name">Scala</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec28" class="chapter-section">
                                                                    <a href="#ch04lvl1sec28">                    
                                                                        <div class="section-name">Java</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec29" class="chapter-section">
                                                                    <a href="#ch04lvl1sec29">                    
                                                                        <div class="section-name">SparkContext metadata</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec30" class="chapter-section">
                                                                    <a href="#ch04lvl1sec30">                    
                                                                        <div class="section-name">Shared Java and Scala APIs</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec31" class="chapter-section">
                                                                    <a href="#ch04lvl1sec31">                    
                                                                        <div class="section-name">Python</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec32" class="chapter-section">
                                                                    <a href="#ch04lvl1sec32">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="5">
                                                        <div class="section-name">5: Loading and Saving Data in Spark</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="5" class="chapter-section">
                                                                    <a href="#ch05">        
                                                                        <div class="section-name">Chapter 5: Loading and Saving Data in Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec33" class="chapter-section">
                                                                    <a href="#ch05lvl1sec33">                    
                                                                        <div class="section-name">RDDs</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec34" class="chapter-section">
                                                                    <a href="#ch05lvl1sec34">                    
                                                                        <div class="section-name">Loading data into an RDD</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec35" class="chapter-section">
                                                                    <a href="#ch05lvl1sec35">                    
                                                                        <div class="section-name">Saving your data</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec36" class="chapter-section">
                                                                    <a href="#ch05lvl1sec36">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="6">
                                                        <div class="section-name">6: Manipulating your RDD</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="6" class="chapter-section">
                                                                    <a href="#ch06">        
                                                                        <div class="section-name">Chapter 6: Manipulating your RDD</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec37" class="chapter-section">
                                                                    <a href="#ch06lvl1sec37">                    
                                                                        <div class="section-name">Manipulating your RDD in Scala and Java</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec38" class="chapter-section">
                                                                    <a href="#ch06lvl1sec38">                    
                                                                        <div class="section-name">Manipulating your RDD in Python</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec39" class="chapter-section">
                                                                    <a href="#ch06lvl1sec39">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="7">
                                                        <div class="section-name">7: Spark SQL</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="7" class="chapter-section">
                                                                    <a href="#ch07">        
                                                                        <div class="section-name">Chapter 7: Spark SQL</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec40" class="chapter-section">
                                                                    <a href="#ch07lvl1sec40">                    
                                                                        <div class="section-name">The Spark SQL architecture</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec41" class="chapter-section">
                                                                    <a href="#ch07lvl1sec41">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="8">
                                                        <div class="section-name">8: Spark with Big Data</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="8" class="chapter-section">
                                                                    <a href="#ch08">        
                                                                        <div class="section-name">Chapter 8: Spark with Big Data</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec42" class="chapter-section">
                                                                    <a href="#ch08lvl1sec42">                    
                                                                        <div class="section-name">Parquet an efficient and interoperable big data format</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec43" class="chapter-section">
                                                                    <a href="#ch08lvl1sec43">                    
                                                                        <div class="section-name">Querying Parquet files with Impala</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec44" class="chapter-section">
                                                                    <a href="#ch08lvl1sec44">                    
                                                                        <div class="section-name">HBase</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec45" class="chapter-section">
                                                                    <a href="#ch08lvl1sec45">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="9">
                                                        <div class="section-name">9: Machine Learning Using Spark MLlib</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="9" class="chapter-section">
                                                                    <a href="#ch09">        
                                                                        <div class="section-name">Chapter 9: Machine Learning Using Spark MLlib</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec46" class="chapter-section">
                                                                    <a href="#ch09lvl1sec46">                    
                                                                        <div class="section-name">The Spark machine learning algorithm table</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec47" class="chapter-section">
                                                                    <a href="#ch09lvl1sec47">                    
                                                                        <div class="section-name">Spark MLlib examples</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec48" class="chapter-section">
                                                                    <a href="#ch09lvl1sec48">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="10">
                                                        <div class="section-name">10: Testing</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="10" class="chapter-section">
                                                                    <a href="#ch10">        
                                                                        <div class="section-name">Chapter 10: Testing</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec49" class="chapter-section">
                                                                    <a href="#ch10lvl1sec49">                    
                                                                        <div class="section-name">Testing in Java and Scala</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec50" class="chapter-section">
                                                                    <a href="#ch10lvl1sec50">                    
                                                                        <div class="section-name">Testing in Python</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec51" class="chapter-section">
                                                                    <a href="#ch10lvl1sec51">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="11">
                                                        <div class="section-name">11: Tips and Tricks</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="11" class="chapter-section">
                                                                    <a href="#ch11">        
                                                                        <div class="section-name">Chapter 11: Tips and Tricks</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="11" data-section-id="ch11lvl1sec52" class="chapter-section">
                                                                    <a href="#ch11lvl1sec52">                    
                                                                        <div class="section-name">Where to find logs</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="11" data-section-id="ch11lvl1sec53" class="chapter-section">
                                                                    <a href="#ch11lvl1sec53">                    
                                                                        <div class="section-name">Concurrency limitations</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="11" data-section-id="ch11lvl1sec54" class="chapter-section">
                                                                    <a href="#ch11lvl1sec54">                    
                                                                        <div class="section-name">Using Spark with other languages</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="11" data-section-id="ch11lvl1sec55" class="chapter-section">
                                                                    <a href="#ch11lvl1sec55">                    
                                                                        <div class="section-name">A quick note on security</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="11" data-section-id="ch11lvl1sec56" class="chapter-section">
                                                                    <a href="#ch11lvl1sec56">                    
                                                                        <div class="section-name">Community developed packages</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="11" data-section-id="ch11lvl1sec57" class="chapter-section">
                                                                    <a href="#ch11lvl1sec57">                    
                                                                        <div class="section-name">Mailing lists</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="11" data-section-id="ch11lvl1sec58" class="chapter-section">
                                                                    <a href="#ch11lvl1sec58">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="backindex">
                                                        <div class="section-name">Appendix A: Index</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="backindex" class="chapter-section">
                                                                    <a href="#backindex">
                                                                        <div class="section-name">Chapter Appendix A: Index</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                            </ul>
                                        </div>
                                        
                                        <div role="tabpanel" class="tab-pane fade" id="author">
                                            <div class="visible-xs">
                                                <h4 class="mobile-title">About the Author</h4>
                                                <hr>
                                            </div>
                                            <p><strong>Krishna Sankar</strong></p>
                                            <div>
                                                <p>Krishna Sankar is a Senior Specialist—AI Data Scientist with Volvo Cars focusing on Autonomous Vehicles. His earlier stints include Chief Data Scientist at <a href="http://cadenttech.tv/" target="_blank">http://cadenttech.tv/</a>, Principal Architect/Data Scientist at Tata America Intl. Corp., Director of Data Science at a bioinformatics startup, and as a Distinguished Engineer at Cisco. He has been speaking at various conferences including ML tutorials at Strata SJC and London 2016, Spark Summit [goo.gl/ab30lD], Strata-Spark Camp, OSCON, PyCon, and PyData, writes about Robots Rules of Order [goo.gl/5yyRv6], Big Data Analytics—Best of the Worst [goo.gl/ImWCaz], predicting NFL, Spark [<a href="http://goo.gl/E4kqMD" target="_blank">http://goo.gl/E4kqMD</a>], Data Science [<a href="http://goo.gl/9pyJMH" target="_blank">http://goo.gl/9pyJMH</a>], Machine Learning [<a href="http://goo.gl/SXF53n" target="_blank">http://goo.gl/SXF53n</a>], Social Media Analysis [<a href="http://goo.gl/D9YpVQ" target="_blank">http://goo.gl/D9YpVQ</a>] as well as has been a guest lecturer at the Naval Postgraduate School. His occasional blogs can be found at <a href="https://doubleclix.wordpress.com/" target="_blank">https://doubleclix.wordpress.com/</a>. His other passion is flying drones (working towards Drone Pilot License (FAA UAS Pilot) and Lego Robotics—you will find him at the St.Louis FLL World Competition as Robots Design Judge.</p>
                                            </div>
                                            <p><strong>Holden Karau</strong></p>
                                            <div>
                                                <p>Holden Karau is a software development engineer and is active in the open source. She has worked on a variety of search, classification, and distributed systems problems at IBM, Alpine, Databricks, Google, Foursquare, and Amazon. She graduated from the University of Waterloo with a bachelor's of mathematics degree in computer science. Other than software, she enjoys playing with fire and hula hoops, and welding.</p>
                                            </div>
                                        </div>
                                        
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    
                    <div class="next-wrapper">
                        <div class="row ns">
                            <hr />
                            <span class="hidden-xs">
                                <h4 class="pull-left">
                                    <strong>Up Next: </strong><span class="section-title"></span>
                                </h4>
                                <a href="#" class="btn btn-primary pull-right btn-lg">
                                    Next Section
                                </a>
                            </span>
                            <span class="visible-xs">
                                <a href="#" class="btn btn-primary btn-block btn-lg">
                                    Next Section
                                </a>
                            </span>
                        </div>
                        <div class="row ns">
                            <hr>
                        </div>
                    </div>
                </div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch01"></a>Chapter 1. Installing Spark and Setting up your Cluster</h2></div></div></div><p>This chapter will detail some common methods to set up Spark. Spark on a single machine is excellent for testing or exploring small datasets, but here you will also learn to use Spark's built-in deployment scripts with a dedicated cluster via SSH (Secure Shell). This chapter will explain the use of Mesos and Hadoop clusters with YARN or Chef to deploy Spark. For Cloud deployments of Spark, this chapter will look at EC2 (both traditional and EC2MR). Feel free to skip this chapter if you already have your local Spark instance installed and want to get straight to programming.</p><p>Regardless of how you are going to deploy <a id="id0" class="indexterm"></a>Spark, you will want to get the latest version <a id="id1" class="indexterm"></a>of Spark from <a class="ulink" href="https://spark.apache.org/downloads.html" target="_blank">https://spark.apache.org/downloads.html</a> (Version 1.2.0 as of this writing). Spark currently releases every 90 days. For coders who want to work with the latest builds, try cloning the code directly from the repository at <a class="ulink" href="https://github.com/apache/spark" target="_blank">https://github.com/apache/spark</a>. The building instructions<a id="id2" class="indexterm"></a> are available at <a class="ulink" href="https://spark.apache.org/docs/latest/building-spark.html" target="_blank">https://spark.apache.org/docs/latest/building-spark.html</a>. Both source code and prebuilt binaries are available at this link. To interact with <span class="strong"><strong>Hadoop Distributed File System</strong></span> (<span class="strong"><strong>HDFS</strong></span>), you need<a id="id3" class="indexterm"></a> to use Spark, which is built against the same version of Hadoop as your cluster. For Version 1.1.0 of Spark, the prebuilt package is built against the available Hadoop Versions 1.x, 2.3, and 2.4. If you are up for the challenge, it's recommended that you build against the source as it gives you the flexibility of choosing which HDFS Version you want to support as well as apply patches with. In this chapter, we will do both.</p><p>To compile the Spark source, you will need the appropriate version of Scala and the matching JDK. The Spark source tar includes the required Scala components. The following discussion is only for information—there is no need to install Scala.</p><p>The Spark<a id="id4" class="indexterm"></a> developers have done a good job of managing the dependencies. Refer to the <a class="ulink" href="https://spark.apache.org/docs/latest/building-spark.html" target="_blank">https://spark.apache.org/docs/latest/building-spark.html</a> web page for the latest information on this. According to the website, "Building Spark using Maven requires Maven 3.0.4 or newer and Java 6+." Scala gets pulled down as a dependency by Maven (currently Scala 2.10.4). Scala does not need to be installed separately, it is just a bundled dependency.</p><p>Just as a note, Spark 1.1.0 requires Scala 2.10.4 while the 1.2.0 version would run on 2.10 and Scala 2.11. I just saw e-mails in the Spark users' group on this.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip02"></a>Tip</h3><p>This brings up another interesting point about the Spark community. The two essential mailing lists are <code class="email">&lt;<a class="email" href="mailto:user@spark.apache.org">user@spark.apache.org</a>&gt;</code> and <code class="email">&lt;<a class="email" href="mailto:dev@spark.apache.org">dev@spark.apache.org</a>&gt;</code>. More details about the Spark community<a id="id5" class="indexterm"></a> are available at <a class="ulink" href="https://spark.apache.org/community.html" target="_blank">https://spark.apache.org/community.html</a>.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec08"></a>Directory organization and convention</h2></div></div><hr /></div><p>One convention that would be <a id="id6" class="indexterm"></a>handy is to download and install software in the <code class="literal">/opt</code> directory. Also <a id="id7" class="indexterm"></a>have a generic soft link to Spark that points to the current version. For example, <code class="literal">/opt/spark</code> points to <code class="literal">/opt/spark-1.1.0</code> with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo ln -f -s spark-1.1.0 spark</strong></span>
</pre></div><p>Later, if you upgrade, say to Spark 1.2, you can change the softlink.</p><p>But remember to copy any configuration changes and old logs when you change to a new distribution. A more flexible way is to change the configuration directory to <code class="literal">/etc/opt/spark</code> and the log files to <code class="literal">/var/log/spark/</code>. That way, these will stay independent of the distribution updates. More <a id="id8" class="indexterm"></a>details are available at <a class="ulink" href="https://spark.apache.org/docs/latest/configuration.html#overriding-configuration-directory" target="_blank">https://spark.apache.org/docs/latest/configuration.html#overriding-configuration-directory</a> and <a class="ulink" href="https://spark.apache.org/docs/latest/configuration.html#configuring-logging" target="_blank">https://spark.apache.org/docs/latest/configuration.html#configuring-logging</a>.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec09"></a>Installing prebuilt distribution</h2></div></div><hr /></div><p>Let's download prebuilt <a id="id9" class="indexterm"></a>Spark and install it. Later, we will also compile a Version and build from the source. The download is straightforward. The <a id="id10" class="indexterm"></a>page to go to for this is <a class="ulink" href="http://spark.apache.org/downloads.html" target="_blank">http://spark.apache.org/downloads.html</a>. Select the options as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/4005_01_01.jpg" /></div><p>We will do a wget from<a id="id11" class="indexterm"></a> the command line. You can do a direct download as well:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>cd /opt</strong></span>
<span class="strong"><strong>sudo wget http://apache.arvixe.com/spark/spark-1.1.1/spark-1.1.1-bin-hadoop2.4.tgz</strong></span>
</pre></div><p>We are downloading the prebuilt version for Apache Hadoop 2.4 from one of the possible mirrors. We could have easily downloaded other prebuilt versions as well, as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/4005_01_02.jpg" /></div><p>To uncompress it, execute the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>tar xvf spark-1.1.1-bin-hadoop2.4.tgz</strong></span>
</pre></div><p>To test the installation, run the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>/opt/spark-1.1.1-bin-hadoop2.4/bin/run-example SparkPi 10</strong></span>
</pre></div><p>It will fire up the Spark <a id="id12" class="indexterm"></a>stack and calculate the value of Pi. The result should be as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/4005_01_03.jpg" /></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec10"></a>Building Spark from source</h2></div></div><hr /></div><p>Let's compile Spark on a new <a id="id13" class="indexterm"></a>AWS instance. That way you can clearly understand<a id="id14" class="indexterm"></a> what all the requirements are to get a Spark stack <a id="id15" class="indexterm"></a>compiled and installed. I am using the Amazon Linux AMI, which has Java and other base stack installed by default. As this is a book on Spark, we can safely assume that you would have the base configurations covered. We will cover the incremental installs for the Spark stack here.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note02"></a>Note</h3><p>The latest instructions for <a id="id16" class="indexterm"></a>building from the source are available at <a class="ulink" href="https://spark.apache.org/docs/latest/building-with-maven.html" target="_blank">https://spark.apache.org/docs/latest/building-with-maven.html</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec07"></a>Downloading the source</h3></div></div></div><p>The first order of business is to<a id="id17" class="indexterm"></a> download the latest source<a id="id18" class="indexterm"></a> from <a class="ulink" href="https://spark.apache.org/downloads.html" target="_blank">https://spark.apache.org/downloads.html</a>. Select <span class="strong"><strong>Source Code</strong></span> from option <span class="strong"><strong>2. Chose a package type</strong></span> and either download directly or select a mirror. The download page is shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/4005_01_04.jpg" /></div><p>We can either download<a id="id19" class="indexterm"></a> from the web page or use wget. We will do the wget from one of the mirrors, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>cd /opt</strong></span>
<span class="strong"><strong>sudo wget http://apache.arvixe.com/spark/spark-1.1.1/spark-1.1.1.tgz</strong></span>
<span class="strong"><strong>sudo tar -xzf spark-1.1.1.tgz</strong></span>
</pre></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip03"></a>Tip</h3><p>The latest <a id="id20" class="indexterm"></a>development source is in GitHub, which is available at <a class="ulink" href="https://github.com/apache/spark" target="_blank">https://github.com/apache/spark</a>. The latest version can be checked out by the Git clone at <a class="ulink" href="https://github.com/apache/spark.git" target="_blank">https://github.com/apache/spark.git</a>. This should be done only when you want to see the developments for the next version or when you are contributing to the source.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec08"></a>Compiling the source with Maven</h3></div></div></div><p>Compilation by nature is <a id="id21" class="indexterm"></a>uneventful, but a lot of information gets displayed on the screen:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>cd /opt/spark-1.1.1</strong></span>
<span class="strong"><strong>export MAVEN_OPTS="-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m"</strong></span>
<span class="strong"><strong>mvn -Pyarn -Phadoop-2.4 -Dhadoop.version=2.4.0 -DskipTests clean package</strong></span>
</pre></div><p>In order for the preceding snippet to work, we will need Maven installed in our system. In case Maven is not installed in your system, the commands to install the latest version of Maven are given here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>wget http://download.nextag.com/apache/maven/maven-3/3.2.5/binaries/apache-maven-3.2.5-bin.tar.gz</strong></span>
<span class="strong"><strong>sudo tar -xzf apache-maven-3.2.5-bin.tar.gz</strong></span>
<span class="strong"><strong>sudo ln -f -s apache-maven-3.2.5 maven</strong></span>
<span class="strong"><strong>export M2_HOME=/opt/maven</strong></span>
<span class="strong"><strong>export PATH=${M2_HOME}/bin:${PATH}</strong></span>
</pre></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip04"></a>Tip</h3><p>Detailed Maven installation instructions<a id="id22" class="indexterm"></a> are available at <a class="ulink" href="http://maven.apache.org/download.cgi#Installation" target="_blank">http://maven.apache.org/download.cgi#Installation</a>.</p><p>Sometimes you will have to debug Maven using the –X switch. When I ran Maven, the Amazon Linux AMI didn't have the Java compiler! I had to install <code class="literal">javac</code> for Amazon Linux AMI using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo yum install java-1.7.0-openjdk-devel</strong></span>
</pre></div></div><p>The compilation<a id="id23" class="indexterm"></a> time varies. On my Mac it took approximately 11 minutes. The Amazon Linux on a t2-medium instance took 18 minutes. In the end, you should see a build success message like the one shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/4005_01_05.jpg" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec09"></a>Compilation switches</h3></div></div></div><p>As an example, the<a id="id24" class="indexterm"></a> switches for compilation of <code class="literal">-Pyarn -Phadoop-2.4 -Dhadoop.version=2.4.0</code> are explained in <a class="ulink" href="https://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version" target="_blank">https://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version</a>.  <code class="literal">–D</code> defines a system property and <code class="literal">–P</code> defines a profile.</p><p>A typical compile configuration that I use (for YARN, Hadoop Version 2.6 with Hive support) is given here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>mvn clean package -Pyarn -Dyarn.version=2.6.0 -Phadoop-2.4 -Dhadoop.version=2.6.0 -Phive -DskipTests</strong></span>
</pre></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip05"></a>Tip</h3><p>You can also compile the source code in IDEA and then upload the built Version to your cluster.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec10"></a>Testing the installation</h3></div></div></div><p>A quick way to test<a id="id25" class="indexterm"></a> the installation is by calculating Pi:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>/opt/spark/bin/run-example SparkPi 10</strong></span>
</pre></div><p>The result should be a few debug messages and then the value of <span class="strong"><strong>Pi</strong></span> as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/4005_01_06.jpg" /></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec11"></a>Spark topology</h2></div></div><hr /></div><p>This is a good time to talk about the<a id="id26" class="indexterm"></a> basic mechanics and mechanisms of Spark. We will progressively dig deeper, but for now let's take a quick look at the top level.</p><p>Essentially, Spark provides a framework to process vast amounts of data, be it in  gigabytes and terabytes and occasionally petabytes. The two main ingredients are computation and scale. The size and effectiveness of the problems we can solve depends on these two factors, that is, the ability to apply complex computations over large amounts of data in a timely fashion. If our monthly runs take 40 days, we have a problem. The key, of course, is parallelism, massive parallelism to be exact. We can make our computational algorithm tasks go parallel, that is instead of doing the steps one after another, we can perform many steps in parallel or carry out data parallelism, that is, we run the same algorithms over a partitioned dataset in parallel. In my humble opinion, Spark is extremely effective in data parallelism in an elegant framework. As you will see in the rest of this book, the two components are <span class="strong"><strong>Resilient Distributed Dataset</strong></span> (<span class="strong"><strong>RDD</strong></span>)<a id="id27" class="indexterm"></a> and cluster manager. The cluster manager distributes the code and manages the data that is represented in RDDs. RDDs with transformations and actions are the main programming abstractions and present parallelized collections. Behind the scenes, a cluster manager controls the distribution and interaction with RDDs, distributes code, and manages fault-tolerant execution. Spark works with three types of cluster managers – standalone, Apache Mesos, and Hadoop YARN. The Spark page at <a class="ulink" href="http://spark.apache.org/docs/latest/cluster-overview.html" target="_blank">http://spark.apache.org/docs/latest/cluster-overview.html</a> has a lot more details on this. I just gave you a quick introduction here.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip06"></a>Tip</h3><p>If you have installed Hadoop 2.0, you are recommended to install Spark on YARN. If you have installed Hadoop 1.0, the standalone version is recommended. If you want to try Mesos, you can choose to install Spark on Mesos. Users are not recommended to install both YARN and Mesos.</p></div><div class="mediaobject"><img src="graphics/4005_01_07.jpg" /></div><p>The Spark driver program <a id="id28" class="indexterm"></a>takes the program classes and hands them over to a cluster manager. The cluster manager, in turn, starts executors in multiple worker nodes, each having a set of tasks. When we ran the example program earlier, all these actions happened transparently in your machine! Later when we install in a cluster, the examples would run, again transparently, but across multiple machines in the cluster. That is the magic of Spark and distributed computing!</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec12"></a>A single machine</h2></div></div><hr /></div><p>A single machine<a id="id29" class="indexterm"></a> is the simplest use case for Spark. It is also a great way to sanity check your build. In the <code class="literal">spark/bin</code> directory, there is a shell script called <code class="literal">run-example</code>, which can be used to launch a Spark job. The <code class="literal">run-example</code> script takes the name of a Spark class and some arguments. Earlier, we used the <code class="literal">run-example</code> script from the <code class="literal">/bin</code> directory to calculate the value of Pi. There is a collection of sample Spark jobs in <code class="literal">examples/src/main/scala/org/apache/spark/examples/</code>.</p><p>All of the sample programs take the parameter <code class="literal">master</code> (the cluster manager), which can be the URL of a distributed cluster or local[N], where N is the number of threads.</p><p>Going back to our <code class="literal">run-example</code> script, it invokes the more general <code class="literal">bin/spark-submit</code> script. For now, let's stick with the <code class="literal">run-example</code> script.</p><p>To run <code class="literal">GroupByTest</code> locally, try running the following code:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>bin/run-example GroupByTest</strong></span>
</pre></div><p>It should produce an output like this given here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>14/11/15 06:28:40 INFO SparkContext: Job finished: count at GroupByTest.scala:51, took 0.494519333 s</strong></span>
<span class="strong"><strong>2000</strong></span>
</pre></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec13"></a>Running Spark on EC2</h2></div></div><hr /></div><p>The <code class="literal">ec2</code> directory <a id="id30" class="indexterm"></a>contains the script to run a Spark cluster in EC2. These <a id="id31" class="indexterm"></a>scripts can be used to run multiple Spark clusters and even run on spot instances. Spark can also be run on Elastic MapReduce, which is Amazon's solution for Map Reduce cluster management, and it gives you more flexibility around scaling instances. The Spark page at <a class="ulink" href="http://spark.apache.org/docs/latest/ec2-scripts.html" target="_blank">http://spark.apache.org/docs/latest/ec2-scripts.html</a> has the <a id="id32" class="indexterm"></a>latest on-running spark on EC2.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec11"></a>Running Spark on EC2 with the scripts</h3></div></div></div><p>To get started, you should<a id="id33" class="indexterm"></a> make sure you have EC2 enabled on your<a id="id34" class="indexterm"></a> account by signing up at <a class="ulink" href="https://portal.aws.amazon.com/gp/aws/manageYourAccount" target="_blank">https://portal.aws.amazon.com/gp/aws/manageYourAccount</a>. Then it is a good idea to generate a separate access key pair for your Spark cluster, which you can do at <a class="ulink" href="https://portal.aws.amazon.com/gp/aws/securityCredentials" target="_blank">https://portal.aws.amazon.com/gp/aws/securityCredentials</a>. You will also need to create an EC2 key pair so that the Spark script can SSH to the launched machines, which can be done at <a class="ulink" href="https://console.aws.amazon.com/ec2/home" target="_blank">https://console.aws.amazon.com/ec2/home</a> by selecting <span class="strong"><strong>Key Pairs</strong></span> under <span class="strong"><strong>Network &amp; Security</strong></span>. Remember that key pairs are created per region, and so you need to make sure you create your key pair in the same region as you intend to run your Spark instances. Make sure to give it a name that you can remember as you will need it for the scripts (this chapter will use <code class="literal">spark-keypair</code> as its example key pair name.). You can also choose to upload your public SSH key instead of generating a new key. These are sensitive; so make sure that you keep them private. You also need to set <code class="literal">AWS_ACCESS_KEY</code> and <code class="literal">AWS_SECRET_KEY</code> as environment variables for the Amazon EC2 scripts:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>chmod 400 spark-keypair.pem</strong></span>
<span class="strong"><strong>export AWS_ACCESS_KEY_ID= AWSACcessKeyId</strong></span>
<span class="strong"><strong>export AWS_SECRET_ACCESS_KEY=AWSSecretKey</strong></span>
</pre></div><p>You will find it useful to download the <a id="id35" class="indexterm"></a>EC2 scripts provided by Amazon from <a class="ulink" href="http://aws.amazon.com/developertools/Amazon-EC2/351" target="_blank">http://aws.amazon.com/developertools/Amazon-EC2/351</a>. Once you unzip the resulting zip file, you can add the bin to your <code class="literal">PATH</code> in a manner similar to what you did with the Spark bin:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>wget http://s3.amazonaws.com/ec2-downloads/ec2-api-tools.zip</strong></span>
<span class="strong"><strong>unzip ec2-api-tools.zip</strong></span>
<span class="strong"><strong>cd ec2-api-tools-*</strong></span>
<span class="strong"><strong>export EC2_HOME=`pwd`</strong></span>
<span class="strong"><strong>export PATH=$PATH:`pwd`/bin</strong></span>
</pre></div><p>In order to test whether this works, try the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ec2-describe-regions</strong></span>
</pre></div><p>This should display the <a id="id36" class="indexterm"></a>following output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>REGION	eu-central-1    ec2.eu-central-1.amazonaws.com</strong></span>
<span class="strong"><strong>REGION	sa-east-1       ec2.sa-east-1.amazonaws.com</strong></span>
<span class="strong"><strong>REGION	ap-northeast-1  ec2.ap-northeast-1.amazonaws.com</strong></span>
<span class="strong"><strong>REGION	eu-west-1       ec2.eu-west-1.amazonaws.com</strong></span>
<span class="strong"><strong>REGION	us-east-1       ec2.us-east-1.amazonaws.com</strong></span>
<span class="strong"><strong>REGION	us-west-1       ec2.us-west-1.amazonaws.com</strong></span>
<span class="strong"><strong>REGION	us-west-2       ec2.us-west-2.amazonaws.com</strong></span>
<span class="strong"><strong>REGION	ap-southeast-2  ec2.ap-southeast-2.amazonaws.com</strong></span>
<span class="strong"><strong>REGION	ap-southeast-1  ec2.ap-southeast-1.amazonaws.com</strong></span>
</pre></div><p>Finally, you can refer to the EC2 command line tools<a id="id37" class="indexterm"></a> reference page <a class="ulink" href="http://docs.aws.amazon.com/AWSEC2/latest/CommandLineReference/set-up- ec2-cli-linux.html" target="_blank">http://docs.aws.amazon.com/AWSEC2/latest/CommandLineReference/set-up-ec2-cli-linux.html</a> as it has all the gory details.</p><p>The Spark EC2 script automatically creates a separate security group and firewall rules for running the Spark cluster. By default, your Spark cluster will be universally accessible on port 8080, which is a somewhat poor form. Sadly, the <code class="literal">spark_ec2.py</code> script does not currently provide an easy way to restrict access to just your host. If you have a static IP address, I strongly recommend limiting access in <code class="literal">spark_ec2.py</code>; simply replace all instances of <code class="literal">0.0.0.0/0</code> with <code class="literal">[yourip]/32</code>. This will not affect intra-cluster communication as all machines within a security group can talk to each other by default.</p><p>Next, try to launch a cluster on EC2:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>./ec2/spark-ec2 -k spark-keypair -i pk-[....].pem -s 1 launch myfirstcluster</strong></span>
</pre></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip07"></a>Tip</h3><p>If you get an error message like <code class="literal">The requested Availability Zone is currently constrained and....</code>, you can specify a different zone by passing in the <code class="literal">--zone</code> flag.</p></div><p>The <code class="literal">-i</code> parameter (in the preceding command line) is provided for specifying the private key to log into the instance; <code class="literal">-i pk-[....].pem</code> represents the path to the private key.</p><p>If you get an error about not being able to SSH to the master, make sure that only you have the permission to read the private key otherwise SSH will refuse to use it.</p><p>You may also encounter this error due to a race condition, when the hosts report themselves as alive but the Spark<code class="literal">-ec2</code> script cannot yet SSH to them. A fix for this issue is pending in <a class="ulink" href="https://github.com/mesos/spark/pull/555" target="_blank">https://github.com/mesos/spark/pull/555</a>. For now, a temporary workaround until the fix is available in the version of Spark you are using is to simply sleep an extra 100 seconds at the start of <code class="literal">setup_cluster</code> using the <code class="literal">–w</code> parameter. The current script has 120 seconds of delay built in.</p><p>If you do get a transient error while launching a cluster, you can finish the launch process using the resume feature by running:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>./ec2/spark-ec2 -i ~/spark-keypair.pem launch myfirstsparkcluster --resume</strong></span>
</pre></div><p>It will go through a bunch<a id="id38" class="indexterm"></a> of scripts, thus setting up Spark, Hadoop and so forth. If everything goes well, you should see something like the following screenshot:</p><div class="mediaobject"><img src="graphics/4005_01_08.jpg" /></div><p>This will give you a bare bones cluster with one master and one worker with all of the defaults on the default machine instance size. Next, verify that it started up and your firewall rules were applied by going to the master on port 8080. You can see in the preceding screenshot that the UI for the master is the output at the end of the script with port at 8080 and ganglia at 5080.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip08"></a>Tip</h3><p><span class="strong"><strong>Downloading the example code</strong></span></p><p>You can download the example code files for all Packt books you have purchased from your account at <a class="ulink" href="http://www.packtpub.com" target="_blank">http://www.packtpub.com</a>. If you purchased this book elsewhere, you can visit <a class="ulink" href="http://www.packtpub.com/support" target="_blank">http://www.packtpub.com/support</a> and register to have the files e-mailed directly to you.</p></div><p>Your AWS EC2 dashboard will show the instances as follows:</p><div class="mediaobject"><img src="graphics/4005_01_09.jpg" /></div><p>The ganglia dashboard <a id="id39" class="indexterm"></a>shown in the following screenshot is a good place to monitor the instances:</p><div class="mediaobject"><img src="graphics/4005_01_10.jpg" /></div><p>Try running one of the <a id="id40" class="indexterm"></a>example jobs on your new cluster to make sure everything is okay, as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/4005_01_11.jpg" /></div><p>The JPS should show this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>root@ip-172-31-45-56 ~]$ jps</strong></span>
<span class="strong"><strong>1904 NameNode</strong></span>
<span class="strong"><strong>2856 Jps</strong></span>
<span class="strong"><strong>2426 Master</strong></span>
<span class="strong"><strong>2078 SecondaryNameNode</strong></span>
</pre></div><p>The script has started Spark master, the Hadoop name node, and data nodes (in slaves).</p><p>Let's run the two<a id="id41" class="indexterm"></a> programs that we ran earlier on our local machine:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>cd spark</strong></span>
<span class="strong"><strong>bin/run-example GroupByTest</strong></span>
<span class="strong"><strong>bin/run-example SparkPi 10</strong></span>
</pre></div><p>The ease with which one can spin up a few nodes in the Cloud, install the Spark stack, and run the program in a distributed manner is interesting.</p><p>The <code class="literal">ec2/spark-ec2 destroy &lt;cluster name&gt;</code> command will terminate the instances.</p><p>Now that you've run a simple job on our EC2 cluster, it's time to configure your EC2 cluster for our Spark jobs. There are a number of options you can use to configure with the <code class="literal">spark-ec2</code> script.</p><p>The <code class="literal">ec2/ spark-ec2 –help</code> command will display all the options available.</p><p>First, consider what instance types you may need. EC2 offers an ever-growing collection of instance types and you can choose a different instance type for the master and the workers. The instance type has the most obvious impact on the performance of your Spark cluster. If your work needs a lot of RAM, you should choose an instance with more RAM. You can specify the instance type with <code class="literal">--instance-type=</code> (name of instance type). By default, the same instance type will be used for both the master and the workers; this can be wasteful if your computations are particularly intensive and the master isn't being heavily utilized. You can specify a different master instance type with <code class="literal">--master-instance-type=</code> (name of instance).</p><p>EC2 also has GPU instance types, which can be useful for workers but would be completely wasted on the master. This text will cover working with Spark and GPUs later on; however, it is important to note that EC2 GPU performance may be lower than what you get while testing locally due to the higher I/O overhead imposed by the hypervisor.</p><p>Spark's EC2 scripts use <span class="strong"><strong>Amazon Machine Images</strong></span> (<span class="strong"><strong>AMI</strong></span>)<a id="id42" class="indexterm"></a> provided by the Spark team. Usually, they are current and sufficient for most of the applications. You might need your own AMI in case of circumstances like custom patches (for example, using a different version of HDFS) for Spark, as they will not be included in the machine image.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec12"></a>Deploying Spark on Elastic MapReduce</h3></div></div></div><p>In addition to the Amazon basic EC2 machine offering, Amazon offers a hosted Map Reduce solution called <a id="id43" class="indexterm"></a>
<span class="strong"><strong>Elastic MapReduce</strong></span> (<span class="strong"><strong>EMR</strong></span>). Amazon <a id="id44" class="indexterm"></a>provides a bootstrap script that simplifies the process of getting started using Spark on EMR. You will need to install the EMR tools from Amazon:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>mkdir emr</strong></span>
<span class="strong"><strong>cd emr</strong></span>
<span class="strong"><strong>wget http://elasticmapreduce.s3.amazonaws.com/elastic-mapreduce-ruby.zip</strong></span>
<span class="strong"><strong>unzip *.zip</strong></span>
</pre></div><p>This way the EMR scripts can access your AWS account you will want, to create a <code class="literal">credentials.json</code> file:</p><div class="informalexample"><pre class="programlisting"> {
    "access-id": "&lt;Your AWS access id here&gt;",
    "private-key": "&lt;Your AWS secret access key here&gt;",
    "key-pair": "&lt;The name of your ec2 key-pair here&gt;",
    "key-pair-file": "&lt;path to the .pem file for your ec2 key pair here&gt;",
    "region": "&lt;The region where you wish to launch your job flows (e.g us-east-1)&gt;"
  }</pre></div><p>Once you have the <a id="id45" class="indexterm"></a>EMR tools installed, you can launch<a id="id46" class="indexterm"></a> a Spark cluster by running:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>elastic-mapreduce --create --alive --name "Spark/Shark Cluster" \</strong></span>
<span class="strong"><strong>--bootstrap-action s3://elasticmapreduce/samples/spark/install-spark-shark.sh \</strong></span>
<span class="strong"><strong>--bootstrap-name "install Mesos/Spark/Shark" \</strong></span>
<span class="strong"><strong>--ami-version 2.0  \</strong></span>
<span class="strong"><strong>--instance-type m1.large --instance-count 2</strong></span>
</pre></div><p>This will give you a running EC2MR instance after about 5 to 10 minutes. You can list the status of the cluster by running <code class="literal">elastic-mapreduce -listode</code>. Once it outputs <code class="literal">j-[jobid]</code>, it is ready.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec14"></a>Deploying Spark with Chef (Opscode)</h2></div></div><hr /></div><p>Chef<a id="id47" class="indexterm"></a> is an open source automation platform that has become increasingly popular for deploying and managing both small and large clusters of machines. Chef can be used to control a traditional static fleet of machines and can also be used with EC2 and other cloud providers. Chef uses cookbooks as the basic building blocks of configuration and can either be generic or site-specific. If you have not used Chef before, a good tutorial for getting started with Chef<a id="id48" class="indexterm"></a> can be found at <a class="ulink" href="https://learnchef.opscode.com/" target="_blank">https://learnchef.opscode.com/</a>. You can use a generic Spark cookbook as the basis for setting up your cluster.</p><p>To get Spark working, you need <a id="id49" class="indexterm"></a>to create a role for both the master and the workers <a id="id50" class="indexterm"></a>as well as configure the workers to connect to the master. Start by getting the cookbook<a id="id51" class="indexterm"></a> from <a class="ulink" href="https://github.com/holdenk/chef-cookbook-spark" target="_blank">https://github.com/holdenk/chef-cookbook-spark</a>. The bare minimum need is setting the master hostname (as master) to enable worker nodes to connect and the username, so that Chef can be installed in the correct place. You will also need to either accept Sun's Java license or switch to an alternative JDK. Most of the settings that are available in <code class="literal">spark-env.sh</code> are also exposed through the cookbook settings. You can see an explanation of the settings in your section on "configuring multiple hosts over SSH". The settings can be set as per-role or you can modify the global defaults.</p><p>Create a role for the master<a id="id52" class="indexterm"></a> with a knife role; create <code class="literal">spark_master_role -e [editor]</code>. This <a id="id53" class="indexterm"></a>will bring up a template role file that you can edit. For a simple master, set it to this:</p><div class="informalexample"><pre class="programlisting">{
  "name": "spark_master_role",
  "description": "",
  "json_class": "Chef::Role",
  "default_attributes": {
  },
  "override_attributes": {
   "username":"spark",
   "group":"spark",
   "home":"/home/spark/sparkhome",
   "master_ip":"10.0.2.15", },
   "chef_type": "role",
   "run_list": [
    "recipe[spark::server]",
    "recipe[chef-client]",
   ],
   "env_run_lists": {
  }
}</pre></div><p>Then create a role for the client in the same manner except that instead of <code class="literal">spark::server</code>, you need to use the <code class="literal">spark::client</code> recipe. Deploy the roles to different hosts:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>knife node run_list add master role[spark_master_role]</strong></span>
<span class="strong"><strong>knife node run_list add worker role[spark_worker_role]</strong></span>
</pre></div><p>Then run <code class="literal">chef-client</code> on your nodes to update. Congrats, you now have a Spark cluster running!</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec15"></a>Deploying Spark on Mesos</h2></div></div><hr /></div><p>Mesos<a id="id54" class="indexterm"></a> is a cluster management platform for running multiple distributed applications or frameworks on a cluster. Mesos can intelligently schedule and run Spark, Hadoop, and other frameworks concurrently on the same cluster. Spark can be run on Mesos either by scheduling individual jobs as separate Mesos tasks or running all of Spark as a<a id="id55" class="indexterm"></a> single Mesos task. Mesos can quickly scale up to handle <a id="id56" class="indexterm"></a>large clusters beyond the size of which you would want to manage with plain old SSH scripts. Mesos, written in C++, was originally created at UC Berkley as a research project; it is currently undergoing Apache incubation and is actively used by Twitter.</p><p>The Spark web page has detailed instructions on installing and running Spark on Mesos.</p><p>To get started with <a id="id57" class="indexterm"></a>Mesos, you can download the latest version from <a class="ulink" href="http://mesos.apache.org/downloads/" target="_blank">http://mesos.apache.org/downloads/</a> and unpack it. Mesos has a number of different configuration scripts you can use; for an Ubuntu installation use <code class="literal">configure.ubuntu-lucid-64</code> and for other cases, the Mesos <code class="literal">README</code> file will point you at the configuration file you need to use. In addition to the requirements of Spark, you will need to ensure that you have the Python C header files installed (<code class="literal">python-dev</code> on Debian systems) or pass <code class="literal">--disable-python</code> to the configure script. Since Mesos needs to be installed on all the machines, you may find it easier to configure Mesos to install somewhere other than on the root, most easily alongside your Spark installation:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>./configure --prefix=/home/sparkuser/mesos &amp;&amp; make &amp;&amp; make check &amp;&amp; make install</strong></span>
</pre></div><p>Much like the configuration of Spark in standalone mode, with Mesos you need to make sure the different Mesos nodes can find each other. Start by having <code class="literal">mesossprefix/var/mesos/deploy/masters</code> to the hostname of the master and adding each worker hostname to <code class="literal">mesossprefix/var/mesos/deploy/slaves</code>. Then you will want to point the workers at the master (and possibly set some other values) in <code class="literal">mesossprefix/var/mesos/conf/mesos.conf</code>.</p><p>Once you have Mesos built, it's time to configure Spark to work with Mesos. This is as simple as copying the <code class="literal">conf/spark-env.sh.template</code> to <code class="literal">conf/spark-env.sh</code> and updating <code class="literal">MESOS_NATIVE_LIBRARY</code> to point to the path where Mesos is installed. You can find more information about the different settings in <code class="literal">spark-env.sh</code> in first table of the next section.</p><p>You will need to install both Mesos and Spark on all of the machines in your cluster. Once both Mesos and Spark are configured, you can copy the build to all of the machines using <code class="literal">pscp</code>, as shown in the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>pscp -v -r -h  -l sparkuser ./mesos /home/sparkuser/mesos</strong></span>
</pre></div><p>You can then start your Mesos clusters using <code class="literal">mesosprefix/sbin/mesos-start-cluster.sh</code> and schedule your Spark on Mesos by using <code class="literal">mesos://[host]:5050</code> as the master.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec16"></a>Spark on YARN</h2></div></div><hr /></div><p>YARN<a id="id58" class="indexterm"></a> is Apache <a id="id59" class="indexterm"></a>Hadoop's NextGen MapReduce. The Spark project provides an easy way to schedule jobs on YARN once you have a Spark assembly built. The Spark<a id="id60" class="indexterm"></a> web page <a class="ulink" href="http://spark.apache.org/docs/latest/running-on-yarn.html" target="_blank">http://spark.apache.org/docs/latest/running-on-yarn.html</a> has the configuration details for YARN, which we had built earlier for when compiling with the –Pyarn switch. It is important that the Spark job you create uses a standalone master URL. The example Spark applications all read the master URL from the command line arguments; so specify <code class="literal">--args standalone</code>.</p><p>To run the same example as given in the SSH section, write the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sbt/sbt assembly #Build the assembly</strong></span>
<span class="strong"><strong>SPARK_JAR=./core/target/spark-core-assembly-1.1.0.jar ./run spark.deploy.yarn.Client --jar examples/target/scala-2.9.2/spark-examples_2.9.2-0.7.0.jar --class spark.examples.GroupByTest --args standalone --num-workers 2 --worker-memory 1g --worker-cores 1</strong></span>
</pre></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec17"></a>Spark Standalone mode</h2></div></div><hr /></div><p>If you have a set of machines without any existing cluster management software, you can deploy Spark over SSH with some handy scripts. This method is known as <span class="strong"><strong>"standalone mode</strong></span>" in the Spark <a id="id61" class="indexterm"></a>documentation at <a class="ulink" href="http://spark.apache.org/docs/latest/spark-standalone.html" target="_blank">http://spark.apache.org/docs/latest/spark-standalone.html</a>. An individual master and worker can be<a id="id62" class="indexterm"></a> started by <code class="literal">sbin/start-master.sh</code>  and <code class="literal">sbin/start-slaves.sh</code> respectively. The default port for the master is 8080. As you likely don't want to go to each of your machines and run these commands by hand, there are a number of helper scripts in <code class="literal">bin/</code> to help you run your servers.</p><p>A prerequisite for using any of the scripts is having password-less SSH access set up from the master to all of the worker machines. You probably want to create a new user for running Spark on the machines and lock it down. This book uses the username "sparkuser". On your master, you can run <code class="literal">ssh-keygen</code> to generate the SSH keys and make sure that you do not set a password. Once you have generated the key, add the public one (if you generated an RSA key, it would be stored in <code class="literal">~/.ssh/id_rsa.pub</code> by default) to <code class="literal">~/.ssh/authorized_keys2</code> on each of the hosts.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip09"></a>Tip</h3><p>The Spark administration scripts require that your usernames match. If this isn't the case, you can configure an alternative username in your <code class="literal">~/.ssh/config</code>.</p></div><p>Now that you have the SSH access to the machines set up, it is time to configure Spark. There is a simple template in <code class="literal">[filepath]conf/spark-env.sh.template[/filepath]</code>, which you should copy to <code class="literal">[filepath]conf/spark-env.sh[/filepath]</code>. You will need to set <code class="literal">SCALA_HOME</code> to the path where you extracted Scala to. You may also find it useful to<a id="id63" class="indexterm"></a> set some (or all) of the following environment variables:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Name</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Purpose</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Default</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p><code class="literal">MESOS_NATIVE_LIBRARY</code></p>
</td><td style="" align="left" valign="top">
<p>Point to math where Mesos lives</p>
</td><td style="" align="left" valign="top">
<p>None</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">SCALA_HOME</code></p>
</td><td style="" align="left" valign="top">
<p>Point to where you extracted Scala</p>
</td><td style="" align="left" valign="top">
<p>None, must be set</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">SPARK_MASTER_IP</code></p>
</td><td style="" align="left" valign="top">
<p>The IP address for the master to listen on and the IP address for the workers to connect to.</p>
</td><td style="" align="left" valign="top">
<p>The result of running hostname</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">SPARK_MASTER_PORT</code></p>
</td><td style="" align="left" valign="top">
<p>The port # for the Spark master to listen on</p>
</td><td style="" align="left" valign="top">
<p>7077</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">SPARK_MASTER_WEBUI_PORT</code></p>
</td><td style="" align="left" valign="top">
<p>The port # of the WEB UI on the master</p>
</td><td style="" align="left" valign="top">
<p>8080</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">SPARK_WORKER_CORES</code></p>
</td><td style="" align="left" valign="top">
<p>Number of cores to use</p>
</td><td style="" align="left" valign="top">
<p>All of them</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">SPARK_WORKER_MEMORY</code></p>
</td><td style="" align="left" valign="top">
<p>How much memory to use</p>
</td><td style="" align="left" valign="top">
<p>Max of (system memory - 1GB, 512MB)</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">SPARK_WORKER_PORT</code></p>
</td><td style="" align="left" valign="top">
<p>What port # the worker runs on</p>
</td><td style="" align="left" valign="top">
<p>Rand</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">SPARK_WEBUI_PORT</code></p>
</td><td style="" align="left" valign="top">
<p>What port # the worker WEB UI runs on</p>
</td><td style="" align="left" valign="top">
<p>8081</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">SPARK_WORKER_DIR</code></p>
</td><td style="" align="left" valign="top">
<p>Where to store files from the worker</p>
</td><td style="" align="left" valign="top">
<p><code class="literal">SPARK_HOME/work_dir</code></p>
</td></tr></tbody></table></div><p>Once you have your configuration done, it's time to get your cluster up and running. You will want to copy the version of Spark and the configuration you have built to all of your machines. You may find it useful to install <code class="literal">pssh</code>, a set of parallel SSH tools including <code class="literal">pscp</code>. The <code class="literal">pscp</code> makes it easy to scp to a number of target hosts, although it will take a while, as shown here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>pscp -v -r -h conf/slaves -l sparkuser ../opt/spark ~/</strong></span>
</pre></div><p>If you end up changing the configuration, you need to distribute the configuration to all of the workers, as shown here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>pscp -v -r -h conf/slaves -l sparkuser conf/spark-env.sh /opt/spark/conf/spark-env.sh</strong></span>
</pre></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip10"></a>Tip</h3><p>If you use a shared NFS on your cluster, while by default Spark names log files and similar with shared names, you should configure a separate worker directory,  otherwise they will be configured to write to the same place. If you want to have your worker directories on the shared NFS, consider adding <code class="literal">`hostname`</code> for example <code class="literal">SPARK_WORKER_DIR=~/work-`hostname`</code>.</p><p>You should also consider having your log files go to a scratch directory for performance.</p></div><p>Then you are ready to <a id="id64" class="indexterm"></a>start the cluster and you can use the <code class="literal">sbin/start-all.sh</code>, <code class="literal">sbin/start-master.sh</code> and <code class="literal">sbin/start-slaves.sh</code> scripts. It is important to note that <code class="literal">start-all.sh</code> and <code class="literal">start-master.sh</code> both assume that they are being run on the node, which is the master for the cluster. The start scripts all daemonize, and so you don't have to worry about running them in a screen:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>ssh master bin/start-all.sh</strong></span>
</pre></div><p>If you get a class not found error stating "<code class="literal">java.lang.NoClassDefFoundError: scala/ScalaObject</code>", check to make sure that you have Scala installed on that worker host and that the <code class="literal">SCALA_HOME</code> is set correctly.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip11"></a>Tip</h3><p>The Spark scripts assume that your master has Spark installed in the same directory as your workers. If this is not the case, you should edit <code class="literal">bin/spark-config.sh</code> and set it to the appropriate directories.</p></div><p>The commands provided by Spark to help you administer your cluster are given in the following table. More details are available in the Spark website at <a class="ulink" href="http://spark.apache.org/docs/latest/spark-standalone.html#cluster-launch-scripts" target="_blank">http://spark.apache.org/docs/latest/spark-standalone.html#cluster-launch-scripts</a>.</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Command</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Use</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>bin/slaves.sh &lt;command&gt;</strong></span>
</pre></div>
</td><td style="" align="left" valign="top">
<p>Runs the provided command on all of the worker hosts. For example, <code class="literal">bin/slave.sh</code> uptime will show how long each of the worker hosts have been up.</p>
</td></tr><tr><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>bin/start-all.sh</strong></span>
</pre></div>
</td><td style="" align="left" valign="top">
<p>Starts the master and all of the worker hosts. Must be run on the master.</p>
</td></tr><tr><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>bin/start-master.sh</strong></span>
</pre></div>
</td><td style="" align="left" valign="top">
<p>Starts the master host. Must be run on the master.</p>
</td></tr><tr><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>bin/start-slaves.sh</strong></span>
</pre></div>
</td><td style="" align="left" valign="top">
<p>Starts the worker hosts.</p>
</td></tr><tr><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>bin/start-slave.sh</strong></span>
</pre></div>
</td><td style="" align="left" valign="top">
<p>Start a specific worker.</p>
</td></tr><tr><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>bin/stop-all.sh</strong></span>
</pre></div>
</td><td style="" align="left" valign="top">
<p>Stops master and workers.</p>
</td></tr><tr><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>bin/stop-master.sh</strong></span>
</pre></div>
</td><td style="" align="left" valign="top">
<p>Stops the master.</p>
</td></tr><tr><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>bin/stop-slaves.sh</strong></span>
</pre></div>
</td><td style="" align="left" valign="top">
<p>Stops all the workers.</p>
</td></tr></tbody></table></div><p>You now have a running <a id="id65" class="indexterm"></a>Spark cluster, as shown in the following screenshot! There is a handy Web UI on the master running on port 8080 you should go and visit, and on all of the workers on port 8081. The Web UI contains such helpful information as the current workers, and current and past jobs.</p><div class="mediaobject"><img src="graphics/4005_01_12.jpg" /></div><p>Now that you have a cluster up and running, let's actually do something with it. As with the single host example, you can use the provided run script to run Spark commands. All of the examples listed in <code class="literal">examples/src/main/scala/spark/org/apache/spark/examples/</code> take a parameter, master, which points them to the master. Assuming that you are on the master host, you could run them like this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>./run-example GroupByTest spark://`hostname`:7077</strong></span>
</pre></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip12"></a>Tip</h3><p>If you run into an issue with <code class="literal">java.lang.UnsupportedClassVersionError</code>, you may need to update your JDK or recompile Spark if you grabbed the binary version. Version 1.1.0 was compiled with JDK 1.7 as the target. You can check the version of the JRE targeted by Spark with the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -verbose -classpath ./core/target/scala-2.9.2/classes/</strong></span>
<span class="strong"><strong>spark.SparkFiles |head -n 20</strong></span>
<span class="strong"><strong>Version 49 is JDK1.5, Version 50 is JDK1.6 and Version 60 is JDK1.7</strong></span>
</pre></div></div><p>If you can't connect to <a id="id66" class="indexterm"></a>localhost, make sure that you've configured your master (<code class="literal">spark.driver.port</code>) to listen to all of the IP addresses (or if you don't want to replace localhost with the IP address configured to listen to). More port configurations are listed at <a class="ulink" href="http://spark.apache.org/docs/latest/configuration.html#networking" target="_blank">http://spark.apache.org/docs/latest/configuration.html#networking</a>.</p><p>If everything has worked correctly, you will see the following log messages output to <code class="literal">stdout</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>13/03/28 06:35:31 INFO spark.SparkContext: Job finished: count at GroupByTest.scala:35, took 2.482816756 s</strong></span>
<span class="strong"><strong>2000</strong></span>
</pre></div><p>References:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><a class="ulink" href="http://archive09.linux.com/feature/151340" target="_blank">http://archive09.linux.com/feature/151340</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://spark-project.org/docs/latest/spark-standalone.html" target="_blank">http://spark-project.org/docs/latest/spark-standalone.html</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://bickson.blogspot.com/2012/10/deploying-graphlabsparkmesos-cluster-on.html" target="_blank">http://bickson.blogspot.com/2012/10/deploying-graphlabsparkmesos-cluster-on.html</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://www.ibm.com/developerworks/library/os-spark/" target="_blank">http://www.ibm.com/developerworks/library/os-spark/</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://mesos.apache.org/" target="_blank">http://mesos.apache.org/</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://aws.amazon.com/articles/Elastic-MapReduce/4926593393724923" target="_blank">http://aws.amazon.com/articles/Elastic-MapReduce/4926593393724923</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://spark-project.org/docs/latest/ec2-scripts.html" target="_blank">http://spark-project.org/docs/latest/ec2-scripts.html</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://spark.apache.org/docs/latest/cluster-overview.html" target="_blank">http://spark.apache.org/docs/latest/cluster-overview.html</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf" target="_blank">https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://research.google.com/pubs/pub41378.html" target="_blank">http://research.google.com/pubs/pub41378.html</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://aws.amazon.com/articles/4926593393724923" target="_blank">http://aws.amazon.com/articles/4926593393724923</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-cli-install.html" target="_blank">http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-cli-install.html</a></p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec18"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we have gotten Spark installed on our machine for local development and set up on our cluster, and so we are ready to run the applications that we write. While installing and maintaining a cluster is a good option, Spark is also available as a service option from Databricks. Databricks' upcoming Databricks Cloud for Spark available at <a class="ulink" href="http://databricks.com/product" target="_blank">http://databricks.com/product</a> is a very convenient offering for anyone who does not want to deal with the set up/maintenance of the cluster. They have the concept of a big data pipeline — from ETL to Analytics. This looks truly interesting to explore!</p><p>In the next chapter, you will learn to use the Spark shell.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch02"></a>Chapter 2. Using the Spark Shell</h2></div></div></div><p>The Spark shell<a id="id67" class="indexterm"></a> is a wonderful tool for rapid prototyping with Spark. It helps to be familiar with Scala, but that isn't necessary. The Spark shell works with Scala and Python. The Spark shell allows you to interactively query and communicate with the Spark cluster. This can be great for debugging, for just trying things out, or interactively exploring new datasets or approaches. The previous chapter should have gotten you to the point of having a Spark instance running, so now, all you need to do is start your Spark shell and point it at your running instance with the command given in the next few lines. Spark will start an instance when you invoke the Spark shell or start a Spark program from an IDE. So, a local installation on a Mac or Linux PC/laptop is sufficient to start exploring the Spark shell. Not having to spin up a real cluster to do the prototyping is an important feature of Spark.</p><p>Assuming that you have installed Spark in the <code class="literal">/opt</code> directory and have a soft link to Spark, run the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>cd /opt/spark </strong></span>
<span class="strong"><strong>export MASTER=spark://`hostname`:7077 </strong></span>
<span class="strong"><strong>bin/spark-shell</strong></span>
</pre></div><p>If you are running Spark in the local mode and don't have a Spark instance already running, you can just run the preceding command without the <code class="literal">MASTER=</code> part. As a result, the shell will run with only one thread; you can specify <code class="literal">local[n]</code> to run <span class="emphasis"><em>n</em></span> threads.</p><p>You will see the shell prompt as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/4005_02_01.jpg" /></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec19"></a>Loading a simple text file</h2></div></div><hr /></div><p>While running a Spark <a id="id68" class="indexterm"></a>shell and connecting to an existing cluster, you should see something specifying the <code class="literal">app ID</code> such as "Connected to Spark cluster with <code class="literal">app ID</code> app-20130330015119-0001." The <code class="literal">app ID</code> will match the application entry as shown in the Web UI under running applications (by default, it will be viewable on port 4040). Start by downloading a dataset to use for some experimentation. There are a number of datasets put together for <span class="emphasis"><em>The Elements of Statistical Learning</em></span>, which are in a very convenient form to use. Grab the spam dataset using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>wget http://www-stat.stanford.edu/~tibs/ElemStatLearn/datasets/spam.data</strong></span>
</pre></div><p>Alternatively, you can find the spam dataset from the GitHub link<a id="id69" class="indexterm"></a> at <a class="ulink" href="https://github.com/xsankar/fdps-vii" target="_blank">https://github.com/xsankar/fdps-vii</a>.</p><p>Now, load it as a text file into Spark with the following command inside your Spark shell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val inFile = sc.textFile("./spam.data")</strong></span>
</pre></div><p>This loads the <code class="literal">spam.data</code> file into Spark with each line being a separate entry in the <a id="id70" class="indexterm"></a>
<span class="strong"><strong>Resilient Distributed Datasets</strong></span> (<span class="strong"><strong>RDD</strong></span>). You will see RDDs in the later chapters, but RDD, in brief, is the basic data structure that Spark relies on. RDDs are very versatile in terms of scaling, computation capabilities, and transformations.</p><p>The path assumes that the data would be in the <code class="literal">/opt/spark</code> directory. Please type in the appropriate directory where you have downloaded the data.</p><p>The <code class="literal">sc</code> in the command line is the Spark context. While applications would create a Spark context explicitly, the Spark shell creates one called <code class="literal">sc</code> for you and that is the one we normally use.</p><p>Note: If you've connected to a <a id="id71" class="indexterm"></a>Spark master, it's possible that it will attempt to load the file on any one of the different machines in the cluster, so make sure that it can be accessed by all the worker nodes in the cluster. In general you will want to put your data in HDFS, S3, or a similar distributed file systems for the future to avoid this problem. In a local mode, you can just load the file directly (for example, <code class="literal">sc.textFile([filepath]</code>)). You can also use the <code class="literal">addFile</code> function on the Spark context to make a file available across all of the machines like this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.SparkFiles</strong></span>
<span class="strong"><strong>scala&gt; val file = sc.addFile("/opt/spark/spam.data")</strong></span>
<span class="strong"><strong>scala&gt; val inFile = sc.textFile(SparkFiles.get("spam.data"))</strong></span>
</pre></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip13"></a>Tip</h3><p>Just like most shells, the Spark shell has a command history; you can press the up arrow key to get to the previous commands. Are you getting tired of typing or not sure what method you want to call on an object? Press <span class="emphasis"><em>Tab</em></span>, and the Spark shell will autocomplete the line of code in the best way it can.</p></div><p>For this example, the RDD with each line as an individual string isn't super useful as our input data is actually space separated numerical information. We can use the <code class="literal">map()</code> operation to iterate over the elements of the RDD and quickly convert it to a usable format (Note: <code class="literal">_.toDouble</code> is the Scala syntactic sugar for <code class="literal">x =&gt; x.toDouble</code>). We use one map operation to convert the line to a set of numbers in string format and then convert each of the number to a double, as shown next:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val nums = inFile.map(line =&gt; line.split(' ').map(_.toDouble))</strong></span>
</pre></div><p>Verify that this is what we want by inspecting some elements in the <code class="literal">nums</code> RDD and comparing them against the original string RDD. Take a look at the first element of each by calling <code class="literal">.first()</code> on the RDDs:</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip14"></a>Tip</h3><p>Most of the output following these commands is extraneous <code class="literal">INFO</code> messages. It is informative to see what Spark is doing under the covers. But if you want to keep the detailed messages out, you can copy <code class="literal">log4j.properties</code> into the current directory and set the <code class="literal">log4j.rootCategory</code> to <code class="literal">ERROR</code> instead of <code class="literal">INFO</code>. Then none of these messages will appear and it will be possible to concentrate just on the commands and the output.</p></div><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; inFile.first()</strong></span>
<span class="strong"><strong>[...]</strong></span>
<span class="strong"><strong>14/11/15 23:46:41 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool </strong></span>
<span class="strong"><strong>14/11/15 23:46:41 INFO DAGScheduler: Stage 0 (first at &lt;console&gt;:15) finished in 0.058 s</strong></span>
<span class="strong"><strong>14/11/15 23:46:41 INFO SparkContext: Job finished: first at &lt;console&gt;:15, took 0.088417 s</strong></span>
<span class="strong"><strong>res0: String = 0 0.64 0.64 0 0.32 0 0 0 0 0 0 0.64 0 0 0 0.32 0 1.29 1.93 0 0.96 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.778 0 0 3.756 61 278 1</strong></span>

<span class="strong"><strong>scala&gt; nums.first()</strong></span>
<span class="strong"><strong>[...]</strong></span>
<span class="strong"><strong>14/11/15 23:46:42 INFO DAGScheduler: Stage 1 (first at &lt;console&gt;:17) finished in 0.008 s</strong></span>
<span class="strong"><strong>14/11/15 23:46:42 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool </strong></span>
<span class="strong"><strong>14/11/15 23:46:42 INFO SparkContext: Job finished: first at &lt;console&gt;:17, took 0.01287 s</strong></span>
<span class="strong"><strong>res1: Array[Double] = Array(0.0, 0.64, 0.64, 0.0, 0.32, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.64, 0.0, 0.0, 0.0, 0.32, 0.0, 1.29, 1.93, 0.0, 0.96, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.778, 0.0, 0.0, 3.756, 61.0, 278.0, 1.0)</strong></span>
</pre></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip15"></a>Tip</h3><p>Operators in Spark<a id="id72" class="indexterm"></a> are divided into transformations and actions. Transformations are evaluated lazily. Spark just creates the RDD's lineage graph when you call a transformation like map. No actual work is done until an action is invoked on the RDD. Creating the RDD and the map functions are transformations. The <code class="literal">.first()</code> function is an action that forces execution.</p><p>So when we created the inFile, it really didn't do anything except for creating a variable and set up the pointers. Only when we call an action like <code class="literal">.first()</code> does Spark evaluate the transformations. As a result, even if we point the inFile to a non-existent directory, Spark will take it. But when we call <code class="literal">inFile.first()</code>, it will throw the <code class="literal">Input path does not exist:</code> error.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec20"></a>Using the Spark shell to run logistic regression</h2></div></div><hr /></div><p>When you run a command<a id="id73" class="indexterm"></a> and do not specify a left-hand<a id="id74" class="indexterm"></a> side of the assignment (that is leaving out the <code class="literal">val x</code> of <code class="literal">val x = y</code>), the Spark shell will assign a default name (that is, <code class="literal">res[number]</code> to the value. Now that you have the data in a more usable format, try to do something cool with it! Use Spark to run logistic regression over the dataset, as <a id="id75" class="indexterm"></a>shown <a id="id76" class="indexterm"></a>here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import breeze.linalg.{Vector, DenseVector}</strong></span>
<span class="strong"><strong>import breeze.linalg.{Vector, DenseVector}</strong></span>

<span class="strong"><strong>scala&gt; case class DataPoint(x: Vector[Double], y: Double)</strong></span>
<span class="strong"><strong>defined class DataPoint</strong></span>

<span class="strong"><strong>scala&gt; </strong></span>

<span class="strong"><strong>scala&gt; def parsePoint(x: Array[Double]): DataPoint = {</strong></span>
<span class="strong"><strong>     |       DataPoint(new DenseVector(x.slice(0,x.size-2)) , x(x.size-1))</strong></span>
<span class="strong"><strong>     |       }</strong></span>
<span class="strong"><strong>parsePoint: (x: Array[Double])DataPoint</strong></span>

<span class="strong"><strong>scala&gt; val points = nums.map(parsePoint(_))</strong></span>
<span class="strong"><strong>points: org.apache.spark.rdd.RDD[DataPoint] = MappedRDD[3] at map at &lt;console&gt;:21</strong></span>
<span class="strong"><strong>scala&gt; import java.util.Random</strong></span>
<span class="strong"><strong>import java.util.Random</strong></span>

<span class="strong"><strong>scala&gt; val rand = new Random(42)</strong></span>
<span class="strong"><strong>rand: java.util.Random = java.util.Random@24c55bf5</strong></span>

<span class="strong"><strong>scala&gt; points.first()</strong></span>
<span class="strong"><strong>14/11/15 23:47:19 INFO SparkContext: Starting job: first at &lt;console&gt;:25</strong></span>
<span class="strong"><strong>[..] </strong></span>
<span class="strong"><strong>14/11/15 23:47:20 INFO SparkContext: Job finished: first at &lt;console&gt;:25, took 0.188923 s</strong></span>
<span class="strong"><strong>res2: DataPoint = DataPoint(DenseVector(0.0, 0.64, 0.64, 0.0, 0.32, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.64, 0.0, 0.0, 0.0, 0.32, 0.0, 1.29, 1.93, 0.0, 0.96, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.778, 0.0, 0.0, 3.756, 61.0),1.0)</strong></span>

<span class="strong"><strong>scala&gt; var w = DenseVector.fill(nums.first.size-2){rand.nextDouble}</strong></span>
<span class="strong"><strong>14/11/15 23:47:36 INFO SparkContext: Starting job: first at &lt;console&gt;:20</strong></span>
<span class="strong"><strong>[..]</strong></span>
<span class="strong"><strong>14/11/15 23:47:36 INFO SparkContext: Job finished: first at &lt;console&gt;:20, took 0.010883 s</strong></span>
<span class="strong"><strong>w: breeze.linalg.DenseVector[Double] = DenseVector(0.7275636800328681, 0.6832234717598454, 0.30871945533265976, 0.27707849007413665, 0.6655489517945736, 0.9033722646721782, 0.36878291341130565, 0.2757480694417024, 0.46365357580915334, 0.7829017787900358, 0.9193277828687169, 0.43649097442328655, 0.7499061812554475, 0.38656687435934867, 0.17737847790937833, 0.5943499108896841, 0.20976756886633208, 0.825965871887821, 0.17221793768785243, 0.5874273817862956, 0.7512804067674601, 0.5710403484148672, 0.5800248845020607, 0.752509948590651, 0.03141823882658079, 0.35791991947712865, 0.8177969308356393, 0.41768754675291875, 0.9740356814958814, 0.7134062578232291, 0.48057451655643435, 0.2916564974118041, 0.9498601346594666, 0.8204918233863466, 0.636644547856282, 0.3691214939418974, 0.36025487536613...</strong></span>
<span class="strong"><strong>scala&gt; val iterations = 100</strong></span>
<span class="strong"><strong>iterations: Int = 100</strong></span>

<span class="strong"><strong>scala&gt; import scala.math._</strong></span>
<span class="strong"><strong>import scala.math._</strong></span>

<span class="strong"><strong>scala&gt; for (i &lt;- 1 to iterations) {</strong></span>
<span class="strong"><strong>     |         val gradient = points.map(p =&gt;</strong></span>
<span class="strong"><strong>     |           p.x * (1 / (1 + exp(-p.y*(w dot p.x))) - 1) * p.y</strong></span>
<span class="strong"><strong>     |         ).reduce(_ + _)</strong></span>
<span class="strong"><strong>     |         w -= gradient</strong></span>
<span class="strong"><strong>     |       }</strong></span>
<span class="strong"><strong>14/11/15 23:48:49 INFO SparkContext: Starting job: reduce at &lt;console&gt;:37</strong></span>
<span class="strong"><strong>14/11/15 23:48:49 INFO DAGScheduler: Got job 4 (reduce at &lt;console&gt;:37) with 2 output partitions (allowLocal=false)</strong></span>
<span class="strong"><strong>[…]</strong></span>

<span class="strong"><strong>14/11/15 23:48:53 INFO DAGScheduler: Stage 103 (reduce at &lt;console&gt;:37) finished in 0.024 s</strong></span>
<span class="strong"><strong>14/11/15 23:48:53 INFO SparkContext: Job finished: reduce at &lt;console&gt;:37, took 0.027829 s</strong></span>

<span class="strong"><strong>scala&gt; w</strong></span>
<span class="strong"><strong>res5: breeze.linalg.DenseVector[Double] = DenseVector(0.7336269947556883, 0.6895025214435749, 0.4721342862007282, 0.27723026762411473, 0.7829698104387295, 0.9109178772078957, 0.4421282714160576, 0.305394995185795, 0.4669066877779788, 0.8357335159675405, 0.9326548346504113, 0.5986886716855019, 0.7726151240395974, 0.3898162675706965, 0.18143939819778826, 0.8501243079114542, 0.28042415484918654, 0.867752122388921, 2.8395263204719647, 0.5976683218335691, 1.0764145195987342, 0.5718553843530828, 0.5876679823887092, 0.7609997638366504, 0.0793768969191899, 0.4177180953298126, 0.8177970052737001, 0.41885534550137715, 0.9741059468651804, 0.7137870996096644, 0.48057587402871155, 0.2916564975512847, 0.9533675296503782, 0.8204918691826701, 0.6367663765600675, 0.3833218016601887, 0.36677476558721556,...</strong></span>
<span class="strong"><strong>scala&gt; </strong></span>
</pre></div><p>If things went well, you were successful in using Spark to run logistic regression. That's awesome! We have just done a number of things; we defined a class and created an RDD and a function. As you can see, the Spark shell is quite powerful. Much of the power comes from it being based on the Scala REPL(the Scala interactive shell), and so it inherits all of the <a id="id77" class="indexterm"></a>power of the Scala REPL. That being <a id="id78" class="indexterm"></a>said, most of them time you will probably prefer to work with more traditional compiled code rather than in the REPL.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec21"></a>Interactively loading data from S3</h2></div></div><hr /></div><p>Now try another exercise <a id="id79" class="indexterm"></a>with the Spark shell. As part of Amazon's EMR Spark <a id="id80" class="indexterm"></a>support, they have handily provided some sample data of Wikipedia traffic statistics in S3 in the format that Spark can use. To access the data, you first need to set your AWS access credentials as shell params. For instructions on signing up for EC2 and setting up the shell parameters, see <span class="emphasis"><em>Running Spark on EC2</em></span> section in <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Installing Spark and Setting up your Cluster</em></span> (S3 access requires additional keys such as, <code class="literal">fs.s3n.awsAccessKeyId/awsSecretAccessKey</code> or using the <code class="literal">s3n://user:pw@</code> syntax). You can also set the shell parameters as <code class="literal">AWS_ACCESS_KEY_ID</code> and <code class="literal">AWS_SECRET_ACCESS_KEY</code>. We will leave the AWS configuration out of this discussion, but it needs to be completed. Once this is done, load the S3 data and take a look at the first line:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val file = sc.textFile("s3n://bigdatademo/sample/wiki/")</strong></span>
<span class="strong"><strong>14/11/16 00:02:43 INFO MemoryStore: ensureFreeSpace(34070) called with curMem=512470, maxMem=278302556</strong></span>
<span class="strong"><strong>14/11/16 00:02:43 INFO MemoryStore: Block broadcast_105 stored as values in memory (estimated size 33.3 KB, free 264.9 MB)</strong></span>
<span class="strong"><strong>file: org.apache.spark.rdd.RDD[String] = s3n://bigdatademo/sample/wiki/ MappedRDD[105] at textFile at &lt;console&gt;:17</strong></span>

<span class="strong"><strong>scala&gt; file.first()</strong></span>
<span class="strong"><strong>14/11/16 00:02:58 INFO BlockManager: Removing broadcast 104</strong></span>
<span class="strong"><strong>14/11/16 00:02:58 INFO BlockManager: Removing block broadcast_104</strong></span>
<span class="strong"><strong>[..]</strong></span>
<span class="strong"><strong>14/11/16 00:03:00 INFO SparkContext: Job finished: first at &lt;console&gt;:20, took 0.442788 s</strong></span>
<span class="strong"><strong>res6: String = aa.b Pecial:Listusers/sysop 1 4695</strong></span>

<span class="strong"><strong>scala&gt; file.take(1)</strong></span>
<span class="strong"><strong>14/11/16 00:05:06 INFO SparkContext: Starting job: take at &lt;console&gt;:20</strong></span>
<span class="strong"><strong>14/11/16 00:05:06 INFO DAGScheduler: Got job 105 (take at &lt;console&gt;:20) with 1 output partitions (allowLocal=true)</strong></span>
<span class="strong"><strong>14/11/16 00:05:06 INFO DAGScheduler: Final stage: Stage 105(take at &lt;console&gt;:20)</strong></span>
<span class="strong"><strong>[…]</strong></span>
<span class="strong"><strong>14/11/16 00:05:07 INFO SparkContext: Job finished: take at &lt;console&gt;:20, took 0.777104 s</strong></span>
<span class="strong"><strong>res7: Array[String] = Array(aa.b Pecial:Listusers/sysop 1 4695)</strong></span>
</pre></div><p>You don't need to set your AWS credentials as shell params; the general form of the S3 path is <code class="literal">s3n://&lt;AWS ACCESS ID&gt;:&lt;AWS SECRET&gt;@bucket/path</code>.</p><p>It is important to take a <a id="id81" class="indexterm"></a>look at the first line of the data; the reason for this is<a id="id82" class="indexterm"></a> that  Spark won't actually bother to load the data unless we force it to materialize something with it. It is useful to note that Amazon had provided a small sample dataset to get started with. The data is pulled from a much larger set available at <a class="ulink" href="http://aws.amazon.com/datasets/4182" target="_blank">http://aws.amazon.com/datasets/4182</a>. This practice can be quite useful when developing in interactive mode as you want fast feedback of your jobs that are completing quickly. If your sample data was too big and your runs were taking too long, you could quickly slim down the RDD by using the <code class="literal">sample</code> functionality built into the Spark shell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val seed  = (100*math.random).toInt</strong></span>
<span class="strong"><strong>seed: Int = 8</strong></span>
<span class="strong"><strong>scala&gt; val sample = file.sample(false,1/10.,seed)</strong></span>
<span class="strong"><strong>res10: spark.RDD[String] = SampledRDD[4] at sample at &lt;console&gt;:17</strong></span>
</pre></div><p>If you wanted to rerun on the sampled data later, you could write it back to S3:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; sample.saveAsTextFile("s3n://mysparkbucket/test")</strong></span>
<span class="strong"><strong>13/04/21 22:46:18 INFO spark.PairRDDFunctions: Saving as hadoop file of type (NullWritable, Text)</strong></span>
<span class="strong"><strong>....</strong></span>
<span class="strong"><strong>13/04/21 22:47:46 INFO spark.SparkContext: Job finished: saveAsTextFile at &lt;console&gt;:19, took 87.462236222 s</strong></span>
</pre></div><p>Now that you have the data loaded, find the most popular articles in a sample. First, parse the data by separating it into name and count. Then, reduce by the key summing the counts as there can be multiple entries with the same name. Finally, we swap the key/value so that when we sort by key, we get back the highest count item:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val parsed = file.sample(false,1/10.,seed).map(x =&gt; x.split(" ")).map(x =&gt; (x(1), x(2).toInt))</strong></span>
<span class="strong"><strong>parsed: spark.RDD[(java.lang.String, Int)] = MappedRDD[5] at map at &lt;console&gt;:16</strong></span>

<span class="strong"><strong>scala&gt; val reduced = parsed.reduceByKey(_+_)</strong></span>
<span class="strong"><strong>13/04/21 23:21:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</strong></span>
<span class="strong"><strong>13/04/21 23:21:49 WARN snappy.LoadSnappy: Snappy native library not loaded</strong></span>
<span class="strong"><strong>13/04/21 23:21:50 INFO mapred.FileInputFormat: Total input paths to process : 1</strong></span>
<span class="strong"><strong>reduced: spark.RDD[(java.lang.String, Int)] = MapPartitionsRDD[8] at reduceByKey at &lt;console&gt;:18</strong></span>

<span class="strong"><strong>scala&gt; val countThenTitle = reduced.map(x =&gt; (x._2, x._1))</strong></span>
<span class="strong"><strong>countThenTitle: spark.RDD[(Int, java.lang.String)] = MappedRDD[9] at map at &lt;console&gt;:20</strong></span>

<span class="strong"><strong>scala&gt; countThenTitle.sortByKey(false).take(10)</strong></span>
<span class="strong"><strong>13/04/21 23:22:08 INFO spark.SparkContext: Starting job: take at &lt;console&gt;:23</strong></span>
<span class="strong"><strong>....</strong></span>
<span class="strong"><strong>13/04/21 23:23:15 INFO spark.SparkContext: Job finished: take at &lt;console&gt;:23, took 66.815676564 s</strong></span>
<span class="strong"><strong>res1: Array[(Int, java.lang.String)] = Array((213652,Main_Page), (14851,Special:Search), (9528,Special:Export/Can_You_Hear_Me), (6454,Wikipedia:Hauptseite), (4189,Special:Watchlist), (3520,%E7%89%B9%E5%88%A5:%E3%81%8A%E3%81%BE%E3%81%8B%E3%81%9B%E8%A1%A8%E7%A4%BA), (2857,Special:AutoLogin), (2416,P%C3%A1gina_principal), (1990,Survivor_(TV_series)), (1953,Asperger_syndrome))</strong></span>
</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec13"></a>Running Spark shell in Python</h3></div></div></div><p>If you are more comfortable<a id="id83" class="indexterm"></a> with Python than Scala, you can also work with<a id="id84" class="indexterm"></a> Spark interactively in Python by running <code class="literal">[cmd]./pyspark[/cdm]</code>. Just to start working in the Python shell, let's perform the commands in<a id="id85" class="indexterm"></a> quick start, as shown at <a class="ulink" href="http://spark.apache.org/docs/1.1.0/quick-start.html" target="_blank">http://spark.apache.org/docs/1.1.0/quick-start.html</a>. This is just a simple exercise. We will see more of Python in <a class="link" href="#" linkend="ch09">Chapter 9</a>, <span class="emphasis"><em>Machine Learning Using Spark Mllib</em></span>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ bin/pyspark</strong></span>
<span class="strong"><strong>[..]</strong></span>
<span class="strong"><strong>Welcome to</strong></span>
<span class="strong"><strong>      ____              __</strong></span>
<span class="strong"><strong>     / __/__  ___ _____/ /__</strong></span>
<span class="strong"><strong>    _\ \/ _ \/ _ `/ __/  '_/</strong></span>
<span class="strong"><strong>   /__ / .__/\_,_/_/ /_/\_\   version 1.1.1</strong></span>
<span class="strong"><strong>      /_/</strong></span>

<span class="strong"><strong>Using Python version 2.7.8 (default, Aug 21 2014 15:21:46)</strong></span>
<span class="strong"><strong>SparkContext available as sc.</strong></span>
<span class="strong"><strong>Let us read in a file</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; textFile = sc.textFile("README.md")</strong></span>
<span class="strong"><strong>14/11/16 00:12:11 INFO MemoryStore: ensureFreeSpace(34046) called with curMem=0, maxMem=278302556</strong></span>
<span class="strong"><strong>14/11/16 00:12:11 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 33.2 KB, free 265.4 MB)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; textFile.count()</strong></span>
<span class="strong"><strong>[..]</strong></span>
<span class="strong"><strong>14/11/16 00:12:23 INFO DAGScheduler: Stage 0 (count at &lt;stdin&gt;:1) finished in 0.733 s</strong></span>
<span class="strong"><strong>14/11/16 00:12:23 INFO SparkContext: Job finished: count at &lt;stdin&gt;:1, took 0.769692 s</strong></span>
<span class="strong"><strong>141</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; textFile.first()</strong></span>
<span class="strong"><strong>14/11/16 00:12:35 INFO SparkContext: Starting job: runJob at PythonRDD.scala:300</strong></span>
<span class="strong"><strong>[..]</strong></span>
<span class="strong"><strong>14/11/16 00:12:35 INFO SparkContext: Job finished: runJob at PythonRDD.scala:300, took 0.029673 s</strong></span>
<span class="strong"><strong>u'# Apache Spark'</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; linesWithSpark = textFile.filter(lambda line: "Spark" in line)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; textFile.filter(lambda line: "Spark" in line).count()</strong></span>
<span class="strong"><strong>14/11/16 00:13:15 INFO SparkContext: Starting job: count at &lt;stdin&gt;:1</strong></span>
<span class="strong"><strong>[..]</strong></span>
<span class="strong"><strong>14/11/16 00:13:15 INFO SparkContext: Job finished: count at &lt;stdin&gt;:1, took 0.0379 s</strong></span>
<span class="strong"><strong>21</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; </strong></span>
</pre></div><p>As you can see, the Python <a id="id86" class="indexterm"></a>operations are very similar to those in <a id="id87" class="indexterm"></a>Scala.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec22"></a>Summary</h2></div></div><hr /></div><p>In this chapter, you learned how to start the Spark shell and load our data, and you also did some simple machine learning. Now that you've seen how Spark's interactive console works, it's time to see how to build Spark jobs in a more traditional and persistent environment in the subsequent chapters.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch03"></a>Chapter 3. Building and Running a Spark Application</h2></div></div></div><p>Using Spark in an interactive mode with the Spark shell has limited permanence and does not work in Java. Building Spark jobs is a bit trickier than building a normal application as all dependencies have to be available on all the machines that are in your cluster. This chapter will cover the process of building a Java and Scala Spark job with Maven or <span class="strong"><strong>sbt</strong></span> (<span class="strong"><strong>simple-build-tool</strong></span>) and how to build Spark jobs with a non-Maven aware build system. A reference website to build Spark<a id="id88" class="indexterm"></a> is <a class="ulink" href="http://spark.apache.org/docs/latest/building-spark.html" target="_blank">http://spark.apache.org/docs/latest/building-spark.html</a>.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec23"></a>Building your Spark project with sbt</h2></div></div><hr /></div><p>The sbt is a popular <a id="id89" class="indexterm"></a>build tool for Scala that supports building both <a id="id90" class="indexterm"></a>Scala and Java codes. Building Spark projects with sbt is one of the easiest options. Spark release was originally built with sbt, but now they use Maven. However, the various members of the team actively use both sbt and Maven. The current normal method of building packages that use sbt is to use a shell script that bootstraps the specific version of sbt your project uses, thus making installation simpler.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip16"></a>Tip</h3><p>If you are using a prebuilt Spark version, you will need to download and create the <code class="literal">sbt</code> directory.</p></div><p>As a first step, take a Spark job that already works and go through the process of creating a build file for it. In the Spark directory, start by copying the <code class="literal">GroupByTest</code> example into a new directory, as shown here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>mkdir -p example-scala-build/src/main/scala/spark/examples/</strong></span>
<span class="strong"><strong>cp -af sbt example-scala-build//</strong></span>
<span class="strong"><strong>cp examples/src/main/scala/org/apache/spark/examples/GroupByTest.scala example-scala-build/src/main/scala/spark/examples/</strong></span>
</pre></div><p>As you are going to<a id="id91" class="indexterm"></a> ship your JAR to the other machines, you will <a id="id92" class="indexterm"></a>want to ensure all dependencies are included. You can either add a bunch of JARs or use a handy sbt plugin called <code class="literal">sbt-assembly</code> to group everything into a single JAR. If you don't have a bunch of transitive dependencies, you may decide that using the assembly extension isn't for your project. Instead of using <code class="literal">sbt-assembly</code>, you probably want to run <code class="literal">sbt/sbt assembly</code> in the Spark project and add the resulting JAR, <code class="literal">core/target/ spark-core_2.10-1.1.1.jar</code>, to your class path. The <code class="literal">sbt assembly</code> package is a great tool to avoid manual management of a large number of JARs. To add the assembly extension to your build, add the following code to <code class="literal">project/plugins.sbt</code>:</p><div class="informalexample"><pre class="programlisting">resolvers += Resolver.url("artifactory", url("http://scalasbt.artifactoryonline.com/scalasbt/sbt-plugin-releases"))(Resolver.ivyStylePatterns)

resolvers += "Typesafe Repository" at "http://repo.typesafe.com/typesafe/releases/"

resolvers += "Spray Repository" at "http://repo.spray.cc/"
addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.8.7")</pre></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip17"></a>Tip</h3><p>For sbt 0.13.6+, add <code class="literal">sbt-assembly</code> as a dependency in <code class="literal">project/assembly.sbt</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.12.0")</strong></span>
</pre></div><p>Resolvers are used by sbt so that it can find out where a package is; you can think of this as similar to specifying an additional apt <span class="strong"><strong>Personal Package Archive</strong></span> (<span class="strong"><strong>PPA</strong></span>)<a id="id93" class="indexterm"></a> source with the exception that it only applies to the one package you are trying to build. If you load up the resolver URLs in your browser, most of them have the directory listing turned on, and so you can see what packages are provided by the resolver. These resolvers point at web URLs, but there are also resolvers available for local paths that can be useful during development. The <code class="literal">addSbt</code> plugin directive is deceptively simple; it tells the user to include the <code class="literal">sbt-assembly</code> package from <code class="literal">com.eed3si9n</code> in Version 0.8.7 and implicitly adds the Scala Version and the sbt Version. Make sure to run the sbt reload clean update to install new plugins.</p></div><p>Here is the build file for one of the examples of <code class="literal">GroupByTest.scala</code> as if it was being built on its own; put the following code in <code class="literal">./build.sbt</code>:</p><div class="informalexample"><pre class="programlisting">//Next two lines only needed if you decide to use the assembly plugin
import AssemblyKeys._assemblySettings

scalaVersion := "2.10.4"

name := "groupbytest"

libraryDependencies ++= Seq(
   "org.spark-project" % "spark-core_2.10" % "1.1.0"
)</pre></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip19"></a>Tip</h3><p>If the preceding code does not work, you can try:</p><div class="informalexample"><pre class="programlisting">libraryDependencies += "org.apache.spark" %% "spark-core" % "1.1.0"</pre></div><p>Otherwise, you can try this code snippet:</p><div class="informalexample"><pre class="programlisting">libraryDependencies += "org.apache.spark" %% "spark-core" % "1.1.1".
</pre></div></div><div class="informalexample"><pre class="programlisting">
resolvers ++= Seq(
   "JBoss Repository" at "http://repository.jboss.org/nexus/content/repositories/releases/",
   "Spray Repository" at "http://repo.spray.cc/","Cloudera Repository" at "https://repository.cloudera.com/artifactory/cloudera-repos/",
   "Akka Repository" at "http://repo.akka.io/releases/","Twitter4J Repository" at "http://twitter4j.org/maven2/")
// Only include if using assembly
mergeStrategy in assembly &lt;&lt;= (mergeStrategy in assembly) { (old) =&gt;
  {
  case PathList("javax", "servlet", xs @ _*) =&gt;
  MergeStrategy.first
  case PathList("org", "apache", xs @ _*) =&gt; MergeStrategy.first
  case "about.html"  =&gt; MergeStrategy.rename
  case x =&gt; old(x)
  }
}</pre></div><p>As you can see, the <a id="id94" class="indexterm"></a>build file is similar to <code class="literal">plugin.sbt</code> in <a id="id95" class="indexterm"></a>format. There are a few unique things about this build file that are worth mentioning. Just like with the plugin file, you need to add a number of resolvers here so that sbt can find all the dependencies. Note that we are including it as <code class="literal">"org.spark-project" % "spark-core_2.10.4" % "1.1.0"</code> rather than using <code class="literal">"org.spark-project" %% "spark-core" % "1.1.0"</code>. If possible, you should try to use the <code class="literal">%%</code> format, which automatically adds the Scala version. Another unique part of this build file is the use of <code class="literal">MergeStrategy</code>. As multiple dependencies can define the same <a id="id96" class="indexterm"></a>files, when you merge everything into a<a id="id97" class="indexterm"></a> single JAR you need to tell the plugin how to handle it. It is a fairly simple build file other than the merge strategy you need to manually specify the Scala version of Spark you are using.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip21"></a>Tip</h3><p>Note: If you have a different JDK on the master than JRE on the workers, you may want to switch the target JDK by adding the following to your build file:</p><div class="informalexample"><pre class="programlisting">javacOptions ++= Seq("-target", "1.6")</pre></div></div><p>Now that your build file is defined, build your <code class="literal">GroupByTest</code> Spark job using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sbt/sbt clean compile package</strong></span>
</pre></div><p>It will then produce <code class="literal">target/scala-2.10.4/groupbytest_2.10.4-0.1-SNAPSHOT.jar</code>.</p><p>Run <code class="literal">sbt/sbt assembly</code> in the Spark directory to make sure you have the Spark assembly available to your class paths. The example requires a pointer to the location where Spark is using <code class="literal">SPARK_HOME;</code> provide a pointer to the example of  JAR with <code class="literal">SPARK_EXAMPLES_JAR</code> for Spark to distribute out. We also need to specify the class path that we built to Scala locally with <code class="literal">-cp</code>. So we can then run the following example:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>SPARK_HOME="../"  SPARK_EXAMPLES_JAR="./target/scala-2.10.4/groupbytest-assembly-0.1-SNAPSHOT.jar"  scala -cp /users/sparkuser/spark-1.1.0/example-scala-build/target/scala-2.10.4/groupbytest_2.10.4-0.1-SNAPSHOT.jar:/users/sparkuser/spark-1.1.0/core/target/spark-core-assembly-1.1.0.jar spark.examples.GroupByTest local[1]</strong></span>
</pre></div><p>If you have decided to build all of your dependencies into a single JAR with the assembly plugin, we need to call it using this command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sbt/sbt assembly</strong></span>
</pre></div><p>This will produce an assembly snapshot at <code class="literal">target/scala-2.10.4/groupbytest-assembly-0.1-SNAPSHOT.jar,</code> which you can then run in a very similar manner, simply without the <code class="literal">spark-core-assembly</code>, as shown here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>SPARK_HOME="../" \ SPARK_EXAMPLES_JAR="./target/scala-2.10.4/groupbytest-assembly-0.1-SNAPSHOT.jar" \</strong></span>
<span class="strong"><strong> scala -cp /users/sparkuser/spark-1.1.0/example-scala-build/target/scala-2.10.4/groupbytest-assembly-0.1-SNAPSHOT.jar spark.examples.GroupByTest local[1]</strong></span>
</pre></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip22"></a>Tip</h3><p>You may <a id="id98" class="indexterm"></a>run into merge issues with sbt assembly if <a id="id99" class="indexterm"></a>things have changed; a quick search of the Web will probably provide better current guidance than anything that could be written in future. So you need to keep in mind future merge problems. In general, <code class="literal">MergeStategy.first</code> should work.</p><p>Your success in the preceding section may have given you a false sense of security. As sbt will resolve from the local cache, deps that were brought in by another project could mean that the code builds on one machine and not others. Delete your local ivy cache and run sbt clean to make sure. If some files fail to download, try looking at Spark's list of resolvers and add any missing ones to your <code class="literal">build.sbt</code>.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec24"></a>Building your Spark job with Maven</h2></div></div><hr /></div><p>Maven is an open source <a id="id100" class="indexterm"></a>Apache project that builds Spark jobs in Java or <a id="id101" class="indexterm"></a>Scala. As of Version 1.2.0, the building Spark site states that Maven is the official recommendation for packaging Spark and is the "build of reference" too. As with sbt, you can include the Spark dependency through Maven central simplifying our build process. Also similar to sbt is the ability of Spark and all of our dependencies to put everything in a single JAR using a plugin or build Spark as a monolithic JAR using <code class="literal">sbt/sbt assembly</code> for inclusion.</p><p>To illustrate the build process for Spark jobs with Maven, this section will use Java as an example as Maven is more commonly used to build Java tasks. As a first step, let's take a Spark job that already works and go through the process of creating a build file for it. We can start by copying the <code class="literal">GroupByTest</code> example into a new directory and generating the Maven template, as shown here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>mkdir example-java-build/; cd example-java-build</strong></span>
<span class="strong"><strong>mvn archetype:generate \</strong></span>
<span class="strong"><strong>   -DarchetypeGroupId=org.apache.maven.archetypes \</strong></span>
<span class="strong"><strong>   -DgroupId=spark.examples \</strong></span>
<span class="strong"><strong>   -DartifactId=JavaWordCount \</strong></span>
<span class="strong"><strong>   -Dfilter=org.apache.maven.archetypes:maven-archetype-quickstart</strong></span>
<span class="strong"><strong>cp ../examples/src/main/java/spark/examples/JavaWordCount.java JavaWordCount/src/main/java/spark/examples/JavaWordCount.java</strong></span>
</pre></div><p>Next, update your Maven <code class="literal">example-java-build/JavaWordCount/pom.xml</code> to include information on the version of Spark we are using. Also, since the example file we are working with requires a JDK version greater than 1.5, we will need to update the Java version that Maven is configured to use; the current version is 1.3. In between the project tags, we will need to add<a id="id102" class="indexterm"></a> the following code:</p><div class="informalexample"><pre class="programlisting">  &lt;dependencies&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;junit&lt;/groupId&gt;
      &lt;artifactId&gt;junit&lt;/artifactId&gt;
      &lt;version&gt;4.11&lt;/version&gt;
      &lt;scope&gt;test&lt;/scope&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;org.spark-project&lt;/groupId&gt;
      &lt;artifactId&gt;spark-core_2.10.4&lt;/artifactId&gt;
      &lt;version&gt;1.1.0&lt;/version&gt;
    &lt;/dependency&gt;
  &lt;/dependencies&gt;
  &lt;build&gt;
    &lt;plugins&gt;
      &lt;plugin&gt;
        &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
        &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
        &lt;configuration&gt;
          &lt;source&gt;1.7&lt;/source&gt;
          &lt;target&gt;1.7&lt;/target&gt;
        &lt;/configuration&gt;
      &lt;/plugin&gt;
    &lt;/plugins&gt;
  &lt;/build&gt;</pre></div><p>We can now build our JAR <a id="id103" class="indexterm"></a>with the <code class="literal">mvn</code> package that can be run with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>SPARK_HOME="../"  SPARK_EXAMPLES_JAR="./target/JavaWordCount-1.0-SNAPSHOT.jar"  java -cp ./target/JavaWordCount-1.0-SNAPSHOT.jar:../../core/target/spark-core-assembly-1.1.0.jar spark.examples.JavaWordCount local[1] ../../README</strong></span>
</pre></div><p>As with sbt, we can use a plugin to include all of the dependencies in our JAR file. Between the <code class="literal">&lt;plugins&gt;</code> tags, add the following code:</p><div class="informalexample"><pre class="programlisting">&lt;plugin&gt;
  &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
  &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;
  &lt;version&gt;2.3&lt;/version&gt;
  &lt;configuration&gt;
    &lt;!-- This transform is used so that merging of akka configuration files works --&gt;
    &lt;transformers&gt;
      &lt;transformer implementation="org.apache.maven.plugins.shade.resource.ApacheLicenseResourceTransformer"&gt;
      &lt;/transformer&gt;
      &lt;transformer implementation="org.apache.maven.plugins.shade.resource.AppendingTransformer"&gt;
        &lt;resource&gt;reference.conf&lt;/resource&gt;
      &lt;/transformer&gt;
    &lt;/transformers&gt;
  &lt;/configuration&gt;
  &lt;executions&gt;
    &lt;execution&gt;
      &lt;phase&gt;package&lt;/phase&gt;
      &lt;goals&gt;
        &lt;goal&gt;shade&lt;/goal&gt;
      &lt;/goals&gt;
    &lt;/execution&gt;
  &lt;/executions&gt;
&lt;/plugin&gt;</pre></div><p>Then run <code class="literal">mvn assembly</code> and the <a id="id104" class="indexterm"></a>resulting JAR can be run as shown in the preceding <a id="id105" class="indexterm"></a>section; but leave out the Spark assembly JAR from the class path.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note03"></a>Note</h3><p>As I was writing this chapter (November 16, 2014), an e-mail chain crossed the Spark user group discussing sbt versus Maven. The use of Maven is recommended unless one needs some special capability of sbt.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec25"></a>Building your Spark job with something else</h2></div></div><hr /></div><p>If neither sbt nor Maven <a id="id106" class="indexterm"></a>suits your needs, you may decide to use another build system. Thankfully, Spark supports building a fat JAR with all its dependencies, which makes it easy to include in the build system of your choice. Simply run <code class="literal">sbt/sbt assembly</code> in the Spark directory and copy the resulting assembly JAR at <code class="literal">core/target/spark-core-assembly-1.1.0.jar</code> to your build dependencies, and you are good to go. It is more common to use the <code class="literal">spark-assembly-1.2.0-hadoop2.6.0.jar</code> file. These files exist in <code class="literal">$SPARK_HOME$/lib</code> (if users use a prebuilt version) or in <code class="literal">$SPARK_HOME$/ assembly/target/scala-2.10/</code> (if users build the source code with Maven or sbt).</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip23"></a>Tip</h3><p>No matter what your build system is, you may find yourself wanting to use a patched version of the Spark libraries. In this case, you can deploy your Spark library locally. I recommend giving it a different version number to ensure that sbt/Maven picks up the modified version. You can change the version by editing <code class="literal">project/SparkBuild.scala</code> and changing the <code class="literal">version:=</code> part according to the version you have installed. If you are using sbt, you should run <code class="literal">sbt/sbt update</code> in the project that is importing the custom version. For other build systems, you just<a id="id107" class="indexterm"></a> need to ensure that you use the new assembly JAR as part of your build.</p></div><p>Some <a id="id108" class="indexterm"></a>references are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><a class="ulink" href="http://spark.apache.org/docs/latest/building-spark.html" target="_blank">http://spark.apache.org/docs/latest/building-spark.html</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://www.scala-sbt.org/" target="_blank">http://www.scala-sbt.org/</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="https://github.com/sbt/sbt-assembly" target="_blank">https://github.com/sbt/sbt-assembly</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://spark-project.org/docs/latest/scala-programming-guide.html" target="_blank">http://spark-project.org/docs/latest/scala-programming-guide.html</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://maven.apache.org/guides/getting-started/" target="_blank">http://maven.apache.org/guides/getting-started/</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://maven.apache.org/plugins/maven-compiler-plugin/examples/set-compiler-source-and-target.html" target="_blank">http://maven.apache.org/plugins/maven-compiler-plugin/examples/set-compiler-source-and-target.html</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://maven.apache.org/plugins/maven-dependency-plugin/" target="_blank">http://maven.apache.org/plugins/maven-dependency-plugin/</a></p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec26"></a>Summary</h2></div></div><hr /></div><p>So now you can build your Spark jobs with Maven, sbt, or the build system of your choice. It's time to jump in and start learning how to do more fun and exciting things such as learning how to create a Spark context in the subsequent chapter.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch04"></a>Chapter 4. Creating a SparkContext</h2></div></div></div><p>This chapter will cover how to create a <code class="literal">SparkContext</code> object in your cluster. A <code class="literal">SparkContext</code> object represents the connection to a Spark cluster and provides the entry point to interact with Spark. We need to create <code class="literal">SparkContext</code> so that we can interact with Spark and distribute our jobs. In <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Using the Spark Shell</em></span>, we interacted with Spark through the Spark shell that created a <code class="literal">SparkContext</code> object. Now you can create RDDs, broadcast variables and counters, and actually do fun things with your data. The Spark shell serves as an example of interacting with the Spark cluster through a <code class="literal">SparkContext</code> object in <code class="literal">./spark/repl/Main.scala</code>, as shown here:</p><div class="informalexample"><pre class="programlisting">def createSparkContext(): SparkContext = {
    val master = this.master match {
      case Some(m) =&gt; m
      case None =&gt; {
        val prop = System.getenv("MASTER")
        if (prop != null) prop else "local"
      }
    }
    sparkContext = new SparkContext(master, "Spark shell")sparkContext
  }</pre></div><p>The preceding code snippet creates a <code class="literal">SparkContext</code> object using the provided <code class="literal">MASTER</code> environment variable (or local if none are set) called <code class="literal">Spark Shell</code> and doesn't specify any dependencies. This is because <code class="literal">Spark Shell</code> is built into Spark, and as such it doesn't have any JARs that needs to be distributed.</p><p>For a client to establish a connection to the Spark cluster, the <code class="literal">SparkContext</code> object needs some basic information, which is given here:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Master URL: Can be <code class="literal">local[n]</code> for local mode or <code class="literal">Spark://[sparkip]</code> for Spark Server or <code class="literal">mesos://path</code> for a Mesos cluster</p></li><li style="list-style-type: disc"><p><code class="literal">application name</code>: This is a human-readable application name</p></li><li style="list-style-type: disc"><p><code class="literal">sparkHome</code>: This is the path to Spark on the master/workers</p></li><li style="list-style-type: disc"><p><code class="literal">jars</code>: This is the path to the JARs required for your job</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec27"></a>Scala</h2></div></div><hr /></div><p>In a <a id="id109" class="indexterm"></a>Scala program, you can create a <code class="literal">SparkContext</code> object<a id="id110" class="indexterm"></a> with the following code:</p><div class="informalexample"><pre class="programlisting">val sparkContext = new SparkContext(master_path, "application name", ["optional spark home path"],["optional list of jars"])</pre></div><p>While you can hardcode all of these values, it's better to read them from the environment with reasonable defaults. This approach provides maximum flexibility to run the code in a changing environment without having to recompile. Using local as the default value for the master makes it easy to launch your application in a test environment locally. By carefully selecting the defaults, you can avoid having to over specify this. Here is an example of it:</p><div class="informalexample"><pre class="programlisting">import spark.sparkContext
import spark.sparkContext._
import scala.util.Properties

val master = Properties.envOrElse("MASTER","local")
val sparkHome = Properties.get("SPARK_HOME")
val myJars = Seq(System.get("JARS"))
val sparkContext = new SparkContext(master, "my app", sparkHome, myJars)</pre></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec28"></a>Java</h2></div></div><hr /></div><p>To create a <a id="id111" class="indexterm"></a>
<code class="literal">SparkContext</code> object in Java, try using the <a id="id112" class="indexterm"></a>following code:</p><div class="informalexample"><pre class="programlisting">import spark.api.java.JavaSparkContext;
…
JavaSparkContext ctx = new JavaSparkContext("master_url", "application name", ["path_to_spark_home", "path_to_jars"]);</pre></div><p>While the preceding code snippet works (once you have replaced the parameters with the correct values for your setup), it requires a code change if you change any of the parameters. So instead of that, you can use reasonable defaults and allow them to be overridden in a similar way to the following example of the Scala code:</p><div class="informalexample"><pre class="programlisting">String master = System.getEnv("MASTER");
if (master == null) {
    master = "local";
}
String sparkHome = System.getEnv("SPARK_HOME");
if (sparkHome == null) {
    sparkHome = "./";
}
String jars = System.getEnv("JARS");
JavaSparkContext ctx = new JavaSparkContext(System.getenv("MASTER"), "my Java app", System.getenv("SPARK_HOME"), System.getenv("JARS"));</pre></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec29"></a>SparkContext – metadata</h2></div></div><hr /></div><p>The <code class="literal">SparkContext</code> object<a id="id113" class="indexterm"></a> has a set of metadata that I found useful. The <a id="id114" class="indexterm"></a>version number, application name, and memory available are useful. At the start of a Spark program, I usually display/log the version number.</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Value</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Use</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p><code class="literal">appName</code></p>
</td><td style="" align="left" valign="top">
<p>This is <a id="id115" class="indexterm"></a>the application name. If you have established a convention, this field can be useful at runtime.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">getConf</code></p>
</td><td style="" align="left" valign="top">
<p>It <a id="id116" class="indexterm"></a>returns configuration information.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">getExecutorMemoryStatus</code></p>
</td><td style="" align="left" valign="top">
<p>This <a id="id117" class="indexterm"></a>retrieves the memory details. It could be useful if you want to check memory details. As Spark is distributed, the values do not mean that you are out of memory.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">Master</code></p>
</td><td style="" align="left" valign="top">
<p>This is <a id="id118" class="indexterm"></a>the name of the master.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">Version</code></p>
</td><td style="" align="left" valign="top">
<p>I found<a id="id119" class="indexterm"></a> this very useful – especially while testing with different versions.</p>
</td></tr></tbody></table></div><p>Execute the following command from shell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>bin/spark-shell</strong></span>
<span class="strong"><strong>scala&gt; sc.version</strong></span>
<span class="strong"><strong>res1: String = 1.1.1</strong></span>
</pre></div><p>As you can see, I am running Version 1.1.1:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; sc.appName</strong></span>
<span class="strong"><strong>res2: String = Spark shell</strong></span>

<span class="strong"><strong>scala&gt; sc.master</strong></span>
<span class="strong"><strong>res3: String = local[*]</strong></span>

<span class="strong"><strong>scala&gt; sc.getExecutorMemoryStatus</strong></span>
<span class="strong"><strong>res4: scala.collection.Map[String,(Long, Long)] = Map(10.0.1.3:56814 -&gt; (278302556,278302556))</strong></span>
</pre></div><p>The <code class="literal">10.0.1.3</code> is the address of the<a id="id120" class="indexterm"></a> machine. The first value is the<a id="id121" class="indexterm"></a> maximum amount of memory allocated for the block manager (for buffering the intermediate data or caching RDDs), while the second value is the amount of remaining memory:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; sc.getConf</strong></span>
<span class="strong"><strong>res5: org.apache.spark.SparkConf = org.apache.spark.SparkConf@7bc17541</strong></span>

<span class="strong"><strong>scala&gt; sc.getConf.toString()</strong></span>
<span class="strong"><strong>res6: String = org.apache.spark.SparkConf@48acaa84</strong></span>

<span class="strong"><strong>scala&gt; </strong></span>
</pre></div><p>A more informative call of this is given here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; sc.getConf.toDebugString</strong></span>
<span class="strong"><strong>res1: String =</strong></span>
<span class="strong"><strong>spark.app.id=local-1422768546091</strong></span>
<span class="strong"><strong>spark.app.name=Spark shell</strong></span>
<span class="strong"><strong>spark.driver.host=10.0.1.3</strong></span>
<span class="strong"><strong>spark.driver.port=51904</strong></span>
<span class="strong"><strong>spark.executor.id=driver</strong></span>
<span class="strong"><strong>spark.fileserver.uri=http://10.0.1.3:51905</strong></span>
<span class="strong"><strong>spark.home=/usr/local/spark</strong></span>
<span class="strong"><strong>spark.jars=</strong></span>
<span class="strong"><strong>spark.master=local[*]</strong></span>
<span class="strong"><strong>spark.repl.class.uri=http://10.0.1.3:51902</strong></span>
<span class="strong"><strong>spark.tachyonStore.folderName=spark-237294fa-1a29-4550-b033-9a73a8222774</strong></span>
</pre></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec30"></a>Shared Java and Scala APIs</h2></div></div><hr /></div><p>Once you have <a id="id122" class="indexterm"></a>a <code class="literal">SparkContext</code> object created, it will serve as your main entry point. In the<a id="id123" class="indexterm"></a> next chapter, you will learn how to use our <code class="literal">SparkContext</code> object to load and save data. You can also use <code class="literal">SparkContext</code> to launch more Spark jobs and add or remove dependencies. Some of the nondata-driven methods you can use on the <code class="literal">SparkContext</code> object are shown here:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Method</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Use</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p><code class="literal">addJar(path)</code></p>
</td><td style="" align="left" valign="top">
<p>This adds <a id="id124" class="indexterm"></a>the JAR for all future jobs run through the <code class="literal">SparkContext</code> object.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">addFile(path)</code></p>
</td><td style="" align="left" valign="top">
<p>This <a id="id125" class="indexterm"></a>downloads the file to all nodes on the cluster.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">stop()</code></p>
</td><td style="" align="left" valign="top">
<p>This<a id="id126" class="indexterm"></a> shuts down <code class="literal">SparkContext.</code></p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">clearFiles()</code></p>
</td><td style="" align="left" valign="top">
<p>This <a id="id127" class="indexterm"></a>removes the files so that new nodes will not download them.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">clearJars()</code></p>
</td><td style="" align="left" valign="top">
<p>This removes <a id="id128" class="indexterm"></a>the JARs from being required for future jobs.</p>
</td></tr></tbody></table></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec31"></a>Python</h2></div></div><hr /></div><p>The <a id="id129" class="indexterm"></a>Python <code class="literal">SparkContext</code> object is a bit different than the Scala and Java <a id="id130" class="indexterm"></a>contexts as Python doesn't use JARs to distribute dependencies. As you are still likely to have dependencies, set <code class="literal">pyFiles</code> with a ZIP file containing all the dependent libraries as desired on <code class="literal">SparkContext</code> (or leave it empty if you don't have any files to distribute). Create a Python <code class="literal">SparkContext</code> object using this code:</p><div class="informalexample"><pre class="programlisting">from pyspark import SparkContext

sc = SparkContext("master","my python app", sparkHome="sparkhome", pyFiles="placeholderdeps.zip")</pre></div><p>The context metadata from Python is similar to that in Spark, as shown here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>bin/pyspark</strong></span>

<span class="strong"><strong>Welcome to</strong></span>
<span class="strong"><strong>      ____              __</strong></span>
<span class="strong"><strong>     / __/__  ___ _____/ /__</strong></span>
<span class="strong"><strong>    _\ \/ _ \/ _ `/ __/  '_/</strong></span>
<span class="strong"><strong>   /__ / .__/\_,_/_/ /_/\_\   version 1.1.1</strong></span>
<span class="strong"><strong>      /_/</strong></span>

<span class="strong"><strong>Using Python version 2.7.8 (default, Aug 21 2014 15:21:46)</strong></span>
<span class="strong"><strong>SparkContext available as sc.</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; sc.version</strong></span>
<span class="strong"><strong>u'1.1.1'</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; sc.appName</strong></span>
<span class="strong"><strong>u'PySparkShell'</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; sc.master</strong></span>
<span class="strong"><strong>u'local[*]'</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; sc.getExecutorMemoryStatus</strong></span>
<span class="strong"><strong>Traceback (most recent call last):</strong></span>
<span class="strong"><strong>  File "&lt;stdin&gt;", line 1, in &lt;module&gt;</strong></span>
<span class="strong"><strong>AttributeError: 'SparkContext' object has no attribute 'getExecutorMemoryStatus'</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; from pyspark.conf import SparkConf</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; conf = SparkConf()</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; conf.toDebugString()</strong></span>
<span class="strong"><strong>u'spark.app.name=pyspark-shell\nspark.master=local[*]\nspark.submit.pyFiles='</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; </strong></span>
</pre></div><p>PySpark does <a id="id131" class="indexterm"></a>not have the <code class="literal">getExecutorMemoryStatus</code> call yet but we can get some information with the <code class="literal">.toDebugString</code> <a id="id132" class="indexterm"></a>call.</p><p>Now that you are able to create a connection with your Spark cluster, it's time to start loading our data into Spark.</p><a id="id133" class="indexterm"></a><p>Some more information is as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><a class="ulink" href="http://spark-project.org/docs/latest/quick-start.html" target="_blank">http://spark-project.org/docs/latest/quick-start.html</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://www-stat.stanford.edu/~tibs/ElemStatLearn/data.html" target="_blank">http://www-stat.stanford.edu/~tibs/ElemStatLearn/data.html</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="https://github.com/mesos/spark/blob/master/repl/src/main/scala/spark/repl/SparkILoop.scala" target="_blank">https://github.com/mesos/spark/blob/master/repl/src/main/scala/spark/repl/SparkILoop.scala</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://spark.apache.org/docs/latest/api/python/pyspark.context.SparkContext-class.html" target="_blank">http://spark.apache.org/docs/latest/api/python/pyspark.context.SparkContext-class.html</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://www.scala-lang.org/api/current/index.html#scala.util.Properties$" target="_blank">http://www.scala-lang.org/api/current/index.html#scala.util.Properties$</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html" target="_blank">http://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html</a></p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec32"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we covered how to connect to our Spark cluster using a <code class="literal">SparkContext</code> object. By using this knowledge, we will look at the different data sources we can use to load data into Spark in the next chapter.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch05"></a>Chapter 5. Loading and Saving Data in Spark</h2></div></div></div><p>By this point in the book, you have already experimented with the Spark shell, figured out how to create a connection with the Spark cluster, and built jobs for deployment. Now to make those jobs useful, you will learn how to load and save data in Spark. Spark's primary unit for representation of data is an RDD, which allows for easy parallel operations on the data. Other forms of data, such as counters, have their own representation. Spark can load and save RDDs from a variety of sources.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec33"></a>RDDs</h2></div></div><hr /></div><p>Spark RDDs can be <a id="id134" class="indexterm"></a>created from any supported Hadoop source. Native collections in Scala, Java, and Python can also serve as the basis for an RDD. Creating RDDs from a native collection is especially useful for testing.</p><p>Before jumping into the details on the supported data sources/links, take some time to learn about what RDDs are and what they are not. It is crucial to understand that even though an RDD is defined, it does not actually contain data but just creates the pipeline for it. (As an RDD follows the principle of lazy evaluation, it evaluates an expression only when it is needed, that is, when an action is called for.) This means that when you go to access the data in an RDD, it could fail. The computation to create the data in an RDD is only done when the data is referenced by caching or writing out the RDD. This also means that you can chain a large number of operations and not have to worry about excessive blocking in a computational thread. It's important to note during application development that you can write code, compile it, and even run your job; unless you materialize the RDD, your code may not have even tried to load the original data.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip24"></a>Tip</h3><p>Each time you materialize an RDD, it is recomputed; if we are going to be using something frequently, a performance improvement can be achieved by caching the RDD.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec34"></a>Loading data into an RDD</h2></div></div><hr /></div><p>Now the chapter will <a id="id135" class="indexterm"></a>examine the different sources you can use for your RDD. If you<a id="id136" class="indexterm"></a> decide to run it through the examples in the Spark shell, you can call <code class="literal">.cache()</code> or <code class="literal">.first()</code> on the RDDs you generate to verify that it can be loaded. In <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Using the Spark Shell</em></span>, you learned how to load data text from a file and from S3. In this chapter, we will look at different formats of data (text file and CSV) and the different sources (filesystem, HDFS) supported.</p><p>One of the easiest ways of creating an RDD is taking an existing Scala collection and converting it into an RDD. The <code class="literal">SparkContext</code> object provides a function called <code class="literal">parallelize</code> that takes a Scala collection and turns it into an RDD over the same type as the input collection, as shown here:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Scala:</p><div class="informalexample"><pre class="programlisting">val dataRDD = sc.parallelize(List(1,2,4))
dataRDD.take(3)</pre></div></li><li style="list-style-type: disc"><p>Java:</p><div class="informalexample"><pre class="programlisting">import java.util.Arrays;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.*;
import org.apache.spark.api.java.function.Function;

public class LDSV01 {

  public static void main(String[] args) {
    // TODO Auto-generated method stub
    SparkConf conf = new SparkConf().setAppName("Chapter 05").setMaster("local");
    JavaSparkContext ctx = new JavaSparkContext(conf);
    JavaRDD&lt;Integer&gt; dataRDD = ctx.parallelize(Arrays.asList(1,2,4));
    System.out.println(dataRDD.count());
    System.out.println(dataRDD.take(3));
  }

}
[..]
14/11/22 13:37:46 INFO SparkContext: Job finished: count at Test01.java:13, took 0.153231 s
3
[..]
14/11/22 13:37:46 INFO SparkContext: Job finished: take at Test01.java:14, took 0.010193 s
[1, 2, 4]</pre></div></li></ul></div><p>The reason for a full <a id="id137" class="indexterm"></a>program in Java is that you can use the Scala and Python shell, but for Java you need to compile and run the program. I use Eclipse and add the JAR file <code class="literal">/usr/local/spark-1.1.1/assembly/target/scala-2.10/spark-assembly-1.1.1-hadoop2.4.0.jar</code> in the Java build path.</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Python:</p><div class="informalexample"><pre class="programlisting">rdd = sc.parallelize([1,2,3])
rdd.take(3)</pre></div></li></ul></div><p>The simplest method for<a id="id138" class="indexterm"></a> loading external data is loading text from a file. This has a requirement that the file should be available on all the nodes in the cluster, which isn't much of a problem for local mode. When you're in a distributed mode, you will want to use Spark's <code class="literal">addFile</code> functionality to copy the file to all of the machines in your cluster. Assuming your <code class="literal">SparkContext</code> object is called <code class="literal">sc,</code> we could load text data from a file (you need to create the file):</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Scala:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark.SparkFiles;
...
sc.addFile("spam.data")
val inFile = sc.textFile(SparkFiles.get("spam.data"))
inFile.first()</pre></div></li><li style="list-style-type: disc"><p>Java:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark.SparkConf;
import org.apache.spark.api.java.*;
import org.apache.spark.SparkFiles;;

public class LDSV02 {

  public static void main(String[] args) {
    SparkConf conf = new SparkConf().setAppName("Chapter 05").setMaster("local");
    JavaSparkContext ctx = new JavaSparkContext(conf);
    System.out.println("Running Spark Version : " +ctx.version());
    ctx.addFile("/Users/ksankar/fpds-vii/data/spam.data");
    JavaRDD&lt;String&gt; lines = ctx.textFile(SparkFiles.get("spam.data"));
    System.out.println(lines.first());
  }
} </pre></div><p>The runtime<a id="id139" class="indexterm"></a> messages are<a id="id140" class="indexterm"></a> interesting:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Running Spark Version : 1.1.1</strong></span>
<span class="strong"><strong>&lt;It copied the file to a temporary directory in the cluster. This would work in local mode as well as in a spark cluster of many machines&gt;</strong></span>
<span class="strong"><strong>14/11/22 14:05:43 INFO Utils: Copying /Users/ksankar/Tech/spark/book/spam.data to /var/folders/gq/70vnnyfj6913b6lms_td7gb40000gn/T/spark-f4c60229-8290-4db3-a39b-2941f63aabf8/spam.data</strong></span>
<span class="strong"><strong>14/11/22 14:05:43 INFO SparkContext: Added file /Users/ksankar/Tech/spark/book/spam.data at http://10.0.1.3:52338/files/spam.data with timestamp 1416693943289</strong></span>
<span class="strong"><strong>14/11/22 14:05:43 INFO MemoryStore: ensureFreeSpace(163705) called with curMem=0, maxMem=2061647216</strong></span>
<span class="strong"><strong>14/11/22 14:05:43 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 159.9 KB, free 1966.0 MB)</strong></span>
<span class="strong"><strong>14/11/22 14:05:43 INFO FileInputFormat: Total input paths to process : 1</strong></span>
<span class="strong"><strong>[..]</strong></span>
<span class="strong"><strong>14/11/22 14:05:43 INFO SparkContext: Job finished: first at Test02.java:13, took 0.191388 s</strong></span>
<span class="strong"><strong>0 0.64 0.64 0 0.32 0 0 0 0 0 0 0.64 0 0 0 0.32 0 1.29 1.93 0 0.96 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.778 0 0 3.756 61 278 1</strong></span>
</pre></div></li><li style="list-style-type: disc"><p>Python:</p><div class="informalexample"><pre class="programlisting">from pyspark.files import SparkFiles
…
sc.addFile("spam.data")
in_file = sc.textFile(SparkFiles.get("spam.data"))
in_file.take(1)</pre></div></li></ul></div><p>The resulting RDD is of the string type, with each line being a unique element in the RDD. <code class="literal">take(1)</code> is an action that picks the first element from the RDD.</p><p>Frequently, your input files will be CSV or TSV files, which you will want to read and parse and then create RDDs for processing. The two ways of reading CSV files are either reading and parsing them using our own functions or using a CSV library like <code class="literal">opencsv</code>.</p><p>Let's first look at <a id="id141" class="indexterm"></a>parsing<a id="id142" class="indexterm"></a> using our own functions:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Scala:</p><div class="informalexample"><pre class="programlisting">val inFile = sc.textFile("Line_of_numbers.csv")
val numbersRDD = inFile.map(line =&gt; line.split(','))
scala&gt; numbersRDD.take(10)
[..]
14/11/22 12:13:11 INFO SparkContext: Job finished: take at &lt;console&gt;:18, took 0.010062 s
res7: Array[Array[String]] = Array(Array(42, 42, 55, 61, 53, 49, 43, 47, 49, 60, 68, 54, 34, 35, 35, 39))
It is an array of String. We need float or double
val numbersRDD = inFile.map(line =&gt; line.split(',')).map(_.toDouble)
scala&gt; val numbersRDD = inFile.map(line =&gt; line.split(',')).map(_.toDouble)
&lt;console&gt;:15: error: value toDouble is not a member of Array[String]
       val numbersRDD = inFile.map(line =&gt; line.split(',')).map(_.toDouble)
This will not work as we have an array of array of strings. This is where flatMap comes handy!
scala&gt; val numbersRDD = inFile.flatMap(line =&gt; line.split(',')).map(_.toDouble)
numbersRDD: org.apache.spark.rdd.RDD[Double] = MappedRDD[10] at map at &lt;console&gt;:15

scala&gt; numbersRDD.collect()
 [..]
res10: Array[Double] = Array(42.0, 42.0, 55.0, 61.0, 53.0, 49.0, 43.0, 47.0, 49.0, 60.0, 68.0, 54.0, 34.0, 35.0, 35.0, 39.0)
scala&gt; numbersRDD.sum()
[..]
14/11/22 12:19:15 INFO SparkContext: Job finished: sum at &lt;console&gt;:18, took 0.013293 s
res9: Double = 766.0
scala&gt;</pre></div></li><li style="list-style-type: disc"><p>Python:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>inp_file = sc.textFile("Line_of_numbers.csv")</strong></span>
<span class="strong"><strong>numbers_rdd = inp_file.map(lambda line: line.split(','))</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; numbers_rdd.take(10)</strong></span>
<span class="strong"><strong>[..]</strong></span>
<span class="strong"><strong>14/11/22 11:12:25 INFO SparkContext: Job finished: runJob at PythonRDD.scala:300, took 0.023086 s</strong></span>
<span class="strong"><strong>[[u'42', u'42', u'55', u'61', u'53', u'49', u'43', u'47', u'49', u'60', u'68', u'54', u'34', u'35', u'35', u'39']]</strong></span>
<span class="strong"><strong>&gt;&gt;&gt;</strong></span>
<span class="strong"><strong>But we want the values as integers or double</strong></span>
<span class="strong"><strong>numbers_rdd = inp_file.flatMap(lambda line: line.split(',')).map(lambda x:float(x))</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; numbers_rdd.take(10)</strong></span>
<span class="strong"><strong>14/11/22 11:52:39 INFO SparkContext: Job finished: runJob at PythonRDD.scala:300, took 0.022838 s</strong></span>
<span class="strong"><strong>[42.0, 42.0, 55.0, 61.0, 53.0, 49.0, 43.0, 47.0, 49.0, 60.0]</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; numbers_sum = numbers_rdd.sum()</strong></span>
<span class="strong"><strong>[..]</strong></span>
<span class="strong"><strong>14/11/22 12:03:16 INFO SparkContext: Job finished: sum at &lt;stdin&gt;:1, took 0.026984 s</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; numbers_sum</strong></span>
<span class="strong"><strong>766.0</strong></span>
<span class="strong"><strong>&gt;&gt;&gt;</strong></span>
</pre></div></li><li style="list-style-type: disc"><p>Java:</p><div class="informalexample"><pre class="programlisting">import java.util.Arrays;
import java.util.List;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.*;
import org.apache.spark.api.java.function.DoubleFunction;
import org.apache.spark.api.java.function.FlatMapFunction;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.api.java.function.Function2;
import org.apache.spark.SparkFiles;;

public class LDSV03 {

  public static void main(String[] args) {
    SparkConf conf = new SparkConf().setAppName("Chapter 05").setMaster("local");
    JavaSparkContext ctx = new JavaSparkContext(conf);
    System.out.println("Running Spark Version : " +ctx.version());
    ctx.addFile("/Users/ksankar/fdps-vii/data/Line_of_numbers.csv");
    //
    JavaRDD&lt;String&gt; lines = ctx.textFile(SparkFiles.get("Line_of_numbers.csv"));
    //
    JavaRDD&lt;String[]&gt; numbersStrRDD = lines.map(new Function&lt;String,String[]&gt;() {
      public String[] call(String line) {return line.split(",");}
    });
    List&lt;String[]&gt; val = numbersStrRDD.take(1);
    for (String[] e : val) {
      for (String s : e) {
        System.out.print(s+" ");
      }
      System.out.println();
    }
    //
    JavaRDD&lt;String&gt; strFlatRDD = lines.flatMap(new FlatMapFunction&lt;String,String&gt;() {
      public Iterable&lt;String&gt; call(String line) {return Arrays.asList(line.split(","));}
    });
    List&lt;String&gt; val1 = strFlatRDD.collect();
    for (String s : val1) {
      System.out.print(s+" ");
      }
    System.out.println();
    //
    JavaRDD&lt;Integer&gt; numbersRDD = strFlatRDD.map(new Function&lt;String,Integer&gt;() {
      public Integer call(String s) {return Integer.parseInt(s);}
    });
    List&lt;Integer&gt; val2 = numbersRDD.collect();
    for (Integer s : val2) {
      System.out.print(s+" ");
      }
    System.out.println();
    //
    Integer sum = numbersRDD.reduce(new Function2&lt;Integer,Integer,Integer&gt;() {
      public Integer call(Integer a, Integer b) {return a+b;}
    });
    System.out.println("Sum = "+sum);
  }
}</pre></div><p>The results <a id="id143" class="indexterm"></a>are as <a id="id144" class="indexterm"></a>expected:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[..]</strong></span>
<span class="strong"><strong>14/11/22 16:02:18 INFO AkkaUtils: Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@10.0.1.3:56479/user/HeartbeatReceiver</strong></span>
<span class="strong"><strong>Running Spark Version : 1.1.1</strong></span>
<span class="strong"><strong>14/11/22 16:02:18 INFO Utils: Copying /Users/ksankar/Tech/spark/book/Line_of_numbers.csv to /var/folders/gq/70vnnyfj6913b6lms_td7gb40000gn/T/spark-9a4bed6d-adb5-4e08-b5c5-5e9089d6e54b/Line_of_numbers.csv</strong></span>
<span class="strong"><strong>14/11/22 16:02:18 INFO SparkContext: Added file /Users/ksankar/fdps-vii/data/Line_of_numbers.csv at http://10.0.1.3:56484/files/Line_of_numbers.csv with timestamp 1416700938334</strong></span>
<span class="strong"><strong>14/11/22 16:02:18 INFO MemoryStore: ensureFreeSpace(163705) called with curMem=0, maxMem=2061647216</strong></span>
<span class="strong"><strong>14/11/22 16:02:18 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 159.9 KB, free 1966.0 MB)</strong></span>
<span class="strong"><strong>14/11/22 16:02:18 INFO FileInputFormat: Total input paths to process : 1</strong></span>
<span class="strong"><strong>14/11/22 16:02:18 INFO SparkContext: Starting job: take at Test03.java:25</strong></span>
<span class="strong"><strong>[..]</strong></span>
<span class="strong"><strong>14/11/22 16:02:18 INFO SparkContext: Job finished: take at Test03.java:25, took 0.155961 s</strong></span>
<span class="strong"><strong>42 42 55 61 53 49 43 47 49 60 68 54 34 35 35 39 </strong></span>
<span class="strong"><strong>14/11/22 16:02:18 INFO BlockManager: Removing broadcast 1</strong></span>
<span class="strong"><strong>[..]</strong></span>
<span class="strong"><strong>14/11/22 16:02:18 INFO SparkContext: Job finished: collect at Test03.java:36, took 0.016938 s</strong></span>
<span class="strong"><strong>42 42 55 61 53 49 43 47 49 60 68 54 34 35 35 39 </strong></span>
<span class="strong"><strong>14/11/22 16:02:18 INFO SparkContext: Starting job: collect at Test03.java:45</strong></span>
<span class="strong"><strong>[..]</strong></span>
<span class="strong"><strong>14/11/22 16:02:18 INFO SparkContext: Job finished: collect at Test03.java:45, took 0.016657 s</strong></span>
<span class="strong"><strong>42 42 55 61 53 49 43 47 49 60 68 54 34 35 35 39 </strong></span>
<span class="strong"><strong>14/11/22 16:02:18 INFO SparkContext: Starting job: reduce at Test03.java:51</strong></span>
<span class="strong"><strong>[..]</strong></span>
<span class="strong"><strong>14/11/22 16:02:18 INFO SparkContext: Job finished: reduce at Test03.java:51, took 0.019349 s</strong></span>
<span class="strong"><strong>Sum = 766</strong></span>
</pre></div></li></ul></div><p>This also <a id="id145" class="indexterm"></a>illustrates one of the ways of getting data out of Spark; you can<a id="id146" class="indexterm"></a> transform it to a standard Scala array using the <code class="literal">collect()</code> function. The <code class="literal">collect()</code> function is especially useful for testing, in much the same way that the <code class="literal">parallelize()</code> function is. The <code class="literal">collect()</code> function collects the job's execution results, while <code class="literal">parallelize()</code> partitions the input data and makes it an RDD. The <code class="literal">collect</code> function only works if your data fits in memory in a single host (where your code runs on), and even in that case, it adds to the bottleneck that everything has to come back to a single machine.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip25"></a>Tip</h3><p>The <code class="literal">collect()</code> function brings all the data to the machine that runs the code. So beware of accidentally doing <code class="literal">collect()</code> on a large RDD!</p></div><p>The <code class="literal">split()</code> and <code class="literal">toDouble()</code> functions doesn't always work out so well for more complex CSV files. <code class="literal">opencsv</code> is a versatile library for Java and Scala. For Python the CSV library does the trick. Let's use the <code class="literal">opencsv</code> library to parse the CSV files in Scala.</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Scala:</p><div class="informalexample"><pre class="programlisting">import au.com.bytecode.opencsv.CSVReader
import java.io.StringReader

sc.addFile("Line_of_numbers.csv")
val inFile = sc.textFile("Line_of_numbers.csv")
val splitLines = inFile.map(line =&gt; {
  val reader = new CSVReader(new StringReader(line))
  reader.readNext()
})
val numericData = splitLines.map(line =&gt; line.map(_.toDouble))
val summedData = numericData.map(row =&gt; row.sum)
println(summedData.collect().mkString(","))
[..]
14/11/22 12:37:43 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
14/11/22 12:37:43 INFO SparkContext: Job finished: collect at &lt;console&gt;:28, took 0.0234 s
766.0</pre></div></li></ul></div><p>While loading<a id="id147" class="indexterm"></a> text files into Spark is certainly easy, text files on local disk<a id="id148" class="indexterm"></a> are often not the most convenient format for storing large chunks of data. Spark supports loading from all of the different Hadoop formats (sequence files, regular text files, and so on) and from all of the support Hadoop storage sources (HDFS, S3, HBase, and so on). You can also load your CSV into HBase using some of their bulk loading tools (like import TSV) and get your CSV data.</p><p>Sequence files are binary flat files consisting of key value pairs; they are one of the common ways of storing data for use with Hadoop. Loading a sequence file into Spark is similar to loading a text file, but you also need to let it know about the types of the keys and values. The types must either be subclasses of Hadoop's <code class="literal">Writable</code> class or be implicitly convertible to such a type. For Scala users, some natives are convertible through implicits in <code class="literal">WritableConverter</code>. As of Version 1.1.0, the standard <code class="literal">WritableConverter</code> types are int, long, double, float, boolean, byte arrays, and string. Let's illustrate by looking at the process of loading a sequence file of String to Integer, as shown here:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Scala:</p><div class="informalexample"><pre class="programlisting">val data = sc.sequenceFile[String, Int](inputFile)</pre></div></li><li style="list-style-type: disc"><p>Java:</p><div class="informalexample"><pre class="programlisting">JavaPairRDD&lt;Text, IntWritable&gt; dataRDD = sc.sequenceFile(file, Text.class, IntWritable.class);
JavaPairRDD&lt;String, Integer&gt; cleanData = dataRDD.map(new PairFunction&lt;Tuple2&lt;Text, IntWritable&gt;, String, Integer&gt;() {
 @Override
public Tuple2&lt;String, Integer&gt; call(Tuple2&lt;Text, IntWritable&gt; pair) {
return new Tuple2&lt;String, Integer&gt;(pair._1().toString(), pair._2().get());
}
});</pre></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip26"></a>Tip</h3><p>Note that in the preceding cases, like with the text input, the file need not be a traditional file; it can reside on S3, HDFS, and so on. Also note that for Java, you can't rely on implicit conversions between types.</p></div><p>HBase is <a id="id149" class="indexterm"></a>a Hadoop-based database designed to <a id="id150" class="indexterm"></a>support random read/write access to entries. Loading data from HBase is a bit different from text files and sequence in files with respect to how we tell Spark what types to use for the data.</p></li><li style="list-style-type: disc"><p>Scala:</p><div class="informalexample"><pre class="programlisting">import spark._
import org.apache.hadoop.hbase.{HBaseConfiguration, HTableDescriptor}
import org.apache.hadoop.hbase.client.HBaseAdmin
import org.apache.hadoop.hbase.mapreduce.TableInputFormat
….
val conf = HBaseConfiguration.create()
conf.set(TableInputFormat.INPUT_TABLE, input_table)
 // Initialize hBase table if necessary
val admin = new HBaseAdmin(conf)
if(!admin.isTableAvailable(input_table)) {
  val tableDesc = new HTableDescriptor(input_table)
  admin.createTable(tableDesc)
}
val hBaseRDD =  sc.newAPIHadoopRDD(conf, classOf[TableInputFormat], classOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable], classOf[org.apache.hadoop.hbase.client.Result])</pre></div></li><li style="list-style-type: disc"><p>Java:</p><div class="informalexample"><pre class="programlisting">import spark.api.java.JavaPairRDD;
import spark.api.java.JavaSparkContext;
import spark.api.java.function.FlatMapFunction;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.HTableDescriptor;
import org.apache.hadoop.hbase.client.HBaseAdmin;
import org.apache.hadoop.hbase.mapreduce.TableInputFormat;
import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
import org.apache.hadoop.hbase.client.Result;
...
JavaSparkContext sc = new JavaSparkContext(args[0], "sequence load", System.getenv("SPARK_HOME"), System.getenv("JARS"));
Configuration conf = HBaseConfiguration.create();
conf.set(TableInputFormat.INPUT_TABLE, args[1]);
// Initialize hBase table if necessary
HBaseAdmin admin = new HBaseAdmin(conf);
if(!admin.isTableAvailable(args[1])) {
    HTableDescriptor tableDesc = new HTableDescriptor(args[1]);
    admin.createTable(tableDesc);
}
JavaPairRDD&lt;ImmutableBytesWritable, Result&gt; hBaseRDD = sc.newAPIHadoopRDD( conf, TableInputFormat.class, ImmutableBytesWritable.class, Result.class);</pre></div></li></ul></div><p>The method that <a id="id151" class="indexterm"></a>you used to load the HBase data can be generalized for loading <a id="id152" class="indexterm"></a>all other sorts of Hadoop data. If a helper method in <code class="literal">SparkContext</code> does not already exist for loading the data, simply create a configuration specifying how to load the data and pass it into a new <code class="literal">APIHadoopRDD</code> function. Helper methods exist for plain text files and sequence files. A helper method also exists for Hadoop files similar to the Sequence file API.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec35"></a>Saving your data</h2></div></div><hr /></div><p>While distributed computational <a id="id153" class="indexterm"></a>jobs are a lot of fun, they are much more useful when the results are stored in a useful place. While the methods for loading an RDD are largely found in the <code class="literal">SparkContext</code> class, the methods for saving an RDD are defined on the RDD classes. In Scala, implicit conversions exist so that an RDD, that can be saved as a sequence file, is converted to the appropriate type, and in Java explicit conversion must be used.</p><p>Here are the different ways to save an RDD:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>For Scala:</p><div class="informalexample"><pre class="programlisting">rddOfStrings.saveAsTextFile("out.txt")
keyValueRdd.saveAsObjectFile("sequenceOut")</pre></div></li><li style="list-style-type: disc"><p>For Java:</p><div class="informalexample"><pre class="programlisting">rddOfStrings.saveAsTextFile("out.txt")
keyValueRdd.saveAsObjectFile("sequenceOut")</pre></div></li><li style="list-style-type: disc"><p>For Python:</p><div class="informalexample"><pre class="programlisting">rddOfStrings.saveAsTextFile("out.txt")</pre></div></li></ul></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip27"></a>Tip</h3><p>In addition, users can save the RDD as a compressed text file by using the following function:</p><div class="informalexample"><pre class="programlisting">
<code class="literal">saveAsTextFile(path: String, codec: Class[_ &lt;: CompressionCodec])</code>
</pre></div></div><p>Some references are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><a class="ulink" href="http://spark-project.org/docs/latest/scala-programming-guide.html#hadoop-datasets" target="_blank">http://spark-project.org/docs/latest/scala-programming-guide.html#hadoop-datasets</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://opencsv.sourceforge.net/" target="_blank">http://opencsv.sourceforge.net/</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://commons.apache.org/proper/commons-csv/" target="_blank">http://commons.apache.org/proper/commons-csv/</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/SequenceFileInputFormat.html" target="_blank">http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/SequenceFileInputFormat.html</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/InputFormat.html" target="_blank">http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/InputFormat.html</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/" target="_blank">http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://spark.apache.org/docs/latest/api/python/" target="_blank">http://spark.apache.org/docs/latest/api/python/</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://wiki.apache.org/hadoop/SequenceFile" target="_blank">http://wiki.apache.org/hadoop/SequenceFile</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://hbase.apache.org/book/quickstart.html" target="_blank">http://hbase.apache.org/book/quickstart.html</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/TableInputFormat.html" target="_blank">http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/TableInputFormat.html</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="https://spark.apache.org/docs/latest/api/java/org/apache/spark/api/java/JavaPairRDD.html" target="_blank">https://spark.apache.org/docs/latest/api/java/org/apache/spark/api/java/JavaPairRDD.html</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="https://bzhangusc.wordpress.com/2014/06/18/csv-parser/" target="_blank">https://bzhangusc.wordpress.com/2014/06/18/csv-parser/</a></p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec36"></a>Summary</h2></div></div><hr /></div><p>In this chapter, you saw how to load data from a variety of different sources. We also looked at basic parsing of the data from text input files. Now that we can get our data loaded into a Spark RDD, it is time to explore the different operations we can perform on our data in the next chapter.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch06"></a>Chapter 6. Manipulating your RDD</h2></div></div></div><p>The last few chapters have been the necessary groundwork to get Spark working. Now that you know how to load and save your data in different ways, it's time for the big payoff, that is, manipulating data. The API to manipulate your RDD is similar among the languages but not identical. Unlike the previous chapters, each language is covered in its own section; you likely only need to read the one pertaining to the language you are interested in using. Particularly, the Python implementation is currently not fully at feature parity with the Scala/Java API, but it supports most of the basic functionality as of version 1.1.0 and has plans to improve feature parity in the future versions.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec37"></a>Manipulating your RDD in Scala and Java</h2></div></div><hr /></div><p>RDDs are the primary<a id="id154" class="indexterm"></a> abstraction in Spark. From a structural view, they are<a id="id155" class="indexterm"></a> just a bunch of elements—but elements that can be <a id="id156" class="indexterm"></a>operated in parallel!</p><p>Manipulating your RDD in <a id="id157" class="indexterm"></a>Scala is quite simple, especially if you are familiar with Scala's collection library. Many of the standard functions are available directly on Spark's RDDs with the primary catch being that they are immutable. This makes porting existing Scala code to be distributed much simpler than porting Java or Python code. At least in theory, this is true. While Scala encourages functional programming, one can always use Scala in a non-functional way. Vice versa, while using Python, one can, to a large extent, apply a functional approach to programming. In other words, the difference lies in whether it is the functional/immutable style of programming or not, and the programs written in a functional way can be ported to Spark easily.</p><p>Manipulating your RDD in Java is fairly simple but a little more awkward at times than it is in Scala. There are a couple of reasons for this. The main reason has to do with <span class="strong"><strong>type inference</strong></span><a id="id158" class="indexterm"></a> and also with the fact that Java doesn't have anonymous functions. In the following code snippets, sometimes the Java code is more unwieldy because Java lacks type inference and anonymous functions. Java 8 has <a id="id159" class="indexterm"></a>
<span class="strong"><strong>lambda</strong></span>, which would make Java a lot more elegant with Spark. Secondly, as Java doesn't have implicit conversions, we have to be more explicit with our types. While the return types are Java friendly, Spark requires the use of Scala's <code class="literal">Tuple2</code> class for key-value pairs.</p><p>The hallmark of a MapReduce system are the two primitives: <span class="strong"><strong>map</strong></span><a id="id160" class="indexterm"></a> and <a id="id161" class="indexterm"></a>
<span class="strong"><strong>reduce</strong></span>. We've seen the map function used in the past chapters. Map works by taking in a function, which acts on each individual element in the input RDD and produces a new output element. For example, to produce a new RDD where you add one to every number, use <code class="literal">rdd.map(x =&gt; x+1)</code>.</p><p>Alternatively, in <a id="id162" class="indexterm"></a>Java, you <a id="id163" class="indexterm"></a>can use this:</p><div class="informalexample"><pre class="programlisting">rdd.map(new Function&lt;Integer, Integer&gt;() { public Integer call(Integer x) { return x+1;} });</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note04"></a>Note</h3><p>There are actually two types of map function—<code class="literal">map</code> and <code class="literal">flatMap</code>. It is easy to get confused between them. The <code class="literal">map</code> function<a id="id164" class="indexterm"></a> takes an element and returns an element. The element could be a single entity, a tuple, or a list; nevertheless, there is a one-to-one correspondence with the <code class="literal">map</code> function. The <code class="literal">flatMap</code> function, on the other hand, takes one element and will return one or more elements. Actually, the <code class="literal">map</code> in Hadoop MapReduce is <code class="literal">flatMap</code>. In fact, the Spark word count example is implemented using the <code class="literal">flatMap()</code>, <code class="literal">map()</code>, and <code class="literal">reduceByKey()</code> functions.</p></div><p>It is important to <a id="id165" class="indexterm"></a>understand that the <code class="literal">map</code> function and the other Spark functions <a id="id166" class="indexterm"></a>do not modify/update the existing elements; rather, they return a new RDD with new elements—the RDDs are immutable. The <code class="literal">reduce</code> function takes a function that operates on pairs to combine all the data. The <code class="literal">reduce</code> function you provide needs to be commutative and associative (that is, <span class="emphasis"><em>f(a,b) == f(b,a)</em></span>, and <span class="emphasis"><em>f(a,f(b,c)) == f(f(a,b),c</em></span>). For example, to sum all of the elements, you need to use <code class="literal">rdd.reduce(x,y =&gt; x+y)</code> or <code class="literal">rdd.reduce(new Function2&lt;Integer, Integer&gt;(){ public Integer call(Integer x, Integer y) { return x+y;} }</code>.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note05"></a>Note</h3><p>All functions are not commutative; for example, while multiplication is commutative 2*3 = 3*2, subtraction is not, that is, 3-2 is not the same as 2-3, and division is not, that is, 4/2 is not the same as 2/4. The same applies for associativity; sum is associative, that is, 2+3+4 = (2+3)+4 or 2+(3+4), but average is not, that is, average (2,3,4,5,6) is not equal to average (2,3) + average (4,5,6).</p></div><p>The <code class="literal">flatMap</code> function<a id="id167" class="indexterm"></a> is a useful utility function that lets you write a function that returns an iterable of the type you want and then flattens the results. A simple example of this is a case where you want to parse all of the data, but some of it might fail to be parsed. The <code class="literal">flatMap</code> function can be used to output an empty list if it has failed or a list with success if it has worked. Another example when the output collection has a different size than the input collection is while parsing a document and splitting in words; here every line may contain one or more words.</p><p>In addition to the <code class="literal">reduce</code> function, there is a corresponding <code class="literal">reduceByKey</code> function that works on RDDs, which are key-value pairs to produce another RDD. Unlike when you're using map on a list in Scala, your function will run on a number of different machines, and so you can't depend on the shared state with this.</p><p>Before <a id="id168" class="indexterm"></a>continuing into other wonderful functions for manipulating your RDD, you <a id="id169" class="indexterm"></a>need to read a bit about shared states. In the<a id="id170" class="indexterm"></a> example given earlier where we added one to every <a id="id171" class="indexterm"></a>integer, we didn't really share states. However, for even simple tasks such as distributed parsing of data that we did when loading the CSV file, it can be quite handy to have shared counters for things such as keeping track of the number of rejected records. Spark supports both shared immutable data, which it calls <span class="strong"><strong>broadcast</strong></span><a id="id172" class="indexterm"></a> and <span class="strong"><strong>accumulate</strong></span><a id="id173" class="indexterm"></a> (via accumulators):</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>You can create a new broadcast by calling <code class="literal">sc.broadcast(value)</code>. While you don't have to explicitly broadcast values as Spark does its magic in the background, broadcasting ensures that the value is sent to each node only once. Broadcasts are often used for things such as side inputs (for example, a hashmap that you need to look up as part of the <code class="literal">map</code> function). This returns an object that can be used to reference the broadcast value.</p></li><li style="list-style-type: disc"><p>Another method for sharding states is using an accumulator. To create an accumulator, use <code class="literal">sc.accumulator(initialvalue</code>). This returns an object you can add to in a distributed context and then get back the value by calling <code class="literal">.value()</code>. The <code class="literal">accumulableCollection</code> can be used to create a collection that is appended in a distributed fashion; however, if you find yourself using this, ask yourself whether you could use the results of a map output instead. If the predefined accumulators don't work for your use case, you can use <code class="literal">accumulable</code> to define your own accumulation type. A broadcast value can be read by all of the workers and an accumulator can be written by all of the workers but read by only the driver.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip28"></a>Tip</h3><p>If you are writing Scala code that interacts with a Java Spark process (say for testing), you may find it useful to use the <code class="literal">int</code> accumulator and similar others on the Java Spark context; otherwise, your accumulator types might not quite match up.</p><p>If you find that your accumulator isn't increasing in value like you expect, remember that Spark follows the principle of lazy evaluation. This means that Spark won't actually perform the maps, reductions, or other computation on RDDs until the data has to be output.</p></div></li></ul></div><p>Look at the previous example, which parsed CSV files, and make it a bit more robust. In your previous work, you had assumed that the input was well formatted and if any errors occur, our entire pipeline would fail. While this can be the correct behavior for some kind of work, we may want to accept some number of malformed records while dealing with data from third parties. On <a id="id174" class="indexterm"></a>the other hand, we don't want to just throw out all of the records and declare it a success; we might miss an important format change<a id="id175" class="indexterm"></a> and produce meaningless<a id="id176" class="indexterm"></a> results. Consider <a id="id177" class="indexterm"></a>the following code:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkFiles;

import au.com.bytecode.opencsv.CSVReader

import java.io.StringReader

object LoadCsvWithCountersExample {
  def main(args: Array[String]) {
    if (args.length != 2) {
      System.err.println("Usage: LoadCsvExample &lt;master&gt; &lt;inputfile&gt;")
      System.exit(1)
    }
    val master = args(0)
    val inputFile = args(1)
    val sc = new SparkContext(master, "Load CSV With Counters Example",
                 System.getenv("SPARK_HOME"),
                 Seq(System.getenv("JARS")))
    val invalidLineCounter = sc.accumulator(0)
    val invalidNumericLineCounter = sc.accumulator(0)
    sc.addFile(inputFile)
    val inFile = sc.textFile(inputFile)
    val splitLines = inFile.flatMap(line =&gt; {
      try {
    val reader = new CSVReader(new StringReader(line))
      Some(reader.readNext())
      } catch {
    case _ =&gt; {
      invalidLineCounter += 1
      None
    }
      }
    }
            )
    val numericData = splitLines.flatMap(line =&gt; {
      try {
    Some(line.map(_.toDouble))
      } catch {
    case _ =&gt; {
      invalidNumericLineCounter += 1
      None
    }
      }
    }
    )
    val summedData = numericData.map(row =&gt; row.sum)
    println(summedData.collect().mkString(","))
    println("Errors: "+invalidLineCounter+","+invalidNumericLineCounter)
  }
}</pre></div><p>You can run the<a id="id178" class="indexterm"></a> code with parameters <code class="literal">local/path/Line_of_numbers.csv</code> and the <a id="id179" class="indexterm"></a>code will run with the<a id="id180" class="indexterm"></a> following<a id="id181" class="indexterm"></a> result:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[..]</strong></span>
<span class="strong"><strong>2014-11-22 18:15:48,399 INFO  [main] spark.SparkContext (Logging.scala:logInfo(59)) - Job finished: collect at LoadCsvWithCountersExample.scala:47, took 0.256383 s</strong></span>
<span class="strong"><strong>766.0</strong></span>
<span class="strong"><strong>Errors: 0,0</strong></span>
</pre></div><p>Alternatively, in Java you can do the following:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark.Accumulator;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.FlatMapFunction;

import au.com.bytecode.opencsv.CSVReader;

import java.io.StringReader;
import java.util.Arrays;
import java.util.List;
import java.util.ArrayList;

public class JavaLoadCsvCounters {
  public static void main(String[] args) throws Exception {
    if (args.length != 2) {
    System.err.println("Usage: JavaLoadCsvCounters &lt;master&gt; &lt;inputfile&gt;");
    System.exit(1);
    }
    String master = args[0];
    String inputFile = args[1];
    JavaSparkContext sc = new JavaSparkContext(master, "java load csv with counters",
        System.getenv("SPARK_HOME"), System.getenv("JARS"));
    final Accumulator&lt;Integer&gt; errors = sc.accumulator(0);
    JavaRDD&lt;String&gt; inFile = sc.textFile(inputFile);
    JavaRDD&lt;Integer[] &gt; splitLines = inFile.flatMap(new FlatMapFunction&lt;String, Integer[]&gt; (){
        public Iterable&lt;Integer[]&gt; call(String line) {
        ArrayList&lt;Integer[]&gt; result = new ArrayList&lt;Integer[]&gt;();
        try {
            CSVReader reader = new CSVReader(new StringReader(line));
              String[] parsedLine = reader.readNext();
              Integer[] intLine = new Integer[parsedLine.length];
              for (int i = 0; i &lt; parsedLine.length; i++) {
                intLine[i] = Integer.parseInt(parsedLine[i]);
             }
              result.add(intLine);
         } catch (Exception e) {
              errors.add(1);
         }
           return result;
         }
    }
    );
    List &lt;Integer[]&gt; res = splitLines.collect();
    System.out.print("Loaded data ");
    for (Integer[] e : res) {
      for (Integer val:e) {
        System.out.print(val+" ");
      }
      System.out.println();
    }
    System.out.println("Error count "+errors.value());
  }
}</pre></div><p>You can<a id="id182" class="indexterm"></a> run the <a id="id183" class="indexterm"></a>code with<a id="id184" class="indexterm"></a> parameters <code class="literal">local/path/Line_of_numbers.csv</code> and the code will run with the following result:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[..]</strong></span>
<span class="strong"><strong>14/11/22 19:33:05 INFO SparkContext: Job finished: collect at JavaLoadCsvCounters.java:44, took 0.106908 s</strong></span>
<span class="strong"><strong>Loaded data 42 42 55 61 53 49 43 47 49 60 68 54 34 35 35 39 </strong></span>
<span class="strong"><strong>Error count 0</strong></span>
</pre></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip29"></a>Tip</h3><p>The preceding code example illustrates the usefulness of <code class="literal">flatMap</code>. In general, <code class="literal">flatMap</code> can be used when the required output collection is of a different size than that of the input collection. You can do this because in general there are nested collections or types involved, which need to be flattened. As the options in Scala can be used as sequences through an implicit conversion, you can avoid having to explicitly filter out the <code class="literal">None</code> result and just use <code class="literal">flatMap</code>.</p></div><p>Summary<a id="id185" class="indexterm"></a> statistics can be quite useful when examining large datasets. In the preceding example, you loaded the data as <code class="literal">Doubles</code> to use Spark's provided summary statistics capabilities on the RDD. In Java, this requires explicitly using the <code class="literal">JavaDoubleRDD</code> type. For Java, it is important to use <code class="literal">DoubleFunction&lt;Integer[]&gt;</code> rather than <code class="literal">Function&lt;Integer[], Double&gt;</code> in the example as the second option won't result in the <code class="literal">JavaDoubleRDD</code> type. No such consideration is required for Scala as implicit conversions deal with the details. Compute the mean and the variance or compute them together with stats. You can extend this by adding it at the end of the preceding function to print out the summary statistics as <code class="literal">println(summedData.stats())</code>.</p><p>To do this with Java, we would do it as follows:</p><div class="informalexample"><pre class="programlisting">JavaDoubleRDD summedData = splitLines.map(new DoubleFunction&lt;Integer[]&gt;() {
        public Double call(Integer[] in) {
          Double ret = 0.;
          for (int i = 0; i &lt; in.length; i++) {
            ret += in[i];
          }
          return ret;
        }
}
);
System.out.println(summedData.stats());</pre></div><p>While working with key-value pair data, it can be quite useful to group data with the same key together (for example, if the key represents a user or a sample). The <code class="literal">groupByKey</code> function provides an easy way to group data together by key. The <code class="literal">groupByKey</code> function is a special case of <code class="literal">combineByKey</code>. There are several functions in the <code class="literal">PairRDD</code> class that are all implemented very closely on top of <code class="literal">combineByKey</code>. If you find yourself using <code class="literal">groupByKey</code> or one of the other functions derived from <code class="literal">combineByKey</code> and immediately transforming the result, you should check to see whether there is a function better suited to the task. A common thing to do while starting out is to perform <code class="literal">groupByKey</code> and then sum the results with <code class="literal">groupByKey().map({case (x,y) =&gt; (x,y.sum)})</code>. Alternatively, in Java you can do the following:</p><div class="informalexample"><pre class="programlisting">pairData.groupByKey().mapValues(new Function&lt;List&lt;Integer&gt;, Integer &gt;(){
        public Integer call(List&lt;Integer&gt; x) {
          Integer sum = 0;
          for (Integer i : x) {
            sum += i;
          }
          return sum;
        }
}
); or in python .map(lambda (x,y): (x,sum(y))).collect()</pre></div><p>By<a id="id186" class="indexterm"></a> using <code class="literal">reduceByKey</code>, it could be simplified to <code class="literal">reduceByKey((x,y) =&gt; x+y)</code> or in Java, as follows:</p><div class="informalexample"><pre class="programlisting">pairData.groupByKey().mapValues(
  new Function&lt;Iterable&lt;Integer&gt;, Integer &gt;(){
    public Integer call(Iterable&lt;Integer&gt; x) {
      Integer sum = 0; for (Integer i : x) {
        sum += i;
      }
    return sum;
    }
  }
);</pre></div><p>In fact, this may be <a id="id187" class="indexterm"></a>much more efficient. No big shuffle is needed, as<a id="id188" class="indexterm"></a> is the case for the <code class="literal">groupBy</code>. The only thing <a id="id189" class="indexterm"></a>required is an aggregation of the values, which is important.</p><p>The <code class="literal">foldByKey(zeroValue)(function)</code> function is similar to a traditional fold operation, which works per key. In a traditional fold, a list that is provided would be called with the initial value and the first element of the list, and then the resulting value and the next element of the list would be the input to the next call of fold. Doing this requires sequentially processing the entire list, and so <code class="literal">foldByKey</code> behaves slightly differently. There is a handy table of functions of PairRDDs at the end of this section.</p><p>Sometimes, you will only want to update the values of a key-value pair data structure such as a PairRDD. You've learned about <code class="literal">foldByKey</code> and how it doesn't quite work as a traditional fold. If you're a Scala developer and you require the "traditional" fold behavior, you can perform the <code class="literal">groupByKey</code> function and then map a fold by value over the resulting RDD. This is an example of a case where you only want to change the value and we don't care about the key of the RDD; so examine the following code:</p><div class="informalexample"><pre class="programlisting">rdd.groupByKey().mapValues(x =&gt; {x.fold(0)((a,b) =&gt; a+b)})</pre></div><p>The preceding code is interesting as it combines the Spark function <code class="literal">groupByKey</code> with a Scala function <code class="literal">fold()</code>.The <code class="literal">groupBy()</code> function shuffles the data so that the values are "together".  The fold mentioned is a "local" Scala fold on the nodes in parallel.</p><p>Often your data won't come in cleanly from a single source and you will want to join the data together for processing, which can be done with <code class="literal">coGroup</code>. This can be done when you are joining web access logs with transaction data or just joining two different computations on the same data. Provided that the RDDs have the same key, we can join two RDDs together with <code class="literal">rdd.coGroup(otherRdd)</code>. There are a number of different join functions for different purposes illustrated in the table at the end of this section.</p><p>The next <a id="id190" class="indexterm"></a>task you will learn is distributing files among the cluster. We<a id="id191" class="indexterm"></a> illustrate this by adding GeoIP support and mixing<a id="id192" class="indexterm"></a> it together with the gradient descent example <a id="id193" class="indexterm"></a>from the earlier chapter. Sometimes, the libraries you will use need files distributed along with them. While it is possible to add them to the JAR and access them as class objects, Spark provides a simple way to distribute the required files by calling <code class="literal">addFile()</code>, as shown here:</p><div class="informalexample"><pre class="programlisting">import scala.math

import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkFiles;
import org.apache.spark.util.Vector

import au.com.bytecode.opencsv.CSVReader

import java.util.Random
import java.io.StringReader
import java.io.File

import com.snowplowanalytics.maxmind.geoip.IpGeo

case class DataPoint(x: Vector, y: Double)

object GeoIpExample {
  
  def main(args: Array[String]) {
    if (args.length != 2) {
      System.err.println("Usage: GeoIpExample &lt;master&gt; &lt;inputfile&gt;")
      System.exit(1)
    }
    val master = args(0)
    val inputFile = args(1)
    val iterations = 100
    val maxMindPath = "GeoLiteCity.dat"
    val sc = new SparkContext(master, "GeoIpExample",
                 System.getenv("SPARK_HOME"),
                 Seq(System.getenv("JARS")))
    val invalidLineCounter = sc.accumulator(0)
    val inFile = sc.textFile(inputFile)
    val parsedInput = inFile.flatMap(line =&gt; {
      try {
        val row = (new CSVReader(new StringReader(line))).readNext()
          Some((row(0),row.drop(1).map(_.toDouble)))
      } catch {
        case _ =&gt; {
        invalidLineCounter += 1
        None}
      }
    })
    val geoFile = sc.addFile(maxMindPath)
    // getLocation gives back an option so we use flatMap to only output if its a some type
    val ipCountries = parsedInput.flatMapWith(_ =&gt; IpGeo(dbFile = SparkFiles.get(maxMindPath) ))((pair, ipGeo) =&gt; {
     ipGeo.getLocation(pair._1).map(c =&gt; (pair._1, c.countryCode)).toSeq
     })
    ipCountries.cache()
    val countries = ipCountries.values.distinct().collect()
    val countriesBc = sc.broadcast(countries)
    val countriesSignal = ipCountries.mapValues(country =&gt; countriesBc.value.map(s =&gt; if (country == s) 1. else 0.))
    val dataPoints = parsedInput.join(countriesSignal).map(input =&gt; {
      input._2 match {
    case (countryData, originalData) =&gt; DataPoint(new Vector(countryData++originalData.slice(1,originalData.size-2)) , originalData(originalData.size-1))
      }
    })
    countriesSignal.cache()
    dataPoints.cache()
    val rand = new Random(53)
    var w = Vector(dataPoints.first.x.length, _ =&gt; rand.nextDouble)
    for (i &lt;- 1 to iterations) {
      val gradient = dataPoints.map(p =&gt;
    (1 / (1 + math.exp(-p.y*(w dot p.x))) - 1) * p.y * p.x).reduce(_ + _)
      w -= gradient
    }
    println("Final w: "+w)
  }
}</pre></div><p>In this <a id="id194" class="indexterm"></a>example, you see multiple Spark computations. The <a id="id195" class="indexterm"></a>first is to determine all of the countries where our data<a id="id196" class="indexterm"></a> is; so we can map the country to a binary feature. The <a id="id197" class="indexterm"></a>code then uses a public list of proxies and the reported latency to try and estimate the latency I measured. This also illustrates the use of <code class="literal">mapWith</code>. If you have a mapping job that needs to create a per partition resource, <code class="literal">mapWith</code> can be used to do this. This can be useful for connections to backends or the creation of something like a PRNG. Some elements also can't be serialized over the wire (such as the <code class="literal">IpCountry</code> in the example), and so you have to create them per shard. You can also see that we cache a number of our RDDs to keep them from having to be recomputed.</p><p>There are several options when working with multiple RDDs.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec14"></a>Scala RDD functions</h3></div></div></div><p>These are PairRDD functions <a id="id198" class="indexterm"></a>based on <code class="literal">combineByKey</code>. All operate on RDDs of type <code class="literal">[K,V]</code>:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Function</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Param options</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Explanation</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Return type</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p><code class="literal">foldByKey</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">(zeroValue)(func(V,V)=&gt;V)
(zeroValue, partitioner)(func(V,V=&gt;V)
(zeroValue, partitions)(func(V,V=&gt;V)</pre></div>
</td><td style="" align="left" valign="top">
<p><code class="literal">foldByKey</code> merges<a id="id199" class="indexterm"></a> the values using the provided function. Unlike a traditional fold over a list, the <code class="literal">zeroValue</code> can be added an arbitrary number of times.</p>
</td><td style="" align="left" valign="top">
<p>RDD[K,V]</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">reduceByKey</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">(func(V,V)=&gt;V)
(func(V,V)=&gt;V,numTasks)</pre></div>
</td><td style="" align="left" valign="top">
<p><code class="literal">reduceByKey</code> is the parallel <a id="id200" class="indexterm"></a>version of reduce that merges the values for each key using the provided function and returns an RDD.</p>
</td><td style="" align="left" valign="top">
<p>RDD[K,V]</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">groupByKey</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">()
(numPartitions)</pre></div>
</td><td style="" align="left" valign="top">
<p>This <a id="id201" class="indexterm"></a>groups elements together by key.</p>
</td><td style="" align="left" valign="top">
<p>RDD[K,Seq[V]]</p>
</td></tr></tbody></table></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec15"></a>Functions for joining PairRDDs</h3></div></div></div><p>Often while working <a id="id202" class="indexterm"></a>with two or more key-value RDDs, it is useful to join them together. There are a few different methods to do this depending on what your desired behavior is:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Function</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Param options</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Explanation</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Return type</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p><code class="literal">coGroup</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">(otherRdd[K,W]...)</pre></div>
</td><td style="" align="left" valign="top">
<p>Join <a id="id203" class="indexterm"></a>two (or more) RDDs by the shared key. Note if an element is missing in one RDD but present in the other one, the <code class="literal">Seq</code> value will simply be empty.</p>
</td><td style="" align="left" valign="top">
<p>RDD[(K,(Seq[V],Seq[W]...))]</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">join</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">(otherRdd[K,W])
(otherRdd[K,W], partitioner)
(otherRdd[K,W], numPartitions)</pre></div>
</td><td style="" align="left" valign="top">
<p>Join an <a id="id204" class="indexterm"></a>RDD with another RDD. The result is only present for elements where the key is present in both RDDs.</p>
</td><td style="" align="left" valign="top">
<p>RDD[(K,(V,W))]</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">subtractKey</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">(otherRdd[K,W])
(otherRdd[K,W], partitioner)
(otherRdd[K,W], numPartitions)</pre></div>
</td><td style="" align="left" valign="top">
<p>This <a id="id205" class="indexterm"></a>returns an RDD with only keys not present in the other RDD.</p>
</td><td style="" align="left" valign="top">
<p>RDD[(K,V)]</p>
</td></tr></tbody></table></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec16"></a>Other PairRDD functions</h3></div></div></div><p>Some functions only make <a id="id206" class="indexterm"></a>sense when working on key-value pairs, as follows:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Function</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Param options</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Explanation</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Return type</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p><code class="literal">lookup</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">(key: K)</pre></div>
</td><td style="" align="left" valign="top">
<p>This <a id="id207" class="indexterm"></a>looks up a specific element in the RDD. It uses the RDD's partitioner to figure out which shard(s) to look at.</p>
</td><td style="" align="left" valign="top">
<p>Seq[V]</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">mapValues</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">(f: V =&gt; U)</pre></div>
</td><td style="" align="left" valign="top">
<p>This is a <a id="id208" class="indexterm"></a>specialized version of map for PairRDDs when you only want to change the value of the key-value pair. It takes the provided <code class="literal">map</code> function and applies it to the value. If you need to make your change based on both key and value, you must use one of the normal RDD map functions.</p>
</td><td style="" align="left" valign="top">
<p>RDD[(K,U)]</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">collectAsMap</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">()</pre></div>
</td><td style="" align="left" valign="top">
<p>This <a id="id209" class="indexterm"></a>takes an RDD and returns a concrete map. Your RDD must be able to fit in memory.</p>
</td><td style="" align="left" valign="top">
<p>Map[K, V]</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">countByKey</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">()</pre></div>
</td><td style="" align="left" valign="top">
<p>This <a id="id210" class="indexterm"></a>counts the number of elements for each key.</p>
</td><td style="" align="left" valign="top">
<p>Map[K, Long]</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">partitionBy</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">(partitioner: Partitioner, mapSideCombine: Boolean)</pre></div>
</td><td style="" align="left" valign="top">
<p>This<a id="id211" class="indexterm"></a> returns a new RDD with the same data but partitioned by the new partitioner. The Boolean flag <code class="literal">mapSideCombine</code> controls whether Spark should group values with the same key together before repartitioning. It defaults to <code class="literal">false</code> and sets to <code class="literal">true</code> if you have a large percentage of duplicate keys.</p>
</td><td style="" align="left" valign="top">
<p>RDD[(K,V)]</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">flatMapValues</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">(f: V =&gt; TraversableOnce[U])</pre></div>
</td><td style="" align="left" valign="top">
<p>This is<a id="id212" class="indexterm"></a> similar to <code class="literal">MapValues</code>. It's a specialized version of <code class="literal">flatMap</code> for PairRDDs when you only want to change the value of the key-value pair. It takes the provided <code class="literal">map</code> function and applies it to the value. The resulting sequence is then "flattened", that is, instead of getting <code class="literal">Seq[Seq[V]]</code>, you get <code class="literal">Seq[V]</code>. If you need to make your change based on both key and value, you must use one of the normal RDD map functions.</p>
</td><td style="" align="left" valign="top">
<p>RDD[(K,U)]</p>
</td></tr></tbody></table></div><p>For information on saving PairRDDs, refer to the previous chapter.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec17"></a>Double RDD functions</h3></div></div></div><p>Spark defines a number of<a id="id213" class="indexterm"></a> convenience functions that work when your RDD is comprised of doubles, as follows:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Function</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Arguments</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Return value</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p><code class="literal">Mean</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">()</pre></div>
</td><td style="" align="left" valign="top">
<p>Average</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">sampleStdev</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">()</pre></div>
</td><td style="" align="left" valign="top">
<p>Standard<a id="id214" class="indexterm"></a> deviation for a sample rather than a population (as it divides by <span class="emphasis"><em>N-1</em></span> rather than <span class="emphasis"><em>N</em></span>).</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">Stats</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">()</pre></div>
</td><td style="" align="left" valign="top">
<p>Mean, variance, <a id="id215" class="indexterm"></a>and count as a <code class="literal">StatCounter</code>.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">Stdev</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">()</pre></div>
</td><td style="" align="left" valign="top">
<p>Standard<a id="id216" class="indexterm"></a> deviation (for population).</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">Sum</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">()</pre></div>
</td><td style="" align="left" valign="top">
<p>Sum<a id="id217" class="indexterm"></a> of <a id="id218" class="indexterm"></a>the elements.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">variance</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">()</pre></div>
</td><td style="" align="left" valign="top">
<p>Variance</p>
</td></tr></tbody></table></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec18"></a>General RDD functions</h3></div></div></div><p>The remaining RDD<a id="id219" class="indexterm"></a> functions are defined on all RDDs:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Function</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Arguments</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Returns</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p><code class="literal">aggregate</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">(zero: U)(seqOp: (U,T) =&gt; T, combOp (U, U) =&gt; U)</pre></div>
</td><td style="" align="left" valign="top">
<p>It <a id="id220" class="indexterm"></a>aggregates all of the elements of each partition of an RDD and then combines them using <code class="literal">combOp</code>. The zero value should be neutral (that is 0 for + and 1 for *).</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">cache</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">()</pre></div>
</td><td style="" align="left" valign="top">
<p>It caches an <a id="id221" class="indexterm"></a>RDD reused without re-computing. It's the same as <code class="literal">persist(StorageLevel.MEMORY_ONLY)</code>.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">collect</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">()</pre></div>
</td><td style="" align="left" valign="top">
<p>It<a id="id222" class="indexterm"></a> returns an array of all of the elements in the RDD.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">count</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">()</pre></div>
</td><td style="" align="left" valign="top">
<p>It returns <a id="id223" class="indexterm"></a>the number of elements in an RDD.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">countByValue</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">()</pre></div>
</td><td style="" align="left" valign="top">
<p>It returns a map of<a id="id224" class="indexterm"></a> value to the number of times that value occurs.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">distinct</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">()
(partitions: Int)</pre></div>
</td><td style="" align="left" valign="top">
<p>It returns an<a id="id225" class="indexterm"></a> RDD that contains only distinct elements.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">filter</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">(f: T =&gt; Boolean)</pre></div>
</td><td style="" align="left" valign="top">
<p>It <a id="id226" class="indexterm"></a>returns an RDD that contains only elements matching <code class="literal">f</code>.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">filterWith</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">(constructA: Int =&gt; A )(f: (T, A) =&gt; Boolean)</pre></div>
</td><td style="" align="left" valign="top">
<p>It is <a id="id227" class="indexterm"></a>similar to filter, but <code class="literal">f</code> takes an additional parameter generated by <code class="literal">constructA</code>, which is called per-partition. The original motivation for this came from providing PRNG generation per shard.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">first</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">()</pre></div>
</td><td style="" align="left" valign="top">
<p>It<a id="id228" class="indexterm"></a> returns the "first" element of the RDD.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">flatMap</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">(f: T =&gt; TraversableOnce[U])</pre></div>
</td><td style="" align="left" valign="top">
<p>It<a id="id229" class="indexterm"></a> returns an RDD of type <code class="literal">U</code>.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">fold</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">(zeroValue: T)(op: (T,T) =&gt; T)</pre></div>
</td><td style="" align="left" valign="top">
<p>It <a id="id230" class="indexterm"></a>merges values using the provided operation, first on each partition, and then merges the merged result.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">foreach</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">(f: T =&gt; Unit)</pre></div>
</td><td style="" align="left" valign="top">
<p>It <a id="id231" class="indexterm"></a>applies the function <code class="literal">f</code> to each element.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">groupBy</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">(f: T =&gt; K)
(f: T =&gt; K, p: Partitioner)
(f: T =&gt; K, numPartitions:Int)</pre></div>
</td><td style="" align="left" valign="top">
<p>It<a id="id232" class="indexterm"></a> takes in an RDD and produces a PairRDD of type (K,Seq[T]) using the result of <code class="literal">f</code> for the key for each element.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">keyBy</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">(f: T =&gt; K)
(f: T =&gt; K, p: Partitioner)
(f: T =&gt; K, numPartitions:Int)</pre></div>
</td><td style="" align="left" valign="top">
<p>It is the<a id="id233" class="indexterm"></a> same as <code class="literal">groupBy</code> but does not group results together with duplicate keys. It returns an RDD of (K,T).</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">map</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">(f: T =&gt; U)</pre></div>
</td><td style="" align="left" valign="top">
<p>It<a id="id234" class="indexterm"></a> returns an RDD of the result of applying <code class="literal">f</code> to every element in the input RDD.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">mapPartitions</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">(f: Iterator[T] =&gt; Iterator[U])</pre></div>
</td><td style="" align="left" valign="top">
<p>It is <a id="id235" class="indexterm"></a>similar to <code class="literal">map</code> except that the provided function takes and returns an iterator and is applied to each partition.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">mapPartitionsWithIndex</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">(f: (Int, Iterator[T]) =&gt; Iterator[U], preservePartitions)</pre></div>
</td><td style="" align="left" valign="top">
<p>It is the<a id="id236" class="indexterm"></a> same as <code class="literal">mapPartitions</code> but also provides the index of the original partition.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">mapWith</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">(constructA: Int =&gt; A)(f: (T, A) =&gt; U)</pre></div>
</td><td style="" align="left" valign="top">
<p>It is<a id="id237" class="indexterm"></a> similar to <code class="literal">map</code>, but <code class="literal">f</code> takes an additional parameter generated by <code class="literal">constructA</code>, which is called per-partition. The original motivation for this came from providing PRNG generation per shard.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">persist</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">()
(newLevel: StorageLevel)</pre></div>
</td><td style="" align="left" valign="top">
<p>Sets the <a id="id238" class="indexterm"></a>RDD storage level, which can cause the RDD to be stored after it is computed. Different <code class="literal">StorageLevel</code> values can be seen in <code class="literal">StorageLevel.scala</code> (<code class="literal">NONE</code>, <code class="literal">DISK_ONLY</code>, <code class="literal">MEMORY_ONLY</code>, and <code class="literal">MEMORY_AND_DISK</code> are the common ones).</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">pipe</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">(command: Seq[String])
(command: Seq[String], env: Map[String, String])</pre></div>
</td><td style="" align="left" valign="top">
<p>It <a id="id239" class="indexterm"></a>takes an RDD and calls the specified command with the optional environment. Then, it pipes each element through the command. That results in an RDD of type string.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">sample</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">(withReplacement: Boolean, fraction: Double, seed: Int)</pre></div>
</td><td style="" align="left" valign="top">
<p>It<a id="id240" class="indexterm"></a> returns an RDD of that fraction.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">takeSample</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">(withReplacement: Boolean, num: Int, seed: Int)</pre></div>
</td><td style="" align="left" valign="top">
<p>It returns <a id="id241" class="indexterm"></a>an array of the requested number of elements. It works by over sampling the RDD and then grabbing a subset.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">toDebugString</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">()</pre></div>
</td><td style="" align="left" valign="top">
<p>It's a <a id="id242" class="indexterm"></a>handy function that outputs the recursive deps of the RDD.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">union</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">(other: RDD[T])</pre></div>
</td><td style="" align="left" valign="top">
<p>It's an <a id="id243" class="indexterm"></a>RDD containing elements of both RDDs. Here, duplicates are not removed.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">unpersist</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">()</pre></div>
</td><td style="" align="left" valign="top">
<p>Remove <a id="id244" class="indexterm"></a>all blocks of the RDD from memory/disk if they've persisted.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">zip</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">(other: RDD[U])</pre></div>
</td><td style="" align="left" valign="top">
<p>It is <a id="id245" class="indexterm"></a>important to note that it requires that the RDDs have the same number of partitions and the same size of each partition. It returns an RDD of key-value pairs RDD[T,U].</p>
</td></tr></tbody></table></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec19"></a>Java RDD functions</h3></div></div></div><p>Many of the <a id="id246" class="indexterm"></a>Java RDD functions <a id="id247" class="indexterm"></a>are quite similar to the Scala RDD functions, but the type signatures are somewhat different.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec01"></a>Spark Java function classes</h4></div></div></div><p>For the Java RDD API, we<a id="id248" class="indexterm"></a> need to <a id="id249" class="indexterm"></a>extend one of the provided function classes while implementing our function:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Name</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Params</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Purpose</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p><code class="literal">Function&lt;T,R&gt;</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">R call(T t)</pre></div>
</td><td style="" align="left" valign="top">
<p>It is a <a id="id250" class="indexterm"></a>function that takes something of type <code class="literal">T</code> and returns something of type <code class="literal">R</code>. It is commonly used for maps.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">DoubleFunction&lt;T&gt;</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">Double call(T t)</pre></div>
</td><td style="" align="left" valign="top">
<p>It is the <a id="id251" class="indexterm"></a>same as <code class="literal">Function&lt;T, Double&gt;</code>, but the result of the map-like  call returns a JavaDoubleRDD (for summary statistics).</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">PairFunction&lt;T, K, V&gt;</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">Tuple2&lt;K, V&gt; call(T t)</pre></div>
</td><td style="" align="left" valign="top">
<p>It is a <a id="id252" class="indexterm"></a>function that results in a JavaPairRDD. If you're working on <code class="literal">JavaPairRDD&lt;A,B&gt;</code>, have <code class="literal">T</code> of type <code class="literal">Tuple2&lt;A,B&gt;</code>.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">FlatMapFunction&lt;T, R&gt;</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">Iterable&lt;R&gt; call(T t)</pre></div>
</td><td style="" align="left" valign="top">
<p>It is a <a id="id253" class="indexterm"></a>function for producing a RDD through <code class="literal">flatMap</code>.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">PairFlatMapFunction&lt;T, K, V&gt;</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">Iterable&lt;Tuple2&lt;K, V&gt;&gt; call(T t)</pre></div>
</td><td style="" align="left" valign="top">
<p>It's a <a id="id254" class="indexterm"></a>function that results in a JavaPairRDD. If you're working on <code class="literal">JavaPairRDD&lt;A,B&gt;</code>, have <code class="literal">T</code> of type <code class="literal">Tuple2&lt;A,B&gt;</code>.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">DoubleFlatMapFunction&lt;T&gt;</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">Iterable&lt;Double&gt; call(T t)</pre></div>
</td><td style="" align="left" valign="top">
<p>It is<a id="id255" class="indexterm"></a> the same as <code class="literal">FlatMapFunction&lt;T, Double&gt;</code>, but the result of the map-like call returns a JavaDoubleRDD (for summary statistics).</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">Function2&lt;T1, T2, R&gt;</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">R call(T1 t1, T2 t2)</pre></div>
</td><td style="" align="left" valign="top">
<p>It is a <a id="id256" class="indexterm"></a>function for taking two inputs and returning an output. It is used by fold and similar.</p>
</td></tr></tbody></table></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec02"></a>Common Java RDD functions</h4></div></div></div><p>These RDD functions are<a id="id257" class="indexterm"></a> available regardless of the type of<a id="id258" class="indexterm"></a> RDD.</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Name</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Params</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Purpose</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p><code class="literal">cache</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">()</pre></div>
</td><td style="" align="left" valign="top">
<p>It makes <a id="id259" class="indexterm"></a>an RDD persist in memory.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">coalesce</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">numPartitions: Int</pre></div>
</td><td style="" align="left" valign="top">
<p>It<a id="id260" class="indexterm"></a> returns a new RDD with <code class="literal">numPartitions</code> partitions.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">collect</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">()</pre></div>
</td><td style="" align="left" valign="top">
<p>It <a id="id261" class="indexterm"></a>returns the List representation of the entire RDD.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">count</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">()</pre></div>
</td><td style="" align="left" valign="top">
<p>It <a id="id262" class="indexterm"></a>returns the number of elements.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">countByValue</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">()</pre></div>
</td><td style="" align="left" valign="top">
<p>It returns<a id="id263" class="indexterm"></a> a map of each unique value to the number of times that value shows up.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">distinct</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">()
(Int numPartitions)</pre></div>
</td><td style="" align="left" valign="top">
<p>It is an <a id="id264" class="indexterm"></a>RDD consisting of all of the distinct elements of the RDD, optionally in the provided number of partitions.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">filter</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">(Function&lt;T, Boolean&gt; f)</pre></div>
</td><td style="" align="left" valign="top">
<p>It is an <a id="id265" class="indexterm"></a>RDD consisting only of the elements for which the provided function returns <code class="literal">true</code>.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">first</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">()</pre></div>
</td><td style="" align="left" valign="top">
<p>It is the <a id="id266" class="indexterm"></a>first element of the RDD.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">flatMap</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">(FlatMapFunction&lt;T, U&gt; f)
(DoubleFlatMapFunction&lt;T&gt; f)
(PairFlatMapFunction&lt;T, K, V&gt; f)</pre></div>
</td><td style="" align="left" valign="top">
<p>It is <a id="id267" class="indexterm"></a>an RDD of the specified types (<code class="literal">U</code>, <code class="literal">Double</code> and <code class="literal">Pair&lt;K,V&gt;</code> respectively).</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">fold</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">(T zeroValue, Function2&lt;T, T, T&gt; f)</pre></div>
</td><td style="" align="left" valign="top">
<p>It <a id="id268" class="indexterm"></a>returns the result <code class="literal">T</code>. Each partition is folded individually with the zero value and then the results are folded.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">foreach</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">(VoidFunction&lt;T&gt; f)</pre></div>
</td><td style="" align="left" valign="top">
<p>It <a id="id269" class="indexterm"></a>applies the function to each element in the RDD.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">groupBy</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">(Function&lt;T, K&gt; f)
(Function&lt;T, K&gt; f, Int numPartitions)</pre></div>
</td><td style="" align="left" valign="top">
<p>It returns <a id="id270" class="indexterm"></a>a JavaPairRDD of grouped elements.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">map</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">(DoubleFunction&lt;T&gt; f)
(PairFunction&lt;T, K2, V2&gt; f)
(Function&lt;T, U&gt; f)</pre></div>
</td><td style="" align="left" valign="top">
<p>It<a id="id271" class="indexterm"></a> returns an RDD of an appropriate type for the input function (see previous table) by calling the provided function on each element in the input RDD.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">mapPartitions</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">(DoubleFunction&lt;Iterator&lt;T&gt;&gt; f)
(PairFunction&lt;Iterator&lt;T&gt;, K2, V2&gt; f)
(Function&lt;Iterator&lt;T&gt;, U&gt; f)</pre></div>
</td><td style="" align="left" valign="top">
<p>It is <a id="id272" class="indexterm"></a>similar to map, but the provided function is called per-partition. This can be useful if you have done some setup work that is necessary for each partition.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">reduce</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">(Function2&lt;T, T, T&gt; f)</pre></div>
</td><td style="" align="left" valign="top">
<p>It <a id="id273" class="indexterm"></a>uses the provided function to reduce down all of the elements.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">sample</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">(Boolean withReplacement, Double fraction, Int seed)</pre></div>
</td><td style="" align="left" valign="top">
<p>It returns <a id="id274" class="indexterm"></a>a smaller RDD consisting of only the requested fraction of the data.</p>
</td></tr></tbody></table></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec03"></a>Methods for combining JavaRDDs</h4></div></div></div><p>There are a number of different<a id="id275" class="indexterm"></a> functions that we can use to combine RDDs:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Name</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Params</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Purpose</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p><code class="literal">subtract</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">(JavaRDD&lt;T&gt; other)
(JavaRDD&lt;T&gt; other, Partitioner p)
(JavaRDD&lt;T&gt; other, Int numPartitions)</pre></div>
</td><td style="" align="left" valign="top">
<p>It <a id="id276" class="indexterm"></a>returns an RDD with only the elements initially present in the first RDD and not present in the other RDD.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">union</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">(JavaRDD&lt;T&gt; other)</pre></div>
</td><td style="" align="left" valign="top">
<p>It is the <a id="id277" class="indexterm"></a>union of the two RDDs.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">zip</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">(JavaRDD&lt;U&gt; other)</pre></div>
</td><td style="" align="left" valign="top">
<p>It<a id="id278" class="indexterm"></a> returns an RDD of key-value pairs RDD[T,U].</p>
<p>It is important to note that it requires that the RDDs should have the same number of partitions and the size of each partition.</p>
</td></tr></tbody></table></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec04"></a>Functions on JavaPairRDDs</h4></div></div></div><p>Some functions are only <a id="id279" class="indexterm"></a>defined on key-value PairRDDs:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Name</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Params</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Purpose</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p><code class="literal">cogroup</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">(JavaPairRDD&lt;K, W&gt; other)
(JavaPairRDD&lt;K, W&gt; other, Int numPartitions)
(JavaPairRDD&lt;K, W&gt; other1, JavaPairRDD&lt;K, W&gt; other2)
(JavaPairRDD&lt;K, W&gt; other1, JavaPairRDD&lt;K, W&gt; other2, Int numPartitions)</pre></div>
</td><td style="" align="left" valign="top">
<p>It <a id="id280" class="indexterm"></a>joins two (or more) RDDs by the shared key. Note that if an element is missing in one RDD but present in the other one, the list will simply be empty.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">combineByKey</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">(Function&lt;V, C&gt; createCombiner
Function2&lt;C, V, C&gt; mergeValue,
Function2&lt;C,C,C&gt; mergeCombiners)</pre></div>
</td><td style="" align="left" valign="top">
<p>It's a <a id="id281" class="indexterm"></a>generic function to combine elements by key. The <code class="literal">createCombiner</code> function turns something of type <code class="literal">V</code> into something of type <code class="literal">C</code>. The <code class="literal">mergeValue</code> function adds <code class="literal">V</code> to <code class="literal">C</code> and <code class="literal">mergeCombiners</code> is used to combine two <code class="literal">C</code> values into a single <code class="literal">C</code> value.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">collectAsMap</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">()</pre></div>
</td><td style="" align="left" valign="top">
<p>It<a id="id282" class="indexterm"></a> returns a map of the key-value pairs.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">countByKey</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">()</pre></div>
</td><td style="" align="left" valign="top">
<p>It <a id="id283" class="indexterm"></a>returns a map of the key to the number of elements with that key.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">flatMapValues</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">(Function[T] f, Iterable[V] v)</pre></div>
</td><td style="" align="left" valign="top">
<p>It<a id="id284" class="indexterm"></a> returns an RDD of type <code class="literal">V</code>.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">join</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">(JavaPairRDD&lt;K, W&gt; other)
(JavaPairRDD&lt;K, W&gt; other, Int integers)</pre></div>
</td><td style="" align="left" valign="top">
<p>It<a id="id285" class="indexterm"></a> joins an RDD with another RDD. The result is only present for elements where the key is present in both the RDDs.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">keys</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">()</pre></div>
</td><td style="" align="left" valign="top">
<p>It <a id="id286" class="indexterm"></a>returns an RDD of only the keys.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">lookup</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">(Key k)</pre></div>
</td><td style="" align="left" valign="top">
<p>It <a id="id287" class="indexterm"></a>looks up a specific element in the RDD. It uses the RDD's partitioner to figure out which shard(s) to look at.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">reduceByKey</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">(Function2[V,V,V] f)</pre></div>
</td><td style="" align="left" valign="top">
<p>The <code class="literal">reduceByKey</code> function is <a id="id288" class="indexterm"></a>the parallel version of <code class="literal">reduce</code> that merges the values for each key using the provided function and returns an RDD.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">sortByKey</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">(Comparator[K] comp, Boolean ascending)
(Comparator[K] comp)
(Boolean ascending)</pre></div>
</td><td style="" align="left" valign="top">
<p>It<a id="id289" class="indexterm"></a> sorts the RDD by key; so each partition contains a fixed range.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">values</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">()</pre></div>
</td><td style="" align="left" valign="top">
<p>It returns <a id="id290" class="indexterm"></a>an RDD of only the values.</p>
</td></tr></tbody></table></div></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec38"></a>Manipulating your RDD in Python</h2></div></div><hr /></div><p>Spark has a more <a id="id291" class="indexterm"></a>limited Python API than Java and Scala, but it <a id="id292" class="indexterm"></a>supports for most of the core functionality.</p><p>The hallmark of a MapReduce system are the two commands <code class="literal">map</code> and <code class="literal">reduce</code>. You've seen the <code class="literal">map</code> function used in the past chapters. The <code class="literal">map</code> function works by taking in a function that works on each individual element in the input RDD and produces a new output element. For example, to produce a new RDD where you have added one to every number, you would use <code class="literal">rdd.map(lambda x: x+1)</code>. It's important to understand that the <code class="literal">map</code> function and the other Spark functions, do not transform the existing elements; rather they return a new RDD with new elements. The <code class="literal">reduce</code> function takes a function that operates on pairs to combine all the data. This is returned to the calling program. If you were to sum all of the elements, you would use <code class="literal">rdd.reduce(lambda x, y: x+y)</code>. The <code class="literal">flatMap</code> function is a useful utility function that allows you to write a function that returns an iterable of the type you want and then flattens the results. A simple example of this is a case where you want to parse all of the data, but some of it might fail to parse. The <code class="literal">flatMap</code> function can output an empty list if it has failed or a list with its success if it has worked. In addition to <code class="literal">reduce</code>, there is a corresponding <code class="literal">reduceByKey</code> function that works on RDDs, which are key-value pairs, and produces another RDD.</p><p>Many of the mapping operations are also defined with a partition's variant. In this case, the function you need to provide takes and returns an iterator, which represents all of the data on that partition, thus performing work on a per-partition level. The <code class="literal">mapPartitions(func)</code> function can be quite useful if the operation you need to perform has to do expensive work on each shard/partition. An example of this is establishing a connection to a backend server. Another reason for using <code class="literal">mapPartitions(func)</code> is to do setup work for your <code class="literal">map</code> function that can't be serialized across the network. A good example of this is parsing some expensive side input, as shown here:</p><div class="informalexample"><pre class="programlisting">def f(iterator):
      // Expensive work goes here
     for i in iterator:
          yield per_element_function(i)</pre></div><p>Often, your data can be expressed with key-value mappings. As such, many of the functions defined on Python's RDD class only work if your data is in a key-value mapping. The <code class="literal">mapValues</code> function is used when you only want to update the key-value pair you are working with.</p><p>In addition to performing simple operations on the data, Spark also provides support for broadcast values and accumulators. Broadcast values can be used to broadcast a read-only value to all of the partitions, which can save the need to re-serialize a given value multiple times. Accumulators allow all of the shards to add to the accumulator and the result can then be read on the master. You can create an accumulator by doing <code class="literal">counter = sc.accumulator(initialValue)</code>. If you want customized add behavior, you can also provide an <code class="literal">AccumulatorParam</code> to the accumulator. The return can then be incremented as <code class="literal">counter += x</code> on any of the workers. The resulting value can then be read with <code class="literal">counter.value()</code>. The broadcast value is created with <code class="literal">bc = sc.broadcast(value)</code> and then accessed by <code class="literal">bc.value()</code> on any worker. The accumulator can only be read on the master, and the broadcast value can be read on all of the shards.</p><p>Let's look at a quick<a id="id293" class="indexterm"></a> Python example that shows multiple RDD <a id="id294" class="indexterm"></a>operations. We have two text files <code class="literal">2009-2014-BO.txt</code> and <code class="literal">1861-1864-AL.txt</code>. These are the <span class="emphasis"><em>State Of the Union</em></span> speeches by Presidents Barack Obama and Abraham Lincoln. We want to compare the mood of the nation by comparing the salient difference in the words used.</p><p>The first step is reading the files and creating the word frequency vector, that is, each word and the number of times it is used in the speech. I am sure you would recognize this as a canonical word count MapReduce example and, in traditional Hadoop Map Reduce, it takes around 100 lines of code. In Spark, as we shall see, it takes only 5 lines of code:</p><div class="informalexample"><pre class="programlisting">from pyspark.context import SparkContext
print "Running Spark Version %s" % (sc.version)
from pyspark.conf import SparkConf
conf = SparkConf()
print conf.toDebugString()</pre></div><p>The MapReduce code is shown here:</p><div class="informalexample"><pre class="programlisting">from operator import add
lines = sc.textFile("sotu/2009-2014-BO.txt")
word_count_bo = lines.flatMap(lambda x: x.split(' ')).\
    map(lambda x: (x.lower().rstrip().lstrip().rstrip(',').rstrip('.'), 1)).\
    reduceByKey(add)
word_count_bo.count()
#6658 without lower, 6299 with lower, rstrip,lstrip 4835 
lines = sc.textFile("sotu/1861-1864-AL.txt")
word_count_al = lines.flatMap(lambda x: x.split(' ')).map(lambda x: (x.lower().rstrip().lstrip().rstrip(',').rstrip('.'), 1)).reduceByKey(add)
word_count_al.count()</pre></div><p>Sorting an RDD by any column is very easy as shown next:</p><div class="informalexample"><pre class="programlisting">word_count_bo_1 = word_count_bo.sortBy(lambda x: x[1],ascending=False)</pre></div><p>We can collect the word vector. But don't print it! It is a long list:</p><div class="informalexample"><pre class="programlisting">for x in word_count_bo_1.take(10):
    print x</pre></div><p>Now, let's take out common words, as shown here:</p><div class="informalexample"><pre class="programlisting">common_words = ["us","has","all", "they", "from", "who","what","on","by","more","as","not","their","can","new","it","but","be","are","--","i","have","this","will","for","with","is","that","in","our","we","a","of","to","and","the","that's","or","make","do","you","at","it\'s","than","if","know","last","about","no","just","now","an","because","&lt;p&gt;we","why","we\'ll","how","two","also","every","come","we've","year","over","get","take","one","them","we\'re","need","want","when","like","most","-","been","first","where","so","these","they\'re","good","would","there","should","--&gt;","&lt;!--","up","i\'m","his","their","which","may","were","such","some","those","was","here","she","he","its","her","his","don\'t","i\'ve","what\'s","didn\'t","shouldn\'t","(applause.)","let\'s","doesn\'t"]</pre></div><p>Filtering out common<a id="id295" class="indexterm"></a> words is also a single filter operation. Of course, as<a id="id296" class="indexterm"></a> RDDs are immutable, we would create a new filtered RDD:</p><div class="informalexample"><pre class="programlisting">word_count_bo_clean = word_count_bo_1.filter(lambda x: x[0] not in common_words)
word_count_al_clean = word_count_al.filter(lambda x: x[0] not in common_words)</pre></div><p>Finding the words that were spoken by Obama but not by Lincoln, is a single RDD operation. You need to use <code class="literal">subractByKey</code> and then use <code class="literal">sortBy</code> on the count to see the different but most frequent words, as shown here:</p><div class="informalexample"><pre class="programlisting">for x in word_count_bo_clean.subtractByKey(word_count_al_clean).sortBy(lambda x: x[1],ascending=False).take(15): #collect():
    print x</pre></div><p>The preceding program should give you a good grip on the RDD functions and how to use them in Python.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec20"></a>Standard RDD functions</h3></div></div></div><p>These functions are <a id="id297" class="indexterm"></a>available on all RDDs in Python:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Name</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Params</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Purpose</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p><code class="literal">flatMap</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">f, preservesPartitioning=False</pre></div>
</td><td style="" align="left" valign="top">
<p>It takes a <a id="id298" class="indexterm"></a>function that returns an iterator of type <code class="literal">U</code> for each input of type <code class="literal">T</code> and returns a flattened RDD of type <code class="literal">U</code>.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">mapParitions</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">f, preservesPartitioning=False</pre></div>
</td><td style="" align="left" valign="top">
<p>It takes a <a id="id299" class="indexterm"></a>function that takes in an iterator of type <code class="literal">T</code> and returns an iterator of type <code class="literal">U</code>, which then results in an RDD of type <code class="literal">U</code>. It's useful for map operations with expensive per machine setup work.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">filter</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">f</pre></div>
</td><td style="" align="left" valign="top">
<p>It takes<a id="id300" class="indexterm"></a> a function and returns an RDD with only the elements for which the function returns <code class="literal">true</code>.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">distinct</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">()</pre></div>
</td><td style="" align="left" valign="top">
<p>It returns <a id="id301" class="indexterm"></a>an RDD with distinct elements (for example, 1, 1, 2 gives the output as 1, 2).</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">union</code></p>
</td><td style="" align="left" valign="top">
<p>other</p>
</td><td style="" align="left" valign="top">
<p>It returns <a id="id302" class="indexterm"></a>a union of two RDDs.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">cartesian</code></p>
</td><td style="" align="left" valign="top">
<p>other</p>
</td><td style="" align="left" valign="top">
<p>It returns<a id="id303" class="indexterm"></a> the cartesian product of the RDD with the other RDD.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">groupBy</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">f, numPartitions=None</pre></div>
</td><td style="" align="left" valign="top">
<p>It returns <a id="id304" class="indexterm"></a>an RDD with the elements grouped together for the value that <code class="literal">f</code> outputs.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">pipe</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">command, env={}</pre></div>
</td><td style="" align="left" valign="top">
<p>It<a id="id305" class="indexterm"></a> pipes each element of the RDD to the provided command and returns an RDD of the result.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">foreach</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">f</pre></div>
</td><td style="" align="left" valign="top">
<p>It <a id="id306" class="indexterm"></a>applies the function <code class="literal">f</code> to each element in the RDD.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">reduce</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">f</pre></div>
</td><td style="" align="left" valign="top">
<p>It reduces<a id="id307" class="indexterm"></a> the elements using the provided function.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">fold</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">zeroValue, op</pre></div>
</td><td style="" align="left" valign="top">
<p>Each<a id="id308" class="indexterm"></a> partition is folded individually with zero value and then the results are folded.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">countByValue</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">()</pre></div>
</td><td style="" align="left" valign="top">
<p>It returns a<a id="id309" class="indexterm"></a> dictionary mapping of each distinct value to the number of times it is found in the RDD.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">take</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">num</pre></div>
</td><td style="" align="left" valign="top">
<p>It returns <a id="id310" class="indexterm"></a>a list of <code class="literal">num</code> elements. This can be slow for large values of <code class="literal">num</code>; so use <code class="literal">collect</code> if you want to get back the entire RDD.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">partitionBy</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">numPartitions, partitionFunc=hash</pre></div>
</td><td style="" align="left" valign="top">
<p>Make a new <a id="id311" class="indexterm"></a>RDD partitioned by the provided partitioning function. The <code class="literal">partitionFunc</code> function simply needs to map the input key to an integer number and the <code class="literal">partitionBy</code> calculates the partition by that number mod <code class="literal">numPartitions</code>.</p>
</td></tr></tbody></table></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec21"></a>PairRDD functions</h3></div></div></div><p>These functions are only available <a id="id312" class="indexterm"></a>on key-value pair functions:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Name</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Params</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Purpose</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p><code class="literal">collectAsMap</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">()</pre></div>
</td><td style="" align="left" valign="top">
<p>This<a id="id313" class="indexterm"></a> returns a dictionary consisting of all of the key-value pairs of the RDD.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">reduceByKey</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">func, numPartitions=None</pre></div>
</td><td style="" align="left" valign="top">
<p>The <code class="literal">reduceByKey</code> function is the parallel<a id="id314" class="indexterm"></a> version of <code class="literal">reduce</code>, which merges the values for each key using the provided function and returns an RDD.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">countByKey</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">()</pre></div>
</td><td style="" align="left" valign="top">
<p>This <a id="id315" class="indexterm"></a>returns a dictionary of the number of elements for each key.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">join</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">other, numPartitions=None</pre></div>
</td><td style="" align="left" valign="top">
<p>This<a id="id316" class="indexterm"></a> joins an RDD with another RDD. The result is only present for elements where the key is present in both RDDs. The value that gets stored for each key is a tuple of the values from each RDD.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">rightOuterJoin</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">other, numPartitions=None</pre></div>
</td><td style="" align="left" valign="top">
<p>This<a id="id317" class="indexterm"></a> joins an RDD with another RDD. It outputs a given key-value pair only if the key it's being joined with is present in the RDD. If key is not present in the source RDD, the first value in the tuple will be <code class="literal">None</code>.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">leftOuterJoin</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">other, numPartitions=None</pre></div>
</td><td style="" align="left" valign="top">
<p>This<a id="id318" class="indexterm"></a> joins an RDD with another RDD. It outputs a given key-value pair only if the key is present in the source RDD. If the key is not present in other RDD, the second value in the tuple will be <code class="literal">None</code>.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">combineByKey</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">createCombiner, mergeValues, mergeCombiners</pre></div>
</td><td style="" align="left" valign="top">
<p>This <a id="id319" class="indexterm"></a>combines elements by key. It takes an RDD of type (K,V) and returns an RDD of type (K,C). The <code class="literal">createCombiner</code> function turns something of type <code class="literal">V</code> into something of type <code class="literal">C</code>. The <code class="literal">mergeValue</code> function adds a <code class="literal">V</code> to a <code class="literal">C</code>, and <code class="literal">mergeCombiners</code> is used to combine two <code class="literal">C</code> values into a single <code class="literal">C</code> value.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">zip</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">other</pre></div>
</td><td style="" align="left" valign="top">
<p>This <a id="id320" class="indexterm"></a>returns key-value pairs, pairing one element from each RDD. The first key-value pair would be the 1st element from this RDD, and the value would be the 1st element from the "other" RDD; the second pair would be the respective second elements from each of the RDDs and so on.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">groupByKey</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">numPartitions=None</pre></div>
</td><td style="" align="left" valign="top">
<p>This <a id="id321" class="indexterm"></a>groups the values in the RDD by the key they have.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">cogroup</code></p>
</td><td style="" align="left" valign="top">
<div class="informalexample"><pre class="programlisting">other, numPartitions=None</pre></div>
</td><td style="" align="left" valign="top">
<p>This <a id="id322" class="indexterm"></a>joins two (or more) RDDs by the shared key. Note that if an element is missing in one RDD but present in the other one, the list will simply be empty.</p>
</td></tr></tbody></table></div><p>Some references <a id="id323" class="indexterm"></a>are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><a class="ulink" href="http://www.scala-lang.org/api/current/index.html#scala.collection.immutable.List" target="_blank">http://www.scala-lang.org/api/current/index.html#scala.collection.immutable.List</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD" target="_blank">http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaPairRDD" target="_blank">http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaPairRDD</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaDoubleRDD" target="_blank">http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaDoubleRDD</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext" target="_blank">https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://abshinn.github.io/python/apache-spark/2014/10/11/using-combinebykey-in-apache-spark/" target="_blank">http://abshinn.github.io/python/apache-spark/2014/10/11/using-combinebykey-in-apache-spark/</a></p></li><li style="list-style-type: disc"><p>Good examples of RDD transformations (<a class="ulink" href="https://github.com/JerryLead/SparkLearning/tree/master/src" target="_blank">https://github.com/JerryLead/SparkLearning/tree/master/src</a>)</p></li></ul></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec39"></a>Summary</h2></div></div><hr /></div><p>This chapter looked at how to perform computations on data in a distributed fashion once it's loaded into an RDD. With our knowledge of how to load and save RDDs, we can now write distributed programs using Spark.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch07"></a>Chapter 7. Spark SQL</h2></div></div></div><p>Spark SQL<a id="id324" class="indexterm"></a> holds an important feature in the Spark ecosystem, that is, integration with different data sources as well as the capability to interact with other subsystems such as visualization. As we know that in modern data stacks, no stack is an island by itself and in many ways, the versatility of integration with other components is an important capability. Obviously, the role of Spark SQL is not to replace SQL databases. We see it more as a versatile query interface to Spark data that complements the data wrangling and input capabilities of Spark. The ability to scale complex data operations makes sense only when one can utilize the results in flexible ways and Spark SQL achieves that. We'll cover the following topics in this chapter:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Interfacing Spark to dashboards (such as Tableau and Qlik) that know how to fire off SQL statements from a visualization interface based on what a user selects.</p></li><li style="list-style-type: disc"><p>Another use case for Spark SQL is programming queries to Spark data without employing RDD semantics. While RDD manipulations are required to implement data algorithms, the final dataset can be in a SchemaRDD, which can be queried using SQL. Sometimes, a combination of both works very well.</p></li><li style="list-style-type: disc"><p>Leveraging the knowledge of SQL queries. There is a huge amount of SQL knowledge among various people with roles ranging from data analysts and programmers to data engineers who have developed interesting SQL queries over their data. Spark needs to leverage that and it does that via Spark SQL.</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec40"></a>The Spark SQL architecture</h2></div></div><hr /></div><p>Interestingly as I <a id="id325" class="indexterm"></a>was writing this<a id="id326" class="indexterm"></a> chapter, Michael Armbrust from Databricks wrote a blog about the data sources API and an architecture diagram, from which I got the inspiration to create the following diagram:</p><div class="mediaobject"><img src="graphics/4005_07_01.jpg" /></div><p>The bottom layer<a id="id327" class="indexterm"></a> is the flexible data access (and store) that works via<a id="id328" class="indexterm"></a> multiple formats, usually a distributed filesystem such as the HDFS. The computation layer is the place where we leverage the distributed-at-scale processing of the Spark engine including the streaming data. The computation layer usually acts on RDDs. The Spark SQL then overlays the SchemaRDD veneer and provides the data access for applications, dashboards, BI tools, and so forth.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec22"></a>Spark SQL how-to in a nutshell</h3></div></div></div><p>The heart of the <a id="id329" class="indexterm"></a>Spark SQL is the SchemaRDD, which, as you can guess, associates a schema with an RDD. Of course, internally it does a lot of magic by leveraging the ability to scale and distribute processing, and that of flexible storage.</p><p>In many ways, the data access via Spark SQL is deceptively simple, that is, creating one or more appropriate RDDs paying attention to the layout, data types, and so on and then accessing via SchemaRDDs. We get to use all the interesting features of Spark for creating the RDDs: structured data from Hive or Parquet, unstructured data from any source, and the ability to apply the RDD operations at scale. Then you need to overlay respective schemas to the RDDs by creating SchemaRDDs. Viola! You now have the ability to run SQL over RDDs. You can see the SchemaRDDs being created in the log entries.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec23"></a>Spark SQL programming</h3></div></div></div><p>Let's not get our hands dirty and <a id="id330" class="indexterm"></a>work through various examples. We will start with a simple dataset and then progressively perform more sophisticated SQL statements. While writing other chapters, I was wondering what a good dataset that brings out the various aspects of SQL would be. And I hit upon an idea! Long time ago, the Northwind database was the canonical database to learn Microsoft Access and later SQL server. And that would be a good dataset for learning Spark SQL as well!</p><p>Let's use some of the tables and data to dig deeper into Spark SQL. The SQL scripts to create the Northwind database<a id="id331" class="indexterm"></a> is available at <a class="ulink" href="https://northwinddatabase.codeplex.com/releases/view/71634" target="_blank">https://northwinddatabase.codeplex.com/releases/view/71634</a>. In our case, we will load data from a set of CSV files and create an appropriate SchemaRDDs in Spark. Then we will fire off SQL queries of increasing complexity. A good reference for this is the Spark SQL programming guide<a id="id332" class="indexterm"></a> available at <a class="ulink" href="https://spark.apache.org/docs/latest/sql-programming-guide.html" target="_blank">https://spark.apache.org/docs/latest/sql-programming-guide.html</a>.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl3sec05"></a>SQL access to a simple data table</h4></div></div></div><p>Let's load a small <a id="id333" class="indexterm"></a>CSV file to the <code class="literal">employee</code> table, as shown here:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._ // for implicit conversations
import org.apache.spark.sql._

object BigData01 {
  // register case class external to main
  case class Employee(EmployeeID : Int, 
    LastName : String, FirstName : String, Title : String,
    BirthDate : String, HireDate : String,
    City : String, State : String, Zip : String, Country : String,
    ReportsTo : String)
    //
  def main(args: Array[String]): Unit = {
val sc = new SparkContext("local","Chapter 7")
    println(s"Running Spark Version ${sc.version}")
    //
val sqlContext = new org.apache.spark.sql.SQLContext(sc)
import sqlContext.createSchemaRDD // to implicitly convert an RDD to a SchemaRDD.
import sqlContext._
    //
val employeeFile = sc.textFile("/Users/ksankar/fdps-vii/NW-Employees-NoHdr.csv")
    println("Employee File has %d Lines.".format(employeeFile.count()))
    val employees = employeeFile.map(_.split(",")).
      map(e =&gt; Employee( e(0).trim.toInt,
        e(1), e(2), e(3), 
        e(4), e(5), 
        e(6), e(7), e(8), e(9), e(10)))
     println(employees.count)
     employees.registerTempTable("Employees")
     var result = sqlContext.sql("SELECT * from Employees")
     result.foreach(println)
     result = sqlContext.sql("SELECT * from Employees WHERE State = 'WA'")
     result.foreach(println)
  }
}</pre></div><p>The code is <a id="id334" class="indexterm"></a>straightforward. We create a <code class="literal">case</code> class that represents the <code class="literal">employee</code> table. We then parse the CSV file and create an RDD that has the <code class="literal">Employee</code> classes as its elements.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note06"></a>Note</h3><p>The datafiles<a id="id335" class="indexterm"></a> are available from at <a class="ulink" href="https://github.com/xsankar/fdps-vii" target="_blank">https://github.com/xsankar/fdps-vii</a>.</p></div><p>The screenshot of the process and output of running the code from the Spark shell is shown here:</p><div class="mediaobject"><img src="graphics/4005_07_02.jpg" /></div><p>We declare a <code class="literal">case</code> class and parse the file to <code class="literal">RDD[Employee]</code>, as shown here:</p><div class="mediaobject"><img src="graphics/4005_07_03.jpg" /></div><p>Now, you'll learn<a id="id336" class="indexterm"></a> about the SQL magic. We turn the RDD into a SchemaRDD and then run SQL queries, as shown in this screenshot:</p><div class="mediaobject"><img src="graphics/4005_07_04.jpg" /></div><p>You can see the <a id="id337" class="indexterm"></a>query plan and see that finally an RDD is returned as the query result.</p><p>Let us try a filter query <code class="literal">SELECT * from Employees WHERE State = 'WA'</code> and see how it works. Here is a screenshot of this:</p><div class="mediaobject"><img src="graphics/4005_07_05.jpg" /></div><p>Great, it worked as <a id="id338" class="indexterm"></a>expected! You can see that the filter did get into the query plan.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl3sec06"></a>Handling multiple tables with Spark SQL</h4></div></div></div><p>Now that we have <a id="id339" class="indexterm"></a>mastered the art of Spark SQL, let's try <a id="id340" class="indexterm"></a>multiple datasets and slightly larger datasets. The <code class="literal">Orders</code> table's dataset has 830 records and the <code class="literal">Order Details</code> has approximately 2000 records. These would give us a good representation of a few queries with joins that span the two tables.</p><p>Let's start by loading the <code class="literal">Orders</code> table, as shown next:</p><div class="informalexample"><pre class="programlisting">val ordersFile = sc.textFile("/Users/ksankar/fdps-vii/NW-Orders-NoHdr.csv")
    println("Orders File has %d Lines.".format(ordersFile.count()))
    val orders = ordersFile.map(_.split(",")).
      map(e =&gt; Order( e(0), e(1), e(2),e(3), e(4) ))
     println(orders.count)
     orders.registerTempTable("Orders")
     var result = sqlContext.sql("SELECT * from Orders")
     result.take(10).foreach(println)
     //</pre></div><p>The output of this is shown in the next screenshot. This is nothing different from our earlier work. You can see where it casts the variable result as a SchemaRDD. We have 830 orders in our table, as you can see here:</p><div class="mediaobject"><img src="graphics/4005_07_06.jpg" /></div><p>In this chapter, we are <a id="id341" class="indexterm"></a>trying to create a few queries. So we really<a id="id342" class="indexterm"></a> do not need hundreds of records. But the dataset has more records so that you can try out various queries on your own. The dataset is big enough to do meaningful queries but small enough to work on a laptop with limited resources. This would be a good exercise for you to experiment with Spark SQL. Look at the following screenshot for the results of this exercise:</p><div class="mediaobject"><img src="graphics/4005_07_07.jpg" /></div><p>Now let's load the <code class="literal">Order Details</code> table. By now, we are an old hand at doing this. The following is the code for the loading process of the table:</p><div class="informalexample"><pre class="programlisting">val orderDetFile = sc.textFile("/Users/ksankar/fdps-vii/NW-Order-Details-NoHdr.csv")
    println("Order Details File has %d Lines.".format(orderDetFile.count()))
val orderDetails = orderDetFile.map(_.split(",")).
      map(e =&gt; OrderDetails( e(0), e(1), e(2).trim.toFloat,e(3).trim.toInt, e(4).trim.toFloat ))
     println(orderDetails.count)
     orderDetails.registerTempTable("OrderDetails")
     result = sqlContext.sql("SELECT * from OrderDetails")
     result.take(10).foreach(println)</pre></div><p>The output from the <a id="id343" class="indexterm"></a>Spark shell is again as expected. It has 2,155<a id="id344" class="indexterm"></a> order details, as shown in the next screenshot:</p><div class="mediaobject"><img src="graphics/4005_07_08.jpg" /></div><p>Let's create <a id="id345" class="indexterm"></a>the <code class="literal">Orderdetails</code> table and make sure it works as<a id="id346" class="indexterm"></a> expected. The table is shown here:</p><div class="mediaobject"><img src="graphics/4005_07_09.jpg" /></div><p>Now comes the <a id="id347" class="indexterm"></a>interesting part. Let's join the two tables and <a id="id348" class="indexterm"></a>see how that query works. In this process, you might make some mistakes and learn a few things. Have a look at the following screenshot:</p><div class="mediaobject"><img src="graphics/4005_07_10.jpg" /></div><p>Here, the error was that <code class="literal">Order.ID</code> is a wrong name. So, we get the <code class="literal">identifier expected</code> error. Have a look at the following screenshot:</p><div class="mediaobject"><img src="graphics/4005_07_17.jpg" /></div><p>This is interesting. It doesn't like the <code class="literal">;</code> at the end!. Now, have a look at the following screenshot:</p><div class="mediaobject"><img src="graphics/4005_07_11.jpg" /></div><p>This one took me a<a id="id349" class="indexterm"></a> little time to figure out. The culprit was <code class="literal">OrderId</code>, which <a id="id350" class="indexterm"></a>is really <code class="literal">OrderID</code>!. Now, consider the following screenshot:</p><div class="mediaobject"><img src="graphics/4005_07_12.jpg" /></div><p>Now it understands all the attributes. Of course, there are two <code class="literal">OrderID</code> values, one from the <code class="literal">Orders</code> table and another from the <code class="literal">OrderDetails</code> table. Have a look at the next screenshot:</p><div class="mediaobject"><img src="graphics/4005_07_13.jpg" /></div><p>Finally, after correcting the errors, it works fine! Good stuff! Now, have a look at the following screenshot:</p><div class="mediaobject"><img src="graphics/4005_07_14.jpg" /></div><p>Interestingly, this <a id="id351" class="indexterm"></a>worked on the first try, the credit for which<a id="id352" class="indexterm"></a> goes to the Spark developers. In my machine, Spark progressively spawned many tasks with lots of shuffle and broadcast stages. You will see so many pages of logs entries (approximately 2,500 lines!); we suggest you just quickly browse through them to get a feel for the workflow graph.</p><p>Before we end this chapter, let us try printing all the results. The call skips <code class="literal">take(10)</code>.</p><p>The <code class="literal">scala&gt; result.foreach(println)</code> command works fine, but the results were mixed with the log entries. Take a quick look at the query plan it has printed out. It gives us an insight on the different operations it performs on the RDD, as shown in the next screenshot:</p><div class="mediaobject"><img src="graphics/4005_07_15.jpg" /></div><p>I did a count and <a id="id353" class="indexterm"></a>then printed out all the records, as shown in the <a id="id354" class="indexterm"></a>next screenshot. It worked out well. We could also format the printout with currency as well. I leave that as an exercise to be done by you!</p><div class="mediaobject"><img src="graphics/4005_07_16.jpg" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl3sec07"></a>Aftermath</h4></div></div></div><p>As seen in the preceding screenshot, this was a good exercise. We are thoroughly impressed! We just created the last query and it ran fine! The Spark developers have done a good job. Good work, guys.</p><p>The dataset also includes the <code class="literal">product</code> table, which I leave to you as an exercise. For example, you can work on a query that gives the sales by product or one that shows which products are selling more. The dataset also has date fields such as <code class="literal">order dates</code>, which you can use to query sales by quarter or reports like <span class="emphasis"><em>Product sales for 1997</em></span>. The dates are now read in as strings. They need to be converted to the <code class="literal">TIMESTAMP</code> data type.</p><a id="id355" class="indexterm"></a><p>Some more information can be found at the following sites:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><a class="ulink" href="https://northwinddatabase.codeplex.com/releases/view/71634" target="_blank">https://northwinddatabase.codeplex.com/releases/view/71634</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="https://databricks.com/blog/2015/01/09/spark-sql-data-sources-api-unified-data-access-for-the-spark-platform.html" target="_blank">https://databricks.com/blog/2015/01/09/spark-sql-data-sources-api-unified-data-access-for-the-spark-platform.html</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="https://spark.apache.org/docs/latest/sql-programming-guide.html" target="_blank">https://spark.apache.org/docs/latest/sql-programming-guide.html</a></p></li></ul></div></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec41"></a>Summary</h2></div></div><hr /></div><p>This was an important chapter that discussed the integration aspects of Spark. We have covered the main parts, namely, SchemaRDD and programmatic access. But there are more capabilities such as the JDBC/ODBC server for direct SQL queries as well as the Spark SQL CLI. On the integration side, you will see more integration capabilities in <a class="link" href="#" linkend="ch08">Chapter 8</a>, <span class="emphasis"><em>Spark with Big Data</em></span>. Spark SQL will be getting more features in future versions and I think this will be one of the areas that will grow at a much faster pace; interesting features such as partitioning, persistent tables, and optional user specified schema are slated for Spark 1.3.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch08"></a>Chapter 8. Spark with Big Data</h2></div></div></div><p>As we mentioned in <a class="link" href="#" linkend="ch07">Chapter 7</a>, <span class="emphasis"><em>Spark SQL</em></span>, the big data compute stack doesn't work in isolation. Integration points across multiple stacks and technologies are essential. In this chapter, we will look at how Spark works with some of the big data technologies that are part of the Hadoop ecosystem. We will cover the following topics in this chapter:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><span class="strong"><strong>Parquet</strong></span>: This is<a id="id356" class="indexterm"></a> an efficient storage format</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>HBase</strong></span>: This is<a id="id357" class="indexterm"></a> the database in the Hadoop Ecosystem</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec42"></a>Parquet – an efficient and interoperable big data format</h2></div></div><hr /></div><p>Parquet is essentially an interoperable storage format; its main goals are space efficiency<a id="id358" class="indexterm"></a> and query efficiency. Parquet's origin is based on Google's Dremel and was developed by Twitter and Cloudera. Parquet is now an Apache incubator project. The nested storage format from Google Dremel is implemented in Parquet. Parquet stores data in a columnar format and has an evolvable schema. This enables you to optimize queries (it can restrict columns that you need to access, and so you need not bring all columns into memory and discard the ones not needed), and it allows storage optimization (by decoding at the column level, which gives a much higher compression ratio). In addition to the ability to restrict column fetches during queries, Parquet 2.0 would implement push-down predicates. While writing this book, the Parquet version was 1.6.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec24"></a>Saving files to the Parquet format</h3></div></div></div><p>In <a class="link" href="#" linkend="ch07">Chapter 7</a>, <span class="emphasis"><em>Spark SQL</em></span>, we loaded the <code class="literal">Orders</code> tables from the <code class="literal">.csv</code> format. Let's save the data in the<a id="id359" class="indexterm"></a> Parquet format so that we can query the data from Impala. Usually <a id="id360" class="indexterm"></a>one would take a <code class="literal">.csv</code> file, do transformations, and then store it in the Parquet format (for example, the Sales By Country RDD that we had created). This is shown here:</p><div class="informalexample"><pre class="programlisting">     //
     // Parquet Operations
     //
valparquetFileOrders     val parquetFileOrders = orders.saveAsParquetFile("/Users/ksankar/fdps-vii/Orders.parquet")
valparquetFileOrderDet     val parquetFileOrderDet = orderDetails.saveAsParquetFile("/Users/ksankar/fdps-vii/OrderDetails.parquet")</pre></div><p>The<a id="id361" class="indexterm"></a> output<a id="id362" class="indexterm"></a> is shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/4005_08_01.jpg" /></div><p>Even though in this example we store the Parquet file in the local filesystem, in the actual production system you would use HDFS to store the files. We can inspect the log entries and see that it has started a job with the <code class="literal">ParquetTableOperations</code> class. The scheme used to save this was <span class="strong"><strong>Run Length Encoding</strong></span> (<span class="strong"><strong>RLE</strong></span>). As you can see, we need only a couple<a id="id363" class="indexterm"></a> of lines of code and Spark does all the hard work under the covers. It creates a directory, data, and metadata files underneath the main directory. It has created two files corresponding to the two jobs for two partitions, as shown here:</p><div class="mediaobject"><img src="graphics/4005_08_02.jpg" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec25"></a>Loading Parquet files</h3></div></div></div><p>Let's now<a id="id364" class="indexterm"></a> load the <code class="literal">Orders</code> Parquet files and see whether the data got saved<a id="id365" class="indexterm"></a> correctly. The code, again, is deceptively simple, as shown here:</p><div class="informalexample"><pre class="programlisting">     //
     // Let us read back the file
     //
valsqlContext= new org.apache.spark.sql.SQLContext(sc)
val parquetOrders= sqlContext.parquetFile("/Users/ksankar/fdps-vii/Orders.parquet")
     parquetOrders.registerTempTable("ParquetOrders")
     val result = sqlContext.sql("SELECT * from ParquetOrders")
     result.take(10).foreach(e=&gt;println("%5s | %5s | %s | %10s | %15s |".format(e(0),e(1),e(2),e(3),e(4))))</pre></div><p>The output is as follows:</p><div class="mediaobject"><img src="graphics/4005_08_04.jpg" /></div><p>As you can see, the first few lines create all the scaffolding and needed definitions. The lazy<a id="id366" class="indexterm"></a> evaluation does not do anything unless we ask for some <a id="id367" class="indexterm"></a>action, such as <code class="literal">take(10)</code>, as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/4005_08_05.jpg" /></div><p>It does all the work. You can see that Spark figured out that there are two files to process along with the field names and their types. It actually fails with an error, because <code class="literal">EmployeeID</code> was defined as string and I tried to print it with the '<code class="literal">%d</code>' mask. Now that's interesting, Spark keeps the data type in the Parquet metadata and can read it back. Once I used the <code class="literal">%s</code> mask, everything worked out fine.</p><p>Note that you cannot overwrite a Parquet file, as shown:</p><div class="mediaobject"><img src="graphics/4005_08_06.jpg" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec26"></a>Saving processed RDD in the Parquet format</h3></div></div></div><p>Now let's<a id="id368" class="indexterm"></a> save our <code class="literal">SalesByCountry</code> report in the Parquet <a id="id369" class="indexterm"></a>format. We create a SQL table <code class="literal">ieaSchemaRDD</code> and then save that as a Parquet file:</p><div class="informalexample"><pre class="programlisting">     //
     // Save our Sales By Country Report as parquet
     //
valsalesByCountry = sqlContext.sql("SELECT ShipCountry, Sum(OrderDetails.UnitPrice * Qty * Discount) AS ProductSales FROM Orders INNER JOIN OrderDetails ON Orders.OrderID = OrderDetails.OrderID GROUP BY ShipCountry")
     salesByCountry.registerTempTable("SalesByCountry")
     result = sqlContext.sql("SELECT * from SalesByCountry")
     result.take(30).foreach(e=&gt;println("%15s | %9.2f |".format(e(0),e(1))))
valparquetSALES = salesByCountry.saveAsParquetFile("/Users/ksankar/fdps-vii/SalesByCountry.parquet") </pre></div><p>By now we know the drill, and as expected, the files are created, as shown next:</p><div class="mediaobject"><img src="graphics/4005_08_07.jpg" /></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec43"></a>Querying Parquet files with Impala</h2></div></div><hr /></div><p>Impala is a <span class="strong"><strong>massively parallel processing</strong></span> (<span class="strong"><strong>MPP</strong></span>) data layer that is focused on SQL queries<a id="id370" class="indexterm"></a> over large data sets and suited for <a id="id371" class="indexterm"></a>exploratory data analytics. The main utility<a id="id372" class="indexterm"></a> is the ability of SQL queries over Hadoop data; this means that the data is stored in HDFS in different formats by MapReduce and Spark. Let's fire up Impala and see if we can query our <code class="literal">Orders</code> database.</p><p>The best way to try out Impala is through Cloudera's QuickStart VM available at <a class="ulink" href="http://www.cloudera.com/content/cloudera/en/downloads/quickstart_vms/cdh-5-3-x.html" target="_blank">http://www.cloudera.com/content/cloudera/en/downloads/quickstart_vms/cdh-5-3-x.html</a>. While<a id="id373" class="indexterm"></a> the details are outside the scope of this book, let me quickly outline the top level steps for MacOS:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Download the VMWare VM and install via VMWare Fusion.</p></li><li><p>The VM starts and it has a single-node Hadoop cluster with all the stack including Impala, Spark, and HBase, and so on.</p></li><li><p>The VM is CentOS 6.4. You need to start the terminal from <span class="strong"><strong>Applications</strong></span> | <span class="strong"><strong>System Tools</strong></span> | <span class="strong"><strong>Terminal</strong></span>.</p></li><li><p>At the terminal prompt, type <code class="literal">Impala-shell</code> to start the Impala shell and verify that it works. Type <code class="literal">Exit</code> and exit out of it.</p></li><li><p>Copy the files under the <code class="literal">Orders.parquet</code> directory to the <code class="literal">Orders</code> directory in HDFS in the Cloudera VM using a USB disk. The commands that I used for this are shown here:</p><p>Copy the files to a USB disk:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>cp -rv ~/fdps-vii/Orders.parquet /Volumes/USB\ DISK/</strong></span>
</pre></div><p>Connect the USB to the VM.</p><p>In the VM, copy the files to a local directory first and then to a directory in the HDFS (<code class="literal">hdfsdfs –copyToLocal</code> gives unexpected <code class="literal">urisyntaxexception</code> if copied directly from the USB disk—probably the way VMware maps the USB disk), as shown here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[cloudera@quickstart ~]$ mkdir Orders</strong></span>
<span class="strong"><strong>[cloudera@quickstart ~]$ cp /media/USB\ DISK/fdps-vii/Orders.parquet/* Orders/.</strong></span>
<span class="strong"><strong>[cloudera@quickstart ~]$ hdfsdfs -mkdir Orders</strong></span>
<span class="strong"><strong>[cloudera@quickstart ~]$ hdfsdfs -copyFromLocal /Orders/* Orders/.</strong></span>
</pre></div></li><li><p>Verify that the files are indeed in HDFS, as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/4005_08_08.jpg" /></div></li><li><p>Get <a id="id374" class="indexterm"></a>back to Impala using the following <a id="id375" class="indexterm"></a>command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[cloudera@quickstart ~]$ impala-shell</strong></span>
</pre></div></li><li><p>Create an external table pointing to the HDFS directory where we have copied the files, as shown here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[quickstart.cloudera:21000] &gt; create external table orders (ordered string,customerID string,employeeid string,orderdate string,shipCountry string) stored as parquet location '/user/cloudera/Orders';</strong></span>
</pre></div><p>The result is shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/4005_08_09.jpg" /></div></li><li><p>Finally, execute the following SQL statement:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>select * from orders limit 10;</strong></span>
</pre></div><p>And <a id="id376" class="indexterm"></a>you can see the records, as<a id="id377" class="indexterm"></a> shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/4005_08_10.jpg" /></div></li></ol></div><p>We can also use the Hue graphical query UI and execute the queries, as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/4005_08_11.jpg" /></div><p>That<a id="id378" class="indexterm"></a> was not so hard. Once we master the various <a id="id379" class="indexterm"></a>steps and commands, the rest is easy.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec44"></a>HBase</h2></div></div><hr /></div><p>HBase is the NoSQL datastore in the Hadoop ecosystem. Integration with a database is essential for Spark. It could read data from an HBase table or write to one. In fact, Spark supports HBase <a id="id380" class="indexterm"></a>very well via the HadoopdataSet calls.</p><p>Before working through the examples, let's first create a table and three records in HBase. For testing, you can install a local standalone version of HBase that works from the local filesystem. So there's no need for Hadoop or HDFS. But that won't be suitable for production.</p><p>I created a <code class="literal">test</code> table with three records via the HBase shell as shown in the next screenshot:</p><div class="mediaobject"><img src="graphics/4005_08_12.jpg" /></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec27"></a>Loading from HBase</h3></div></div></div><p>The HBase test code in the Apache Spark examples is a good start to test our HBase connectivity <a id="id381" class="indexterm"></a>and the loading data. The code is not that difficult, but we do need to keep track of the data types, that is, keys as bytes, values as strings, and so on. The test code is given here:</p><div class="informalexample"><pre class="programlisting">Val sc = new SparkContext("local","Chapter 8")
println(s"Running Spark Version ${sc.version}")
//
val conf = HBaseConfiguration.create()
conf.set(TableInputFormat.INPUT_TABLE, "test")

val admin = new HBaseAdmin(conf)
println(admin.isTableAvailable("test"))

val hBaseRDD = sc.newAPIHadoopRDD(conf, classOf[TableInputFormat],
classOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable],
classOf[org.apache.hadoop.hbase.client.Result])
      println(hBaseRDD.count())
      //
      hBaseRDD.foreach(println) // will print bytes
      hBaseRDD.foreach(e=&gt; ( println("%s | %s |".format( Bytes.toString(e._1.get()),e._2) ) ) )
//
println("** Read Done **")</pre></div><p>The output of this is shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/4005_08_13.jpg" /></div><p>This is just<a id="id382" class="indexterm"></a> a starting point. You would need to convert the bytes from HBase to the actual data types of your data structures. You need to experiment a bit to get it right.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec28"></a>Saving to HBase</h3></div></div></div><p>Now let's <a id="id383" class="indexterm"></a>store a new record in our test table—key as <code class="literal">row4</code> and value as <code class="literal">value4</code>. It does require a few more classes and manipulations but nothing fancy, as shown next:</p><div class="informalexample"><pre class="programlisting">      //
      // create a pair RDD "row4":"value4"
      // save it in column family "d"
      //
      val testMap = Map("row4" -&gt; "value4")
      val pairs = sc.parallelize(List(("row4","value4")))
      pairs.foreach(println)
      //
      //Function to convert our RDD to the required format for HBase
      //
      def convert(triple: (String, String)) = {
        val p = new Put(Bytes.toBytes(triple._1))
        p.add(Bytes.toBytes("cf"), Bytes.toBytes("d"), Bytes.toBytes(triple._2))
        (neworg.apache.hadoop.hbase.io.ImmutableBytesWritable, p)
      }
      //
      valjobConfig: JobConf = new JobConf(conf, this.getClass)
      jobConfig.setOutputFormat(classOf[TableOutputFormat])
      jobConfig.set(TableOutputFormat.OUTPUT_TABLE, "test")
      //
newPairRDDFunctions(pairs.map(convert)).saveAsHadoopDataset(jobConfig)
      //
      println("** Write Done **")</pre></div><p>The<a id="id384" class="indexterm"></a> program runs and prints out as shown in the next screenshot:</p><div class="mediaobject"><img src="graphics/4005_08_14.jpg" /></div><p>Now let's go back to the HBase shell and verify that the fourth record is added, as shown in the next screenshot:</p><div class="mediaobject"><img src="graphics/4005_08_15.jpg" /></div><p>Good. We<a id="id385" class="indexterm"></a> can see the fourth record and a later timestamp!</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec29"></a>Other HBase operations</h3></div></div></div><p>We can<a id="id386" class="indexterm"></a> also get the metadata about the HBase server and environment, as shown here:</p><div class="informalexample"><pre class="programlisting">val status = admin.getClusterStatus();
println("HBase Version : " +status.getHBaseVersion())
println("Average Load : "+status.getAverageLoad())
println("Backup Master Size : " + status.getBackupMastersSize())
println("Balancer On : " + status.getBalancerOn())
println("Cluster ID : "+ status.getClusterId())
println("Server Info : " + status.getServerInfo())</pre></div><p>The output prints out the details, as you can see in the following screenshot:</p><div class="mediaobject"><img src="graphics/4005_08_16.jpg" /></div><p>Some more information is available at the following websites:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><a class="ulink" href="https://github.com/apache/spark/tree/master/examples/src/main/scala/org/apache/spark/examples" target="_blank">https://github.com/apache/spark/tree/master/examples/src/main/scala/org/apache/spark/examples</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://parquet.incubator.apache.org/documentation/latest/" target="_blank">http://parquet.incubator.apache.org/documentation/latest/</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://www.slideshare.net/cloudera/hadoop-summit-36479635?ref=http://parquet.incubator.apache.org/presentations/" target="_blank">http://www.slideshare.net/cloudera/hadoop-summit-36479635?ref=http://parquet.incubator.apache.org/presentations/</a></p></li><li style="list-style-type: disc"><p>Google Dremel paper at <a class="ulink" href="http://research.google.com/pubs/pub36632.html" target="_blank">http://research.google.com/pubs/pub36632.html</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="https://blog.twitter.com/2013/dremel-made-simple-with-parquet" target="_blank">https://blog.twitter.com/2013/dremel-made-simple-with-parquet</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://planetcassandra.org/getting-started-with-apache-spark-and-cassandra/" target="_blank">http://planetcassandra.org/getting-started-with-apache-spark-and-cassandra/</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://blog.cloudera.com/blog/2014/12/new-in-cloudera-labs-sparkonhbase/" target="_blank">http://blog.cloudera.com/blog/2014/12/new-in-cloudera-labs-sparkonhbase/</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://www.vidyasource.com/blog/Programming/Scala/Java/Data/Hadoop/Analytics/2014/01/25/lighting-a-spark-with-hbase" target="_blank">http://www.vidyasource.com/blog/Programming/Scala/Java/Data/Hadoop/Analytics/2014/01/25/lighting-a-spark-with-hbase</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/HBaseTest.scala" target="_blank">https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/HBaseTest.scala</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="https://federicodayan.wordpress.com/2010/09/28/hbase-textgetbytes-and-immutablebyteswritabletostring/" target="_blank">https://federicodayan.wordpress.com/2010/09/28/hbase-textgetbytes-and-immutablebyteswritabletostring/</a></p></li></ul></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec45"></a>Summary</h2></div></div><hr /></div><p>This chapter was focused on the integration of Spark with other big data technologies. The Parquet format is an excellent way to expose the data processed by Spark to external systems, and Impala makes this very easy. The advantage of the Parquet format is that it is very efficient in terms of storage and expressive enough to capture the schema. We also looked at the process of interfacing with HBase. Thus, we can have our cake and eat it too! This means that we can leverage Spark for distributed scalable data processing, without losing the capability to integrate with other big data technologies.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch09"></a>Chapter 9. Machine Learning Using Spark MLlib</h2></div></div></div><p>One of the major attractions of Spark is the ability to scale computation massively, and that is exactly what you need for machine learning algorithms. But the caveat is that all machine learning algorithms cannot be effectively parallelized. Each algorithm has its own challenges for parallelization, whether it is task parallelism or data parallelism. Having said that, Spark is becoming the de-facto platform for building machine learning algorithms and applications. For example, Apache Mahout is moving away from Hadoop MapReduce and implementing the algorithms in Spark (see the first reference at the end of this chapter). The <a id="id387" class="indexterm"></a>developers working on the Spark MLlib are implementing more and more machine algorithms in a scalable and concise manner in the Spark<a id="id388" class="indexterm"></a> framework. For the latest information on this, you can refer to the Spark site at <a class="ulink" href="https://spark.apache.org/docs/latest/mllib-guide.html" target="_blank">https://spark.apache.org/docs/latest/mllib-guide.html</a>, which is the authoritative source.</p><p>This chapter covers the following machine learning algorithms:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Basic statistics</p></li><li style="list-style-type: disc"><p>Linear regression</p></li><li style="list-style-type: disc"><p>Classification</p></li><li style="list-style-type: disc"><p>Clustering</p></li><li style="list-style-type: disc"><p>Recommendations</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec46"></a>The Spark machine learning algorithm table</h2></div></div><hr /></div><p>The<a id="id389" class="indexterm"></a> Spark machine learning algorithms implemented in Spark 1.1.0 <code class="literal">org.apache.spark.mllib</code> for Scala and Java, and in <code class="literal">pyspark.mllib</code> for Python is shown in the following table:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Algorithm</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Feature</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Notes</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>Basic statistics</p>
</td><td style="" align="left" valign="top">
<p>Summary statistics</p>
</td><td style="" align="left" valign="top">
<p>Mean, variance, count, max, min, and numNonZeros</p>
</td></tr><tr><td style="" align="left" valign="top"> </td><td style="" align="left" valign="top">
<p>Correlations</p>
</td><td style="" align="left" valign="top">
<p>Spearman and Pearson correlation</p>
</td></tr><tr><td style="" align="left" valign="top"> </td><td style="" align="left" valign="top">
<p>Stratified sampling</p>
</td><td style="" align="left" valign="top">
<p>sampleBykey, sampleByKeyExact—With and without replacement</p>
</td></tr><tr><td style="" align="left" valign="top"> </td><td style="" align="left" valign="top">
<p>Hypothesis testing</p>
</td><td style="" align="left" valign="top">
<p>Pearson's chi-squared goodness of fit test</p>
</td></tr><tr><td style="" align="left" valign="top"> </td><td style="" align="left" valign="top">
<p>Random data generation</p>
</td><td style="" align="left" valign="top">
<p>RandomRDDs</p>
<p>Normal, Poisson, and so on</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Regression</p>
</td><td style="" align="left" valign="top">
<p>Linear models</p>
</td><td style="" align="left" valign="top">
<p>Linear regression—least square, Lasso, and ridge regression</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Classification</p>
</td><td style="" align="left" valign="top">
<p>Binary classification</p>
</td><td style="" align="left" valign="top">
<p>Logistic regression, SVM, decision trees, and naïve Bayes</p>
</td></tr><tr><td style="" align="left" valign="top"> </td><td style="" align="left" valign="top">
<p>Multi-class classification</p>
</td><td style="" align="left" valign="top">
<p>Decision trees, naïve Bayes, and so on</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Recommendation</p>
</td><td style="" align="left" valign="top">
<p>Collaborative filtering</p>
</td><td style="" align="left" valign="top">
<p>Alternating least squares</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Clustering</p>
</td><td style="" align="left" valign="top">
<p>k-means</p>
</td><td style="" align="left" valign="top"> </td></tr><tr><td style="" align="left" valign="top">
<p>Dimensionality reduction</p>
</td><td style="" align="left" valign="top">
<p>SVD</p>
<p>PCA</p>
</td><td style="" align="left" valign="top"> </td></tr><tr><td style="" align="left" valign="top">
<p>Feature extraction</p>
</td><td style="" align="left" valign="top">
<p>TF-IDF</p>
<p>Word2Vec</p>
<p>StandardScaler</p>
<p>Normalizer</p>
</td><td style="" align="left" valign="top"> </td></tr><tr><td style="" align="left" valign="top">
<p>Optimization</p>
</td><td style="" align="left" valign="top">
<p>SGD</p>
<p>L-BFGS</p>
</td><td style="" align="left" valign="top"> </td></tr></tbody></table></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec47"></a>Spark MLlib examples</h2></div></div><hr /></div><p>Now, let's look at how to use the algorithms. Naturally, we need interesting datasets to implement the <a id="id390" class="indexterm"></a>algorithms; we will use appropriate datasets for the algorithms shown in the next section. In the book text, we will use Scala, but I have included iPython notebooks of the algorithm examples in Python as well.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note07"></a>Note</h3><p>The code<a id="id391" class="indexterm"></a> and data files are available in the GitHub repository at <a class="ulink" href="https://github.com/xsankar/fdps-vii" target="_blank">https://github.com/xsankar/fdps-vii</a>. We'll keep it updated with corrections.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec30"></a>Basic statistics</h3></div></div></div><p>Let's read the<a id="id392" class="indexterm"></a> car mileage data into an RDD and then <a id="id393" class="indexterm"></a>compute some basic statistics. We will use a simple parse class to parse a line of data. This will work if you know the type and the structure of your CSV file. We will use this technique for the examples in this chapter:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark.SparkContext
import org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}
import org.apache.spark.mllib.linalg.Vector
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.rdd.RDD

object MLlib01 {
  //
  def getCurrentDirectory = new java.io.File( "." ).getCanonicalPath
  //
  def parseCarData(inpLine : String) : Array[Double] = {
    val values = inpLine.split(',')
    val mpg = values(0).toDouble
    val displacement = values(1).toDouble
    val hp = values(2).toInt
    val torque = values(3).toInt
    val CRatio = values(4).toDouble
    val RARatio = values(5).toDouble
    val CarbBarrells = values(6).toInt
    val NoOfSpeed = values(7).toInt
    val length = values(8).toDouble
    val width = values(9).toDouble
    val weight = values(10).toDouble
    val automatic = values(11).toInt
    return Array(mpg,displacement,hp,
    torque,CRatio,RARatio,CarbBarrells,
    NoOfSpeed,length,width,weight,automatic)
  }
  //
  def main(args: Array[String]) {
    println(getCurrentDirectory)
    val sc = new SparkContext("local","Chapter 9")
    println(s"Running Spark Version ${sc.version}")
    //
   val dataFile = sc.textFile("/Users/ksankar/fdps-vii/data/car-milage-no-hdr.csv")
   val carRDD = dataFile.map(line =&gt; parseCarData(line))
   //
   // Let us find summary statistics
   //
   val vectors: RDD[Vector] = carRDD.map(v =&gt; Vectors.dense(v))
   val summary = Statistics.colStats(vectors)
   carRDD.foreach(ln=&gt; {ln.foreach(no =&gt; print("%6.2f | ".format(no))); println()})
   print("Max  :");summary.max.toArray.foreach(m =&gt; print("%5.1f | ".format(m)));println
   print("Min  :");summary.min.toArray.foreach(m =&gt; print("%5.1f | ".format(m)));println
   print("Mean :");summary.mean.toArray.foreach(m =&gt; print("%5.1f | ".format(m)));println
   }
}</pre></div><p>This<a id="id394" class="indexterm"></a> program <a id="id395" class="indexterm"></a>will produce the following output:</p><div class="mediaobject"><img src="graphics/4005_09_01.jpg" /></div><p>Let's<a id="id396" class="indexterm"></a> also<a id="id397" class="indexterm"></a> run some correlations, as shown here:</p><div class="informalexample"><pre class="programlisting">//
// correlations
//
<span class="strong"><strong>val</strong></span> hp = vectors.map(x =&gt; x(2))
<span class="strong"><strong>val</strong></span> weight = vectors.map(x =&gt; x(10))
<span class="strong"><strong>var</strong></span> corP = Statistics.corr(hp,weight,"pearson") // default
println("hp to weight : Pearson Correlation = %2.4f".format(corP))
<span class="strong"><strong>var</strong></span> corS = Statistics.corr(hp,weight,"spearman") // Need to specify
println("hp to weight : Spearman Correlation = %2.4f".format(corS)) 
//
<span class="strong"><strong>val</strong></span> raRatio = vectors.map(x =&gt; x(5))
<span class="strong"><strong>val</strong></span> width = vectors.map(x =&gt; x(9))
corP = Statistics.corr(raRatio,width,"pearson") // default
println("raRatio to width : Pearson Correlation = %2.4f".format(corP))
corS = Statistics.corr(raRatio,width,"spearman") // Need to specify
println("raRatio to width : Spearman Correlation = %2.4f".format(corS)) 
//</pre></div><p>This<a id="id398" class="indexterm"></a> will produce interesting results as shown in <a id="id399" class="indexterm"></a>the next screenshot:</p><div class="mediaobject"><img src="graphics/4005_09_02.jpg" /></div><p>While this might seem too much work to calculate the correlation of a tiny dataset, remember that this will scale to datasets consisting of 1,000,000 rows or even a billion rows!</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec31"></a>Linear regression</h3></div></div></div><p>Linear regression<a id="id400" class="indexterm"></a> takes a little more work than <a id="id401" class="indexterm"></a>statistics. We need the <code class="literal">LabeledPoint</code> class as well as a few more parameters such as the learning rate, that is, the step size. We will also split the dataset into <code class="literal">training</code> and <code class="literal">test</code>, as shown here:</p><div class="informalexample"><pre class="programlisting">   //
   //
  def carDataToLP(inpArray : Array[Double]) : LabeledPoint = {
    return new LabeledPoint( inpArray(0),Vectors.dense ( inpArray(1), inpArray(2), inpArray(3), inpArray(4), inpArray(5), inpArray(6), inpArray(7), inpArray(8), inpArray(9), inpArray(10), inpArray(11) ) )
    }
// Linear Regression
   //
   val carRDDLP = carRDD.map(x =&gt; carDataToLP(x)) // create a labeled point RDD
   println(carRDDLP.count())
   println(carRDDLP.first().label)
   println(carRDDLP.first().features)
   //
   // Let us split the data set into training &amp; test set using a very simple filter
   //
   val carRDDLPTrain = carRDDLP.filter( x =&gt; x.features(9) &lt;= 4000)
   val carRDDLPTest = carRDDLP.filter( x =&gt; x.features(9) &gt; 4000)
   println("Training Set : " + "%3d".format(carRDDLPTrain.count()))
   println("Training Set : " + "%3d".format(carRDDLPTest.count()))
   //
   // Train a Linear Regression Model
   // numIterations = 100, stepsize = 0.000000001
   // without such a small step size the algorithm will diverge
   //
   val mdlLR = LinearRegressionWithSGD.train(carRDDLPTrain,100,0.000000001)
   println(mdlLR.intercept) // Intercept is turned off when using LinearRegressionSGD object, so intercept will always be 0 for this code

   println(mdlLR.weights)
   //
   // Now let us use the model to predict our test set
   //
   val valuesAndPreds = carRDDLPTest.map(p =&gt; (p.label, mdlLR.predict(p.features)))
   val mse = valuesAndPreds.map( vp =&gt; math.pow( (vp._1 - vp._2),2 ) ).
       reduce(_+_) / valuesAndPreds.count()
   println("Mean Squared Error      = " + "%6.3f".format(mse))
    println("Root Mean Squared Error = " + "%6.3f".format(math.sqrt(mse)))
    // Let us print what the model predicted
    valuesAndPreds.take(20).foreach(m =&gt; println("%5.1f | %5.1f |".format(m._1,m._2)))</pre></div><p>The<a id="id402" class="indexterm"></a> run result will be as expected, as shown in<a id="id403" class="indexterm"></a> the next screenshot:</p><div class="mediaobject"><img src="graphics/4005_09_03.jpg" /></div><p>The prediction is not that impressive. There are a couple of reasons for this. There might be quadratic effects; some of the variables might be correlated (for example, length, width, and weight, and so we might not need all three to predict the <code class="literal">mpg</code> value). Finally, we might not need all the 10 features anyways. I leave it to you to try with different combinations of features. (In the <code class="literal">parseCarData</code> function, take only a subset of the variables; for<a id="id404" class="indexterm"></a> example take hp, weight, and <a id="id405" class="indexterm"></a>number of speed and see which combination minimizes the <code class="literal">mse</code> value.)</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec32"></a>Classification</h3></div></div></div><p>Classification is very similar to linear regression. The algorithms take labeled points, and the train <a id="id406" class="indexterm"></a>process has various parameters to tweak the<a id="id407" class="indexterm"></a> algorithm to fit the needs of an application. The returned model can be used to predict the class of a labeled point. Here is a quick example using the <code class="literal">titanic</code> dataset:</p><p>For our example, we will keep the same structure as the linear regression example. First, we will parse the full dataset line and then later keep it simple by creating a labeled point with a set of selected features, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark.SparkContext
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.tree.DecisionTree

object Chapter0802 {
  //
  def getCurrentDirectory = new java.io.File( "." ).getCanonicalPath
  //
  //  0 pclass,1 survived,2 l.name,3.f.name, 4 sex,5 age,6 sibsp,7 parch,8 ticket,9 fare,10 cabin,
  // 11 embarked,12 boat,13 body,14 home.dest
  //
  def str2Double(x: String) : Double = {
    try {
      x.toDouble
    } catch {
      case e: Exception =&gt; 0.0
    }
  }
  //
  def parsePassengerDataToLP(inpLine : String) : LabeledPoint = {
    val values = inpLine.split(',')
    //println(values)
    //println(values.length)
    //
    val pclass = str2Double(values(0))
    val survived = str2Double(values(1))
    // skip last name, first name
    var sex = 0
    if (values(4) == "male") {
      sex = 1
    }
    var age = 0.0 // a better choice would be the average of all ages
    age = str2Double(values(5))
    //
    var sibsp = 0.0
    age = str2Double(values(6))
    //
    var parch = 0.0
    age = str2Double(values(7))
    //
    var fare = 0.0
    fare = str2Double(values(9))
    return new LabeledPoint(survived,Vectors.dense(pclass,sex,age,sibsp,parch,fare))
  }</pre></div><p>Now <a id="id408" class="indexterm"></a>that we have setup the routines to parse the <a id="id409" class="indexterm"></a>data, let's dive into the main program:</p><div class="informalexample"><pre class="programlisting">  //
  def main(args: Array[String]): Unit = {
    println(getCurrentDirectory)
    val sc = new SparkContext("local","Chapter 8")
    println(s"Running Spark Version ${sc.version}")
    //
    val dataFile = sc.textFile("/Users/ksankar/bdtc-2014/titanic/titanic3_01.csv")
    val titanicRDDLP = dataFile.map(_.trim).filter( _.length &gt; 1).
      map(line =&gt; parsePassengerDataToLP(line))
    //
    println(titanicRDDLP.count())
    //titanicRDDLP.foreach(println)
    //
    println(titanicRDDLP.first().label)
    println(titanicRDDLP.first().features)
    //
    val categoricalFeaturesInfo = Map[Int, Int]()
    val mdlTree = DecisionTree.trainClassifier(titanicRDDLP, 2, // numClasses
        categoricalFeaturesInfo, // all features are continuous
        "gini", // impurity
        5, // Maxdepth
        32) //maxBins
    //
    println(mdlTree.depth)
    println(mdlTree)</pre></div><p>The<a id="id410" class="indexterm"></a> tree is interesting to inspect. Check it out<a id="id411" class="indexterm"></a> here:</p><div class="informalexample"><pre class="programlisting">    //
    // Let us predict on the dataset and see how well it works.
    // In the real world, we should split the data to train &amp; test and then predict the test data:
    //
    val predictions = mdlTree.predict(titanicRDDLP.map(x=&gt;x.features))
    val labelsAndPreds = titanicRDDLP.map(x=&gt;x.label).zip(predictions)
    //
    val mse = labelsAndPreds.map( vp =&gt; math.pow( (vp._1 - vp._2),2 ) ).
       reduce(_+_) / labelsAndPreds.count()
    println("Mean Squared Error = " + "%6f".format(mse))
    //
    // labelsAndPreds.foreach(println)
    //
    val correctVals = labelsAndPreds.aggregate(0.0)((x, rec) =&gt; x + (rec._1 == rec._2).compare(false), _ + _)
    val accuracy = correctVals/labelsAndPreds.count()
    println("Accuracy = " + "%3.2f%%".format(accuracy*100))
    //
    println("*** Done ***")
  }
}</pre></div><p>The result <a id="id412" class="indexterm"></a>obtained when you run the program<a id="id413" class="indexterm"></a> is as expected. The printout of the tree is interesting, as shown here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Running Spark Version 1.1.1</strong></span>
<span class="strong"><strong>14/11/28 18:41:27 INFO MemoryStore: ensureFreeSpace(163705) called with curMem=0, maxMem=2061647216</strong></span>
<span class="strong"><strong>[..]</strong></span>
<span class="strong"><strong>14/11/28 18:41:27 INFO SparkContext: Job finished: count at Chapter0802.scala:56, took 0.260993 s</strong></span>
<span class="strong"><strong>1309</strong></span>
<span class="strong"><strong>14/11/28 18:41:27 INFO SparkContext: Starting job: first at Chapter0802.scala:59</strong></span>
<span class="strong"><strong>[..]</strong></span>
<span class="strong"><strong>14/11/28 18:41:27 INFO SparkContext: Job finished: first at Chapter0802.scala:59, took 0.016479 s</strong></span>
<span class="strong"><strong>1.0</strong></span>
<span class="strong"><strong>14/11/28 18:41:27 INFO SparkContext: Starting job: first at Chapter0802.scala:60</strong></span>
<span class="strong"><strong>[..]</strong></span>
<span class="strong"><strong>14/11/28 18:41:27 INFO SparkContext: Job finished: first at Chapter0802.scala:60, took 0.014408 s</strong></span>
<span class="strong"><strong>[1.0,0.0,0.0,0.0,0.0,211.3375]</strong></span>
<span class="strong"><strong>14/11/28 18:41:27 INFO SparkContext: Starting job: take at DecisionTreeMetadata.scala:66</strong></span>
<span class="strong"><strong>[..]</strong></span>
<span class="strong"><strong>14/11/28 18:41:28 INFO DecisionTree: Internal timing for DecisionTree:</strong></span>
<span class="strong"><strong>14/11/28 18:41:28 INFO DecisionTree:   init: 0.36408</strong></span>
<span class="strong"><strong>  total: 0.95518</strong></span>
<span class="strong"><strong>  extractNodeInfo: 7.3E-4</strong></span>
<span class="strong"><strong>  findSplitsBins: 0.249814</strong></span>
<span class="strong"><strong>  extractInfoForLowerLevels: 7.74E-4</strong></span>
<span class="strong"><strong>  findBestSplits: 0.565394</strong></span>
<span class="strong"><strong>  chooseSplits: 0.201012</strong></span>
<span class="strong"><strong>  aggregation: 0.362411</strong></span>
<span class="strong"><strong>5</strong></span>
<span class="strong"><strong>DecisionTreeModel classifier</strong></span>
<span class="strong"><strong>  If (feature 1 &lt;= 0.0)</strong></span>
<span class="strong"><strong>   If (feature 0 &lt;= 2.0)</strong></span>
<span class="strong"><strong>    If (feature 5 &lt;= 26.0)</strong></span>
<span class="strong"><strong>     If (feature 2 &lt;= 1.0)</strong></span>
<span class="strong"><strong>      If (feature 0 &lt;= 1.0)</strong></span>
<span class="strong"><strong>       Predict: 1.0</strong></span>
<span class="strong"><strong>      Else (feature 0 &gt; 1.0)</strong></span>
<span class="strong"><strong>       Predict: 1.0</strong></span>
<span class="strong"><strong>     Else (feature 2 &gt; 1.0)</strong></span>
<span class="strong"><strong>      Predict: 1.0</strong></span>
<span class="strong"><strong>    Else (feature 5 &gt; 26.0)</strong></span>
<span class="strong"><strong>     If (feature 2 &lt;= 1.0)</strong></span>
<span class="strong"><strong>      If (feature 5 &lt;= 38.0021)</strong></span>
<span class="strong"><strong>       Predict: 1.0</strong></span>
<span class="strong"><strong>      Else (feature 5 &gt; 38.0021)</strong></span>
<span class="strong"><strong>       Predict: 1.0</strong></span>
<span class="strong"><strong>     Else (feature 2 &gt; 1.0)</strong></span>
<span class="strong"><strong>      If (feature 5 &lt;= 79.42500000000001)</strong></span>
<span class="strong"><strong>       Predict: 1.0</strong></span>
<span class="strong"><strong>      Else (feature 5 &gt; 79.42500000000001)</strong></span>
<span class="strong"><strong>       Predict: 1.0</strong></span>
<span class="strong"><strong>   Else (feature 0 &gt; 2.0)</strong></span>
<span class="strong"><strong>    If (feature 5 &lt;= 25.4667)</strong></span>
<span class="strong"><strong>     If (feature 5 &lt;= 7.2292)</strong></span>
<span class="strong"><strong>      If (feature 5 &lt;= 7.05)</strong></span>
<span class="strong"><strong>       Predict: 1.0</strong></span>
<span class="strong"><strong>      Else (feature 5 &gt; 7.05)</strong></span>
<span class="strong"><strong>       Predict: 1.0</strong></span>
<span class="strong"><strong>     Else (feature 5 &gt; 7.2292)</strong></span>
<span class="strong"><strong>      If (feature 5 &lt;= 15.5646)</strong></span>
<span class="strong"><strong>       Predict: 0.0</strong></span>
<span class="strong"><strong>      Else (feature 5 &gt; 15.5646)</strong></span>
<span class="strong"><strong>       Predict: 1.0</strong></span>
<span class="strong"><strong>    Else (feature 5 &gt; 25.4667)</strong></span>
<span class="strong"><strong>     If (feature 5 &lt;= 38.0021)</strong></span>
<span class="strong"><strong>      If (feature 5 &lt;= 30.6958)</strong></span>
<span class="strong"><strong>       Predict: 0.0</strong></span>
<span class="strong"><strong>      Else (feature 5 &gt; 30.6958)</strong></span>
<span class="strong"><strong>       Predict: 0.0</strong></span>
<span class="strong"><strong>     Else (feature 5 &gt; 38.0021)</strong></span>
<span class="strong"><strong>      Predict: 0.0</strong></span>
<span class="strong"><strong>  Else (feature 1 &gt; 0.0)</strong></span>
<span class="strong"><strong>   If (feature 0 &lt;= 1.0)</strong></span>
<span class="strong"><strong>    If (feature 5 &lt;= 26.0)</strong></span>
<span class="strong"><strong>     If (feature 5 &lt;= 7.05)</strong></span>
<span class="strong"><strong>      If (feature 5 &lt;= 0.0)</strong></span>
<span class="strong"><strong>       Predict: 0.0</strong></span>
<span class="strong"><strong>      Else (feature 5 &gt; 0.0)</strong></span>
<span class="strong"><strong>       Predict: 0.0</strong></span>
<span class="strong"><strong>     Else (feature 5 &gt; 7.05)</strong></span>
<span class="strong"><strong>      Predict: 0.0</strong></span>
<span class="strong"><strong>    Else (feature 5 &gt; 26.0)</strong></span>
<span class="strong"><strong>     If (feature 5 &lt;= 30.6958)</strong></span>
<span class="strong"><strong>      If (feature 2 &lt;= 0.0)</strong></span>
<span class="strong"><strong>       Predict: 0.0</strong></span>
<span class="strong"><strong>      Else (feature 2 &gt; 0.0)</strong></span>
<span class="strong"><strong>       Predict: 0.0</strong></span>
<span class="strong"><strong>     Else (feature 5 &gt; 30.6958)</strong></span>
<span class="strong"><strong>      If (feature 2 &lt;= 1.0)</strong></span>
<span class="strong"><strong>       Predict: 0.0</strong></span>
<span class="strong"><strong>      Else (feature 2 &gt; 1.0)</strong></span>
<span class="strong"><strong>       Predict: 1.0</strong></span>
<span class="strong"><strong>   Else (feature 0 &gt; 1.0)</strong></span>
<span class="strong"><strong>    If (feature 2 &lt;= 0.0)</strong></span>
<span class="strong"><strong>     If (feature 5 &lt;= 38.0021)</strong></span>
<span class="strong"><strong>      If (feature 5 &lt;= 14.4583)</strong></span>
<span class="strong"><strong>       Predict: 0.0</strong></span>
<span class="strong"><strong>      Else (feature 5 &gt; 14.4583)</strong></span>
<span class="strong"><strong>       Predict: 0.0</strong></span>
<span class="strong"><strong>     Else (feature 5 &gt; 38.0021)</strong></span>
<span class="strong"><strong>      If (feature 0 &lt;= 2.0)</strong></span>
<span class="strong"><strong>       Predict: 0.0</strong></span>
<span class="strong"><strong>      Else (feature 0 &gt; 2.0)</strong></span>
<span class="strong"><strong>       Predict: 1.0</strong></span>
<span class="strong"><strong>    Else (feature 2 &gt; 0.0)</strong></span>
<span class="strong"><strong>     If (feature 5 &lt;= 26.0)</strong></span>
<span class="strong"><strong>      If (feature 2 &lt;= 1.0)</strong></span>
<span class="strong"><strong>       Predict: 0.0</strong></span>
<span class="strong"><strong>      Else (feature 2 &gt; 1.0)</strong></span>
<span class="strong"><strong>       Predict: 0.0</strong></span>
<span class="strong"><strong>     Else (feature 5 &gt; 26.0)</strong></span>
<span class="strong"><strong>      If (feature 0 &lt;= 2.0)</strong></span>
<span class="strong"><strong>       Predict: 0.0</strong></span>
<span class="strong"><strong>      Else (feature 0 &gt; 2.0)</strong></span>
<span class="strong"><strong>       Predict: 0.0</strong></span>

<span class="strong"><strong>14/11/28 18:41:28 INFO SparkContext: Starting job: reduce at Chapter0802.scala:79</strong></span>
<span class="strong"><strong>[..]</strong></span>
<span class="strong"><strong>14/11/28 18:41:28 INFO SparkContext: Job finished: count at Chapter0802.scala:79, took 0.077973 s</strong></span>
<span class="strong"><strong>Mean Squared Error = 0.200153</strong></span>
<span class="strong"><strong>14/11/28 18:41:28 INFO SparkContext: Starting job: aggregate at Chapter0802.scala:84</strong></span>
<span class="strong"><strong>[..]</strong></span>
<span class="strong"><strong>14/11/28 18:41:28 INFO SparkContext: Job finished: count at Chapter0802.scala:85, took 0.042592 s</strong></span>
<span class="strong"><strong>Accuracy = 79.98%</strong></span>
<span class="strong"><strong>*** Done ***</strong></span>
</pre></div><p>In the<a id="id414" class="indexterm"></a> real world, one would create a <code class="literal">training</code> and a <code class="literal">test</code> dataset and train the model on the <code class="literal">training</code> dataset and then predict on <a id="id415" class="indexterm"></a>the <code class="literal">test</code> dataset. Then we can calculate the <code class="literal">mse</code> and minimize it on various feature combinations, some of which could also be engineered features.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec33"></a>Clustering</h3></div></div></div><p>Spark MLlib has<a id="id416" class="indexterm"></a> implemented the k-means clustering<a id="id417" class="indexterm"></a> algorithm. The model training and prediction interfaces are similar to other machine learning algorithms. Let's see how it works by going through an example.</p><p>Let's use a sample data that has two dimensions <span class="emphasis"><em>x</em></span> and <span class="emphasis"><em>y</em></span>. The plot of the points would look like the following screenshot:</p><div class="mediaobject"><img src="graphics/4005_09_04.jpg" /></div><p>From the preceding graph, we can see that four clusters form one solution. Let's try with <span class="emphasis"><em>k=2</em></span> and <span class="emphasis"><em>k=4</em></span>. Let's <a id="id418" class="indexterm"></a>see how the Spark clustering<a id="id419" class="indexterm"></a> algorithm handles this dataset and the groupings:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark.SparkContext
import org.apache.spark.mllib.linalg.{Vector,Vectors}
import org.apache.spark.mllib.clustering.KMeans

object Chapter0803 {
  def parsePoints(inpLine : String) : Vector = {
    val values = inpLine.split(',')
    val x = values(0).toInt
    val y = values(1).toInt
    return Vectors.dense(x,y)
  }
  //

  def main(args: Array[String]): Unit = {
    val sc = new SparkContext("local","Chapter 8")
    println(s"Running Spark Version ${sc.version}")
    //
    val dataFile = sc.textFile("/Users/ksankar/bdtc-2014/cluster-points/cluster-points.csv")
    val points = dataFile.map(_.trim).filter( _.length &gt; 1).map(line =&gt; parsePoints(line))
    //
    println(points.count())
    //
    var numClusters = 2
    val numIterations = 20
    var mdlKMeans = KMeans.train(points, numClusters, numIterations)
    //
    println(mdlKMeans.clusterCenters)
    //
    var clusterPred = points.map(x=&gt;mdlKMeans.predict(x))
    var clusterMap = points.zip(clusterPred)
    //
    clusterMap.foreach(println)
    //
    clusterMap.saveAsTextFile("/Users/ksankar/bdtc-2014/cluster-points/2-cluster.csv")
    //
    // Now let us try 4 centers:
    //
    numClusters = 4
    mdlKMeans = KMeans.train(points, numClusters, numIterations)
    clusterPred = points.map(x=&gt;mdlKMeans.predict(x))
    clusterMap = points.zip(clusterPred)
    clusterMap.saveAsTextFile("/Users/ksankar/bdtc-2014/cluster-points/4-cluster.csv")
    clusterMap.foreach(println)
  }
}</pre></div><p>The<a id="id420" class="indexterm"></a> results of the run would be as shown in the next<a id="id421" class="indexterm"></a> screenshot (your run could give slightly different results):</p><div class="mediaobject"><img src="graphics/4005_09_05.jpg" /></div><p>The <span class="emphasis"><em>k=2</em></span> graph shown in the next screenshot looks as expected:</p><div class="mediaobject"><img src="graphics/4005_09_06.jpg" /></div><p>With <span class="emphasis"><em>k=4</em></span> the<a id="id422" class="indexterm"></a> results are as shown in the following<a id="id423" class="indexterm"></a> screenshot:</p><div class="mediaobject"><img src="graphics/4005_09_07.jpg" /></div><p>The plot shown in the following screenshot confirms that the clusters are obtained as expected. Spark does understand clustering!</p><div class="mediaobject"><img src="graphics/4005_09_08.jpg" /></div><p>Bear in <a id="id424" class="indexterm"></a>mind that the results could vary a little between<a id="id425" class="indexterm"></a> runs because the clustering algorithm picks the centers randomly and grows from there. With <span class="emphasis"><em>k=4</em></span>, the results are stable; but with <span class="emphasis"><em>k=2</em></span>, there is room for partitioning the points in different ways. Try it out a few times and see the results.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec34"></a>Recommendation</h3></div></div></div><p>The <a id="id426" class="indexterm"></a>recommendation algorithms fall under five <a id="id427" class="indexterm"></a>general mechanisms, namely, knowledge-based, demographic-based, content-based, collaborative filtering (item-based or user-based), and latent factor-based. Usually, the collaborative filtering is computationally intensive—Spark implements the <span class="strong"><strong>Alternating Least Square</strong></span> (<span class="strong"><strong>ALS</strong></span>) algorithm<a id="id428" class="indexterm"></a> authored by Yehuda Koren, available at <a class="ulink" href="http://dl.acm.org/citation.cfm?id=1608614" target="_blank">http://dl.acm.org/citation.cfm?id=1608614</a>. It is user-based collaborative <a id="id429" class="indexterm"></a>filtering using the method of learning latent factors, which can scale to a large dataset. Let's quickly use the <code class="literal">movielens</code> medium dataset to implement a recommendation using Spark.</p><p>There are some interesting RDD transformations. Apart from that, the code is not that complex, as shown next:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._ // for implicit conversations
import org.apache.spark.mllib.recommendation.Rating
import org.apache.spark.mllib.recommendation.ALS

object Chapter0804 {
  
  def parseRating1(line : String) : (Int,Int,Double,Int) = {
    //println(x)
    val x = line.split("::")
    val userId = x(0).toInt
    val movieId = x(1).toInt
    val rating = x(2).toDouble
    val timeStamp = x(3).toInt/10
    return (userId,movieId,rating,timeStamp)
  }
  //
  def parseRating(x : (Int,Int,Double,Int)) : Rating = {
    val userId = x._1
    val movieId = x._2
    val rating = x._3
    val timeStamp = x._4 // ignore
    return new Rating(userId,movieId,rating)
  }
  //</pre></div><p>Now<a id="id430" class="indexterm"></a> that we have the parsers in place, let's focus <a id="id431" class="indexterm"></a>on the main program, as shown next:</p><div class="informalexample"><pre class="programlisting">  def main(args: Array[String]): Unit = {
    val sc = new SparkContext("local","Chapter 8")
    println(s"Running Spark Version ${sc.version}")
    //
    val moviesFile = sc.textFile("/Users/ksankar/bdtc-2014/movielens/medium/movies.dat")
    val moviesRDD = moviesFile.map(line =&gt; line.split("::"))
    println(moviesRDD.count())
    //
    val ratingsFile = sc.textFile("/Users/ksankar/bdtc-2014/movielens/medium/ratings.dat")
    val ratingsRDD = ratingsFile.map(line =&gt; parseRating1(line))
    println(ratingsRDD.count())
    //
    ratingsRDD.take(5).foreach(println) // always check the RDD
    //
    val numRatings = ratingsRDD.count()
    val numUsers = ratingsRDD.map(r =&gt; r._1).distinct().count()
    val numMovies = ratingsRDD.map(r =&gt; r._2).distinct().count()
    println("Got %d ratings from %d users on %d movies.".
         format(numRatings, numUsers, numMovies))</pre></div><p>Split the dataset into <code class="literal">training</code>, <code class="literal">validation</code>, and <code class="literal">test</code>. We can use any random dataset. But here we will use the last digit of the timestamp:</p><div class="informalexample"><pre class="programlisting">val trainSet = ratingsRDD.filter(x =&gt; (x._4 % 10) &lt; 6).map(x=&gt;parseRating(x))
    val validationSet = ratingsRDD.filter(x =&gt; (x._4 % 10) &gt;= 6 &amp; (x._4 % 10) &lt; 8).map(x=&gt;parseRating(x))
    val testSet = ratingsRDD.filter(x =&gt; (x._4 % 10) &gt;= 8).map(x=&gt;parseRating(x))
    println("Training: "+ "%d".format(trainSet.count()) + 
      ", validation: " + "%d".format(validationSet.count()) + ", test: " + "%d".format(testSet.count()) + ".")
    //
    // Now train the model using the training set:
    val rank = 10
    val numIterations = 20
    val mdlALS = ALS.train(trainSet,rank,numIterations)
    //
    // prepare validation set for prediction
    //
    val userMovie = validationSet.map { 
      case Rating(user, movie, rate) =&gt;(user, movie)
    }
    //
    // Predict and convert to Key-Value PairRDD
    val predictions = mdlALS.predict(userMovie).map {
      case Rating(user, movie, rate) =&gt; ((user, movie), rate)
    }
    //
    println(predictions.count())
    predictions.take(5).foreach(println)
    //
    // Now convert the validation set to PairRDD:
    //
    val validationPairRDD = validationSet.map(r =&gt; ((r.user, r.product), r.rating))
    println(validationPairRDD.count())
    validationPairRDD.take(5).foreach(println)
    println(validationPairRDD.getClass())
    println(predictions.getClass())
    //
    // Now join the validation set with predictions.
    // Then we can figure out how good our recommendations are.
    // Tip:
    //   Need to import org.apache.spark.SparkContext._ 
    //   Then MappedRDD would be converted implicitly to PairRDD
    //
    val ratingsAndPreds = validationPairRDD.join(predictions) 
    println(ratingsAndPreds.count())
    ratingsAndPreds.take(3).foreach(println)
    //
    val mse = ratingsAndPreds.map(r =&gt; {
      math.pow((r._2._1 - r._2._2),2)
    }).reduce(_+_) / ratingsAndPreds.count()
    val rmse = math.sqrt(mse)
    println("MSE = %2.5f".format(mse) + " RMSE = %2.5f".format(rmse))
    println("** Done **")
  }
}</pre></div><p>The run<a id="id432" class="indexterm"></a> results, as shown in the next screenshot, are<a id="id433" class="indexterm"></a> obtained as expected:</p><div class="mediaobject"><img src="graphics/4005_09_09.jpg" /></div><p>Check the following screenshot as well:</p><div class="mediaobject"><img src="graphics/4005_09_10.jpg" /></div><a id="id434" class="indexterm"></a><p>Some more information is available at:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The <span class="emphasis"><em>Goodby MapReduce</em></span> article <a id="id435" class="indexterm"></a>from <a id="id436" class="indexterm"></a>Mahout News (<a class="ulink" href="https://mahout.apache.org/" target="_blank">https://mahout.apache.org/</a>)</p></li><li style="list-style-type: disc"><p><a class="ulink" href="https://spark.apache.org/docs/latest/mllib-guide.html" target="_blank">https://spark.apache.org/docs/latest/mllib-guide.html</a></p></li><li style="list-style-type: disc"><p>A Collaborative Filtering ALS paper (<a class="ulink" href="http://dl.acm.org/citation.cfm?id=1608614" target="_blank">http://dl.acm.org/citation.cfm?id=1608614</a>)</p></li><li style="list-style-type: disc"><p>A good presentation on decision trees (<a class="ulink" href="http://spark-summit.org/wp-content/uploads/2014/07/Scalable-Distributed-Decision-Trees-in-Spark-Made-Das-Sparks-Talwalkar.pdf" target="_blank">http://spark-summit.org/wp-content/uploads/2014/07/Scalable-Distributed-Decision-Trees-in-Spark-Made-Das-Sparks-Talwalkar.pdf</a>)</p></li><li style="list-style-type: disc"><p>A recommended hands-on exercise from Spark Summit 2014 (<a class="ulink" href="https://databricks-training.s3.amazonaws.com/movie-recommendation-with-mllib.html" target="_blank">https://databricks-training.s3.amazonaws.com/movie-recommendation-with-mllib.html</a>)</p></li></ul></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec48"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we looked at the most common machine learning algorithms. Naturally, ML is a vast subject and requires lot more study, experimentation, and practical experience on interesting data science problems. Two books that are relevant to Spark Machine Learning are Packt's own book <span class="emphasis"><em>Machine Learning with Spark</em></span>, <span class="emphasis"><em>Nick Pentreath</em></span>, and O'Reilly's <span class="emphasis"><em>Advanced Analytics with Spark</em></span>, <span class="emphasis"><em>Sandy Ryza</em></span>, <span class="emphasis"><em>Uri Laserson</em></span>, <span class="emphasis"><em>Sean Owen</em></span>, and <span class="emphasis"><em>Josh Wills</em></span>. Both are excellent books that you can refer to.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch10"></a>Chapter 10. Testing</h2></div></div></div><p>Writing effective software without tests is quite challenging. Effective testing, especially in cases with slow end-to-end running times, such as distributed systems, can help improve developer effectiveness greatly. This chapter isn't going to try to convince you that you should be testing; however, if you really want to ride without a seat belt, that's fine too.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec49"></a>Testing in Java and Scala</h2></div></div><hr /></div><p>For the sake of simplicity, this <a id="id437" class="indexterm"></a>chapter covers using ScalaTest and JUnit as testing <a id="id438" class="indexterm"></a>libraries. ScalaTest can be used to test both Scala and Java code and is the testing library currently used in Spark. To use ScalaTest with sbt, you need to add this to the <code class="literal">.sbt</code> file: <code class="literal">libraryDependencies += "org.scalatest" % "scalatest_2.10" % "2.0" % "test"</code>. JUnit is a popular testing framework for Java.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec35"></a>Making your code testable</h3></div></div></div><p>If you have code that can be isolated<a id="id439" class="indexterm"></a> from the RDD interaction or SparkContext interaction, that code can be tested using standard methodologies. While it can be quite convenient to use anonymous functions when writing Spark code, you cannot test them independently without the expensive overhead of setting up SparkContext. So the best practice is to write named functions. For example, in your CSV parser, you could take the following code:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Scala code could be the following:</p><div class="informalexample"><pre class="programlisting">val splitLines = inFile.map(line =&gt; {
    val reader = new CSVReader(new StringReader(line))
    reader.readNext().map(_.toDouble)
  }</pre></div></li><li style="list-style-type: disc"><p>Java code could be the following:</p><div class="informalexample"><pre class="programlisting">   JavaRDD&lt;Integer[]&gt; splitLines = inFile.flatMap(new FlatMapFunction&lt;String, Integer[]&gt; (){public Iterable&lt;Integer[]&gt; call(String line) {ArrayList&lt;Integer[]&gt; result = new ArrayList&lt;Integer[]&gt;();
          try {CSVReader reader = new CSVReader(new StringReader(line));
              String[] parsedLine = reader.readNext();
              Integer[] intLine = new Integer[parsedLine.length];
              for (int i = 0; i &lt; parsedLine.length; i++) {intLine[i] = Integer.parseInt(parsedLine[i]);
              }
              result.add(intLine);
          } catch (Exception e) {errors.add(1);
          }
          return result;
       }
   }
  );</pre></div></li></ul></div><p>Instead of this, you could <a id="id440" class="indexterm"></a>write the code as shown next:</p><div class="informalexample"><pre class="programlisting"> def parseLine(line: String): Array[Double] = {
      val reader = new CSVReader(new StringReader(line))
      reader.readNext().map(_.toDouble)
  }</pre></div><p>Alternatively, in Java, you could write the code as shown here:</p><div class="informalexample"><pre class="programlisting">public class JavaLoadCsvTestable {
    public static class ParseLine extends Function&lt;String, Integer[]&gt; {
    public Integer[] call(String line) throws Exception {
        CSVReader reader = new CSVReader(new StringReader(line));
        String[] parsedLine = reader.readNext();
        Integer[] intLine = new Integer[parsedLine.length];
        for (int i = 0; i &lt; parsedLine.length; i++) {
          intLine[i] = Integer.parseInt(parsedLine[i]);
        }
        return intLine;
    }
  }
}</pre></div><p>You can then test it without having to worry about any Spark specific setup or logic as shown in the following code:</p><div class="informalexample"><pre class="programlisting">import org.scalatest.FunSuite
import org.scalatest.matchers.ShouldMatchers

class TestableLoadCsvExampleSuite extends FunSuite with ShouldMatchers {
    test("should parse a csv line with numbers") {
      TestableLoadCsvExample.parseLine("1,2") should equal (Array[Double](1.0,2.0))
      TestableLoadCsvExample.parseLine("100,-1,1,2,2.5") should equal (Array[Double](100,-1,1.0,2.0,2.5))
    }
    test("should error if there is a non-number") {
      evaluating { TestableLoadCsvExample.parseLine("pandas")  } should produce [NumberFormatException]
    }
}</pre></div><p>Alternatively, to test the <a id="id441" class="indexterm"></a>Java code, you would do something like the following code (note that the test is still written in Scala; don't worry as we will look at JUnit tests later):</p><div class="informalexample"><pre class="programlisting">class JavaLoadCsvExampleSuite extends FunSuite with ShouldMatchers {

    test("should parse a csv line with numbers") {
      val parseLine = new JavaLoadCsvTestable.ParseLine();
      parseLine.call("1,2") should equal (Array[Integer](1,2))
      parseLine.call("100,-1,1,2,2") should equal (Array[Integer](100,-1,1,2,2))
    }
    test("should error if there is a non-integer") {
      val parseLine = new JavaLoadCsvTestable.ParseLine();
      evaluating { parseLine.call("pandas")  } should produce [NumberFormatException]
      evaluating {parseLine.call("100,-1,1,2.2,2") should equal (Array[Integer](100,-1,1,2,2)) } should produce [NumberFormatException]
    }
}</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec36"></a>Testing interactions with SparkContext</h3></div></div></div><p>You may, however, remember<a id="id442" class="indexterm"></a> that you later extended the CSV parser to<a id="id443" class="indexterm"></a> increment counters on invalid input to gracefully handle failures. To verify that behavior, you could provide mock counters and other mock objects for the Spark components you use. You are restricted to only test the parts of the code that do not depend on Spark. Instead, you could re-factor the code to make the core testable without Spark and to do a more complete test using a provided SparkContext, as shown:</p><div class="informalexample"><pre class="programlisting">object MoreTestableLoadCsvExample {
  def parseLine(line: String): Array[Double] = {
    val reader = new CSVReader(new StringReader(line))
    reader.readNext().map(_.toDouble)
  }
  def handleInput(invalidLineCounter: Accumulator[Int], inFile: RDD[String]): RDD[Double] = {
    val numericData = inFile.flatMap(line =&gt; {
      try {
    Some(parseLine(line))
      } catch {
    case _ =&gt; {
      invalidLineCounter += 1
      None
    }
      }
    })
    numericData.map(row =&gt; row.sum)
  }

  def main(args: Array[String]) {
    if (args.length != 2) {
      System.err.println("Usage: TestableLoadCsvExample &lt;master&gt; &lt;inputfile&gt;")
      System.exit(1)
    }
    val master = args(0)
    val inputFile = args(1)
    val sc = new SparkContext(master, "Load CSV Example",
                      System.getenv("SPARK_HOME"),
                      Seq(System.getenv("JARS")))
    sc.addFile(inputFile)
    val inFile = sc.textFile(inputFile)
    val invalidLineCounter = sc.accumulator(0)
    val summedData = handleInput(invalidLineCounter, inFile)
    println(summedData.collect().mkString(","))
    println("Errors: "+invalidLineCounter)
    println(summedData.stats())
  }

}</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note08"></a>Note</h3><p>This does <a id="id444" class="indexterm"></a>have the downside of requiring that your tests run serially, else sbt (or other build infrastructure) may try to launch multiple<a id="id445" class="indexterm"></a> Spark contexts at the same time, which will cause confusing error messages. We can force the tests to execute sequentially in sbt with <code class="literal">parallelExecution in Test := false</code>.</p></div><p>We test this by using the following code:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark._
import org.apache.spark.SparkContext._
import org.scalatest.FunSuite
import org.scalatest.matchers.ShouldMatchers

class MoreTestableLoadCsvExampleSuite extends FunSuite with ShouldMatchers {
  test("summ data on input") {
    val sc = new SparkContext("local", "Load CSV Example")
    val counter = sc.accumulator(0)
    val input = sc.parallelize(List("1,2","1,3"))
    val result = MoreTestableLoadCsvExample.handleInput(counter, input)
    result.collect() should equal (Array[Int](3,4))
  }
  test("should parse a csv line with numbers") {
    MoreTestableLoadCsvExample.parseLine("1,2") should equal (Array[Double](1.0,2.0))
    MoreTestableLoadCsvExample.parseLine("100,-1,1,2,2.5") should equal (Array[Double](100,-1,1.0,2.0,2.5))
  }
  test("should error if there is a non-number") {
    evaluating { MoreTestableLoadCsvExample.parseLine("pandas")  } should produce [NumberFormatException]
  }
}</pre></div><p>In Java, you<a id="id446" class="indexterm"></a> can test <a id="id447" class="indexterm"></a>with the following code:</p><div class="informalexample"><pre class="programlisting">public class JavaLoadCsvMoreTestable {
    public static class ParseLineWithAcc extends FlatMapFunction&lt;String, Integer[]&gt; {
    Accumulator&lt;Integer&gt; acc;
    ParseLineWithAcc(Accumulator&lt;Integer&gt; acc) {
        this.acc = acc;
    }
   public Iterable&lt;Integer[]&gt; call(String line) throws Exception {
        ArrayList&lt;Integer[]&gt; result = new ArrayList&lt;Integer[]&gt;();
        try {
            CSVReader reader = new CSVReader(new StringReader(line));
        String[] parsedLine = reader.readNext();
        Integer[] intLine = new Integer[parsedLine.length];
        for (int i = 0; i &lt; parsedLine.length; i++) {
            intLine[i] = Integer.parseInt(parsedLine[i]);
        }
        result.add(intLine);
        } catch (Exception e) {
        acc.add(1);
        }
        return result;
    }
    }
    public static JavaDoubleRDD processData(Accumulator&lt;Integer&gt; acc, JavaRDD&lt;String&gt; input) {
    JavaRDD&lt;Integer[]&gt; splitLines = input.flatMap(new ParseLineWithAcc(acc));
    JavaDoubleRDD summedData = splitLines.map(new DoubleFunction&lt;Integer[]&gt;() {
         public Double call(Integer[] in) {
            Double ret = 0.;
            for (int i = 0; i &lt; in.length; i++) {
                ret += in[i];
            }
            return ret;
        }
      }
    );
return summedData;
    }</pre></div><p>You can test <a id="id448" class="indexterm"></a>this in Scala code as shown here (note that we add an<a id="id449" class="indexterm"></a> invalid input for the counter here):</p><div class="informalexample"><pre class="programlisting">class JavaLoadCsvMoreTestableSuite extends FunSuite with ShouldMatchers {
  test("sum data on input") {
    val sc = new JavaSparkContext("local", "Load Java CSV test")
    val counter: Accumulator[Integer] = sc.intAccumulator(0)
    val input: JavaRDD[String] = sc.parallelize(List("1,2","1,3","murh"))
    val javaLoadCsvMoreTestable = new JavaLoadCsvMoreTestable();
    val resultRDD = JavaLoadCsvMoreTestable.processData(counter,input)
    resultRDD.cache();
    val resultCount = resultRDD.count()
    val result = resultRDD.collect().toArray()
    resultCount should equal (2)
    result should equal (Array[Double](3.0, 4.0))
    counter.value should equal (1)
    sc.stop()
  }
}</pre></div><p>You can test<a id="id450" class="indexterm"></a> this in<a id="id451" class="indexterm"></a> Java with Junit4, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">package pandaspark.examples;

import org.apache.spark.*;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaDoubleRDD;
import org.scalatest.FunSuite;
import org.scalatest.matchers.ShouldMatchers;

import static org.junit.Assert.assertEquals;
import org.junit.Test;
import org.junit.Ignore;
import org.junit.runner.RunWith;
import org.junit.runners.JUnit4;

import java.util.Arrays;
import java.util.List;
import java.util.ArrayList;


@RunWith(JUnit4.class)
public class JavaLoadCsvMoreTestableSuiteJunit {
    @Test
    public void testSumDataOnInput() {
    JavaSparkContext sc = new JavaSparkContext("local", "Load Java CSV test");
    Accumulator&lt;Integer&gt; counter = sc.intAccumulator(0);
    String[] inputArray = {"1,2","1,3","murh"};
    JavaRDD&lt;String&gt; input = sc.parallelize(Arrays.asList(inputArray));
    JavaDoubleRDD resultRDD = JavaLoadCsvMoreTestable.processData(counter, input);
    long resultCount = resultRDD.count();
    assertEquals(resultCount, 2);
    int errors = counter.value();
    assertEquals(errors, 1);
    sc.stop();
  }
}</pre></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec50"></a>Testing in Python</h2></div></div><hr /></div><p>Python testing of Spark<a id="id452" class="indexterm"></a> is very similar in concept to testing in Java and Scala, but the testing libraries are a bit different. PySpark<a id="id453" class="indexterm"></a> uses both doctest and unittest to test itself. doctest makes it easy to create tests based on the expected output of code run in the Python interpreter. We can run the tests using the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>export SPARK_TESTING=1</strong></span>
<span class="strong"><strong>export PYSPARK_DOC_TEST=1</strong></span>
<span class="strong"><strong>bin/pyspark [pathtocode]</strong></span>
</pre></div><p>By taking the <code class="literal">wordcount.py</code> example from Spark and factoring out <code class="literal">countWords</code>, you can test the word count functionality using doctest. Some doctest examples are shown next:</p><div class="informalexample"><pre class="programlisting">"""
&gt;&gt;&gt; from pyspark.context import SparkContext
&gt;&gt;&gt; sc = SparkContext('local', 'test')
&gt;&gt;&gt; b = sc.parallelize(["pandas are awesome","and ninjas are also awesome"])
&gt;&gt;&gt; countWords(b)
[('also', 1), ('and', 1), ('are', 2), ('awesome', 2), ('ninjas', 1), ('pandas', 1)]
"""

import sys
from operator import add

from pyspark import SparkContext

def countWords(lines):
    counts = lines.flatMap(lambda x: x.split(' ')) \
                  .map(lambda x: (x, 1)) \
                  .reduceByKey(add)
    return sorted(counts.collect())


if __name__ == "__main__":
    if len(sys.argv) &lt; 3:
        print &gt;&gt; sys.stderr, \
            "Usage: PythonWordCount &lt;master&gt; &lt;file&gt;"
        exit(-1)
    sc = SparkContext(sys.argv[1], "PythonWordCount")
    lines = sc.textFile(sys.argv[2], 1)
    output = countWords(lines)
    for (word, count) in output:
        print "%s : %i" % (word, count)</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note09"></a>Note</h3><p><span class="strong"><strong>Note about doctest</strong></span></p><p>You put the test in between<a id="id454" class="indexterm"></a> triple quotes. The testing code is prefixed with <code class="literal">&gt;&gt;&gt;</code> as if it's running in the Python shell. The expected output that would be seen is added exactly as if it's returned in the Python shell.</p></div><p>We can also test <a id="id455" class="indexterm"></a>something similar to our Java and Scala programs, as shown next:</p><div class="informalexample"><pre class="programlisting">"""
&gt;&gt;&gt; from pyspark.context import SparkContext
&gt;&gt;&gt; sc = SparkContext('local', 'test')
&gt;&gt;&gt; b = sc.parallelize(["1,2","1,3"])
&gt;&gt;&gt; handleInput(b)
[3, 4]
"""

import sys
from operator import add

from pyspark import SparkContext
def handleInput(lines):
    data = lines.map(lambda x: sum(map(int, x.split(','))))
    return sorted(data.collect())


if __name__ == "__main__":
    if len(sys.argv) &lt; 3:
        print &gt;&gt; sys.stderr, \
            "Usage: PythonLoadCsv &lt;master&gt; &lt;file&gt;"
        exit(-1)
    sc = SparkContext(sys.argv[1], "PythonLoadCsv")
    lines = sc.textFile(sys.argv[2], 1)
    output = handleInput(lines)
    for sum in output:
        print sum</pre></div><a id="id456" class="indexterm"></a><p>Some more information can be found at the following sites:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><a class="ulink" href="http://blog.quantifind.com/posts/spark-unit-test/" target="_blank">http://blog.quantifind.com/posts/spark-unit-test/</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://www.scalatest.org/" target="_blank">http://www.scalatest.org/</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://junit.org/" target="_blank">http://junit.org/</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://docs.python.org/2/library/unittest.html" target="_blank">http://docs.python.org/2/library/unittest.html</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://docs.python.org/2/library/doctest.html" target="_blank">http://docs.python.org/2/library/doctest.html</a></p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec51"></a>Summary</h2></div></div><hr /></div><p>This chapter discussed how to structure your code so that it is testable as well as the testing framework that is used within Spark. Effective testing can save large amounts of debugging time, which can be especially painful in large distributed systems. In the next chapter, we will look at some tips and tricks such as tuning and securing Spark.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch11"></a>Chapter 11. Tips and Tricks</h2></div></div></div><p>As discussed in the earlier chapters, you have the tools to build and test Spark jobs as well as set up a Spark cluster to run them on, so now it's time to figure out how to make the most of your time as a Spark developer. The <a id="id457" class="indexterm"></a>Spark documentation includes good tips on tuning and is available at <a class="ulink" href="http://spark.apache.org/docs/latest/tuning.html" target="_blank">http://spark.apache.org/docs/latest/tuning.html</a>.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch11lvl1sec52"></a>Where to find logs</h2></div></div><hr /></div><p>Spark has very useful logs to <a id="id458" class="indexterm"></a>figure out what's going on when things are not going as expected. Spark keeps a per machine log on each machine by default in the <code class="literal">SPARK_HOME/work</code> subdirectory. Spark's web UI provides a convenient place to see <code class="literal">STDOUT</code> and <code class="literal">STDERR</code> of each job, running and completed jobs, separated out per worker.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch11lvl1sec53"></a>Concurrency limitations</h2></div></div><hr /></div><p>Spark's concurrency for <a id="id459" class="indexterm"></a>operations is limited by the number of partitions. Conversely, having too many partitions can cause excess overhead by launching too many tasks. If you have too many partitions, you can shrink it by using the <code class="literal">coalesce(numPartitions,shuffle)</code> method. The <code class="literal">coalesce</code> method is a good method to pack and rebalance your RDDs (for example, after a filter operation where you have less data after the action). If the new number of partitions is more than what you have now, set <code class="literal">shuffle=True</code>, else set <code class="literal">shuffle=false</code>. While creating a new RDD, you can specify the number of partitions to be used. Also, the grouping/joining mechanism on RDDs of pairs can take the number of partitions or a custom <code class="literal">partitioner</code> class. The default number of partitions for new RDDs is controlled by <code class="literal">spark.default.parallelism</code>, which also controls the number of tasks used by <code class="literal">groupByKey</code> and other shuffle operations that need shuffling.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec37"></a>Memory usage and garbage collection</h3></div></div></div><p>To measure the <a id="id460" class="indexterm"></a>impact of garbage collection, you can ask the JVM to print details about the garbage collection. You can do this by adding <code class="literal">-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps</code> to your <code class="literal">SPARK_JAVA_OPTS</code> in <code class="literal">conf/spark-env.sh</code>. You can also include the <code class="literal">-Xloggc</code> option to print the log messages to a separate file so that log messages are kept separate. The details will then be printed to the standard out when you run your job, which will be available as described in the first section of this chapter.</p><p>If you find that your Spark cluster uses too much time collecting garbage, you can reduce the amount of space used for RDD caching by changing <code class="literal">spark.storage.memoryFraction</code>; here, the default is <code class="literal">0.6</code>. If you are planning to run Spark for a long time on a cluster, you may wish to enable <code class="literal">spark.cleaner.ttl</code>. By default, Spark does not clean up any metadata (stages generated, tasks generated, and so on); set this to a non-zero value in seconds to clean up the metadata after that length of time. The <a id="id461" class="indexterm"></a>documentation page (<a class="ulink" href="https://spark.apache.org/docs/latest/configuration.html" target="_blank">https://spark.apache.org/docs/latest/configuration.html</a>) has the default settings and details about all the configuration options.</p><p>You can also control the RDD storage level if you find that you use too much memory. I usually use <code class="literal">top</code> to see the memory consumption of the processes. If your RDDs don't fit within memory and you still wish to cache them, you can try using a different storage level shown as follows (also check the<a id="id462" class="indexterm"></a> documentation page for the latest information on RDD persistence options at <a class="ulink" href="http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence" target="_blank">http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence</a>):</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><code class="literal">MEMORY_ONLY</code>: This stores the entire RDD in memory if it can, which is the default</p></li><li style="list-style-type: disc"><p><code class="literal">MEMORY_AND_DISK</code>: This stores each partition in memory if it can fit; else it stores it on disk</p></li><li style="list-style-type: disc"><p><code class="literal">DISK_ONLY</code>: This stores each partition on disk regardless of whether it can fit in memory</p></li></ul></div><p>These options are set when you call the persist function (<code class="literal">rdd.persist()</code>) on your RDD. By default, the RDDs are stored in a deserialized form, which requires less parsing. We can save space by adding <code class="literal">_SER</code> to the storage level (for example, <code class="literal">MEMORY_ONLY_SER</code>, <code class="literal">MEMORY_AND_DISK_SER</code>), in which case Spark will serialize the data to be stored, which normally saves some space but increases the execution time.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec38"></a>Serialization</h3></div></div></div><p>Spark supports different serialization<a id="id463" class="indexterm"></a> mechanisms; the choice is a trade-off between speed, space efficiency, and full support of all Java objects. If you are using the serializer to cache your RDDs, you should strongly consider a fast serializer. The default serializer uses Java's default serialization. The KyroSerializer is much faster and generally uses about one tenth of the memory as the default serializer. You can switch the serializer by setting <code class="literal">spark.serializer</code> to <code class="literal">spark.KryoSerializer</code>. If you want to use KyroSerializer, you need to make sure that the classes are serializable by KyroSerializer. Spark provides a trait <code class="literal">KryoRegistrator</code>, which you can extend to register your classes with Kyro, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">class Reigstrer extends spark.KyroRegistrator {
    override def registerClasses(kyro: Kyro) {
              kyro.register(classOf[MyClass])
    }
}</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note10"></a>Note</h3><p>Take a look at <a class="ulink" href="https://code.google.com/p/kryo/#Quickstart" target="_blank">https://code.google.com/p/kryo/#Quickstart</a> to figure out how to write custom serializers<a id="id464" class="indexterm"></a> for your classes if you need something customized. You can substantially decrease the amount of space used for your objects by customizing your serializers. For example, rather than writing out the full class name, you can give them an integer ID by calling <code class="literal">kyro.register(classOf[MyClass],100)</code>.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec39"></a>IDE integration</h3></div></div></div><p>For Emacs users, the ENSIME sbt plugin is a <a id="id465" class="indexterm"></a>good addition. <span class="strong"><strong>ENhanced Scala Interaction Mode for Emacs</strong></span> (<span class="strong"><strong>ENSIME</strong></span>) provides many features that are<a id="id466" class="indexterm"></a> available in IDEs such as error checking and symbol inspection. You can install the latest ENSIME<a id="id467" class="indexterm"></a> from <a class="ulink" href="https://github.com/aemoncannon/ensime/downloads" target="_blank">https://github.com/aemoncannon/ensime/downloads</a> (make sure you choose the one that matches your Scala version). Or, you can run the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>wget https://github.com/downloads/aemoncannon/ensime/ ensime_2.10.0-RC3-0.9.8.2.tar.gz</strong></span>

<span class="strong"><strong>tar -xvf ensime_2.10.0-RC3-0.9.8.2.tar.gz </strong></span>
</pre></div><p>In your <code class="literal">.emacs</code>, add this:</p><div class="informalexample"><pre class="programlisting">;; Load the ensime lisp code...
(add-to-list 'load-path "ENSIME_ROOT/elisp/")
(require 'ensime)
;; This step causes the ensime-mode to be started whenever;; scala-mode is started for a buffer. You may have to customize ;; this step if you're not using the standard scala mode.
(add-hook 'scala-mode-hook 'ensime-scala-mode-hook)</pre></div><p>You can then add the ENSIME sbt plugin to your project (in <code class="literal">project/plugins.sbt</code>):</p><div class="informalexample"><pre class="programlisting">addSbtPlugin("org.ensime" % "ensime-sbt-cmd" % "0.1.0")</pre></div><p>You should then run the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sbt</strong></span>
<span class="strong"><strong>&gt; ensime generate</strong></span>
</pre></div><p>If you are using Git, you <a id="id468" class="indexterm"></a>will probably want to add <code class="literal">.ensime</code> to the <code class="literal">.gitignore</code> file if it isn't already present.</p><p>If you have an IntelliJ, a similar plugin exists called sbt-idea, which can be used to generate IntelliJ idea files. You can add the IntelliJ sbt plugin to your project (in <code class="literal">project/plugins.sbt</code>) like this:</p><div class="informalexample"><pre class="programlisting">addSbtPlugin("com.github.mpeltonen" % "sbt-idea" % "1.5.1")</pre></div><p>You should then run the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sbt</strong></span>
<span class="strong"><strong>&gt; gen-idea</strong></span>
</pre></div><p>This will generate the idea file, which can be loaded into IntelliJ.</p><p>Eclipse users can also use sbt to generate Eclipse project files with the sbteclipse plugin. You can add the Eclipse sbt plugin to your project (in <code class="literal">project/plugins.sbt</code>) like this:</p><div class="informalexample"><pre class="programlisting">addSbtPlugin("com.typesafe.sbteclipse" % "sbteclipse-plugin" % "2.3.0")</pre></div><p>You should then run the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sbt</strong></span>
<span class="strong"><strong>&gt; eclipse</strong></span>
</pre></div><p>This will generate the Eclipse project files and you can then import them into your Eclipse project using the Import Wizard in Eclipse. Eclipse users might also find the spark-plug project useful, which can be used to launch clusters from within Eclipse.</p><p>An import step is to add <code class="literal">spark-assembly-1.2.0-hadoop2.6.0.jar</code> in your Java build path or Maven dependency. Pay attention so you match the Spark version number (1.2.0) with the Hadoop<a id="id469" class="indexterm"></a> version number (2.6.0).</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch11lvl1sec54"></a>Using Spark with other languages</h2></div></div><hr /></div><p>If you find yourself wanting <a id="id470" class="indexterm"></a>to work with your RDD in another language, there are a few options available for you. From Java/Scala you can try using JNI, and with Python you can use the FFI. Sometimes however, you will want to work with a language that isn't C or work with an already compiled program. In that case, the easiest thing to do is to use the pipe interface that is available in all three of the APIs. The stream API works by taking the RDD and serializing it to strings and then piping it to the specified program. If your data happens to be plain strings, this is very convenient, but if it's not so, you will need to serialize your data in such a way that it can be understood on either side. JSON or protocol buffers can be good options for this depending on how structured your data is.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch11lvl1sec55"></a>A quick note on security</h2></div></div><hr /></div><p>Another important consideration in your Spark setup is <a id="id471" class="indexterm"></a>security. If you are using Spark on EC2 with the default scripts, you will notice that the access to your Spark cluster is restricted. This is a good idea to do even if you aren't running inside of EC2 since your Spark cluster will likely have access to the data you would rather not share with the world (and even if it doesn't have it, you probably don't want to allow arbitrary code execution by strangers). If your Spark cluster is already on a private network, that is great, otherwise you should talk with your system administrator about setting up some IPtables rules to restrict access.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch11lvl1sec56"></a>Community developed packages</h2></div></div><hr /></div><p>A new package index site<a id="id472" class="indexterm"></a> (<a class="ulink" href="http://spark-packages.org/" target="_blank">http://spark-packages.org/</a>) has a lot of <a id="id473" class="indexterm"></a>packages and libraries that work with Apache Spark. It's an essential site to visit and make use of.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch11lvl1sec57"></a>Mailing lists</h2></div></div><hr /></div><p>Probably the most useful tip to finish this chapter with is<a id="id474" class="indexterm"></a> that the Spark user's mailing list is an excellent source of up-to-date information about other people's experiences in using Spark. The best place to get information on meetups, slides, and so forth is <a class="ulink" href="https://spark.apache.org/community.html" target="_blank">https://spark.apache.org/community.html</a>. The two Spark users mailing lists are <code class="email">&lt;<a class="email" href="mailto:user@spark.apache.org">user@spark.apache.org</a>&gt;</code> and <code class="email">&lt;<a class="email" href="mailto:dev@spark.apache.org">dev@spark.apache.org</a>&gt;</code>.</p><a id="id475" class="indexterm"></a><p>Some more information can be found at the following sites:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><a class="ulink" href="http://blog.quantifind.com/posts/logging-post/" target="_blank">http://blog.quantifind.com/posts/logging-post/</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://jawher.net/2011/01/17/scala-development-environment-emacs-sbt-ensime/" target="_blank">http://jawher.net/2011/01/17/scala-development-environment-emacs-sbt-ensime/</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="https://www.assembla.com/spaces/liftweb/wiki/Emacs-ENSIME" target="_blank">https://www.assembla.com/spaces/liftweb/wiki/Emacs-ENSIME</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="https://github.com/shivaram/spark-ec2/blob/master/ganglia/init.sh" target="_blank">https://github.com/shivaram/spark-ec2/blob/master/ganglia/init.sh</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="https://spark.apache.org/docs/latest/tuning.html" target="_blank">https://spark.apache.org/docs/latest/tuning.html</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://spark.apache.org/docs/latest/running-on-mesos.html" target="_blank">http://spark.apache.org/docs/latest/running-on-mesos.html</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://kryo.googlecode.com/svn/api/v2/index.html" target="_blank">http://kryo.googlecode.com/svn/api/v2/index.html</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="https://code.google.com/p/kryo/" target="_blank">https://code.google.com/p/kryo/</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://scala-ide.org/download/current.html" target="_blank">http://scala-ide.org/download/current.html</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://syndeticlogic.net/?p=311" target="_blank">http://syndeticlogic.net/?p=311</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://mail-archives.apache.org/mod_mbox/incubator-spark-user/" target="_blank">http://mail-archives.apache.org/mod_mbox/incubator-spark-user/</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="https://groups.google.com/forum/?fromgroups#!forum/spark-users" target="_blank">https://groups.google.com/forum/?fromgroups#!forum/spark-users</a></p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch11lvl1sec58"></a>Summary</h2></div></div><hr /></div><p>That wraps up some common things that you can use to help improve your Spark development experience. I wish you the best of luck with your Spark projects; now go and solve some fun problems! :)</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">

    <div id="backindex">
      <h1 class="title">Index</h1>
      <h2>A</h2>
      <ul>
        <li>accumulate<ul><li>about / <a href="#ch06lvl1sec37" title="Manipulating your RDD in Scala and Java" class="link">Manipulating your RDD in Scala and Java</a></li></ul></li>
        <li>Alternating Least Square (ALS) algorithm<ul><li>about / <a href="#ch09lvl1sec47" title="Recommendation" class="link">Recommendation</a></li><li>reference link / <a href="#ch09lvl1sec47" title="Recommendation" class="link">Recommendation</a></li></ul></li>
        <li>Amazon Machine Images (AMI) / <a href="#ch01lvl1sec13" title="Running Spark on EC2 with the scripts" class="link">Running Spark on EC2 with the scripts</a></li>
        <li>architecture, Spark SQL<ul><li>about / <a href="#ch07lvl1sec40" title="The Spark SQL architecture" class="link">The Spark SQL architecture</a></li></ul></li>
      </ul>
      <h2>B</h2>
      <ul>
        <li>basic statistics, Spark MLlib examples<ul><li>about / <a href="#ch09lvl1sec47" title="Basic statistics" class="link">Basic statistics</a></li></ul></li>
        <li>broadcast<ul><li>about / <a href="#ch06lvl1sec37" title="Manipulating your RDD in Scala and Java" class="link">Manipulating your RDD in Scala and Java</a></li></ul></li>
      </ul>
      <h2>C</h2>
      <ul>
        <li>Chef<ul><li>about / <a href="#ch01lvl1sec14" title="Deploying Spark with Chef (Opscode)" class="link">Deploying Spark with Chef (Opscode)</a></li><li>URL / <a href="#ch01lvl1sec14" title="Deploying Spark with Chef (Opscode)" class="link">Deploying Spark with Chef (Opscode)</a></li><li>Spark, deploying with / <a href="#ch01lvl1sec14" title="Deploying Spark with Chef (Opscode)" class="link">Deploying Spark with Chef (Opscode)</a></li><li>URL, for cookbook / <a href="#ch01lvl1sec14" title="Deploying Spark with Chef (Opscode)" class="link">Deploying Spark with Chef (Opscode)</a></li></ul></li>
        <li>classification, Spark MLlib examples<ul><li>about / <a href="#ch09lvl1sec47" title="Classification" class="link">Classification</a></li></ul></li>
        <li>clustering, Spark MLlib examples<ul><li>about / <a href="#ch09lvl1sec47" title="Clustering" class="link">Clustering</a></li></ul></li>
        <li>code testable<ul><li>making / <a href="#ch10lvl1sec49" title="Making your code testable" class="link">Making your code testable</a></li></ul></li>
        <li>commands, quick start<ul><li>URL / <a href="#ch02lvl1sec21" title="Running Spark shell in Python" class="link">Running Spark shell in Python</a></li></ul></li>
        <li>community developed packages<ul><li>about / <a href="#ch11lvl1sec56" title="Community developed packages" class="link">Community developed packages</a></li></ul></li>
        <li>concurrency, limitations<ul><li>about / <a href="#ch11lvl1sec53" title="Concurrency limitations" class="link">Concurrency limitations</a></li><li>memory usage, and garbage collection / <a href="#ch11lvl1sec53" title="Memory usage and garbage collection" class="link">Memory usage and garbage collection</a></li><li>serialization / <a href="#ch11lvl1sec53" title="Serialization" class="link">Serialization</a></li><li>IDE integration / <a href="#ch11lvl1sec53" title="IDE integration" class="link">IDE integration</a></li></ul></li>
        <li>custom serializers<ul><li>references / <a href="#ch11lvl1sec53" title="Serialization" class="link">Serialization</a></li></ul></li>
      </ul>
      <h2>D</h2>
      <ul>
        <li>data<ul><li>loading, from S3 / <a href="#ch02lvl1sec21" title="Interactively loading data from S3" class="link">Interactively loading data from S3</a></li><li>loading, into RDD / <a href="#ch05lvl1sec34" title="Loading data into an RDD" class="link">Loading data into an RDD</a></li><li>saving / <a href="#ch05lvl1sec35" title="Saving your data" class="link">Saving your data</a></li></ul></li>
        <li>datafiles, GitHub<ul><li>reference link / <a href="#ch07lvl1sec40" title="SQL access to a simple data table" class="link">SQL access to a simple data table</a></li></ul></li>
        <li>directory<ul><li>organization / <a href="#ch01lvl1sec08" title="Directory organization and convention" class="link">Directory organization and convention</a></li><li>convention / <a href="#ch01lvl1sec08" title="Directory organization and convention" class="link">Directory organization and convention</a></li><li>references / <a href="#ch01lvl1sec08" title="Directory organization and convention" class="link">Directory organization and convention</a></li></ul></li>
        <li>doctest / <a href="#ch10lvl1sec50" title="Testing in Python" class="link">Testing in Python</a></li>
        <li>double RDD functions<ul><li>about / <a href="#ch06lvl1sec37" title="Double RDD functions" class="link">Double RDD functions</a></li><li>sampleStdev / <a href="#ch06lvl1sec37" title="Double RDD functions" class="link">Double RDD functions</a></li><li>Stats / <a href="#ch06lvl1sec37" title="Double RDD functions" class="link">Double RDD functions</a></li><li>Stdev / <a href="#ch06lvl1sec37" title="Double RDD functions" class="link">Double RDD functions</a></li><li>Sum / <a href="#ch06lvl1sec37" title="Double RDD functions" class="link">Double RDD functions</a></li><li>variance / <a href="#ch06lvl1sec37" title="Double RDD functions" class="link">Double RDD functions</a></li></ul></li>
      </ul>
      <h2>E</h2>
      <ul>
        <li>EC2<ul><li>Spark, running on / <a href="#ch01lvl1sec13" title="Running Spark on EC2" class="link">Running Spark on EC2</a>, <a href="#ch01lvl1sec13" title="Running Spark on EC2 with the scripts" class="link">Running Spark on EC2 with the scripts</a></li></ul></li>
        <li>EC2 command line tools<ul><li>references / <a href="#ch01lvl1sec13" title="Running Spark on EC2 with the scripts" class="link">Running Spark on EC2 with the scripts</a></li></ul></li>
        <li>EC2 scripts, Amazon<ul><li>URL / <a href="#ch01lvl1sec13" title="Running Spark on EC2 with the scripts" class="link">Running Spark on EC2 with the scripts</a></li></ul></li>
        <li>Elastic MapReduce (EMR)<ul><li>Spark, deploying on / <a href="#ch01lvl1sec13" title="Deploying Spark on Elastic MapReduce" class="link">Deploying Spark on Elastic MapReduce</a></li></ul></li>
        <li>ENhanced Scala Interaction Mode for Emacs<ul><li>ENSIME / <a href="#ch11lvl1sec53" title="IDE integration" class="link">IDE integration</a></li></ul></li>
        <li>ENSIME<ul><li>URL / <a href="#ch11lvl1sec53" title="IDE integration" class="link">IDE integration</a></li></ul></li>
      </ul>
      <h2>F</h2>
      <ul>
        <li>files<ul><li>saving, to Parquet / <a href="#ch08lvl1sec42" title="Saving files to the Parquet format" class="link">Saving files to the Parquet format</a></li><li>loading, to Parquet / <a href="#ch08lvl1sec42" title="Loading Parquet files" class="link">Loading Parquet files</a></li></ul></li>
        <li>flatMap function<ul><li>about / <a href="#ch06lvl1sec37" title="Manipulating your RDD in Scala and Java" class="link">Manipulating your RDD in Scala and Java</a></li></ul></li>
        <li>functions, for joining Pair RDDs<ul><li>about / <a href="#ch06lvl1sec37" title="Functions for joining PairRDDs" class="link">Functions for joining PairRDDs</a></li><li>coGroup / <a href="#ch06lvl1sec37" title="Functions for joining PairRDDs" class="link">Functions for joining PairRDDs</a></li><li>join / <a href="#ch06lvl1sec37" title="Functions for joining PairRDDs" class="link">Functions for joining PairRDDs</a></li><li>subtractKey / <a href="#ch06lvl1sec37" title="Functions for joining PairRDDs" class="link">Functions for joining PairRDDs</a></li></ul></li>
        <li>functions, on JavaPairRDDs<ul><li>about / <a href="#ch06lvl1sec37" title="Functions on JavaPairRDDs" class="link">Functions on JavaPairRDDs</a></li><li>cogroup / <a href="#ch06lvl1sec37" title="Functions on JavaPairRDDs" class="link">Functions on JavaPairRDDs</a></li><li>combineByKey / <a href="#ch06lvl1sec37" title="Functions on JavaPairRDDs" class="link">Functions on JavaPairRDDs</a></li><li>collectAsMap / <a href="#ch06lvl1sec37" title="Functions on JavaPairRDDs" class="link">Functions on JavaPairRDDs</a></li><li>countByKey / <a href="#ch06lvl1sec37" title="Functions on JavaPairRDDs" class="link">Functions on JavaPairRDDs</a></li><li>flatMapValues / <a href="#ch06lvl1sec37" title="Functions on JavaPairRDDs" class="link">Functions on JavaPairRDDs</a></li><li>join / <a href="#ch06lvl1sec37" title="Functions on JavaPairRDDs" class="link">Functions on JavaPairRDDs</a></li><li>keys / <a href="#ch06lvl1sec37" title="Functions on JavaPairRDDs" class="link">Functions on JavaPairRDDs</a></li><li>lookup / <a href="#ch06lvl1sec37" title="Functions on JavaPairRDDs" class="link">Functions on JavaPairRDDs</a></li><li>reduceByKey / <a href="#ch06lvl1sec37" title="Functions on JavaPairRDDs" class="link">Functions on JavaPairRDDs</a></li><li>sortByKey / <a href="#ch06lvl1sec37" title="Functions on JavaPairRDDs" class="link">Functions on JavaPairRDDs</a></li><li>values / <a href="#ch06lvl1sec37" title="Functions on JavaPairRDDs" class="link">Functions on JavaPairRDDs</a></li></ul></li>
      </ul>
      <h2>G</h2>
      <ul>
        <li>general RDD functions<ul><li>about / <a href="#ch06lvl1sec37" title="General RDD functions" class="link">General RDD functions</a></li><li>aggregate / <a href="#ch06lvl1sec37" title="General RDD functions" class="link">General RDD functions</a></li><li>cache / <a href="#ch06lvl1sec37" title="General RDD functions" class="link">General RDD functions</a></li><li>collect / <a href="#ch06lvl1sec37" title="General RDD functions" class="link">General RDD functions</a></li><li>count / <a href="#ch06lvl1sec37" title="General RDD functions" class="link">General RDD functions</a></li><li>countByValue / <a href="#ch06lvl1sec37" title="General RDD functions" class="link">General RDD functions</a></li><li>distinct / <a href="#ch06lvl1sec37" title="General RDD functions" class="link">General RDD functions</a></li><li>filter / <a href="#ch06lvl1sec37" title="General RDD functions" class="link">General RDD functions</a></li><li>filterWith / <a href="#ch06lvl1sec37" title="General RDD functions" class="link">General RDD functions</a></li><li>first / <a href="#ch06lvl1sec37" title="General RDD functions" class="link">General RDD functions</a></li><li>flatMap / <a href="#ch06lvl1sec37" title="General RDD functions" class="link">General RDD functions</a></li><li>fold / <a href="#ch06lvl1sec37" title="General RDD functions" class="link">General RDD functions</a></li><li>foreach / <a href="#ch06lvl1sec37" title="General RDD functions" class="link">General RDD functions</a></li><li>groupBy / <a href="#ch06lvl1sec37" title="General RDD functions" class="link">General RDD functions</a></li><li>keyBy / <a href="#ch06lvl1sec37" title="General RDD functions" class="link">General RDD functions</a></li><li>map / <a href="#ch06lvl1sec37" title="General RDD functions" class="link">General RDD functions</a></li><li>mapPartitions / <a href="#ch06lvl1sec37" title="General RDD functions" class="link">General RDD functions</a></li><li>mapPartitionsWithIndex / <a href="#ch06lvl1sec37" title="General RDD functions" class="link">General RDD functions</a></li><li>mapWith / <a href="#ch06lvl1sec37" title="General RDD functions" class="link">General RDD functions</a></li><li>persist / <a href="#ch06lvl1sec37" title="General RDD functions" class="link">General RDD functions</a></li><li>pipe / <a href="#ch06lvl1sec37" title="General RDD functions" class="link">General RDD functions</a></li><li>sample / <a href="#ch06lvl1sec37" title="General RDD functions" class="link">General RDD functions</a></li><li>takeSample / <a href="#ch06lvl1sec37" title="General RDD functions" class="link">General RDD functions</a></li><li>toDebugString / <a href="#ch06lvl1sec37" title="General RDD functions" class="link">General RDD functions</a></li><li>union / <a href="#ch06lvl1sec37" title="General RDD functions" class="link">General RDD functions</a></li><li>unpersist / <a href="#ch06lvl1sec37" title="General RDD functions" class="link">General RDD functions</a></li><li>zip / <a href="#ch06lvl1sec37" title="General RDD functions" class="link">General RDD functions</a></li></ul></li>
        <li>GitHub repository<ul><li>reference link, for data files / <a href="#ch09lvl1sec47" title="Spark MLlib examples" class="link">Spark MLlib examples</a></li></ul></li>
      </ul>
      <h2>H</h2>
      <ul>
        <li>HBase<ul><li>about / <a href="#ch08lvl1sec44" title="HBase" class="link">HBase</a></li><li>data, loading / <a href="#ch08lvl1sec44" title="Loading from HBase" class="link">Loading from HBase</a></li><li>data, saving / <a href="#ch08lvl1sec44" title="Saving to HBase" class="link">Saving to HBase</a></li><li>metadata, obtaining / <a href="#ch08lvl1sec44" title="Other HBase operations" class="link">Other HBase operations</a></li></ul></li>
      </ul>
      <h2>I</h2>
      <ul>
        <li>Impala<ul><li>Parquet files, querying / <a href="#ch08lvl1sec43" title="Querying Parquet files with Impala" class="link">Querying Parquet files with Impala</a></li></ul></li>
        <li>interactions<ul><li>testing, with SparkContext / <a href="#ch10lvl1sec49" title="Testing interactions with SparkContext" class="link">Testing interactions with SparkContext</a></li></ul></li>
      </ul>
      <h2>J</h2>
      <ul>
        <li>Java<ul><li>SparkContext object, creating in / <a href="#ch04lvl1sec28" title="Java" class="link">Java</a></li><li>RDD, manipulating in / <a href="#ch06lvl1sec37" title="Manipulating your RDD in Scala and Java" class="link">Manipulating your RDD in Scala and Java</a></li><li>using, as testing library / <a href="#ch10lvl1sec49" title="Testing in Java and Scala" class="link">Testing in Java and Scala</a></li></ul></li>
        <li>Java RDD functions<ul><li>about / <a href="#ch06lvl1sec37" title="Java RDD functions" class="link">Java RDD functions</a>, <a href="#ch06lvl1sec37" title="Common Java RDD functions" class="link">Common Java RDD functions</a></li><li>Spark Java function classes / <a href="#ch06lvl1sec37" title="Spark Java function classes" class="link">Spark Java function classes</a></li><li>common Java RDD functions / <a href="#ch06lvl1sec37" title="Common Java RDD functions" class="link">Common Java RDD functions</a></li><li>cache / <a href="#ch06lvl1sec37" title="Common Java RDD functions" class="link">Common Java RDD functions</a></li><li>coalesce / <a href="#ch06lvl1sec37" title="Common Java RDD functions" class="link">Common Java RDD functions</a></li><li>collect / <a href="#ch06lvl1sec37" title="Common Java RDD functions" class="link">Common Java RDD functions</a></li><li>count / <a href="#ch06lvl1sec37" title="Common Java RDD functions" class="link">Common Java RDD functions</a></li><li>countByValue / <a href="#ch06lvl1sec37" title="Common Java RDD functions" class="link">Common Java RDD functions</a></li><li>distinct / <a href="#ch06lvl1sec37" title="Common Java RDD functions" class="link">Common Java RDD functions</a></li><li>filter / <a href="#ch06lvl1sec37" title="Common Java RDD functions" class="link">Common Java RDD functions</a></li><li>first / <a href="#ch06lvl1sec37" title="Common Java RDD functions" class="link">Common Java RDD functions</a></li><li>flatMap / <a href="#ch06lvl1sec37" title="Common Java RDD functions" class="link">Common Java RDD functions</a></li><li>fold / <a href="#ch06lvl1sec37" title="Common Java RDD functions" class="link">Common Java RDD functions</a></li><li>foreach / <a href="#ch06lvl1sec37" title="Common Java RDD functions" class="link">Common Java RDD functions</a></li><li>groupBy / <a href="#ch06lvl1sec37" title="Common Java RDD functions" class="link">Common Java RDD functions</a></li><li>map / <a href="#ch06lvl1sec37" title="Common Java RDD functions" class="link">Common Java RDD functions</a></li><li>mapPartitions / <a href="#ch06lvl1sec37" title="Common Java RDD functions" class="link">Common Java RDD functions</a></li><li>reduce / <a href="#ch06lvl1sec37" title="Common Java RDD functions" class="link">Common Java RDD functions</a></li><li>sample / <a href="#ch06lvl1sec37" title="Common Java RDD functions" class="link">Common Java RDD functions</a></li></ul></li>
      </ul>
      <h2>L</h2>
      <ul>
        <li>lambda<ul><li>about / <a href="#ch06lvl1sec37" title="Manipulating your RDD in Scala and Java" class="link">Manipulating your RDD in Scala and Java</a></li></ul></li>
        <li>latest development source, Spark<ul><li>references / <a href="#ch01lvl1sec10" title="Downloading the source" class="link">Downloading the source</a></li></ul></li>
        <li>linear regression, Spark MLlib examples<ul><li>about / <a href="#ch09lvl1sec47" title="Linear regression" class="link">Linear regression</a></li></ul></li>
        <li>Logistic regression<ul><li>running, Spark shell used / <a href="#ch02lvl1sec20" title="Using the Spark shell to run logistic regression" class="link">Using the Spark shell to run logistic regression</a></li></ul></li>
        <li>logs<ul><li>finding / <a href="#ch11lvl1sec52" title="Where to find logs" class="link">Where to find logs</a></li></ul></li>
      </ul>
      <h2>M</h2>
      <ul>
        <li>mailing lists<ul><li>about / <a href="#ch11lvl1sec57" title="Mailing lists" class="link">Mailing lists</a></li><li>references / <a href="#ch11lvl1sec57" title="Mailing lists" class="link">Mailing lists</a></li></ul></li>
        <li>map<ul><li>about / <a href="#ch06lvl1sec37" title="Manipulating your RDD in Scala and Java" class="link">Manipulating your RDD in Scala and Java</a></li></ul></li>
        <li>map function<ul><li>about / <a href="#ch06lvl1sec37" title="Manipulating your RDD in Scala and Java" class="link">Manipulating your RDD in Scala and Java</a></li></ul></li>
        <li>massively parallel processing (MPP)<ul><li>about / <a href="#ch08lvl1sec43" title="Querying Parquet files with Impala" class="link">Querying Parquet files with Impala</a></li></ul></li>
        <li>Maven<ul><li>Spark job, building with / <a href="#ch03lvl1sec24" title="Building your Spark job with Maven" class="link">Building your Spark job with Maven</a></li></ul></li>
        <li>maven installation instructions<ul><li>references / <a href="#ch01lvl1sec10" title="Compiling the source with Maven" class="link">Compiling the source with Maven</a></li></ul></li>
        <li>Mesos<ul><li>about / <a href="#ch01lvl1sec15" title="Deploying Spark on Mesos" class="link">Deploying Spark on Mesos</a></li><li>Spark, deploying on / <a href="#ch01lvl1sec15" title="Deploying Spark on Mesos" class="link">Deploying Spark on Mesos</a></li><li>URL / <a href="#ch01lvl1sec15" title="Deploying Spark on Mesos" class="link">Deploying Spark on Mesos</a></li></ul></li>
        <li>metadata, SparkContext object / <a href="#ch04lvl1sec29" title="SparkContext – metadata" class="link">SparkContext – metadata</a><ul><li>appName / <a href="#ch04lvl1sec29" title="SparkContext – metadata" class="link">SparkContext – metadata</a></li><li>getConf / <a href="#ch04lvl1sec29" title="SparkContext – metadata" class="link">SparkContext – metadata</a></li><li>getExecutorMemoryStatus / <a href="#ch04lvl1sec29" title="SparkContext – metadata" class="link">SparkContext – metadata</a></li><li>Master / <a href="#ch04lvl1sec29" title="SparkContext – metadata" class="link">SparkContext – metadata</a></li><li>Version / <a href="#ch04lvl1sec29" title="SparkContext – metadata" class="link">SparkContext – metadata</a></li><li>about / <a href="#ch04lvl1sec29" title="SparkContext – metadata" class="link">SparkContext – metadata</a></li></ul></li>
        <li>methods, for combining JavaRDDs<ul><li>about / <a href="#ch06lvl1sec37" title="Methods for combining JavaRDDs" class="link">Methods for combining JavaRDDs</a></li><li>subtract / <a href="#ch06lvl1sec37" title="Methods for combining JavaRDDs" class="link">Methods for combining JavaRDDs</a></li><li>union / <a href="#ch06lvl1sec37" title="Methods for combining JavaRDDs" class="link">Methods for combining JavaRDDs</a></li><li>zip / <a href="#ch06lvl1sec37" title="Methods for combining JavaRDDs" class="link">Methods for combining JavaRDDs</a></li></ul></li>
        <li>multiple tables<ul><li>handling, with Spark SQL / <a href="#ch07lvl1sec40" title="Handling multiple tables with Spark SQL" class="link">Handling multiple tables with Spark SQL</a></li></ul></li>
      </ul>
      <h2>N</h2>
      <ul>
        <li>nondata driven methods, SparkContext object<ul><li>addJar(path) / <a href="#ch04lvl1sec30" title="Shared Java and Scala APIs" class="link">Shared Java and Scala APIs</a></li><li>addFile(path) / <a href="#ch04lvl1sec30" title="Shared Java and Scala APIs" class="link">Shared Java and Scala APIs</a></li><li>stop() / <a href="#ch04lvl1sec30" title="Shared Java and Scala APIs" class="link">Shared Java and Scala APIs</a></li><li>clearFiles() / <a href="#ch04lvl1sec30" title="Shared Java and Scala APIs" class="link">Shared Java and Scala APIs</a></li><li>clearJars() / <a href="#ch04lvl1sec30" title="Shared Java and Scala APIs" class="link">Shared Java and Scala APIs</a></li></ul></li>
      </ul>
      <h2>P</h2>
      <ul>
        <li>package index site<ul><li>reference link / <a href="#ch11lvl1sec56" title="Community developed packages" class="link">Community developed packages</a></li></ul></li>
        <li>pair RDD functions<ul><li>about / <a href="#ch06lvl1sec38" title="PairRDD functions" class="link">PairRDD functions</a></li><li>collectAsMap / <a href="#ch06lvl1sec38" title="PairRDD functions" class="link">PairRDD functions</a></li><li>reduceByKey / <a href="#ch06lvl1sec38" title="PairRDD functions" class="link">PairRDD functions</a></li><li>countByKey / <a href="#ch06lvl1sec38" title="PairRDD functions" class="link">PairRDD functions</a></li><li>join / <a href="#ch06lvl1sec38" title="PairRDD functions" class="link">PairRDD functions</a></li><li>rightOuterJoin / <a href="#ch06lvl1sec38" title="PairRDD functions" class="link">PairRDD functions</a></li><li>leftOuterJoin / <a href="#ch06lvl1sec38" title="PairRDD functions" class="link">PairRDD functions</a></li><li>combineByKey / <a href="#ch06lvl1sec38" title="PairRDD functions" class="link">PairRDD functions</a></li><li>zip / <a href="#ch06lvl1sec38" title="PairRDD functions" class="link">PairRDD functions</a></li><li>groupByKey / <a href="#ch06lvl1sec38" title="PairRDD functions" class="link">PairRDD functions</a></li><li>cogroup / <a href="#ch06lvl1sec38" title="PairRDD functions" class="link">PairRDD functions</a></li></ul></li>
        <li>PairRDD functions<ul><li>about / <a href="#ch06lvl1sec37" title="Other PairRDD functions" class="link">Other PairRDD functions</a></li><li>lookup / <a href="#ch06lvl1sec37" title="Other PairRDD functions" class="link">Other PairRDD functions</a></li><li>mapValues / <a href="#ch06lvl1sec37" title="Other PairRDD functions" class="link">Other PairRDD functions</a></li><li>collectAsMap / <a href="#ch06lvl1sec37" title="Other PairRDD functions" class="link">Other PairRDD functions</a></li><li>countByKey / <a href="#ch06lvl1sec37" title="Other PairRDD functions" class="link">Other PairRDD functions</a></li><li>partitionBy / <a href="#ch06lvl1sec37" title="Other PairRDD functions" class="link">Other PairRDD functions</a></li><li>flatMapValues / <a href="#ch06lvl1sec37" title="Other PairRDD functions" class="link">Other PairRDD functions</a></li></ul></li>
        <li>Parquet<ul><li>about / <a href="#ch08lvl1sec42" title="Parquet – an efficient and interoperable big data format" class="link">Parquet – an efficient and interoperable big data format</a></li><li>files, saving / <a href="#ch08lvl1sec42" title="Saving files to the Parquet format" class="link">Saving files to the Parquet format</a></li><li>files, loading / <a href="#ch08lvl1sec42" title="Loading Parquet files" class="link">Loading Parquet files</a></li><li>processed RDD, saving / <a href="#ch08lvl1sec42" title="Saving processed RDD in the Parquet format" class="link">Saving processed RDD in the Parquet format</a></li></ul></li>
        <li>Parquet files<ul><li>querying, with Impala / <a href="#ch08lvl1sec43" title="Querying Parquet files with Impala" class="link">Querying Parquet files with Impala</a></li></ul></li>
        <li>Personal Package Archive (PPA) / <a href="#ch03lvl1sec23" title="Building your Spark project with sbt" class="link">Building your Spark project with sbt</a></li>
        <li>prebuilt distribution<ul><li>installing / <a href="#ch01lvl1sec09" title="Installing prebuilt distribution" class="link">Installing prebuilt distribution</a></li></ul></li>
        <li>processed RDD<ul><li>saving, in Parquet / <a href="#ch08lvl1sec42" title="Saving processed RDD in the Parquet format" class="link">Saving processed RDD in the Parquet format</a></li></ul></li>
        <li>PySpark / <a href="#ch10lvl1sec50" title="Testing in Python" class="link">Testing in Python</a></li>
        <li>Python<ul><li>Spark shell, running in / <a href="#ch02lvl1sec21" title="Running Spark shell in Python" class="link">Running Spark shell in Python</a></li><li>SparkContext object, creating in / <a href="#ch04lvl1sec31" title="Python" class="link">Python</a></li><li>RDD, manipulating in / <a href="#ch06lvl1sec38" title="Manipulating your RDD in Python" class="link">Manipulating your RDD in Python</a></li></ul></li>
        <li>Python testing, of Spark / <a href="#ch10lvl1sec50" title="Testing in Python" class="link">Testing in Python</a></li>
      </ul>
      <h2>Q</h2>
      <ul>
        <li>QuickStart VM<ul><li>URL / <a href="#ch08lvl1sec43" title="Querying Parquet files with Impala" class="link">Querying Parquet files with Impala</a></li></ul></li>
      </ul>
      <h2>R</h2>
      <ul>
        <li>RDD<ul><li>about / <a href="#ch05lvl1sec33" title="RDDs" class="link">RDDs</a></li><li>data, loading into / <a href="#ch05lvl1sec34" title="Loading data into an RDD" class="link">Loading data into an RDD</a></li><li>manipulating, in Scala / <a href="#ch06lvl1sec37" title="Manipulating your RDD in Scala and Java" class="link">Manipulating your RDD in Scala and Java</a></li><li>manipulating, in Java / <a href="#ch06lvl1sec37" title="Manipulating your RDD in Scala and Java" class="link">Manipulating your RDD in Scala and Java</a></li><li>manipulating, in Python / <a href="#ch06lvl1sec38" title="Manipulating your RDD in Python" class="link">Manipulating your RDD in Python</a></li><li>references / <a href="#ch06lvl1sec38" title="PairRDD functions" class="link">PairRDD functions</a></li></ul></li>
        <li>recommendation, Spark MLlib examples<ul><li>about / <a href="#ch09lvl1sec47" title="Recommendation" class="link">Recommendation</a></li><li>reference link / <a href="#ch09lvl1sec47" title="Recommendation" class="link">Recommendation</a></li></ul></li>
        <li>reduce<ul><li>about / <a href="#ch06lvl1sec37" title="Manipulating your RDD in Scala and Java" class="link">Manipulating your RDD in Scala and Java</a></li></ul></li>
        <li>Resilient Distributed Dataset (RDD) / <a href="#ch01lvl1sec11" title="Spark topology" class="link">Spark topology</a></li>
        <li>Resilient Distributed Datasets (RDD) / <a href="#ch02lvl1sec19" title="Loading a simple text file" class="link">Loading a simple text file</a></li>
        <li>Run Length Encoding (RLE)<ul><li>about / <a href="#ch08lvl1sec42" title="Saving files to the Parquet format" class="link">Saving files to the Parquet format</a></li></ul></li>
      </ul>
      <h2>S</h2>
      <ul>
        <li>S3<ul><li>data, loading from / <a href="#ch02lvl1sec21" title="Interactively loading data from S3" class="link">Interactively loading data from S3</a></li></ul></li>
        <li>sbt<ul><li>Spark project, building with / <a href="#ch03lvl1sec23" title="Building your Spark project with sbt" class="link">Building your Spark project with sbt</a></li></ul></li>
        <li>Scala<ul><li>SparkContext object, creating in / <a href="#ch04lvl1sec27" title="Scala" class="link">Scala</a></li><li>RDD, manipulating in / <a href="#ch06lvl1sec37" title="Manipulating your RDD in Scala and Java" class="link">Manipulating your RDD in Scala and Java</a></li></ul></li>
        <li>Scala APIs / <a href="#ch04lvl1sec30" title="Shared Java and Scala APIs" class="link">Shared Java and Scala APIs</a></li>
        <li>Scala RDD functions<ul><li>about / <a href="#ch06lvl1sec37" title="Scala RDD functions" class="link">Scala RDD functions</a></li><li>foldByKey / <a href="#ch06lvl1sec37" title="Scala RDD functions" class="link">Scala RDD functions</a></li><li>reduceByKey / <a href="#ch06lvl1sec37" title="Scala RDD functions" class="link">Scala RDD functions</a></li><li>groupByKey / <a href="#ch06lvl1sec37" title="Scala RDD functions" class="link">Scala RDD functions</a></li></ul></li>
        <li>ScalaTest<ul><li>using, as testing library / <a href="#ch10lvl1sec49" title="Testing in Java and Scala" class="link">Testing in Java and Scala</a></li></ul></li>
        <li>security<ul><li>about / <a href="#ch11lvl1sec55" title="A quick note on security" class="link">A quick note on security</a></li></ul></li>
        <li>shared Java APIs / <a href="#ch04lvl1sec30" title="Shared Java and Scala APIs" class="link">Shared Java and Scala APIs</a></li>
        <li>simple text file<ul><li>loading / <a href="#ch02lvl1sec19" title="Loading a simple text file" class="link">Loading a simple text file</a></li></ul></li>
        <li>single machine<ul><li>about / <a href="#ch01lvl1sec12" title="A single machine" class="link">A single machine</a></li></ul></li>
        <li>source<ul><li>Spark, building from / <a href="#ch01lvl1sec10" title="Building Spark from source" class="link">Building Spark from source</a></li></ul></li>
        <li>spam dataset, GitHub link<ul><li>URL / <a href="#ch02lvl1sec19" title="Loading a simple text file" class="link">Loading a simple text file</a></li></ul></li>
        <li>Spark<ul><li>URL, for downloading / <a href="#ch01lvl1sec09" title="Installing prebuilt distribution" class="link">Installing prebuilt distribution</a></li><li>building, from source / <a href="#ch01lvl1sec10" title="Building Spark from source" class="link">Building Spark from source</a></li><li>URL, for building from source / <a href="#ch01lvl1sec10" title="Building Spark from source" class="link">Building Spark from source</a></li><li>URL, for downloading latest source / <a href="#ch01lvl1sec10" title="Downloading the source" class="link">Downloading the source</a></li><li>installation, testing / <a href="#ch01lvl1sec10" title="Testing the installation" class="link">Testing the installation</a></li><li>running, on EC2 / <a href="#ch01lvl1sec13" title="Running Spark on EC2" class="link">Running Spark on EC2</a></li><li>reference link, for latest onrunning spark on EC2 / <a href="#ch01lvl1sec13" title="Running Spark on EC2" class="link">Running Spark on EC2</a></li><li>running on EC2, with scripts / <a href="#ch01lvl1sec13" title="Running Spark on EC2 with the scripts" class="link">Running Spark on EC2 with the scripts</a></li><li>deploying, on Elastic MapReduce (EMR) / <a href="#ch01lvl1sec13" title="Deploying Spark on Elastic MapReduce" class="link">Deploying Spark on Elastic MapReduce</a></li><li>deploying, with Chef / <a href="#ch01lvl1sec14" title="Deploying Spark with Chef (Opscode)" class="link">Deploying Spark with Chef (Opscode)</a></li><li>deploying, on Mesos / <a href="#ch01lvl1sec15" title="Deploying Spark on Mesos" class="link">Deploying Spark on Mesos</a></li><li>URL, for configuration details on YARN / <a href="#ch01lvl1sec16" title="Spark on YARN" class="link">Spark on YARN</a></li><li>standalone mode / <a href="#ch01lvl1sec17" title="Spark Standalone mode" class="link">Spark Standalone mode</a></li><li>references / <a href="#ch03lvl1sec25" title="Building your Spark job with something else" class="link">Building your Spark job with something else</a></li><li>uisng, with other languages / <a href="#ch11lvl1sec54" title="Using Spark with other languages" class="link">Using Spark with other languages</a></li></ul></li>
        <li>Spark, building from source<ul><li>about / <a href="#ch01lvl1sec10" title="Building Spark from source" class="link">Building Spark from source</a></li><li>download source / <a href="#ch01lvl1sec10" title="Downloading the source" class="link">Downloading the source</a></li><li>source, compiling with Maven / <a href="#ch01lvl1sec10" title="Compiling the source with Maven" class="link">Compiling the source with Maven</a></li><li>compilation switches / <a href="#ch01lvl1sec10" title="Compilation switches" class="link">Compilation switches</a></li></ul></li>
        <li>Spark, on YARN<ul><li>about / <a href="#ch01lvl1sec16" title="Spark on YARN" class="link">Spark on YARN</a></li></ul></li>
        <li>SparkContext<ul><li>references / <a href="#ch04lvl1sec31" title="Python" class="link">Python</a></li><li>interactions, testing with / <a href="#ch10lvl1sec49" title="Testing interactions with SparkContext" class="link">Testing interactions with SparkContext</a></li></ul></li>
        <li>SparkContext object<ul><li>creating, in Scala / <a href="#ch04lvl1sec27" title="Scala" class="link">Scala</a></li><li>creating, in Java / <a href="#ch04lvl1sec28" title="Java" class="link">Java</a></li><li>metadata / <a href="#ch04lvl1sec29" title="SparkContext – metadata" class="link">SparkContext – metadata</a></li><li>creating, in Python / <a href="#ch04lvl1sec31" title="Python" class="link">Python</a></li></ul></li>
        <li>Spark documentation<ul><li>URL, for configuration / <a href="#ch11lvl1sec53" title="Memory usage and garbage collection" class="link">Memory usage and garbage collection</a></li><li>URL, for RDDs / <a href="#ch11lvl1sec53" title="Memory usage and garbage collection" class="link">Memory usage and garbage collection</a></li></ul></li>
        <li>Spark Java function classes<ul><li>about / <a href="#ch06lvl1sec37" title="Spark Java function classes" class="link">Spark Java function classes</a></li><li>Function&lt;T,R&gt; / <a href="#ch06lvl1sec37" title="Spark Java function classes" class="link">Spark Java function classes</a></li><li>DoubleFunction&lt;T&gt; / <a href="#ch06lvl1sec37" title="Spark Java function classes" class="link">Spark Java function classes</a></li><li>PairFunction&lt;T, K, V&gt; / <a href="#ch06lvl1sec37" title="Spark Java function classes" class="link">Spark Java function classes</a></li><li>FlatMapFunction&lt;T, R&gt; / <a href="#ch06lvl1sec37" title="Spark Java function classes" class="link">Spark Java function classes</a></li><li>PairFlatMapFunction&lt;T, K, V&gt; / <a href="#ch06lvl1sec37" title="Spark Java function classes" class="link">Spark Java function classes</a></li><li>DoubleFlatMapFunction&lt;T&gt; / <a href="#ch06lvl1sec37" title="Spark Java function classes" class="link">Spark Java function classes</a></li><li>Function2&lt;T1, T2, R&gt; / <a href="#ch06lvl1sec37" title="Spark Java function classes" class="link">Spark Java function classes</a></li></ul></li>
        <li>Spark job<ul><li>building, with Maven / <a href="#ch03lvl1sec24" title="Building your Spark job with Maven" class="link">Building your Spark job with Maven</a></li><li>building / <a href="#ch03lvl1sec25" title="Building your Spark job with something else" class="link">Building your Spark job with something else</a></li></ul></li>
        <li>Spark machine learning algorithm table<ul><li>about / <a href="#ch09lvl1sec46" title="The Spark machine learning algorithm table" class="link">The Spark machine learning algorithm table</a></li></ul></li>
        <li>Spark MLlib examples<ul><li>about / <a href="#ch09lvl1sec47" title="Spark MLlib examples" class="link">Spark MLlib examples</a></li><li>basic statistics / <a href="#ch09lvl1sec47" title="Basic statistics" class="link">Basic statistics</a></li><li>linear regression / <a href="#ch09lvl1sec47" title="Linear regression" class="link">Linear regression</a></li><li>classification / <a href="#ch09lvl1sec47" title="Classification" class="link">Classification</a></li><li>clustering / <a href="#ch09lvl1sec47" title="Clustering" class="link">Clustering</a></li><li>recommendation / <a href="#ch09lvl1sec47" title="Recommendation" class="link">Recommendation</a></li></ul></li>
        <li>Spark project<ul><li>building, with sbt / <a href="#ch03lvl1sec23" title="Building your Spark project with sbt" class="link">Building your Spark project with sbt</a></li></ul></li>
        <li>Spark shell<ul><li>used, for running Logistic regression / <a href="#ch02lvl1sec20" title="Using the Spark shell to run logistic regression" class="link">Using the Spark shell to run logistic regression</a></li><li>running, in Python / <a href="#ch02lvl1sec21" title="Running Spark shell in Python" class="link">Running Spark shell in Python</a></li></ul></li>
        <li>Spark SQL<ul><li>architecture / <a href="#ch07lvl1sec40" title="The Spark SQL architecture" class="link">The Spark SQL architecture</a></li><li>overview / <a href="#ch07lvl1sec40" title="Spark SQL how-to in a nutshell" class="link">Spark SQL how-to in a nutshell</a></li><li>multiple tables, handling with / <a href="#ch07lvl1sec40" title="Handling multiple tables with Spark SQL" class="link">Handling multiple tables with Spark SQL</a></li><li>references / <a href="#ch07lvl1sec40" title="Aftermath" class="link">Aftermath</a></li></ul></li>
        <li>Spark SQL programming<ul><li>about / <a href="#ch07lvl1sec40" title="Spark SQL programming" class="link">Spark SQL programming</a></li><li>SQL access, to simple data table / <a href="#ch07lvl1sec40" title="SQL access to a simple data table" class="link">SQL access to a simple data table</a></li></ul></li>
        <li>Spark SQL programming guide<ul><li>reference link / <a href="#ch07lvl1sec40" title="Spark SQL programming" class="link">Spark SQL programming</a></li></ul></li>
        <li>Spark topology<ul><li>about / <a href="#ch01lvl1sec11" title="Spark topology" class="link">Spark topology</a></li></ul></li>
        <li>SQL scripts, Northwind database<ul><li>reference link / <a href="#ch07lvl1sec40" title="Spark SQL programming" class="link">Spark SQL programming</a></li></ul></li>
        <li>standalone mode, Spark<ul><li>reference link / <a href="#ch01lvl1sec17" title="Spark Standalone mode" class="link">Spark Standalone mode</a></li></ul></li>
        <li>standard RDD functions<ul><li>about / <a href="#ch06lvl1sec38" title="Standard RDD functions" class="link">Standard RDD functions</a></li><li>flatMap / <a href="#ch06lvl1sec38" title="Standard RDD functions" class="link">Standard RDD functions</a></li><li>mapParitions / <a href="#ch06lvl1sec38" title="Standard RDD functions" class="link">Standard RDD functions</a></li><li>filter / <a href="#ch06lvl1sec38" title="Standard RDD functions" class="link">Standard RDD functions</a></li><li>distinct / <a href="#ch06lvl1sec38" title="Standard RDD functions" class="link">Standard RDD functions</a></li><li>union / <a href="#ch06lvl1sec38" title="Standard RDD functions" class="link">Standard RDD functions</a></li><li>cartesian / <a href="#ch06lvl1sec38" title="Standard RDD functions" class="link">Standard RDD functions</a></li><li>groupBy / <a href="#ch06lvl1sec38" title="Standard RDD functions" class="link">Standard RDD functions</a></li><li>pipe / <a href="#ch06lvl1sec38" title="Standard RDD functions" class="link">Standard RDD functions</a></li><li>foreach / <a href="#ch06lvl1sec38" title="Standard RDD functions" class="link">Standard RDD functions</a></li><li>reduce / <a href="#ch06lvl1sec38" title="Standard RDD functions" class="link">Standard RDD functions</a></li><li>fold / <a href="#ch06lvl1sec38" title="Standard RDD functions" class="link">Standard RDD functions</a></li><li>countByValue / <a href="#ch06lvl1sec38" title="Standard RDD functions" class="link">Standard RDD functions</a></li><li>take / <a href="#ch06lvl1sec38" title="Standard RDD functions" class="link">Standard RDD functions</a></li><li>partitionBy / <a href="#ch06lvl1sec38" title="Standard RDD functions" class="link">Standard RDD functions</a></li></ul></li>
      </ul>
      <h2>T</h2>
      <ul>
        <li>testing<ul><li>references / <a href="#ch10lvl1sec50" title="Testing in Python" class="link">Testing in Python</a></li></ul></li>
        <li>type inference / <a href="#ch06lvl1sec37" title="Manipulating your RDD in Scala and Java" class="link">Manipulating your RDD in Scala and Java</a></li>
      </ul>
      <h2>Y</h2>
      <ul>
        <li>YARN<ul><li>about / <a href="#ch01lvl1sec16" title="Spark on YARN" class="link">Spark on YARN</a></li></ul></li>
      </ul>
    </div>
  </div></div></div>
</div></div></div></body></html>
