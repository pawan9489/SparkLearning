<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="">

    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0"> <!--320-->
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no, user-scalable=no">

    <link rel="icon" href="../../mapt/images/favicon.ico">

    <link rel="stylesheet" href="../../mapt/css/font-awesome.css">
    <link rel="stylesheet" href="../../mapt/css/google-fonts.css">
    <link rel="stylesheet" href="../../mapt/css/devicon.css">

    <link rel="stylesheet" href="../../mapt/css/bootstrap.css">
    <link rel="stylesheet" href="../../mapt/css/bootstrap-xl.css">
    <link rel="stylesheet" href="../../mapt/css/magnific-popup.css">
    <link rel="stylesheet" href="../../mapt/css/prism.css">
    <link rel="stylesheet" href="../../mapt/css/hljs-github.css">

    <link rel="stylesheet" href="../../mapt/css/mapt.css">
    <link rel="stylesheet" href="../../mapt/css/custom.css">

    <script src="../../mapt/js/jquery.js"></script>
    <script src="../../mapt/js/bootstrap.js"></script>
    <script src="../../mapt/js/jquery.magnific-popup.js"></script>
    <script src="../../mapt/js/highlight.min.js"></script>

    <script src="../../mapt/js/custom.js"></script>
    
    <title>Apache Spark Graph Processing</title>
</head>

<body class="home-body">
    <div id="wrapper">
        <div id="sidebar-wrapper">    
            <ul class="sidebar-nav">
                <div class="list-group" id="sidebar-nav" role="tablist">
                    <li>
                        <a href="../../index.html" class="sidenav-menu-holder back-btn" id="back-link">
                            <span class="sidenav-menu">Book List</span>
                            <span class="pull-left mr5"><i class="fa fa-chevron-left"></i></span>
                        </a>
                    </li>
                    
                    <li class="book-info copyright">
                        <span class="info text-nowrap"><span class="copyleft">&copy;</span><span><strong>RuTracker</strong>.org</span></span>
                    </li>          
                    <li class="book-info copyright">
                        <span class="info text-nowrap">Pub date: <strong>10 Sep 2015</strong></span>
                    </li>         
                    <li class="book-info">
                        <span class="info text-nowrap">Price: â‚¬<strong>25.99</strong></span>
                        <span class="info text-nowrap">ISBN: <strong>9781784391805</strong></span>
                    </li>     
            
                    <li>
                        <a href="graphics/cover.jpg" class="sidenav-menu-holder cover-img">
                            <img src="graphics/cover.jpg" class="cover-image">
                        </a>
                    </li>        
            
                    <div class="book_navigation">
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse1">
                                <div class="section-name">1: Getting Started with Spark and GraphX</div>
                            </a>
                        </li>
                        <div id="collapse1" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="1" class="sub-nav">
                                <a href="#ch01">
                                    <div class="section-name">Chapter 1: Getting Started with Spark and GraphX</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec09" class="sub-nav">
                                <a href="#ch01lvl1sec09">                    
                                    <div class="section-name">Downloading and installing Spark 1.4.1</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec10" class="sub-nav">
                                <a href="#ch01lvl1sec10">                    
                                    <div class="section-name">Experimenting with the Spark shell</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec11" class="sub-nav">
                                <a href="#ch01lvl1sec11">                    
                                    <div class="section-name">Getting started with GraphX</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec12" class="sub-nav">
                                <a href="#ch01lvl1sec12">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse2">
                                <div class="section-name">2: Building and Exploring Graphs</div>
                            </a>
                        </li>
                        <div id="collapse2" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="2" class="sub-nav">
                                <a href="#ch02">
                                    <div class="section-name">Chapter 2: Building and Exploring Graphs</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec13" class="sub-nav">
                                <a href="#ch02lvl1sec13">                    
                                    <div class="section-name">Network datasets</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec14" class="sub-nav">
                                <a href="#ch02lvl1sec14">                    
                                    <div class="section-name">Graph builders</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec15" class="sub-nav">
                                <a href="#ch02lvl1sec15">                    
                                    <div class="section-name">Building graphs</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec16" class="sub-nav">
                                <a href="#ch02lvl1sec16">                    
                                    <div class="section-name">Computing the degrees of the network nodes</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec17" class="sub-nav">
                                <a href="#ch02lvl1sec17">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse3">
                                <div class="section-name">3: Graph Analysis and Visualization</div>
                            </a>
                        </li>
                        <div id="collapse3" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="3" class="sub-nav">
                                <a href="#ch03">
                                    <div class="section-name">Chapter 3: Graph Analysis and Visualization</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec18" class="sub-nav">
                                <a href="#ch03lvl1sec18">                    
                                    <div class="section-name">Network datasets</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec19" class="sub-nav">
                                <a href="#ch03lvl1sec19">                    
                                    <div class="section-name">The graph visualization</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec20" class="sub-nav">
                                <a href="#ch03lvl1sec20">                    
                                    <div class="section-name">The analysis of network connectedness</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec21" class="sub-nav">
                                <a href="#ch03lvl1sec21">                    
                                    <div class="section-name">The network centrality and PageRank</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec22" class="sub-nav">
                                <a href="#ch03lvl1sec22">                    
                                    <div class="section-name">Scala Build Tool revisited</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec23" class="sub-nav">
                                <a href="#ch03lvl1sec23">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse4">
                                <div class="section-name">4: Transforming and Shaping Up Graphs to Your Needs</div>
                            </a>
                        </li>
                        <div id="collapse4" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="4" class="sub-nav">
                                <a href="#ch04">
                                    <div class="section-name">Chapter 4: Transforming and Shaping Up Graphs to Your Needs</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec24" class="sub-nav">
                                <a href="#ch04lvl1sec24">                    
                                    <div class="section-name">Transforming the vertex and edge attributes</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec25" class="sub-nav">
                                <a href="#ch04lvl1sec25">                    
                                    <div class="section-name">Modifying graph structures</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec26" class="sub-nav">
                                <a href="#ch04lvl1sec26">                    
                                    <div class="section-name">Joining graph datasets</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec27" class="sub-nav">
                                <a href="#ch04lvl1sec27">                    
                                    <div class="section-name">Data operations on VertexRDD and EdgeRDD</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec28" class="sub-nav">
                                <a href="#ch04lvl1sec28">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse5">
                                <div class="section-name">5: Creating Custom Graph Aggregation Operators</div>
                            </a>
                        </li>
                        <div id="collapse5" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="5" class="sub-nav">
                                <a href="#ch05">
                                    <div class="section-name">Chapter 5: Creating Custom Graph Aggregation Operators</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec29" class="sub-nav">
                                <a href="#ch05lvl1sec29">                    
                                    <div class="section-name">NCAA College Basketball datasets</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec30" class="sub-nav">
                                <a href="#ch05lvl1sec30">                    
                                    <div class="section-name">The aggregateMessages operator</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec31" class="sub-nav">
                                <a href="#ch05lvl1sec31">                    
                                    <div class="section-name">Joining average stats into a graph</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec32" class="sub-nav">
                                <a href="#ch05lvl1sec32">                    
                                    <div class="section-name">Performance optimization</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec33" class="sub-nav">
                                <a href="#ch05lvl1sec33">                    
                                    <div class="section-name">The MapReduceTriplets operator</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec34" class="sub-nav">
                                <a href="#ch05lvl1sec34">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse6">
                                <div class="section-name">6: Iterative Graph-Parallel Processing with Pregel</div>
                            </a>
                        </li>
                        <div id="collapse6" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="6" class="sub-nav">
                                <a href="#ch06">
                                    <div class="section-name">Chapter 6: Iterative Graph-Parallel Processing with Pregel</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec35" class="sub-nav">
                                <a href="#ch06lvl1sec35">                    
                                    <div class="section-name">The Pregel computational model</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec36" class="sub-nav">
                                <a href="#ch06lvl1sec36">                    
                                    <div class="section-name">The Pregel API in GraphX</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec37" class="sub-nav">
                                <a href="#ch06lvl1sec37">                    
                                    <div class="section-name">Community detection through label propagation</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec38" class="sub-nav">
                                <a href="#ch06lvl1sec38">                    
                                    <div class="section-name">The Pregel implementation of PageRank</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec39" class="sub-nav">
                                <a href="#ch06lvl1sec39">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse7">
                                <div class="section-name">7: Learning Graph Structures</div>
                            </a>
                        </li>
                        <div id="collapse7" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="7" class="sub-nav">
                                <a href="#ch07">
                                    <div class="section-name">Chapter 7: Learning Graph Structures</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec40" class="sub-nav">
                                <a href="#ch07lvl1sec40">                    
                                    <div class="section-name">Community clustering in graphs</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec41" class="sub-nav">
                                <a href="#ch07lvl1sec41">                    
                                    <div class="section-name">Applications music fan community detection</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec42" class="sub-nav">
                                <a href="#ch07lvl1sec42">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapseappA">
                                <div class="section-name">Appendix A: References</div>
                            </a>
                        </li>
                        <div id="collapseappA" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="appA" class="sub-nav">
                                <a href="#appA">
                                    <div class="section-name">Chapter Appendix A: References</div>
                                </a>
                            </li>
                            <li data-chapter="appA" data-section-id="ch07lvl1sec43" class="sub-nav">
                                <a href="#ch07lvl1sec43">                    
                                    <div class="section-name">Chapter 2, Building and Exploring Graphs</div>
                                </a>
                            </li>
                            <li data-chapter="appA" data-section-id="ch07lvl1sec44" class="sub-nav">
                                <a href="#ch07lvl1sec44">                    
                                    <div class="section-name">Chapter 3, Graph Analysis and Visualization</div>
                                </a>
                            </li>
                            <li data-chapter="appA" data-section-id="ch07lvl1sec45" class="sub-nav">
                                <a href="#ch07lvl1sec45">                    
                                    <div class="section-name">Chapter 7, Learning Graph Structures</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapsebackindex">
                                <div class="section-name">Appendix B: Index</div>
                            </a>
                        </li>
                        <div id="collapsebackindex" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="backindex" class="sub-nav">
                                <a href="#backindex">
                                    <div class="section-name">Chapter Appendix B: Index</div>
                                </a>
                            </li>
                        </div>
                    </div>
                </div>
            </ul>
        </div>
        
        <div id="page-content-wrapper" class="book-page">
            <a href="#" id="menu-toggle" class="toggle-nav"><i class="fa fa-bars fa-2x mr5"></i></a>
            
            <a href="#" id="back_to_top" class="back-to-top"><img src="../../mapt/images/kopimi.svg"></a>
            
            <div class="col-sm-12 col-xl-offset-2 col-xl-8 col-lg-offset-1 col-lg-10">
                <div class="btn-group pull-right mt15 mb30" role="group">
                    <a href="#home" class="btn btn-default">
                        <i class="fa fa-share fa-lg no-text-padding"></i>
                        <span class="hidden-xs ml5">Book Home</span>
                    </a>
                    <button class="btn btn-default" data-nid="21578" id="code-download">
                        <i class="fa fa-file fa-lg"></i>
                        <span class="hidden-xs ml5">Download Code Files</span>
                    </button>
                </div>
            </div>
            <div class="clearfix"></div>
            
            <div id="book-wrapper" class="container-fluid">
                <div class="col-sm-12 col-xl-offset-2 col-xl-8 col-lg-offset-1 col-lg-10" id="home">
                    <h2 class="product-title">Apache Spark Graph Processing</h2>
                    <hr>
                    <div class="row">
                        <div class="col-sm-12">
                            <h5 class="mt10">By Rindra Ramamonjison</h5>
                            <div>
                                <p class="mb20"><b>Build, process and analyze large-scale graph data effectively with Spark</b></p>
                                <a href="#ch01" class="btn btn-info btn-lg pull-right hidden-xs">
                                    Start Reading <i class="fa fa-chevron-right ml5"></i>
                                </a>
                                <a href="#ch01" class="btn btn-info btn-lg btn-block mt20 mb20 visible-xs">
                                    Start Reading <i class="fa fa-chevron-right ml5"></i>
                                </a>
                                <div class="clearfix"></div>
                                <div class="col-sm-12">
                                    <ul id="myTabs" class="nav nav-tabs nav-justified hidden-xs mt20" role="tablist">
                                        <li class="active">
                                            <a href="#info" role="tab" id="info-tab" data-toggle="tab">
                                                <h5>Info</h5>
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#content" role="tab" id="content-tab" data-toggle="tab">
                                                <h5>Contents</h5>
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#author" role="tab" id="author-tab" data-toggle="tab">
                                                <h5>Author</h5>
                                            </a>
                                        </li>
                                    </ul>
                
                                    <ul id="myTabsMobile" class="nav nav-pills text-center nav-stacked visible-xs mb60" role="tablist">
                                        <li class="active">
                                            <a href="#info" role="tab" id="info-tab-responsive" data-toggle="tab">
                                                Info
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#content" role="tab" id="content-tab-responsive" data-toggle="tab">
                                                Contents
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#author" role="tab" id="author-tab-responsive" data-toggle="tab">
                                                Author
                                            </a>
                                        </li>
                                    </ul>
                
                                    <div id="myTabContent" class="tab-content pt30">
                                    
                                        <div role="tabpanel" class="tab-pane active fade in" id="info">
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">Features</h5>
                                            </div>
                                            <div class="hidden-xs">
                                                <h5>Features</h5>
                                            </div>
                                            <hr>
                                            <div>
                                                <ul>
                <li>Find solutions for every stage of data processing from loading and transforming graph data to</li>
                <li>Improve the scalability of your graphs with a variety of real-world applications with complete Scala code.</li>
                <li>A concise guide to processing large-scale networks with Apache Spark.</li>
                </ul>
                                            </div>
                                            <br>
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">Learning</h5>
                                            </div>
                                            <div class="hidden-xs">
                                                <h5>Learning</h5>
                                            </div>
                                            <hr>
                                            <div>
                                                <ul>
                <li>Write, build and deploy Spark applications with the Scala Build Tool.</li>
                <li>Build and analyze large-scale network datasets</li>
                <li>Analyze and transform graphs using RDD and graph-specific operations</li>
                <li>Implement new custom graph operations tailored to specific needs.</li>
                <li>Develop iterative and efficient graph algorithms using message aggregation and Pregel abstraction</li>
                <li>Extract subgraphs and use it to discover common clusters</li>
                <li>Analyze graph data and solve various data science problems using real-world datasets.</li>
                </ul>
                                            </div>
                                            <br>
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">About</h5>
                                            </div>
                                            <div class="hidden-xs">
                                                <h5>About</h5>
                                            </div>
                                            <hr>
                                            <div>
                                                <p>Apache Spark is the next standard of open-source cluster-computing engine for processing big data. Many practical computing problems concern large graphs, like the Web graph and various social networks. The scale of these graphs - in some cases billions of vertices, trillions of edges - poses challenges to their efficient processing. Apache Spark GraphX API combines the advantages of both data-parallel and graph-parallel systems by efficiently expressing graph computation within the Spark data-parallel framework.</p>
                <p>This book will teach the user to do graphical programming in Apache Spark, apart from an explanation of the entire process of graphical data analysis. You will journey through the creation of graphs, its uses, its exploration and analysis and finally will also cover the conversion of graph elements into graph structures.</p>
                <p>This book begins with an introduction of the Spark system, its libraries and the Scala Build Tool. Using a hands-on approach, this book will quickly teach you how to install and leverage Spark interactively on the command line and in a standalone Scala program. Then, it presents all the methods for building Spark graphs using illustrative network datasets. Next, it will walk you through the process of exploring, visualizing and analyzing different network characteristics. This book will also teach you how to transform raw datasets into a usable form. In addition, you will learn powerful operations that can be used to transform graph elements and graph structures. Furthermore, this book also teaches how to create custom graph operations that are tailored for specific needs with efficiency in mind. The later chapters of this book cover more advanced topics such as clustering graphs, implementing graph-parallel iterative algorithms and learning methods from graph data.</p>
                                            </div>
                                        </div>
                                        
                                        <div role="tabpanel" class="tab-pane fade in" id="content">
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">Contents</h5>
                                                <hr>
                                            </div>
                                            <ul>
                                                <div>
                                                    <li data-chapter="1">
                                                        <div class="section-name">1: Getting Started with Spark and GraphX</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="1" class="chapter-section">
                                                                    <a href="#ch01">        
                                                                        <div class="section-name">Chapter 1: Getting Started with Spark and GraphX</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec09" class="chapter-section">
                                                                    <a href="#ch01lvl1sec09">                    
                                                                        <div class="section-name">Downloading and installing Spark 1.4.1</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec10" class="chapter-section">
                                                                    <a href="#ch01lvl1sec10">                    
                                                                        <div class="section-name">Experimenting with the Spark shell</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec11" class="chapter-section">
                                                                    <a href="#ch01lvl1sec11">                    
                                                                        <div class="section-name">Getting started with GraphX</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec12" class="chapter-section">
                                                                    <a href="#ch01lvl1sec12">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="2">
                                                        <div class="section-name">2: Building and Exploring Graphs</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="2" class="chapter-section">
                                                                    <a href="#ch02">        
                                                                        <div class="section-name">Chapter 2: Building and Exploring Graphs</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec13" class="chapter-section">
                                                                    <a href="#ch02lvl1sec13">                    
                                                                        <div class="section-name">Network datasets</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec14" class="chapter-section">
                                                                    <a href="#ch02lvl1sec14">                    
                                                                        <div class="section-name">Graph builders</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec15" class="chapter-section">
                                                                    <a href="#ch02lvl1sec15">                    
                                                                        <div class="section-name">Building graphs</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec16" class="chapter-section">
                                                                    <a href="#ch02lvl1sec16">                    
                                                                        <div class="section-name">Computing the degrees of the network nodes</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec17" class="chapter-section">
                                                                    <a href="#ch02lvl1sec17">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="3">
                                                        <div class="section-name">3: Graph Analysis and Visualization</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="3" class="chapter-section">
                                                                    <a href="#ch03">        
                                                                        <div class="section-name">Chapter 3: Graph Analysis and Visualization</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec18" class="chapter-section">
                                                                    <a href="#ch03lvl1sec18">                    
                                                                        <div class="section-name">Network datasets</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec19" class="chapter-section">
                                                                    <a href="#ch03lvl1sec19">                    
                                                                        <div class="section-name">The graph visualization</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec20" class="chapter-section">
                                                                    <a href="#ch03lvl1sec20">                    
                                                                        <div class="section-name">The analysis of network connectedness</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec21" class="chapter-section">
                                                                    <a href="#ch03lvl1sec21">                    
                                                                        <div class="section-name">The network centrality and PageRank</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec22" class="chapter-section">
                                                                    <a href="#ch03lvl1sec22">                    
                                                                        <div class="section-name">Scala Build Tool revisited</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec23" class="chapter-section">
                                                                    <a href="#ch03lvl1sec23">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="4">
                                                        <div class="section-name">4: Transforming and Shaping Up Graphs to Your Needs</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="4" class="chapter-section">
                                                                    <a href="#ch04">        
                                                                        <div class="section-name">Chapter 4: Transforming and Shaping Up Graphs to Your Needs</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec24" class="chapter-section">
                                                                    <a href="#ch04lvl1sec24">                    
                                                                        <div class="section-name">Transforming the vertex and edge attributes</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec25" class="chapter-section">
                                                                    <a href="#ch04lvl1sec25">                    
                                                                        <div class="section-name">Modifying graph structures</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec26" class="chapter-section">
                                                                    <a href="#ch04lvl1sec26">                    
                                                                        <div class="section-name">Joining graph datasets</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec27" class="chapter-section">
                                                                    <a href="#ch04lvl1sec27">                    
                                                                        <div class="section-name">Data operations on VertexRDD and EdgeRDD</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec28" class="chapter-section">
                                                                    <a href="#ch04lvl1sec28">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="5">
                                                        <div class="section-name">5: Creating Custom Graph Aggregation Operators</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="5" class="chapter-section">
                                                                    <a href="#ch05">        
                                                                        <div class="section-name">Chapter 5: Creating Custom Graph Aggregation Operators</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec29" class="chapter-section">
                                                                    <a href="#ch05lvl1sec29">                    
                                                                        <div class="section-name">NCAA College Basketball datasets</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec30" class="chapter-section">
                                                                    <a href="#ch05lvl1sec30">                    
                                                                        <div class="section-name">The aggregateMessages operator</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec31" class="chapter-section">
                                                                    <a href="#ch05lvl1sec31">                    
                                                                        <div class="section-name">Joining average stats into a graph</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec32" class="chapter-section">
                                                                    <a href="#ch05lvl1sec32">                    
                                                                        <div class="section-name">Performance optimization</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec33" class="chapter-section">
                                                                    <a href="#ch05lvl1sec33">                    
                                                                        <div class="section-name">The MapReduceTriplets operator</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec34" class="chapter-section">
                                                                    <a href="#ch05lvl1sec34">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="6">
                                                        <div class="section-name">6: Iterative Graph-Parallel Processing with Pregel</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="6" class="chapter-section">
                                                                    <a href="#ch06">        
                                                                        <div class="section-name">Chapter 6: Iterative Graph-Parallel Processing with Pregel</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec35" class="chapter-section">
                                                                    <a href="#ch06lvl1sec35">                    
                                                                        <div class="section-name">The Pregel computational model</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec36" class="chapter-section">
                                                                    <a href="#ch06lvl1sec36">                    
                                                                        <div class="section-name">The Pregel API in GraphX</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec37" class="chapter-section">
                                                                    <a href="#ch06lvl1sec37">                    
                                                                        <div class="section-name">Community detection through label propagation</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec38" class="chapter-section">
                                                                    <a href="#ch06lvl1sec38">                    
                                                                        <div class="section-name">The Pregel implementation of PageRank</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec39" class="chapter-section">
                                                                    <a href="#ch06lvl1sec39">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="7">
                                                        <div class="section-name">7: Learning Graph Structures</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="7" class="chapter-section">
                                                                    <a href="#ch07">        
                                                                        <div class="section-name">Chapter 7: Learning Graph Structures</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec40" class="chapter-section">
                                                                    <a href="#ch07lvl1sec40">                    
                                                                        <div class="section-name">Community clustering in graphs</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec41" class="chapter-section">
                                                                    <a href="#ch07lvl1sec41">                    
                                                                        <div class="section-name">Applications music fan community detection</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec42" class="chapter-section">
                                                                    <a href="#ch07lvl1sec42">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="appA">
                                                        <div class="section-name">Appendix A: References</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="appA" class="chapter-section">
                                                                    <a href="#appA">
                                                                        <div class="section-name">Chapter Appendix A: References</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="appA" data-section-id="ch07lvl1sec43" class="chapter-section">
                                                                    <a href="#ch07lvl1sec43">                    
                                                                        <div class="section-name">Chapter 2, Building and Exploring Graphs</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="appA" data-section-id="ch07lvl1sec44" class="chapter-section">
                                                                    <a href="#ch07lvl1sec44">                    
                                                                        <div class="section-name">Chapter 3, Graph Analysis and Visualization</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="appA" data-section-id="ch07lvl1sec45" class="chapter-section">
                                                                    <a href="#ch07lvl1sec45">                    
                                                                        <div class="section-name">Chapter 7, Learning Graph Structures</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="backindex">
                                                        <div class="section-name">Appendix B: Index</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="backindex" class="chapter-section">
                                                                    <a href="#backindex">
                                                                        <div class="section-name">Chapter Appendix B: Index</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                            </ul>
                                        </div>
                                        
                                        <div role="tabpanel" class="tab-pane fade" id="author">
                                            <div class="visible-xs">
                                                <h4 class="mobile-title">About the Author</h4>
                                                <hr>
                                            </div>
                                            <p><strong>Rindra Ramamonjison</strong></p>
                                            <div>
                                                <p>Rindra Ramamonjison is a fourth year PhD student of electrical engineering at the University of British Columbia, Vancouver. He received his master's degree from Tokyo Institute of Technology. He has played various roles in many engineering companies, within telecom and finance industries. His primary research interests are machine learning, optimization, graph processing, and statistical signal processing. Rindra is also the co-organizer of the Vancouver Spark Meetup.</p>
                                            </div>
                                        </div>
                                        
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    
                    <div class="next-wrapper">
                        <div class="row ns">
                            <hr />
                            <span class="hidden-xs">
                                <h4 class="pull-left">
                                    <strong>Up Next: </strong><span class="section-title"></span>
                                </h4>
                                <a href="#" class="btn btn-primary pull-right btn-lg">
                                    Next Section
                                </a>
                            </span>
                            <span class="visible-xs">
                                <a href="#" class="btn btn-primary btn-block btn-lg">
                                    Next Section
                                </a>
                            </span>
                        </div>
                        <div class="row ns">
                            <hr>
                        </div>
                    </div>
                </div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch01"></a>ChapterÂ 1.Â Getting Started with Spark and GraphX</h2></div></div></div><p>Apache Spark is a cluster-computing platform for the processing of large distributed datasets. Data processing in Spark is both fast and easy, thanks to its optimized parallel computation engine and its flexible and unified API. The core abstraction in Spark is based on the concept of <a id="id0" class="indexterm"></a>
<span class="strong"><strong>Resilient Distributed Dataset</strong></span> (<span class="strong"><strong>RDD</strong></span>). By extending the MapReduce framework, Spark's Core API makes analytics jobs easier to write. On top of the Core API, Spark offers an integrated set of high-level libraries that can be used for specialized tasks such as graph processing or machine learning. In particular, GraphX is the library to perform graph-parallel processing in Spark.</p><p>This chapter will introduce you to Spark and GraphX by building a social network and exploring the links between<a id="id1" class="indexterm"></a> people in the network. In addition, you will learn to use the <span class="strong"><strong>Scala Build Tool</strong></span> (<span class="strong"><strong>SBT</strong></span>) to build and run a Spark program. By the end of this chapter, you will know how to:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Install Spark successfully on your computer<a id="id2" class="indexterm"></a></p></li><li style="list-style-type: disc"><p>Experiment with the Spark shell and review Spark's data abstractions</p></li><li style="list-style-type: disc"><p>Create a graph and explore the links using base RDD and graph operations</p></li><li style="list-style-type: disc"><p>Build and submit a standalone Spark application with SBT</p></li></ul></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec09"></a>Downloading and installing Spark 1.4.1</h2></div></div><hr /></div><p>In the following<a id="id3" class="indexterm"></a> section, we will go through the Spark installation<a id="id4" class="indexterm"></a> process in detail. Spark is built <a id="id5" class="indexterm"></a>on Scala and runs on the <a id="id6" class="indexterm"></a>
<span class="strong"><strong>Java Virtual Machine </strong></span>(<span class="strong"><strong>JVM</strong></span>). Before installing Spark, you should first have <span class="strong"><strong>Java Development Kit 7</strong></span> (<span class="strong"><strong>JDK</strong></span>) installed<a id="id7" class="indexterm"></a> on your computer.</p><p>Make sure you install JDK instead of <span class="strong"><strong>Java Runtime Environment</strong></span> (<span class="strong"><strong>JRE</strong></span>). You can download<a id="id8" class="indexterm"></a> it from <a class="ulink" href="http://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads-1880260.html" target="_blank">http://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads-1880260.html</a>.</p><p>Next, download the latest<a id="id9" class="indexterm"></a> release of Spark from the project website <a class="ulink" href="https://spark.apache.org/downloads.html" target="_blank">https://spark.apache.org/downloads.html</a>. Perform the following three steps to get Spark installed on your computer:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Select the package<a id="id10" class="indexterm"></a> type: <span class="strong"><strong>Pre-built for Hadoop 2.6 and later</strong></span> and then <span class="strong"><strong>Direct Download</strong></span>. Make sure you choose a prebuilt version for Hadoop instead of the source code.</p></li><li><p>Download the compressed TAR file called <code class="literal">spark-1.4.1-bin-hadoop2.6.tgz</code> and place it into a directory on your computer.</p></li><li><p>Open the terminal and<a id="id11" class="indexterm"></a> change to the previous directory. Using the following commands, extract the TAR file, rename the Spark root<a id="id12" class="indexterm"></a> folder to <code class="literal">spark-1.4.1</code>, and then list the installed files and subdirectories:</p><div class="informalexample"><pre class="programlisting">tar -xf spark-1.4.1-bin-hadoop2.6.tgz
  mv spark-1.4.1-bin-hadoop2.6 spark-1.4.1
  cd spark-1.4.1
  ls </pre></div></li></ol></div><p>That's it! You now have Spark and its libraries installed on your computer. Note the following files and directories in the <code class="literal">spark-1.4.1</code> home folder:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><code class="literal">core</code>: This directory <a id="id13" class="indexterm"></a>contains the source code for the core components and API of Spark</p></li><li style="list-style-type: disc"><p><code class="literal">bin</code>: This directory contains <a id="id14" class="indexterm"></a>the executable files that are used to submit and deploy Spark applications or also to interact with Spark in a <span class="strong"><strong>Spark shell</strong></span></p></li><li style="list-style-type: disc"><p><code class="literal">graphx</code>, <code class="literal">mllib</code>, <code class="literal">sql</code>, and <code class="literal">streaming</code>: These are Spark libraries that provide a unified interface to<a id="id15" class="indexterm"></a> do different types of data processing, namely graph processing, machine learning, queries, and <a id="id16" class="indexterm"></a>stream <a id="id17" class="indexterm"></a>processing</p></li><li style="list-style-type: disc"><p><code class="literal">examples</code>: This directory<a id="id18" class="indexterm"></a> contains demos and examples of Spark applications</p></li></ul></div><p>It is often convenient to create shortcuts to the Spark home folder and Spark example folders. In Linux or Mac, open or create the <code class="literal">~/.bash_profile</code> file in your home folder and insert the following lines:</p><div class="informalexample"><pre class="programlisting">export SPARKHOME="/[Where you put Spark]/spark-1.4.1/"
export SPARKSCALAEX="ls ../spark- 1.4.1/examples/src/main/scala/org/apache/spark/examples/"</pre></div><p>Then, execute the following command for the previous shortcuts to take effect:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>source ~/.bash_profile</strong></span>
</pre></div><p>As a result, you can quickly access these folders in the terminal or Spark shell. For example, the<a id="id19" class="indexterm"></a> example<a id="id20" class="indexterm"></a> named <code class="literal">LiveJournalPageRank.scala</code> can be accessed <a id="id21" class="indexterm"></a>with:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$SPARKSCALAEX/graphx/LiveJournalPageRank.scala</strong></span>
</pre></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec10"></a>Experimenting with the Spark shell</h2></div></div><hr /></div><p>The best way to learn <a id="id22" class="indexterm"></a>Spark is through the Spark shell. There are two different shells for Scala and Python. But since the GraphX library is the most complete in Scala at the time this book was written, we are going to use the <code class="literal">spark-shell</code>, that is, the Scala shell. Let's launch the Spark shell inside the <code class="literal">$SPARKHOME/bin</code> from the command line:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$SPARKHOME/bin/spark-shell</strong></span>
</pre></div><p>If you set the current directory (<code class="literal">cd</code>) to <code class="literal">$SPARKHOME</code>, you can simply launch the shell with:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>cd $SPARKHOME</strong></span>
<span class="strong"><strong>./bin/spark-shell</strong></span>
</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note02"></a>Note</h3><p>If you happen to get an error saying something like: <code class="literal">Failed to find Spark assembly in spark-1.4.1/assembly/target/scala-2.10. You need to build Spark before running this program</code>, then it means that you have downloaded the Spark source code instead of a prebuilt version of Spark. In that case, go back to the project website and choose a prebuilt version of Spark.</p></div><p>If you were successful in launching the Spark shell, you should see the welcome message like this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>  Welcome to</strong></span>
<span class="strong"><strong>      ____              __</strong></span>
<span class="strong"><strong>     / __/__  ___ _____/ /__</strong></span>
<span class="strong"><strong>    _\ \/ _ \/ _ '/ __/  '_/</strong></span>
<span class="strong"><strong>   /___/ .__/\_,_/_/ /_/\_\   version 1.4.1</strong></span>
<span class="strong"><strong>      /_/</strong></span>
<span class="strong"><strong>Using Scala version 2.10.4 (Java HotSpot(TM) 64-Bit Server VM, Java)</strong></span>
</pre></div><p>For a sanity check, you can type in some Scala expressions or declarations and have them evaluated. Let's type some commands into the shell now:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; sc</strong></span>
<span class="strong"><strong>res1: org.apache.spark.SparkContext = org.apache.spark.SparkContext@52e52233</strong></span>
<span class="strong"><strong>scala&gt; val myRDD = sc.parallelize(List(1,2,3,4,5))</strong></span>
<span class="strong"><strong>myRDD: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:12</strong></span>
<span class="strong"><strong>scala&gt; sc.textFile("README.md").filter(line =&gt; line contains "Spark").count()</strong></span>
<span class="strong"><strong>res2: Long = 21</strong></span>
</pre></div><p>Here is what you can tell about the preceding code. First, we displayed the Spark context defined by the variable <code class="literal">sc</code>, which is automatically created when you launch the Spark shell. The Spark <a id="id23" class="indexterm"></a>context is the point of entry to the Spark API. Second, we created an RDD named <code class="literal">myRDD</code> that was obtained by calling the <code class="literal">parallelize</code> function for a list of five numbers. Finally, we loaded the <code class="literal">README.md</code> file into an RDD, filtered the lines that contain the word <code class="literal">"Spark"</code>, and finally invoked an action on the filtered RDD to count the number of those lines.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note03"></a>Note</h3><p>It is expected that you are already familiar with the basic RDD transformations and actions, such as map, reduce, and filter. If that is not the case, I recommend that you<a id="id24" class="indexterm"></a> learn them first, perhaps by reading the programming guide at <a class="ulink" href="https://spark.apache.org/docs/latest/programming-guide.html" target="_blank">https://spark.apache.org/docs/latest/programming-guide.html</a> or an introductory book such as <span class="emphasis"><em>Fast Data Processing with Spark</em></span> by Packt Publishing and <span class="emphasis"><em>Learning Spark</em></span> by O'Reilly Media.</p></div><p>Don't panic if you did not fully grasp the mechanisms behind RDDs. The following refresher, however, helps you to remember the important points. RDD is the core data abstraction in Spark to represent a distributed collection of large datasets that can be partitioned and processed in parallel across a cluster of machines. The Spark API provides a uniform set of operations to transform and reduce the data within an RDD. On top of these abstractions and operations, the GraphX library also offers a flexible API that enables us to create graphs and operate on them easily.</p><p>Perhaps, when you ran the preceding commands in the Spark shell, you were overwhelmed by the long list of logging statements that start with <code class="literal">INFO</code>. There is a way to reduce the amount of information that Spark outputs in the shell.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip02"></a>Tip</h3><p>You can reduce the level of verbosity of the Spark shell as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>First, go to the <code class="literal">$SCALAHOME/conf</code> folder</p></li><li style="list-style-type: disc"><p>Then, create a new file called <code class="literal">log4j.properties</code></p></li><li style="list-style-type: disc"><p>Inside the <code class="literal">conf</code> folder, open the template file <code class="literal">log4j.properties.template</code> and copy all its content into <code class="literal">log4j.properties</code></p></li><li style="list-style-type: disc"><p>Find and replace the line <code class="literal">log4j.rootCategory=INFO, console</code> with either one of these two lines:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><code class="literal">log4j.rootCategory=WARN, console</code></p></li><li style="list-style-type: disc"><p><code class="literal">log4j.rootCategory=ERROR, console</code></p></li></ul></div></li><li style="list-style-type: disc"><p>Finally, restart the Spark shell and you should now see fewer logging messages in the shell outputs</p></li></ul></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec11"></a>Getting started with GraphX</h2></div></div><hr /></div><p>Now that we have installed Spark and experimented with the Spark shell, let's create our first graph in Spark by<a id="id25" class="indexterm"></a> writing our code in the shell, and then building upon that code to develop and run a standalone program. We have three learning goals in this section:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>First, you will learn how to construct and explore graphs using the Spark Core and GraphX API through a concrete example.</p></li><li><p>Second, you will review some important Scala programming features that are important to know when doing graph processing in Spark.</p></li><li><p>Third, you will learn how to develop and run a standalone Spark application.</p></li></ol></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec07"></a>Building a tiny social network</h3></div></div></div><p>Let's create a tiny <a id="id26" class="indexterm"></a>social network and explore the relationships among the different people in the network. Again, the best way to learn Spark is inside the shell. Our workflow is therefore to first experiment in the shell and then <a id="id27" class="indexterm"></a>migrate our code later into a standalone Spark application. Before launching the shell, make sure to change the current directory to <code class="literal">$SPARKHOME</code>.</p><p>First, we need to import the GraphX and RDD module, as shown, so that we can invoke its APIs with their shorter names:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.graphx._</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.rdd.RDD</strong></span>
</pre></div><p>As said previously, <code class="literal">SparkContext</code> is the main point of entry into a Spark program and it is created automatically in the Spark shell. It also offers useful methods to create RDDs from local collections, to load data from a local or Hadoop file system into RDDs, and to save output data on disks.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec01"></a>Loading the data</h4></div></div></div><p>In this example, we will work with two CSV files <code class="literal">people.csv</code> and <code class="literal">links.csv</code>, which are contained in<a id="id28" class="indexterm"></a> the directory <code class="literal">$SPARKHOME/data/</code>. Let's type the following commands to load these files into Spark:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val people = sc.textFile("./data/people.csv")</strong></span>
<span class="strong"><strong>people: org.apache.spark.rdd.RDD[String] = ./data/people.csv MappedRDD[81] at textFile at &lt;console&gt;:33</strong></span>

<span class="strong"><strong>scala&gt; val links = sc.textFile("./data/links.csv")</strong></span>
<span class="strong"><strong>links: org.apache.spark.rdd.RDD[String] = ./data/links.csv MappedRDD[83] at textFile at &lt;console&gt;:33</strong></span>
</pre></div><p>Loading the CSV files just gave us back two RDDs of strings. To create our graph, we need to parse these strings into two suitable collections of vertices and edges.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note04"></a>Note</h3><p>It is important that your current directory inside the shell is <code class="literal">$SPARKHOME</code>. Otherwise, you get an error later because Spark cannot find the files.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec02"></a>The property graph</h4></div></div></div><p>Before going further, let's introduce<a id="id29" class="indexterm"></a> some key definitions and graph abstractions. In Spark, a<a id="id30" class="indexterm"></a> graph is represented by a <span class="strong"><strong>property graph</strong></span>, which is defined in the <code class="literal">Graph</code> class as:</p><div class="informalexample"><pre class="programlisting">class Graph[VD, ED] {
val vertices: VertexRDD[VD]
  val edges: EdgeRDD[ED,VD]
}</pre></div><p>This means that the <code class="literal">Graph</code> class provides getters to access its vertices and its edges. These are later abstracted by the RDD subclasses <code class="literal">VertexRDD[VD]</code> and <code class="literal">EdgeRDD[ED, VD]</code>. Note that <code class="literal">VD</code> and <code class="literal">ED</code> here denote some Scala-type parameters of the classes <code class="literal">VertexRDD</code>, <code class="literal">EdgeRDD</code>, and <code class="literal">Graph</code>. These types of parameters can be primitive types, such as <code class="literal">String,</code> or also user-defined classes, such as the <code class="literal">Person</code> class, in our example of a social graph. It is important to note that the property graph in Spark is a directed multigraph. It means that the graph is permitted to have multiple edges between any pair of vertices. Moreover, each edge is directed and defines a unidirectional relationship. This is easy to grasp, for instance, in a Twitter graph where a user can follow another one but the converse does not need to be true. To model bidirectional links, such as a Facebook friendship, we need to define two edges between the nodes, and these edges should point in opposite directions. Additional properties about the relationship can be stored as an attribute of the edge.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note05"></a>Note</h3><p>A property graph is a graph with user-defined objects attached to each vertex and edge. The classes of these objects describe the properties of the graph. This is done in practice by parameterizing the class <code class="literal">Graph</code>, <code class="literal">VertexRDD</code>, and <code class="literal">EdgeRDD</code>. Moreover, each<a id="id31" class="indexterm"></a> edge of the graph defines a unidirectional relationship but multiple edges can exist between any pair of vertices.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec03"></a>Transforming RDDs to VertexRDD and EdgeRDD</h4></div></div></div><p>Going back to our example, let's construct the graph in three steps, as follows:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>We define a <a id="id32" class="indexterm"></a>case class <code class="literal">Person</code>, which has <code class="literal">name</code> and <code class="literal">age</code> as class parameters. Case classes are very useful <a id="id33" class="indexterm"></a>when we need to do pattern matching on an object <code class="literal">Person</code> later on:</p><div class="informalexample"><pre class="programlisting">case class Person(name: String, age: Int)</pre></div></li><li><p>Next, we are going to parse each line of the CSV texts inside people and links into new objects of type <code class="literal">Person</code> and <code class="literal">Edge</code> respectively, and collect the results in <code class="literal">RDD[(VertexId, Person)]</code> and <code class="literal">RDD[Edge[String]]</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>val peopleRDD: RDD[(VertexId, Person)] = people map { line =&gt; </strong></span>
<span class="strong"><strong>  val row = line split ','</strong></span>
<span class="strong"><strong>  (row(0).toInt, Person(row(1), row(2).toInt))</strong></span>
<span class="strong"><strong>}</strong></span>
<span class="strong"><strong>scala&gt; type Connection = String</strong></span>
<span class="strong"><strong>scala&gt; val linksRDD: RDD[Edge[Connection]] = links map {line =&gt; </strong></span>
<span class="strong"><strong>  val row = line split ','</strong></span>
<span class="strong"><strong>  Edge(row(0).toInt, row(1).toInt, row(2))</strong></span>
<span class="strong"><strong>}</strong></span>
</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note06"></a>Note</h3><p>To paste or write code in multiple lines in the shell:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Type the command <code class="literal">:paste</code></p></li><li style="list-style-type: disc"><p>Paste or write the given code</p></li><li style="list-style-type: disc"><p>Evaluate the code by pressing the keys <span class="emphasis"><em>Cmd</em></span> + <span class="emphasis"><em>D</em></span> on Mac or <span class="emphasis"><em>Ctrl</em></span> + <span class="emphasis"><em>D</em></span> in Windows</p></li></ul></div><p><code class="literal">VertexId</code> is simply a type alias for <code class="literal">Long</code> as defined in GraphX. In addition, the <code class="literal">Edge</code> class is defined in <code class="literal">org.apache.spark.graphx.Edge</code> as:</p><p><code class="literal">class Edge(srcId: VertexId, dstId: VertexId, attr: ED)</code></p><p>The class parameters <code class="literal">srcId</code> and <code class="literal">dstId</code> are the vertex IDs of the source and destination, which are linked by the edge. In our social network example, the link between two people is unidirectional and its property is described in the <code class="literal">attr</code> of type <code class="literal">Connection</code>. Note that we defined <code class="literal">Connection</code> as a type alias for <code class="literal">String</code>. For clarity, it often helps to give a meaningful name to the type parameter of <code class="literal">Edge</code>.</p></div></li><li><p>Now, we <a id="id34" class="indexterm"></a>can create our social graph and name it <code class="literal">tinySocial</code> using the factory method <code class="literal">Graph(â€¦)</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val tinySocial: Graph[Person, Connection] = Graph(peopleRDD, linksRDD)</strong></span>
<span class="strong"><strong>tinySocial: org.apache.spark.graphx.Graph[Person,Connection] = org.apache.spark.graphx.impl.GraphImpl@128cd92a</strong></span>
</pre></div></li></ol></div><p>There are two things to note about this constructor. I told you earlier that the member vertices and edges of the graph are instances of <code class="literal">VertexRDD[VD]</code> and <code class="literal">EdgeRDD[ED,VD]</code>. However, we passed <code class="literal">RDD[(VertexId, Person)]</code> and <code class="literal">RDD[Edge[Connection]]</code> into the above factory<a id="id35" class="indexterm"></a> method <code class="literal">Graph</code>. How did that work? It worked because <code class="literal">VertexRDD[VD]</code> and <code class="literal">EdgeRDD[ED,VD]</code> are subclasses of <code class="literal">RDD[(VertexId, Person)]</code> and <code class="literal">RDD[Edge[Connection]]</code> respectively. In addition, <code class="literal">VertexRDD[VD]</code> adds the constraint that <code class="literal">VertexID</code> occurs only once. Basically, two people in our social network cannot have the same vertex ID. Furthermore, <code class="literal">VertexRDD[VD]</code> and <code class="literal">EdgeRDD[ED,VD]</code> provide several other operations to transform vertex and edge attributes. We will see more of these in later chapters.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec04"></a>Introducing graph operations</h4></div></div></div><p>Finally, we are going<a id="id36" class="indexterm"></a> to look at the vertices and edges in the network by accessing and collecting them:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; tinySocial.vertices.collect()</strong></span>
<span class="strong"><strong>res: Array[(org.apache.spark.graphx.VertexId, Person)] = Array((4,Person(Dave,25)), (6,Person(Faith,21)), (8,Person(Harvey,47)), (2,Person(Bob,18)), (1,Person(Alice,20)), (3,Person(Charlie,30)), (7,Person(George,34)), (9,Person(Ivy,21)), (5,Person(Eve,30)))</strong></span>
<span class="strong"><strong>scala&gt; tinySocial.edges.collect()</strong></span>
<span class="strong"><strong>res: Array[org.apache.spark.graphx.Edge[Connection]] = Array(Edge(1,2,friend), Edge(1,3,sister), Edge(2,4,brother), Edge(3,2,boss), Edge(4,5,client), Edge(1,9,friend), Edge(6,7,cousin), Edge(7,9,coworker), Edge(8,9,father))</strong></span>
</pre></div><p>We used the <code class="literal">edges</code> and <code class="literal">vertices</code> getters in the <code class="literal">Graph</code> class and used the RDD action <code class="literal">collect</code> to put the result into a local collection.</p><p>Now, suppose we <a id="id37" class="indexterm"></a>want to print only the professional connections that are listed in the following <code class="literal">profLinks</code> list:</p><div class="informalexample"><pre class="programlisting">val profLinks: List[Connection] = List("Coworker", "Boss", "Employee","Client", "Supplier")</pre></div><p>A bad way to arrive at the desired result is to filter the edges corresponding to professional connections, then loop through the filtered edges, extract the corresponding vertices' names, and print the connections between the source and destination vertices. We can write that method in the following code:</p><div class="informalexample"><pre class="programlisting">val profNetwork = 
tinySocial.edges.filter{ case Edge(_,_,link) =&gt; profLinks.contains(link)}
for {
  Edge(src, dst, link) &lt;- profNetwork.collect() 
  srcName = (peopleRDD.filter{case (id, person) =&gt; id == src} first)._2.name
  dstName = (peopleRDD.filter{case (id, person) =&gt; id == dst} first)._2.name
} println(srcName + " is a " + link + " of " + dstName)

Charlie is a boss of Bob
Dave is a client of Eve
George is a coworker of Ivy</pre></div><p>There are two problems with the preceding code. First, it could be more concise and expressive. Second, it is not efficient due to the filtering operations inside the for loop.</p><p>Luckily, there is a better alternative. The GraphX library provides two different ways to view data: either as a graph or as tables of edges, vertices, and triplets. For each view, the library offers a rich set operations whose implementations are optimized for execution. That means that we can often process a graph using a predefined graph operation or algorithm, easily. For instance, we could simplify the previous code and make it more efficient, as follows:</p><div class="informalexample"><pre class="programlisting">tinySocial.subgraph(profLinks contains _.attr).
     triplets.foreach(t =&gt; println(t.srcAttr.name + " is a " + t.attr + " of " + t.dstAttr.name))
  Charlie is a boss of Bob
  Dave is a client of Eve
  George is a coworker of Ivy</pre></div><p>We simply used the <code class="literal">subgraph</code> operation to filter the professional links. Then, we used the <span class="strong"><strong>triplet view</strong></span> to <a id="id38" class="indexterm"></a>access the attributes of the edges and vertices simultaneously. In brief, the triplet operator returns <a id="id39" class="indexterm"></a>an RDD of <code class="literal">EdgeTriplet[Person, Connection]</code>. Note that <code class="literal">EdgeTriplet</code> is simply an alias for the parameterized type of 3-tuple <code class="literal">((VertexId, Person), (VertexId, Person), Connection)</code> that contains all the information about the source node, the destination node, and the edge property.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec08"></a>Building and submitting a standalone application</h3></div></div></div><p>Let's<a id="id40" class="indexterm"></a> conclude this <a id="id41" class="indexterm"></a>chapter by developing and running a<a id="id42" class="indexterm"></a> standalone Spark application<a id="id43" class="indexterm"></a> for our social network example.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec05"></a>Writing and configuring a Spark program</h4></div></div></div><p>Satisfied with our <a id="id44" class="indexterm"></a>experiment in the shell, let's now write our first Spark program. Open your favorite text editor and create a new file <a id="id45" class="indexterm"></a>named <code class="literal">simpleGraph.scala</code> and <a id="id46" class="indexterm"></a>put it in the folder <code class="literal">$SPARKHOME/exercises/chap1</code>. A template<a id="id47" class="indexterm"></a> for a Spark program looks like the following code:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.apache.spark.rdd.RDD
import org.apache.spark.graphx._
object SimpleGraphApp {
  def main(args: Array[String]){

    // Configure the program 
    val conf = new SparkConf()
          .setAppName("Tiny Social")
          .setMaster("local")
          .set("spark.driver.memory", "2G")
    val sc = new SparkContext(conf)

    // Load some data into RDDs
    val people = sc.textFile("./data/people.csv")
    val links = sc.textFile("./data/links.csv")
 
    // After that, we use the Spark API as in the shell
    // ...
  }
}</pre></div><p>You can also<a id="id48" class="indexterm"></a> see the entire code of our <code class="literal">SimpleGraph.scala</code> file in the example files, which you can <a id="id49" class="indexterm"></a>download from the Packt website.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip03"></a>Tip</h3><p><span class="strong"><strong>Downloading the example code</strong></span></p><p>You can download the example code files from your account at <a class="ulink" href="http://www.packtpub.com" target="_blank">http://www.packtpub.com</a> for all the Packt Publishing books you have purchased. If you purchased this book elsewhere, you can visit <a class="ulink" href="http://www.packtpub.com/support" target="_blank">http://www.packtpub.com/support</a> and register to have the files e-mailed directly to you.</p></div><p>Let's go over this code to<a id="id50" class="indexterm"></a> understand what is required to create and configure a Spark standalone program in Scala.</p><p>As a Scala program, our <a id="id51" class="indexterm"></a>Spark application should be constructed within a top-level Scala object, which must have a <code class="literal">main</code> function that has the signature: <code class="literal">def main(args: Array[String]): Unit</code>. In other words, the main program accepts an array of strings as a parameter and returns nothing. In our example, the top-level object is <code class="literal">SimpleGraphApp</code>.</p><p>At the beginning of <code class="literal">simpleGraph.scala</code>, we have put the following import statements:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf</pre></div><p>The first two lines import the <code class="literal">SparkContext</code> class as well as some implicit conversions defined in its companion object. It is not very important to know what the implicit conversions are. Just make sure you import both <code class="literal">SparkContext</code> and <code class="literal">SparContext._</code></p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note08"></a>Note</h3><p>When we worked in the Spark shell, <code class="literal">SparkContext</code> and <code class="literal">SparContext._</code> were imported automatically for us.</p></div><p>The third line imports <code class="literal">SparkConf</code>, which is a wrapper class that contains the configuration settings <a id="id52" class="indexterm"></a>of a Spark application, such as its application name, the memory size of each executor, and the address of the master or cluster manager.</p><p>Next, we have<a id="id53" class="indexterm"></a> imported some RDD and <a id="id54" class="indexterm"></a>GraphX-specific class constructors and operators with these lines:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark.rdd.RDD
import org.apache.spark.graphx._</pre></div><p>The underscore after <code class="literal">org.apache.spark.graphx</code> makes sure that all public APIs in GraphX get imported.</p><p>Within <code class="literal">main</code>, we <a id="id55" class="indexterm"></a>had to first configure the Spark program. To do this, we created an object called <code class="literal">SparkConf</code> and set the application settings through a chain of setter methods on the <code class="literal">SparkConf</code> object. <code class="literal">SparkConf</code> provides specific setters for some common properties, such as the application name or master. Alternatively, a generic <code class="literal">set</code> method can be used to set multiple properties together by passing them as a sequence of key-value pairs. The most common configuration parameters are listed in the following table with their default values and usage. The extensive list <a id="id56" class="indexterm"></a>can be found at <a class="ulink" href="https://spark.apache.org/docs/latest/configuration.html" target="_blank">https://spark.apache.org/docs/latest/configuration.html</a>:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Spark property name</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Usage and default value</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p><code class="literal">spark.app.name</code></p>
</td><td style="" align="left" valign="top">
<p>This is the name <a id="id57" class="indexterm"></a>of your application. This will appear in the UI and in the log data.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">spark.master</code></p>
</td><td style="" align="left" valign="top">
<p>This is the <a id="id58" class="indexterm"></a>cluster manager to connect to, for example, <code class="literal">spark://host:port</code>, <code class="literal">mesos://host:port</code>, <code class="literal">yarn</code>, or <code class="literal">local</code>.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">spark.executor.memory</code></p>
</td><td style="" align="left" valign="top">
<p>This is the amount of memory to use per executor process, in the same<a id="id59" class="indexterm"></a> format as JVM memory strings (for example, 512 M, 2 G). The default value is 1 G.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">spark.driver.memory</code></p>
</td><td style="" align="left" valign="top">
<p>When you<a id="id60" class="indexterm"></a> run Spark locally with <code class="literal">spark.master=local</code>, your executor becomes the driver and you need to set this parameter instead of <code class="literal">spark.executor.memory</code>. The default value is 512 M.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">spark.storage.memoryFraction</code></p>
</td><td style="" align="left" valign="top">
<p>This is the<a id="id61" class="indexterm"></a> fraction of Java heap to use for Spark's memory cache. The default is 0.6.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p><code class="literal">spark.serializer</code></p>
</td><td style="" align="left" valign="top">
<p>This is the <a id="id62" class="indexterm"></a>class used to serialize objects to be sent over the network or to be cached in serialized form. This is the subclass of the default class <code class="literal">org.apache.spark.serializer.JavaSerializer</code>.</p>
</td></tr></tbody></table></div><p>In our example, we initialized the program as follows:</p><div class="informalexample"><pre class="programlisting">val conf = new SparkConf()
      .setAppName("Tiny Social")
      .setMaster("local")
      .set("spark.driver.memory", "2G")
val sc = new SparkContext(conf)</pre></div><p>Precisely, we set the name of our application to <code class="literal">"Tiny Social"</code> and the master to be the local machine on <a id="id63" class="indexterm"></a>which we submit the application. In addition, the driver memory is set to 2 GB. Should we have set the master to be a cluster<a id="id64" class="indexterm"></a> instead of local, we can specify the memory per executor by setting <code class="literal">spark.executor.memory</code> instead of <code class="literal">spark.driver.memory</code>.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note09"></a>Note</h3><p>In principle, the driver and executor have different roles and, in general, they run on different processes except when we set the master to be local. The driver is the process that compiles our program into tasks, schedules these tasks to one of more executors, and maintains the physical location of every RDD. Each executor is responsible for executing the tasks, and storing and caching RDDs in memory.</p></div><p>It is not mandatory to set the Spark application settings in the <code class="literal">SparkConf</code> object inside your program. Alternatively, when submitting our application, we could set these parameters as command-line options of the <code class="literal">spark-submit</code> tool. We will cover that part in detail in the following sections. In this case, we will just create our <code class="literal">SparkContext</code> object as:</p><div class="informalexample"><pre class="programlisting">val sc = new SparkContext(new SparkConf())</pre></div><p>After configuring the program, the next task is to load the data that we want to process by calling utility methods such as <code class="literal">sc.textFile</code> on the <code class="literal">SparkContext</code> object <code class="literal">sc</code>:</p><div class="informalexample"><pre class="programlisting">val people = sc.textFile("./data/people.csv")
val links = sc.textFile("./data/links.csv")</pre></div><p>Finally, the rest of the program consists of the same operations on RDDs and graphs that we have used in the shell.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note10"></a>Note</h3><p>To avoid confusion when passing a relative file path to I/O actions such as <code class="literal">sc.textFile()</code>, the convention used in this book is that the current directory of the command line is always set to the project root folder. For instance, if our Tiny Social app's<a id="id65" class="indexterm"></a> root folder is <code class="literal">$SPARKHOME/exercises/chap1</code>, then Spark will look for the data to be loaded in <code class="literal">$SPARKHOME/exercises/chap1/data</code>. This assumes that we have put the files in that <code class="literal">data</code><a id="id66" class="indexterm"></a> folder.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec06"></a>Building the program with the Scala Build Tool</h4></div></div></div><p>After writing our entire<a id="id67" class="indexterm"></a> program, we are going to build it using the <span class="strong"><strong>Scala Build Tool</strong></span> (<span class="strong"><strong>SBT</strong></span>). If you do not have SBT installed on your computer yet, you need to install it first. Detailed instructions<a id="id68" class="indexterm"></a> on how to install SBT are available at <a class="ulink" href="http://www.scala-sbt.org/0.13/tutorial/index.html" target="_blank">http://www.scala-sbt.org/0.13/tutorial/index.html</a> for most operating<a id="id69" class="indexterm"></a> systems. While there are different ways to install SBT, I recommend using a package manager instead<a id="id70" class="indexterm"></a> of the manual installation. After the installation, execute the following command to append the SBT installation folder to the <code class="literal">PATH</code> environment variable:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ export PATH=$PATH:/usr/local/bin/sbtl</strong></span>
</pre></div><p>Once we have SBT properly installed, we can use it to build our application with all its dependencies inside a single JAR package file, also called <span class="strong"><strong>uber jar</strong></span>. In fact, when running a Spark application on several worker machines, an error will occur if some machines do not have the right dependency JAR.</p><p>By packaging an uber jar with SBT, the application code and its dependencies are all distributed to the workers. Concretely, we need to create a build definition file in which we set the project settings. Moreover, we must specify the dependencies and the resolvers that help SBT find the packages that are needed by our program. A resolver<a id="id71" class="indexterm"></a> indicates the name and location of the repository that has the required JAR file. Let's create a file called <code class="literal">build.sbt</code> in the project root folder <code class="literal">$SPARKHOME/exercises/chap1</code> and insert the following lines:</p><div class="informalexample"><pre class="programlisting">name := "Simple Project"

version := "1.0"

scalaVersion := "2.10.4"

libraryDependencies ++= Seq(
  "org.apache.spark" %% "spark-core" % "1.4.1",
  "org.apache.spark" %% "spark-graphx" % "1.4.1"
)

resolvers += "Akka Repository" at "http://repo.akka.io/releases/"</pre></div><p>By convention, the settings are defined by Scala expressions and they need to be delimited by blank lines. Earlier, we set the project name, its version number, the version of Scala, as well as the<a id="id72" class="indexterm"></a> Spark library dependencies. To build the program, we then enter the command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sbt package</strong></span>
</pre></div><p>This will create a JAR file inside <code class="literal">$SPARKHOME/exercises/chap1/target/scala-2.10/simple-project_2.10-1.0.jar</code>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec07"></a>Deploying and running with spark-submit</h4></div></div></div><p>Finally, we are<a id="id73" class="indexterm"></a> going to invoke the <code class="literal">spark-submit</code> script in <a id="id74" class="indexterm"></a>
<code class="literal">$SPARKHOME/bin/</code> to<a id="id75" class="indexterm"></a> run the program from the root <a id="id76" class="indexterm"></a>directory <code class="literal">$SPARKHOME/exercises/chap1</code> in the terminal:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ../../bin/spark-submit --class \ </strong></span>
<span class="strong"><strong>"SimpleGraphApp" \       </strong></span>
<span class="strong"><strong>./target/scala-2.10/simple-project_2.10-1.0.jar </strong></span>
<span class="strong"><strong>Spark assembly has been built with Hive, including Datanucleus jars on classpath</strong></span>
<span class="strong"><strong>Charlie is a boss of Bob</strong></span>
<span class="strong"><strong>Dave is a client of Eve</strong></span>
<span class="strong"><strong>George is a coworker of Ivy</strong></span>
</pre></div><p>The required options for the <code class="literal">spark-submit</code> command are the Scala application object name and the JAR file that we<a id="id77" class="indexterm"></a> previously built with SBT. In case we did not set the master setting when<a id="id78" class="indexterm"></a> creating the <code class="literal">SparkConf</code> object, we also would have to specify the <code class="literal">--master</code> option in <code class="literal">spark-submit</code>.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip04"></a>Tip</h3><p>You can list all the <a id="id79" class="indexterm"></a>available options for the <code class="literal">spark-submit</code> script with the command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>spark-submit --help</strong></span>
</pre></div><p>More details about how to submit a Spark application to a remote cluster are available at <a class="ulink" href="http://spark.apache.org/docs/latest/submitting-applications.html" target="_blank">http://spark.apache.org/docs/latest/submitting-applications.html</a>.</p></div></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec12"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we took a whirlwind tour of graph processing in Spark. Specifically, we installed the Java Development Kit, a prebuilt version of Spark and the SBT tool. Furthermore, you were introduced to graph abstraction and operations in Spark by creating a social network in the Spark shell and also in a standalone program.</p><p>In the next chapter, you will learn more about how to build and explore graphs in Spark.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch02"></a>ChapterÂ 2.Â Building and Exploring Graphs</h2></div></div></div><p>This chapter aims to teach us how to represent various types of networks and complex systems as property graphs in Spark and GraphX. Before we can describe the behavior, and analyze the inner structure of these systems, we first need to map their components to vertices or nodes, and map the interactions between the individual components to edges or links. Building on what we learned in the previous chapter, we will delve into the details on how graphs are stored and represented in GraphX. In addition, this chapter introduces the language of graph theory, and the basic characteristics of graphs. Throughout this chapter, we will use real-world datasets that we will map to the different types of graphs. The examples include e-mail communication networks, food flavor network, and social ego networks. On completing this chapter, you will understand how to:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Load data and build Spark graphs in many ways</p></li><li style="list-style-type: disc"><p>Use the join operator to mix external data into existing graphs</p></li><li style="list-style-type: disc"><p>Build bipartite graphs and multigraphs</p></li><li style="list-style-type: disc"><p>Explore graphs and compute their basic statistics</p></li></ul></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec13"></a>Network datasets</h2></div></div><hr /></div><p>In the previous chapter, we<a id="id80" class="indexterm"></a> constructed a small social network as a toy example. From this chapter onwards, we are going to work with real-world datasets, drawn from various applications. In fact, graphs are used to represent any complex system as it describes the interactions between the components of the system. Despite the diversity in form, size, nature, and granularity of different systems, graph theory provides a common language, and a set of tools, for representing and analyzing complex systems.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note11"></a>Note</h3><p>In brief, a graph <a id="id81" class="indexterm"></a>consists of a set of vertices connected by a set of edges. Each edge represents the relationship between a pair of connected vertices. In this book, we will sometimes use the less technical terms network nodes to refer to vertices, and links to refer to edges. Note that Spark supports multigraphs, that is, it is permitted to have multiple edges between any pair of nodes.</p></div><p>Let's get a preview of the networks that we are going to build in this chapter.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec09"></a>The communication network</h3></div></div></div><p>The first type of <a id="id82" class="indexterm"></a>communication network that we will encounter is an <a id="id83" class="indexterm"></a>
<span class="strong"><strong>email communication graph</strong></span>. A history of <a id="id84" class="indexterm"></a>e-mails that are exchanged within an organization can be mapped to a communication graph, so as to understand the informal structure behind the organization. Such graphs can also be used to determine influential people or the hubs of the organization that might not necessarily be the high-ranked ones. The email communication network is a canonical example of a directed graph, as each e-mail links a source node to the destination node. We will use the <a id="id85" class="indexterm"></a>
<span class="strong"><strong>Enron Corpus</strong></span>, which is a database of e-mails generated by 158 employees of the Enron Corporation. It is one of the only mass collections of corporate e-mails that are open to public on the web. The Enron Corpus is particularly interesting, as it captures all the communication that occurred inside the company before the scandal that led to its bankruptcy. The original dataset was released by William Cohen<a id="id86" class="indexterm"></a> at CMU, which can be downloaded from <a class="ulink" href="https://www.cs.cmu.edu/~./enron/" target="_blank">https://www.cs.cmu.edu/~./enron/</a>. A detailed description of the complete dataset was done by Klimmt and Yang, 2004. A cleaner version of the dataset, which we use here, is provided by Leskovec et al., 2009, and can be obtained from <a class="ulink" href="https://snap.stanford.edu/data/email-Enron.html" target="_blank">https://snap.stanford.edu/data/email-Enron.html</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec10"></a>Flavor networks</h3></div></div></div><p>Another example that we<a id="id87" class="indexterm"></a> will borrow from the culinary world is the ingredient-compound network, introduced by Ahn et al., 2011. It is a bipartite graph in the sense that the nodes are divided into two disjoint sets: the <span class="strong"><strong>ingredient nodes</strong></span> <a id="id88" class="indexterm"></a>and the <a id="id89" class="indexterm"></a>
<span class="strong"><strong>compound nodes</strong></span>. Each link connects an ingredient to a compound when the chemical compound is present in the food ingredient. From the ingredient-compound network, it is also possible to create what is called a<a id="id90" class="indexterm"></a> <span class="strong"><strong>flavor network</strong></span>. Instead of connecting food ingredients to compounds, the flavor network links pairs of ingredients whenever a pair of ingredients shares at least one chemical compound.</p><p>We will build the ingredient-compound network in this chapter, and in <a class="link" href="#" linkend="ch04">Chapter 4</a>, <span class="emphasis"><em>Transforming and Shaping Up Graphs to Your Needs</em></span>, we will construct the flavor network from the ingredient-compound network. Analyzing such graphs is fascinating because they help us understand more about food pairing and food culture. The flavor network can also help food scientists or amateur cooks create new recipes. The datasets that we will use consist of <a id="id91" class="indexterm"></a>ingredient-compound data and the recipes collected from <a class="ulink" href="http://www.epicurious.com/" target="_blank">http://www.epicurious.com/</a>, <a class="ulink" href="http://allrecipes.com" target="_blank">allrecipes.com</a>, and <a class="ulink" href="http://www.menupan.com/" target="_blank">http://www.menupan.com/</a>. The datasets are available at <a class="ulink" href="http://yongyeol.com/2011/12/15/paper-flavor-network.html" target="_blank">http://yongyeol.com/2011/12/15/paper-flavor-network.html</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec11"></a>Social ego networks</h3></div></div></div><p>The last dataset that we will <a id="id92" class="indexterm"></a>explore in this chapter is a collection of social<a id="id93" class="indexterm"></a> ego networks from Google+. The data was collected by (McAuley and Leskovec, 2012) from the users who had manually shared their social circles using the <a id="id94" class="indexterm"></a>
<span class="strong"><strong>share circle</strong></span> feature. The dataset includes the user profiles, their circles, and their ego networks and can be downloaded<a id="id95" class="indexterm"></a> from Stanford's SNAP project website at <a class="ulink" href="http://snap.stanford.edu/data/egonets-Gplus.html" target="_blank">http://snap.stanford.edu/data/egonets-Gplus.html</a>.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note12"></a>Note</h3><p>These datasets are <span class="emphasis"><em>not</em></span> provided with the Spark installation. They must first be downloaded from their source websites and copied into the <code class="literal">$SPARKHOME/data</code> folder. When different sizes of the datasets are available, we chose to use the smaller version of the datasets to quickly demonstrate the concepts taught in this book.</p></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec14"></a>Graph builders</h2></div></div><hr /></div><p>In GraphX, there are four <a id="id96" class="indexterm"></a>functions for building a property graph. Each of these functions requires that the data from which the graph is constructed should be structured in a specified manner.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec12"></a>The Graph factory method</h3></div></div></div><p>The first<a id="id97" class="indexterm"></a> one is the <code class="literal">Graph</code> factory method<a id="id98" class="indexterm"></a> that we have already seen in the previous chapter. It is defined in the apply method of the companion object called <code class="literal">Graph</code>, which is as follows:</p><div class="informalexample"><pre class="programlisting">def apply[VD, ED](
      vertices: RDD[(VertexId, VD)],
      edges: RDD[Edge[ED]],
      defaultVertexAttr: VD = null)
    : Graph[VD, ED]</pre></div><p>As we have seen before, this<a id="id99" class="indexterm"></a> function takes two RDD collections: <code class="literal">RDD[(VertexId, VD)]</code> and <code class="literal">RDD[Edge[ED]]</code> as parameters for the vertices and edges respectively, to<a id="id100" class="indexterm"></a> construct a <code class="literal">Graph[VD, ED]</code> parameter. The <code class="literal">defaultVertexAttr</code> attribute is used to assign the default attribute for the vertices that are present in the edge RDD but not in the vertex RDD. The <code class="literal">Graph</code> factory method is convenient when the RDD collections of edges and vertices are readily available.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec13"></a>edgeListFile</h3></div></div></div><p>A more common <a id="id101" class="indexterm"></a>situation is that your original dataset only represents the <a id="id102" class="indexterm"></a>edges. In this case, GraphX provides the following <code class="literal">GraphLoader.edgeListFile</code> function that is defined in <code class="literal">GraphLoader</code>:</p><div class="informalexample"><pre class="programlisting">def edgeListFile(
      sc: SparkContext,
      path: String,
      canonicalOrientation: Boolean = false,
      minEdgePartitions: Int = 1)
      : Graph[Int, Int]</pre></div><p>It takes as an argument a path to the file that contains a list of edges. Each line of the file represents an edge of the graph with two integers in the form: <code class="literal">sourceID destinationID</code>. When reading the list, it ignores any comment line starting with <code class="literal">#</code>. Then, it constructs a graph from the specified edges with the corresponding vertices.</p><p>The <code class="literal">minEdgePartitions</code> argument is the minimum number of edge partitions to generate. If the adjacency list is partitioned with more blocks than <code class="literal">minEdgePartitions</code>, then more partitions will be created.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec14"></a>fromEdges</h3></div></div></div><p>Similar to <code class="literal">GraphLoader.edgeListFile</code>, the third<a id="id103" class="indexterm"></a> function named <code class="literal">Graph.fromEdges</code> enables you to<a id="id104" class="indexterm"></a> create a graph from an <code class="literal">RDD[Edge[ED]]</code> collection. Moreover, it automatically creates the vertices using the <code class="literal">VertexID</code> parameters specified by the edge RDD, as well as the <code class="literal">defaultValue</code> argument as a default vertex attribute:</p><div class="informalexample"><pre class="programlisting">def fromEdges[VD, ED](
      edges: RDD[Edge[ED]],
      defaultValue: VD)
: Graph[VD, ED]</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec15"></a>fromEdgeTuples</h3></div></div></div><p>The last graph<a id="id105" class="indexterm"></a> builder function is <code class="literal">Graph.fromEdgeTuples</code>, which<a id="id106" class="indexterm"></a> creates a graph from only an RDD of edge tuples, that is, a collection of the <code class="literal">RDD[(VertexId, VertexId)]</code> type. It assigns the edges the attribute value 1 by default:</p><div class="informalexample"><pre class="programlisting">def fromEdgeTuples[VD](
      rawEdges: RDD[(VertexId, VertexId)],
      defaultValue: VD,
      uniqueEdges: Option[PartitionStrategy] = None)
: Graph[VD, Int]</pre></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec15"></a>Building graphs</h2></div></div><hr /></div><p>Let's now open our Spark shell <a id="id107" class="indexterm"></a>and build three types of graphs: a directed email communication network, a bipartite graph of ingredient-compound connections, and a multigraph using the previous graph builders.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note13"></a>Note</h3><p>Unless otherwise stated, we always assume that the Spark shell is launched from the <code class="literal">$SPARKHOME</code> directory. It then becomes the current directory for any relative file path used in this book.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec16"></a>Building directed graphs</h3></div></div></div><p>The first graph that we will <a id="id108" class="indexterm"></a>build is the Enron email communication network. If you have restarted your Spark shell, you need to again import the GraphX library. First, create a new folder called <code class="literal">data</code> inside <code class="literal">$SPARKHOME</code> and copy the dataset into it. This file contains the adjacency list of the email communications between the employees. Assuming that the current directory is <code class="literal">$SPARKHOME</code>, we can pass the file path to the <code class="literal">GraphLoader.edgeListFile</code> method:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.graphx._</strong></span>
<span class="strong"><strong>import org.apache.spark.graphx._</strong></span>

<span class="strong"><strong>scala&gt; import org.apache.spark.rdd._</strong></span>
<span class="strong"><strong>import org.apache.spark.rdd._</strong></span>

<span class="strong"><strong>scala&gt; val emailGraph = GraphLoader.edgeListFile(sc, "./data/emailEnron.txt")</strong></span>
<span class="strong"><strong>emailGraph: org.apache.spark.graphx.Graph[Int,Int] = org.apache.spark.graphx.impl.GraphImpl@609db0e</strong></span>
</pre></div><p>Notice that the <code class="literal">GraphLoader.edgeListFile</code> method always returns a graph object, whose vertex and edge attributes have a type <code class="literal">Int</code>. Their default values are 1. We can check this by looking at the first five vertices and edges in the graph:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; emailGraph.vertices.take(5)</strong></span>
<span class="strong"><strong>res: Array[(org.apache.spark.graphx.VertexId, Int)] = Array((19021,1), (28730,1), (23776,1), (31037,1), (34207,1))</strong></span>

<span class="strong"><strong>scala&gt; emailGraph.edges.take(5)</strong></span>
<span class="strong"><strong>res: Array[org.apache.spark.graphx.Edge[Int]] = Array(Edge(0,1,1), Edge(1,0,1), Edge(1,2,1), Edge(1,3,1), Edge(1,4,1))</strong></span>
</pre></div><p>The first node <code class="literal">(19021,1)</code> has the vertex ID <code class="literal">19021</code> and its attribute is indeed set to <code class="literal">1</code>. Similarly, the first edge <code class="literal">Edge(0,1,1)</code> captures the communication between the source 0 and destination 1.</p><p>In GraphX, all the <a id="id109" class="indexterm"></a>edges must be directed. To express non-directed or bidirectional graphs, we can link each connected pair in both directions. In our email network, we can verify for instance that the <code class="literal">19021</code> node has both incoming and outgoing links. First, we collect the destination nodes that node <code class="literal">19021</code> communicates to:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; emailGraph.edges.filter(_.srcId == 19021).map(_.dstId).collect()</strong></span>
<span class="strong"><strong>res: Array[org.apache.spark.graphx.VertexId] = Array(696, 4232, 6811, 8315, 26007)</strong></span>
</pre></div><p>It turns out that these same nodes are also the source nodes for the incoming edges to <code class="literal">19021</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; emailGraph.edges.filter(_.dstId == 19021).map(_.srcId).collect()</strong></span>
<span class="strong"><strong>res: Array[org.apache.spark.graphx.VertexId] = Array(696, 4232, 6811, 8315, 26007)</strong></span>
</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec17"></a>Building a bipartite graph</h3></div></div></div><p>In some applications, it is<a id="id110" class="indexterm"></a> useful to represent a view of a system as a bipartite graph. A bipartite graph is composed of two sets of nodes. The nodes within the same set cannot be connected but only the pairs belonging to the different sets can be. An example of such a graph is the food ingredient-compound network.</p><p>Here, we will work with the files <code class="literal">ingr_info.tsv</code>, <code class="literal">comp_info.tsv</code>, and <code class="literal">ingr_comp.tsv</code>, which should be copied into the <code class="literal">$SPARKHOME/data</code> folder. The first two files contain the information about the food ingredients and compounds respectively.</p><p>Let's have a quick look at the first lines of these two files using the <code class="literal">Source.fromFile</code> method of <code class="literal">scala.io.Source</code>. Our only requirement for this method is to simply inspect the beginning of the text files:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import scala.io.Source</strong></span>
<span class="strong"><strong>import scala.io.Source</strong></span>

<span class="strong"><strong>scala&gt; Source.fromFile("./data/ingr_info.tsv").getLines().</strong></span>
<span class="strong"><strong>      take(7).foreach(println)</strong></span>
<span class="strong"><strong># id  ingredient name  category</strong></span>
<span class="strong"><strong>0  magnolia_tripetala  flower</strong></span>
<span class="strong"><strong>1  calyptranthes_parriculata  plant</strong></span>
<span class="strong"><strong>2  chamaecyparis_pisifera_oil  plant derivative</strong></span>
<span class="strong"><strong>3  mackerel  fish/seafood</strong></span>
<span class="strong"><strong>4  mimusops_elengi_flower  flower</strong></span>
<span class="strong"><strong>5  hyssop  herb</strong></span>

<span class="strong"><strong>scala&gt; Source.fromFile("./data/comp_info.tsv").getLines().</strong></span>
<span class="strong"><strong>take(7).foreach(println)</strong></span>
<span class="strong"><strong># id  Compound name  CAS number</strong></span>
<span class="strong"><strong>0  jasmone  488-10-8</strong></span>
<span class="strong"><strong>1  5-methylhexanoic_acid  628-46-6</strong></span>
<span class="strong"><strong>2  l-glutamine  56-85-9</strong></span>
<span class="strong"><strong>3  1-methyl-3-methoxy-4-isopropylbenzene  1076-56-8</strong></span>
<span class="strong"><strong>4  methyl-3-phenylpropionate  103-25-3</strong></span>
<span class="strong"><strong>5  3-mercapto-2-methylpentan-1-ol_(racemic)  227456-27-1</strong></span>
</pre></div><p>The third file contains the adjacency list between the ingredients and the compounds:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; Source.fromFile("./data/ingr_comp.tsv").getLines().</strong></span>
<span class="strong"><strong>take(7).foreach(println)</strong></span>
<span class="strong"><strong># ingredient id  compound id</strong></span>
<span class="strong"><strong>1392  906</strong></span>
<span class="strong"><strong>1259  861</strong></span>
<span class="strong"><strong>1079  673</strong></span>
<span class="strong"><strong>22    906</strong></span>
<span class="strong"><strong>103    906</strong></span>
<span class="strong"><strong>1005  906 </strong></span>
</pre></div><p>In practice, the datasets from which we build the graphs will not come in a form that the graph builders in Spark expect them to be in. For example, in the food network example, we have two problems with the datasets. First, we cannot simply create a graph from the adjacency list<a id="id111" class="indexterm"></a> because the indices of the ingredients and compounds both start at zero and overlap with each other. Therefore, there is no way to distinguish the two nodes if they happen to have the same vertex ID. Second, we have two kinds of nodes--ingredients and compounds:</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip06"></a>Tip</h3><p>In order to create a bipartite graph, we first need to create the case classes named <code class="literal">Ingredient</code> and <code class="literal">Compound</code>, and use Scala's inheritance so that these two classes are the children of a <code class="literal">FNNode</code> class.</p></div><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; class FNNode(val name: String)</strong></span>
<span class="strong"><strong>defined class FNNode</strong></span>

<span class="strong"><strong>scala&gt; case class Ingredient(override val name: String, category: String) extends FNNode(name)</strong></span>
<span class="strong"><strong>defined class Ingredient</strong></span>

<span class="strong"><strong>scala&gt; case class Compound(override val name: String, cas: String) extends FNNode(name)</strong></span>
<span class="strong"><strong>defined class Compound</strong></span>
</pre></div><p>After this, we need to load all the <code class="literal">Compound</code> and <code class="literal">Ingredient</code> objects into an <code class="literal">RDD[FNNode]</code> collection. This part requires some data wrangling:</p><div class="informalexample"><pre class="programlisting">val ingredients: RDD[(VertexId, FNNode)] = 
sc.textFile("./data/ingr_info.tsv").
      filter(! _.startsWith("#")).
      map {line =&gt; 
             val row = line split '\t'
             (row(0).toInt, Ingredient(row(1), row(2)))
          }                           
ingredients: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, FNNode)] = MappedRDD[32] at map at &lt;console&gt;:26</pre></div><p>In the preceding code, we first loaded the text in <code class="literal">comp_info.tsv</code> into an RDD of <code class="literal">String</code>, and filtered out the comment lines starting with <code class="literal">#</code>. Then, we parsed the tab-delimited lines<a id="id112" class="indexterm"></a> into RDD of <code class="literal">Ingredient</code> vertices. Now, let's do a similar thing with <code class="literal">comp_info.tsv</code> and create an RDD of <code class="literal">Compound</code> vertices:</p><div class="informalexample"><pre class="programlisting">val compounds: RDD[(VertexId, FNNode)] = 
sc.textFile("./data/comp_info.tsv").
      filter(! _.startsWith("#")).
      map {line =&gt; 
             val row = line split '\t'
             (10000L + row(0).toInt, Compound(row(1), row(2)))
          }                              
compounds: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, FNNode)] = MappedRDD[28] at map at &lt;console&gt;:26     </pre></div><p>However, there is a critical thing that we did earlier. Since the index of each node should be unique, we had to shift the range of the compound indices by <code class="literal">10000L</code>, so that there is no index that refers to an ingredient and a compound at the same time.</p><p>Next, we create an <code class="literal">RDD[Edge[Int]]</code> collection from the dataset named <code class="literal">ingr_comp.tsv</code>:</p><div class="informalexample"><pre class="programlisting">val links: RDD[Edge[Int]] = 
  sc.textFile("./data/ingr_comp.tsv").
     filter(! _.startsWith("#")).
     map {line =&gt; 
        val row = line split '\t'
        Edge(row(0).toInt, 10000L + row(1).toInt, 1)
     }</pre></div><p>When parsing the lines of the adjacency list in <code class="literal">ingr_comp.tsv</code>, we also shift the indices of compounds by <code class="literal">10000L</code>. This quick fix works perfectly because we knew in advance, from the dataset description, how many ingredients and compounds there were in the dataset. Be more careful with real messy datasets! Next, as the links between ingredients and compounds do not contain any weight or meaningful attributes, we just parameterized the <code class="literal">Edge</code> class with the <code class="literal">Int</code> type, and set a default value of <code class="literal">1</code> for the attribute of each link.</p><p>Finally, we concatenate the two sets of nodes into a single RDD, and pass it to the <code class="literal">Graph()</code> factory method along with the RDD link:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val nodes = ingredients ++ compounds</strong></span>
<span class="strong"><strong>nodes: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, FNNode)] = UnionRDD[61] at $plus$plus at &lt;console&gt;:30</strong></span>

<span class="strong"><strong>scala&gt; val foodNetwork = Graph(nodes, links)</strong></span>
<span class="strong"><strong>foodNetwork: org.apache.spark.graphx.Graph[FNNode,Int] = org.apache.spark.graphx.impl.GraphImpl@635933c1</strong></span>
</pre></div><p>So, let's explore the ingredient-compound graph:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; def showTriplet(t: EdgeTriplet[FNNode,Int]): String = "The ingredient " ++ t.srcAttr.name ++ " contains " ++ t.dstAttr.name</strong></span>
<span class="strong"><strong>showTriplet: (t: EdgeTriplet[FNNode,Int])String</strong></span>

<span class="strong"><strong>scala&gt; foodNetwork.triplets.take(5).</strong></span>
<span class="strong"><strong>     foreach(showTriplet _ andThen println _)</strong></span>
<span class="strong"><strong>The ingredient calyptranthes_parriculata contains citral_(neral)</strong></span>
<span class="strong"><strong>The ingredient chamaecyparis_pisifera_oil contains undecanoic_acid</strong></span>
<span class="strong"><strong>The ingredient hyssop contains myrtenyl_acetate</strong></span>
<span class="strong"><strong>The ingredient hyssop contains 4-(2,6,6-trimethyl-cyclohexa-1,3-dienyl) but-2-en-4-one</strong></span>
<span class="strong"><strong>The ingredient buchu contains menthol</strong></span>
</pre></div><p>First, we defined <a id="id113" class="indexterm"></a>a helper function called <code class="literal">showTriplet</code> that returns a <code class="literal">String</code> description of an ingredient-compound triplet. Then, we took the first five triplets and printed them out on the console. In the preceding example, we used Scala's function composition in the <code class="literal">showTriplet _ andThen println _</code> argument and it passed to the <code class="literal">foreach</code> method.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec18"></a>Building a weighted social ego network</h3></div></div></div><p>As a final example, let's <a id="id114" class="indexterm"></a>build an ego network from the Google+ dataset that we presented earlier in this chapter. An ego network is a graph representation of one person's connections. Precisely, it focuses on a single node called the focal node and only represents the links between that node and its neighbors. Although the entire dataset from the SNAP website contains the ego networks of 133 Google+ users, we are only going to build one person's ego network as an illustration. The files that we are going to work with are placed in <code class="literal">$SPARKHOME/data</code>.</p><p>Their description is given as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><code class="literal">ego.edges</code>: These are directed edges in the ego network. The <code class="literal">ego</code> node does not appear in this list, but it is assumed that it follows every node ID that appears in the file.</p></li><li style="list-style-type: disc"><p><code class="literal">ego.feat</code> : This features for each of the nodes that appear in the edge file.</p></li><li style="list-style-type: disc"><p><code class="literal">ego.featnames</code>: This is the name of each of the feature dimensions. The feature is <code class="literal">1</code> if the user has this property in their profile, and <code class="literal">0</code> otherwise.</p></li></ul></div><p>First, let's import the absolute value function and the <code class="literal">SparseVector</code> class from the Breeze library, which we will be using:</p><div class="informalexample"><pre class="programlisting">import scala.math.abs
import breeze.linalg.SparseVector</pre></div><p>Then, let's also define<a id="id115" class="indexterm"></a> a type synonym called <code class="literal">Feature</code> for <code class="literal">SparseVector[Int]</code>:</p><div class="informalexample"><pre class="programlisting">type Feature = breeze.linalg.SparseVector[Int]</pre></div><p>Using the following code, we can read the features inside the <code class="literal">ego.feat</code> file and collect them in a map whose keys and values are of the <code class="literal">Long</code> and <code class="literal">Feature</code> types, respectively:</p><div class="informalexample"><pre class="programlisting">val featureMap: Map[Long, Feature] = 
  Source.fromFile("./data/ego.feat").
     getLines().
     map{line =&gt; 
     val row = line split ' '
     val key = abs(row.head.hashCode.toLong)
     val feat = SparseVector(row.tail.map(_.toInt))
     (key, feat)
     }.toMap</pre></div><p>Let's step back and take a quick look inside the <code class="literal">ego.feat</code> file to understand what the preceding chain of RDD transformations is doing, and why it is needed. Each line in <code class="literal">ego.feat</code> has the following form:</p><div class="mediaobject"><img src="graphics/B07392_02_01.jpg" /></div><p>The first number in each line corresponds to a node's ID in the ego network. The remaining string of <code class="literal">0</code> and <code class="literal">1</code> numbers indicate which feature this particular node has. For example, the first 1 after the node's ID corresponds to the <code class="literal">gender:1</code> feature. In fact, each feature is by the design of the <code class="literal">description:value</code> form. In practice, we usually have a limited control over the format of the datasets that we are working with. As in this example, there is always some data wrangling that we need to do. First, each vertex in the ego network should have a vertex ID of the <code class="literal">Long</code> type. However, the node IDs in the dataset, such as 114985346359714431656, exceed the permitted range for <code class="literal">Long</code>.</p><p>Therefore, we have to create new indices for the nodes. Second, we need to parse the string of 0 and 1 in the data to create a feature vector that has a more convenient form.</p><p>Luckily, these issues do have easy fixes. To convert the original node ID to a vertex ID, we simply hash the string that corresponds to the node ID, as follows:</p><div class="informalexample"><pre class="programlisting">val key = abs(row.head.hashCode.toLong)</pre></div><p>Then, we took advantage<a id="id116" class="indexterm"></a> of the <code class="literal">SparseVector</code> representation in the Breeze library to efficiently store the feature indices.</p><p>Next, we can read the <code class="literal">ego.edges</code> file to create an <code class="literal">RDD[Edge[Int]]</code> collection of the links in the ego network. In contrast to our previous graph examples, we model the ego network as a weighted graph. Precisely, the attribute of each link will correspond to the number of common features that each connected pair has. This is done by the following transformations:</p><div class="informalexample"><pre class="programlisting">val edges: RDD[Edge[Int]] = 
  sc.textFile("./data/ego.edges").
     map {line =&gt; 
        val row = line split ' '
        val srcId = abs(row(0).hashCode.toLong)
        val dstId = abs(row(1).hashCode.toLong)
        val srcFeat = featureMap(srcId)
        val dstFeat = featureMap(dstId)
        val numCommonFeats = srcFeat dot dstFeat
        Edge(srcId, dstId, numCommonFeats)
     }</pre></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip07"></a>Tip</h3><p>To find the number of common features between the source and destination nodes, we just used the dot product operation of the <code class="literal">SparseVector</code> class in Breeze. Again, we also had to compute new vertex IDs using the <code class="literal">hashCode</code> attribute of the node IDs in the dataset.</p></div><p>Finally, we can now <a id="id117" class="indexterm"></a>create an ego network using the <code class="literal">Graph.fromEdges</code> function. This function takes as arguments the <code class="literal">RDD[Edge[Int]]</code> collection and the default value for the vertices:</p><div class="informalexample"><pre class="programlisting">val egoNetwork: Graph[Int,Int] = Graph.fromEdges(edges, 1)</pre></div><p>Then, we can check how many of the nodes in the ego node's connections have some features in common with their adjacent nodes:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; egoNetwork.edges.filter(_.attr == 3).count()</strong></span>
<span class="strong"><strong>res: Long = 1852</strong></span>

<span class="strong"><strong>scala&gt; egoNetwork.edges.filter(_.attr == 2).count()</strong></span>
<span class="strong"><strong>res: Long = 9353</strong></span>

<span class="strong"><strong>scala&gt; egoNetwork.edges.filter(_.attr == 1).count()</strong></span>
<span class="strong"><strong>res: Long = 107934</strong></span>
</pre></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec16"></a>Computing the degrees of the network nodes</h2></div></div><hr /></div><p>We are now going to <a id="id118" class="indexterm"></a>explore the three graphs, and introduce an important property of a network node, which is the degree of the node.</p><p>The degree of a node represents the number of links it has to other nodes. In a directed graph, we can make a distinction between the incoming degree of a node or an in-degree, which is the number of its incoming links, and its outgoing degree or out-degree, which is the number of nodes that it points to. In the following sections, we will explore the degree distributions of the three example networks.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec19"></a>In-degree and out-degree of the Enron email network</h3></div></div></div><p>For the Enron <a id="id119" class="indexterm"></a>email network, we can confirm that <a id="id120" class="indexterm"></a>there are roughly ten times more links than nodes:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; emailGraph.numEdges</strong></span>
<span class="strong"><strong>res: Long = 367662</strong></span>

<span class="strong"><strong>scala&gt; emailGraph.numVertices</strong></span>
<span class="strong"><strong>res: Long = 36692</strong></span>
</pre></div><p>Indeed, the in-degree and out-degree of the employees are exactly the same in this example as the email graph is bi-directed. This can be confirmed by looking at the average degrees:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; emailGraph.inDegrees.map(_._2).sum / emailGraph.numVertices</strong></span>
<span class="strong"><strong>res: Double = 10.020222391802028</strong></span>

<span class="strong"><strong>scala&gt; emailGraph.outDegrees.map(_._2).sum / emailGraph.numVertices</strong></span>
<span class="strong"><strong>res: Double = 10.020222391802028</strong></span>
</pre></div><p>If we want to find the person that has e-mailed to the largest number of people, we can define and use the following <code class="literal">max</code> function:</p><div class="informalexample"><pre class="programlisting">def max(a: (VertexId, Int), b: (VertexId, Int)): (VertexId, Int) = {
  if (a._2 &gt; b._2) a else b
}</pre></div><p>Let's see the output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; emailGraph.outDegrees.reduce(max)</strong></span>
<span class="strong"><strong>res: (org.apache.spark.graphx.VertexId, Int) = (5038,1383)</strong></span>
</pre></div><p>This person could be an executive or an employee, acting as a hub to the organization. Similarly, we can define a <code class="literal">min</code> function to find people. Now, let's check if there are some isolated groups of employees at Enron using the following code:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; emailGraph.outDegrees.filter(_._2 &lt;= 1).count</strong></span>
<span class="strong"><strong>res83: Long = 11211</strong></span>
</pre></div><p>It seems that there are<a id="id121" class="indexterm"></a> many employees who receive<a id="id122" class="indexterm"></a> e-mails from only one employeeâ€”perhaps their bosses or from the human resources department.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec20"></a>Degrees in the bipartite food network</h3></div></div></div><p>For the bipartite <a id="id123" class="indexterm"></a>ingredient-compound graph, we can also explore which food has the largest number of compounds, or which compound is the most prevalent in our list of ingredients:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; foodNetwork.outDegrees.reduce(max)</strong></span>
<span class="strong"><strong>res: (org.apache.spark.graphx.VertexId, Int) = (908,239)</strong></span>

<span class="strong"><strong>scala&gt; foodNetwork.vertices.filter(_._1 == 908).collect()</strong></span>
<span class="strong"><strong>res: Array[(org.apache.spark.graphx.VertexId, FNNode)] = Array((908,Ingredient(black_tea,plant derivative)))</strong></span>

<span class="strong"><strong>scala&gt; foodNetwork.inDegrees.reduce(max)</strong></span>
<span class="strong"><strong>res: (org.apache.spark.graphx.VertexId, Int) = (10292,299)</strong></span>

<span class="strong"><strong>scala&gt; foodNetwork.vertices.filter(_._1 == 10292).collect()</strong></span>
<span class="strong"><strong>res: Array[(org.apache.spark.graphx.VertexId, FNNode)] = Array((10292,Compound(1-octanol,111-87-5)))</strong></span>
</pre></div><p>The answers to the earlier two questions turn out to be the black tea and the compound <code class="literal">1-octanol</code>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec21"></a>Degree histogram of the social ego networks</h3></div></div></div><p>Similarly, we can compute<a id="id124" class="indexterm"></a> the degrees of the connections in the ego network. Let's look at the maximum and minimum degrees in the network:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; egoNetwork.degrees.reduce(max)</strong></span>
<span class="strong"><strong>res91: (org.apache.spark.graphx.VertexId, Int) = (1643293729,1084)</strong></span>

<span class="strong"><strong>scala&gt; egoNetwork.degrees.reduce(min)</strong></span>
<span class="strong"><strong>res92: (org.apache.spark.graphx.VertexId, Int) = (550756674,1)</strong></span>
</pre></div><p>Suppose that we <a id="id125" class="indexterm"></a>now want to have the histogram data of the degrees. Then, we can write the following code to do just that:</p><div class="informalexample"><pre class="programlisting">egoNetwork.degrees.
  map(t =&gt; (t._2,t._1)).
  groupByKey.map(t =&gt; (t._1,t._2.size)).
  sortBy(_._1).collect()

res: Array[(Int, Int)] = Array((1,15), (2,19), (3,12), (4,17), (5,11), (6,19), (7,14), (8,9), (9,8), (10,10), (11,1), (12,9), (13,6), (14,7), (15,8), (16,6), (17,5), (18,5), (19,7), (20,6), (21,8), (22,5), (23,8), (24,1), (25,2), (26,5), (27,8), (28,4), (29,6), (30,7), (31,5), (32,10), (33,6), (34,10), (35,5), (36,9), (37,7), (38,8), (39,5), (40,4), (41,3), (42,1), (43,3), (44,5), (45,7), (46,6), (47,3), (48,6), (49,1), (50,9), (51,5),...</pre></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec17"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we have learned about the different ways to build graphs in Spark by working with concrete examples borrowed from online social networks, food science, and e-mail communications. We have seen that constructing a graph requires some data preparation and wrangling efforts. Nonetheless, GraphX offers various graph builder functions from which we can choose, depending on the graph representation that we need to create, and on the shape of the available datasets. Such usable functionalities are the advantages of GraphX against other similar graph-processing frameworks. Moreover, we looked at some basic statistics and properties of graphs, which are rather useful in characterizing their structure and in understanding their representation.</p><p>In the next chapter, we will go deeper into the analysis of graphs, using data visualization tools and new graph-theoretical concepts and algorithms, such as connectedness, triangle counting, and PageRank.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch03"></a>ChapterÂ 3.Â Graph Analysis and Visualization</h2></div></div></div><p>In this chapter, we will learn how to analyze the characteristics of graphs using visualization tools and graph algorithms. For example, we will use some of the algorithms available in GraphX to see how connected a graph is. In addition, we will compute metrics that are commonly used, such as triangle counting and clustering coefficients. Furthermore, we will learn through a concrete example how the PageRank algorithm can be used to rank the importance of the nodes in a network. Along the way, we will introduce new RDD operations that will prove out to be useful here and in later chapters. Finally, this chapter offers practical tips on building Spark applications that rely on the third-party libraries. After doing the activities in this chapter, you will have learned the tools and concepts to:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Visualize large-scale graph data</p></li><li style="list-style-type: disc"><p>Compute the connected components of a network</p></li><li style="list-style-type: disc"><p>Use the PageRank algorithm to rank the node importance in networks</p></li><li style="list-style-type: disc"><p>Build Spark applications that use third-party libraries using SBT</p></li></ul></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec18"></a>Network datasets</h2></div></div><hr /></div><p>We will be using the same <a id="id126" class="indexterm"></a>datasets introduced in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Building and Exploring Graphs</em></span>, including the social ego network, email graph, and food-compound network.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec19"></a>The graph visualization</h2></div></div><hr /></div><p>Spark and GraphX do not <a id="id127" class="indexterm"></a>provide any built-in functionality for data visualization, since their focus is on data processing. However, pictures are worth than thousands of numbers when it comes to data analysis. In the following sections, we will build a Spark application for visualizing and analyzing the connectedness of graphs. We will rely on the third-party library called <span class="strong"><strong>GraphStream</strong></span><a id="id128" class="indexterm"></a> for drawing networks, and <span class="strong"><strong>BreezeViz</strong></span><a id="id129" class="indexterm"></a> for plotting structural properties of graphs, such as degree distribution. These <a id="id130" class="indexterm"></a>libraries are not perfect and have limitations but they are relatively stable and simple to use. So, we will use them for exploring the graph examples that are used in this chapter.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note14"></a>Note</h3><p>Currently, there is still a lack of graph visualization engines and libraries for drawing large-scale networks, without requiring a huge amount of computing resources. For example, the popular network analysis software SNAP currently relies on the GraphViz engine to draw networks, but it can only draw small- to medium-sized networks. Gephi<a id="id131" class="indexterm"></a> is another tool for doing interactive network visualization. Although it has nice features, such as a multilevel layout and a built-in 3D rendering engine, Gephi still requires a high CPU and memory requirements. For drawing standards plots, the new project Apache Zeppelin<a id="id132" class="indexterm"></a> offers a web-based notebook for interactive data analysis and visualization. It also provides a built-in integration with Spark. Visit the official website for more information.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec22"></a>Installing the GraphStream and BreezeViz libraries</h3></div></div></div><p>Let's get going by installing the <a id="id133" class="indexterm"></a>third-party libraries and their dependencies in the <code class="literal">$SPARKHOME /lib</code> folder. GraphStream is an awesome Java library that enables the visualization of dynamic networks, which can evolve with time. For our purpose, we only need to <a id="id134" class="indexterm"></a>display static networks so that we only need to download two JAR files called <code class="literal">gs-core-1.2.jar</code> and <code class="literal">gs-ui-1.2.jar</code> for the core and UI libraries. They can<a id="id135" class="indexterm"></a> be downloaded<a id="id136" class="indexterm"></a> from the following repositories:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><a class="ulink" href="https://oss.sonatype.org/content/repositories/releases/org/graphstream/gs-core/1.2/" target="_blank">https://oss.sonatype.org/content/repositories/releases/org/graphstream/gs-core/1.2/</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="https://oss.sonatype.org/content/repositories/releases/org/graphstream/gs-ui/1.2/" target="_blank">https://oss.sonatype.org/content/repositories/releases/org/graphstream/gs-ui/1.2/</a></p></li></ul></div><p>Put these two JAR files in the <code class="literal">lib</code> folder, within the project root directory. Next, download the <code class="literal">breeze_2.10-0.9.jar</code> and <code class="literal">breeze-viz_2.10-0.9.jar</code> libraries from the following repositories:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><a class="ulink" href="http://repo.spring.io/libs-release-remote/org/scalanlp/breeze_2.10/0.9/" target="_blank">http://repo.spring.io/libs-release-remote/org/scalanlp/breeze_2.10/0.9/</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://repo1.maven.org/maven2/org/scalanlp/breeze-viz_2.10/0.9/" target="_blank">http://repo1.maven.org/maven2/org/scalanlp/breeze-viz_2.10/0.9/</a></p></li></ul></div><p>Since BreezeViz is a Scala library that depends on another Java library called <span class="strong"><strong>JfreeChart</strong></span>, you<a id="id137" class="indexterm"></a> will also need to install <code class="literal">jcommon-1.0.16.jar</code> and <code class="literal">jfreechart-1.0.13.jar</code>. These JAR files can be found in the following repositories:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><a class="ulink" href="https://repository.jboss.org/nexus/content/repositories/thirdparty-releases/jfree/jcommon/1.0.16/" target="_blank">https://repository.jboss.org/nexus/content/repositories/thirdparty-releases/jfree/jcommon/1.0.16/</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://repo1.maven.org/maven2/jfree/jfreechart/1.0.13/" target="_blank">http://repo1.maven.org/maven2/jfree/jfreechart/1.0.13/</a></p></li></ul></div><p>After you have downloaded all these four JAR files, copy them into the <code class="literal">lib</code> folder within the project root directory. You are now ready to draw your first graph from the Spark shell.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec23"></a>Visualizing the graph data</h3></div></div></div><p>Open the terminal, with the <a id="id138" class="indexterm"></a>current directory set to <code class="literal">$SPARKHOME</code>. Launch the Spark shell. This time you will need to specify the third-party JAR files with the <code class="literal">--jars</code> option:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ./bin/spark-shell --jars \</strong></span>
<span class="strong"><strong>lib/breeze-viz_2.10-0.9.jar,\</strong></span>
<span class="strong"><strong>lib/breeze_2.10-0.9.jar,\</strong></span>
<span class="strong"><strong>lib/gs-core-1.2.jar,\</strong></span>
<span class="strong"><strong>lib/gs-ui-1.2.jar,\</strong></span>
<span class="strong"><strong>lib/jcommon-1.0.16.jar,\</strong></span>
<span class="strong"><strong>lib/jfreechart-1.0.13.jar</strong></span>
</pre></div><p>Alternatively, you can save yourself some typing with the shorter command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$./bin/spark-shell  --jars \</strong></span>
<span class="strong"><strong>$(find "." -name '*.jar' | xargs echo | tr ' ' ',')</strong></span>
</pre></div><p>Instead of specifying each JAR one at a time, the preceding command loads all the JARs.</p><p>As a first example, we will visualize the social ego network that we have seen in the previous chapter. First, we need to import the <code class="literal">GraphStream</code> classes with the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.graphstream.graph.{Graph =&gt; GraphStream}</strong></span>
<span class="strong"><strong>import org.graphstream.graph.{Graph=&gt;GraphStream}</strong></span>
<span class="strong"><strong>scala&gt; import org.graphstream.graph.implementations._ </strong></span>
<span class="strong"><strong>import org.graphstream.graph.implementations._</strong></span>
</pre></div><p>It is important that we rename <code class="literal">org.graphstream.graph.Graph</code> to <code class="literal">GraphStream</code>, to avoid a namespace collision with the <code class="literal">Graph</code> class of GraphX.</p><p>Next, load the social ego network data using <code class="literal">Graph.fromEdges</code>, as we did in the previous chapter. After that, we will create a <code class="literal">SingleGraph</code> object:</p><div class="informalexample"><pre class="programlisting">// Create a SingleGraph class for GraphStream visualization 
val graph: SingleGraph = new SingleGraph("EgoSocial") </pre></div><p>The <code class="literal">SingleGraph</code> object is<a id="id139" class="indexterm"></a> a <code class="literal">GraphStream</code> abstraction that enables the manipulation and visualization of graph data. Concretely, we can invoke the <code class="literal">addNode</code> and <code class="literal">addEdge</code> methods of the <code class="literal">SingleGraph</code> object to add the network nodes and links. We can also invoke the <code class="literal">addAttribute</code> method on either the graph, or each individual edge and node to set their visual attributes. What's cool about the <code class="literal">GraphStream</code> API is that it cleanly separates the graph structure and visualization using a CSS-like style sheet to control the way the graph elements are displayed. It is much easier to see this in action. So, let's create a file named <code class="literal">stylesheet</code> and put it in a new <code class="literal">./style/</code> folder. Insert the following lines in the style sheet:</p><div class="informalexample"><pre class="programlisting">node { 
    fill-color: #a1d99b; 
    size: 20px; 
    text-size: 12; 
    text-alignment: at-right; 
    text-padding: 2; 
    text-background-color: #fff7bc; 
} 
edge { 
    shape: cubic-curve; 
    fill-color: #dd1c77; 
    z-index: 0; 
    text-background-mode: rounded-box; 
    text-background-color: #fff7bc; 
    text-alignment: above; 
    text-padding: 2; 
}</pre></div><p>The preceding style sheet describes the visual styles of the graph elements using selectors nodes and edges, and specifying their visual attributes using key-value pairs. In this example, we set the colors and shapes of the nodes, edges, and their text attributes. An exhaustive reference for the style sheet attributes used in <code class="literal">GraphStream</code> is available<a id="id140" class="indexterm"></a> at <a class="ulink" href="http://graphstream-project.org/doc/Tutorials/Graph-Visualisation_1.1/" target="_blank">http://graphstream-project.org/doc/Tutorials/Graph-Visualisation_1.1/</a>.</p><p>With the style sheet now ready, we can connect it to the <code class="literal">SingleGraph</code> object graph:</p><div class="informalexample"><pre class="programlisting">// Set up the visual attributes for graph visualization 
graph.addAttribute("ui.stylesheet","url(file:.//style/stylesheet)") 
graph.addAttribute("ui.quality") 
graph.addAttribute("ui.antialias") </pre></div><p>In the last two lines, we <a id="id141" class="indexterm"></a>simply informed the rendering engine to favor quality instead of speed. Next, we have to reload the graph that we built in the previous chapter. To avoid repetitions, we omit the graph building part. After this, we now load <code class="literal">VertexRDD</code> and <code class="literal">EdgeRDD</code> of the social network into the <code class="literal">GraphStream</code> graph object, with the following code:</p><div class="informalexample"><pre class="programlisting">// Given the egoNetwork, load the graphX vertices into GraphStream
for ((id,_) &lt;- egoNetwork.vertices.collect()) { 
val node = graph.addNode(id.toString).asInstanceOf[SingleNode] 
} 
// Load the graphX edges into GraphStream edges 
for (Edge(x,y,_) &lt;- egoNetwork.edges.collect()) { 
val edge = graph.addEdge(x.toString ++ y.toString, x.toString, y.toString, 
true).
     asInstanceOf[AbstractEdge] 
}</pre></div><p>To add a node, we simply pass its vertex ID as a string argument. For the edges, we need to pass four arguments to the <code class="literal">addEdge</code> method. The first one is a string identifier for each edge. Since this identifier is not available in the original dataset or in the GraphX graph, we had to create one. Well, here the simplest solution was to concatenate the vertex IDs of the nodes that each edge links to.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip08"></a>Tip</h3><p>In the preceding code, we had to use a subtle trick to avoid an interoperability issue between our Scala code and the <code class="literal">GraphStream</code> Java library. As described in the <code class="literal">org.graphstream.graph.implementations.AbstractGraph</code> API of <code class="literal">GraphStream</code>, the <code class="literal">addNode</code> and <code class="literal">addEdge</code> methods return the node and edge respectively. However, as <code class="literal">GraphStream</code> is a third-party Java library, we had to force the return types of <code class="literal">addNode</code> and <code class="literal">addEdge</code> using the <code class="literal">asInstanceOf[T]</code> method with the <code class="literal">T</code> type being <code class="literal">SingleNode</code> and <code class="literal">AbstractEdge</code>, respectively. So what would have happened if we omitted these explicit type conversions? You would get a rather strange exception saying:</p><div class="informalexample"><pre class="programlisting">java.lang.ClassCastException: org.graphstream.graph.implementations.SingleNode cannot be cast to scala.runtime.Nothing$</pre></div></div><p>Now what? The only<a id="id142" class="indexterm"></a> thing to do here is to make the social ego network display it. Just call the display method on graph:</p><div class="informalexample"><pre class="programlisting">graph.display()</pre></div><p>Voila! You now will see the network drawn in a new window, as shown in the following:</p><div class="mediaobject"><img src="graphics/B07392_03_01.jpg" /></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip09"></a>Tip</h3><p>If your graph is not displayed with the colors above, you should check that the stylesheet's file path is correct when setting the graph's attribute called <code class="literal">ui.stylesheet</code>.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec24"></a>Plotting the degree distribution</h3></div></div></div><p>As shown by this<a id="id143" class="indexterm"></a> visualization, each person in the ego network seems to be either isolated or connected to a large group of mutual friends. We can further analyze this fact by plotting the degree distribution of the network. To do this with the help of the Spark shell is as easy as before. Make sure that you first import some classes from JFreeChart and Breeze:</p><div class="informalexample"><pre class="programlisting">import org.jfree.chart.axis.ValueAxis 
import breeze.linalg._ 
import breeze.plot._ </pre></div><p>We will then employ the <code class="literal">degreeHistogram</code> function that we built in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Building and Exploring Graphs</em></span>. For convenience, its definition is shown as follows:</p><div class="informalexample"><pre class="programlisting">def degreeHistogram(net: Graph[Int, Int]): Array[(Int, Int)] =  
    net.degrees.map(t =&gt; (t._2,t._1)). 
          groupByKey.map(t =&gt; (t._1,t._2.size)). 
          sortBy(_._1).collect()</pre></div><p>From the degree histogram, we can obtain the degree distribution, which is the probability distribution of the node degrees over the whole network. For this, we just normalize the node degrees by the total number of nodes, so that the degree probabilities add up to one:</p><div class="informalexample"><pre class="programlisting">val nn = egoNetwork.numVertices 
val egoDegreeDistribution = degreeHistogram(egoNetwork).map({case (d,n) =&gt; (d,n.toDouble/nn)})</pre></div><p>To display the degree distribution, we first create a <code class="literal">Figure</code> object called <code class="literal">f</code> and two plot objects called <code class="literal">p1</code> and <code class="literal">p2</code>. In the following code, <code class="literal">p1 = f.subplot(2,1,0)</code> and <code class="literal">p2 = f.subplot(2,1,1)</code> specify that <code class="literal">f</code> will have two subplots, and that <code class="literal">p1</code> is displayed above <code class="literal">p2</code>. Indeed, the first two arguments of the subplot are the number of rows and columns of the figure, whereas the third argument denotes the subplot index, which starts at 0:</p><div class="informalexample"><pre class="programlisting">val f = Figure()
val p1 = f.subplot(2,1,0) 
val x = new DenseVector(egoDegreeDistribution map (_._1.toDouble)) 
val y = new DenseVector(egoDegreeDistribution map (_._2))
p1.xlabel = "Degrees" 
p1.ylabel = "Distribution" 
p1 += plot(x, y) 
p1.title = "Degree distribution of social ego network"
val p2 = f.subplot(2,1,1) 
val egoDegrees = egoNetwork.degrees.map(_._2).collect()

p1.xlabel = "Degrees" 
p1.ylabel = "Histogram of node degrees" 
p2 += hist(egoDegrees, 10)</pre></div><p>This code will then<a id="id144" class="indexterm"></a> display the degree distribution and degree frequencies of the ego network:</p><div class="mediaobject"><img src="graphics/B07392_03_02.jpg" /></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec20"></a>The analysis of network connectedness</h2></div></div><hr /></div><p>Next, we are going to visually<a id="id145" class="indexterm"></a> explore and analyze the connectedness of the <a id="id146" class="indexterm"></a>food network. Reload the ingredient and compound datasets using the steps explained in the previous chapter. After you are done, create a <code class="literal">GraphStream</code> graph object:</p><div class="informalexample"><pre class="programlisting">// Create a SingleGraph class for GraphStream visualization 
val graph: SingleGraph = new SingleGraph("FoodNetwork")
Then, set the <code class="literal">ui.stylesheet</code> attribute of the graph. Since the food network is a bipartite graph, it would be nice to display the nodes with two different colors. We do that using a new style sheet. While we are at it, let's also reduce the node size and hide the text attributes:
node {
    size: 5px;
    text-mode: hidden;
    z-index: 1;
    fill-mode: dyn-plain;
    fill-color: "#e7298a", "#43a2ca";
}
edge {
    shape: line;
    fill-color: #fee6ce;
    arrow-size: 2px, 1px;
    z-index: 0;
}</pre></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip10"></a>Tip</h3><p>The color value in the style sheet is set in hexadecimal using <code class="literal">#</code>. You can choose your favorite colors from the awesome<a id="id147" class="indexterm"></a> <span class="strong"><strong>ColorBrewer</strong></span> palettes available at <a class="ulink" href="http://colorbrewer2.org/" target="_blank">http://colorbrewer2.org/</a>.</p></div><p>Let's now load the<a id="id148" class="indexterm"></a> nodes and edges from <code class="literal">foodNetwork</code> to the<a id="id149" class="indexterm"></a> <code class="literal">GraphStream</code> graph again, using the <code class="literal">addNode</code> and <code class="literal">addEdge</code> methods. This time, we are going to dynamically set the color of the nodes, depending on whether it is an ingredient or a compound:</p><div class="informalexample"><pre class="programlisting">// Load the graphX vertices into GraphStream nodes
for ((id:VertexId, fnn:FNNode) &lt;- foodNetwork.vertices.collect()) 
{
val node = graph.addNode(id.toString).asInstanceOf[SingleNode]
node.addAttribute("name", fnn.name)
node.addAttribute("ui.label", fnn.name)
if (fnn.isInstanceOf[Compound])
  node.addAttribute("ui.color",1: java.lang.Double)
else if(fnn.isInstanceOf[Compound])
  node.addAttribute("ui.color",0: java.lang.Double)
}</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note15"></a>Note</h3><p>You may ask yourself why we used <code class="literal">isInstanceOf[T]</code> to determine the type of the nodes when loading the nodes with <code class="literal">addNode</code>. Why did we not use Scala's awesome pattern matching feature? We could have used it in a standalone Spark program, but it is not currently possible to pattern match on case classes in the Spark shell. So, that is why we used <code class="literal">isInstanceOf[T]</code>.</p></div><p>Loading the nodes of <a id="id150" class="indexterm"></a>the food network is almost the same as for the social ego<a id="id151" class="indexterm"></a> network. The only difference is setting different colors for the nodes. In a similar fashion, load the edges into the <code class="literal">GraphStream</code> graph object:</p><div class="informalexample"><pre class="programlisting">// Load the graphX edges into GraphStream edges 
for (Edge(x,y,_) &lt;- foodNetwork.edges.collect()) { 
  val edge = graph.addEdge(x.toString ++ y.toString,\
     x.toString, y.toString, 
     true).
asInstanceOf[AbstractEdge] 
}</pre></div><p>To visualize the food network, call <code class="literal">graph.display()</code>. You will get something like this:</p><div class="mediaobject"><img src="graphics/B07392_03_03.jpg" /></div><p>From this picture, we can see that many ingredients share the same compounds, whereas some compounds can only be found in some ingredients. Similar to the social ego network, this network<a id="id152" class="indexterm"></a> consists of some isolated nodes, and a giant component of  connected nodes. This leads to our next topic, which is the measure of the connectedness of graphs.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec25"></a>Finding the connected components</h3></div></div></div><p>In a network, two nodes are <a id="id153" class="indexterm"></a>connected if there is a path between them on the graph. A network is called <span class="strong"><strong>connected</strong></span> if all the node pairs are connected. Otherwise, a <span class="strong"><strong>disconnected</strong></span> network has many components, each of which is connected. To find the connected components of a graph is easy in GraphX using the <code class="literal">connectedComponents</code> method.</p><p>Using the food network as an example, we can verify that it has exactly 27 components:</p><div class="informalexample"><pre class="programlisting">// Connected Components  
scala&gt; val cc = foodNetwork.connectedComponents() 
cc: org.apache.spark.graphx.Graph[VertexId,Int] 
 
// Number of components  
scala&gt; cc.vertices.map(_._2).collect.distinct.size 
res: Int = 27</pre></div><p>Given the type of <code class="literal">cc</code> above, we see it returns another graph with the same number of vertices. The vertices belonging to the same component have the same attribute whose value is the smallest vertex ID in that component. In other words, the attribute of each node identifies its component. Let's see these component identifiers:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; cc.vertices.map(_._2).distinct.collect</strong></span>
<span class="strong"><strong>res6: Array[org.apache.spark.graphx.VertexId] = Array(892, 0, 1344, 528, 468, 392, 960, 409, 557, 529, 585, 1105, 233, 181, 481, 1146, 970, 622, 1186, 514, 1150, 511, 47, 711, 1211, 1511, 363)</strong></span>
</pre></div><p>Now, suppose we want to list the components and its number of nodes in the descending order. To do this, we can employ Spark's <code class="literal">PairedRDD</code> operations which are <code class="literal">groupBy</code> and <code class="literal">sortBy</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; cc.vertices.groupBy(_._2).</strong></span>
<span class="strong"><strong>   map((p =&gt; (p._1,p._2.size))).</strong></span>
<span class="strong"><strong>   sortBy(x =&gt; x._2, false).collect()</strong></span>
<span class="strong"><strong>res: Array[(VertexId, Int)] = Array((0,2580), (528,8), (1344,3), (392,3), (585,3), (481,3), (892,2), (960,2), (409,2), (557,2), (529,2), (1105,2), (181,2), (970,2), (622,2), (1186,2), (1150,2), (511,2), (47,2), (711,2), (1511,2), (363,2), (468,1), (233,1), (1146,1), (514,1), (1211,1))</strong></span>
</pre></div><p>The giant component <a id="id154" class="indexterm"></a>has 2580 ingredient and compound nodes, among which the node with the smallest vertex ID is 0. In general, we can define a function that takes the graph of connected components, and returns the smallest vertex ID and number of nodes in the largest component, as follows:</p><div class="informalexample"><pre class="programlisting">def largestComponent(cc: Graph[VertexId, Int]): (VertexId, Int) = 
cc.vertices.map(x =&gt; (x._2,x._1)).
      groupBy(_._1).
      map(p =&gt; (p._1,p._2.size)).
      max()(Ordering.by(_._2))</pre></div><p>In this function, we grouped the vertices of the components graph by the component ID. Then, we mapped each component to a key-value pair where the key is the component ID, and the value is the number of nodes of the component. Finally, we use the reduction operator called <code class="literal">max</code> to return the key-value pair, corresponding to the largest component. In the preceding example, we had to pass to the <code class="literal">max</code> method two lists of arguments. The first one is always empty, whereas the second one is an implicit and takes an ordering. To sort pairs on the second element, we had to pass the right ordering to <code class="literal">max</code> as <code class="literal">Ordering.by(_._2)</code>:</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note16"></a>Note</h3><p>In addition to GraphX's graph-specific operations, Spark's RDD and pair RDD operations can be very useful to certain tasks. This function is a canonical example of a chain of data processing in Spark, which is entirely done with Spark's RDD and pair RDD operations. For more details, see the API documentation for Spark<a id="id155" class="indexterm"></a> and GraphX at <a class="ulink" href="http://spark.apache.org/docs/1.1.0/api/scala/index.html#org.apache.spark.package" target="_blank">http://spark.apache.org/docs/1.1.0/api/scala/index.html#org.apache.spark.package</a>.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec26"></a>Counting triangles and computing clustering coefficients</h3></div></div></div><p>In the following, we will<a id="id156" class="indexterm"></a> use the Enron email dataset to<a id="id157" class="indexterm"></a> illustrate the analysis of a graph connectedness with counting triangle and the clustering coefficients. A triangle is a connected subgraph of three nodes. Counting how many triangles pass through each node helps to quantify the connectedness of graphs. In particular, counting triangle is required to compute the clustering coefficients, which measure the local density of the neighborhood of each node in the network.</p><p>Currently, there is a restriction imposed by the triangle counting implementation in Spark on the input graph. Specifically, the edges of the input graph should be in a canonical direction; that is, the <code class="literal">sourceId</code> parameter must be less than the <code class="literal">destId</code> parameter. For the email graph, this implies that there should be at most one directed link between the two people. This restriction is not that severe since we can still assume that each directed link in the email graph <a id="id158" class="indexterm"></a>implies a bidirectional <a id="id159" class="indexterm"></a>communication between the two people. We can impose this constraint by filtering out the edges for which the ID of the source is larger than that of the destination node. In addition to this restriction, the input graph must also have been partitioned with <code class="literal">partitionBy</code>. Thus, we can load the email graph as:</p><div class="informalexample"><pre class="programlisting">val emailGraph = 
GraphLoader.edgeListFile(sc,"./data/emailEnron.txt").
subgraph(epred = t =&gt; t.srcId &lt; t.dstId).
partitionBy(PartitionStrategy.RandomVertexCut)</pre></div><p>Once the Enron email graph is loaded, we can compute the triangle counts:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; emailGraph.triangleCount()</strong></span>
<span class="strong"><strong>res: Graph[Int,Int] </strong></span>
<span class="strong"><strong>scala&gt; val triangleCounts = emailGraph.triangleCount().vertices</strong></span>
<span class="strong"><strong>triangleCounts:VertexRDD[Int] </strong></span>
</pre></div><p>Similar to <code class="literal">connectedComponent</code>, the <code class="literal">triangleCount</code> algorithm also returns a new graph with the same number of nodes. However, the triangle count becomes the vertex attribute.</p><p>How easy was that? Now, let's calculate the local clustering coefficients of the email network. First, we define a function that calculates the local clustering coefficient of a specific node. The clustering coefficient, at a given node, captures the network's local density at that node. The more densely interconnected its neighborhood is, the closer to 1 is its local clustering coefficient. It can be calculated by the following function:</p><div class="informalexample"><pre class="programlisting">def clusterCoeff(tup: (VertexId, (Int,Int))): (VertexId, Double) = 
tup match {case (vid, (t, d)) =&gt;
(vid, (2*t.toDouble/(d*(d-1))))
}</pre></div><p>The argument of <code class="literal">clusterCoeff</code> is a tuple whose elements consist of the vertex ID of the node at which we compute the local cluster coefficient, and of another tuple, containing the triangle count and degree of the node. Then, it returns the cluster coefficient with the vertex ID as a tuple. Actually, the local cluster coefficient of a given node is an estimate of the probability that each pair of its neighbors is connected. Therefore, the coefficient can be calculated as the ratio between the total links between the node's neighbors, which is also equal to the number of triangles that pass through the node, and the number of all possible pairs of neighboring nodes.</p><p>With this, we can compute the cluster coefficients for all the nodes:</p><div class="informalexample"><pre class="programlisting">def clusterCoefficients(graph: Graph[Int,Int]): 
RDD[(VertexId, Double)] = {
val gRDD: RDD[(VertexId, (Int, Int))] = 
graph.triangleCount().vertices join graph.degrees 
gRDD map clusterCoeff
}</pre></div><p>This last function <a id="id160" class="indexterm"></a>takes a graph as an input, and <a id="id161" class="indexterm"></a>returns a pair of RDD collections, whose elements contain the vertex identifiers and the corresponding local coefficients.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note17"></a>Note</h3><p>The formula for the local clustering coefficient at a given node is well-defined only when its degree, that is the number of its neighbors, is larger than one. If the node has a degree of one or zero, the <code class="literal">clusterCoeff</code> function will return a <code class="literal">NaN</code> value for the clustering coefficient. Therefore, we must first check if some nodes are isolated in the network when we want to compute an average or global clustering coefficient for a network. Not only must we filter out the leaves and isolated nodes but also, we must adjust the formula of the global clustering coefficient to avoid a biased assessment of the neighborhood clustering.</p></div><p>Let's now use the previous functions to compute the cluster coefficients for the email graph:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val coeffs = clusterCoefficients(emailGraph)</strong></span>
<span class="strong"><strong>scala&gt; coeffs.take(10) </strong></span>
<span class="strong"><strong>res: Array[(VertexId, Double)] = Array((19021,0.9), (28730,1.0), (23776,1.0), (31037,0.0), (34207,NaN), (29127,1.0), (9831,NaN), (5354,0.0380952380952381), (32676,0.46153846153846156), (4926,1.0))</strong></span>
</pre></div><p>We see that for some nodes, the returned clustering coefficient has a <code class="literal">NaN</code> value. In fact, this is the case for 25481 out of the 36692 nodes:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>// Check the NaN values. </strong></span>
<span class="strong"><strong>scala&gt; coeffs.filter (x =&gt; !x._2.isNaN).count </strong></span>
<span class="strong"><strong>res: Long = 25481</strong></span>
</pre></div><p>To remedy this <a id="id162" class="indexterm"></a>fact, we need to filter out these <a id="id163" class="indexterm"></a>nodes when averaging the cluster coefficients:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>// Calculate the adjusted global cluster coefficient </strong></span>
<span class="strong"><strong>scala&gt; val nonIsolatedNodes = coeffs.filter(x =&gt; !x._2.isNaN) nonIsolatedNodes: RDD[(VertexId, Double)] </strong></span>
<span class="strong"><strong>scala&gt; val globalCoeff = </strong></span>
<span class="strong"><strong>   nonIsolatedNodes.map(_._2).sum / nonIsolatedNodes.count globalCoeff: Double = 0.7156424032347621</strong></span>
</pre></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec21"></a>The network centrality and PageRank</h2></div></div><hr /></div><p>Previously, we have <a id="id164" class="indexterm"></a>used the degree distribution and clustering coefficients of a network to understand how connected a network is. In particular, we have learned how to find the <a id="id165" class="indexterm"></a>largest connected components and the nodes that have the highest degree. Then, we visualized the networks and saw the nodes that have higher chances to play the role of hubs in the network since many nodes are connected to them. In some sense, the degree of a node can be interpreted as a centrality measure that determines how important that node is relative to the rest of the network. In this section, we are going to introduce a different centrality measure as well as the PageRank algorithm, which is useful for ranking nodes in graphs.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note18"></a>Note</h3><p>There exist many other measures of centrality for graphs. For example, betweenness centrality is useful for information flow problems. Given a node, its betweenness is the number of shortest paths from all vertices to all others that pass through this node. In contrast to PageRank, betweenness centrality assigns a high score to the nodes that are strategically connected on the shortest paths, connecting the pairs of other nodes. Other measures are the connected centrality and Katz centrality. There are no predefined algorithms in GraphX to compute these measures. One of the reasons is the greater complexity required for exactly computing the betweenness centrality. Therefore, approximate algorithms still need to be developed and will be an excellent open source contribution for extending the current GraphX library.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec27"></a>How PageRank works</h3></div></div></div><p>PageRank<a id="id166" class="indexterm"></a> is the famous algorithm behind Google's incredibly successful web search engine. In response to each search query, Google wants to display important web pages first. In brief, PageRank assigns a probability score to each page. The higher the score for a node, the more likely a user will land on that page in the long term.</p><p>To find the final PageRank scores, the algorithm simulates the behavior of a random surfer by walking her through the web graph. At each step, the surfer can either visit a page that it links to or jump to another random page (this is not necessarily a neighboring page). This is done according to the transition probabilities that are specified by the structure of the graph. For example, a web graph with one thousand nodes will be associated to a 1000 by 1000 transition probability matrix. The element in row <code class="literal">i</code> and column <code class="literal">j</code> of that matrix has a value of <code class="literal">1/k</code> where the <code class="literal">j</code> page has <code class="literal">k</code> outgoing links, and one of them is to the <code class="literal">i</code> page. Otherwise, it is zero. The PageRank algorithm starts at a random node and, at each step, the PageRank scores are updated.</p><p>A sketch implementation of this algorithm is shown as follows:</p><div class="informalexample"><pre class="programlisting">var PR = Array.fill(n)(1.0)
val oldPR = Array.fill(n)(1.0)
while( iter &lt;= maxIter || max(abs(PR - oldPr)) &gt; tol) {
  swap(oldPR, PR)
  for(i &lt;- 0 until n) {
    PR[i] = d + (1 - d) * inNbrs[i].map(j =&gt; oldPR[j] / outDeg[j]).sum
  }
}</pre></div><p>In the <a id="id167" class="indexterm"></a>preceding code, alpha is the random reset probability with a default value of <code class="literal">0.15</code>. Next, <code class="literal">inNbrs[i]</code> is the set of neighbors, which link to <code class="literal">i</code>, and <code class="literal">outDeg[j]</code> is the out-degree of the <code class="literal">j</code> vertex.</p><p>The first term in the update is due to the fact that the surfer can choose to skip the neighbor and instead jump to a random page with a probability as <code class="literal">d</code>. The second term updates the important score of each page, based on the previous scores of the neighbors that link to the page. This process is repeated until the PageRank scores converge to a fixed value, or until a maximum number of iterations are reached.</p><p>In GraphX, there are two implementations of the PageRank algorithm. The first implementation uses the <code class="literal">Pregel</code> interface and runs PageRank for a fixed number of iterations <code class="literal">numIter</code>. The second one uses the standalone <code class="literal">Graph</code> interface and runs PageRank until the change in PageRank score is smaller than a specific error tolerance <code class="literal">tol</code>.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note19"></a>Note</h3><p>These <a id="id168" class="indexterm"></a>PageRank algorithms exploit data-parallelization over vertices. In particular, the <code class="literal">Pregel</code> implementation relies on local message passing for updating the PageRank scores. Another point to note is that the PageRank scores that are returned are not normalized. Thus, they do not represent a probability distribution. Moreover, pages that have no incoming links will have a PageRank score of alpha. Nonetheless, the top pages can be still be found by sorting the vertices of the returned PageRank graph by their <code class="literal">score</code> attribute.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec28"></a>Ranking web pages</h3></div></div></div><p>Here, we will use a new <a id="id169" class="indexterm"></a>dataset for demonstrating PageRank. The first one is a web graph of pages from the University of Notre Dame. Directed edges represent hyperlinks between them. Using PageRank, we will rank and find the most important pages.</p><p>The dataset can be downloaded from <a class="ulink" href="http://snap.stanford.edu/data/web-NotreDame.html" target="_blank">http://snap.stanford.edu/data/web-NotreDame.html</a>, which was first used by (Albert, Jeong &amp; Barabasi, 1999):</p><div class="informalexample"><pre class="programlisting">// Load web graph
val webGraph = GraphLoader.edgeListFile(sc,"./data/web- NotreDame.txt")

// Run PageRank with an error tolerance of 0.0001
val ranks = webGraph.pageRank(0.001).vertices

// Find the top 10 pages of University of Notre Dame 
val topPages = ranks.sortBy(_._2, false).take(10) </pre></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec22"></a>Scala Build Tool revisited</h2></div></div><hr /></div><p>Previously, we have <a id="id170" class="indexterm"></a>used the Scala console to interact with Spark. If we want to build a standalone application instead, it becomes unwieldy to manually manage the third-party library dependencies. Remember that first we had to download the JAR files for GraphStream and BreezeViz, as well as those of the libraries that they depend on. Then, we had to put them in the <code class="literal">/lib</code> folder and specify this list of JAR files when we submitted the Spark application using the <code class="literal">--jars</code> option. This process becomes extremely cumbersome when the application reuses many third-party libraries, which may also depend on several libraries. Fortunately, we can automate this process with SBT. Let's see how to manage the library dependencies, and how to create an uber JAR or assembly JAR with SBT. If you already know how to do this, feel free to skip this section and go ahead to the next chapter.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec29"></a>Organizing build definitions</h3></div></div></div><p>SBT offers flexibility <a id="id171" class="indexterm"></a>and power in defining builds and tracking library dependencies. In addition, SBT makes the build process reproducible and interactive. Despite this flexibility, learning all its features can be very discouraging to the unfamiliar user. Instead, we will focus on the essentials.</p><p>First, SBT assumes the same directory structure as Maven for the Spark project's source files, which is as follows:</p><div class="informalexample"><pre class="programlisting">src/
  main/
    resources/
&lt;files to include in main jar here&gt;
    scala/
&lt;main Scala sources&gt;
  test/
    resources
&lt;files to include in test jar here&gt;
    scala/
&lt;test Scala sources&gt;</pre></div><p>These paths are relative to the project's base directory. On the other hand, build definitions can be put in different files, and can be organized recursively within the project structure. Specifically, there are three places where we can put build definitions:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>A multi-project <code class="literal">.sbt</code> build file is recommended in situations where multiple related projects share common settings and dependencies which can be defined in a single build.</p></li><li style="list-style-type: disc"><p>The bare <code class="literal">.sbt</code> build files are useful for simple projects. Each <code class="literal">.sbt</code> build file defines a list of build and project settings.</p></li><li style="list-style-type: disc"><p>The <code class="literal">.scala</code> builds files are combined with the <code class="literal">.sbt</code> files to form the complete build definition. Prior to SBT 0.13, this was the old way to share common settings between the multiple projects.</p></li></ul></div><p>In this book, we will work on simple projects, and the bare <code class="literal">.sbt</code> build files<a id="id172" class="indexterm"></a> will suffice. For details about the mentioned options, refer to the tutorial at <a class="ulink" href="http://www.scala-sbt.org/0.13/tutorial/Basic-Def.html" target="_blank">http://www.scala-sbt.org/0.13/tutorial/Basic-Def.html</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec30"></a>Managing library dependencies</h3></div></div></div><p>We can manage <a id="id173" class="indexterm"></a>library dependencies manually or automatically. In manual mode, we will have to download all libraries in the dependency graph, and then manually copy them in the <code class="literal">lib</code> folder. In automatic mode, SBT handles all the work for us by leveraging Apache Ivy mechanisms behind the scenes. With this second method, we need to define three important settings in an SBT build file:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><span class="strong"><strong>Dependencies</strong></span>: These <a id="id174" class="indexterm"></a>are the libraries that our application depends on</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Resolvers</strong></span>: These are the <a id="id175" class="indexterm"></a>repositories' locations where SBT will look for the JAR files of these libraries</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>SBT</strong></span>: These are the <a id="id176" class="indexterm"></a>plugin settings</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note20"></a>Note</h3><p>The third set of settings is needed if we want to extend the build definitions using SBT plugins. For example, we will use the <code class="literal">sbt-assembly</code> plugin to package a Spark application and the JAR files, it depends on, into a single "uber JAR" file. For this, we need to specify some extra settings such as the uber JAR name as well as the options for creating the uber JAR.</p></div></li></ul></div><p>Once we have declared these settings, SBT will take care of the rest for us. Let's look at a concrete example to make sense of all this. We are going to build a Spark application that loads and visualizes food ingredient networks. Earlier in this chapter, we have used the Spark shell and manually managed the dependencies. This time, we will create a standalone application and handle the dependencies automatically.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec08"></a>A preview of the steps</h4></div></div></div><p>As a preview, here are the <a id="id177" class="indexterm"></a>steps that we will take to build the Spark application:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Create the <code class="literal">plugins.sbt</code> file inside the <code class="literal">/project</code> folder. Specify the <code class="literal">sbt-assembly</code> plugin in that file.</p></li><li><p>Create a <code class="literal">build.sbt</code> file in the base directory, and declare the project settings.</p></li><li><p>Specify the library dependencies and resolvers.</p></li><li><p>Set up the <code class="literal">sbt-assembly</code> plugin.</p></li><li><p>Use the SBT commands to assemble the uber JAR.</p></li></ol></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch03lvl4sec01"></a>Step 1 â€“ Enable the sbt-assembly plugin</h5></div></div></div><p>First, let's <a id="id178" class="indexterm"></a>enable the <code class="literal">sbt-assembly</code> plugin. This plugin creates a single deployable uber JAR that contains our built application, and all the libraries that we depend on (except some that we will intentionally exclude from the build). So, let's create the <code class="literal">plugins.sbt</code> file inside a <code class="literal">new/project</code> folder. The filename is not important, but it has to be inside the <code class="literal">/project</code> folder. Then, add this line in the file:</p><div class="informalexample"><pre class="programlisting">addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.12.0")</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch03lvl4sec02"></a>Step 2 â€“ Create a build.sbt file</h5></div></div></div><p>Now, create another <code class="literal">.sbt</code> file and <a id="id179" class="indexterm"></a>put it in the base directory. Let's give it a meaningful name, say, <code class="literal">build.sbt</code> file. As mentioned before, this single file will suffice for our simple project. For more complex ones, it is okay to put the definitions in the multiple <code class="literal">.sbt</code> files.</p><p>As we did in <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Getting Started with Spark and GraphX</em></span>, the first things we define in <code class="literal">build.sbt</code> are the project settings, that is the project name, its version, and the Scala version under which we will build the project. Add the following lines in <code class="literal">build.sbt</code>:</p><div class="informalexample"><pre class="programlisting">name := "Simple Visualization"

version := "1.0"

scalaVersion := "2.10.4"</pre></div><p>The <code class="literal">build.sbt</code> file defines a sequence of build settings. Each element in the sequence is a key-value pair of type <code class="literal">Setting[T]</code>, where <code class="literal">T</code> is the expected value type. Each line in <code class="literal">build.sbt</code> is then a Scala expression, which becomes one element in the sequence called <code class="literal">Seq[Setting[_]]</code>. For instance, in the expression <code class="literal">name:= "Simple Visualization"</code>, the left-hand name is a key that has a type <code class="literal">SettingKey[String]</code>. Each key has a method called <code class="literal">:=</code>, which returns a <code class="literal">Setting[T]</code>. In our example, the return type of the full expression name <code class="literal">:= "Simple Visualization"</code> is thus <code class="literal">Setting[String]</code>. In fact, this Scala expression is a syntactic sugar for the method call <code class="literal">nameâ€”:=("Simple Visualization")</code>.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip11"></a>Tip</h3><p>Do not forget to add empty lines between each setting. Since SBT uses a domain-specific language, the empty lines are mandatory to delineate the build expressions. These blank lines will no longer be needed after the release 0.13.7.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch03lvl4sec03"></a>Step 3 â€“ Declare library dependencies and resolvers</h5></div></div></div><p>To manage <a id="id180" class="indexterm"></a>the third-party libraries, we <a id="id181" class="indexterm"></a>will need to attach these libraries to the key called <code class="literal">libraryDependencies</code> in <code class="literal">build.sbt</code>. Since an application depends on more than one library, the value type corresponding to <code class="literal">libraryDependencies</code> is a sequence. Therefore, <code class="literal">libraryDependencies</code> accepts the append method <code class="literal">+=</code> to append a dependency, or the concatenate method <code class="literal">++=</code> to add a list of dependencies. However, it does not accept the operator <code class="literal">:=</code>.</p><p>Our application depends on Spark Core, GraphX, GraphStream, and Breeze libraries. In <code class="literal">build.sbt</code>, we will attach a list of dependencies, which are as follows:</p><div class="informalexample"><pre class="programlisting">libraryDependencies ++= Seq(
  "org.apache.spark" %% "spark-core" % "1.1.0" % "provided",
  "org.apache.spark" %% "spark-graphx" % "1.1.0" % "provided",
  "org.graphstream" % "gs-core" % "1.2+",
  "org.graphstream" % "gs-ui" % "1.2+",
  "org.scalanlp" % "breeze-viz_2.10" % "0.9",
  "org.scalanlp" % "breeze_2.10" % "0.9"
)</pre></div><p>Each sequence element in the right-hand side is a Scala expression that returns a <code class="literal">ModuleID</code> object. Each <code class="literal">ModuleID</code> object is constructed like thisâ€”<code class="literal">groupID % artifactID % revision</code>. The <code class="literal">groupID</code>, <code class="literal">artifactID</code>, and <code class="literal">revision</code> objects are all <code class="literal">String</code> objects.</p><p>In short, the <code class="literal">%</code> method creates the <code class="literal">ModuleID</code> objects from the passed strings, then we attach those <code class="literal">ModuleID</code> objects to the setting key <code class="literal">libraryDependencies</code>.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip12"></a>Tip</h3><p>Each dependency must correspond with the version of Scala that you are using. For libraries that were built with SBT, such as <code class="literal">spark-core</code> and <code class="literal">spark-graphx</code>, we can use the operator <code class="literal">%%</code> instead of <code class="literal">%</code> as <code class="literal">groupID %% artifactID % revision</code>. This will use the right JAR for the dependency, built with the same version of Scala that you are using.</p><p>We can also add configuration information to the <code class="literal">ModuleID</code> like this:</p><div class="informalexample"><pre class="programlisting">groupID % artifactID % revision % configuration</pre></div><p>For example, in <code class="literal">"org.apache.spark" %% "spark-core" % "1.1.0" % "provided"</code>, the configuration provided will inform the plugin <code class="literal">sbt-assembly</code> to exclude JAR files when packaging the uber JAR.</p></div><p>Sometimes, there<a id="id182" class="indexterm"></a> are pathological <a id="id183" class="indexterm"></a>cases where two libraries depend on the same library with different versions, and SBT cannot resolve the dependency conflict. For instance, if you try to package and run the application with the <code class="literal">build.sbt</code> definition, you will get an error like this due to the unresolved dependencies:</p><div class="informalexample"><pre class="programlisting">[error] (*:assembly)  deduplicate: different file contents found in the following:
~/.ivy2/cache/org.jfree/jfreechart/jars/jfreechart- 1.0.14.jar:org/jfree/chart/ChartPanel.class
~/.ivy2/cache/jfree/jfreechart/jars/jfreechart- 1.0.13.jar:org/jfree/chart/ChartPanel.class</pre></div><p>This error occurs because both the GraphStream and BreezeViz libraries depend on the Java libraries JFreeChart and JCommon. However, BreezeViz is rarely maintained and is stuck with the <code class="literal">jfreechart-1.0.13</code> library. To fix this, we have to exclude one of every duplicate JARs. To exclude specific JARs in the dependency graph of a given library, we call one of the methods <code class="literal">exclude</code> and <code class="literal">excludeAll</code> on the <code class="literal">ModuleID</code> object. In our case, we replace the <code class="literal">"org.scalanlp" % "breeze-viz_2.10" % "0.9"</code> expression by:</p><div class="informalexample"><pre class="programlisting">("org.scalanlp" % "breeze-viz_2.10" % "0.9").
    exclude("jfree","jfreechart").
    exclude("jfree","jcommon")</pre></div><p>The <code class="literal">exclude</code> method returns a new <code class="literal">ModuleID</code> object, but will not include the passed libraries in the final build.</p><p>After setting the dependencies, we have to tell SBT where it can download them. This is similarly done by attaching a sequence of repositories to the <code class="literal">resolvers</code> key as follows:</p><div class="informalexample"><pre class="programlisting">resolvers ++= Seq(
    "Akka Repository" at "http://repo.akka.io/releases/",
    "Sonatype OSS Snapshots" at "https://oss.sonatype.org/content/repositories/snapshots",
    "Sonatype Releases" at "http://oss.sonatype.org/content/repositories/releases")</pre></div><p>Each repository is declared using the form called <code class="literal">name</code> at location, where the method is invoked on the <code class="literal">String</code> objects. By default, SBT combines these declared resolvers with the default ones, such as Maven Central or a local Ivy repository.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch03lvl4sec04"></a>Step 4 â€“ Set up the sbt-assembly plugin</h5></div></div></div><p>Next, let's configure the<a id="id184" class="indexterm"></a> settings of the <code class="literal">sbt-assembly</code> plugin. Put the following in <code class="literal">build.sbt</code>:</p><div class="informalexample"><pre class="programlisting">jarName in assembly := "graph-Viz-assembly.jar"</pre></div><p>This configures the name of the uber JAR or assembly JAR to <code class="literal">graph-Viz-assembly.jar</code>.</p><p>We also need to exclude all the classes from the Scala language distribution. To do this, we tell SBT to exclude all the JARs that either start with <code class="literal">"scala-"</code>, or are part of the Scala distribution:</p><div class="informalexample"><pre class="programlisting">assemblyOption in assembly := (assemblyOption in assembly).value.copy(includeScala = false)</pre></div><p>After this step, <code class="literal">build.sbt</code> will finally look like this:</p><div class="informalexample"><pre class="programlisting">name := "Simple Visualization"

version := "1.0"

scalaVersion := "2.10.4"

libraryDependencies ++= Seq(
  "org.apache.spark" %% "spark-core" % "1.1.0" % "provided",
  "org.apache.spark" %% "spark-graphx" % "1.1.0" % "provided",
  "org.graphstream" % "gs-core" % "1.2+",
  "org.graphstream" % "gs-ui" % "1.2+",
  ("org.scalanlp" % "breeze-viz_2.10" % "0.9").exclude("jfree","jfreechart").exclude("jfree","jcommon"),
  "org.scalanlp" % "breeze_2.10" % "0.9"
)

resolvers ++= Seq(
  "Akka Repository" at "http://repo.akka.io/releases/",
  "Sonatype OSS Snapshots" at "https://oss.sonatype.org/content/repositories/snapshots",
  "Sonatype Releases" at "http://oss.sonatype.org/content/repositories/releases")

// Configure jar named used with the assembly plug-in
jarName in assembly := "graph-Viz-assembly.jar"

// Exclude Scala library (JARs that start with scala- and are included in the binary Scala distribution) 
assemblyOption in assembly := (assemblyOption in assembly).value.copy(includeScala = false)</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch03lvl4sec05"></a>Step 5 â€“ Create the uber JAR</h5></div></div></div><p>All that needs to be done<a id="id185" class="indexterm"></a> now is to run the command called <code class="literal">sbt assembly</code> in the console to build the uber JAR. This must be done with the current directory set to the project base directory:</p><div class="informalexample"><pre class="programlisting">sbt clean assembly</pre></div><p>This will create the uber JAR within the <code class="literal">target/scala-2.10/</code> folder. You can look inside the built uber JAR to see all the classes that it contains, which are as follows:</p><div class="informalexample"><pre class="programlisting">jar tf target/scala-2.10/graph-Viz-assembly.jar</pre></div><p>Finally, we can submit the built application with the <code class="literal">spark-submit</code> script by passing the assembly JAR this time:</p><div class="informalexample"><pre class="programlisting">../../bin/spark-submit --class com.github.giocode.graphxbook.SimpleGraphVizApp --master local target/scala-2.10/graph-Viz-assembly.jar</pre></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec09"></a>Running tasks with SBT commands</h4></div></div></div><p>SBT provides<a id="id186" class="indexterm"></a> different, useful commands for interacting with the<a id="id187" class="indexterm"></a> build in the SBT console. These are listed as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><code class="literal">clean</code>: This removes the files that were previously produced by the build, such as generated sources, compiled classes, and task caches</p></li><li style="list-style-type: disc"><p><code class="literal">console</code>: This starts the Scala shell with the project classes on the classpath</p></li><li style="list-style-type: disc"><p><code class="literal">compile</code>: This command compiles the sources</p></li><li style="list-style-type: disc"><p><code class="literal">update</code>: The execution of this command resolves and retrieves the dependencies, if required</p></li><li style="list-style-type: disc"><p><code class="literal">package</code>: This builds and produces a deployable JAR</p></li><li style="list-style-type: disc"><p><code class="literal">assembly</code>: This builds a uber JAR using the <code class="literal">sbt-assembly</code> plugin</p></li></ul></div></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec23"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we learned about the different ways to visualize and analyze graphs in Spark. We studied the connectedness of different networks by looking at their degree distribution, finding their connected components, and by calculating their cluster coefficients. In addition, we also learned how to visualize graph data using GraphStream. After this, we showed how the PageRank algorithm can be used to rank node importance in different networks. This chapter also showed us how to use SBT to build a Spark program that relies on third-party libraries.</p><p>Throughout this chapter, we have also studied how the basic Spark RDD operations can be used to transform, join, and filter collections of graph vertices and edges. In the next chapter, we will learn about the graph-specific and higher-level operations that are used to transform and manipulate the structure of graphs.</p><p>In the next chapter, we will learn about graph-specific operators that help change the properties of graph elements or modify the graph structure.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch04"></a>ChapterÂ 4.Â Transforming and Shaping Up Graphs to Your Needs</h2></div></div></div><p>In this chapter, we will learn to transform graphs using different sets of operators. In particular, we will cover graph-specific operators that either change the properties of graph elements or modify the structure of graphs. In other words, all the operators that we use here are methods that are invoked on a graph and return a new graph. In addition, we will use join methods to combine graph data with other datasets. Using real-world datasets, you will understand when and how to:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Use property operators to modify vertex or edge properties</p></li><li style="list-style-type: disc"><p>Use structural operators to modify the shape of a graph</p></li><li style="list-style-type: disc"><p>Join additional RDD collections with a property graph</p></li></ul></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec24"></a>Transforming the vertex and edge attributes</h2></div></div><hr /></div><p>The map operator is a core<a id="id188" class="indexterm"></a> method for transforming distributed datasets or <a id="id189" class="indexterm"></a>RDDs in Spark. Similarly, property graphs also have three map operators defined as follows:</p><div class="informalexample"><pre class="programlisting">class Graph[VD, ED] {
  def mapVertices[VD2](mapFun: (VertexId, VD) =&gt; VD2): Graph[VD2, ED]
  def mapEdges[ED2](mapFun: Edge[ED] =&gt; ED2): Graph[VD, ED2]
  def mapTriplets[ED2](mapFun: EdgeTriplet[VD, ED] =&gt; ED2): Graph[VD, ED2]
}</pre></div><p>Each of these methods is called on a property graph with vertex attribute type <code class="literal">VD</code> and edge attribute type <code class="literal">ED</code>. Each of them also takes a user-defined mapping function <code class="literal">mapFun</code> that performs one of the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>For <code class="literal">mapVertices</code>, <code class="literal">mapFun</code> takes a pair of <code class="literal">(VertexId, VD)</code> as input and returns a transformed vertex attribute of type <code class="literal">VD2</code>.</p></li><li style="list-style-type: disc"><p>For <code class="literal">mapEdges</code>, <code class="literal">mapFun</code> takes an <code class="literal">Edge</code> object as input and returns a transformed edge attribute of type <code class="literal">ED2</code>.</p></li><li style="list-style-type: disc"><p>For <code class="literal">mapTriplets</code>, <code class="literal">mapFun</code> takes an <code class="literal">EdgeTriplet</code> object as input and returns a <a id="id190" class="indexterm"></a>transformed<a id="id191" class="indexterm"></a> edge attribute of type <code class="literal">ED2</code>.</p></li></ul></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note21"></a>Note</h3><p>In each case, the graph structure remains intact, meaning these map operators never change the links between the vertices or their vertex indices. This is one key advantage of these operators compared to the basic RDD map operator. Although the latter can be used to achieve the same result, the former is also more efficient, thanks to the GraphX system optimization. Therefore, these three mapping operators should always be used if you just want to transform a graph's attributes without modifying its structure.</p><p>The difference between <code class="literal">mapEdges</code> and <code class="literal">mapTriplets</code> is that, for the latter, both the edge and source attributes are available in the triplet input of <code class="literal">mapFun</code> to create a new edge attribute. In contrast, the <code class="literal">mapFun</code> in <code class="literal">mapEdges</code> has access to only the edge attribute.</p></div><p>Now, let's see them in action through some simple examples.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec31"></a>mapVertices</h3></div></div></div><p>Consider a social graph <a id="id192" class="indexterm"></a>between people, where the vertex attribute has a type <code class="literal">Person</code> and the edge attribute has a type <code class="literal">Link</code>. First, let's create these Scala types as follows:</p><div class="informalexample"><pre class="programlisting">case class Person(first: String, last: String, age: Int)
case class Link(relationship: String, duration: Float)  </pre></div><p>Suppose we build the graph from <code class="literal">VertexRDD</code> called <code class="literal">people</code> and an <code class="literal">EdgeRDD</code> collection named <code class="literal">links</code>:</p><div class="informalexample"><pre class="programlisting">val inputGraph: Graph[Person, Link] = Graph(people, links)</pre></div><p>If we want, we can transform the attributes of the people to contain only their name using <code class="literal">mapVertices</code>:</p><div class="informalexample"><pre class="programlisting">val outputGraph: Graph[String, Link] = 
inputGraph.mapVertices((_, person) =&gt; person.first + person.last)</pre></div><p>The new <code class="literal">outputGraph</code> now has a vertex attribute of type <code class="literal">String</code> instead of <code class="literal">Person</code>. The links between the people remain unchanged.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec32"></a>mapEdges</h3></div></div></div><p>Similarly, suppose we are<a id="id193" class="indexterm"></a> interested only in the nature of relationships, not their duration. This time, we can use <code class="literal">mapEdges</code> to change the edge attribute as follows:</p><div class="informalexample"><pre class="programlisting">val outputGraph: Graph[Person, String] = 
inputGraph.mapEdges(link =&gt; link.relationship)</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec33"></a>mapTriplets</h3></div></div></div><p>Finally, suppose we want to keep <a id="id194" class="indexterm"></a>track of the people's ages from when they first met and add this information into the edge attribute. We can do that by using <code class="literal">mapTriplets</code>:</p><div class="informalexample"><pre class="programlisting">val outputGraph: Graph[Person, (Int, Int)] = 
inputGraph.mapTriplets(t =&gt; (t.srcAttr.age - t.attr.duration, 
t.dstAttr.age - t.attr.duration))</pre></div><p>If we want to change both the edge and vertex attributes of a graph, we can simply chain <code class="literal">mapEdges</code> or <code class="literal">mapTriplets</code> with <code class="literal">mapVertices</code> since each of these methods always returns a property graph.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec25"></a>Modifying graph structures</h2></div></div><hr /></div><p>The GraphX library also<a id="id195" class="indexterm"></a> comes with four useful methods for changing the structure of graphs. Their method signatures are listed as follows:</p><div class="informalexample"><pre class="programlisting">class Graph[VD, ED] {
  def reverse: Graph[VD, ED]
  
  def subgraph(epred: EdgeTriplet[VD,ED] =&gt; Boolean,
               vpred: (VertexId, VD) =&gt; Boolean): Graph[VD, ED]
               
  def mask[VD2, ED2](other: Graph[VD2, ED2]): Graph[VD, ED]
  
  def groupEdges(merge: (ED, ED) =&gt; ED): Graph[VD,ED]
}</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec34"></a>The reverse operator</h3></div></div></div><p>As its name suggests, the<a id="id196" class="indexterm"></a> reverse operator returns a new graph with all the edge<a id="id197" class="indexterm"></a> directions reversed. It does not modify any vertex or edge properties, or change the number of edges. Moreover, its implementation does not produce any data movement or duplication.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec35"></a>The subgraph operator</h3></div></div></div><p>Next on the list is <code class="literal">subgraph</code>, which is <a id="id198" class="indexterm"></a>useful for filtering graphs. It takes two<a id="id199" class="indexterm"></a> predicate functions as arguments that return Boolean values. The first predicate <code class="literal">epred</code> takes an <code class="literal">EdgeTriplet</code> and returns <code class="literal">true</code> when the triplet satisfies the predicate. Similarly, the <code class="literal">vpred</code> predicate takes a pair of <code class="literal">(VertexId, VD)</code> and returns <code class="literal">true</code> when the vertex satisfies the predicate condition.</p><p>Using these predicates, <code class="literal">subgraph</code> returns the graph containing only the nodes that satisfy the vertex predicate and keeps only the edges satisfying the edge predicate between the remaining nodes. By default, the vertex or edge predicate functions are set to return true when they are not provided. That means that we can pass only an edge predicate, only a vertex predicate, or both. If only a vertex predicate is passed to <code class="literal">subgraph</code> and two connected vertices are filtered out, then the edge connecting these nodes will automatically be filtered out as well.</p><p>The <code class="literal">subgraph</code> operator is very handy for countless situations. For instance, it is often the case, in practice, that the graphs have isolated nodes or edges with missing vertex information. We can eliminate these graph elements using <code class="literal">subgraph</code>. Another situation where <code class="literal">subgraph</code> is useful is when we want to remove hubs in the graph, for example, nodes that are connected to too many nodes.</p><p>As a concrete example, let's use <code class="literal">subgraph</code> to answer the following question often encountered in social networks: "Which people in my friends' list of friends are not yet my friends?":</p><div class="informalexample"><pre class="programlisting">// Given a social network 
type Name = String
class Person(name: Name, friends: List[Name])
val socialNetwork: Graph[Person, Int] = ... 

// that I am part of
val me = Person(myName, myFriends)

// I want know my friends' friends that are not yet my friends
val potentialFriends = socialNetwork.subgraph(vpred = 
(_, p: Person) =&gt; !(me.friends contains p.name)) </pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note22"></a>Note</h3><p>Note that we did not pass an edge predicate as an argument to <code class="literal">subgraph</code>. Thus, Scala uses the default value for <code class="literal">epred</code>, which is a function that always returns true. On the other hand, we should pass <code class="literal">vpred</code> as a named parameter so that Scala knows which predicate is passed or is missing.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec36"></a>The mask operator</h3></div></div></div><p>The mask operator<a id="id200" class="indexterm"></a> also filters a graph on which it is invoked. In contrast<a id="id201" class="indexterm"></a> to <code class="literal">subgraph</code>, <code class="literal">mask</code> does not take predicate functions as arguments. Instead, it takes another graph. Then, the expression <code class="literal">graph.mask(anotherGraph)</code> constructs a subgraph of <code class="literal">graph</code> by returning a graph that contains the vertices and edges that are also found in <code class="literal">anotherGraph</code>. This can be used together with the <code class="literal">subgraph</code> operator to filter a graph based on the properties in another related graph.</p><p>Consider the following situation where we want to find the connected components of a graph but we want to remove vertices with missing attribute information in the resulting graph. We can then run the <code class="literal">connectedComponent</code> algorithm we previously saw and use <code class="literal">subgraph</code> and <code class="literal">mask</code> together to obtain the desired result. This is shown in the following code:</p><div class="informalexample"><pre class="programlisting">// Run Connected Components
val ccGraph = graph.connectedComponents() 

// Remove vertices with missing attribute values and the edges connected to them
val validGraph = graph.subgraph(vpred = 
(_, attr) =&gt; attr.info != "NA")

// Restrict the resulting components to the valid subgraph
val validCCGraph = ccGraph.mask(validGraph) </pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec37"></a>The groupEdges operator</h3></div></div></div><p>Spark's <a id="id202" class="indexterm"></a>property graphs <a id="id203" class="indexterm"></a>are allowed to pair any of the connected nodes to have multiple edges. The <code class="literal">groupEdges</code> operator is another structural operator that merges duplicate edges between each pair of nodes into a single edge. To do that, <code class="literal">groupEdges</code> requires one function argument named <code class="literal">merge</code>, which takes a pair of edge attributes of type <code class="literal">ED</code> and combines them into a single attribute value of the same type. As a result, the graph returned by <code class="literal">groupEdges</code> has the same type as the original one. Later in this chapter, we will work on a detailed example in which we will see <code class="literal">groupEdges</code> in action.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec26"></a>Joining graph datasets</h2></div></div><hr /></div><p>In addition to the <a id="id204" class="indexterm"></a>previous mapping and filtering operations, GraphX also provides APIs for joining RDD datasets with graphs. This can be useful when we want to add extra information to the vertex attributes of a graph or when we want to merge the vertex attributes of two related graphs. These tasks can be accomplished using the following join operators.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec38"></a>joinVertices</h3></div></div></div><p>The following is the <a id="id205" class="indexterm"></a>method signature for the first <a id="id206" class="indexterm"></a>operator <code class="literal">joinVertices</code>:</p><div class="informalexample"><pre class="programlisting">def joinVertices[U](table: RDD[(VertexId, U)])(map: (VertexId, VD, U) =&gt; VD): Graph[VD, ED]</pre></div><p>It is invoked on a <code class="literal">Graph[VD, ED]</code> object and requires two inputs, which are passed as curried parameters. First, <code class="literal">joinVertices</code> joins a graph's vertex attributes with an input vertex RDD table of type <code class="literal">RDD[(VertexId, U)]</code>. Second, a user-defined <code class="literal">map</code> function is also passed to <code class="literal">joinVertices</code>. This <code class="literal">map</code> function joins the original and passed attributes of each vertex into a new attribute. The return type of this new attribute must be the same as the original one. Moreover, vertices without a matching value in the passed RDD retain their original value.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec39"></a>outerJoinVertices</h3></div></div></div><p>The second <a id="id207" class="indexterm"></a>join operator is <code class="literal">outerJoinVertices</code>, which is a <a id="id208" class="indexterm"></a>more general method than <code class="literal">joinVertices</code>. Its method signature is shown as follows:</p><div class="informalexample"><pre class="programlisting">def outerJoinVertices[U, VD2](table: RDD[(VertexId, U)])(map: (VertexId, VD, Option[U]) =&gt; VD2): Graph[VD2, ED]</pre></div><p>While <code class="literal">outerJoinVertices</code> also expects a vertex RDD and a user-defined <code class="literal">map</code> function as parameters, the <code class="literal">map</code> function is allowed to change the vertex attribute type. Furthermore, all vertices in the original graph are transformed even if they are not present in the passed RDD table.</p><p>As a result of this, the <code class="literal">map</code> function takes an <code class="literal">Option</code> type parameter <code class="literal">Option[U]</code> instead of simply <code class="literal">U</code> as in <code class="literal">joinVertices</code>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec40"></a>Example â€“ Hollywood movie graph</h3></div></div></div><p>An example will help <a id="id209" class="indexterm"></a>illustrate these differences. For that, let's go to Hollywood and build a small graph of movie actors and actresses:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val actors: RDD[(VertexId, String)] = sc.parallelize(List(</strong></span>
<span class="strong"><strong>    (1L, "George Clooney"),(2L, "Julia Stiles"), </strong></span>
<span class="strong"><strong>    (3L, "Will Smith"), (4L, "Matt Damon"), </strong></span>
<span class="strong"><strong>    (5L, "Salma Hayek")))</strong></span>
<span class="strong"><strong>actors: RDD[(VertexId, String)]</strong></span>
</pre></div><p>Two people in the graph will be connected if they appeared in a movie together. Each edge will contain the movie title. Let's load that information into an edge RDD called <code class="literal">movies</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt;  val movies: RDD[Edge[String]] = sc.parallelize(List(</strong></span>
<span class="strong"><strong>    Edge(1L,4L,"Ocean's Eleven"), </strong></span>
<span class="strong"><strong>    Edge(2L, 4L, "Bourne Ultimatum"), </strong></span>
<span class="strong"><strong>    Edge(3L, 5L, "Wild Wild West"), </strong></span>
<span class="strong"><strong>    Edge(1L, 5L, "From Dusk Till Dawn"), </strong></span>
<span class="strong"><strong>    Edge(3L, 4L, "The Legend of Bagger Vance"))</strong></span>
<span class="strong"><strong>)</strong></span>
<span class="strong"><strong>movies: RDD[Edge[String]]</strong></span>
</pre></div><p>Now, we can build the movie graph and see what's inside:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val movieGraph = Graph(actors, movies)</strong></span>
<span class="strong"><strong>movieGraph: Graph[String,String] </strong></span>

<span class="strong"><strong>scala&gt; movieGraph.triplets.foreach(t =&gt; println(</strong></span>
<span class="strong"><strong>t.srcAttr + " &amp; " + t.dstAttr + " appeared in " + t.attr))</strong></span>

<span class="strong"><strong>George Clooney &amp; Matt Damon appeared in Ocean's Eleven</strong></span>
<span class="strong"><strong>Julia Stiles &amp; Matt Damon appeared in Bourne Ultimatum</strong></span>
<span class="strong"><strong>George Clooney &amp; Salma Hayek appeared in From Dusk Till Dawn</strong></span>
<span class="strong"><strong>Will Smith &amp; Matt Damon appeared in The Legend of Bagger Vance</strong></span>
<span class="strong"><strong>Will Smith &amp; Salma Hayek appeared in Wild Wild West</strong></span>
</pre></div><p>For now, our vertices contain only the name of each actor/actress:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; movieGraph.vertices.foreach(println)</strong></span>
<span class="strong"><strong>(2,Julia Stiles)</strong></span>
<span class="strong"><strong>(1,George Clooney)</strong></span>
<span class="strong"><strong>(5,Salma Hayek)</strong></span>
<span class="strong"><strong>(4,Matt Damon)</strong></span>
<span class="strong"><strong>(3,Will Smith)</strong></span>
</pre></div><p>Suppose we have <a id="id210" class="indexterm"></a>access to a dataset of actor biographies. For this example, let's quickly load one such dataset into a vertex RDD:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; case class Biography(birthname: String, hometown: String)</strong></span>
<span class="strong"><strong>defined class Biography</strong></span>

<span class="strong"><strong>scala&gt; val bio: RDD[(VertexId, Biography)] = sc.parallelize(List(</strong></span>
<span class="strong"><strong>  (2, Biography("Julia O'Hara Stiles", "NY City, NY, USA")),</strong></span>
<span class="strong"><strong>  (3, Biography("Willard Christopher Smith Jr.", "Philadelphia, PA, USA")),</strong></span>
<span class="strong"><strong>  (4, Biography("Matthew Paige Damon", "Boston, MA, USA")),</strong></span>
<span class="strong"><strong>  (5, Biography("Salma Valgarma Hayek-Jimenez", "Coatzacoalcos, Veracruz, Mexico")),</strong></span>
<span class="strong"><strong>  (6, Biography("JosÃ© Antonio DomÃ­nguez Banderas", "MÃ¡laga, AndalucÃ­a, Spain")),</strong></span>
<span class="strong"><strong>  (7, Biography("Paul William Walker IV", "Glendale, CA, USA"))</strong></span>
<span class="strong"><strong>))</strong></span>
<span class="strong"><strong>bio: RDD[(VertexId, Biography)]</strong></span>
</pre></div><p>We are going to use <code class="literal">joinVertices</code> to join this information to our movie graph. To do that, let's create the user-defined function that appends the hometown of an actor/actress to their name:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; def appendHometown(id: VertexId, name: String, bio: Biography): String = name + ":"+ bio.hometown</strong></span>
<span class="strong"><strong>appendHometown: (id: VertexId, name: String, bio: Biography)String</strong></span>
</pre></div><p>Remember for <code class="literal">joinVertices</code>, the <code class="literal">mapping</code> function should return a string because that's the vertex attribute type of the original graph, for example <code class="literal">String</code>. Now, we can join the biography to the vertex attributes of our Hollywood graph:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val movieJoinedGraph =   </strong></span>
<span class="strong"><strong>movieGraph.joinVertices(bio)(appendHometown) </strong></span>
<span class="strong"><strong>movieJoinedGraph: Graph[String,String] </strong></span>

<span class="strong"><strong>scala&gt; movieJoinedGraph.vertices.foreach(println) </strong></span>

<span class="strong"><strong>(1,George Clooney)</strong></span>
<span class="strong"><strong>(5,Salma Hayek:Coatzacoalcos, Veracruz, Mexico)</strong></span>
<span class="strong"><strong>(2,Julia Stiles:NY City, NY, USA)</strong></span>
<span class="strong"><strong>(4,Matt Damon:Boston, MA, USA)</strong></span>
<span class="strong"><strong>(3,Will Smith:Philadelphia, PA, USA)</strong></span>
</pre></div><p>Next, let's use <code class="literal">outerJoinVertices</code> to see the difference. This time, we will directly pass an anonymous <code class="literal">map</code> function that joins the name and biography, and return this pair as is:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val movieOuterJoinedGraph = </strong></span>
<span class="strong"><strong>movieGraph.outerJoinVertices(bio)((_,name, bio) =&gt; (name,bio)) </strong></span>
<span class="strong"><strong>movieOuterJoinedGraph: Graph[(String, Option[Biography]), String]</strong></span>
</pre></div><p>Notice<a id="id211" class="indexterm"></a> how <code class="literal">outerJoinVertices</code> changed the vertex attribute type from a <code class="literal">String</code> to a <code class="literal">tuple (String, Option[Biography])</code>. Now, let's print the vertices:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; movieOuterJoinedGraph.vertices.foreach(println)</strong></span>
<span class="strong"><strong>(1,(George Clooney,None))</strong></span>
<span class="strong"><strong>(4,(Matt Damon,Some(Biography(Matthew Paige Damon,Boston, MA, USA))))</strong></span>
<span class="strong"><strong>(5,(Salma Hayek,Some(Biography(Salma Valgarma Hayek-Jimenez,Coatzacoalcos, Veracruz, Mexico))))</strong></span>
<span class="strong"><strong>(2,(Julia Stiles,Some(Biography(Julia O'Hara Stiles,NY City, NY, USA))))</strong></span>
<span class="strong"><strong>(3,(Will Smith,Some(Biography(Willard Christopher Smith Jr.,Philadelphia, PA, USA))))</strong></span>
</pre></div><p>As mentioned previously, even if there was not a biography of George Clooney in the <code class="literal">bio</code> dataset passed to <code class="literal">outerJoinVertices</code>, its new attribute has been changed to <code class="literal">None</code>, which is a valid instance of the optional type <code class="literal">Option[Biography]</code>.</p><p>Sometimes, it can be convenient to extract the information outside of the optional value. For this we can use the <code class="literal">getOrElse</code> method defined on <code class="literal">Option[T]</code> and provide a default new attribute value for the vertices that are not present in the passed vertex RDD:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val movieOuterJoinedGraph = movieGraph.outerJoinVertices(bio)((_, name, bio) =&gt; </strong></span>
<span class="strong"><strong>(name,bio.getOrElse(Biography("NA","NA")))) </strong></span>

<span class="strong"><strong>movieOuterJoinedGraph: Graph[(String, Biography),String] </strong></span>

<span class="strong"><strong>scala&gt; movieOuterJoinedGraph.vertices.foreach(println)</strong></span>
<span class="strong"><strong>(1,(George Clooney,Biography(NA,NA)))</strong></span>
<span class="strong"><strong>(2,(Julia Stiles,Biography(Julia O'Hara Stiles,NY City, NY, USA)))</strong></span>
<span class="strong"><strong>(5,(Salma Hayek,Biography(Salma Valgarma Hayek-Jimenez,Coatzacoalcos, Veracruz, Mexico)))</strong></span>
<span class="strong"><strong>(4,(Matt Damon,Biography(Matthew Paige Damon,Boston, MA, USA)))</strong></span>
<span class="strong"><strong>(3,(Will Smith,Biography(Willard Christopher Smith Jr.,Philadelphia, PA, USA)))</strong></span>
</pre></div><p>Alternatively, it is possible to create a new return type for the joined vertices. For instance, we can create a type <code class="literal">Actor</code> to generate a new graph of type <code class="literal">Graph[Actor,String]</code> as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; case class Actor(name: String, birthname: String, hometown: String) </strong></span>
<span class="strong"><strong>defined class Actor</strong></span>

<span class="strong"><strong>scala&gt; val movieOuterJoinedGraph = movieGraph.outerJoinVertices(bio)((_, name, b) =&gt; b match { </strong></span>
<span class="strong"><strong>       case Some(bio) =&gt; Actor(name, bio.birthname, bio.hometown)</strong></span>
<span class="strong"><strong>       case None =&gt; Actor(name, "", "")</strong></span>
<span class="strong"><strong>     })</strong></span>
<span class="strong"><strong>movieOuterJoinedGraph: Graph[Actor,String]</strong></span>
</pre></div><p>Listing the <a id="id212" class="indexterm"></a>vertices of the new graph, we get the expected result:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; movieOuterJoinedGraph.vertices.foreach(println)</strong></span>
<span class="strong"><strong>(4,Actor(Matt Damon,Matthew Paige Damon,Boston, MA, USA))</strong></span>
<span class="strong"><strong>(1,Actor(George Clooney,,))</strong></span>
<span class="strong"><strong>(5,Actor(Salma Hayek,Salma Valgarma Hayek-Jimenez,Coatzacoalcos, Veracruz, Mexico))</strong></span>
<span class="strong"><strong>(2,Actor(Julia Stiles,Julia O'Hara Stiles,NY City, NY, USA))</strong></span>
<span class="strong"><strong>(3,Actor(Will Smith,Willard Christopher Smith Jr.,Philadelphia, PA, USA))</strong></span>
</pre></div><p>Notice that no new vertices will be created for <code class="literal">Antonio Banderas</code> or <code class="literal">Paul Walker</code> despite their presence in the <code class="literal">bio</code> RDD because they do not belong to the original graph.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note23"></a>Note</h3><p>When calling <code class="literal">outerJoinVertices</code> mentioned previously, we have passed the argument <code class="literal">map</code> function without type annotation. This is optional as long as the definition of the <code class="literal">map</code> function conforms to the expected input and output types.</p><p>Although it's possible for the RDD dataset passed to <code class="literal">joinVertices</code> or <code class="literal">outerJoinVertices</code> to have more than one value for a vertex, only one value will be used. Therefore, it is recommended that the RDD is made to contain unique vertices.</p><p>For both <code class="literal">joinVertices</code> and <code class="literal">outerJoinVertices</code>, the vertices in the output graphs will be the same. Only their vertex attributes will be different. No new vertex will be created as their role is to join information from the passed RDD into existing vertices.</p></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec27"></a>Data operations on VertexRDD and EdgeRDD</h2></div></div><hr /></div><p>All of the operations we've seen<a id="id213" class="indexterm"></a> previously are graph operations. They are invoked on a graph and<a id="id214" class="indexterm"></a> they return a new graph object. In this section, we will introduce operations that transform <code class="literal">VertexRDD</code> and <code class="literal">EdgeRDD</code> collections. The types of these collections are subtypes of <code class="literal">RDD[(VertexID, VD)]</code> and <code class="literal">RDD[Edge[ED]]</code> respectively.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec41"></a>Mapping VertexRDD and EdgeRDD</h3></div></div></div><p>First, <code class="literal">mapValues</code> takes a <code class="literal">map</code> function<a id="id215" class="indexterm"></a> as input, which transforms each vertex attribute<a id="id216" class="indexterm"></a> in the <code class="literal">VertexRDD</code>. Then, it returns a new <code class="literal">VertexRDD</code> object while preserving the original vertex indices. The method <code class="literal">mapValues</code> is overloaded so that the <code class="literal">map</code> function can take an input with a type <code class="literal">VD</code> or <code class="literal">(VertexId, VD)</code>. The type of the new vertex attributes can be different to <code class="literal">VD</code>:</p><div class="informalexample"><pre class="programlisting">def mapValues[VD2](map: VD =&gt; VD2): VertexRDD[VD2]
def mapValues[VD2](map: (VertexId, VD) =&gt; VD2): VertexRDD[VD2]</pre></div><p>For illustration, let's take the biographies of the previous Hollywood stars in a <code class="literal">VertexRDD</code> collection:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val actorsBio = movieJoinedGraph.vertices</strong></span>
<span class="strong"><strong>actorsBio: VertexRDD[String] </strong></span>

<span class="strong"><strong>scala&gt; actorsBio.foreach(println)</strong></span>
<span class="strong"><strong>(4,Matt Damon:Boston, Massachusetts, USA)</strong></span>
<span class="strong"><strong>(1,George Clooney)</strong></span>
<span class="strong"><strong>(5,Salma Hayek:Coatzacoalcos, Veracruz, Mexico)</strong></span>
<span class="strong"><strong>(3,Will Smith:Philadelphia, Pennsylvania, USA)</strong></span>
<span class="strong"><strong>(2,Julia Stiles:New York City, New York, USA)</strong></span>
</pre></div><p>Now, we can use <code class="literal">mapValues</code> to extract their names into a new <code class="literal">VertexRDD</code> collection:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; actorsBio.mapValues(s =&gt; s.split(':')(0)).foreach(println)</strong></span>
<span class="strong"><strong>(2,Julia Stiles)</strong></span>
<span class="strong"><strong>(1,George Clooney)</strong></span>
<span class="strong"><strong>(5,Salma Hayek)</strong></span>
<span class="strong"><strong>(4,Matt Damon)</strong></span>
<span class="strong"><strong>(3,Will Smith)</strong></span>
</pre></div><p>Using the overloaded <code class="literal">mapValues</code> method, we can include the vertex IDs in the input of the <code class="literal">map</code> function and still get a similar result:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; actorsBio.mapValues((vid,s) =&gt; s.split(':')(0)).foreach(println)</strong></span>
<span class="strong"><strong>(1,George Clooney)</strong></span>
<span class="strong"><strong>(5,Salma Hayek)</strong></span>
<span class="strong"><strong>(3,Will Smith)</strong></span>
<span class="strong"><strong>(4,Matt Damon)</strong></span>
<span class="strong"><strong>(2,Julia Stiles)</strong></span>
</pre></div><p>There is also one <code class="literal">mapValues</code> method for transforming <code class="literal">EdgeRDDs</code>:</p><div class="informalexample"><pre class="programlisting">def mapValues[ED2](f: Edge[ED] =&gt; ED2): EdgeRDD[ED2]</pre></div><p>Similarly, <code class="literal">mapValues</code> changes<a id="id217" class="indexterm"></a> only the edge attributes. It does not remove or add edges, nor <a id="id218" class="indexterm"></a>does it modify the direction of the edges.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec42"></a>Filtering VertexRDDs</h3></div></div></div><p>Using the <code class="literal">filter</code> method, we <a id="id219" class="indexterm"></a>can also filter <code class="literal">VertexRDD</code> collections. While not changing the vertex indexing, <code class="literal">filter</code> removes the vertices that do not satisfy a user-defined predicate, which is passed to <code class="literal">filter</code>. Contrary to <code class="literal">mapValues</code>, <code class="literal">filter</code> is not overloaded so the type of the predicate must be <code class="literal">(VertexId, VD) =&gt; Boolean</code>. This is summarized as follows:</p><div class="informalexample"><pre class="programlisting">def filter(pred: (VertexId, VD) =&gt; Boolean): VertexRDD[VD]</pre></div><p>In addition to <code class="literal">filter</code>, the <code class="literal">diff</code> operation also filters vertices inside a <code class="literal">VertexRDD</code> collection. It takes another <code class="literal">VertexRDD</code> set as input and removes vertices from the original set that are also in the input set:</p><div class="informalexample"><pre class="programlisting">def diff(other: VertexRDD[VD]): VertexRDD[VD]</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note24"></a>Note</h3><p>GraphX does not provide a similar filter operation for <code class="literal">EdgeRDD</code> collections because filtering edges can be directly and efficiently achieved using the graph operation <code class="literal">subgraph</code>. See the previous section on <span class="emphasis"><em>Modifying graph structures</em></span>.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec43"></a>Joining VertexRDDs</h3></div></div></div><p>The following <a id="id220" class="indexterm"></a>join operators are optimized for <code class="literal">VertexRDD</code> collections:</p><div class="informalexample"><pre class="programlisting">def innerJoin[U, VD2](other: RDD[(VertexId, U)])(f: (VertexId, VD, U) =&gt; VD2): VertexRDD[VD2]

def leftJoin[U, VD2](other: RDD[(VertexId, VD2)])(f: (VertexId, VD, Option[U]) =&gt; VD2): VertexRDD[VD2]</pre></div><p>The first operator is <code class="literal">innerJoin</code>, which takes <code class="literal">VertexRDD</code> and a user-defined function <code class="literal">f</code> as inputs. Using this function, it joins the attributes of vertices that are present in both the original and input <code class="literal">VertexRDD</code> sets. In other words, <code class="literal">innerJoin</code> returns the intersection set of vertices and merges their attributes according to <code class="literal">f</code>.</p><p>So, given the vertex RDD from <code class="literal">movieGraph</code>, the result of <code class="literal">innerJoin</code> with the RDD of biographies will not contain <code class="literal">George Clooney</code>, <code class="literal">Paul Walker</code> or <code class="literal">JosÃ© Antonio DomÃ­nguez Banderas</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val actors = movieGraph.vertices</strong></span>
<span class="strong"><strong>actors: VertexRDD[String]</strong></span>

<span class="strong"><strong>scala&gt; actors.innerJoin(bio)((vid, name, b) =&gt; name + " is from " + b.hometown).foreach(println)</strong></span>
<span class="strong"><strong>(4,Matt Damon is from Boston, Massachusetts, USA)</strong></span>
<span class="strong"><strong>(5,Salma Hayek is from Coatzacoalcos, Veracruz, Mexico)</strong></span>
<span class="strong"><strong>(2,Julia Stiles is from New York City, New York, USA)</strong></span>
<span class="strong"><strong>(3,Will Smith is from Philadelphia, Pennsylvania, USA)</strong></span>
</pre></div><p>The second operator <code class="literal">leftJoin</code> is<a id="id221" class="indexterm"></a> similar to the operator <code class="literal">outerJoinVertices</code> defined in <code class="literal">Graph[VD,ED]</code>. It also takes a user-defined function <code class="literal">f</code> of type <code class="literal">(VertexId, VD, Option[U]) =&gt; VD2)</code> in addition to an input <code class="literal">VertexRDD</code> set. The resulting <code class="literal">VertexRDD</code> will also contain the same vertices as the original <code class="literal">VertexRDD</code>. Since the third input of the function <code class="literal">f</code> is <code class="literal">Option[U]</code>, it should handle the case when a vertex in the original <code class="literal">VertexRDD</code> set is not present in the input RDD. Using the previous example, we would do something like:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; actors.leftJoin(bio)((vid, name, b) =&gt; b match {</strong></span>
<span class="strong"><strong>    case Some(bio) =&gt; name + " is from " + bio.hometown</strong></span>
<span class="strong"><strong>    case None =&gt; name + "\'s hometown is unknown"</strong></span>
<span class="strong"><strong>}).foreach(println)</strong></span>

<span class="strong"><strong>(4,Matt Damon is from Boston, Massachusetts, USA)</strong></span>
<span class="strong"><strong>(1,George Clooney's hometown is unknown)</strong></span>
<span class="strong"><strong>(5,Salma Hayek is from Coatzacoalcos, Veracruz, Mexico)</strong></span>
<span class="strong"><strong>(2,Julia Stiles is from New York City, New York, USA)</strong></span>
<span class="strong"><strong>(3,Will Smith is from Philadelphia, Pennsylvania, USA)</strong></span>
</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec44"></a>Joining EdgeRDDs</h3></div></div></div><p>In GraphX, there exists a <a id="id222" class="indexterm"></a>join operator <code class="literal">innerJoin</code> for joining two <code class="literal">EdgeRDD</code>:</p><div class="informalexample"><pre class="programlisting">def innerJoin[ED2, ED3](other: EdgeRDD[ED2])(f: (VertexId, VertexId, ED, ED2) =&gt; ED3): EdgeRDD[ED3]</pre></div><p>It is similar to the <code class="literal">innerJoin</code> method for <code class="literal">VertexRDD</code>, except that now its input function has the type: <code class="literal">f: (VertexId, VertexId, ED, ED2) =&gt; ED3</code>. Moreover, <code class="literal">innerJoin</code> uses the same<a id="id223" class="indexterm"></a> partitioning strategy as the original <code class="literal">EdgeRDD</code>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec45"></a>Reversing edge directions</h3></div></div></div><p>Previously, we have <a id="id224" class="indexterm"></a>seen the <code class="literal">reverse</code> operation that reverses all the edges of graph. When we want to reverse only a subset of edges in a graph, the following <code class="literal">reverse</code> method defined as <code class="literal">EdgeRDD</code> objects becomes useful:</p><div class="informalexample"><pre class="programlisting">def reverse: EdgeRDD[ED]</pre></div><p>For instance, we know that graph properties must be directed in Spark. The only way to model a non-directed graph is to add a reverse link for each edge. This can easily be done using the <code class="literal">reverse</code> operator as follows. First, we extract the edges of the movie graph into the <code class="literal">EdgeRDD</code> movie:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val movies = movieGraph.edges</strong></span>
<span class="strong"><strong>movies: EdgeRDD[String,String] </strong></span>

<span class="strong"><strong>scala&gt; movies.foreach(println)</strong></span>
<span class="strong"><strong>Edge(1,4,Ocean's Eleven)</strong></span>
<span class="strong"><strong>Edge(3,5,Wild Wild West)</strong></span>
<span class="strong"><strong>Edge(2,4,Bourne Ultimatum)</strong></span>
<span class="strong"><strong>Edge(1,5,From Dusk Till Dawn)</strong></span>
<span class="strong"><strong>Edge(3,4,The Legend of Bagger Vance)</strong></span>
</pre></div><p>Then, we create a new <code class="literal">EdgeRDD</code> collection with the links reversed. Then, we obtain the bidirected graph using the union of these two <code class="literal">EdgeRDD</code> collections:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val bidirectedGraph = Graph(actors, movies union </strong></span>
<span class="strong"><strong>       movies.reverse)</strong></span>
</pre></div><p>We can see that this works by printing the new set of edges:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; bidirectedGraph.edges.foreach(println)</strong></span>
<span class="strong"><strong>Edge(1,5,From Dusk Till Dawn)</strong></span>
<span class="strong"><strong>Edge(3,4,The Legend of Bagger Vance)</strong></span>
<span class="strong"><strong>Edge(3,5,Wild Wild West)</strong></span>
<span class="strong"><strong>Edge(1,4,Ocean's Eleven)</strong></span>
<span class="strong"><strong>Edge(2,4,Bourne Ultimatum)</strong></span>
<span class="strong"><strong>Edge(4,1,Ocean's Eleven)</strong></span>
<span class="strong"><strong>Edge(4,2,Bourne Ultimatum)</strong></span>
<span class="strong"><strong>Edge(5,3,Wild Wild West)</strong></span>
<span class="strong"><strong>Edge(4,3,The Legend of Bagger Vance)</strong></span>
<span class="strong"><strong>Edge(5,1,From Dusk Till Dawn)</strong></span>
</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note25"></a>Note</h3><p><code class="literal">EdgeRDD[ED]</code> is a subtype of <code class="literal">RDD[Edge[ED]]</code> and it organizes the edges in to blocks partitioned using one of the partitioning strategies defined in <code class="literal">PartitionStrategy</code>. The edge attributes and adjacency structure are stored separately within each partition so that the structure can be reused when only the edge attributes are changed.</p><p>In Spark 1.0 and 1.1, the type signature of <code class="literal">EdgeRDD</code> has been changed <code class="literal">EdgeRDD[ED, VD]</code> for optimization purposes. Since Spark 1.2, the signature has switched back to the simpler <code class="literal">EdgeRDD[ED]</code> type definition while implementing the caching optimization in a different way.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec46"></a>Collecting neighboring information</h3></div></div></div><p>When doing <a id="id225" class="indexterm"></a>graph computations, we may want to use neighboring information, such as the attributes of neighboring vertices. The two operators, <code class="literal">collectNeighborIds</code> and <code class="literal">collectNeighbors</code> explicitly allow us to do that. <code class="literal">collectNeighborIds</code> collects into a <code class="literal">VertexRDD</code> only the vertex IDs of each node's neighbors, whereas <code class="literal">collectNeighbors</code> also collects their attributes:</p><div class="informalexample"><pre class="programlisting">def collectNeighborIds(edgeDirection: EdgeDirection): VertexRDD[Array[VertexId]]
def collectNeighbors(edgeDirection: EdgeDirection): VertexRDD[Array[(VertexId, VD)]]</pre></div><p>These two methods are invoked on a property graph and are passed with <code class="literal">EdgeDirection</code> as an input. An <code class="literal">EdgeDirection</code> attribute can take four possible values:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><code class="literal">Edge.Direction.In</code>: When this option is specified, each vertex collects only the attributes of neighbors that have an incoming link to it</p></li><li style="list-style-type: disc"><p><code class="literal">Edge.Direction.Out</code>: Each vertex collects only the attributes of neighbors that it links to</p></li><li style="list-style-type: disc"><p><code class="literal">Edge.Direction.Either</code>: Each vertex collects the attributes of all its neighbors</p></li><li style="list-style-type: disc"><p><code class="literal">Edge.Direction.Both</code>: Each vertex collects the attributes of the neighbors with which it has both an incoming edge and outgoing one</p></li></ul></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip14"></a>Tip</h3><p>For optimal performance, it is best to avoid using these two operators and rewrite the computation using the more generic and efficient <code class="literal">aggregateMessages</code> operator presented in the next chapter. The efficiency gain can be substantial especially when implementing an iterative graph-parallel algorithm. But for simple graph transformations that are done only once, it is ok to use <code class="literal">collectNeighors</code> and <code class="literal">collectNeighborIds</code>.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec47"></a>Example â€“ from food network to flavor pairing</h3></div></div></div><p>In <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Building and Exploring Graphs</em></span>, we presented the food ingredient dataset and built a bipartite graph that connects each food ingredient to its compounds. In the following, we will build another graph, which consists of only food ingredients. A pair of food ingredients is connected in the new graph only if they share at least one compound. We'll call this new graph the flavor network. We can later use this graph to create new recipes by experimenting with new food pairings.</p><p>Let's start with the bipartite food network<a id="id226" class="indexterm"></a> that we built in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Building and Exploring Graphs</em></span>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val nodes = ingredients ++ compounds</strong></span>
<span class="strong"><strong>scala&gt; val foodNetwork = Graph(nodes, links)</strong></span>
<span class="strong"><strong>foodNetwork: Graph[Node,Int]</strong></span>
</pre></div><p>To create the new flavor network, we need to know which ingredients share some compounds. This can be done by first collecting the ingredient IDs for each compound node in the <code class="literal">foodNetwork</code> graph. Concretely, we collect and group ingredient IDs that have that same compound into an RDD collection of tuples <code class="literal">(compound id, Array[ingredient id])</code>, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val similarIngr: RDD[(VertexId, Array[VertexId])] = </strong></span>
<span class="strong"><strong>foodNetwork.collectNeighborIds(EdgeDirection.In)</strong></span>
<span class="strong"><strong>similarIngr: RDD[(VertexId, Array[VertexId])]</strong></span>
</pre></div><p>Next, we create a function <code class="literal">pairIngredients</code> that takes one such tuple of <code class="literal">(compound id, Array[ingredient id])</code> and creates an edge between every pair of ingredients in the array:</p><div class="informalexample"><pre class="programlisting">def pairIngredients(ingPerComp: (VertexId, Array[VertexId])): Seq[Edge[Int]] =
    for {
        x &lt;- ingPerComp._2
        y &lt;- ingPerComp._2
        if x != y
  }   yield Edge(x,y,1)
pairIngredients:   
(ingPerComp:(VertexId,Array[VertexId]))Seq[Edge[Int]]</pre></div><p>Once we have that, we can create an <code class="literal">EdgeRDD</code> collection for every pair of ingredients that share the same compounds from the food network, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val flavorPairsRDD: RDD[Edge[Int]] = similarIngr flatMap pairIngredients</strong></span>
<span class="strong"><strong>flavorPairsRDD: RDD[Edge[Int]]</strong></span>
</pre></div><p>Finally, we can create the<a id="id227" class="indexterm"></a> new flavor network:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val flavorNetwork = Graph(ingredients, flavorPairsRDD).cache</strong></span>
<span class="strong"><strong>flavorNetwork: Graph[Node,Int]</strong></span>
</pre></div><p>Let's print the first 20 triplets in <code class="literal">flavorNetwork</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; flavorNetwork.triplets.take(20).foreach(println)</strong></span>
<span class="strong"><strong>((3,Ingredient(mackerel,fish/seafood)),(9,Ingredient(peanut_butter,plant derivative)),1)</strong></span>
<span class="strong"><strong>((3,Ingredient(mackerel,fish/seafood)),(9,Ingredient(peanut_butter,plant derivative)),1)</strong></span>
<span class="strong"><strong>((3,Ingredient(mackerel,fish/seafood)),(9,Ingredient(peanut_butter,plant derivative)),1)</strong></span>
<span class="strong"><strong>((3,Ingredient(mackerel,fish/seafood)),(9,Ingredient(peanut_butter,plant derivative)),1)</strong></span>
<span class="strong"><strong>((3,Ingredient(mackerel,fish/seafood)),(9,Ingredient(peanut_butter,plant derivative)),1)</strong></span>
<span class="strong"><strong>((3,Ingredient(mackerel,fish/seafood)),(9,Ingredient(peanut_butter,plant derivative)),1)</strong></span>
<span class="strong"><strong>((3,Ingredient(mackerel,fish/seafood)),(9,Ingredient(peanut_butter,plant derivative)),1)</strong></span>
<span class="strong"><strong>((3,Ingredient(mackerel,fish/seafood)),(9,Ingredient(peanut_butter,plant derivative)),1)</strong></span>
<span class="strong"><strong>((3,Ingredient(mackerel,fish/seafood)),(9,Ingredient(peanut_butter,plant derivative)),1)</strong></span>
<span class="strong"><strong>((3,Ingredient(mackerel,fish/seafood)),(9,Ingredient(peanut_butter,plant derivative)),1)</strong></span>
<span class="strong"><strong>((3,Ingredient(mackerel,fish/seafood)),(9,Ingredient(peanut_butter,plant derivative)),1)</strong></span>
<span class="strong"><strong>((3,Ingredient(mackerel,fish/seafood)),(9,Ingredient(peanut_butter,plant derivative)),1)</strong></span>
<span class="strong"><strong>((3,Ingredient(mackerel,fish/seafood)),(9,Ingredient(peanut_butter,plant derivative)),1)</strong></span>
<span class="strong"><strong>((3,Ingredient(mackerel,fish/seafood)),(17,Ingredient(red_bean,vegetable)),1)</strong></span>
<span class="strong"><strong>((3,Ingredient(mackerel,fish/seafood)),(17,Ingredient(red_bean,vegetable)),1)</strong></span>
<span class="strong"><strong>((3,Ingredient(mackerel,fish/seafood)),(17,Ingredient(red_bean,vegetable)),1)</strong></span>
<span class="strong"><strong>((3,Ingredient(mackerel,fish/seafood)),(17,Ingredient(red_bean,vegetable)),1)</strong></span>
<span class="strong"><strong>((3,Ingredient(mackerel,fish/seafood)),(17,Ingredient(red_bean,vegetable)),1)</strong></span>
<span class="strong"><strong>((3,Ingredient(mackerel,fish/seafood)),(17,Ingredient(red_bean,vegetable)),1)</strong></span>
<span class="strong"><strong>((3,Ingredient(mackerel,fish/seafood)),(17,Ingredient(red_bean,vegetable)),1)</strong></span>
</pre></div><p>It seems mackerel, peanut butter and red beans have something in common. Before we try a new recipe, let's slightly modify the network. Notice that duplicate edges are possible when a pair of ingredients share more than one compound. Suppose we want to group parallel edges between each pair of ingredients into a single edge, which contains the number of shared compounds between the two ingredients. We can do that using the <code class="literal">groupEdges</code> method:</p><div class="informalexample"><pre class="programlisting">val flavorWeightedNetwork = flavorNetwork.partitionBy(PartitionStrategy.EdgePartition2D). groupEdges((x,y) =&gt; x+y)
flavorWeightedNetwork: Graph[Node,Int]</pre></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip15"></a>Tip</h3><p><code class="literal">groupEdges</code> requires the graph to be repartitioned because it assumes that identical edges will be co-located on the same partition. Thus, you must call <code class="literal">partitionBy</code> prior to grouping the edges.</p></div><p>Now, let's print the 20 <a id="id228" class="indexterm"></a>pairs of ingredients that share the most compounds:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; flavorWeightedNetwork.triplets.</strong></span>
<span class="strong"><strong>sortBy(t =&gt; t.attr, false).take(20).</strong></span>
<span class="strong"><strong>foreach(t =&gt; println(t.srcAttr.name + " and " + t.dstAttr.name + " share " + t.attr + " compounds."))</strong></span>
<span class="strong"><strong>bantu_beer and beer share 227 compounds.</strong></span>
<span class="strong"><strong>beer and bantu_beer share 227 compounds.</strong></span>
<span class="strong"><strong>roasted_beef and grilled_beef share 207 compounds.</strong></span>
<span class="strong"><strong>grilled_beef and roasted_beef share 207 compounds.</strong></span>
<span class="strong"><strong>grilled_beef and fried_beef share 200 compounds.</strong></span>
<span class="strong"><strong>fried_beef and grilled_beef share 200 compounds.</strong></span>
<span class="strong"><strong>beef and roasted_beef share 199 compounds.</strong></span>
<span class="strong"><strong>beef and grilled_beef share 199 compounds.</strong></span>
<span class="strong"><strong>beef and raw_beef share 199 compounds.</strong></span>
<span class="strong"><strong>beef and fried_beef share 199 compounds.</strong></span>
<span class="strong"><strong>roasted_beef and beef share 199 compounds.</strong></span>
<span class="strong"><strong>roasted_beef and raw_beef share 199 compounds.</strong></span>
<span class="strong"><strong>roasted_beef and fried_beef share 199 compounds.</strong></span>
<span class="strong"><strong>grilled_beef and beef share 199 compounds.</strong></span>
<span class="strong"><strong>grilled_beef and raw_beef share 199 compounds.</strong></span>
<span class="strong"><strong>raw_beef and beef share 199 compounds.</strong></span>
<span class="strong"><strong>raw_beef and roasted_beef share 199 compounds.</strong></span>
<span class="strong"><strong>raw_beef and grilled_beef share 199 compounds.</strong></span>
<span class="strong"><strong>raw_beef and fried_beef share 199 compounds.</strong></span>
<span class="strong"><strong>fried_beef and beef share 199 compounds.</strong></span>
</pre></div><p>It is not too surprising <a id="id229" class="indexterm"></a>that roasted beef and grilled beef have lots of things in common. While the example did not teach us much about culinary arts, it showed that we could mix multiple operators to change a graph into a desired form.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec28"></a>Summary</h2></div></div><hr /></div><p>To summarize, GraphX offers several methods and operators for transforming graph elements and modifying its structure. We can use graph-specific operators, which transform a graph into a new one. In addition, we can use special methods that operate on <code class="literal">VertexRDD</code> and <code class="literal">EdgeRDD</code> collections. Moreover, we used join methods to combine graph data with other datasets. You can use all these methods to wrangle new graph datasets and put them in to a shape that suits your specific needs.</p><p>In the next chapter, you will learn how to create custom graph operators of your own using generic optimized methods, such as <code class="literal">aggregateMessages</code> and <code class="literal">mapReduceTriplets</code>.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch05"></a>ChapterÂ 5.Â Creating Custom Graph Aggregation Operators</h2></div></div></div><p>In the previous chapter, we have seen various operations for transforming the elements of a graph and for modifying its structure. Here, we will learn to use a generic and powerful operator named <code class="literal">aggregateMessages</code> that is useful for aggregating the neighborhood information of all nodes in the graph. In fact, many graph-processing algorithms rely on iteratively accessing the properties of neighboring nodes and adjacent edges. One such example is the PageRank algorithm.</p><p>By applying <code class="literal">aggregateMessages</code> to the NCAA College Basketball datasets, you will be able to:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Understand the basic mechanisms and patterns of <code class="literal">aggregateMessages</code></p></li><li style="list-style-type: disc"><p>Apply it to create custom graph aggregation operations</p></li><li style="list-style-type: disc"><p>Optimize the performance and efficiency of <code class="literal">aggregateMessages</code></p></li></ul></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec29"></a>NCAA College Basketball datasets</h2></div></div><hr /></div><p>We will again learn by doing in <a id="id230" class="indexterm"></a>this chapter. This time, we will take the NCAA College Basketball as an illustrative example. Specifically, we use two CSV datasets. The first one <code class="literal">teams.csv</code> contains the list of all college teams that played in the NCAA Division I competition. Each team is associated with a four-digit ID number. The second dataset <code class="literal">stats.csv</code> contains the score and statistics of every game during the 2014-2015 regular season. Using the techniques learned in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Building and Exploring Graphs</em></span>, let's parse and load these datasets and load them into RDDs:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>We create a class <code class="literal">GameStats</code> that records the statistics of one team during a specific basketball game:</p><div class="informalexample"><pre class="programlisting">case class GameStats(
    val score: Int,
    val fieldGoalMade:   Int,
    val fieldGoalAttempt: Int, 
    val threePointerMade: Int,
    val threePointerAttempt: Int,
    val threeThrowsMade: Int,
    val threeThrowsAttempt: Int, 
    val offensiveRebound: Int,
    val defensiveRebound: Int,
    val assist: Int,
    val turnOver: Int,
    val steal: Int,
    val block: Int,
    val personalFoul: Int
)</pre></div></li><li><p>We also add the <a id="id231" class="indexterm"></a>following methods to <code class="literal">GameStats</code> in order to know how efficient a team's offense was during a game:</p><div class="informalexample"><pre class="programlisting">// Field Goal percentage
def fgPercent: Double = 100.0 * fieldGoalMade / fieldGoalAttempt

// Three Point percentage
def tpPercent: Double = 100.0 * threePointerMade / threePointerAttempt

// Free throws percentage
def ftPercent: Double = 100.0 * threeThrowsMade / threeThrowsAttempt
override def toString: String = "Score: " + score </pre></div></li><li><p>We now create a couple of classes for the games' result:</p><div class="informalexample"><pre class="programlisting">abstract class GameResult(
    val season:     Int, 
    val day:        Int,
    val loc:        String
)

case class FullResult(
    override val season:    Int, 
    override val day:       Int,
    override val loc:       String, 
    val winnerStats:        GameStats,
    val loserStats:         GameStats 
) extends GameResult(season, day, loc)</pre></div><p><code class="literal">FullResult</code> has the year and day of the season, the location where the game was played, and the game statistics of both the winning and losing teams.</p></li><li><p>We will then create a statistics graph of the regular seasons. In this graph, the nodes are the teams, whereas each edge corresponds to a specific game. To create the graph, let's parse the CSV file <code class="literal">teams.csv</code> into the RDD teams:</p><div class="informalexample"><pre class="programlisting">val teams: RDD[(VertexId, String)] =
    sc.textFile("./data/teams.csv").
    filter(! _.startsWith("#")).
    map {line =&gt;
        val row = line split ','
        (row(0).toInt, row(1))
    }</pre></div></li><li><p>We can check the first few teams in this new RDD:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; teams.take(3).foreach{println}</strong></span>
<span class="strong"><strong>(1101,Abilene Chr)</strong></span>
<span class="strong"><strong>(1102,Air Force)</strong></span>
<span class="strong"><strong>(1103,Akron)</strong></span>
</pre></div></li><li><p>We do the same<a id="id232" class="indexterm"></a> thing to obtain an RDD of the game results, which will have a type <code class="literal">RDD[Edge[FullResult]]</code>. We just parse <code class="literal">stats.csv</code> and record the fields that we needâ€” the ID of the winning team, the ID of the losing team, and the game statistics of both teams:</p><div class="informalexample"><pre class="programlisting">val detailedStats: RDD[Edge[FullResult]] =
  sc.textFile("./data/stats.csv").
  filter(! _.startsWith("#")).
  map {line =&gt;
      val row = line split ','
      Edge(row(2).toInt, row(4).toInt, 
          FullResult(
              row(0).toInt, row(1).toInt, 
              row(6),
              GameStats(      
                              score = row(3).toInt,
                      fieldGoalMade = row(8).toInt,
                   fieldGoalAttempt = row(9).toInt, 
                   threePointerMade = row(10).toInt,
                threePointerAttempt = row(11).toInt,   
                    threeThrowsMade = row(12).toInt,
                 threeThrowsAttempt = row(13).toInt, 
                   offensiveRebound = row(14).toInt,
                   defensiveRebound = row(15).toInt,
                             assist = row(16).toInt,
                           turnOver = row(17).toInt,
                              steal = row(18).toInt,
                              block = row(19).toInt,
                       personalFoul = row(20).toInt
              ),
              GameStats(
                              score = row(5).toInt,
                      fieldGoalMade = row(21).toInt,
                   fieldGoalAttempt = row(22).toInt, 
                   threePointerMade = row(23).toInt,
                threePointerAttempt = row(24).toInt,
                    threeThrowsMade = row(25).toInt,
                 threeThrowsAttempt = row(26).toInt, 
                   offensiveRebound = row(27).toInt,
                   defensiveRebound = row(28).toInt,
                             assist = row(20).toInt,
                           turnOver = row(30).toInt,
                              steal = row(31).toInt,
                              block = row(32).toInt,
                       personalFoul = row(33).toInt
              )
          )
      )
  }</pre></div><p>Let's check what we have got:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; detailedStats.take(3).foreach(println)</strong></span>
<span class="strong"><strong>Edge(1165,1384,FullResult(2006,8,N,Score: 75-54))</strong></span>
<span class="strong"><strong>Edge(1393,1126,FullResult(2006,8,H,Score: 68-37))</strong></span>
<span class="strong"><strong>Edge(1107,1324,FullResult(2006,9,N,Score: 90-73))</strong></span>
</pre></div></li><li><p>We then create our graph of stats:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val scoreGraph = Graph(teams, detailedStats)</strong></span>
</pre></div></li></ol></div><p>For curiosity, let's<a id="id233" class="indexterm"></a> see which team has won against the 2015 NCAA champions Duke in the regular season. To do that, we filter the graph triplets whose destination attribute is <code class="literal">Duke</code>. This is because when we created our stats graph, each edge is directed from the winner node to the loser node. So, Duke has lost only four <a id="id234" class="indexterm"></a>games in the regular season:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; scoreGraph.triplets.filter(_.dstAttr == "Duke").foreach(println)</strong></span>
<span class="strong"><strong>((1274,Miami FL),(1181,Duke),FullResult(2015,71,A,Score: 90-74))</strong></span>
<span class="strong"><strong>((1301,NC State),(1181,Duke),FullResult(2015,69,H,Score: 87-75))</strong></span>
<span class="strong"><strong>((1323,Notre Dame),(1181,Duke),FullResult(2015,86,H,Score: 77-73))</strong></span>
<span class="strong"><strong>((1323,Notre Dame),(1181,Duke),FullResult(2015,130,N,Score: 74-64))</strong></span>
</pre></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec30"></a>The aggregateMessages operator</h2></div></div><hr /></div><p>Once we have our graph<a id="id235" class="indexterm"></a> ready, let's start our mission, which is aggregating the stats data in <code class="literal">scoreGraph</code>. In GraphX, <code class="literal">aggregateMessages</code> is the operator for that kind of job.</p><p>For example, let's find out the average field goals made per game by the winning teams. In other words, the games that the teams lost will not be counted. To get the average for each team, we first need to have the number of games won by the team and the total field goals that the team made in those games:</p><div class="informalexample"><pre class="programlisting">// Aggregate the total field goals made by winning teams
type FGMsg = (Int, Int)
val winningFieldGoalMade: VertexRDD[FGMsg] = scoreGraph aggregateMessages(
    // sendMsg
    triplet =&gt; triplet.sendToSrc(1, triplet.attr.winnerStats.fieldGoalMade)
    // mergeMsg
    ,(x, y) =&gt; (x._1 + y._1, x._2+ y._2)
)
// Aggregate the total field goals made by winning teams
type Msg = (Int, Int)
type Context = EdgeContext[String, FullResult, Msg] 
val winningFieldGoalMade: VertexRDD[Msg] = scoreGraph aggregateMessages(
    // sendMsg
    (ec: Context) =&gt; ec.sendToSrc(1, ec.attr.winnerStats.fieldGoalMade),
    
    // mergeMsg
    (x: Msg, y: Msg) =&gt; (x._1 + y._1, x._2+ y._2)
)</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec48"></a>EdgeContext</h3></div></div></div><p>There is a lot<a id="id236" class="indexterm"></a> going on in the previous <a id="id237" class="indexterm"></a>call to <code class="literal">aggregateMessages</code>. So, let's see it working in slow motion. When we called <code class="literal">aggregateMessages</code> on the <code class="literal">scoreGraph</code> method, we had to pass two functions as arguments.</p><p>The first function has a signature <code class="literal">EdgeContext[VD, ED, Msg] =&gt; Unit</code>. It takes an <code class="literal">EdgeContext</code> parameter as input. It does not return anything but it can produce side effects, such as sending a message to a node.</p><p>Ok, but what is that <code class="literal">EdgeContext</code> type? Similar to <code class="literal">EdgeTriplet</code>, <code class="literal">EdgeContext</code> represents an edge along with its neighboring nodes. It can access both the edge attribute, and the source and destination nodes' attributes. In addition, <code class="literal">EdgeContext</code> has two methods to send messages along the edge to its source node or to its destination node. These methods are <code class="literal">sendToSrc</code> and <code class="literal">sendToDst</code> respectively. Then, the type of message that we want each triplet in the graph to send is defined by <code class="literal">Msg</code>. Similar to <code class="literal">VD</code> and <code class="literal">ED</code>, we can define the concrete type that <code class="literal">Msg</code> takes.</p><p>In our example, we need to aggregate the number of games played and the number of field goals made. Therefore, we define <code class="literal">Msg</code> as a pair of <code class="literal">Int</code>. Furthermore, each edge context sends a message to only its source node, that is the winning team, because we are interested in the total field goals made by the teams for only the games that they won. The actual message sent to each winner node is a pair of integers <code class="literal">(1, ec.attr.winnerStats.fieldGoalMade)</code>. The first integer serves as a counter for the games won by the source node, whereas the second one corresponds to the number of field goals made by the winner. This latter integer is then extracted from the edge attribute.</p><p>In addition to <code class="literal">sendMsg</code>, the second function that we need to pass to <code class="literal">aggregateMessages</code> is a <code class="literal">mergeMsg</code> function with the signature <code class="literal">(Msg, Msg) =&gt; Msg</code>. As its name implies, <code class="literal">mergeMsg</code> is used to merge two messages received at each node into a new one. Its output type must be the same, for example <code class="literal">Msg</code>. Using these two functions, <code class="literal">aggregateMessages</code> returns the aggregated messages inside <code class="literal">VertexRDD[Msg]</code>.</p><p>Returning to our example, we set out to compute the average field goals per winning game for all teams. To get this final result, we simply apply <code class="literal">mapValues</code> to the output of <code class="literal">aggregateMessages</code>, as follows:</p><div class="informalexample"><pre class="programlisting">// Average field goals made per Game by winning teams
val avgWinningFieldGoalMade: VertexRDD[Double] = 
    winningFieldGoalMade mapValues (
        (id: VertexId, x: Msg) =&gt; x match {
            case (count: Int, total: Int) =&gt; total.toDouble/count
})</pre></div><p>Let's check the output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; avgWinningFieldGoalMade.take(5).foreach(println)</strong></span>
<span class="strong"><strong>(1260,24.71641791044776)</strong></span>
<span class="strong"><strong>(1410,23.56578947368421)</strong></span>
<span class="strong"><strong>(1426,26.239436619718308)</strong></span>
<span class="strong"><strong>(1166,26.137614678899084)</strong></span>
<span class="strong"><strong>(1434,25.34285714285714)</strong></span>
</pre></div><p>The definitions<a id="id238" class="indexterm"></a> of <code class="literal">aggregateMessages</code> and<a id="id239" class="indexterm"></a> <code class="literal">EdgeContext</code>, as we explained previously, are shown as follows:</p><div class="informalexample"><pre class="programlisting">class Graph[VD, ED] {
  def aggregateMessages[Msg: ClassTag](
      sendMsg: EdgeContext[VD, ED, Msg] =&gt; Unit,
      mergeMsg: (Msg, Msg) =&gt; Msg,
      tripletFields: TripletFields = TripletFields.All)
    : VertexRDD[Msg]
}



abstract class EdgeContext[VD, ED, A] {
    
    // Attribute associated with the edge:
    abstract def attr: ED
    
    // Vertex attribute of the edge's source vertex.    
    abstract def srcAttr: VD

    // Vertex attribute of the edge's destination vertex.    
    abstract def dstAttr: VD

    // Vertex id of the edge's source vertex.
    abstract def srcId: VertexId
    
    // Vertex id of the edge's destination vertex.
    abstract def dstId: VertexId

    // Sends a message to the destination vertex.
    abstract def sendToDst(msg: A): Unit

    // Sends a message to the source vertex.
    abstract def sendToSrc(msg: A): Unit    
}</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec49"></a>Abstracting out the aggregation</h3></div></div></div><p>That was kinda cool! We <a id="id240" class="indexterm"></a>can do the same to average the points per game scored by winning teams:</p><div class="informalexample"><pre class="programlisting">// Aggregate the points scored by winning teams
val winnerTotalPoints: VertexRDD[(Int, Int)] = scoreGraph.aggregateMessages(
    // sendMsg
    triplet =&gt; triplet.sendToSrc(1, triplet.attr.winnerStats.score), 
    // mergeMsg
    (x, y) =&gt; (x._1 + y._1, x._2+ y._2)
)

// Average field goals made per Game by winning teams 
var winnersPPG: VertexRDD[Double] = 
            winnerTotalPoints mapValues (
                (id: VertexId, x: (Int, Int)) =&gt; x match {
                    case (count: Int, total: Int) =&gt; total.toDouble/count
                })</pre></div><p>Let's check the output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; winnersPPG.take(5).foreach(println)</strong></span>
<span class="strong"><strong>(1260,71.19402985074628)</strong></span>
<span class="strong"><strong>(1410,71.11842105263158)</strong></span>
<span class="strong"><strong>(1426,76.30281690140845)</strong></span>
<span class="strong"><strong>(1166,76.89449541284404)</strong></span>
<span class="strong"><strong>(1434,74.28571428571429)</strong></span>
</pre></div><p>Now, the coach wants us to list the top five teams with the highest average three-pointer made per winning game. By the way, he also wants to know which teams are the most efficient in three-pointers.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec50"></a>Keeping things DRY</h3></div></div></div><p>We can copy and <a id="id241" class="indexterm"></a>modify the previous code but that would be repetitive. Instead, let's abstract out the average aggregation operator so that it can work on any statistics that the coach needs. Luckily, Scala's higher-order functions are there to help in this task.</p><p>For each statistic that our coach wants, let's define a function that takes a team's <code class="literal">GameStats</code> as input and returns the statistic that we are interested in. For now, we will need the number of three-pointers made and the average three-pointer percentage:</p><div class="informalexample"><pre class="programlisting">        // Getting individual stats
        def threePointMade(stats: GameStats) = stats.threePointerMade
        def threePointPercent(stats: GameStats) = stats.tpPercent</pre></div><p>Then, we create a generic function that takes as inputs a stats graph and one of the functions defined previously, which has a signature <code class="literal">GameStats =&gt; Double</code>:</p><div class="informalexample"><pre class="programlisting">// Generic function for stats averaging
def averageWinnerStat(graph: Graph[String, FullResult])(getStat: GameStats =&gt; Double): VertexRDD[Double] = {
    type Msg = (Int, Double)
    val winningScore: VertexRDD[Msg] = graph.aggregateMessages[Msg](
        // sendMsg
        triplet =&gt; triplet.sendToSrc(1, getStat(triplet.attr.winnerStats)), 
        // mergeMsg
        (x, y) =&gt; (x._1 + y._1, x._2+ y._2)
    )
    winningScore mapValues (
        (id: VertexId, x: Msg) =&gt; x match {
            case (count: Int, total: Double) =&gt; total/count
        })
}</pre></div><p>Then, we can use the <a id="id242" class="indexterm"></a>average stats by passing the functions <code class="literal">threePointMade</code> and <code class="literal">threePointPercent</code> to <code class="literal">averageWinnerStat</code>:</p><div class="informalexample"><pre class="programlisting">val winnersThreePointMade = 
averageWinnerStat(scoreGraph)(threePointMade) 
val winnersThreePointPercent = 
averageWinnerStat(scoreGraph)(threePointPercent) </pre></div><p>With little effort, we can tell the coach which five winning teams scored the highest number of threes per game:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; winnersThreePointMade.sortBy(_._2,false).take(5).foreach(println)</strong></span>
<span class="strong"><strong>(1440,11.274336283185841)</strong></span>
<span class="strong"><strong>(1125,9.521929824561404)</strong></span>
<span class="strong"><strong>(1407,9.008849557522124)</strong></span>
<span class="strong"><strong>(1172,8.967441860465117)</strong></span>
<span class="strong"><strong>(1248,8.915384615384616)</strong></span>
</pre></div><p>While we are at it, let's find out the five most efficient teams in three-pointers:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; winnersThreePointPercent.sortBy(_._2,false).take(5).foreach(println)</strong></span>
<span class="strong"><strong>(1101,46.90555728464225)</strong></span>
<span class="strong"><strong>(1147,44.224282479431224)</strong></span>
<span class="strong"><strong>(1294,43.754532434101534)</strong></span>
<span class="strong"><strong>(1339,43.52308905887638)</strong></span>
<span class="strong"><strong>(1176,43.080814169045105)</strong></span>
</pre></div><p>Interestingly, the<a id="id243" class="indexterm"></a> teams that made the most three-pointers per winning game are not always the ones who are the most efficient at it. But, they still won those games, which is more important.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec51"></a>Coach wants more numbers</h3></div></div></div><p>The coach seems <a id="id244" class="indexterm"></a>unsatisfied with that argument and wants us to get the same statistics but wants us to average them over all the games that each team has played.</p><p>Thus, we have to aggregate the information from all the nodes of our graph, and not only at the destination nodes. To make our previous abstraction more flexible, let's create the following types:</p><div class="informalexample"><pre class="programlisting">trait Teams
case class Winners extends Teams 
case class Losers extends Teams
case class AllTeams extends Teams</pre></div><p>We modify the previous higher-order function to have an extra argument <code class="literal">Teams</code>, which will help us specify at which nodes we want to collect and aggregate the required game stats. The new function becomes:</p><div class="informalexample"><pre class="programlisting">def averageStat(graph: Graph[String, FullResult])(getStat: GameStats =&gt; Double, tms: Teams): VertexRDD[Double] = {
    type Msg = (Int, Double)
    val aggrStats: VertexRDD[Msg] = graph.aggregateMessages[Msg](
        // sendMsg
        tms match {
            case _ : Winners =&gt; t =&gt; t.sendToSrc((1, getStat(t.attr.winnerStats)))
            case _ : Losers  =&gt; t =&gt; t.sendToDst((1, getStat(t.attr.loserStats)))
            case _       =&gt; t =&gt; {
                t.sendToSrc((1, getStat(t.attr.winnerStats)))
                t.sendToDst((1, getStat(t.attr.loserStats)))
            }
        }
        , 
        // mergeMsg
        (x, y) =&gt; (x._1 + y._1, x._2+ y._2)
    )

    aggrStats mapValues (
        (id: VertexId, x: Msg) =&gt; x match {
            case (count: Int, total: Double) =&gt; total/count
            })
    }</pre></div><p>Compared to <code class="literal">averageWinnerStat</code>, <code class="literal">aggregateStat</code> allows us to choose whether we want to aggregate<a id="id245" class="indexterm"></a> the stats for winners only, for losers only, or for all teams. Since the coach wants the overall stats averaged over all games played, we aggregate the stats by passing the <code class="literal">AllTeams()</code> flag in <code class="literal">aggregateStat</code>. In this case, we simply define the <code class="literal">sendMsg</code> argument in <code class="literal">aggregateMessages</code> so that the required stats are sent to both the source (the winner) and to the destination (the loser) using the <code class="literal">EdgeContext</code> class's <code class="literal">sendToSrc</code> and <code class="literal">sendToDst</code> functions respectively. This mechanism is pretty straightforward. We just need to make sure we send the right information to the right node. In this case, we send <code class="literal">winnerStats</code> to the winner and <code class="literal">loserStats</code> to the loser.</p><p>Ok, you've got the idea now. So, let's apply it to please our coach. Here are the teams with the overall highest three-pointers per page:</p><div class="informalexample"><pre class="programlisting">// Average Three Point Made Per Game for All Teams 
val allThreePointMade = averageStat(scoreGraph)(threePointMade, AllTeams())   </pre></div><p>Let's see the output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; allThreePointMade.sortBy(_._2, false).take(5).foreach(println)</strong></span>
<span class="strong"><strong>(1440,10.180811808118081)</strong></span>
<span class="strong"><strong>(1125,9.098412698412698)</strong></span>
<span class="strong"><strong>(1172,8.575657894736842)</strong></span>
<span class="strong"><strong>(1184,8.428571428571429)</strong></span>
<span class="strong"><strong>(1407,8.411149825783973) </strong></span>
</pre></div><p>Here are the five most efficient teams overall in three-pointers per game:</p><div class="informalexample"><pre class="programlisting">// Average Three Point Percent for All Teams
val allThreePointPercent = averageStat(scoreGraph)(threePointPercent, AllTeams())</pre></div><p>The output is:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; allThreePointPercent.sortBy(_._2,false).take(5).foreach(println)</strong></span>
<span class="strong"><strong>(1429,38.8351815824302)</strong></span>
<span class="strong"><strong>(1323,38.522819895594)</strong></span>
<span class="strong"><strong>(1181,38.43052051444854)</strong></span>
<span class="strong"><strong>(1294,38.41227053353959)</strong></span>
<span class="strong"><strong>(1101,38.097896464168954)</strong></span>
</pre></div><p>Actually, there is <a id="id246" class="indexterm"></a>only a 2 percent difference between the most efficient team and the one in the fiftieth position. Most NCAA teams are therefore pretty efficient behind the line. I bet the coach knew that already!</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec52"></a>Calculating average points per game</h3></div></div></div><p>We can also<a id="id247" class="indexterm"></a> reuse the <code class="literal">averageStat</code> function to get the average points per game for the winners. In particular, let's take a look at the two teams that won games with the highest and lowest scores:</p><div class="informalexample"><pre class="programlisting">// Winning teams
val winnerAvgPPG = averageStat(scoreGraph)(score, Winners())</pre></div><p>Let's check the output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; winnerAvgPPG.max()(Ordering.by(_._2))</strong></span>
<span class="strong"><strong>res36: (org.apache.spark.graphx.VertexId, Double) = (1322,90.73333333333333)</strong></span>

<span class="strong"><strong>scala&gt; winnerAvgPPG.min()(Ordering.by(_._2))</strong></span>
<span class="strong"><strong>res39: (org.apache.spark.graphx.VertexId, Double) = (1197,60.5)</strong></span>
</pre></div><p>Apparently, the most defensive team can win games by scoring only 60 points, whereas the most offensive team can score an average of 90 points.</p><p>Next, let's average the points per game for all games played and look at the two teams with the best and worst offense during the 2015 season:</p><div class="informalexample"><pre class="programlisting">// Average Points Per Game of All Teams
val allAvgPPG = averageStat(scoreGraph)(score, AllTeams())</pre></div><p>The output is:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; allAvgPPG.max()(Ordering.by(_._2))</strong></span>
<span class="strong"><strong>res42: (org.apache.spark.graphx.VertexId, Double) = (1322,83.81481481481481)</strong></span>

<span class="strong"><strong>scala&gt; allAvgPPG.min()(Ordering.by(_._2))</strong></span>
<span class="strong"><strong>res43: (org.apache.spark.graphx.VertexId, Double) = (1212,51.111111111111114)</strong></span>
</pre></div><p>To no surprise, the <a id="id248" class="indexterm"></a>best offensive team is the same as the one who scored most in winning games. To win a game, 50 points is not enough of an average for a team.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec53"></a>Defense stats â€“ D matters as in direction</h3></div></div></div><p>Previously, we obtained <a id="id249" class="indexterm"></a>some statistics such as field goals or the three-point percentages that a team achieves. What if instead we want to aggregate the average points or rebounds that each team concedes to their opponents? To compute that, we define a new higher-order function <code class="literal">averageConcededStat</code>. Compared to <code class="literal">averageStat</code>, this function needs to send <code class="literal">loserStats</code> to the winning team and <code class="literal">winnerStats</code> to the losing team. To make things more interesting, we are going to make the team name part of the message <code class="literal">Msg</code>:</p><div class="informalexample"><pre class="programlisting">def averageConcededStat(graph: Graph[String, FullResult])(getStat: GameStats =&gt; Double, rxs: Teams): VertexRDD[(String, Double)] = {
    type Msg = (Int, Double, String)
    val aggrStats: VertexRDD[Msg] = graph.aggregateMessages[Msg](
        // sendMsg
        rxs match {
            case _ : Winners =&gt; t =&gt; t.sendToSrc((1, getStat(t.attr.loserStats), t.srcAttr))
            case _ : Losers  =&gt; t =&gt; t.sendToDst((1, getStat(t.attr.winnerStats), t.dstAttr))
            case _       =&gt; t =&gt; {
                t.sendToSrc((1, getStat(t.attr.loserStats),t.srcAttr))
                t.sendToDst((1, getStat(t.attr.winnerStats),t.dstAttr))
            }
        }
        , 
        // mergeMsg
        (x, y) =&gt; (x._1 + y._1, x._2+ y._2, x._3)
    )

    aggrStats mapValues (
        (id: VertexId, x: Msg) =&gt; x match {
            case (count: Int, total: Double, name: String) =&gt; (name, total/count)
        })
}</pre></div><p>With that, we can calculate<a id="id250" class="indexterm"></a> the average points conceded by the winning and losing teams as follows:</p><div class="informalexample"><pre class="programlisting">val winnersAvgConcededPoints = averageConcededStat(scoreGraph)(score, Winners())
val losersAvgConcededPoints = averageConcededStat(scoreGraph)(score, Losers())</pre></div><p>Let's check the output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; losersAvgConcededPoints.min()(Ordering.by(_._2))</strong></span>
<span class="strong"><strong>res: (VertexId, (String, Double)) = (1101,(Abilene Chr,74.04761904761905))</strong></span>

<span class="strong"><strong>scala&gt; winnersAvgConcededPoints.min()(Ordering.by(_._2))</strong></span>
<span class="strong"><strong>res: (org.apache.spark.graphx.VertexId, (String, Double)) = (1101,(Abilene Chr,74.04761904761905))</strong></span>

<span class="strong"><strong>scala&gt; losersAvgConcededPoints.max()(Ordering.by(_._2))</strong></span>
<span class="strong"><strong>res: (VertexId, (String, Double)) = (1464,(Youngstown St,78.85714285714286))</strong></span>

<span class="strong"><strong>scala&gt; winnersAvgConcededPoints.max()(Ordering.by(_._2))</strong></span>
<span class="strong"><strong>res: (VertexId, (String, Double)) = (1464,(Youngstown St,71.125))</strong></span>
</pre></div><p>The previous code tells us that Abilene Christian University is the most defensive team. They concede the least points whether they win a game or not. On the other hand, Youngstown has the worst defense.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec31"></a>Joining average stats into a graph</h2></div></div><hr /></div><p>The previous<a id="id251" class="indexterm"></a> example shows us how flexible the <code class="literal">aggregateMessages</code> <a id="id252" class="indexterm"></a>operator is. We can define the type <code class="literal">Msg</code> of the messages to be aggregated to fit our needs. Moreover, we can select which nodes receive the messages. Finally, we can also define how we want to merge the messages.</p><p>As a final example, let's aggregate many statistics about each team and join this information into the nodes of the graph:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>To start, we create its own class for the team stats:</p><div class="informalexample"><pre class="programlisting">// Average Stats of All Teams 
case class TeamStat(
        wins: Int  = 0      // Number of wins
     ,losses: Int  = 0      // Number of losses
        ,ppg: Int  = 0      // Points per game
        ,pcg: Int  = 0      // Points conceded per game
        ,fgp: Double  = 0   // Field goal percentage
        ,tpp: Double  = 0   // Three point percentage
        ,ftp: Double  = 0   // Free Throw percentage
     ){
    override def toString = wins + "-" + losses
}</pre></div></li><li><p>We collect the<a id="id253" class="indexterm"></a> average stats for all teams using <code class="literal">aggregateMessages</code>. For that, we define the type of the message to be <a id="id254" class="indexterm"></a>an 8-element tuple that holds the counter for games played, won, lost, and other statistics that will be stored in <code class="literal">TeamStat</code>, as listed previously:</p><div class="informalexample"><pre class="programlisting">type Msg = (Int, Int, Int, Int, Int, Double, Double, Double)

val aggrStats: VertexRDD[Msg] = scoreGraph.aggregateMessages(
  // sendMsg
  t =&gt; {
          t.sendToSrc((   1,
                          1, 0, 
                          t.attr.winnerStats.score, 
                          t.attr.loserStats.score,
                          t.attr.winnerStats.fgPercent,
                          t.attr.winnerStats.tpPercent,
                          t.attr.winnerStats.ftPercent
                     ))
          t.sendToDst((   1,
                          0, 1, 
                          t.attr.loserStats.score, 
                          t.attr.winnerStats.score,
                          t.attr.loserStats.fgPercent,
                          t.attr.loserStats.tpPercent,
                          t.attr.loserStats.ftPercent
                      ))
       }
  , 
  // mergeMsg
  (x, y) =&gt; ( x._1 + y._1, x._2 + y._2, 
              x._3 + y._3, x._4 + y._4,
              x._5 + y._5, x._6 + y._6,
              x._7 + y._7, x._8 + y._8
          )
)</pre></div></li><li><p>Given the <a id="id255" class="indexterm"></a>aggregate message <code class="literal">aggrStats</code>, we map<a id="id256" class="indexterm"></a> them into a collection of <code class="literal">TeamStats</code>:</p><div class="informalexample"><pre class="programlisting">val teamStats: VertexRDD[TeamStat] = aggrStats mapValues {
  (id: VertexId, m: Msg) =&gt; m match {
      case ( count: Int, 
              wins: Int, 
              losses: Int,
              totPts: Int, 
              totConcPts: Int, 
              totFG: Double,
              totTP: Double, 
              totFT: Double)  =&gt; TeamStat( wins, losses,
                                          totPts/count,
                                          totConcPts/count,
                                          totFG/count,
                                          totTP/count,
                                          totFT/count)
    }
}</pre></div></li><li><p>Let's join <code class="literal">teamStats</code> into the graph. For that, we first create a class <code class="literal">Team</code> as a new type for the vertex attribute. <code class="literal">Team</code> will have the name and the <code class="literal">TeamStat</code> option:</p><div class="informalexample"><pre class="programlisting">case class Team(name: String, stats: Option[TeamStat]) {
    override def toString = name + ": " + stats
}</pre></div></li><li><p>We use the <code class="literal">joinVertices</code> operator, which we have seen in the previous chapter:</p><div class="informalexample"><pre class="programlisting">// Joining the average stats to vertex attributes
def addTeamStat(id: VertexId, t: Team, stats: TeamStat) = Team(t.name, Some(stats))

val statsGraph: Graph[Team, FullResult] = 
    scoreGraph.mapVertices((_, name) =&gt; Team(name, None)).
               joinVertices(teamStats)(addTeamStat)</pre></div></li><li><p>We can see that <a id="id257" class="indexterm"></a>the join has worked well by printing the first<a id="id258" class="indexterm"></a> three vertices in the new graph <code class="literal">statsGraph</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; statsGraph.vertices.take(3).foreach(println)</strong></span>
<span class="strong"><strong>(1260,Loyola-Chicago: Some(17-13))</strong></span>
<span class="strong"><strong>(1410,TX Pan American: Some(7-21))</strong></span>
<span class="strong"><strong>(1426,UT Arlington: Some(15-15))</strong></span>
</pre></div></li><li><p>To conclude this task, let's find out the top 10 teams in the regular seasons. To do so, we define an <code class="literal">Ordering</code> option for <code class="literal">Option[TeamStat]</code> as follows:</p><div class="informalexample"><pre class="programlisting">import scala.math.Ordering 
object winsOrdering extends Ordering[Option[TeamStat]] {
    def compare(x: Option[TeamStat], y: Option[TeamStat]) = (x, y) match {
        case (None, None)       =&gt; 0 
        case (Some(a), None)    =&gt; 1
        case (None, Some(b))    =&gt; -1
        case (Some(a), Some(b)) =&gt; if (a.wins == b.wins) a.losses compare b.losses
        else a.wins compare b.wins
    }
}</pre></div></li><li><p>Finally:</p><div class="informalexample"><pre class="programlisting">import scala.reflect.classTag
import scala.reflect.ClassTag
scala&gt; statsGraph.vertices.sortBy(v =&gt; v._2.stats,false)(winsOrdering, classTag[Option[TeamStat]]).
     |                             take(10).foreach(println)
(1246,Kentucky: Some(34-0))
(1437,Villanova: Some(32-2))
(1112,Arizona: Some(31-3))
(1458,Wisconsin: Some(31-3))
(1211,Gonzaga: Some(31-2))
(1320,Northern Iowa: Some(30-3))
(1323,Notre Dame: Some(29-5))
(1181,Duke: Some(29-4))
(1438,Virginia: Some(29-3))
(1268,Maryland: Some(27-6))</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note26"></a>Note</h3><p>Note that the <code class="literal">ClassTag</code> parameter<a id="id259" class="indexterm"></a> is required in <code class="literal">sortBy</code> to make <a id="id260" class="indexterm"></a>use of Scala's reflection. That is why we had the previous imports.</p></div></li></ol></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec32"></a>Performance optimization</h2></div></div><hr /></div><p>In addition<a id="id261" class="indexterm"></a> to the <code class="literal">sendMsg</code> and <code class="literal">mergeMsg</code> methods, <code class="literal">aggregateMessages</code> can also take an optional argument <code class="literal">TripletFields</code>, which indicates what data is accessed in <code class="literal">EdgeContext</code>. The main reason for explicitly specifying such information is to help optimize the performance of the <code class="literal">aggregateMessages</code> operation.</p><p>In fact, <code class="literal">TripletFields</code> represents a subset of the fields of <code class="literal">_EdgeTriplet_</code> and it enables GraphX to populate only those fields that are necessary.</p><p>The default value is <code class="literal">TripletFields.All</code>, which means that the <code class="literal">sendMsg</code> function may access any of the fields in the <code class="literal">EdgeContext</code> class. Otherwise, the <code class="literal">TripletFields</code> argument is used to tell GraphX that only part of <code class="literal">EdgeContext</code> will be required so that an efficient join strategy can be used. All possible options for the <code class="literal">TripletFields</code> are listed as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><code class="literal">TripletFields.All</code>: This option exposes all the fields (source, edge, and destination)</p></li><li style="list-style-type: disc"><p><code class="literal">TripletFields.Dst</code>: This one exposes the destination and edge fields but not the source field</p></li><li style="list-style-type: disc"><p><code class="literal">TripletFields.EdgeOnly</code>: This option exposes only the edge field but not the source or destination field</p></li><li style="list-style-type: disc"><p><code class="literal">TripletFields.None</code>: With this option none of the triplet fields are exposed</p></li><li style="list-style-type: disc"><p><code class="literal">TripletFields.Src</code>: This one exposes the source and edge fields but not the destination field</p></li></ul></div><p>Using our previous example, if we are interested in computing the total number of wins and losses for each team, we will not need to access any fields of the <code class="literal">EdgeContext</code> class. In this case, we should use <code class="literal">TripletFields.None</code> to indicate so:</p><div class="informalexample"><pre class="programlisting">// Number of wins of the teams
val numWins: VertexRDD[Int] = scoreGraph.aggregateMessages(
    triplet =&gt; {
        triplet.sendToSrc(1)    // No attribute is passed but an integer
    },
    (x, y) =&gt; x + y,
    TripletFields.None
)

// Number of losses of the teams
val numLosses: VertexRDD[Int] = scoreGraph.aggregateMessages(
    triplet =&gt; {
        triplet.sendToDst(1)    // No attribute is passed but an integer
    },
    (x, y) =&gt; x + y,
    TripletFields.None
)</pre></div><p>To see that this works, let's print the top five and bottom five teams:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; numWins.sortBy(_._2,false).take(5).foreach(println)</strong></span>
<span class="strong"><strong>(1246,34)</strong></span>
<span class="strong"><strong>(1437,32)</strong></span>
<span class="strong"><strong>(1112,31)</strong></span>
<span class="strong"><strong>(1458,31)</strong></span>
<span class="strong"><strong>(1211,31)</strong></span>

<span class="strong"><strong>scala&gt; numLosses.sortBy(_._2, false).take(5).foreach(println)</strong></span>
<span class="strong"><strong>(1363,28)</strong></span>
<span class="strong"><strong>(1146,27)</strong></span>
<span class="strong"><strong>(1212,27)</strong></span>
<span class="strong"><strong>(1197,27)</strong></span>
<span class="strong"><strong>(1263,27)</strong></span>
</pre></div><p>Should you<a id="id262" class="indexterm"></a> want the name of the top five teams, you need to access the <code class="literal">srcAttr</code> attribute. In this case, we need to set <code class="literal">tripletFields</code> to <code class="literal">TripletFields.Src</code>.</p><p>Kentucky as the undefeated team in the regular season:</p><div class="informalexample"><pre class="programlisting">val numWinsOfTeams: VertexRDD[(String, Int)] = scoreGraph.aggregateMessages(
    t =&gt; {
        t.sendToSrc(t.srcAttr, 1)         // Pass source attribute only
    },
    (x, y) =&gt; (x._1, x._2 + y._2),
    TripletFields.Src
)</pre></div><p>Et voila!:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; numWinsOfTeams.sortBy(_._2._2, false).take(5).foreach(println)</strong></span>
<span class="strong"><strong>(1246,(Kentucky,34))</strong></span>
<span class="strong"><strong>(1437,(Villanova,32))</strong></span>
<span class="strong"><strong>(1112,(Arizona,31))</strong></span>
<span class="strong"><strong>(1458,(Wisconsin,31))</strong></span>
<span class="strong"><strong>(1211,(Gonzaga,31))</strong></span>

<span class="strong"><strong>scala&gt; numWinsOfTeams.sortBy(_._2._2).take(5).foreach(println)</strong></span>
<span class="strong"><strong>(1146,(Cent Arkansas,2))</strong></span>
<span class="strong"><strong>(1197,(Florida A&amp;M,2))</strong></span>
<span class="strong"><strong>(1398,(Tennessee St,3))</strong></span>
<span class="strong"><strong>(1263,(Maine,3))</strong></span>
<span class="strong"><strong>(1420,(UMBC,4))</strong></span>
</pre></div><p>Kentucky has <a id="id263" class="indexterm"></a>not lost any of its 34 games during the regular season. Too bad that they could not make it into the championship final.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec33"></a>The MapReduceTriplets operator</h2></div></div><hr /></div><p>Prior to Spark 1.2, there was<a id="id264" class="indexterm"></a> no <code class="literal">aggregateMessages</code> method in Graph. Instead, the now deprecated <code class="literal">mapReduceTriplets</code> was the primary aggregation operator. The API for <code class="literal">mapReduceTriplets</code> is:</p><div class="informalexample"><pre class="programlisting">class Graph[VD, ED] {
  def mapReduceTriplets[Msg](
      map: EdgeTriplet[VD, ED] =&gt; Iterator[(VertexId, Msg)],
      reduce: (Msg, Msg) =&gt; Msg)
    : VertexRDD[Msg]
}</pre></div><p>Compared to <code class="literal">mapReduceTriplets</code>, the new operator <code class="literal">aggregateMessages</code> is more expressive as it employs the message passing mechanism instead of returning an iterator of messages as <code class="literal">mapReduceTriplets</code> does. In addition, <code class="literal">aggregateMessages</code> explicitly requires the user to specify the <code class="literal">TripletFields</code> object for performance improvement as we explained previously. In addition to API improvements, <code class="literal">aggregateMessages</code> is optimized for performance.</p><p>Since <code class="literal">mapReduceTriplets</code> is now deprecated, we will not discuss it further. If you have to use it with earlier versions of Spark, you can refer to the Spark programming guide.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec34"></a>Summary</h2></div></div><hr /></div><p><code class="literal">AggregateMessages</code> provides a functional abstraction for aggregating neighborhood information in Spark graphs. This operator applies a user-defined <code class="literal">sendMsg</code> function to each edge in the graph using <code class="literal">EdgeContext</code>. Each <code class="literal">EdgeContext</code> class accesses the required information about the edge and passes that information to its source node and/or destination node using the <code class="literal">sendToSrc</code> and/or <code class="literal">sendToDst</code> methods respectively. After all messages have been received by the nodes, the <code class="literal">mergeMsg</code> function is used to aggregate those messages at each node.</p><p>In the next chapter, we will introduce another operator called <code class="literal">Pregel</code>, which will be useful for creating custom iterative graph-processing algorithms.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch06"></a>ChapterÂ 6.Â Iterative Graph-Parallel Processing with Pregel</h2></div></div></div><p>Graphs<a id="id265" class="indexterm"></a> are a very useful abstraction for solving many practical computing problems. For example, we can search through nearly five billion web pages today, thanks to the PageRank graph algorithm. Apart from the web search, there are other applications, such as social media, for which iterative graph processing is needed. In this chapter, we will learn how to use <a id="id266" class="indexterm"></a>
<span class="strong"><strong>Pregel</strong></span>, a computational model, which is suitable for this task. Pregel was initially proposed by Google and has also been adopted by Spark as a generic programming interface for iterative graph computations. In this chapter, you will understand the Pregel model of computation. In addition, our learning goal is to clarify both the interface and implementation of the Pregel operator in Spark. After working through the concrete examples, you will be able to formulate your own algorithms with the Pregel interface.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec35"></a>The Pregel computational model</h2></div></div><hr /></div><p>A <a id="id267" class="indexterm"></a>Pregel program is a sequence of iterations called <a id="id268" class="indexterm"></a>
<span class="strong"><strong>supersteps</strong></span>, in each of which a vertex can receive inbound messages that are sent by its neighbors in the previous iteration, and modify its attribute and its edges. In addition, each vertex also sends messages to its neighbors by the end of each superstep. By thinking as a vertex, this abstraction makes it simple to reason about parallel graph processing. All we need to think about is the type of message that each vertex should be receiving, the processing that it should do on its inbound messages, and the message that its neighbors need for the next superstep. Luckily, this message-passing approach is flexible enough to express a large class of graph algorithms. More importantly, a graph algorithm can make use of Spark's scalable architecture to process the messages in bulk and in a synchronous manner. This synchronous model of computation makes it easy to express most graph-parallel algorithms.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec54"></a>Example â€“ iterating towards the social equality</h3></div></div></div><p>Before presenting the <a id="id269" class="indexterm"></a>Pregel API, let's illustrate these concepts with a hypothetical example of a social network, in which each person is extremely altruistic. We will assume that everyone knows how much money their friends have in the banks. However, they need an algorithm that will attempt to equalize their wealth. This is just an example (luckily or sadly, depending on your philosophy), but it will help clarify how Pregel works. In essence, each person will compare their money with their friends, and will send some of it to those who have less.</p><p>By using Pregel, they will equalize their wealth by sending money to each other through a sequence of iterations. In this case, we can use <code class="literal">Double</code> as the message type for our algorithm. In the beginning of each iteration, each person will first receive a sum of money that was donated by their friends in the previous iteration. Based on their knowledge of how much their friends now own, they will compare their new wealth against their friends' situations. This means they need to find out who earn less, and then calculate how much they should send to those friends. At the same time, they also decide how much to keep in their account. As we described it, each Pregel iteration consists of three consecutive tasks, and this is why it is <a id="id270" class="indexterm"></a>called a <span class="emphasis"><em>superstep</em></span>.</p><p>Hence, they first need a function called <code class="literal">mergeMsg</code> to combine the inbound money transfers that they may receive from their well off friends:</p><div class="informalexample"><pre class="programlisting">def mergeMsg(fromA: Double, fromB: Double): Double = fromA + fromB </pre></div><p>Second, they will also need a function, called <a id="id271" class="indexterm"></a>
<span class="strong"><strong>vertex program</strong></span>, to calculate how much money they have after receiving money in the previous superset:</p><div class="informalexample"><pre class="programlisting">def vprog(id: VertexId, balance: Double, credit: Double) = balance + credit</pre></div><p>Finally, a function called <code class="literal">sendMsg</code> is also needed for sending money between friends:</p><div class="informalexample"><pre class="programlisting">def sendMsg(triplet: EdgeTriplet[VD, ED]): Iterator[(VertexId, A)]</pre></div><p>As seen from the previous function signature, <code class="literal">sendMsg</code> takes an edge triplet as an input instead of a vertex, so that we have access to both the source and destination nodes. We figure out the correct implementation of <code class="literal">sendMsg</code> in the next section.</p><p>Let's further simplify our example by considering a triangle network between three friends:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val nodes: RDD[(Long,Double)] = sc.parallelize(List((1,10.0),(2,3.0),(3,5.0)))</strong></span>
<span class="strong"><strong>nodes: RDD[(Long, Double)]</strong></span>
<span class="strong"><strong>scala&gt; val edges = sc.parallelize(List(Edge(1,2,1),Edge(2,1,1),Edge(1,3,1),Edge(3,1,1),Edge(2,3,1),Edge(3,2,1)))</strong></span>
<span class="strong"><strong>edges: Edge[Int]]</strong></span>
<span class="strong"><strong>scala&gt; val graph = Graph(nodes, edges)</strong></span>
<span class="strong"><strong>graph: Graph[Double,Int]</strong></span>

<span class="strong"><strong>scala&gt; graph.vertices.foreach(println)</strong></span>
<span class="strong"><strong>(1,10.0)</strong></span>
<span class="strong"><strong>(2,3.0)</strong></span>
<span class="strong"><strong>(3,5.0)</strong></span>
</pre></div><p>For simplicity, assume <a id="id272" class="indexterm"></a>that each person will distribute five percent of her wealth to each of its poor friends. She will not need to worry if a friend receives too much since selfishness and greed are out of the equation here. So, here is our first attempt at implementing the <code class="literal">sendMsg</code> function:</p><div class="informalexample"><pre class="programlisting">def sendMsg(t: EdgeTriplet[Double, Double]) = 
    if (t.srcAttr &lt;= t.dstAttr) Iterator.empty 
    else Iterator((t.dstId,t.srcAttr * 0.05))</pre></div><p>This seems reasonable. If a person is better off than her friends, she will offer five percent of her money to that friend. Otherwise, she gives nothing. After ten iterations, our new graph thus becomes:</p><div class="informalexample"><pre class="programlisting">val newGraph = graph.pregel(0.0,10)(vprog, sendMsg, mergeMsg) </pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note27"></a>Note</h3><p>Notice that Pregel takes two argument lists (for example, <code class="literal">graph.pregel(list1)(list2)</code>). The first argument list includes an initial message to send to all vertices in the beginning of the algorithm as well as the maximum number of iterations. The second argument list contains the three user-defined functions for combining, receiving, and computing messages.</p></div><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; newGraph.vertices.foreach(println)</strong></span>
<span class="strong"><strong>(3,10.951096875000001)</strong></span>
<span class="strong"><strong>(2,10.246937500000001)</strong></span>
<span class="strong"><strong>(1,10.512346875)</strong></span>
</pre></div><p>Something is not right here. The group started with 18 dollars in total, and ended up with more than 30 dollars. This cannot be true! So, what did we do wrong? To uncover our mistake, let's see what happened after one iteration of Pregel:</p><div class="informalexample"><pre class="programlisting">val newGraph1 = graph.pregel(0.0,1)(vprog, sendMsg, mergeMsg) </pre></div><p>Let's see the output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; newGraph1.vertices.foreach(println)</strong></span>
<span class="strong"><strong>(1,10.0)</strong></span>
<span class="strong"><strong>(2,3.75)</strong></span>
<span class="strong"><strong>(3,5.5)</strong></span>
</pre></div><p>Again, their total <a id="id273" class="indexterm"></a>wealth exceeds 18 dollars after one iteration. This is because when a person sent an amount of money to their friend, that amount was not debited from that person's account. We can fix this by sending messages to the person that received the money as well as the one that sent it. So, if person <span class="emphasis"><em>A</em></span> sends <span class="emphasis"><em>X</em></span> dollars to person <span class="emphasis"><em>B</em></span>, we should send <span class="emphasis"><em>X</em></span> dollars to <span class="emphasis"><em>B</em></span>, and -<span class="emphasis"><em>X</em></span> dollars to <span class="emphasis"><em>A</em></span>:</p><div class="informalexample"><pre class="programlisting">def sendMsg(t: EdgeTriplet[Double, Double]) = 
    if (t.srcAttr &lt;= t.dstAttr) Iterator.empty 
    else  Iterator((t.dstId, t.srcAttr * 0.05),
                   (t.srcId, - t.srcAttr * 0.05))
                 
val afterOneIter = graph.pregel(0.0, 1)(vprog, sendMsg, mergeMsg)  </pre></div><p>Let's see the output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; afterOneIter.vertices.foreach(println)</strong></span>
<span class="strong"><strong>(1,9.0)</strong></span>
<span class="strong"><strong>(2,3.75)</strong></span>
<span class="strong"><strong>(3,5.25)</strong></span>
</pre></div><p>You can verify that things now work as expected. So, what if we increase the maximum number of iterations? Let's see what happens then:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; afterTenIters.vertices.foreach(println)</strong></span>
<span class="strong"><strong>(1,5.999611965064453)</strong></span>
<span class="strong"><strong>(2,6.37018749852539)</strong></span>
<span class="strong"><strong>(3,5.630200536410156)</strong></span>

<span class="strong"><strong>scala&gt; afterHundredIters.vertices.foreach(println)</strong></span>
<span class="strong"><strong>(1,6.206716647163644)</strong></span>
<span class="strong"><strong>(2,6.207038273723298)</strong></span>
<span class="strong"><strong>(3,5.586245079113054)</strong></span>
</pre></div><p>Even with 100 iterations, we can see that the account balances do not converge to the idealistic value of 6 dollars, but fluctuate around it. This is expected in our simplistic example.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec36"></a>The Pregel API in GraphX</h2></div></div><hr /></div><p>Now, let's formalize the<a id="id274" class="indexterm"></a> programming interface for the Pregel operator. Here is its definition:</p><div class="informalexample"><pre class="programlisting">class GraphOps[VD, ED] {
  def pregel[A]
      (initialMsg: A,
       maxIter: Int = Int.MaxValue,
       activeDir: EdgeDirection = EdgeDirection.Out)
      (vprog: (VertexId, VD, A) =&gt; VD,
       sendMsg: EdgeTriplet[VD, ED] =&gt; Iterator[(VertexId, A)],
       mergeMsg: (A, A) =&gt; A)
    : Graph[VD, ED]
}</pre></div><p>The <code class="literal">pregel</code> method is<a id="id275" class="indexterm"></a> invoked on a property graph, and returns a new graph with the same type and structure. While the edges remain intact, the attributes of the vertices may change from one superset to the next one. Pregel takes the following two lists of arguments. The first list contains:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>An initial message with a user-defined type <span class="emphasis"><em>A</em></span>â€”this message is received by each vertex when the algorithm starts</p></li><li style="list-style-type: disc"><p>A maximum number of iterations</p></li><li style="list-style-type: disc"><p>The edge direction along which to send messages</p></li></ul></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note28"></a>Note</h3><p>A Pregel algorithm terminates when either there are no more messages to be sent, or when a specified maximum number of iterations is reached. When implementing an algorithm, it is important to always limit the number of iterations, especially when the algorithm is not guaranteed to converge.</p><p>If no active edge direction is specified, Pregel assumes that messages are only sent for the outgoing edges of each vertex. Moreover, if a vertex did not receive a message in the previous superset, no message will be sent along its outgoing edge, at the end of the current superset.</p></div><p>In addition, the second list of arguments must include the three functions:</p><div class="informalexample"><pre class="programlisting">vprog: (VertexId, VD, A) =&gt; VD: this vertex program updates the attributes of all vertices who received messages from the previous iteration
mergeMsg: (A, A) =&gt; A): this function merges the messages to be received by each vertex.
sendMsg: EdgeTriplet[VD, ED] =&gt; Iterator[(VertexId, A)]: this function takes an edge triplet and creates the messages to be sent to the source node and/or destination node.</pre></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec37"></a>Community detection through label propagation</h2></div></div><hr /></div><p>In the following section, we are going to implement a community detection algorithm using the Pregel interface. <span class="strong"><strong>Label Propagation Algorithm</strong></span> (<span class="strong"><strong>LPA</strong></span>)<a id="id276" class="indexterm"></a> is a simple and fast method for detecting communities within graphs. By construction, the communities obtained by the label propagation process require each node to have at least as many neighbors within its community as it has with each of the other communities.</p><p>Let's quickly <a id="id277" class="indexterm"></a>describe how the LPA works. First, each node is initially given its vertex ID as its label. At the subsequent iterations, each node determines its community, based on the labels of its neighbors. Specifically, the node chooses to join the community to which the maximum number of its neighbors belong to. If there is a tie, one of the majority labels is picked randomly. As we propagate the labels in this way across the graph, most labels will disappear, whereas the remaining ones define the communities. Ideally, this iterative algorithm converges when no node in the network changes its label. As a result, nodes having the same labels are grouped together as one community.</p><p>By implementing this algorithm in Pregel, we want to obtain a graph in which the vertex attributes are the labels of the community affiliations. Hence, we'll first initialize the LPA graph by setting the label of each vertex to its identifier:</p><div class="informalexample"><pre class="programlisting">val lpaGraph = graph.mapVertices { case (vid, _) =&gt; vid }</pre></div><p>Next, we'll define the type of message to <code class="literal">Map[Label, Long]</code>, which associates a community label to the number of neighbors that have this label. The initial message that will be sent to each node is simply an empty map:</p><div class="informalexample"><pre class="programlisting">type Label = VertexId
val initialMessage = Map[Label, Long]()</pre></div><p>Following the Pregel programming model, we define a <code class="literal">sendMsg</code> function, which is used by each node to inform its neighbors of its current label. For each triplet, the source node will receive the destination node's label, and vice versa:</p><div class="informalexample"><pre class="programlisting">def sendMsg(e: EdgeTriplet[Label, ED]): Iterator[(VertexId, Map[Label, Long])] = 
    Iterator((e.srcId, Map(e.dstAttr -&gt; 1L)), (e.dstId, Map(e.srcAttr -&gt; 1L)))</pre></div><p>After receiving the messages from its neighbors, a node determines its community label as the one to which the majority of its neighbors currently belong to. Hence, each node will use the following vertex program function to do so:</p><div class="informalexample"><pre class="programlisting">def vprog(vid: VertexId, attr: Long, message: Map[Label, Long]): VertexId = if (message.isEmpty) attr else message.maxBy(_._2)._1</pre></div><p>The previous function returns, in each iteration, the label (that is, a <code class="literal">VertexId</code> attribute) of the community to which the majority of its neighbors currently belong to.</p><p>We also need a <code class="literal">mergeMsg</code> function to combine all the messages, received by a node from its neighbors into a single map. If both the messages contain the same label, we simply sum up the corresponding number of neighbors for this label:</p><div class="informalexample"><pre class="programlisting">def mergeMsg(count1: Map[Label, Long], count2: Map[Label, Long])
  : Map[VertexId, Long] = {
  (count1.keySet ++ count2.keySet).map { i =&gt;
    val count1Val = count1.getOrElse(i, 0L)
    val count2Val = count2.getOrElse(i, 0L)
    i -&gt; (count1Val + count2Val)
  }.toMap
}</pre></div><p>Finally, we can<a id="id278" class="indexterm"></a> run the LPA algorithm as we did for equalizing the social wealth by calling the <code class="literal">pregel</code> method on the graph:</p><div class="informalexample"><pre class="programlisting">lpaGraph.pregel(initialMessage, 50)(vprog, sendMsg, mergeMsg)</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note29"></a>Note</h3><p>The main benefits of LPA are its simplicity and time efficiency. In fact, the number of iterations to convergence has been observed to be independent of the graph size whereas each iteration has a linear time complexity. Despite its advantages, the label propagation algorithm may not necessarily converge and it may also result in uninteresting solutions, such as each node being identified as a single community. Actually, the algorithm may oscillate for graphs that are bipartite or have a nearly bipartite structure.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec38"></a>The Pregel implementation of PageRank</h2></div></div><hr /></div><p>We have already seen that <a id="id279" class="indexterm"></a>GraphX has a PageRank API. In the following, let us see how this famous web search algorithmic can be easily implemented using Pregel. Since we already explained in the previous chapter how PageRank works, we will now simply explain its Pregel implementation:</p><p>First of all, we need to initialize the ranking graph with each edge attribute set to 1, divided by the out-degree, and each vertex attribute to set 1.0:</p><div class="informalexample"><pre class="programlisting">val rankGraph: Graph[(Double, Double), Double] = 
    // Associate the degree with each vertex
    graph.outerJoinVertices(graph.outDegrees) {
        (vid, vdata, deg) =&gt; deg.getOrElse(0)
    }.mapTriplets( e =&gt; 1.0 / e.srcAttr )
     .mapVertices( (id, attr) =&gt; (0.0, 0.0) )</pre></div><p>Following the Pregel abstraction, we define the three functions that are needed to implement PageRank in GraphX. First, we define the vertex program as follows:</p><div class="informalexample"><pre class="programlisting">val resetProb = 0.15
def vProg(id: VertexId, attr: (Double, Double), msgSum: Double): (Double, Double) = {
  val (oldPR, lastDelta) = attr
  val newPR = oldPR + (1.0 - resetProb) * msgSum
  (newPR, newPR - oldPR)
}</pre></div><p>Next is the function that creates the messages:</p><div class="informalexample"><pre class="programlisting">val tol = 0.001
def sendMessage(edge: EdgeTriplet[(Double, Double), Double]) = {
  if (edge.srcAttr._2 &gt; tol) {
    Iterator((edge.dstId, edge.srcAttr._2 * edge.attr))
  } else {
    Iterator.empty
  }
}</pre></div><p>The third function<a id="id280" class="indexterm"></a> called <code class="literal">mergeMsg</code> simply adds the rank:</p><div class="informalexample"><pre class="programlisting">def mergeMsg(a: Double, b: Double): Double = a + b</pre></div><p>Then we will get the vertex ranking as follows:</p><div class="informalexample"><pre class="programlisting">rankGraph.pregel(initialMessage, activeDirection = EdgeDirection.Out)
                (vProg, sendMsg, mergeMsg)
                .mapVertices((vid, attr) =&gt; attr._1)</pre></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec39"></a>Summary</h2></div></div><hr /></div><p>In summary, Pregel is a generic and simplified interface for writing custom iterative, and parallel algorithms on large graphs. In this chapter, we have seen how to implement different iterative graph processing using this simple abstraction. In the next chapter, we will see how to use Spark's MLlib and GraphX to solve some machine learning problems with graph data.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch07"></a>ChapterÂ 7.Â Learning Graph Structures</h2></div></div></div><p>In this chapter, we will show you how to learn interesting structures from graphs in Spark. In principle, one learns and finds relationships from data by first selecting the problem of interest. The most common learning problems are regression, classification, ranking, and clustering. In this book, we will focus on clustering. In particular, we will focus on graph data, and apply clustering to detect communities within the graphs. Here is our roadmap for this chapter. First, we will introduce the concepts of spectral clustering. Then, we will study a specific method, which allows us to cluster graphs in Spark. Finally, we will apply these techniques to music and song playlist datasets. This application will also serve as an opportunity to review the tools and techniques that we covered in the previous chapters. We will bring them together in this chapter.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec40"></a>Community clustering in graphs</h2></div></div><hr /></div><p>Clustering <a id="id281" class="indexterm"></a>is a learning problem in which given entities, such as objects or people, are partitioned into subsets, according to a defined similarity measure. The entities within the same cluster are very similar, and are different from all entities in other clusters. Clustering <a id="id282" class="indexterm"></a>is done with an unsupervised method. In other words, it operates on unlabeled data, which are the attributes or features of the entities. Moreover, clustering methods can be broadly classified into parametric versus non parametric approaches. The parametric approaches impose a probability model on the data. Some examples of the parametric methods are <span class="strong"><strong>Gaussian Mixture Model</strong></span> (<span class="strong"><strong>GMM</strong></span>)<a id="id283" class="indexterm"></a> and <a id="id284" class="indexterm"></a>
<span class="strong"><strong>Latent Dirichlet Allocation</strong></span> (<span class="strong"><strong>LDA</strong></span>). On the other hand, the non parametric models infer the structure of the clusters from the data itself. Examples include k-means and spectral clustering. All these cited methods are available in Spark's MLlib library.</p><p>Before we continue, it is important to understand why clustering is related to graph processing. There are two reasons for this. The first reason is that clustering is very useful for detecting "communities" in graphs. These communities are essentially clusters of nodes that share similar features. While two nodes are not explicitly connected, clustering can reveal their similarities by learning from their attribute data. For instance, online social and dating websites use such information to suggest people you may know, or the partners you would be interested to meet. Conversely, clustering can be helpful to uncover interesting structures in highly connected networks. The second reason is that the clustering method that we will see here is based on graph processing. In particular, we will focus our attention on the <a id="id285" class="indexterm"></a>
<span class="strong"><strong>power iteration clustering</strong></span> (<span class="strong"><strong>PIC</strong></span>), which is a simple and fast spectral clustering method.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec55"></a>Spectral clustering</h3></div></div></div><p>As mentioned previously, the<a id="id286" class="indexterm"></a> aim of clustering is to divide the data points into several<a id="id287" class="indexterm"></a> clusters in such a way that the points in the same cluster are very similar, and the points in different clusters are dissimilar to each other. A "similarity graph" is a nice way to represent the similarities of the data points. Each point becomes a node in the similarity graph, whereas each edge has, as its attribute, the "similarity measure" of the connected nodes. As a result, the clustering problem reduces to find a partition of a graph in such a way that the edges between the different groups have very low weights, and the edges within a group have high weights. To do this, we use the spectral clustering technique, which basically reduces the high-dimensional similarity graph to a low-dimensional representation. To keep our discussion really simple, we will avoid the math. However, the technical <a id="id288" class="indexterm"></a>details can be learned by reading some good tutorials, such as the one available at <a class="ulink" href="http://arxiv.org/abs/0711.0189" target="_blank">http://arxiv.org/abs/0711.0189</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec56"></a>Power iteration clustering</h3></div></div></div><p>An efficient and<a id="id289" class="indexterm"></a> scalable spectral clustering method is the<a id="id290" class="indexterm"></a> power iteration <a id="id291" class="indexterm"></a>clustering (PIC) method. It is defined in the MLlib library, precisely in <a class="ulink" href="http://spark.apache.org/docs/latest/mllib-clustering.html#power-iteration-clustering-pic" target="_blank">http://spark.apache.org/docs/latest/mllib-clustering.html#power-iteration-clustering-pic</a>. It is implemented in Spark using GraphX's processing APIs, and the caching optimizations. Here is the API for this PIC clustering method:</p><div class="informalexample"><pre class="programlisting">class PowerIterationClustering {

    // Run the PIC algorithm.
    def run(similarities: RDD[(Long, Long, Double)]): PowerIterationClusteringModel
    
    // Set the initialization mode. Either "random" or "degree"
    def setInitializationMode(mode: String): PowerIterationClustering.this.type

    // Set the number of clusters.
    def setK(k: Int): PowerIterationClustering.this.type

    // Set maximum number of iterations of the power iteration loop
    def setMaxIterations(maxIterations: Int): PowerIterationClustering.this.type
}</pre></div><p>To apply the PIC clustering to graphs, we will need to follow these five steps:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>First, load the data into a Spark graph property.</p></li><li><p>Second, extract the features of the nodes.</p></li><li><p>Third, define a similarity measure between the two nodes.</p></li><li><p>Next, create an affinity matrix, based on the initial graph using the similarity measure.</p></li><li><p>Finally, run the k-means clustering on the affinity matrix.</p></li></ol></div><p>Steps 1 and 2 can be<a id="id292" class="indexterm"></a> done using the graph builder methods that <a id="id293" class="indexterm"></a>we learned in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Building and Exploring Graphs</em></span>. Step 3 simply requires us to define a function that determines how similar the two nodes are. The choice of similarity measure depends on the nodes' features, and the problem at hand. Nonetheless, there exists standard measures from which we can choose. For instance, if the node feature is a binary vector, we can use the Jaccard similarity. On the other hand, a Gaussian kernel function can be used when the node feature is a real vector. These are not the only possibilities, and we can also define our own measure.</p><p>In Step 4, the affinity matrix <code class="literal">similarities</code> should be represented by an RDD of (<span class="emphasis"><em>i</em></span>, <span class="emphasis"><em>j</em></span>, <span class="emphasis"><em>sim</em></span>) tuples. The similarity <span class="emphasis"><em>sim</em></span> must be a nonnegative number. For any edge (<span class="emphasis"><em>i j</em></span>) with a nonzero similarity, there should be either (<span class="emphasis"><em>i</em></span>, <span class="emphasis"><em>j</em></span>, <span class="emphasis"><em>sim</em></span>), or (<span class="emphasis"><em>j</em></span>, <span class="emphasis"><em>i</em></span>, <span class="emphasis"><em>sim</em></span>) in the input. Since the affinity matrix must be symmetric, if only (<span class="emphasis"><em>i</em></span>, <span class="emphasis"><em>j</em></span>, <span class="emphasis"><em>sim</em></span>) is available in the data, the reciprocal (<span class="emphasis"><em>j</em></span>, <span class="emphasis"><em>i</em></span>, <span class="emphasis"><em>sim</em></span>) is assumed, and vice versa. Moreover, tuples with <span class="emphasis"><em>i = j</em></span> are simply ignored.</p><p>The last step consists of two steps. First, we create <code class="literal">PowerIterationClusteringModel</code> from the <code class="literal">similarities</code> matrix, and then we run a k-means clustering on it. Before running the clustering model, we must also choose two parameters:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The maximum number of iterations for the k-means clustering</p></li><li style="list-style-type: disc"><p>The maximum number of clusters, <span class="emphasis"><em>K</em></span></p></li></ul></div><p>A sketch of the application of PIC is shown in the following code:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark.mllib.clustering.PowerIterationClustering

// Define pairwise similarities based on initial graph
val similarities: RDD[(Long, Long, Double)] = ...

// Create the PIC clustering model
val pic = new PowerIteartionClustering()
  .setK(maxClusterNumber)
  .setMaxIterations(maxIterations)

// Run the PIC clustering model
val clusteringResult: RDD[Assignment] = pic.run(similarities).assignments

clusteringResult.collect().foreach { a =&gt;
  println(s"${a.id} -&gt; ${a.cluster}")
}</pre></div><p>The <a id="id294" class="indexterm"></a>PIC method returns an RDD of assignment, which <a id="id295" class="indexterm"></a>abstracts a tuple of <code class="literal">VertexId</code>, and <code class="literal">Int</code> that corresponds to the node ID, and its cluster group.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec41"></a>Applications â€“ music fan community detection</h2></div></div><hr /></div><p>We are now ready<a id="id296" class="indexterm"></a> to apply the previous graph clustering method to the cluster music songs, according to the tags attached to each song. Alternatively, a dataset of the song playlists can also be used to cluster songs that are often played in many lists. The datasets that we are going to work with can be downloaded from <a class="ulink" href="http://www.cs.cornell.edu/~shuochen/lme/data_page.html" target="_blank">http://www.cs.cornell.edu/~shuochen/lme/data_page.html</a>. The datasets consist of the following files:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><code class="literal">train.txt</code>: This file contains the playlist data by using the integer ID to represent songs</p></li><li style="list-style-type: disc"><p><code class="literal">tags.txt</code>: This file includes the social tags by using the integer ID to represent songs</p></li><li style="list-style-type: disc"><p><code class="literal">song_hash.txt</code>: This file maps a song ID to its title and artist</p></li><li style="list-style-type: disc"><p><code class="literal">tag_hash.txt</code>: This one maps a tag ID to its name</p></li></ul></div><p>Each file has a particular format as explained here:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><span class="strong"><strong>Format of the playlist data</strong></span>: The first line of the data file consists of the IDs (not the integer ID, but the IDs from other sources for identifying the songs) for the songs, separated by a space. We will not need this first line here, and thus it can be skipped. The second line consists of the number of appearances of each song in the file, also separated by a space. Starting from the third line are the playlists, with each song represented by its integer ID in this file (from 0 to the total number of songs minus one). Note that in the playlist data file, each line ends with a space.</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Format of the tag data</strong></span>: The tag data file has the same number of lines as the total number of songs in the playlist data. Each line consists of the IDs of the tags for a song, represented by integers, and separated by space. If a song does not have a tag, its line is just a <code class="literal">#</code>. Note that for the tag file, there is no space at the end of each line.</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Format of the song mapping file</strong></span>: Each line corresponds to one song, and has the format called <code class="literal">Integer_ID\tTitle\tArtist\n</code>.</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Format of the tag mapping file</strong></span>: Each line corresponds to one song, and has the format called <code class="literal">Integer_ID, Name\n</code>.</p></li></ul></div><p>First, let's follow the previous five steps to cluster the songs by their tags.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec57"></a>Step 1 â€“ load the data into a Spark graph property</h3></div></div></div><p>We define a class<a id="id297" class="indexterm"></a> song. Each song has as its attributes a title, an artist name, and a set of tags:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; case class Song(title: String, artist: String, tags: Set[String]) {</strong></span>
<span class="strong"><strong>           override def toString: String = title + ", "  + artist</strong></span>
<span class="strong"><strong>       }</strong></span>
<span class="strong"><strong>defined class Song</strong></span>
</pre></div><p>Now, we import the songs into <code class="literal">RDD[(VertexId, Song)]</code>, and initialize each song with an empty set of tags:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; var songs: RDD[(VertexId, Song)] =</strong></span>
<span class="strong"><strong>           sc.textFile("./data/song_hash.txt").</strong></span>
<span class="strong"><strong>           map {line =&gt; </strong></span>
<span class="strong"><strong>               val row = line split '\t'</strong></span>
<span class="strong"><strong>               val vid = row(0).toLong</strong></span>
<span class="strong"><strong>               val song =  Song(row(1), row(2), Set.empty)</strong></span>
<span class="strong"><strong>               (vid, song)</strong></span>
<span class="strong"><strong>           }</strong></span>
<span class="strong"><strong>songs: RDD[(VertexId, Song)] </strong></span>
</pre></div><p>Then, we can <a id="id298" class="indexterm"></a>create a graph property, whose nodes are the songs. It will not add any edges into the graph at first, and will simply pass an empty RDD to <code class="literal">Graph.apply</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val graphFromSongs: Graph[Song, Int] = {</strong></span>
<span class="strong"><strong>           val zeroEdge: RDD[Edge[Int]] = sc.parallelize(Nil)</strong></span>
<span class="strong"><strong>           Graph(songs, zeroEdge)</strong></span>
<span class="strong"><strong>       }</strong></span>
<span class="strong"><strong>graphFromSongs: Graph[Song,Int] </strong></span>


<span class="strong"><strong>scala&gt; graphFromSongs.vertices.take(5).foreach(println)</strong></span>
<span class="strong"><strong>(1084,Song(Tequila Sunrise,Fiji,Set()))</strong></span>
<span class="strong"><strong>(1410,Song(The Sweetest Taboo,Sade,Set()))</strong></span>
<span class="strong"><strong>(3066,Song(Bow Chicka Wow Wow,Mike Posner,Set()))</strong></span>
<span class="strong"><strong>(1894,Song(Love Your Love The Most,Eric Church,Set()))</strong></span>
<span class="strong"><strong>(466,Song(Stupify,Disturbed,Set()))</strong></span>
</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec58"></a>Step 2 â€“ extract the features of nodes</h3></div></div></div><p>Now, let's join the<a id="id299" class="indexterm"></a> tags from the dataset called <code class="literal">tags.txt</code> into the nodes. To do this, we first need to create <code class="literal">RDD[(VertexId, Set[String])]</code>, which we will then join into <code class="literal">graphFromSong</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val tagIter: Iterator[(VertexId, Set[String])] = </strong></span>
<span class="strong"><strong>           Source.fromFile("./data/tags.txt").getLines.zipWithIndex.</strong></span>
<span class="strong"><strong>           map {</strong></span>
<span class="strong"><strong>               x =&gt; </strong></span>
<span class="strong"><strong>               val tags = x._1 split ' '</strong></span>
<span class="strong"><strong>               (x._2.toLong, tags.toSet)</strong></span>
<span class="strong"><strong>           }</strong></span>
<span class="strong"><strong>tagIter: Iterator[(VertexId, Set[String])] = non-empty iterator</strong></span>

<span class="strong"><strong>scala&gt; val tagRDD = sc.parallelize(tagIter.toSeq)</strong></span>
<span class="strong"><strong>tagRDD: RDD[(VertexId, Set[String])] </strong></span>
</pre></div><p>For now, we have only the mapping between the song ID and the set of tag IDs in our <code class="literal">tagRDD</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; tagRDD.take(3).foreach(println)</strong></span>
<span class="strong"><strong>(0,Set(115, 173))</strong></span>
<span class="strong"><strong>(1,Set(62, 88, 110, 90, 123, 155, 173, 14, 190, 214, 115, 27))</strong></span>
<span class="strong"><strong>(2,Set(115, 173))</strong></span>
</pre></div><p>What we want is<a id="id300" class="indexterm"></a> to extract the tag names from <code class="literal">tag_hash.txt</code> given the tag ID. We can now call <code class="literal">joinVertices</code> on <code class="literal">graphFromSongs</code>, and pass the RDD of tags <code class="literal">tagRDD</code> with a function that extracts the tags. Note that in the dataset called <code class="literal">tags.txt</code>, a <code class="literal">#</code> tag assigned next to the song ID means that no tag is associated with that song. In such a case, we simply return the initial song with an empty tag. Otherwise, we add the set of tags into the song:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val songsNtags = graphFromSongs.joinVertices(tagRDD){</strong></span>
<span class="strong"><strong>           (id, s, ks) =&gt; ks.toList match {</strong></span>
<span class="strong"><strong>               case List("#") =&gt; s</strong></span>
<span class="strong"><strong>               case _         =&gt; {</strong></span>
<span class="strong"><strong>                   val tags: Map[Int, String] = </strong></span>
<span class="strong"><strong>                   Source.fromFile("./data/tag_hash.txt").getLines().</strong></span>
<span class="strong"><strong>                   map {</strong></span>
<span class="strong"><strong>                       line =&gt; </strong></span>
<span class="strong"><strong>                       val row  = line split ", "</strong></span>
<span class="strong"><strong>                       row(0).toInt -&gt; row(1)</strong></span>
<span class="strong"><strong>                   }.toMap</strong></span>
<span class="strong"><strong>       </strong></span>
<span class="strong"><strong>                   val songTags = ks.map(_.toInt) flatMap (tags get)</strong></span>
<span class="strong"><strong>                   Song(s.title, s.artist, songTags.toSet)</strong></span>
<span class="strong"><strong>               }</strong></span>
<span class="strong"><strong>           }   </strong></span>
<span class="strong"><strong>       }</strong></span>
<span class="strong"><strong>songsNtags: Graph[Song,Int] </strong></span>

<span class="strong"><strong>scala&gt; songsNtags.vertices.take(3).foreach(println)</strong></span>
<span class="strong"><strong>(1084,Tequila Sunrise, Fiji)</strong></span>
<span class="strong"><strong>(1410,The Sweetest Taboo, Sade)</strong></span>
<span class="strong"><strong>(3066,Bow Chicka Wow Wow, Mike Posner)</strong></span>
</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec59"></a>Step 3 â€“ define a similarity measure between two nodes</h3></div></div></div><p>Since we want to <a id="id301" class="indexterm"></a>cluster the songs by their social tags, a natural way to measure the similarity between two songs is the Jaccard metric. Simply put, it is the ratio of the number of common tags between two songs, and their total number of tags. If none of the songs is tagged, we assume that their similarity score is zero:</p><div class="informalexample"><pre class="programlisting">def similarity(one: Song, other: Song):Double = {
        val numCommonTags = (one.tags intersect other.tags).size
        val numTotalTags = (one.tags union other.tags).size
        if (numTotalTags &gt; 0)
             numCommonTags.toDouble / numTotalTags.toDouble
        else 0.0
}</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec60"></a>Step 4 â€“ create an affinity matrix</h3></div></div></div><p>Now, we need to <a id="id302" class="indexterm"></a>calculate the similarity between each pair of songs in our database. If there are 1,000 songs, we will have to compute, and store, one million similarity scores. What if we had 1,000,000 songs? Obviously, computing similarities between every pair will be inefficient. Instead, we can restrict this to the songs that have a relatively high similarity score. At the end of the day, we want to cluster songs that are similar. Therefore, we will filter the nodes with the following function:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; def quiteSimilar(one: Song, other: Song, threshold: Double): Boolean = {</strong></span>
<span class="strong"><strong>           val commonTags = one.tags intersect other.tags</strong></span>
<span class="strong"><strong>           val combinedTags = one.tags union other.tags</strong></span>
<span class="strong"><strong>           commonTags.size &gt;  combinedTags.size * threshold</strong></span>
<span class="strong"><strong>       }</strong></span>
<span class="strong"><strong>quiteSimilar: (one: Song, other: Song, threshold: Double)Boolean</strong></span>
</pre></div><p>This next function helps to remove the duplicate songs in our graph data:</p><div class="informalexample"><pre class="programlisting">def differentSong(one: Song, other: Song): Boolean = 
    one.title != other.title || one.artist != other.artist </pre></div><p>With these two functions, we can now create <code class="literal">RDD[Edge[Double]]</code> that will contain a similarity measure between the nodes that are quite similar:</p><div class="informalexample"><pre class="programlisting">// First, get the songs with tags
songs = songsNtags.vertices

// Then, compute the similarity between each pair of songs 
// with a similarity score larger than 0.7
val similarConnections: RDD[Edge[Double]] = {
    val ss = songs cartesian songs
    val similarSongs = ss filter {
        p =&gt; p._1._1 != p._2._1 &amp;&amp; 
        similarByTags(p._1._2, p._2._2, 0.7) &amp;&amp; 
        differentSong(p._1._2, p._2._2)
    }
    
    similarSongs map {
        p =&gt; {
            val jacIdx = similarity(p._1._2, p._2._2)
            Edge(p._1._1, p._2._1, jacIdx)
        }
    }
}</pre></div><p>A simple check shows that we only need to store 1,506 similarity scores instead of 10 million:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; similarConnections.count</strong></span>
<span class="strong"><strong>res8: Long = 1506</strong></span>
<span class="strong"><strong>scala&gt; songs.count</strong></span>
<span class="strong"><strong>res9: Long = 3168</strong></span>

<span class="strong"><strong>scala&gt; 3168 * 3168</strong></span>
<span class="strong"><strong>res10: Int = 10036224</strong></span>
</pre></div><p>While we are at it, let's create our similarity graph:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val similarByTagsGraph = Graph(songs, similarConnections)</strong></span>
</pre></div><p>Some of <a id="id303" class="indexterm"></a>our songs have very few tags, so let's filter those out:</p><div class="informalexample"><pre class="programlisting">val similarHighTagsGraph = similarByTagsGraph.subgraph(vpred = (id: VertexId, attr: Song) =&gt; attr.tags.size &gt; 5)</pre></div><p>Let's check the output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; similarHighTagsGraph.vertices.count</strong></span>
<span class="strong"><strong>res12: Long = 2144</strong></span>
<span class="strong"><strong>scala&gt; similarHighTagsGraph.edges.count</strong></span>
<span class="strong"><strong>res13: Long = 126</strong></span>
</pre></div><p>Let's look closer into the graph:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; similarHighTagsGraph.triplets.take(6).foreach(t =&gt; println(t.srcAttr + " ~~~ " + t.dstAttr + " =&gt; " + t.attr))</strong></span>
<span class="strong"><strong>Fancy (w\/ T.I. &amp; Swizz Beatz), Drake ~~~ Any Girl (w\/ Lloyd), Lloyd Banks =&gt; 0.8571428571428571</strong></span>
<span class="strong"><strong>Roll With It, Easton Corbin ~~~ You Lie, The Band Perry =&gt; 0.7142857142857143</strong></span>
<span class="strong"><strong>Any Girl (w\/ Lloyd), Lloyd Banks ~~~ Fancy (w\/ T.I. &amp; Swizz Beatz), Drake =&gt; 0.8571428571428571</strong></span>
<span class="strong"><strong>Any Girl (w\/ Lloyd), Lloyd Banks ~~~ I'm Going In (w\/ Young Jeezy &amp; Lil Wayne), Drake =&gt; 0.7142857142857143</strong></span>
<span class="strong"><strong>Everything Falls, Fee ~~~ Needful Hands, Jars Of Clay =&gt; 0.7142857142857143</strong></span>
<span class="strong"><strong>Bring The Rain, MercyMe ~~~ Needful Hands, Jars Of Clay =&gt; 0.75</strong></span>
</pre></div><p>So, we can see that <code class="literal">Fancy</code> by <code class="literal">Drake</code> is similar to <code class="literal">Any Girl</code> by <code class="literal">Lloyd Banks</code>. Of course, they are rap songs.</p><p>Let's finally <a id="id304" class="indexterm"></a>create the affinity matrix of type <code class="literal">RDD[(Long, Long, Double)]</code>, which is needed to run the PIC algorithm:</p><div class="informalexample"><pre class="programlisting">val similarities: RDD[(Long,Long,Double)] = similarHighTagsGraph.triplets.map{t =&gt; (t.srcId, t.dstId, t.attr)}</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec61"></a>Step 5 â€“ run k-means clustering on the affinity matrix</h3></div></div></div><p>We can choose the<a id="id305" class="indexterm"></a> number of clusters to be <span class="emphasis"><em>K = 7</em></span>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val similarities: RDD[(Long,Long,Double)] = similarHighTagsGraph.triplets.map{t =&gt; (t.srcId, t.dstId, t.attr)}</strong></span>
<span class="strong"><strong>scala&gt; val pic = new PowerIterationClustering().setK(15).setMaxIterations(20)</strong></span>
<span class="strong"><strong>pic: org.apache.spark.mllib.clustering.PowerIterationClustering</strong></span>

<span class="strong"><strong>scala&gt; val clusteringModel = pic.run(similarities)</strong></span>
<span class="strong"><strong>clusteringModel: org.apache.spark.mllib.clustering.PowerIterationClusteringModel </strong></span>

<span class="strong"><strong>scala&gt; clusteringModel.assignments.foreach { a =&gt;</strong></span>
<span class="strong"><strong>     |     println(s"${a.id} -&gt; ${a.cluster}") </strong></span>
<span class="strong"><strong>     | }</strong></span>
<span class="strong"><strong>327 -&gt; 0</strong></span>
<span class="strong"><strong>715 -&gt; 0</strong></span>
<span class="strong"><strong>3063 -&gt; 2</strong></span>
<span class="strong"><strong>2879 -&gt; 2</strong></span>
<span class="strong"><strong>1623 -&gt; 0</strong></span>
<span class="strong"><strong>3003 -&gt; 0</strong></span>
<span class="strong"><strong>2539 -&gt; 0</strong></span>
<span class="strong"><strong>2283 -&gt; 0</strong></span>
<span class="strong"><strong>2163 -&gt; 0</strong></span>
<span class="strong"><strong>2979 -&gt; 0</strong></span>
<span class="strong"><strong>2615 -&gt; 5</strong></span>
<span class="strong"><strong>2147 -&gt; 1</strong></span>
<span class="strong"><strong>2667 -&gt; 3</strong></span>
<span class="strong"><strong>2531 -&gt; 0</strong></span>
<span class="strong"><strong>2149 -&gt; 4</strong></span>
<span class="strong"><strong>Extra Step: Looking into the clustering results</strong></span>
</pre></div><p>Well, we cannot really see anything through these numbers. So, let's explore the clusters and see what common tags the songs within each cluster have.</p><p>First, let's use the results of the clustering to create a graph whose nodes contain the actual song, as well as its clustering ID. To do this, we use the <code class="literal">VertexRDD</code> collection's <code class="literal">innerJoin</code> method twice. First, we join the clustering assignment to the graph of songs. Since <code class="literal">innerJoin</code> can alter the attribute type of the vertices, it does not matter what the initial graph's vertex type is. For simplicity, we initialize each vertex attribute to <code class="literal">0.0</code>. The second application of <code class="literal">innerJoin</code> joins the <code class="literal">VertexRDD</code> collection of songs into the result of the first application:</p><div class="informalexample"><pre class="programlisting">val clustering: RDD[(Long, Int)] = clusteringModel.assignments.map(a =&gt; (a.id, a.cluster))

val graph: VertexRDD[Double] = Graph.fromEdges[Double,Double](similarities.map(t =&gt; Edge(t._1,t._2,t._3)), 0.0).vertices

val clusteredSongs: VertexRDD[(Song, Int)] = graph.innerJoin(clustering){ (id, _, cluster) =&gt; cluster }.innerJoin(songs){ (id, cluster, s) =&gt; (s, cluster)}</pre></div><p>As a result, we<a id="id306" class="indexterm"></a> obtain a new <code class="literal">VertexRDD</code> collection <code class="literal">clusteredSongs</code>, which contains both the songs and their cluster IDs:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; clusteredSongs.first</strong></span>
<span class="strong"><strong>res25: (VertexId, (Song, Int)) = (2372,(Hold My Heart, Tenth Avenue North,7))</strong></span>
</pre></div><p>We can put this into a property graph with the similarity scores:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val clusterNScoreGraph = Graph(clusteredSongs, similarities.map(t =&gt; Edge(t._1,t._2,t._3)))</strong></span>
<span class="strong"><strong>clusterNScoreGraph: Graph[(Song, Int),Double]</strong></span>

<span class="strong"><strong>scala&gt; clusterNScoreGraph.triplets.first</strong></span>
<span class="strong"><strong>res37: EdgeTriplet[(Song, Int),Double] = ((38,(Fancy (w\/ T.I. &amp; Swizz Beatz), Drake,2)),(1976,(Any Girl (w\/ Lloyd), Lloyd Banks,2)),0.8571428571428571)</strong></span>
</pre></div><p>Because not all the neighboring songs correspond to the same cluster, we can filter out the edges between two songs that belong to two different clusters. In other words, these songs have some similarity, but they do not really belong together:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val clusteredSongGraph = clusterNScoreGraph.subgraph(epred = t =&gt; t.srcAttr._2 == t.dstAttr._2)</strong></span>
<span class="strong"><strong>clusteredSongGraph: Graph[(Song, Int),Double]</strong></span>

<span class="strong"><strong>scala&gt; clusteredSongGraph.edges.count</strong></span>
<span class="strong"><strong>res5: Long = 50</strong></span>
</pre></div><p>Next, we replace the attribute of the remaining edges by the set of common tags between the songs that they connect. This can be easily done, thanks to the <code class="literal">mapTriplets</code> method:</p><div class="informalexample"><pre class="programlisting">val clusteredTagsGraph = clusteredSongGraph.mapTriplets(t =&gt; t.srcAttr._1.tags intersect t.dstAttr._1.tags)</pre></div><p>Let's see what we get:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; clusteredTagsGraph.triplets.take(3).foreach(println)</strong></span>
<span class="strong"><strong>((482,(Roll With It, Easton Corbin,0)),(2866,(You Lie, The Band Perry,0)),Set(new country, modern country, country, great song, my favorite))</strong></span>
<span class="strong"><strong>((1976,(Any Girl (w\/ Lloyd), Lloyd Banks,6)),(2470,(I'm Going In (w\/ Young Jeezy &amp; Lil Wayne), Drake,6)),Set(rap, wdzh-fm, wjlb-fm, whtd-fm, wkqi-fm))</strong></span>
<span class="strong"><strong>((2364,(While I'm Waiting, John Waller,0)),(2372,(Hold My Heart, Tenth Avenue North,0)),Set(worship, favorite, christian, contemporary christian, christian rock))</strong></span>
</pre></div><p>Now, if we <a id="id307" class="indexterm"></a>want to approximately find the common tags in each cluster, we can make use of the Pregel operator to do so. Remember that the Pregel implementation in Spark allows the passing of the message only between neighboring nodes. However, our <code class="literal">clusteredTagsGraph</code> has nodes that are not directly connected by an edge, but still belong to the same cluster, maybe through other nodes. Thus, the Pregel operator will not find the absolute intersection of tags in each cluster, but it will still be helpful to see some patterns:</p><div class="informalexample"><pre class="programlisting">val commonTagsByCluster = clusteredTagsGraph.pregel[Set[String]](initialMsg = Set.empty, maxIterations = 10){
    (id, sc, m) =&gt; sc, 
    t =&gt; Iterator((t.srcId, t.srcAttr._1.tags intersect t.dstAttr._1.tags), 
    (t.dstId, t.srcAttr._1.tags intersect t.dstAttr._1.tags)), (s1, s2) =&gt; s1 intersect s2
}</pre></div><p>Looking at the results, we can find some straightforward clustering. The cluster #1 consists of worship songs:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; commonTagsByCluster.triplets.filter(_.srcAttr._2 == 1).foreach(t =&gt; println(t.srcAttr._1 + " =&gt; " + t.attr))</strong></span>
<span class="strong"><strong>Lead Me, Sanctus Real =&gt; Set(rock, worship, christian, contemporary christian, christian rock, happy)</strong></span>
<span class="strong"><strong>Needful Hands, Jars Of Clay =&gt; Set(rock, worship, christian, contemporary christian, christian rock, favorites)</strong></span>
<span class="strong"><strong>Revelation, Third Day =&gt; Set(rock, worship, christian, contemporary christian, christian rock, favorites)</strong></span>
<span class="strong"><strong>Give You Glory, Jeremy Camp =&gt; Set(rock, worship, christian, contemporary christian, christian rock, favorites)</strong></span>
<span class="strong"><strong>Revelation, Third Day =&gt; Set(rock, worship, christian, contemporary christian, christian rock, favorites)</strong></span>
<span class="strong"><strong>Cry Holy, Sonicflood =&gt; Set(rock, worship, christian, contemporary christian, christian rock)</strong></span>
</pre></div><p>The cluster #2 is about country music and RnB songs:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; commonTagsByCluster.triplets.filter(_.srcAttr._2 == 2).foreach(t =&gt; println(t.srcAttr._1 + " =&gt; " + t.attr))</strong></span>
<span class="strong"><strong>This Is Country Music, Brad Paisley =&gt; Set(beautiful, new country, memories, country, great song, my favorite)</strong></span>
<span class="strong"><strong>Bring It Back, Travis Porter =&gt; Set(wdzh-fm, hip hop, 2010s, wjlb-fm, whtd-fm, energetic)</strong></span>
<span class="strong"><strong>Anything Like Me, Brad Paisley =&gt; Set(beautiful, new country, memories, country, great song, my favorite)</strong></span>
<span class="strong"><strong>The Boys Of Fall, Kenny Chesney =&gt; Set(beautiful, new country, memories, country, great song, my favorite)</strong></span>
<span class="strong"><strong>Anything Like Me, Brad Paisley =&gt; Set(beautiful, new country, memories, country, great song, my favorite)</strong></span>
<span class="strong"><strong>Grove St. Party (w\/ Kebo Gotti), Waka Flocka Flame =&gt; Set(wdzh-fm, hip hop, 2010s, wjlb-fm, whtd-fm, energetic, wkqi-fm)</strong></span>
<span class="strong"><strong>The Boys Of Fall, Kenny Chesney =&gt; Set(beautiful, new country, memories, country, great song, my favorite)</strong></span>
<span class="strong"><strong>Make It Rain, Travis Porter =&gt; Set(wdzh-fm, hip hop, 2010s, wjlb-fm, whtd-fm, energetic)</strong></span>
<span class="strong"><strong>Grove St. Party (w\/ Kebo Gotti), Waka Flocka Flame =&gt; Set(wdzh-fm, hip hop, 2010s, wjlb-fm, whtd-fm, energetic)</strong></span>
<span class="strong"><strong>Love Faces, Trey Songz =&gt; Set(male vocalists, r&amp;b, 2010s, wjlb-fm, rnb, whtd-fm)</strong></span>
<span class="strong"><strong>Words, Bobby V =&gt; Set(male vocalists, r&amp;b, 2010s, wjlb-fm, rnb, whtd-fm)</strong></span>
<span class="strong"><strong>Cupid, Lloyd =&gt; Set(male vocalists, r&amp;b, 2010s, wjlb-fm, rnb, whtd-fm)</strong></span>
</pre></div><p>If you look at<a id="id308" class="indexterm"></a> the other clusters, cluster #6 is hip hop. However, the last cluster #0 is less straightforward to tell. It is a mix and match of everything. Such a limitation is simply due to the imperfection of the social tag data, and not due to the PIC clustering method itself.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec62"></a>Exercise â€“ collaborative clustering through playlists</h3></div></div></div><p>Clustering music songs<a id="id309" class="indexterm"></a> by social tags is not very effective. Imagine yourself having to tag every single song that you listen to. Instead of using explicit features, such as tags, we can alternatively use shared playlists to infer about the clustering. A playlist is a natural and more pervasive way to organize music. Now, the idea is that if two songs repeatedly appear in many lists, they are more likely to be similar than two other songs that do not belong to any cluster. I will leave the rest as an exercise. Just follow the same 5 steps that we took previously.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec42"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we studied how to solve the clustering problem for large-scale graphs. To do this, we introduced the Power Iteration Clustering method, and showed how to apply it to the clustering of songs using social tags. Using the song clustering example, we also reviewed the main graph building and processing techniques that we learned throughout this book. You should now be well-acquainted with using Spark's graph processing power to solve more interesting problems.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="appendix" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="appA"></a>AppendixÂ A.Â References</h2></div></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec43"></a>Chapter 2, Building and Exploring Graphs</h2></div></div><hr /></div><p>Ahn,Y.-Y.; Ahnert, S.; Bagrow, J. P.; and BarabÃ¡si, A.-L. <span class="emphasis"><em>Flavor network and the principles of food pairing</em></span>, Nature, Scientific Reports 1, 196 (2011).</p><p>Klimmt, B. and Yang, Y. <span class="emphasis"><em>Introducing the Enron corpus</em></span>. CEAS conference, 2004.</p><p>Leskovec, J.; Lang, K.; Dasgupta, A.; and Mahoney, M. <span class="emphasis"><em>Community Structure in Large Networks: Natural Cluster Sizes and the Absence of Large Well-Defined Clusters</em></span>. Internet Mathematics 6(1) 29--123, 2009.</p><p>McAuley, J. and Leskovec, J. <span class="emphasis"><em>Learning to Discover Social Circles in Ego Networks</em></span>. NIPS, 2012.</p><p>Page, L.; Brin, S.; Motwani, R., and Winograd, T. <span class="emphasis"><em>The PageRank Citation Ranking: Bringing Order to the Web</em></span>. Technical Report. Stanford InfoLab, 1999.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec44"></a>Chapter 3, Graph Analysis and Visualization</h2></div></div><hr /></div><p>Albert, R.; Jeong, H.; and Barabasi, A.-L. <span class="emphasis"><em>Diameter of the World-Wide Web</em></span>. Nature, 1999.</p><p>Dutot, A.; Guinand, F.; Olivier, D.; and PignÃ©, Y. <span class="emphasis"><em>GraphStream: A tool for bridging the gap between complex systems and dynamic graphs</em></span> in Emergent Properties in Natural and Artificial Complex Systems (EPNACSâ€™07), Workshop of the 4th European Conference on Complex Systems (ECCSâ€™07), Dresden, Germany.</p><p>Suereth, J. and Farewell, M. <span class="emphasis"><em>SBT in Action: The simple Scala build tool</em></span>, Manning Publications, 2015.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec45"></a>Chapter 7, Learning Graph Structures</h2></div></div><hr /></div><p>Lin, F. and Cohen W. W. (2010) <span class="emphasis"><em>Power Iteration Clustering</em></span>, in Johannes FÃ¼rnkranz and Thorsten Joachims, ed., ICML, Omnipress, pp. 655-662.</p><p>Luxburg, U. von. <span class="emphasis"><em>A Tutorial on Spectral Clustering</em></span>. Statistics and Computing 17(4): 395-416, 2007.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">

    <div id="backindex">
      <h1 class="title">Index</h1>
      <h2>A</h2>
      <ul>
        <li>aggregateMessages operator<ul><li>about / <a href="#ch05lvl1sec30" title="The aggregateMessages operator" class="link">The aggregateMessages operator</a></li><li>EdgeContext parameter / <a href="#ch05lvl1sec30" title="EdgeContext" class="link">EdgeContext</a></li><li>EdgeContext / <a href="#ch05lvl1sec30" title="EdgeContext" class="link">EdgeContext</a></li><li>aggregation, abstracting / <a href="#ch05lvl1sec30" title="Abstracting out the aggregation" class="link">Abstracting out the aggregation</a></li><li>DRY principle / <a href="#ch05lvl1sec30" title="Keeping things DRY" class="link">Keeping things DRY</a></li><li>arguments, adding / <a href="#ch05lvl1sec30" title="Coach wants more numbers" class="link">Coach wants more numbers</a></li><li>average point per game, calculating / <a href="#ch05lvl1sec30" title="Calculating average points per game" class="link">Calculating average points per game</a></li><li>defense stats / <a href="#ch05lvl1sec30" title="Defense stats â€“ D matters as in direction" class="link">Defense stats â€“ D matters as in direction</a></li></ul></li>
        <li>analysis, of network connectedness<ul><li>about / <a href="#ch03lvl1sec20" title="The analysis of network connectedness" class="link">The analysis of network connectedness</a></li><li>connected components, finding / <a href="#ch03lvl1sec20" title="Finding the connected components" class="link">Finding the connected components</a></li><li>triangle, counting / <a href="#ch03lvl1sec20" title="Counting triangles and computing clustering coefficients" class="link">Counting triangles and computing clustering coefficients</a></li><li>clustering coefficients, computing / <a href="#ch03lvl1sec20" title="Counting triangles and computing clustering coefficients" class="link">Counting triangles and computing clustering coefficients</a></li></ul></li>
        <li>Apache Zeppelin<ul><li>about / <a href="#ch03lvl1sec19" title="The graph visualization" class="link">The graph visualization</a></li></ul></li>
        <li>average stats<ul><li>joining, into graph / <a href="#ch05lvl1sec31" title="Joining average stats into a graph" class="link">Joining average stats into a graph</a></li></ul></li>
      </ul>
      <h2>B</h2>
      <ul>
        <li>bipartite food network example<ul><li>about / <a href="#ch04lvl1sec27" title="Example â€“ from food network to flavor pairing" class="link">Example â€“ from food network to flavor pairing</a></li></ul></li>
        <li>bipartite graph<ul><li>building / <a href="#ch02lvl1sec15" title="Building a bipartite graph" class="link">Building a bipartite graph</a></li></ul></li>
        <li>BreezeViz<ul><li>about / <a href="#ch03lvl1sec19" title="The graph visualization" class="link">The graph visualization</a></li><li>installing / <a href="#ch03lvl1sec19" title="Installing the GraphStream and BreezeViz libraries" class="link">Installing the GraphStream and BreezeViz libraries</a></li><li>URL, for downloading / <a href="#ch03lvl1sec19" title="Installing the GraphStream and BreezeViz libraries" class="link">Installing the GraphStream and BreezeViz libraries</a></li></ul></li>
      </ul>
      <h2>C</h2>
      <ul>
        <li>clustering<ul><li>about / <a href="#ch07lvl1sec40" title="Community clustering in graphs" class="link">Community clustering in graphs</a></li></ul></li>
        <li>ColorBrewer<ul><li>URL / <a href="#ch03lvl1sec20" title="The analysis of network connectedness" class="link">The analysis of network connectedness</a></li></ul></li>
        <li>communication network<ul><li>about / <a href="#ch02lvl1sec13" title="The communication network" class="link">The communication network</a></li></ul></li>
        <li>community clustering, in graphs<ul><li>about / <a href="#ch07lvl1sec40" title="Community clustering in graphs" class="link">Community clustering in graphs</a></li><li>spectral clustering / <a href="#ch07lvl1sec40" title="Spectral clustering" class="link">Spectral clustering</a></li><li>power iteration clustering (PIC) / <a href="#ch07lvl1sec40" title="Power iteration clustering" class="link">Power iteration clustering</a></li></ul></li>
        <li>community detection<ul><li>through label propagation / <a href="#ch06lvl1sec37" title="Community detection through label propagation" class="link">Community detection through label propagation</a></li></ul></li>
        <li>compound nodes<ul><li>about / <a href="#ch02lvl1sec13" title="Flavor networks" class="link">Flavor networks</a></li></ul></li>
      </ul>
      <h2>D</h2>
      <ul>
        <li>degree distribution<ul><li>plotting, of network / <a href="#ch03lvl1sec19" title="Plotting the degree distribution" class="link">Plotting the degree distribution</a></li></ul></li>
        <li>degree histogram, of social ego networks<ul><li>computing / <a href="#ch02lvl1sec16" title="Degree histogram of the social ego networks" class="link">Degree histogram of the social ego networks</a></li></ul></li>
        <li>degrees, in bipartite food network<ul><li>computing / <a href="#ch02lvl1sec16" title="Degrees in the bipartite food network" class="link">Degrees in the bipartite food network</a></li></ul></li>
        <li>degrees of network nodes<ul><li>computing / <a href="#ch02lvl1sec16" title="Computing the degrees of the network nodes" class="link">Computing the degrees of the network nodes</a></li></ul></li>
        <li>dependencies<ul><li>about / <a href="#ch03lvl1sec22" title="Managing library dependencies" class="link">Managing library dependencies</a></li></ul></li>
        <li>directed graphs<ul><li>building / <a href="#ch02lvl1sec15" title="Building directed graphs" class="link">Building directed graphs</a></li></ul></li>
      </ul>
      <h2>E</h2>
      <ul>
        <li>edge attributes<ul><li>transforming / <a href="#ch04lvl1sec24" title="Transforming the vertex and edge attributes" class="link">Transforming the vertex and edge attributes</a></li></ul></li>
        <li>EdgeContext parameter / <a href="#ch05lvl1sec30" title="EdgeContext" class="link">EdgeContext</a></li>
        <li>edge directions<ul><li>reversing / <a href="#ch04lvl1sec27" title="Reversing edge directions" class="link">Reversing edge directions</a></li></ul></li>
        <li>edgeListFile graph builder / <a href="#ch02lvl1sec14" title="edgeListFile" class="link">edgeListFile</a></li>
        <li>EdgeRDD<ul><li>data operations on / <a href="#ch04lvl1sec27" title="Data operations on VertexRDD and EdgeRDD" class="link">Data operations on VertexRDD and EdgeRDD</a></li><li>mapping / <a href="#ch04lvl1sec27" title="Mapping VertexRDD and EdgeRDD" class="link">Mapping VertexRDD and EdgeRDD</a></li></ul></li>
        <li>EdgeRDDs<ul><li>joining / <a href="#ch04lvl1sec27" title="Joining EdgeRDDs" class="link">Joining EdgeRDDs</a></li></ul></li>
        <li>email communication graph<ul><li>about / <a href="#ch02lvl1sec13" title="The communication network" class="link">The communication network</a></li></ul></li>
        <li>Enron Corpus<ul><li>about / <a href="#ch02lvl1sec13" title="The communication network" class="link">The communication network</a></li><li>URL / <a href="#ch02lvl1sec13" title="The communication network" class="link">The communication network</a></li></ul></li>
      </ul>
      <h2>F</h2>
      <ul>
        <li>files and directories, Spark 1.4.1<ul><li>core / <a href="#ch01lvl1sec09" title="Downloading and installing Spark 1.4.1" class="link">Downloading and installing Spark 1.4.1</a></li><li>bin / <a href="#ch01lvl1sec09" title="Downloading and installing Spark 1.4.1" class="link">Downloading and installing Spark 1.4.1</a></li><li>graphx / <a href="#ch01lvl1sec09" title="Downloading and installing Spark 1.4.1" class="link">Downloading and installing Spark 1.4.1</a></li><li>mllib, sql / <a href="#ch01lvl1sec09" title="Downloading and installing Spark 1.4.1" class="link">Downloading and installing Spark 1.4.1</a></li><li>streaming / <a href="#ch01lvl1sec09" title="Downloading and installing Spark 1.4.1" class="link">Downloading and installing Spark 1.4.1</a></li><li>examples / <a href="#ch01lvl1sec09" title="Downloading and installing Spark 1.4.1" class="link">Downloading and installing Spark 1.4.1</a></li></ul></li>
        <li>flavor network<ul><li>about / <a href="#ch02lvl1sec13" title="Flavor networks" class="link">Flavor networks</a></li><li>references / <a href="#ch02lvl1sec13" title="Flavor networks" class="link">Flavor networks</a></li></ul></li>
        <li>fromEdges graph builder / <a href="#ch02lvl1sec14" title="fromEdges" class="link">fromEdges</a></li>
        <li>fromEdgeTuples graph builder / <a href="#ch02lvl1sec14" title="fromEdgeTuples" class="link">fromEdgeTuples</a></li>
      </ul>
      <h2>G</h2>
      <ul>
        <li>Gaussian Mixture Model (GMM)<ul><li>about / <a href="#ch07lvl1sec40" title="Community clustering in graphs" class="link">Community clustering in graphs</a></li></ul></li>
        <li>Gephi<ul><li>about / <a href="#ch03lvl1sec19" title="The graph visualization" class="link">The graph visualization</a></li></ul></li>
        <li>graph<ul><li>about / <a href="#ch02lvl1sec13" title="Network datasets" class="link">Network datasets</a></li><li>average stats, joining into / <a href="#ch05lvl1sec31" title="Joining average stats into a graph" class="link">Joining average stats into a graph</a></li></ul></li>
        <li>graph builders<ul><li>about / <a href="#ch02lvl1sec14" title="Graph builders" class="link">Graph builders</a></li><li>Graph factory method / <a href="#ch02lvl1sec14" title="The Graph factory method" class="link">The Graph factory method</a></li><li>edgeListFile / <a href="#ch02lvl1sec14" title="edgeListFile" class="link">edgeListFile</a></li><li>fromEdges / <a href="#ch02lvl1sec14" title="fromEdges" class="link">fromEdges</a></li><li>fromEdgeTuples / <a href="#ch02lvl1sec14" title="fromEdgeTuples" class="link">fromEdgeTuples</a></li></ul></li>
        <li>graph data<ul><li>visualizing / <a href="#ch03lvl1sec19" title="Visualizing the graph data" class="link">Visualizing the graph data</a></li></ul></li>
        <li>graph datasets, joining<ul><li>about / <a href="#ch04lvl1sec26" title="Joining graph datasets" class="link">Joining graph datasets</a></li><li>joinVertices operator / <a href="#ch04lvl1sec26" title="joinVertices" class="link">joinVertices</a></li><li>outerJoinVertices operator / <a href="#ch04lvl1sec26" title="outerJoinVertices" class="link">outerJoinVertices</a></li></ul></li>
        <li>Graph factory method<ul><li>about / <a href="#ch02lvl1sec14" title="The Graph factory method" class="link">The Graph factory method</a></li></ul></li>
        <li>graphs<ul><li>building / <a href="#ch02lvl1sec15" title="Building graphs" class="link">Building graphs</a></li></ul></li>
        <li>GraphStream<ul><li>about / <a href="#ch03lvl1sec19" title="The graph visualization" class="link">The graph visualization</a></li><li>installing / <a href="#ch03lvl1sec19" title="Installing the GraphStream and BreezeViz libraries" class="link">Installing the GraphStream and BreezeViz libraries</a></li><li>URL, for downloading / <a href="#ch03lvl1sec19" title="Installing the GraphStream and BreezeViz libraries" class="link">Installing the GraphStream and BreezeViz libraries</a></li><li>URL / <a href="#ch03lvl1sec19" title="Visualizing the graph data" class="link">Visualizing the graph data</a></li></ul></li>
        <li>graph structures, modifying<ul><li>about / <a href="#ch04lvl1sec25" title="Modifying graph structures" class="link">Modifying graph structures</a></li><li>reverse operator / <a href="#ch04lvl1sec25" title="The reverse operator" class="link">The reverse operator</a></li><li>subgraph operator / <a href="#ch04lvl1sec25" title="The subgraph operator" class="link">The subgraph operator</a></li><li>mask operator / <a href="#ch04lvl1sec25" title="The mask operator" class="link">The mask operator</a></li><li>groupEdges operator / <a href="#ch04lvl1sec25" title="The groupEdges operator" class="link">The groupEdges operator</a></li></ul></li>
        <li>graph visualization<ul><li>about / <a href="#ch03lvl1sec19" title="The graph visualization" class="link">The graph visualization</a></li></ul></li>
        <li>GraphX<ul><li>about / <a href="#ch01lvl1sec11" title="Getting started with GraphX" class="link">Getting started with GraphX</a></li><li>tiny social network, building / <a href="#ch01lvl1sec11" title="Building a tiny social network" class="link">Building a tiny social network</a></li><li>standalone application, building / <a href="#ch01lvl1sec11" title="Building and submitting a standalone application" class="link">Building and submitting a standalone application</a></li><li>standalone application, submitting / <a href="#ch01lvl1sec11" title="Building and submitting a standalone application" class="link">Building and submitting a standalone application</a></li></ul></li>
        <li>groupEdges operator<ul><li>about / <a href="#ch04lvl1sec25" title="The groupEdges operator" class="link">The groupEdges operator</a></li></ul></li>
      </ul>
      <h2>H</h2>
      <ul>
        <li>Hollywood movie graph example<ul><li>about / <a href="#ch04lvl1sec26" title="Example â€“ Hollywood movie graph" class="link">Example â€“ Hollywood movie graph</a></li></ul></li>
      </ul>
      <h2>I</h2>
      <ul>
        <li>in-degree, of Enron email network<ul><li>computing / <a href="#ch02lvl1sec16" title="In-degree and out-degree of the Enron email network" class="link">In-degree and out-degree of the Enron email network</a></li></ul></li>
        <li>ingredient nodes<ul><li>about / <a href="#ch02lvl1sec13" title="Flavor networks" class="link">Flavor networks</a></li></ul></li>
        <li>installation<ul><li>Spark 1.4.1 / <a href="#ch01lvl1sec09" title="Downloading and installing Spark 1.4.1" class="link">Downloading and installing Spark 1.4.1</a></li></ul></li>
      </ul>
      <h2>J</h2>
      <ul>
        <li>Java Development Kit 7 (JDK) / <a href="#ch01lvl1sec09" title="Downloading and installing Spark 1.4.1" class="link">Downloading and installing Spark 1.4.1</a></li>
        <li>Java Runtime Environment (JRE)<ul><li>URL, for download / <a href="#ch01lvl1sec09" title="Downloading and installing Spark 1.4.1" class="link">Downloading and installing Spark 1.4.1</a></li></ul></li>
        <li>Java virtual machine (JVM) / <a href="#ch01lvl1sec09" title="Downloading and installing Spark 1.4.1" class="link">Downloading and installing Spark 1.4.1</a></li>
        <li>JfreeChart<ul><li>about / <a href="#ch03lvl1sec19" title="Installing the GraphStream and BreezeViz libraries" class="link">Installing the GraphStream and BreezeViz libraries</a></li></ul></li>
        <li>joinVertices operator / <a href="#ch04lvl1sec26" title="joinVertices" class="link">joinVertices</a></li>
      </ul>
      <h2>L</h2>
      <ul>
        <li>Label propagation algorithm (LPA)<ul><li>about / <a href="#ch06lvl1sec37" title="Community detection through label propagation" class="link">Community detection through label propagation</a></li></ul></li>
        <li>Latent Dirichlet Allocation (LDA)<ul><li>about / <a href="#ch07lvl1sec40" title="Community clustering in graphs" class="link">Community clustering in graphs</a></li></ul></li>
      </ul>
      <h2>M</h2>
      <ul>
        <li>mapEdges<ul><li>transforming / <a href="#ch04lvl1sec24" title="mapEdges" class="link">mapEdges</a></li></ul></li>
        <li>MapReduceTriplets operator<ul><li>about / <a href="#ch05lvl1sec33" title="The MapReduceTriplets operator" class="link">The MapReduceTriplets operator</a></li></ul></li>
        <li>mapTriplets<ul><li>transforming / <a href="#ch04lvl1sec24" title="mapTriplets" class="link">mapTriplets</a></li></ul></li>
        <li>mapVertices<ul><li>transforming / <a href="#ch04lvl1sec24" title="mapVertices" class="link">mapVertices</a></li></ul></li>
        <li>mask operator<ul><li>about / <a href="#ch04lvl1sec25" title="The mask operator" class="link">The mask operator</a></li></ul></li>
        <li>music fan community detection application<ul><li>about / <a href="#ch07lvl1sec41" title="Applications â€“ music fan community detection" class="link">Applications â€“ music fan community detection</a></li><li>data, loading into Spark graph property / <a href="#ch07lvl1sec41" title="Step 1 â€“ load the data into a Spark graph property" class="link">Step 1 â€“ load the data into a Spark graph property</a></li><li>features of nodes, extracting / <a href="#ch07lvl1sec41" title="Step 2 â€“ extract the features of nodes" class="link">Step 2 â€“ extract the features of nodes</a></li><li>similarity measure, defining between two nodes / <a href="#ch07lvl1sec41" title="Step 3 â€“ define a similarity measure between two nodes" class="link">Step 3 â€“ define a similarity measure between two nodes</a></li><li>affinity matrix, creating / <a href="#ch07lvl1sec41" title="Step 4 â€“ create an affinity matrix" class="link">Step 4 â€“ create an affinity matrix</a></li><li>k-means clustering, running on affinity matrix / <a href="#ch07lvl1sec41" title="Step 5 â€“ run k-means clustering on the affinity matrix" class="link">Step 5 â€“ run k-means clustering on the affinity matrix</a></li><li>collaborative clustering, by playlist / <a href="#ch07lvl1sec41" title="Exercise â€“ collaborative clustering through playlists" class="link">Exercise â€“ collaborative clustering through playlists</a></li></ul></li>
      </ul>
      <h2>N</h2>
      <ul>
        <li>NCAA College Basketball datasets<ul><li>about / <a href="#ch05lvl1sec29" title="NCAA College Basketball datasets" class="link">NCAA College Basketball datasets</a></li></ul></li>
        <li>neighboring information<ul><li>collecting / <a href="#ch04lvl1sec27" title="Collecting neighboring information" class="link">Collecting neighboring information</a></li></ul></li>
        <li>network centrality<ul><li>about / <a href="#ch03lvl1sec21" title="The network centrality and PageRank" class="link">The network centrality and PageRank</a></li></ul></li>
        <li>network connectedness<ul><li>analysis / <a href="#ch03lvl1sec20" title="The analysis of network connectedness" class="link">The analysis of network connectedness</a></li></ul></li>
        <li>network datasets<ul><li>about / <a href="#ch02lvl1sec13" title="Network datasets" class="link">Network datasets</a>, <a href="#ch03lvl1sec18" title="Network datasets" class="link">Network datasets</a></li><li>communication network / <a href="#ch02lvl1sec13" title="The communication network" class="link">The communication network</a></li><li>flavor network / <a href="#ch02lvl1sec13" title="Flavor networks" class="link">Flavor networks</a></li><li>social ego networks / <a href="#ch02lvl1sec13" title="Social ego networks" class="link">Social ego networks</a></li></ul></li>
      </ul>
      <h2>O</h2>
      <ul>
        <li>out-degree, of Enron email network<ul><li>computing / <a href="#ch02lvl1sec16" title="In-degree and out-degree of the Enron email network" class="link">In-degree and out-degree of the Enron email network</a></li></ul></li>
        <li>outerJoinVertices operator / <a href="#ch04lvl1sec26" title="outerJoinVertices" class="link">outerJoinVertices</a></li>
      </ul>
      <h2>P</h2>
      <ul>
        <li>PageRank<ul><li>about / <a href="#ch03lvl1sec21" title="The network centrality and PageRank" class="link">The network centrality and PageRank</a></li><li>working / <a href="#ch03lvl1sec21" title="How PageRank works" class="link">How PageRank works</a></li></ul></li>
        <li>performance optimization<ul><li>about / <a href="#ch05lvl1sec32" title="Performance optimization" class="link">Performance optimization</a></li></ul></li>
        <li>power iteration clustering (PIC)<ul><li>about / <a href="#ch07lvl1sec40" title="Community clustering in graphs" class="link">Community clustering in graphs</a>, <a href="#ch07lvl1sec40" title="Power iteration clustering" class="link">Power iteration clustering</a></li><li>reference link / <a href="#ch07lvl1sec40" title="Power iteration clustering" class="link">Power iteration clustering</a></li></ul></li>
        <li>Pregel API, in GraphX<ul><li>about / <a href="#ch06lvl1sec36" title="The Pregel API in GraphX" class="link">The Pregel API in GraphX</a></li></ul></li>
        <li>Pregel computational model<ul><li>about / <a href="#ch06lvl1sec35" title="The Pregel computational model" class="link">The Pregel computational model</a></li><li>iterating, towards social equality / <a href="#ch06lvl1sec35" title="Example â€“ iterating towards the social equality" class="link">Example â€“ iterating towards the social equality</a></li></ul></li>
        <li>Pregel implementation, of PageRank / <a href="#ch06lvl1sec38" title="The Pregel implementation of PageRank" class="link">The Pregel implementation of PageRank</a></li>
        <li>property graph / <a href="#ch01lvl1sec11" title="The property graph" class="link">The property graph</a></li>
      </ul>
      <h2>R</h2>
      <ul>
        <li>resolvers<ul><li>about / <a href="#ch03lvl1sec22" title="Managing library dependencies" class="link">Managing library dependencies</a></li></ul></li>
        <li>reverse operator<ul><li>about / <a href="#ch04lvl1sec25" title="The reverse operator" class="link">The reverse operator</a></li></ul></li>
      </ul>
      <h2>S</h2>
      <ul>
        <li>.sbt build files<ul><li>reference link / <a href="#ch03lvl1sec22" title="Organizing build definitions" class="link">Organizing build definitions</a></li></ul></li>
        <li>SBT<ul><li>about / <a href="#ch03lvl1sec22" title="Managing library dependencies" class="link">Managing library dependencies</a></li></ul></li>
        <li>SBT commands<ul><li>tasks, running with / <a href="#ch03lvl1sec22" title="Running tasks with SBT commands" class="link">Running tasks with SBT commands</a></li></ul></li>
        <li>Scala Build Tool<ul><li>about / <a href="#ch03lvl1sec22" title="Scala Build Tool revisited" class="link">Scala Build Tool revisited</a></li><li>build definitions, organizing / <a href="#ch03lvl1sec22" title="Organizing build definitions" class="link">Organizing build definitions</a></li><li>library dependencies, managing / <a href="#ch03lvl1sec22" title="Managing library dependencies" class="link">Managing library dependencies</a></li></ul></li>
        <li>Scala Build Tool (SBT)<ul><li>about / <a href="#ch01lvl1sec11" title="Building the program with the Scala Build Tool" class="link">Building the program with the Scala Build Tool</a></li><li>URL / <a href="#ch01lvl1sec11" title="Building the program with the Scala Build Tool" class="link">Building the program with the Scala Build Tool</a></li><li>used, for building program / <a href="#ch01lvl1sec11" title="Building the program with the Scala Build Tool" class="link">Building the program with the Scala Build Tool</a></li></ul></li>
        <li>share circle feature<ul><li>about / <a href="#ch02lvl1sec13" title="Social ego networks" class="link">Social ego networks</a></li></ul></li>
        <li>social ego networks<ul><li>about / <a href="#ch02lvl1sec13" title="Social ego networks" class="link">Social ego networks</a></li><li>reference link / <a href="#ch02lvl1sec13" title="Social ego networks" class="link">Social ego networks</a></li></ul></li>
        <li>Spark<ul><li>URL, for download / <a href="#ch01lvl1sec09" title="Downloading and installing Spark 1.4.1" class="link">Downloading and installing Spark 1.4.1</a></li><li>programming guide, URL / <a href="#ch01lvl1sec10" title="Experimenting with the Spark shell" class="link">Experimenting with the Spark shell</a></li><li>application, URL / <a href="#ch01lvl1sec11" title="Deploying and running with spark-submit" class="link">Deploying and running with spark-submit</a></li><li>URL, for documentation / <a href="#ch03lvl1sec20" title="Finding the connected components" class="link">Finding the connected components</a></li></ul></li>
        <li>spark-submit<ul><li>used, for running standalone application / <a href="#ch01lvl1sec11" title="Deploying and running with spark-submit" class="link">Deploying and running with spark-submit</a></li><li>used, for deploying standalone application / <a href="#ch01lvl1sec11" title="Deploying and running with spark-submit" class="link">Deploying and running with spark-submit</a></li></ul></li>
        <li>Spark 1.4.1<ul><li>downloading / <a href="#ch01lvl1sec09" title="Downloading and installing Spark 1.4.1" class="link">Downloading and installing Spark 1.4.1</a></li><li>installing / <a href="#ch01lvl1sec09" title="Downloading and installing Spark 1.4.1" class="link">Downloading and installing Spark 1.4.1</a></li></ul></li>
        <li>Spark application, building steps<ul><li>about / <a href="#ch03lvl1sec22" title="A preview of the steps" class="link">A preview of the steps</a></li><li>sbt-assembly plugin, enabling / <a href="#ch03lvl1sec22" title="Step 1 â€“ Enable the sbt-assembly plugin" class="link">Step 1 â€“ Enable the sbt-assembly plugin</a></li><li>build.sbt file, creating / <a href="#ch03lvl1sec22" title="Step 2 â€“ Create a build.sbt file" class="link">Step 2 â€“ Create a build.sbt file</a></li><li>library dependencies, declaring / <a href="#ch03lvl1sec22" title="Step 3 â€“ Declare library dependencies and resolvers" class="link">Step 3 â€“ Declare library dependencies and resolvers</a></li><li>resolvers, declaring / <a href="#ch03lvl1sec22" title="Step 3 â€“ Declare library dependencies and resolvers" class="link">Step 3 â€“ Declare library dependencies and resolvers</a></li><li>sbt-assembly plugin, setting up / <a href="#ch03lvl1sec22" title="Step 4 â€“ Set up the sbt-assembly plugin" class="link">Step 4 â€“ Set up the sbt-assembly plugin</a></li><li>uber JAR, creating / <a href="#ch03lvl1sec22" title="Step 5 â€“ Create the uber JAR" class="link">Step 5 â€“ Create the uber JAR</a></li></ul></li>
        <li>Spark program<ul><li>writing / <a href="#ch01lvl1sec11" title="Writing and configuring a Spark program" class="link">Writing and configuring a Spark program</a></li><li>configuring / <a href="#ch01lvl1sec11" title="Writing and configuring a Spark program" class="link">Writing and configuring a Spark program</a></li><li>URL / <a href="#ch01lvl1sec11" title="Writing and configuring a Spark program" class="link">Writing and configuring a Spark program</a></li><li>spark.app.name property / <a href="#ch01lvl1sec11" title="Writing and configuring a Spark program" class="link">Writing and configuring a Spark program</a></li><li>spark.executor.memory property / <a href="#ch01lvl1sec11" title="Writing and configuring a Spark program" class="link">Writing and configuring a Spark program</a></li><li>spark.driver.memory property / <a href="#ch01lvl1sec11" title="Writing and configuring a Spark program" class="link">Writing and configuring a Spark program</a></li><li>spark.storage.memoryFraction property / <a href="#ch01lvl1sec11" title="Writing and configuring a Spark program" class="link">Writing and configuring a Spark program</a></li><li>spark.serializer property / <a href="#ch01lvl1sec11" title="Writing and configuring a Spark program" class="link">Writing and configuring a Spark program</a></li></ul></li>
        <li>Spark shell<ul><li>experimenting with / <a href="#ch01lvl1sec10" title="Experimenting with the Spark shell" class="link">Experimenting with the Spark shell</a></li></ul></li>
        <li>spectral clustering<ul><li>about / <a href="#ch07lvl1sec40" title="Spectral clustering" class="link">Spectral clustering</a></li><li>reference link / <a href="#ch07lvl1sec40" title="Spectral clustering" class="link">Spectral clustering</a></li></ul></li>
        <li>standalone application<ul><li>submitting / <a href="#ch01lvl1sec11" title="Building and submitting a standalone application" class="link">Building and submitting a standalone application</a></li><li>building / <a href="#ch01lvl1sec11" title="Building and submitting a standalone application" class="link">Building and submitting a standalone application</a></li><li>Spark program, writing / <a href="#ch01lvl1sec11" title="Writing and configuring a Spark program" class="link">Writing and configuring a Spark program</a></li><li>Spark program, configuring / <a href="#ch01lvl1sec11" title="Writing and configuring a Spark program" class="link">Writing and configuring a Spark program</a></li><li>program, building with Scala Build Tool / <a href="#ch01lvl1sec11" title="Building the program with the Scala Build Tool" class="link">Building the program with the Scala Build Tool</a></li><li>spark-submit, downloading / <a href="#ch01lvl1sec11" title="Deploying and running with spark-submit" class="link">Deploying and running with spark-submit</a></li><li>spark-submit, running / <a href="#ch01lvl1sec11" title="Deploying and running with spark-submit" class="link">Deploying and running with spark-submit</a></li></ul></li>
        <li>subgraph operator<ul><li>about / <a href="#ch04lvl1sec25" title="The subgraph operator" class="link">The subgraph operator</a></li></ul></li>
        <li>superstep<ul><li>about / <a href="#ch06lvl1sec35" title="Example â€“ iterating towards the social equality" class="link">Example â€“ iterating towards the social equality</a></li></ul></li>
        <li>supersteps<ul><li>about / <a href="#ch06lvl1sec35" title="The Pregel computational model" class="link">The Pregel computational model</a></li></ul></li>
      </ul>
      <h2>T</h2>
      <ul>
        <li>tasks<ul><li>running, with SBT commands / <a href="#ch03lvl1sec22" title="Running tasks with SBT commands" class="link">Running tasks with SBT commands</a></li></ul></li>
        <li>tiny social network<ul><li>building / <a href="#ch01lvl1sec11" title="Building a tiny social network" class="link">Building a tiny social network</a></li><li>data, loading / <a href="#ch01lvl1sec11" title="Loading the data" class="link">Loading the data</a></li><li>property graph / <a href="#ch01lvl1sec11" title="The property graph" class="link">The property graph</a></li><li>RDDs, transforming to VertexRDD / <a href="#ch01lvl1sec11" title="Transforming RDDs to VertexRDD and EdgeRDD" class="link">Transforming RDDs to VertexRDD and EdgeRDD</a></li><li>RDDs, transforming to EdgeRDD / <a href="#ch01lvl1sec11" title="Transforming RDDs to VertexRDD and EdgeRDD" class="link">Transforming RDDs to VertexRDD and EdgeRDD</a></li><li>graph operations / <a href="#ch01lvl1sec11" title="Introducing graph operations" class="link">Introducing graph operations</a></li></ul></li>
        <li>triplet view / <a href="#ch01lvl1sec11" title="Introducing graph operations" class="link">Introducing graph operations</a></li>
      </ul>
      <h2>V</h2>
      <ul>
        <li>vertex attributes<ul><li>transforming / <a href="#ch04lvl1sec24" title="Transforming the vertex and edge attributes" class="link">Transforming the vertex and edge attributes</a></li></ul></li>
        <li>vertex program<ul><li>about / <a href="#ch06lvl1sec35" title="Example â€“ iterating towards the social equality" class="link">Example â€“ iterating towards the social equality</a></li></ul></li>
        <li>VertexRDD<ul><li>data operations on / <a href="#ch04lvl1sec27" title="Data operations on VertexRDD and EdgeRDD" class="link">Data operations on VertexRDD and EdgeRDD</a></li><li>mapping / <a href="#ch04lvl1sec27" title="Mapping VertexRDD and EdgeRDD" class="link">Mapping VertexRDD and EdgeRDD</a></li></ul></li>
        <li>VertexRDDs<ul><li>filtering / <a href="#ch04lvl1sec27" title="Filtering VertexRDDs" class="link">Filtering VertexRDDs</a></li><li>joining / <a href="#ch04lvl1sec27" title="Joining VertexRDDs" class="link">Joining VertexRDDs</a></li></ul></li>
      </ul>
      <h2>W</h2>
      <ul>
        <li>web pages<ul><li>ranking / <a href="#ch03lvl1sec21" title="Ranking web pages" class="link">Ranking web pages</a></li></ul></li>
        <li>weighted social ego network<ul><li>building / <a href="#ch02lvl1sec15" title="Building a weighted social ego network" class="link">Building a weighted social ego network</a></li></ul></li>
      </ul>
    </div>
  </div></div></div>
</div></div></div></body></html>
