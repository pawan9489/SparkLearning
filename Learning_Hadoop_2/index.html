<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="">

    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0"> <!--320-->
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no, user-scalable=no">

    <link rel="icon" href="../../mapt/images/favicon.ico">

    <link rel="stylesheet" href="../../mapt/css/font-awesome.css">
    <link rel="stylesheet" href="../../mapt/css/google-fonts.css">
    <link rel="stylesheet" href="../../mapt/css/devicon.css">

    <link rel="stylesheet" href="../../mapt/css/bootstrap.css">
    <link rel="stylesheet" href="../../mapt/css/bootstrap-xl.css">
    <link rel="stylesheet" href="../../mapt/css/magnific-popup.css">
    <link rel="stylesheet" href="../../mapt/css/prism.css">
    <link rel="stylesheet" href="../../mapt/css/hljs-github.css">

    <link rel="stylesheet" href="../../mapt/css/mapt.css">
    <link rel="stylesheet" href="../../mapt/css/custom.css">

    <script src="../../mapt/js/jquery.js"></script>
    <script src="../../mapt/js/bootstrap.js"></script>
    <script src="../../mapt/js/jquery.magnific-popup.js"></script>
    <script src="../../mapt/js/highlight.min.js"></script>

    <script src="../../mapt/js/custom.js"></script>
    
    <title>Learning Hadoop 2</title>
</head>

<body class="home-body">
    <div id="wrapper">
        <div id="sidebar-wrapper">    
            <ul class="sidebar-nav">
                <div class="list-group" id="sidebar-nav" role="tablist">
                    <li>
                        <a href="../../index.html" class="sidenav-menu-holder back-btn" id="back-link">
                            <span class="sidenav-menu">Book List</span>
                            <span class="pull-left mr5"><i class="fa fa-chevron-left"></i></span>
                        </a>
                    </li>
                    
                    <li class="book-info copyright">
                        <span class="info text-nowrap"><span class="copyleft">&copy;</span><span><strong>RuTracker</strong>.org</span></span>
                    </li>          
                    <li class="book-info copyright">
                        <span class="info text-nowrap">Pub date: <strong>13 Feb 2015</strong></span>
                    </li>         
                    <li class="book-info">
                        <span class="info text-nowrap">Price: â‚¬<strong>23.99</strong></span>
                        <span class="info text-nowrap">ISBN: <strong>9781783285518</strong></span>
                    </li>     
            
                    <li>
                        <a href="graphics/cover.jpg" class="sidenav-menu-holder cover-img">
                            <img src="graphics/cover.jpg" class="cover-image">
                        </a>
                    </li>        
            
                    <div class="book_navigation">
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse1">
                                <div class="section-name">1: Introduction</div>
                            </a>
                        </li>
                        <div id="collapse1" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="1" class="sub-nav">
                                <a href="#ch01">
                                    <div class="section-name">Chapter 1: Introduction</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec08" class="sub-nav">
                                <a href="#ch01lvl1sec08">                    
                                    <div class="section-name">A note on versioning</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec09" class="sub-nav">
                                <a href="#ch01lvl1sec09">                    
                                    <div class="section-name">The background of Hadoop</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec10" class="sub-nav">
                                <a href="#ch01lvl1sec10">                    
                                    <div class="section-name">Components of Hadoop</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec11" class="sub-nav">
                                <a href="#ch01lvl1sec11">                    
                                    <div class="section-name">Hadoop 2 what&#x27;s the big deal?</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec12" class="sub-nav">
                                <a href="#ch01lvl1sec12">                    
                                    <div class="section-name">Distributions of Apache Hadoop</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec13" class="sub-nav">
                                <a href="#ch01lvl1sec13">                    
                                    <div class="section-name">A dual approach</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec14" class="sub-nav">
                                <a href="#ch01lvl1sec14">                    
                                    <div class="section-name">AWS infrastructure on demand from Amazon</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec15" class="sub-nav">
                                <a href="#ch01lvl1sec15">                    
                                    <div class="section-name">Getting started</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec16" class="sub-nav">
                                <a href="#ch01lvl1sec16">                    
                                    <div class="section-name">Running the examples</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec17" class="sub-nav">
                                <a href="#ch01lvl1sec17">                    
                                    <div class="section-name">Data processing with Hadoop</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec18" class="sub-nav">
                                <a href="#ch01lvl1sec18">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse2">
                                <div class="section-name">2: Storage</div>
                            </a>
                        </li>
                        <div id="collapse2" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="2" class="sub-nav">
                                <a href="#ch02">
                                    <div class="section-name">Chapter 2: Storage</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec19" class="sub-nav">
                                <a href="#ch02lvl1sec19">                    
                                    <div class="section-name">The inner workings of HDFS</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec20" class="sub-nav">
                                <a href="#ch02lvl1sec20">                    
                                    <div class="section-name">Command-line access to the HDFS filesystem</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec21" class="sub-nav">
                                <a href="#ch02lvl1sec21">                    
                                    <div class="section-name">Protecting the filesystem metadata</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec22" class="sub-nav">
                                <a href="#ch02lvl1sec22">                    
                                    <div class="section-name">Apache ZooKeeper a different type of filesystem</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec23" class="sub-nav">
                                <a href="#ch02lvl1sec23">                    
                                    <div class="section-name">Automatic NameNode failover</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec24" class="sub-nav">
                                <a href="#ch02lvl1sec24">                    
                                    <div class="section-name">HDFS snapshots</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec25" class="sub-nav">
                                <a href="#ch02lvl1sec25">                    
                                    <div class="section-name">Hadoop filesystems</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec26" class="sub-nav">
                                <a href="#ch02lvl1sec26">                    
                                    <div class="section-name">Managing and serializing data</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec27" class="sub-nav">
                                <a href="#ch02lvl1sec27">                    
                                    <div class="section-name">Storing data</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec28" class="sub-nav">
                                <a href="#ch02lvl1sec28">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse3">
                                <div class="section-name">3: Processing â€“ MapReduce and Beyond</div>
                            </a>
                        </li>
                        <div id="collapse3" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="3" class="sub-nav">
                                <a href="#ch03">
                                    <div class="section-name">Chapter 3: Processing â€“ MapReduce and Beyond</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec29" class="sub-nav">
                                <a href="#ch03lvl1sec29">                    
                                    <div class="section-name">MapReduce</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec30" class="sub-nav">
                                <a href="#ch03lvl1sec30">                    
                                    <div class="section-name">Java API to MapReduce</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec31" class="sub-nav">
                                <a href="#ch03lvl1sec31">                    
                                    <div class="section-name">Writing MapReduce programs</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec32" class="sub-nav">
                                <a href="#ch03lvl1sec32">                    
                                    <div class="section-name">Walking through a run of a MapReduce job</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec33" class="sub-nav">
                                <a href="#ch03lvl1sec33">                    
                                    <div class="section-name">YARN</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec34" class="sub-nav">
                                <a href="#ch03lvl1sec34">                    
                                    <div class="section-name">YARN in the real world Computation beyond MapReduce</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec35" class="sub-nav">
                                <a href="#ch03lvl1sec35">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse4">
                                <div class="section-name">4: Real-time Computation with Samza</div>
                            </a>
                        </li>
                        <div id="collapse4" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="4" class="sub-nav">
                                <a href="#ch04">
                                    <div class="section-name">Chapter 4: Real-time Computation with Samza</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec36" class="sub-nav">
                                <a href="#ch04lvl1sec36">                    
                                    <div class="section-name">Stream processing with Samza</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec37" class="sub-nav">
                                <a href="#ch04lvl1sec37">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse5">
                                <div class="section-name">5: Iterative Computation with Spark</div>
                            </a>
                        </li>
                        <div id="collapse5" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="5" class="sub-nav">
                                <a href="#ch05">
                                    <div class="section-name">Chapter 5: Iterative Computation with Spark</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec38" class="sub-nav">
                                <a href="#ch05lvl1sec38">                    
                                    <div class="section-name">Apache Spark</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec39" class="sub-nav">
                                <a href="#ch05lvl1sec39">                    
                                    <div class="section-name">The Spark ecosystem</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec40" class="sub-nav">
                                <a href="#ch05lvl1sec40">                    
                                    <div class="section-name">Processing data with Apache Spark</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec41" class="sub-nav">
                                <a href="#ch05lvl1sec41">                    
                                    <div class="section-name">Comparing Samza and Spark Streaming</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec42" class="sub-nav">
                                <a href="#ch05lvl1sec42">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse6">
                                <div class="section-name">6: Data Analysis with Apache Pig</div>
                            </a>
                        </li>
                        <div id="collapse6" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="6" class="sub-nav">
                                <a href="#ch06">
                                    <div class="section-name">Chapter 6: Data Analysis with Apache Pig</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec43" class="sub-nav">
                                <a href="#ch06lvl1sec43">                    
                                    <div class="section-name">An overview of Pig</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec44" class="sub-nav">
                                <a href="#ch06lvl1sec44">                    
                                    <div class="section-name">Getting started</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec45" class="sub-nav">
                                <a href="#ch06lvl1sec45">                    
                                    <div class="section-name">Running Pig</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec46" class="sub-nav">
                                <a href="#ch06lvl1sec46">                    
                                    <div class="section-name">Fundamentals of Apache Pig</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec47" class="sub-nav">
                                <a href="#ch06lvl1sec47">                    
                                    <div class="section-name">Programming Pig</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec48" class="sub-nav">
                                <a href="#ch06lvl1sec48">                    
                                    <div class="section-name">Extending Pig (UDFs)</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec49" class="sub-nav">
                                <a href="#ch06lvl1sec49">                    
                                    <div class="section-name">Analyzing the Twitter stream</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec50" class="sub-nav">
                                <a href="#ch06lvl1sec50">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse7">
                                <div class="section-name">7: Hadoop and SQL</div>
                            </a>
                        </li>
                        <div id="collapse7" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="7" class="sub-nav">
                                <a href="#ch07">
                                    <div class="section-name">Chapter 7: Hadoop and SQL</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec51" class="sub-nav">
                                <a href="#ch07lvl1sec51">                    
                                    <div class="section-name">Why SQL on Hadoop</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec52" class="sub-nav">
                                <a href="#ch07lvl1sec52">                    
                                    <div class="section-name">Prerequisites</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec53" class="sub-nav">
                                <a href="#ch07lvl1sec53">                    
                                    <div class="section-name">Hive architecture</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec54" class="sub-nav">
                                <a href="#ch07lvl1sec54">                    
                                    <div class="section-name">Hive and Amazon Web Services</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec55" class="sub-nav">
                                <a href="#ch07lvl1sec55">                    
                                    <div class="section-name">Extending HiveQL</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec56" class="sub-nav">
                                <a href="#ch07lvl1sec56">                    
                                    <div class="section-name">Programmatic interfaces</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec57" class="sub-nav">
                                <a href="#ch07lvl1sec57">                    
                                    <div class="section-name">Stinger initiative</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec58" class="sub-nav">
                                <a href="#ch07lvl1sec58">                    
                                    <div class="section-name">Impala</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec59" class="sub-nav">
                                <a href="#ch07lvl1sec59">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse8">
                                <div class="section-name">8: Data Lifecycle Management</div>
                            </a>
                        </li>
                        <div id="collapse8" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="8" class="sub-nav">
                                <a href="#ch08">
                                    <div class="section-name">Chapter 8: Data Lifecycle Management</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec60" class="sub-nav">
                                <a href="#ch08lvl1sec60">                    
                                    <div class="section-name">What data lifecycle management is</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec61" class="sub-nav">
                                <a href="#ch08lvl1sec61">                    
                                    <div class="section-name">Building a tweet analysis capability</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec62" class="sub-nav">
                                <a href="#ch08lvl1sec62">                    
                                    <div class="section-name">Challenges of external data</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec63" class="sub-nav">
                                <a href="#ch08lvl1sec63">                    
                                    <div class="section-name">Collecting additional data</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec64" class="sub-nav">
                                <a href="#ch08lvl1sec64">                    
                                    <div class="section-name">Pulling it all together</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec65" class="sub-nav">
                                <a href="#ch08lvl1sec65">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse9">
                                <div class="section-name">9: Making Development Easier</div>
                            </a>
                        </li>
                        <div id="collapse9" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="9" class="sub-nav">
                                <a href="#ch09">
                                    <div class="section-name">Chapter 9: Making Development Easier</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec66" class="sub-nav">
                                <a href="#ch09lvl1sec66">                    
                                    <div class="section-name">Choosing a framework</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec67" class="sub-nav">
                                <a href="#ch09lvl1sec67">                    
                                    <div class="section-name">Hadoop streaming</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec68" class="sub-nav">
                                <a href="#ch09lvl1sec68">                    
                                    <div class="section-name">Kite Data</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec69" class="sub-nav">
                                <a href="#ch09lvl1sec69">                    
                                    <div class="section-name">Apache Crunch</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec70" class="sub-nav">
                                <a href="#ch09lvl1sec70">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse10">
                                <div class="section-name">10: Running a Hadoop Cluster</div>
                            </a>
                        </li>
                        <div id="collapse10" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="10" class="sub-nav">
                                <a href="#ch10">
                                    <div class="section-name">Chapter 10: Running a Hadoop Cluster</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec71" class="sub-nav">
                                <a href="#ch10lvl1sec71">                    
                                    <div class="section-name">I&#x27;m a developer I don&#x27;t care about operations!</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec72" class="sub-nav">
                                <a href="#ch10lvl1sec72">                    
                                    <div class="section-name">Cloudera Manager</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec73" class="sub-nav">
                                <a href="#ch10lvl1sec73">                    
                                    <div class="section-name">Ambari the open source alternative</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec74" class="sub-nav">
                                <a href="#ch10lvl1sec74">                    
                                    <div class="section-name">Operations in the Hadoop 2 world</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec75" class="sub-nav">
                                <a href="#ch10lvl1sec75">                    
                                    <div class="section-name">Sharing resources</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec76" class="sub-nav">
                                <a href="#ch10lvl1sec76">                    
                                    <div class="section-name">Building a physical cluster</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec77" class="sub-nav">
                                <a href="#ch10lvl1sec77">                    
                                    <div class="section-name">Building a cluster on EMR</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec78" class="sub-nav">
                                <a href="#ch10lvl1sec78">                    
                                    <div class="section-name">Cluster tuning</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec79" class="sub-nav">
                                <a href="#ch10lvl1sec79">                    
                                    <div class="section-name">Security</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec80" class="sub-nav">
                                <a href="#ch10lvl1sec80">                    
                                    <div class="section-name">Monitoring</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec81" class="sub-nav">
                                <a href="#ch10lvl1sec81">                    
                                    <div class="section-name">Troubleshooting</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec82" class="sub-nav">
                                <a href="#ch10lvl1sec82">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse11">
                                <div class="section-name">11: Where to Go Next</div>
                            </a>
                        </li>
                        <div id="collapse11" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="11" class="sub-nav">
                                <a href="#ch11">
                                    <div class="section-name">Chapter 11: Where to Go Next</div>
                                </a>
                            </li>
                            <li data-chapter="11" data-section-id="ch11lvl1sec83" class="sub-nav">
                                <a href="#ch11lvl1sec83">                    
                                    <div class="section-name">Alternative distributions</div>
                                </a>
                            </li>
                            <li data-chapter="11" data-section-id="ch11lvl1sec84" class="sub-nav">
                                <a href="#ch11lvl1sec84">                    
                                    <div class="section-name">Other computational frameworks</div>
                                </a>
                            </li>
                            <li data-chapter="11" data-section-id="ch11lvl1sec85" class="sub-nav">
                                <a href="#ch11lvl1sec85">                    
                                    <div class="section-name">Other interesting projects</div>
                                </a>
                            </li>
                            <li data-chapter="11" data-section-id="ch11lvl1sec86" class="sub-nav">
                                <a href="#ch11lvl1sec86">                    
                                    <div class="section-name">Other programming abstractions</div>
                                </a>
                            </li>
                            <li data-chapter="11" data-section-id="ch11lvl1sec87" class="sub-nav">
                                <a href="#ch11lvl1sec87">                    
                                    <div class="section-name">AWS resources</div>
                                </a>
                            </li>
                            <li data-chapter="11" data-section-id="ch11lvl1sec88" class="sub-nav">
                                <a href="#ch11lvl1sec88">                    
                                    <div class="section-name">Sources of information</div>
                                </a>
                            </li>
                            <li data-chapter="11" data-section-id="ch11lvl1sec89" class="sub-nav">
                                <a href="#ch11lvl1sec89">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapsebackindex">
                                <div class="section-name">Appendix A: Index</div>
                            </a>
                        </li>
                        <div id="collapsebackindex" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="backindex" class="sub-nav">
                                <a href="#backindex">
                                    <div class="section-name">Chapter Appendix A: Index</div>
                                </a>
                            </li>
                        </div>
                    </div>
                </div>
            </ul>
        </div>
        
        <div id="page-content-wrapper" class="book-page">
            <a href="#" id="menu-toggle" class="toggle-nav"><i class="fa fa-bars fa-2x mr5"></i></a>
            
            <a href="#" id="back_to_top" class="back-to-top"><img src="../../mapt/images/kopimi.svg"></a>
            
            <div class="col-sm-12 col-xl-offset-2 col-xl-8 col-lg-offset-1 col-lg-10">
                <div class="btn-group pull-right mt15 mb30" role="group">
                    <a href="#home" class="btn btn-default">
                        <i class="fa fa-share fa-lg no-text-padding"></i>
                        <span class="hidden-xs ml5">Book Home</span>
                    </a>
                    <button class="btn btn-default" data-nid="16772" id="code-download">
                        <i class="fa fa-file fa-lg"></i>
                        <span class="hidden-xs ml5">Download Code Files</span>
                    </button>
                </div>
            </div>
            <div class="clearfix"></div>
            
            <div id="book-wrapper" class="container-fluid">
                <div class="col-sm-12 col-xl-offset-2 col-xl-8 col-lg-offset-1 col-lg-10" id="home">
                    <h2 class="product-title">Learning Hadoop 2</h2>
                    <hr>
                    <div class="row">
                        <div class="col-sm-12">
                            <h5 class="mt10">By Garry Turkington, Gabriele Modena</h5>
                            <div>
                                <p class="mb20"><b>Design and implement data processing, lifecycle management, and analytic workflows with the cutting-edge toolbox of Hadoop 2</b></p>
                                <a href="#ch01" class="btn btn-info btn-lg pull-right hidden-xs">
                                    Start Reading <i class="fa fa-chevron-right ml5"></i>
                                </a>
                                <a href="#ch01" class="btn btn-info btn-lg btn-block mt20 mb20 visible-xs">
                                    Start Reading <i class="fa fa-chevron-right ml5"></i>
                                </a>
                                <div class="clearfix"></div>
                                <div class="col-sm-12">
                                    <ul id="myTabs" class="nav nav-tabs nav-justified hidden-xs mt20" role="tablist">
                                        <li class="active">
                                            <a href="#info" role="tab" id="info-tab" data-toggle="tab">
                                                <h5>Info</h5>
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#content" role="tab" id="content-tab" data-toggle="tab">
                                                <h5>Contents</h5>
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#author" role="tab" id="author-tab" data-toggle="tab">
                                                <h5>Author</h5>
                                            </a>
                                        </li>
                                    </ul>
                
                                    <ul id="myTabsMobile" class="nav nav-pills text-center nav-stacked visible-xs mb60" role="tablist">
                                        <li class="active">
                                            <a href="#info" role="tab" id="info-tab-responsive" data-toggle="tab">
                                                Info
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#content" role="tab" id="content-tab-responsive" data-toggle="tab">
                                                Contents
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#author" role="tab" id="author-tab-responsive" data-toggle="tab">
                                                Author
                                            </a>
                                        </li>
                                    </ul>
                
                                    <div id="myTabContent" class="tab-content pt30">
                                    
                                        <div role="tabpanel" class="tab-pane active fade in" id="info">
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">Features</h5>
                                            </div>
                                            <div class="hidden-xs">
                                                <h5>Features</h5>
                                            </div>
                                            <hr>
                                            <div>
                                                <ul>
                <li>Construct state-of-the-art applications using higher-level interfaces and tools beyond the traditional MapReduce approach</li>
                <li>Use the unique features of Hadoop 2 to model and analyze Twitterâ€™s global stream of user generated data</li>
                <li>Develop a prototype on a local cluster and deploy to the cloud (Amazon Web Services)</li>
                </ul>
                                            </div>
                                            <br>
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">Learning</h5>
                                            </div>
                                            <div class="hidden-xs">
                                                <h5>Learning</h5>
                                            </div>
                                            <hr>
                                            <div>
                                                <ul>
                <li>Write distributed applications using the MapReduce framework</li>
                <li>Go beyond MapReduce and process data in real time with Samza and iteratively with Spark</li>
                <li>Familiarize yourself with data mining approaches that work with very large datasets</li>
                <li>Prototype applications on a VM and deploy them to a local cluster or to a cloud infrastructure (Amazon Web Services)</li>
                <li>Conduct batch and real time data analysis using SQL-like tools</li>
                <li>Build data processing flows using Apache Pig and see how it enables the easy incorporation of custom functionality</li>
                <li>Define and orchestrate complex workflows and pipelines with Apache Oozie</li>
                <li>Manage your data lifecycle and changes over time</li>
                </ul>
                                            </div>
                                            <br>
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">About</h5>
                                            </div>
                                            <div class="hidden-xs">
                                                <h5>About</h5>
                                            </div>
                                            <hr>
                                            <div>
                                                <p>This book introduces you to the world of building data-processing applications with the wide variety of tools supported by Hadoop 2. Starting with the core components of the frameworkâ€”HDFS and YARNâ€”this book will guide you through how to build applications using a variety of approaches.</p>
                <p>You will learn how YARN completely changes the relationship between MapReduce and Hadoop and allows the latter to support more varied processing approaches and a broader array of applications. These include real-time processing with Apache Samza and iterative computation with Apache Spark. Next up, we discuss Apache Pig and the dataflow data model it provides. You will discover how to use Pig to analyze a Twitter dataset.</p>
                <p>With this book, you will be able to make your life easier by using tools such as Apache Hive, Apache Oozie, Hadoop Streaming, Apache Crunch, and Kite SDK. The last part of this book discusses the likely future direction of major Hadoop components and how to get involved with the Hadoop community.</p>
                                            </div>
                                        </div>
                                        
                                        <div role="tabpanel" class="tab-pane fade in" id="content">
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">Contents</h5>
                                                <hr>
                                            </div>
                                            <ul>
                                                <div>
                                                    <li data-chapter="1">
                                                        <div class="section-name">1: Introduction</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="1" class="chapter-section">
                                                                    <a href="#ch01">        
                                                                        <div class="section-name">Chapter 1: Introduction</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec08" class="chapter-section">
                                                                    <a href="#ch01lvl1sec08">                    
                                                                        <div class="section-name">A note on versioning</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec09" class="chapter-section">
                                                                    <a href="#ch01lvl1sec09">                    
                                                                        <div class="section-name">The background of Hadoop</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec10" class="chapter-section">
                                                                    <a href="#ch01lvl1sec10">                    
                                                                        <div class="section-name">Components of Hadoop</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec11" class="chapter-section">
                                                                    <a href="#ch01lvl1sec11">                    
                                                                        <div class="section-name">Hadoop 2 what&#x27;s the big deal?</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec12" class="chapter-section">
                                                                    <a href="#ch01lvl1sec12">                    
                                                                        <div class="section-name">Distributions of Apache Hadoop</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec13" class="chapter-section">
                                                                    <a href="#ch01lvl1sec13">                    
                                                                        <div class="section-name">A dual approach</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec14" class="chapter-section">
                                                                    <a href="#ch01lvl1sec14">                    
                                                                        <div class="section-name">AWS infrastructure on demand from Amazon</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec15" class="chapter-section">
                                                                    <a href="#ch01lvl1sec15">                    
                                                                        <div class="section-name">Getting started</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec16" class="chapter-section">
                                                                    <a href="#ch01lvl1sec16">                    
                                                                        <div class="section-name">Running the examples</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec17" class="chapter-section">
                                                                    <a href="#ch01lvl1sec17">                    
                                                                        <div class="section-name">Data processing with Hadoop</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec18" class="chapter-section">
                                                                    <a href="#ch01lvl1sec18">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="2">
                                                        <div class="section-name">2: Storage</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="2" class="chapter-section">
                                                                    <a href="#ch02">        
                                                                        <div class="section-name">Chapter 2: Storage</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec19" class="chapter-section">
                                                                    <a href="#ch02lvl1sec19">                    
                                                                        <div class="section-name">The inner workings of HDFS</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec20" class="chapter-section">
                                                                    <a href="#ch02lvl1sec20">                    
                                                                        <div class="section-name">Command-line access to the HDFS filesystem</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec21" class="chapter-section">
                                                                    <a href="#ch02lvl1sec21">                    
                                                                        <div class="section-name">Protecting the filesystem metadata</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec22" class="chapter-section">
                                                                    <a href="#ch02lvl1sec22">                    
                                                                        <div class="section-name">Apache ZooKeeper a different type of filesystem</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec23" class="chapter-section">
                                                                    <a href="#ch02lvl1sec23">                    
                                                                        <div class="section-name">Automatic NameNode failover</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec24" class="chapter-section">
                                                                    <a href="#ch02lvl1sec24">                    
                                                                        <div class="section-name">HDFS snapshots</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec25" class="chapter-section">
                                                                    <a href="#ch02lvl1sec25">                    
                                                                        <div class="section-name">Hadoop filesystems</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec26" class="chapter-section">
                                                                    <a href="#ch02lvl1sec26">                    
                                                                        <div class="section-name">Managing and serializing data</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec27" class="chapter-section">
                                                                    <a href="#ch02lvl1sec27">                    
                                                                        <div class="section-name">Storing data</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec28" class="chapter-section">
                                                                    <a href="#ch02lvl1sec28">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="3">
                                                        <div class="section-name">3: Processing â€“ MapReduce and Beyond</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="3" class="chapter-section">
                                                                    <a href="#ch03">        
                                                                        <div class="section-name">Chapter 3: Processing â€“ MapReduce and Beyond</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec29" class="chapter-section">
                                                                    <a href="#ch03lvl1sec29">                    
                                                                        <div class="section-name">MapReduce</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec30" class="chapter-section">
                                                                    <a href="#ch03lvl1sec30">                    
                                                                        <div class="section-name">Java API to MapReduce</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec31" class="chapter-section">
                                                                    <a href="#ch03lvl1sec31">                    
                                                                        <div class="section-name">Writing MapReduce programs</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec32" class="chapter-section">
                                                                    <a href="#ch03lvl1sec32">                    
                                                                        <div class="section-name">Walking through a run of a MapReduce job</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec33" class="chapter-section">
                                                                    <a href="#ch03lvl1sec33">                    
                                                                        <div class="section-name">YARN</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec34" class="chapter-section">
                                                                    <a href="#ch03lvl1sec34">                    
                                                                        <div class="section-name">YARN in the real world Computation beyond MapReduce</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec35" class="chapter-section">
                                                                    <a href="#ch03lvl1sec35">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="4">
                                                        <div class="section-name">4: Real-time Computation with Samza</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="4" class="chapter-section">
                                                                    <a href="#ch04">        
                                                                        <div class="section-name">Chapter 4: Real-time Computation with Samza</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec36" class="chapter-section">
                                                                    <a href="#ch04lvl1sec36">                    
                                                                        <div class="section-name">Stream processing with Samza</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec37" class="chapter-section">
                                                                    <a href="#ch04lvl1sec37">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="5">
                                                        <div class="section-name">5: Iterative Computation with Spark</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="5" class="chapter-section">
                                                                    <a href="#ch05">        
                                                                        <div class="section-name">Chapter 5: Iterative Computation with Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec38" class="chapter-section">
                                                                    <a href="#ch05lvl1sec38">                    
                                                                        <div class="section-name">Apache Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec39" class="chapter-section">
                                                                    <a href="#ch05lvl1sec39">                    
                                                                        <div class="section-name">The Spark ecosystem</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec40" class="chapter-section">
                                                                    <a href="#ch05lvl1sec40">                    
                                                                        <div class="section-name">Processing data with Apache Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec41" class="chapter-section">
                                                                    <a href="#ch05lvl1sec41">                    
                                                                        <div class="section-name">Comparing Samza and Spark Streaming</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec42" class="chapter-section">
                                                                    <a href="#ch05lvl1sec42">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="6">
                                                        <div class="section-name">6: Data Analysis with Apache Pig</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="6" class="chapter-section">
                                                                    <a href="#ch06">        
                                                                        <div class="section-name">Chapter 6: Data Analysis with Apache Pig</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec43" class="chapter-section">
                                                                    <a href="#ch06lvl1sec43">                    
                                                                        <div class="section-name">An overview of Pig</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec44" class="chapter-section">
                                                                    <a href="#ch06lvl1sec44">                    
                                                                        <div class="section-name">Getting started</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec45" class="chapter-section">
                                                                    <a href="#ch06lvl1sec45">                    
                                                                        <div class="section-name">Running Pig</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec46" class="chapter-section">
                                                                    <a href="#ch06lvl1sec46">                    
                                                                        <div class="section-name">Fundamentals of Apache Pig</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec47" class="chapter-section">
                                                                    <a href="#ch06lvl1sec47">                    
                                                                        <div class="section-name">Programming Pig</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec48" class="chapter-section">
                                                                    <a href="#ch06lvl1sec48">                    
                                                                        <div class="section-name">Extending Pig (UDFs)</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec49" class="chapter-section">
                                                                    <a href="#ch06lvl1sec49">                    
                                                                        <div class="section-name">Analyzing the Twitter stream</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec50" class="chapter-section">
                                                                    <a href="#ch06lvl1sec50">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="7">
                                                        <div class="section-name">7: Hadoop and SQL</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="7" class="chapter-section">
                                                                    <a href="#ch07">        
                                                                        <div class="section-name">Chapter 7: Hadoop and SQL</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec51" class="chapter-section">
                                                                    <a href="#ch07lvl1sec51">                    
                                                                        <div class="section-name">Why SQL on Hadoop</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec52" class="chapter-section">
                                                                    <a href="#ch07lvl1sec52">                    
                                                                        <div class="section-name">Prerequisites</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec53" class="chapter-section">
                                                                    <a href="#ch07lvl1sec53">                    
                                                                        <div class="section-name">Hive architecture</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec54" class="chapter-section">
                                                                    <a href="#ch07lvl1sec54">                    
                                                                        <div class="section-name">Hive and Amazon Web Services</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec55" class="chapter-section">
                                                                    <a href="#ch07lvl1sec55">                    
                                                                        <div class="section-name">Extending HiveQL</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec56" class="chapter-section">
                                                                    <a href="#ch07lvl1sec56">                    
                                                                        <div class="section-name">Programmatic interfaces</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec57" class="chapter-section">
                                                                    <a href="#ch07lvl1sec57">                    
                                                                        <div class="section-name">Stinger initiative</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec58" class="chapter-section">
                                                                    <a href="#ch07lvl1sec58">                    
                                                                        <div class="section-name">Impala</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec59" class="chapter-section">
                                                                    <a href="#ch07lvl1sec59">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="8">
                                                        <div class="section-name">8: Data Lifecycle Management</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="8" class="chapter-section">
                                                                    <a href="#ch08">        
                                                                        <div class="section-name">Chapter 8: Data Lifecycle Management</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec60" class="chapter-section">
                                                                    <a href="#ch08lvl1sec60">                    
                                                                        <div class="section-name">What data lifecycle management is</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec61" class="chapter-section">
                                                                    <a href="#ch08lvl1sec61">                    
                                                                        <div class="section-name">Building a tweet analysis capability</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec62" class="chapter-section">
                                                                    <a href="#ch08lvl1sec62">                    
                                                                        <div class="section-name">Challenges of external data</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec63" class="chapter-section">
                                                                    <a href="#ch08lvl1sec63">                    
                                                                        <div class="section-name">Collecting additional data</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec64" class="chapter-section">
                                                                    <a href="#ch08lvl1sec64">                    
                                                                        <div class="section-name">Pulling it all together</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec65" class="chapter-section">
                                                                    <a href="#ch08lvl1sec65">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="9">
                                                        <div class="section-name">9: Making Development Easier</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="9" class="chapter-section">
                                                                    <a href="#ch09">        
                                                                        <div class="section-name">Chapter 9: Making Development Easier</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec66" class="chapter-section">
                                                                    <a href="#ch09lvl1sec66">                    
                                                                        <div class="section-name">Choosing a framework</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec67" class="chapter-section">
                                                                    <a href="#ch09lvl1sec67">                    
                                                                        <div class="section-name">Hadoop streaming</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec68" class="chapter-section">
                                                                    <a href="#ch09lvl1sec68">                    
                                                                        <div class="section-name">Kite Data</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec69" class="chapter-section">
                                                                    <a href="#ch09lvl1sec69">                    
                                                                        <div class="section-name">Apache Crunch</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec70" class="chapter-section">
                                                                    <a href="#ch09lvl1sec70">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="10">
                                                        <div class="section-name">10: Running a Hadoop Cluster</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="10" class="chapter-section">
                                                                    <a href="#ch10">        
                                                                        <div class="section-name">Chapter 10: Running a Hadoop Cluster</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec71" class="chapter-section">
                                                                    <a href="#ch10lvl1sec71">                    
                                                                        <div class="section-name">I&#x27;m a developer I don&#x27;t care about operations!</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec72" class="chapter-section">
                                                                    <a href="#ch10lvl1sec72">                    
                                                                        <div class="section-name">Cloudera Manager</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec73" class="chapter-section">
                                                                    <a href="#ch10lvl1sec73">                    
                                                                        <div class="section-name">Ambari the open source alternative</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec74" class="chapter-section">
                                                                    <a href="#ch10lvl1sec74">                    
                                                                        <div class="section-name">Operations in the Hadoop 2 world</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec75" class="chapter-section">
                                                                    <a href="#ch10lvl1sec75">                    
                                                                        <div class="section-name">Sharing resources</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec76" class="chapter-section">
                                                                    <a href="#ch10lvl1sec76">                    
                                                                        <div class="section-name">Building a physical cluster</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec77" class="chapter-section">
                                                                    <a href="#ch10lvl1sec77">                    
                                                                        <div class="section-name">Building a cluster on EMR</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec78" class="chapter-section">
                                                                    <a href="#ch10lvl1sec78">                    
                                                                        <div class="section-name">Cluster tuning</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec79" class="chapter-section">
                                                                    <a href="#ch10lvl1sec79">                    
                                                                        <div class="section-name">Security</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec80" class="chapter-section">
                                                                    <a href="#ch10lvl1sec80">                    
                                                                        <div class="section-name">Monitoring</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec81" class="chapter-section">
                                                                    <a href="#ch10lvl1sec81">                    
                                                                        <div class="section-name">Troubleshooting</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec82" class="chapter-section">
                                                                    <a href="#ch10lvl1sec82">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="11">
                                                        <div class="section-name">11: Where to Go Next</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="11" class="chapter-section">
                                                                    <a href="#ch11">        
                                                                        <div class="section-name">Chapter 11: Where to Go Next</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="11" data-section-id="ch11lvl1sec83" class="chapter-section">
                                                                    <a href="#ch11lvl1sec83">                    
                                                                        <div class="section-name">Alternative distributions</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="11" data-section-id="ch11lvl1sec84" class="chapter-section">
                                                                    <a href="#ch11lvl1sec84">                    
                                                                        <div class="section-name">Other computational frameworks</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="11" data-section-id="ch11lvl1sec85" class="chapter-section">
                                                                    <a href="#ch11lvl1sec85">                    
                                                                        <div class="section-name">Other interesting projects</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="11" data-section-id="ch11lvl1sec86" class="chapter-section">
                                                                    <a href="#ch11lvl1sec86">                    
                                                                        <div class="section-name">Other programming abstractions</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="11" data-section-id="ch11lvl1sec87" class="chapter-section">
                                                                    <a href="#ch11lvl1sec87">                    
                                                                        <div class="section-name">AWS resources</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="11" data-section-id="ch11lvl1sec88" class="chapter-section">
                                                                    <a href="#ch11lvl1sec88">                    
                                                                        <div class="section-name">Sources of information</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="11" data-section-id="ch11lvl1sec89" class="chapter-section">
                                                                    <a href="#ch11lvl1sec89">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="backindex">
                                                        <div class="section-name">Appendix A: Index</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="backindex" class="chapter-section">
                                                                    <a href="#backindex">
                                                                        <div class="section-name">Chapter Appendix A: Index</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                            </ul>
                                        </div>
                                        
                                        <div role="tabpanel" class="tab-pane fade" id="author">
                                            <div class="visible-xs">
                                                <h4 class="mobile-title">About the Author</h4>
                                                <hr>
                                            </div>
                                            <p><strong>Garry Turkington</strong></p>
                                            <div>
                                                <p>Garry Turkington has over 15 years of industry experience, most of which has been focused on the design and implementation of large-scale distributed systems. In his current role as the CTO at Improve Digital, he is primarily responsible for the realization of systems that store, process, and extract value from the company's large data volumes. Before joining Improve Digital, he spent time at Amazon.co.uk, where he led several software development teams, building systems that process the Amazon catalog data for every item worldwide. Prior to this, he spent a decade in various government positions in both the UK and the USA.</p>
                                            </div>
                                            <p><strong>Gabriele Modena</strong></p>
                                            <div>
                                                <p>Gabriele Modena is a data scientist at Improve Digital. In his current position, he uses Hadoop to manage, process, and analyze behavioral and machine-generated data. Gabriele enjoys using statistical and computational methods to look for patterns in large amounts of data. Prior to his current job in ad tech he held a number of positions in Academia and Industry where he did research in machine learning and artificial intelligence.</p>
                <p>He holds a BSc degree in Computer Science from the University of Trento, Italy and a Research MSc degree in Artificial Intelligence: Learning Systems, from the University of Amsterdam in the Netherlands.</p>
                                            </div>
                                        </div>
                                        
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    
                    <div class="next-wrapper">
                        <div class="row ns">
                            <hr />
                            <span class="hidden-xs">
                                <h4 class="pull-left">
                                    <strong>Up Next: </strong><span class="section-title"></span>
                                </h4>
                                <a href="#" class="btn btn-primary pull-right btn-lg">
                                    Next Section
                                </a>
                            </span>
                            <span class="visible-xs">
                                <a href="#" class="btn btn-primary btn-block btn-lg">
                                    Next Section
                                </a>
                            </span>
                        </div>
                        <div class="row ns">
                            <hr>
                        </div>
                    </div>
                </div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch01"></a>ChapterÂ 1.Â Introduction</h2></div></div></div><p>This book will teach you how to build amazing systems using the latest release of Hadoop. Before you change the world though, we need to do some groundwork, which is where this chapter comes in.</p><p>In this introductory chapter, we will cover the following topics:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>A brief refresher on the background to Hadoop</p></li><li style="list-style-type: disc"><p>A walk-through of Hadoop's evolution</p></li><li style="list-style-type: disc"><p>The key elements in Hadoop 2 </p></li><li style="list-style-type: disc"><p>The Hadoop distributions we'll use in this book</p></li><li style="list-style-type: disc"><p>The dataset we'll use for examples</p></li></ul></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec08"></a>A note on versioning</h2></div></div><hr /></div><p>In Hadoop 1, the <a id="id0" class="indexterm"></a>version history was somewhat convoluted with multiple <a id="id1" class="indexterm"></a>forked branches in the 0.2x range, leading to odd situations, where a 1.x version could, in some situations, have fewer features than a 0.23 release. In the version 2 codebase, this is fortunately much more straightforward, but it's important to clarify exactly which version we will use in this book.</p><p>Hadoop 2.0 was released in alpha and beta versions, and along the way, several incompatible changes were introduced. There was, in particular, a major API stabilization effort between the beta and final release stages.</p><p>Hadoop 2.2.0 was the first <a id="id2" class="indexterm"></a>
<span class="strong"><strong>general availability</strong></span> (<span class="strong"><strong>GA</strong></span>) release of the Hadoop 2 codebase, and its interfaces are now declared stable and forward compatible. We will therefore use the 2.2 product and interfaces in this book. Though the principles will be usable on a 2.0 beta, in particular, there will be API incompatibilities in the beta. This is particularly important as MapReduce v2 was back-ported to Hadoop 1 by several distribution vendors, but these products were based on the beta and not the GA APIs. If you are using such a product, then you will encounter these incompatible changes. It is recommended that a release based upon Hadoop 2.2 or later is used for both the development and the production deployments of any Hadoop 2 workloads.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec09"></a>The background of Hadoop</h2></div></div><hr /></div><p>We're<a id="id3" class="indexterm"></a> assuming that most readers will have a little familiarity with Hadoop, or at the very least, with big data-processing systems. Consequently, we won't give a detailed background as to why Hadoop is successful or the types of problem it helps to solve in this book. However, particularly because of some aspects of Hadoop 2 and the other products we will use in later chapters, it is useful to give a sketch of how we see Hadoop fitting into the technology landscape and which are the particular problem areas where we believe it gives the most benefit.</p><p>In ancient times, before the term "big data" came into the picture (which equates to maybe a decade ago), there were few options to process datasets of sizes in terabytes and beyond. Some commercial databases could, with very specific and expensive hardware setups, be scaled to this level, but the expertise and capital expenditure required made it an option for only the largest organizations. Alternatively, one could build a custom system aimed at the specific problem at hand. This suffered from some of the same problems (expertise and cost) and added the risk inherent in any cutting-edge system. On the other hand, if a system was successfully constructed, it was likely a very good fit to the need.</p><p>Few small- to mid-size companies even worried about this space, not only because the solutions were out of their reach, but they generally also didn't have anything close to the data volumes that required such solutions. As the ability to generate very large datasets became more common, so did the need to process that data.</p><p>Even though large data became more democratized and was no longer the domain of the privileged few, major architectural changes were required if the data-processing systems could be made affordable to smaller companies. The first big change was to reduce the required upfront capital expenditure on the system; that means no high-end hardware or expensive software licenses. Previously, high-end hardware would have been utilized most commonly in a relatively small number of very large servers and storage systems, each of which had multiple approaches to avoid hardware failures. Though very impressive, such systems are hugely expensive, and moving to a larger number of lower-end servers would be the quickest way to dramatically reduce the hardware cost of a new system. Moving more toward commodity hardware instead of the traditional enterprise-grade equipment would also mean a reduction in capabilities in the area of resilience and fault tolerance. Those responsibilities would need to be taken up by the software layer. <span class="emphasis"><em>Smarter software, dumber hardware</em></span>.</p><p>Google started the change that would eventually be known as Hadoop, when in 2003, and in 2004, they released two academic papers describing the <a id="id4" class="indexterm"></a>
<span class="strong"><strong>Google File System</strong></span> (<span class="strong"><strong>GFS</strong></span>) (<a class="ulink" href="http://research.google.com/archive/gfs.html" target="_blank">http://research.google.com/archive/gfs.html</a>) and <a id="id5" class="indexterm"></a>MapReduce (<a class="ulink" href="http://research.google.com/archive/mapreduce.html" target="_blank">http://research.google.com/archive/mapreduce.html</a>). The two together provided a platform for very large-scale data processing in a highly efficient manner. Google had taken the build-it-yourself approach, but instead of constructing something aimed at one specific problem or dataset, they instead created a platform on which multiple processing applications could be implemented. In particular, they utilized large numbers of commodity servers and built GFS and MapReduce in a way that assumed hardware failures would be commonplace and were simply something that the software needed to deal with.</p><p>At the same time, Doug Cutting <a id="id6" class="indexterm"></a>was working on the Nutch open source web crawler. He was working on elements within the system that resonated strongly once the Google GFS and MapReduce papers were published. Doug started work on open source implementations of these Google ideas, and Hadoop was soon born, firstly, as a subproject of Lucene, and then as its own top-level project within the Apache Software Foundation.</p><p>Yahoo! hired Doug Cutting in 2006 and quickly became one of the most prominent supporters of the Hadoop project. In addition to often publicizing some of the largest Hadoop deployments in the world, Yahoo! allowed Doug and other engineers to contribute to Hadoop while employed by the company, not to mention contributing back some of its own internally developed Hadoop improvements and extensions.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec10"></a>Components of Hadoop</h2></div></div><hr /></div><p>The<a id="id7" class="indexterm"></a> broad Hadoop <a id="id8" class="indexterm"></a>umbrella project has many component subprojects, and we'll discuss several of them in this book. At its core, Hadoop provides two services: storage and computation. A typical Hadoop workflow consists of loading data into the<a id="id9" class="indexterm"></a> <span class="strong"><strong>Hadoop Distributed File System</strong></span> (<span class="strong"><strong>HDFS</strong></span>) and processing using the <span class="strong"><strong>MapReduce</strong></span> API<a id="id10" class="indexterm"></a> or several tools that rely on MapReduce as an execution framework.</p><div class="mediaobject"><img src="graphics/5518_01_01.jpg" /><div class="caption"><p>Hadoop 1: HDFS and MapReduce</p></div></div><p>Both layers are direct<a id="id11" class="indexterm"></a> implementations of Google's own GFS and MapReduce technologies.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec07"></a>Common building blocks</h3></div></div></div><p>Both HDFS and MapReduce exhibit <a id="id12" class="indexterm"></a>several of the architectural principles described in the previous section. In particular, the <a id="id13" class="indexterm"></a>common principles are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Both are designed to run on clusters of commodity (that is, low to medium specification) servers</p></li><li style="list-style-type: disc"><p>Both scale their capacity by adding more servers (scale-out) as opposed to the previous models of using larger hardware (scale-up)</p></li><li style="list-style-type: disc"><p>Both have mechanisms to identify and work around failures</p></li><li style="list-style-type: disc"><p>Both provide most of their services transparently, allowing the user to concentrate on the problem at hand</p></li><li style="list-style-type: disc"><p>Both have an architecture where a software cluster sits on the physical servers and manages aspects such as application load balancing and fault tolerance, without relying on high-end hardware to deliver these capabilities</p></li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec08"></a>Storage</h3></div></div></div><p>HDFS<a id="id14" class="indexterm"></a> is a<a id="id15" class="indexterm"></a> filesystem, though not a POSIX-compliant one. This<a id="id16" class="indexterm"></a> basically means that it does not display the same characteristics as that of a regular filesystem. In particular, the characteristics<a id="id17" class="indexterm"></a> are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>HDFS stores files in blocks that are typically at least 64 MB or (more commonly now) 128 MB in size, much larger than the 4-32 KB seen in most filesystems</p></li><li style="list-style-type: disc"><p>HDFS is optimized for throughput over latency; it is very efficient at streaming reads of large files but poor when seeking for many small ones</p></li><li style="list-style-type: disc"><p>HDFS is optimized for workloads that are generally write-once and read-many</p></li><li style="list-style-type: disc"><p>Instead of handling disk failures by having physical redundancies in disk arrays or similar strategies, HDFS uses replication. Each of the blocks comprising a file is stored on multiple nodes within the cluster, and a service called the NameNode constantly monitors to ensure that failures have not dropped any block below the desired replication factor. If this does happen, then it schedules the making of another copy within the cluster.</p></li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec09"></a>Computation</h3></div></div></div><p>MapReduce <a id="id18" class="indexterm"></a>is an API, an<a id="id19" class="indexterm"></a> execution engine, and a processing paradigm; it provides a series of<a id="id20" class="indexterm"></a> transformations from a source into a result dataset. In the simplest case, the input data is fed through a map function and the resultant temporary data is then fed through a reduce function.</p><p>MapReduce works best on semistructured or unstructured data. Instead of data conforming to rigid schemas, the requirement is instead that the data can be provided to the map function as a series of key-value pairs. The output of the map function is a set of other key-value pairs, and the reduce function performs aggregation to collect the final set of results.</p><p>Hadoop provides a standard specification (that is, interface) for the map and reduce phases, and the implementation of these are often referred to as mappers and reducers. A typical MapReduce application will comprise a number of mappers and reducers, and it's not unusual for several of these to be extremely simple. The developer focuses on expressing the transformation between the source and the resultant data, and the Hadoop framework manages all aspects of job execution and coordination.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec10"></a>Better together</h3></div></div></div><p>It is possible to appreciate the individual merits of HDFS and MapReduce, but they are even more powerful when combined. They can be used individually, but when they are together, they bring out the best in each other, and this close interworking was a major factor in the success and acceptance of Hadoop 1.</p><p>When a MapReduce job is being planned, Hadoop needs to decide on which host to execute the code in order to process the dataset most efficiently. If the MapReduce cluster hosts are all pulling their data from a single storage host or array, then this largely doesn't matter as the storage system is a shared resource that will cause contention. If the storage system was more transparent and allowed MapReduce to manipulate its data more directly, then there would be an opportunity to perform the processing closer to the data, building on the principle of it being less expensive to move processing than data.</p><p>The most common<a id="id21" class="indexterm"></a> deployment model for Hadoop sees the HDFS and MapReduce clusters deployed on the same set of servers. Each host that contains data and the HDFS component to manage the data also hosts a MapReduce component that can schedule and execute data processing. When a job is submitted to Hadoop, it can use the locality optimization to schedule data on the hosts where data resides as much as possible, thus minimizing network traffic and maximizing performance.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec11"></a>Hadoop 2 â€“ what's the big deal?</h2></div></div><hr /></div><p>If we look <a id="id22" class="indexterm"></a>at the two main components of the core Hadoop distribution, storage and computation, we see that Hadoop 2 has a very different impact on each of them. Whereas the HDFS found in Hadoop 2 is mostly a much more feature-rich and resilient product than the HDFS in Hadoop 1, for MapReduce, the changes are much more profound and have, in fact, altered how Hadoop is perceived as a processing platform in general. Let's look at HDFS in Hadoop 2 first.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec11"></a>Storage in Hadoop 2</h3></div></div></div><p>We'll <a id="id23" class="indexterm"></a>discuss <a id="id24" class="indexterm"></a>the HDFS architecture in more detail in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Storage</em></span>, but for now, it's sufficient to think of a master-slave model. The slave nodes (called DataNodes) hold<a id="id25" class="indexterm"></a> the actual filesystem data. In particular, each host running a DataNode will typically have one or more disks onto which files containing the data for each HDFS block are written. The DataNode itself has no understanding of the overall filesystem; its role is to store, serve, and ensure the integrity of the data for which it is responsible.</p><p>The master node (called the NameNode) is <a id="id26" class="indexterm"></a>responsible for knowing which of the DataNodes holds which block and how these blocks are structured to form the filesystem. When a client looks at the filesystem and wishes to retrieve a file, it's via a request to the NameNode that the list of required blocks is retrieved.</p><p>This model <a id="id27" class="indexterm"></a>works well and has been scaled to clusters with tens of thousands <a id="id28" class="indexterm"></a>of nodes at companies such as Yahoo! So, though it is scalable, there is a resiliency risk; if the NameNode becomes unavailable, then the entire cluster is rendered effectively useless. No HDFS operations can be performed, and since the vast majority of installations use HDFS as the storage layer for services, such as MapReduce, they also become unavailable even if they are still running without problems.</p><p>More catastrophically, the NameNode stores the filesystem metadata to a persistent file on its local filesystem. If the NameNode host crashes in a way that this data is not recoverable, then all data on the cluster is effectively lost forever. The data will still exist on the various DataNodes, but the mapping of which blocks comprise which files is lost. This is why, in Hadoop 1, the best practice was to have the NameNode synchronously write its filesystem metadata to both local disks and at least one remote network volume (typically via NFS).</p><p>Several NameNode <span class="strong"><strong>high-availability</strong></span> (<span class="strong"><strong>HA</strong></span>)<a id="id29" class="indexterm"></a> solutions have been made available by third-party suppliers, but the core Hadoop product did not offer such resilience in Version 1. Given this architectural single point of failure and the risk of data loss, it won't be a surprise to hear that <span class="strong"><strong>NameNode HA</strong></span><a id="id30" class="indexterm"></a> is one of the major features of HDFS in Hadoop 2 and is something we'll discuss in detail in later chapters. The feature provides both a standby NameNode that can be automatically promoted to service all requests should the active NameNode fail, but also builds additional resilience for the critical filesystem metadata atop this mechanism.</p><p>HDFS in Hadoop 2 is still a non-POSIX filesystem; it still has a very large block size and it still trades latency for throughput. However, it does now have a few capabilities that can make it look a little more like a traditional filesystem. In particular, the core HDFS in Hadoop 2 now can be remotely mounted as an NFS volume. This is another feature that was previously offered as a proprietary capability by third-party suppliers but is now in the main Apache codebase.</p><p>Overall, the HDFS in Hadoop 2 is more resilient and can be more easily integrated into existing workflows and processes. It's a strong evolution of the product found in Hadoop 1.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec12"></a>Computation in Hadoop 2</h3></div></div></div><p>The<a id="id31" class="indexterm"></a> work <a id="id32" class="indexterm"></a>on HDFS 2 was started before a direction for MapReduce crystallized. This was likely due to the fact that features such as NameNode HA were such an obvious path that the community knew the most critical areas to address. However, MapReduce didn't really have a similar list of areas of improvement, and that's why, when the MRv2 initiative started, it wasn't completely clear where it would lead.</p><p>Perhaps the most<a id="id33" class="indexterm"></a> frequent criticism of MapReduce in Hadoop 1 was how its <a id="id34" class="indexterm"></a>batch processing model was ill-suited to problem domains where faster response times were required. Hive, for example, which we'll discuss in <a class="link" href="#" linkend="ch07">Chapter 7</a>, <span class="emphasis"><em>Hadoop and SQL</em></span>, provides a SQL-like interface onto HDFS data, but, behind the scenes, the statements are converted into MapReduce jobs that are then executed like any other. A number of other products and tools took a similar approach, providing a specific user-facing interface that hid a MapReduce translation layer.</p><p>Though this approach has been very successful, and some amazing products have been built, the fact remains that in many cases, there is a mismatch as all of these interfaces, some of which expect a certain type of responsiveness, are behind the scenes, being executed on a batch-processing platform. When looking to enhance MapReduce, improvements could be made to make it a better fit to these use cases, but the fundamental mismatch would remain. This situation led to a significant change of focus of the MRv2 initiative; perhaps MapReduce itself didn't need change, but the real need was to enable different processing models on the Hadoop platform. Thus was born <a id="id35" class="indexterm"></a>
<span class="strong"><strong>Yet Another Resource Negotiator</strong></span> (<span class="strong"><strong>YARN</strong></span>).</p><p>Looking at MapReduce in Hadoop 1, the product actually did two quite different things; it provided the processing framework to execute MapReduce computations, but it also managed the allocation of this computation across the cluster. Not only did it direct data to and between the specific map and reduce tasks, but it also determined where each task would run, and managed the full job life cycle, monitoring the health of each task and node, rescheduling if any failed, and so on.</p><p>This is not a trivial task, and the automated parallelization of workloads has always been one of the main benefits of Hadoop. If we look at MapReduce in Hadoop 1, we see that after the user defines the key criteria for the job, everything else is the responsibility of the system. Critically, from a scale perspective, the same MapReduce job can be applied to datasets of any volume hosted on clusters of any size. If the data is 1 GB in size and on a single host, then Hadoop will schedule the processing accordingly. If the data is instead 1 PB in size and hosted across 1,000 machines, then it does likewise. From the user's perspective, the actual scale of the data and cluster is transparent, and aside from affecting the time taken to process the job, it does not change the interface with which to interact with the system.</p><p>In Hadoop 2, this role of job scheduling and resource management is separated from that of executing the actual application, and is implemented by YARN.</p><p>YARN is responsible for managing the cluster resources, and so MapReduce exists as an <a id="id36" class="indexterm"></a>application that runs atop the YARN framework. The MapReduce interface in Hadoop 2 is completely compatible with that in Hadoop 1, both semantically and practically. However, under the covers, MapReduce has become a hosted application on the YARN framework.</p><p>The significance <a id="id37" class="indexterm"></a>of this split is that other applications can be written that provide processing models more focused on the actual problem domain and can offload all the <a id="id38" class="indexterm"></a>resource management and scheduling responsibilities to YARN. The latest versions of many different execution engines have been ported onto YARN, either in a production-ready or experimental state, and it has shown that the approach can allow a single Hadoop cluster to run everything from batch-oriented MapReduce jobs through fast-response SQL queries to continuous data streaming and even to implement models such as graph processing and the<a id="id39" class="indexterm"></a> <span class="strong"><strong>Message Passing Interface</strong></span> (<span class="strong"><strong>MPI</strong></span>) from the <a id="id40" class="indexterm"></a>
<span class="strong"><strong>High Performance Computing</strong></span> (<span class="strong"><strong>HPC</strong></span>) world. The following <a id="id41" class="indexterm"></a>diagram shows the architecture of Hadoop 2:</p><div class="mediaobject"><img src="graphics/5518_01_02.jpg" /><div class="caption"><p>Hadoop 2</p></div></div><p>This is why much of the attention and excitement around Hadoop 2 has been focused on YARN and frameworks that sit on top of it, such as Apache Tez and Apache Spark. With YARN, the Hadoop cluster is no longer just a batch-processing engine; it is the single platform on which a vast array of processing techniques can be applied to the enormous data volumes stored in HDFS. Moreover, applications can build on these computation paradigms and execution models.</p><p>The analogy that is <a id="id42" class="indexterm"></a>achieving some traction is to think of YARN as the processing kernel upon which other <a id="id43" class="indexterm"></a>domain-specific applications can be built. We'll discuss YARN in more detail in this book, particularly in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Processing â€“ MapReduce and Beyond</em></span>, <a class="link" href="#" linkend="ch04">Chapter 4</a>, <span class="emphasis"><em>Real-time Computation with Samza</em></span>, and <a class="link" href="#" linkend="ch05">Chapter 5</a>, <span class="emphasis"><em>Iterative Computation with Spark</em></span>.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec12"></a>Distributions of Apache Hadoop</h2></div></div><hr /></div><p>In the <a id="id44" class="indexterm"></a>very early days of Hadoop, the burden of installing (often building from source) and managing each component and its dependencies fell on the user. As the system became more popular and the ecosystem of third-party tools and libraries started to grow, the complexity of installing and managing a Hadoop deployment increased dramatically to the point where providing a coherent offer of software packages, documentation, and training built around the core Apache Hadoop has become a business model. Enter the world of distributions for Apache Hadoop.</p><p>Hadoop distributions are conceptually similar to how Linux distributions provide a set of integrated software around a common core. They take the burden of bundling and packaging software themselves and provide the user with an easy way to install, manage, and deploy Apache Hadoop and a selected number of third-party libraries. In particular, the distribution releases deliver a series of product versions that are certified to be mutually compatible. Historically, putting together a Hadoop-based platform was often greatly complicated<a id="id45" class="indexterm"></a> by the <a id="id46" class="indexterm"></a>various version interdependencies.</p><p>Cloudera (<a class="ulink" href="http://www.cloudera.com" target="_blank">http://www.cloudera.com</a>), Hortonworks (<a class="ulink" href="http://www.hortonworks.com" target="_blank">http://www.hortonworks.com</a>), and<a id="id47" class="indexterm"></a> MapR (<a class="ulink" href="http://www.mapr.com" target="_blank">http://www.mapr.com</a>) are <a id="id48" class="indexterm"></a>amongst the first to <a id="id49" class="indexterm"></a>have<a id="id50" class="indexterm"></a> reached the market, each characterized by different approaches and selling points. Hortonworks positions itself as the open source player; Cloudera is also committed to open source but adds proprietary bits for configuring and managing Hadoop; MapR provides a hybrid open source/proprietary Hadoop distribution characterized by a proprietary NFS layer instead of HDFS and a focus on providing services.</p><p>Another strong player in the distributions ecosystem is Amazon, which offers a version of Hadoop called <a id="id51" class="indexterm"></a>
<span class="strong"><strong>Elastic MapReduce</strong></span> (<span class="strong"><strong>EMR</strong></span>) on top of the <a id="id52" class="indexterm"></a>
<span class="strong"><strong>Amazon Web Services</strong></span> (<span class="strong"><strong>AWS</strong></span>) infrastructure.</p><p>With the advent of Hadoop 2, the number of available distributions for Hadoop has increased dramatically, far in excess of the four we mentioned. A possibly incomplete list of software offerings<a id="id53" class="indexterm"></a> that includes Apache Hadoop can be found at <a class="ulink" href="http://wiki.apache.org/hadoop/Distributions%20and%20Commercial%20Support" target="_blank">http://wiki.apache.org/hadoop/Distributions%20and%20Commercial%20Support</a>.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec13"></a>A dual approach</h2></div></div><hr /></div><p>In this<a id="id54" class="indexterm"></a> book, we will discuss both the building and the management of local Hadoop clusters in addition to showing how to push the processing into the cloud via EMR.</p><p>The reason for this is twofold: firstly, though EMR makes Hadoop much more accessible, there are aspects of the technology that only become apparent when manually administering the cluster. Although it is also possible to use EMR in a more manual mode, we'll generally use a local cluster for such explorations. Secondly, though it isn't necessarily an either/or decision, many organizations use a mixture of in-house and cloud-hosted capacities, sometimes due to a concern of over reliance on a single external provider, but practically speaking, it's often convenient to do development and small-scale tests on local capacity and then deploy at production scale into the cloud.</p><p>In a few of the later chapters, where we discuss additional products that integrate with Hadoop, we'll mostly give examples of local clusters, as there is no difference between how the products work regardless of where they are deployed.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec14"></a>AWS â€“ infrastructure on demand from Amazon</h2></div></div><hr /></div><p>AWS is <a id="id55" class="indexterm"></a>a set of cloud-computing services offered by Amazon. We will use several of these services in this book.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec13"></a>Simple Storage Service (S3)</h3></div></div></div><p>Amazon's <span class="strong"><strong>Simple Storage Service</strong></span> (<span class="strong"><strong>S3</strong></span>), found at <a class="ulink" href="http://aws.amazon.com/s3/" target="_blank">http://aws.amazon.com/s3/</a>, is a storage <a id="id56" class="indexterm"></a>service <a id="id57" class="indexterm"></a>that provides <a id="id58" class="indexterm"></a>a simple key-value storage model. Using web, command-line, or programmatic interfaces to create objects, which can be anything from text files to images to MP3s, you can store and retrieve your data based on a hierarchical model. In this model, you create buckets that contain objects. Each bucket has a unique identifier, and within each bucket, every object is uniquely named. This simple strategy enables an extremely powerful service for which Amazon takes complete responsibility (for service scaling, in addition to reliability and availability of data).</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec14"></a>Elastic MapReduce (EMR)</h3></div></div></div><p>Amazon's Elastic MapReduce, found <a id="id59" class="indexterm"></a>at <a class="ulink" href="http://aws.amazon.com/elasticmapreduce/" target="_blank">http://aws.amazon.com/elasticmapreduce/</a>, is basically Hadoop in the cloud. Using any <a id="id60" class="indexterm"></a>of the multiple interfaces (web console, CLI, or API), a Hadoop workflow is defined with attributes such as the number of Hadoop<a id="id61" class="indexterm"></a> hosts required and the location of the source data. The Hadoop code implementing the MapReduce jobs is provided, and the virtual Go button is pressed.</p><p>In its most impressive mode, EMR can pull source data from S3, process it on a Hadoop cluster it creates on Amazon's virtual host on-demand service EC2, push the results back into S3, and terminate the Hadoop cluster and the EC2 virtual machines hosting it. Naturally, each of these services has a cost (usually on per GB stored and server-time usage basis), but the ability to access such powerful data-processing capabilities with no need for dedicated hardware is a powerful one.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec15"></a>Getting started</h2></div></div><hr /></div><p>We<a id="id62" class="indexterm"></a> will now describe the two environments we will use throughout the book: Cloudera's QuickStart virtual machine will be our reference system on which we will show all examples, but we will additionally demonstrate some examples on Amazon's EMR when there is some particularly valuable aspect to running the example in the on-demand service.</p><p>Although the examples and code provided are aimed at being as general-purpose and portable as possible, our reference setup, when talking about a local cluster, will be Cloudera running atop CentOS Linux.</p><p>For the most part, we will show examples that make use of, or are executed from, a terminal prompt. Although Hadoop's graphical interfaces have improved significantly over the years (for example, the excellent HUE and Cloudera Manager), when it comes to development, automation, and programmatic access to the system, the command line is still the most powerful tool for the job.</p><p>All examples and source code<a id="id63" class="indexterm"></a> presented in this book can be downloaded from <a class="ulink" href="https://github.com/learninghadoop2/book-examples" target="_blank">https://github.com/learninghadoop2/book-examples</a>. In addition, we have a home page for the book where we will publish updates and <a id="id64" class="indexterm"></a>related material at <a class="ulink" href="http://learninghadoop2.com" target="_blank">http://learninghadoop2.com</a>.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec15"></a>Cloudera QuickStart VM</h3></div></div></div><p>One of the <a id="id65" class="indexterm"></a>advantages of Hadoop distributions is that they give <a id="id66" class="indexterm"></a>access to easy-to-install, packaged software. Cloudera takes this one step further and provides a freely downloadable Virtual Machine instance of its latest distribution, known as the CDH QuickStart VM, deployed on top of CentOS Linux.</p><p>In the remaining parts of this book, we will use the CDH5.0.0 VM as the reference and baseline system to run examples and source code. Images of the<a id="id67" class="indexterm"></a> VM are<a id="id68" class="indexterm"></a> available for VMware (<a class="ulink" href="http://www.vmware.com/nl/products/player/" target="_blank">http://www.vmware.com/nl/products/player/</a>), KVM (<a class="ulink" href="http://www.linux-kvm.org/page/Main_Page" target="_blank">http://www.linux-kvm.org/page/Main_Page</a>), and <a id="id69" class="indexterm"></a>VirtualBox (<a class="ulink" href="https://www.virtualbox.org/" target="_blank">https://www.virtualbox.org/</a>) virtualization systems.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec16"></a>Amazon EMR</h3></div></div></div><p>Before <a id="id70" class="indexterm"></a>using <span class="strong"><strong>Elastic MapReduce</strong></span>, we need to set up an AWS account and register it with the necessary services.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec01"></a>Creating an AWS account</h4></div></div></div><p>Amazon <a id="id71" class="indexterm"></a>has integrated its general accounts with AWS, which means that, if you already have an account for any of the Amazon retail websites, this is the only account you will need to use AWS services.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note02"></a>Note</h3><p>Note that AWS services have a cost; you will need an active credit card associated with the account to which charges can be made.</p></div><p>If you require a new <a id="id72" class="indexterm"></a>Amazon account, go to <a class="ulink" href="http://aws.amazon.com" target="_blank">http://aws.amazon.com</a>, select <span class="strong"><strong>Create a new AWS account</strong></span>, and follow the prompts. Amazon has added a free tier for some services, so you might find that in the early days of testing and exploration, you are keeping many of your activities within the noncharged tier. The scope of the free tier has been expanding, so make sure you know what you will and won't be charged for.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec02"></a>Signing up for the necessary services</h4></div></div></div><p>Once you<a id="id73" class="indexterm"></a> have an Amazon account, you will need to register it for use with the required AWS services, that <a id="id74" class="indexterm"></a>is, <span class="strong"><strong>Simple Storage Service</strong></span> (<span class="strong"><strong>S3</strong></span>), <span class="strong"><strong>Elastic Compute Cloud</strong></span> (<span class="strong"><strong>EC2</strong></span>), and<a id="id75" class="indexterm"></a> <span class="strong"><strong>Elastic MapReduce</strong></span>. There<a id="id76" class="indexterm"></a> is no cost to simply sign up to any AWS service; the process just makes the service available to your account.</p><p>Go to the S3, EC2, and EMR pages linked from <a class="ulink" href="http://aws.amazon.com" target="_blank">http://aws.amazon.com</a>, click on the <span class="strong"><strong>Sign up</strong></span>
<span class="strong"><strong> </strong></span>button on each page, and then follow the prompts.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec17"></a>Using Elastic MapReduce</h3></div></div></div><p>Having created <a id="id77" class="indexterm"></a>an account with AWS and registered all the required services, we can proceed to configure programmatic access to EMR.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec18"></a>Getting Hadoop up and running</h3></div></div></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note03"></a>Note</h3><p>Caution! This costs real money!</p></div><p>Before going<a id="id78" class="indexterm"></a> any further, it is critical to understand that use of AWS services will incur charges that will appear on the credit card associated with your Amazon account. Most of the charges are quite small and increase with the amount of infrastructure consumed; storing 10 GB of data in S3 costs 10 times more than 1 GB, and running 20 EC2 instances costs 20 times as much as a single one. There are tiered cost models, so the actual costs tend to have smaller marginal increases at higher levels. But you should read carefully through the pricing sections for each service before using any of them. Note also that currently data transfer out of AWS services, such as EC2 and S3, is chargeable, but data transfer between services is not. This means it is often most cost-effective to carefully design your use of AWS to keep data within AWS through as much of the data processing as possible. For information regarding AWS and EMR, consult <a class="ulink" href="http://aws.amazon.com/elasticmapreduce/#pricing" target="_blank">http://aws.amazon.com/elasticmapreduce/#pricing</a>.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec03"></a>How to use EMR</h4></div></div></div><p>Amazon <a id="id79" class="indexterm"></a>provides both web and command-line interfaces to EMR. Both interfaces are just a frontend to the very same system; a cluster created with the command-line interface can be inspected and managed with the web tools and vice-versa.</p><p>For the most part, we will be using the command-line tools to create and manage clusters programmatically and will fall back on the web interface cases where it makes sense to do so.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec04"></a>AWS credentials</h4></div></div></div><p>Before using<a id="id80" class="indexterm"></a> either programmatic or command-line tools, we need to look at how an account holder authenticates to AWS to make such requests.</p><p>Each AWS account <a id="id81" class="indexterm"></a>has several identifiers, such as the following, that are used when accessing the various services:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Account ID</strong></span>: each<a id="id82" class="indexterm"></a> AWS account has a numeric ID.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Access key</strong></span>: the associated access key<a id="id83" class="indexterm"></a> is used to identify the account making the request.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Secret access key</strong></span>: the <a id="id84" class="indexterm"></a>partner to the access key is the secret access key. The access key is not a secret and could be exposed in service requests, but the secret access key is what you use to validate yourself as the account owner. Treat it like your credit card.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Key pairs</strong></span>: these are the key pairs<a id="id85" class="indexterm"></a> used to log in to EC2 hosts. It is possible to either generate public/private key pairs within EC2 or to import externally generated keys into the system.</p></li></ul></div><p>User credentials and permissions are managed via a web service called <a id="id86" class="indexterm"></a>
<span class="strong"><strong>Identity and Access Management</strong></span> (<span class="strong"><strong>IAM</strong></span>), which you need to sign up to in order to obtain access and secret keys.</p><p>If this sounds confusing, it's because it is, at least at first. When using a tool to access an AWS service, there's usually the single, upfront step of adding the right credentials to a configured file, and then everything just works. However, if you do decide to explore programmatic or command-line tools, it will be worth investing a little time to read the documentation for each service to understand how its security works. More information on creating an <a id="id87" class="indexterm"></a>AWS account and obtaining access credentials can be found at <a class="ulink" href="http://docs.aws.amazon.com/iam" target="_blank">http://docs.aws.amazon.com/iam</a>.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec19"></a>The AWS command-line interface</h3></div></div></div><p>Each <a id="id88" class="indexterm"></a>AWS service historically had its own set of command-line tools. Recently though, Amazon has created a single, unified command-line tool that allows access to most services. The Amazon CLI<a id="id89" class="indexterm"></a> can be found at <a class="ulink" href="http://aws.amazon.com/cli" target="_blank">http://aws.amazon.com/cli</a>.</p><p>It can be installed from a tarball or via the <code class="literal">pip</code> or <code class="literal">easy_install</code> package managers.</p><p>On the CDH QuickStart VM, we can install <code class="literal">awscli</code> using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ pip install awscli</strong></span>
</pre></div><p>In order to access the API, we need to configure the software to authenticate to AWS using our access and secret keys.</p><p>This is also <a id="id90" class="indexterm"></a>a good moment to set up an EC2 key pair<a id="id91" class="indexterm"></a> by following the instructions provided at <a class="ulink" href="https://console.aws.amazon.com/ec2/home?region=us-east-1#c=EC2&amp;s=KeyPairs" target="_blank">https://console.aws.amazon.com/ec2/home?region=us-east-1#c=EC2&amp;s=KeyPairs</a>.</p><p>Although a key pair is not strictly necessary to run an EMR cluster, it will give us the capability to remotely log in to the master node and gain low-level access to the cluster.</p><p>The following command will guide you through a series of configuration steps and store the resulting configuration in the <code class="literal">.aws/credential</code> file:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ aws configure</strong></span>
</pre></div><p>Once the CLI is configured, we can query AWS with <code class="literal">aws &lt;service&gt; &lt;arguments&gt;</code>. To create and query an S3 bucket use something like the following command. Note that S3 buckets need to be globally unique across all AWS accounts, so most common names, such as <code class="literal">s3://mybucket</code>, will not be available:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ aws s3 mb s3://learninghadoop2</strong></span>
<span class="strong"><strong>$ aws s3 ls</strong></span>
</pre></div><p>We can provision an EMR cluster with five <code class="literal">m1.xlarge</code> nodes using the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ aws emr create-cluster --name "EMR cluster" \</strong></span>
<span class="strong"><strong>--ami-version 3.2.0 \</strong></span>
<span class="strong"><strong>--instance-type m1.xlarge  \</strong></span>
<span class="strong"><strong>--instance-count 5 \</strong></span>
<span class="strong"><strong>--log-uri s3://learninghadoop2/emr-logs</strong></span>
</pre></div><p>Where <code class="literal">--ami-version</code> is the ID of an Amazon Machine Image template (<a class="ulink" href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html" target="_blank">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html</a>), and <code class="literal">--log-uri</code> instructs EMR to collect logs and store them in the <code class="literal">learninghadoop2</code> S3 bucket.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note04"></a>Note</h3><p>If you did not specify a default region when setting up the AWS CLI, then you will also have to add one to most EMR commands in the AWS CLI using the --region argument; for example, <code class="literal">--region eu-west-1</code> is run to use the EU Ireland region. You can find details of all available AWS regions at <a class="ulink" href="http://docs.aws.amazon.com/general/latest/gr/rande.html" target="_blank">http://docs.aws.amazon.com/general/latest/gr/rande.html</a>.</p></div><p>We can submit workflows by adding steps to a running cluster using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ aws emr add-steps --cluster-id &lt;cluster&gt; --steps &lt;steps&gt; </strong></span>
</pre></div><p>To terminate the cluster, use the following command line:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ aws emr terminate-clusters --cluster-id &lt;cluster&gt;</strong></span>
</pre></div><p>In later chapters, we will show you how to add steps to execute MapReduce jobs and Pig scripts.</p><p>More information <a id="id92" class="indexterm"></a>on using the AWS CLI can be found at <a class="ulink" href="http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-manage.html" target="_blank">http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-manage.html</a>.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec16"></a>Running the examples</h2></div></div><hr /></div><p>The<a id="id93" class="indexterm"></a> source code of all examples is available at <a class="ulink" href="https://github.com/learninghadoop2/book-examples" target="_blank">https://github.com/learninghadoop2/book-examples</a>.</p><p>Gradle (<a class="ulink" href="http://www.gradle.org" target="_blank">http://www.gradle.org</a>/) scripts <a id="id94" class="indexterm"></a>and configurations are provided to compile most of the Java code. The <code class="literal">gradlew</code> script included with the example will bootstrap Gradle and use it to fetch dependencies and compile code.</p><p>JAR files can be created by invoking the <code class="literal">jar</code> task via a <code class="literal">gradlew</code> script, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>./gradlew jar</strong></span>
</pre></div><p>Jobs are usually executed by submitting a JAR file using the <code class="literal">hadoop jar</code> command, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hadoop jar example.jar &lt;MainClass&gt; [-libjars $LIBJARS] arg1 arg2 â€¦ argN</strong></span>
</pre></div><p>The optional <code class="literal">-libjars</code> parameter specifies runtime third-party dependencies to ship to remote nodes.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note05"></a>Note</h3><p>Some of the frameworks we will work with, such as Apache Spark, come with their own build and package management tools. Additional information and resources will be provided for these particular cases.</p></div><p>The <code class="literal">copyJar</code> Gradle task can be used to download third-party dependencies into <code class="literal">build/libjars/&lt;example&gt;/lib</code>, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>./gradlew copyJar</strong></span>
</pre></div><p>For convenience, we provide a <code class="literal">fatJar</code> Gradle task that bundles the example classes and their dependencies into a single JAR file. Although this approach is discouraged in favor of using <code class="literal">â€“libjar</code>, it might come in handy when dealing with dependency issues.</p><p>The following command will generate <code class="literal">build/libs/&lt;example&gt;-all.jar</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ./gradlew fatJar</strong></span>
</pre></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec17"></a>Data processing with Hadoop</h2></div></div><hr /></div><p>In the remaining<a id="id95" class="indexterm"></a> chapters of this book, we will introduce the<a id="id96" class="indexterm"></a> core components of the Hadoop ecosystem as well as a number of third-party tools and libraries that will make writing robust, distributed code an accessible and hopefully enjoyable task. While reading this book, you will learn how to collect, process, store, and extract information from large amounts of structured and unstructured data.</p><p>We will use a <a id="id97" class="indexterm"></a>dataset generated<a id="id98" class="indexterm"></a> from Twitter's (<a class="ulink" href="http://www.twitter.com" target="_blank">http://www.twitter.com</a>) real-time fire hose. This approach will allow us to experiment with relatively small datasets locally and, once ready, scale the examples up to production-level data sizes.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec20"></a>Why Twitter?</h3></div></div></div><p>Thanks to<a id="id99" class="indexterm"></a> its programmatic APIs, Twitter provides <a id="id100" class="indexterm"></a>an easy way to generate datasets of arbitrary size and inject them into our local- or cloud-based Hadoop clusters. Other than the sheer size, the dataset that we will use has a number of properties that fit several interesting data modeling and processing use cases.</p><p>Twitter data possesses the following properties:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Unstructured</strong></span>: each <a id="id101" class="indexterm"></a>status update is a text message that can contain references to media content such as URLs and images</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Structured</strong></span>: tweets <a id="id102" class="indexterm"></a>are timestamped, sequential records</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Graph</strong></span>: relationships <a id="id103" class="indexterm"></a>such as replies and mentions can be modeled as a network of interactions</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Geolocated</strong></span>: the<a id="id104" class="indexterm"></a> location where a tweet was posted or where a user resides</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Real time</strong></span>: all <a id="id105" class="indexterm"></a>data generated on Twitter  is available via a real-time fire hose</p></li></ul></div><p>These properties will be reflected in the type of application that we can build with Hadoop. These include examples of sentiment analysis, social network, and trend analysis.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec21"></a>Building our first dataset</h3></div></div></div><p>Twitter's terms <a id="id106" class="indexterm"></a>of service prohibit redistribution of user-generated data<a id="id107" class="indexterm"></a> in any form; for this reason, we cannot make available a common dataset. Instead, we will use a Python script to programmatically access the platform and create a dump of user tweets collected from a live stream.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec05"></a>One service, multiple APIs</h4></div></div></div><p>Twitter users <a id="id108" class="indexterm"></a>share more than 200 million tweets, also known as status updates, a day. The platform offers access to this corpus of data via four types of APIs, each of which represents a facet of Twitter and aims at satisfying specific use cases, such as linking and interacting with twitter content from third-party sources (Twitter for Products), programmatic access to specific users' or sites' content (REST), search capabilities across users' or sites' timelines (Search), and access to all content created on the Twitter network in real time (Streaming).</p><p>The Streaming API allows direct access to the Twitter stream, tracking keywords, retrieving geotagged tweets from a certain region, and much more. In this book, we will make use of this API as a data source to illustrate both the batch and real-time capabilities of Hadoop. We will not, however, interact with the API itself; rather, we will make use of third-party libraries to offload chores such as authentication and connection management.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec06"></a>Anatomy of a Tweet</h4></div></div></div><p>Each<a id="id109" class="indexterm"></a> tweet object returned by a call to the real-time APIs is represented as a serialized JSON string that contains a set of attributes and metadata in addition to a textual message. This additional content includes a numerical ID that uniquely identifies the tweet, the location where the tweet was shared, the user who shared it (user object), whether it was republished by other users (retweeted) and how many times (retweet count), the machine-detected language of its text, whether the tweet was posted in reply to someone and, if so, the user and tweet IDs it replied to, and so on.</p><p>The structure of a Tweet, and any other object exposed by the API, is constantly evolving. An up-to-date reference <a id="id110" class="indexterm"></a>can be found at <a class="ulink" href="https://dev.twitter.com/docs/platform-objects/tweets" target="_blank">https://dev.twitter.com/docs/platform-objects/tweets</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec07"></a>Twitter credentials</h4></div></div></div><p>Twitter <a id="id111" class="indexterm"></a>makes use of the OAuth protocol to authenticate and authorize access from third-party software to its platform.</p><p>The application obtains through an external channel, for instance a web form, the following pair of credentials:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Consumer key</p></li><li style="list-style-type: disc"><p>Consumer secret</p></li></ul></div><p>The consumer <a id="id112" class="indexterm"></a>secret is never directly transmitted to the third party as it is used to sign each request.</p><p>The user authorizes the application to access the service via a three-way process that, once completed, grants the application a token consisting of the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Access token</p></li><li style="list-style-type: disc"><p>Access secret</p></li></ul></div><p>Similarly, to the consumer, the access secret is never directly transmitted to the third party, and it is used to sign each request.</p><p>In order to use the Streaming API, we will first need to register an application and grant it programmatic access to the system. If you require a new Twitter account, proceed to the signup page<a id="id113" class="indexterm"></a> at <a class="ulink" href="https://twitter.com/signup" target="_blank">https://twitter.com/signup</a>, and fill in the required information. Once this step is completed, we need to create a sample application that will access the API on our behalf and grant it the proper authorization rights. We will do so using the web form<a id="id114" class="indexterm"></a> found at <a class="ulink" href="https://dev.twitter.com/apps" target="_blank">https://dev.twitter.com/apps</a>.</p><p>When creating a new app, we are asked to give it a name, a description, and a URL. The following screenshot shows the settings of a sample application named <code class="literal">Learning Hadoop 2 Book Dataset</code>. For the purpose of this book, we do not need to specify a valid URL, so we used a placeholder instead.</p><div class="mediaobject"><img src="graphics/5518_01_03.jpg" /></div><p>Once the form<a id="id115" class="indexterm"></a> is filled in, we need to review and accept the terms of service and click on the <span class="strong"><strong>Create Application</strong></span> button in the bottom-left corner of the page.</p><p>We are now presented with a page that summarizes our application details as seen in the following screenshot; the authentication and authorization credentials can be found under the OAuth Tool tab. </p><p>We are finally ready to generate our very first Twitter dataset.</p><div class="mediaobject"><img src="graphics/5518_01_04.jpg" /></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec22"></a>Programmatic access with Python</h3></div></div></div><p>In this section, we<a id="id116" class="indexterm"></a> will use Python and the <code class="literal">tweepy</code> library, found at <a class="ulink" href="https://github.com/tweepy/tweepy" target="_blank">https://github.com/tweepy/tweepy</a>, to collect <a id="id117" class="indexterm"></a>Twitter's data. The <code class="literal">stream.py</code> file found in the <code class="literal">ch1</code> directory of the book code archive instantiates a listener to the real-time fire hose, grabs a data sample, and echoes each tweet's text to standard output.</p><p>The <code class="literal">tweepy</code> library can be installed using either the <code class="literal">easy_install</code> or <code class="literal">pip</code> package managers or by cloning the repository at <a class="ulink" href="https://github.com/tweepy/tweepy" target="_blank">https://github.com/tweepy/tweepy</a>.</p><p>On the CDH QuickStart VM, we can install <code class="literal">tweepy</code> using the following command line:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ pip install tweepy</strong></span>
</pre></div><p>When <a id="id118" class="indexterm"></a>invoked with the <code class="literal">-j</code> parameter, the script will output a JSON tweet to standard output; <code class="literal">-t</code> extracts and prints the text field. We specify how many tweets to print with<code class="literal">â€“n &lt;num tweets&gt;</code>. When <code class="literal">â€“n</code> is not specified, the script will run indefinitely. Execution can be terminated by pressing <span class="emphasis"><em>Ctrl</em></span> + <span class="emphasis"><em>C</em></span>.</p><p>The script expects <a id="id119" class="indexterm"></a>OAuth credentials to be stored as shell environment variables; the following credentials will have to be set in the terminal session from where <code class="literal">stream.py</code> will be executed.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ export TWITTER_CONSUMER_KEY="your_consumer_key"</strong></span>
<span class="strong"><strong>$ export TWITTER_CONSUMER_SECRET="your_consumer_secret"</strong></span>
<span class="strong"><strong>$ export TWITTER_ACCESS_KEY="your_access_key"</strong></span>
<span class="strong"><strong>$ export TWITTER_ACCESS_SECRET="your_access_secret"</strong></span>
</pre></div><p>Once the required dependency has been installed and the OAuth data in the shell environment has been set, we can run the program as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ python stream.py â€“t â€“n 1000 &gt; tweets.txt</strong></span>
</pre></div><p>We are relying on Linux's shell I/O to redirect the output with the <code class="literal">&gt;</code> operator of <code class="literal">stream.py</code> to a file called <code class="literal">tweets.txt</code>. If everything was executed correctly, you should see a wall of text, where each line is a tweet.</p><p>Notice that in this example, we did not make use of Hadoop at all. In the next chapters, we will show how to import a dataset generated from the Streaming API into Hadoop and analyze its content on the local cluster and Amazon EMR.</p><p>For now, let's take a look at the <a id="id120" class="indexterm"></a>source code of <code class="literal">stream.py</code>, which can be found at<code class="literal"> </code>
<a class="ulink" href="https://github.com/learninghadoop2/book-examples/blob/master/ch1/stream.py" target="_blank">https://github.com/learninghadoop2/book-examples/blob/master/ch1/stream.py</a>:</p><div class="informalexample"><pre class="programlisting">import tweepy
import os
import json
import argparse

consumer_key = os.environ['TWITTER_CONSUMER_KEY']
consumer_secret = os.environ['TWITTER_CONSUMER_SECRET']
access_key = os.environ['TWITTER_ACCESS_KEY']
access_secret = os.environ['TWITTER_ACCESS_SECRET']

class EchoStreamListener(tweepy.StreamListener):
    def __init__(self, api, dump_json=False, numtweets=0):
        self.api = api
        self.dump_json = dump_json
        self.count = 0
        self.limit = int(numtweets)
        super(tweepy.StreamListener, self).__init__()

    def on_data(self, tweet):
        tweet_data = json.loads(tweet)
        if 'text' in tweet_data:
            if self.dump_json:
                print tweet.rstrip()
            else:
                print tweet_data['text'].encode("utf-8").rstrip()

            self.count = self.count+1
            return False if self.count == self.limit else True

    def on_error(self, status_code):
        return True

    def on_timeout(self):
        return True
â€¦
if __name__ == '__main__':
    parser = get_parser()
    args = parser.parse_args()

    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
    auth.set_access_token(access_key, access_secret)
    api = tweepy.API(auth)
    sapi = tweepy.streaming.Stream(
        auth, EchoStreamListener(
            api=api, 
            dump_json=args.json, 
            numtweets=args.numtweets))
    sapi.sample()</pre></div><p>First, we<a id="id121" class="indexterm"></a> import three dependencies: <code class="literal">tweepy</code>, and the <code class="literal">os</code> and <code class="literal">json</code> modules, which come with the Python interpreter <a id="id122" class="indexterm"></a>version 2.6 or greater.</p><p>We then define a class, <code class="literal">EchoStreamListener</code>, that inherits and extends <code class="literal">StreamListener</code> from <code class="literal">tweepy</code>. As the name suggests, <code class="literal">StreamListener</code> listens for events and tweets being published on the real-time stream and performs actions accordingly.</p><p>Whenever a new event is detected, it triggers a call to <code class="literal">on_data()</code>. In this method, we extract the <code class="literal">text</code> field from a tweet object and print it to standard output with UTF-8 encoding. Alternatively, if the script is invoked with <code class="literal">-j</code>, we print the whole JSON tweet. When the script is executed, we instantiate a <code class="literal">tweepy.OAuthHandler</code> object with the OAuth <a id="id123" class="indexterm"></a>credentials that identify our Twitter account, and then we use this object to authenticate with the application access and secret key. We then use the <code class="literal">auth</code> object to create an instance of the <code class="literal">tweepy.API</code> class (<code class="literal">api</code>)</p><p>Upon <a id="id124" class="indexterm"></a>successful authentication, we tell Python to listen for events on the real-time stream using <code class="literal">EchoStreamListener</code>.</p><p>An http GET request to the <code class="literal">statuses/sample</code> endpoint is performed by <code class="literal">sample()</code>. The request returns a random sample of all public statuses. </p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note06"></a>Note</h3><p>Beware! By default, <code class="literal">sample()</code> will run indefinitely. Remember to explicitly terminate the method call by pressing <span class="emphasis"><em>Ctrl</em></span> + <span class="emphasis"><em>C</em></span>.</p></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec18"></a>Summary</h2></div></div><hr /></div><p>This chapter gave a whirlwind tour of where Hadoop came from, its evolution, and why the version 2 release is such a major milestone. We also described the emerging market in Hadoop distributions and how we will use a combination of local and cloud distributions in the book.</p><p>Finally, we described how to set up the needed software, accounts, and environments required in subsequent chapters and demonstrated how to pull data from the Twitter stream that we will use for examples.</p><p>With this background out of the way, we will now move on to a detailed examination of the storage layer within Hadoop.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch02"></a>ChapterÂ 2.Â Storage</h2></div></div></div><p>After the overview of Hadoop in the previous chapter, we will now start looking at its various component parts in more detail. We will start at the conceptual bottom of the stack in this chapter: the means and mechanisms for storing data within Hadoop. In particular, we will discuss the following topics:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Describe the architecture of the <span class="strong"><strong>Hadoop Distributed File System</strong></span> (<span class="strong"><strong>HDFS</strong></span>)</p></li><li style="list-style-type: disc"><p>Show what enhancements to HDFS have been made in Hadoop 2</p></li><li style="list-style-type: disc"><p>Explore how to access HDFS using command-line tools and the Java API</p></li><li style="list-style-type: disc"><p>Give a brief description of ZooKeeperâ€”another (sort of) filesystem within Hadoop</p></li><li style="list-style-type: disc"><p>Survey considerations for storing data in Hadoop and the available file formats</p></li></ul></div><p>In <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Processing â€“ MapReduce and Beyond</em></span>, we will describe how Hadoop provides the framework to allow data to be processed.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec19"></a>The inner workings of HDFS</h2></div></div><hr /></div><p>In <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Introduction</em></span>, we<a id="id125" class="indexterm"></a> gave a very high-level overview of HDFS; we will now explore it in a little more detail. As mentioned in that chapter, HDFS can be viewed as a filesystem, though one with very specific performance characteristics and semantics. It's implemented with two main server processes: the <span class="strong"><strong>NameNode</strong></span><a id="id126" class="indexterm"></a> and the<a id="id127" class="indexterm"></a> <span class="strong"><strong>DataNodes</strong></span>, configured in a master/slave setup. If you view the NameNode as holding all the filesystem metadata and the DataNodes as holding the actual filesystem data (blocks), then this is a good starting point. Every file placed onto HDFS will be split into multiple blocks that might reside on numerous DataNodes, and it's the NameNode that understands how these blocks can be combined to construct the files.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec23"></a>Cluster startup</h3></div></div></div><p>Let's explore the<a id="id128" class="indexterm"></a> various responsibilities of these nodes and the <a id="id129" class="indexterm"></a>communication between them by assuming we have an HDFS cluster that was previously shut down and then examining the startup behavior.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec08"></a>NameNode startup</h4></div></div></div><p>We'll<a id="id130" class="indexterm"></a> firstly <a id="id131" class="indexterm"></a>consider the startup of the NameNode (though there is no actual ordering requirement for this and we are doing it for narrative reasons alone). The NameNode actually stores two types of data about the filesystem:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The structure of the filesystem, that is, directory names, filenames, locations, and attributes</p></li><li style="list-style-type: disc"><p>The blocks that comprise each file on the filesystem</p></li></ul></div><p>This data is stored in files that the NameNode reads at startup. Note that the NameNode does not persistently store the mapping of the blocks that are stored on particular DataNodes; we'll see how that information is communicated shortly.</p><p>Because the NameNode relies on this in-memory representation of the filesystem, it tends to have quite different hardware requirements compared to the DataNodes. We'll explore hardware selection in more detail in <a class="link" href="#" linkend="ch10">Chapter 10</a>, <span class="emphasis"><em>Running a Hadoop Cluster</em></span>; for now, just remember that the NameNode tends to be quite memory hungry. This is particularly true on very large clusters with many (millions or more) files, particularly if these files have very long names. This scaling limitation on the NameNode has also led to an additional Hadoop 2 feature that we will not explore in much detail: NameNode federation, whereby multiple NameNodes (or NameNode HA pairs) work collaboratively to provide the overall metadata for the full filesystem.</p><p>The main file written by the NameNode is called <code class="literal">fsimage</code>; this is the single most important piece of data in the entire cluster, as without it, the knowledge of how to reconstruct all the data blocks into the usable filesystem is lost. This file is read into memory and all future modifications to the filesystem are applied to this in-memory representation of the filesystem. The NameNode does not write out new versions of <code class="literal">fsimage</code> as new changes are applied after it is run; instead, it writes another file called <code class="literal">edits</code>, which is a list of the changes that have been made since the last version of <code class="literal">fsimage</code> was written.</p><p>The NameNode startup process is to first read the <code class="literal">fsimage</code> file, then to read the <code class="literal">edits</code> file, and apply all the changes stored in the <code class="literal">edits</code> file to the in-memory copy of <code class="literal">fsimage</code>. It then writes to disk a new up-to-date version of the <code class="literal">fsimage</code> file and is ready to receive client requests.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec09"></a>DataNode startup</h4></div></div></div><p>When<a id="id132" class="indexterm"></a> the DataNodes start up, they first catalog the blocks for which they hold copies. Typically, these blocks will be written simply as <a id="id133" class="indexterm"></a>files on the local DataNode filesystem. The DataNode will perform some block consistency checking and then report to the NameNode the list of blocks for which it has valid copies. This is how the NameNode constructs the final mapping it requiresâ€”by learning which blocks are stored on which DataNodes. Once the DataNode has registered itself with the NameNode, an ongoing series of heartbeat requests will be sent between the nodes to allow the NameNode to detect DataNodes that have shut down, become unreachable, or have newly entered the cluster.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec24"></a>Block replication</h3></div></div></div><p>HDFS <a id="id134" class="indexterm"></a>replicates<a id="id135" class="indexterm"></a> each block onto multiple DataNodes; the default replication factor is 3, but this is configurable on a per-file level. HDFS can also be configured to be able to determine whether given DataNodes are in the same physical hardware rack or not. Given smart block placement and this knowledge of the cluster topology, HDFS will attempt to place the second replica on a different host but in the same equipment rack as the first and the third on a host outside the rack. In this way, the system can survive the failure of as much as a full rack of equipment and still have at least one live replica for each block. As we'll see in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Processing â€“ MapReduce and Beyond</em></span>, knowledge of block placement also allows Hadoop to schedule processing as near as possible to a replica of each block, which can greatly improve performance.</p><p>Remember that replication is a strategy for resilience but is not a backup mechanism; if you have data mastered in HDFS that is critical, then you need to consider backup or other approaches that give protection for errors, such as accidentally deleted files, against which replication will not defend.</p><p>When the NameNode starts up and is receiving the block reports from the DataNodes, it will remain in safe mode until a configurable threshold of blocks (the default is 99.9 percent) have been reported as live. While in safe mode, clients cannot make any modifications to the filesystem.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec20"></a>Command-line access to the HDFS filesystem</h2></div></div><hr /></div><p>Within<a id="id136" class="indexterm"></a> the Hadoop distribution, there is a command-line utility called <code class="literal">hdfs</code>, which is the primary way to interact with the filesystem from<a id="id137" class="indexterm"></a> the command line. Run this without any arguments to see the various subcommands available. There are many, though; several are used to do things like starting or stopping various HDFS components. The general form of the<a id="id138" class="indexterm"></a> <code class="literal">hdfs</code> command is:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>hdfs &lt;sub-command&gt; &lt;command&gt; [arguments]</strong></span>
</pre></div><p>The two main subcommands we will <a id="id139" class="indexterm"></a>use in this book are:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">dfs</code>: This is<a id="id140" class="indexterm"></a> used for general filesystem access and manipulation, including reading/writing and accessing files and directories</p></li><li style="list-style-type: disc"><p>
<code class="literal">dfsadmin</code>: This <a id="id141" class="indexterm"></a>is used for administration and maintenance of the filesystem. We will not cover this command in detail, though. Have a look at the <code class="literal">-report</code> command, which gives a listing of the state of the filesystem and all DataNodes:</p><div class="informalexample"><pre class="programlisting">$ hdfs dfsadmin -report</pre></div></li></ul></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note07"></a>Note</h3><p>Note that the <code class="literal">dfs</code> and <code class="literal">dfsadmin</code> commands can also be used with the main Hadoop command-line utility, for example, <code class="literal">hadoop fs -ls /</code>. This was the approach in earlier versions of Hadoop but is now deprecated in favor of the <code class="literal">hdfs</code> command.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec25"></a>Exploring the HDFS filesystem</h3></div></div></div><p>Run the<a id="id142" class="indexterm"></a> following to get a list of the available commands provided by the <code class="literal">dfs</code> subcommand:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hdfs dfs</strong></span>
</pre></div><p>As will be seen from the output of the preceding command, many of these look similar to standard Unix filesystem commands and, not surprisingly, they work as would be expected. In our test VM, we have a user account called <code class="literal">cloudera</code>. Using this user, we can list the root of the filesystem as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hdfs dfs -ls /</strong></span>
<span class="strong"><strong>Found 7 items</strong></span>
<span class="strong"><strong>drwxr-xr-x   - hbase hbase               0 2014-04-04 15:18 /hbase</strong></span>
<span class="strong"><strong>drwxr-xr-x   - hdfs  supergroup          0 2014-10-21 13:16 /jar</strong></span>
<span class="strong"><strong>drwxr-xr-x   - hdfs  supergroup          0 2014-10-15 15:26 /schema</strong></span>
<span class="strong"><strong>drwxr-xr-x   - solr  solr                0 2014-04-04 15:16 /solr</strong></span>
<span class="strong"><strong>drwxrwxrwt   - hdfs  supergroup          0 2014-11-12 11:29 /tmp</strong></span>
<span class="strong"><strong>drwxr-xr-x   - hdfs  supergroup          0 2014-07-13 09:05 /user</strong></span>
<span class="strong"><strong>drwxr-xr-x   - hdfs  supergroup          0 2014-04-04 15:15 /var</strong></span>
</pre></div><p>The output is very similar to the Unix <code class="literal">ls</code> command. The file attributes work the same as the <code class="literal">user</code>/<code class="literal">group</code>/<code class="literal">world</code> attributes on a Unix filesystem (including the <code class="literal">t</code> sticky bit as can be seen) plus details of the owner, group, and modification time of the directories. The column between the group name and the modified date is the size; this is 0 for directories but will have a value for files as we'll see in the code following the next information box:</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note08"></a>Note</h3><p>If relative paths are used, they are taken from the home directory of the user. If there is no home directory, we can create it using the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo -u hdfs hdfs dfs â€“mkdir /user/cloudera</strong></span>
<span class="strong"><strong>$ sudo -u hdfs hdfs dfs â€“chown cloudera:cloudera /user/cloudera</strong></span>
</pre></div><p>The <code class="literal">mkdir</code> and <code class="literal">chown</code> steps require superuser privileges (<code class="literal">sudo -u hdfs</code>).</p></div><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hdfs dfs -mkdir testdir</strong></span>
<span class="strong"><strong>$ hdfs dfs -ls</strong></span>
<span class="strong"><strong>Found 1 items</strong></span>
<span class="strong"><strong>drwxr-xr-x   - cloudera cloudera     0 2014-11-13 11:21 testdir</strong></span>
</pre></div><p>Then, we can<a id="id143" class="indexterm"></a> create a file, copy it to HDFS, and read its contents directly from its location on HDFS, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ echo "Hello world" &gt; testfile.txt</strong></span>
<span class="strong"><strong>$ hdfs dfs -put testfile.txt testdir</strong></span>
</pre></div><p>Note that there is an older command called <code class="literal">-copyFromLocal</code>, which works in the same way as <code class="literal">-put</code>; you might see it in older documentation online. Now, run the following command and check the output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hdfs dfs -ls testdir</strong></span>
<span class="strong"><strong>Found 1 items</strong></span>
<span class="strong"><strong>-rw-r--r--   3 cloudera cloudera         12 2014-11-13 11:21 testdir/testfile.txt</strong></span>
</pre></div><p>Note the new column between the file attributes and the owner; this is the replication factor of the file. Now, finally, run the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hdfs dfs -tail testdir/testfile.txt</strong></span>
<span class="strong"><strong>Hello world</strong></span>
</pre></div><p>Much of the rest of the <code class="literal">dfs</code> subcommands are pretty intuitive; play around. We'll explore snapshots and programmatic access to HDFS later in this chapter.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec21"></a>Protecting the filesystem metadata</h2></div></div><hr /></div><p>Because the <a id="id144" class="indexterm"></a>
<code class="literal">fsimage</code> file is so critical to the filesystem, its loss is a catastrophic failure. In Hadoop 1, where the NameNode was a single point of failure, the best practice was to configure the NameNode to synchronously write the <code class="literal">fsimage</code> and edits files to both local storage plus at least one other location on a remote filesystem (often NFS). In the event of NameNode failure, a replacement NameNode could<a id="id145" class="indexterm"></a> be started using this up-to-date copy of the filesystem metadata. The process would require non-trivial manual intervention, however, and would result in a period of complete cluster unavailability.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec26"></a>Secondary NameNode not to the rescue</h3></div></div></div><p>The most <a id="id146" class="indexterm"></a>unfortunately named component in all of Hadoop 1 was the Secondary NameNode, which, not <a id="id147" class="indexterm"></a>unreasonably, many people expect to be some sort of backup or standby NameNode. It is not; instead, the Secondary NameNode was responsible<a id="id148" class="indexterm"></a> only for periodically reading the latest version of the <code class="literal">fsimage</code> and edits file and creating a new up-to-date <code class="literal">fsimage</code> with the outstanding edits applied. On a busy cluster, this checkpoint could significantly speed up the restart of the NameNode by reducing the number of edits it had to apply before being able to service clients.</p><p>In Hadoop 2, the naming is more clear; there are Checkpoint nodes, which do the role previously performed by the Secondary NameNode, plus Backup NameNodes, which keep a local up-to-date copy of the filesystem metadata even though the process to promote a Backup node to be the primary NameNode is still a multistage manual process.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec27"></a>Hadoop 2 NameNode HA</h3></div></div></div><p>In most <a id="id149" class="indexterm"></a>production Hadoop 2 clusters, however, it makes more sense to use the full High Availability (HA) solution instead of <a id="id150" class="indexterm"></a>relying on Checkpoint and Backup nodes. It is actually an error to try to combine NameNode HA with the Checkpoint and Backup node mechanisms.</p><p>The core idea is for a pair (currently no more than two are supported) of NameNodes configured in an active/passive cluster. One NameNode acts as the live master that services all client requests, and the second remains ready to take over should the primary fail. In particular, Hadoop 2 HDFS<a id="id151" class="indexterm"></a> enables this HA through two mechanisms:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Providing a means for both NameNodes to have consistent views of the filesystem</p></li><li style="list-style-type: disc"><p>Providing a means for clients to always connect to the master NameNode</p></li></ul></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec10"></a>Keeping the HA NameNodes in sync</h4></div></div></div><p>There are <a id="id152" class="indexterm"></a>actually two mechanisms by which the active and standby NameNodes keep their views of the filesystem consistent; use of an <span class="strong"><strong>NFS</strong></span> share<a id="id153" class="indexterm"></a> or <span class="strong"><strong>Quorum Journal Manager </strong></span>(<span class="strong"><strong>QJM</strong></span>).</p><p>In the NFS case, there is an obvious requirement on an external remote NFS file shareâ€”note that as use of NFS was best practice in Hadoop 1 for a second copy of filesystem metadata many clusters already have one. If high availability is a concern, though it should be borne in mind that making NFS highly available often requires high-end and expensive hardware. In Hadoop 2, HA uses NFS; however, the NFS location becomes the primary location for the filesystem metadata. As the active NameNode writes all filesystem changes to the NFS share, the standby node detects these changes and updates its copy of the filesystem metadata accordingly.</p><p>The QJM mechanism<a id="id154" class="indexterm"></a> uses an external service (the Journal Managers) instead of a filesystem. The Journal Manager cluster is an odd number of services (3, 5, and 7 are the most common) running on that number of hosts. All changes to the filesystem are submitted to the QJM service, and a change is treated as committed only when a majority of the QJM nodes have committed the change. The standby NameNode receives change updates from the QJM service and uses this information to keep its copy of the filesystem metadata up to date.</p><p>The QJM mechanism does not require additional hardware as the Checkpoint nodes are lightweight and can be co-located with other services. There is also no single point of failure in the model. Consequently, the QJM HA is usually the preferred option.</p><p>In either case, both in NFS-based HA and QJM-based HA, the DataNodes send block status reports to both NameNodes to ensure that both have up-to-date information of the mapping of blocks to DataNodes. Remember that this block assignment information is not held in the <code class="literal">fsimage</code>/edits data.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec28"></a>Client configuration</h3></div></div></div><p>The clients<a id="id155" class="indexterm"></a> to the HDFS cluster remain mostly unaware of the fact that NameNode HA is being used. The configuration files need to include the details of both NameNodes, but the mechanisms for determining which is the active NameNodeâ€”and when to switch to the standbyâ€”are fully encapsulated in the client libraries. The fundamental concept though is that instead of referring to an explicit NameNode host as in Hadoop 1, HDFS in Hadoop 2 identifies a nameservice ID for the NameNode within which multiple individual NameNodes (each with its own NameNode ID) are defined for HA. Note that the concept of nameservice ID is also used by NameNode federation, which we briefly mentioned earlier.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec29"></a>How a failover works</h3></div></div></div><p>Failover <a id="id156" class="indexterm"></a>can be either manual or automatic. A manual failover requires an administrator to trigger the switch that promotes the standby to the currently active NameNode. Though automatic failover has the greatest impact on maintaining system availability, there might be conditions in which this is not always desirable. Triggering a manual failover requires running only a few commands and, therefore, even in this mode, the failover is significantly easier than in the case of Hadoop 1 or with Hadoop 2 Backup nodes, where the transition to a new NameNode requires substantial manual effort.</p><p>Regardless of whether the failover is triggered manually or automatically, it has two main phases: confirmation that the previous master is no longer serving requests and the promotion of the standby to be the master.</p><p>The greatest risk in a failover is to have a period in which both NameNodes are servicing requests. In such a situation, it is possible that conflicting changes might be made to the filesystem on the two NameNodes or that they might become out of sync. Even though this should not be possible if the QJM is being used (it only ever accepts connections from a single client), out-of-date information might be served to clients, who might then try to make incorrect decisions based on this stale metadata. This is, of course, particularly likely if the previous master NameNode is behaving incorrectly in some way, which is why the need for the failover is identified in the first place.</p><p>To ensure only one NameNode is active at any time, a fencing mechanism is used to validate that the existing NameNode master has been shut down. The simplest included mechanism will try to ssh into the NameNode host and actively kill the process though a custom script can also be executed, so the mechanism is flexible. The failover will not continue until the fencing is successful and the system has confirmed that the previous master NameNode is now dead and has released any required resources.</p><p>Once fencing succeeds, the standby NameNode becomes the master and will start writing to the NFS-mounted <code class="literal">fsimage</code> and edits logs if NFS is being used for HA or will become the single client to the QJM if that is the HA mechanism.</p><p>Before discussing automatic failover, we need a slight segue to introduce another Apache project that is used to enable this feature.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec22"></a>Apache ZooKeeper â€“ a different type of filesystem</h2></div></div><hr /></div><p>Within Hadoop, we<a id="id157" class="indexterm"></a> will mostly talk about HDFS when discussing filesystems and data storage. But, inside almost all Hadoop 2 installations, there is another service that looks somewhat like a filesystem, but which provides significant capability crucial to the proper functioning of distributed systems. This service is Apache ZooKeeper (<a class="ulink" href="http://zookeeper.apache.org" target="_blank">http://zookeeper.apache.org</a>) and, <a id="id158" class="indexterm"></a>as it is a key part of the implementation of HDFS HA, we will introduce it in this chapter. It is, however, also used by multiple other Hadoop components and related projects, so we will touch on it several more times throughout the book.</p><p>ZooKeeper<a id="id159" class="indexterm"></a> started out as a subcomponent of HBase and was used to enable several operational capabilities of the service. When any complex distributed system is built, there are a series of activities that are almost always required and which are always difficult to get right. These activities include things such as handling shared locks, detecting component failure, and supporting leader election within a group of collaborating services. ZooKeeper was created as the coordination service that would provide a series of primitive operations upon which HBase could implement these types of operationally critical features. Note that ZooKeeper also takes inspiration from the Google Chubby system<a id="id160" class="indexterm"></a> described at <a class="ulink" href="http://research.google.com/archive/chubby-osdi06.pdf" target="_blank">http://research.google.com/archive/chubby-osdi06.pdf</a>.</p><p>ZooKeeper runs as a cluster of instances referred to as an ensemble. The ensemble provides a data structure, which is somewhat analogous to a filesystem. Each location in the structure is called a ZNode and can have children as if it were a directory but can also have content as if it were a file. Note that ZooKeeper is not a suitable place to store very large amounts of data, and by default, the maximum amount of data in a ZNode is 1 MB. At any point in time, one server in the ensemble is the master and makes all decisions about client requests. There are very well-defined rules around the responsibilities of the master, including that it has to ensure that a request is only committed when a majority of the ensemble have committed the change, and that once committed any conflicting change is rejected.</p><p>You should have ZooKeeper installed within your Cloudera Virtual Machine. If not, use Cloudera Manager to install it as a single node on the host. In production systems, ZooKeeper has very specific semantics around absolute majority voting, so some of the logic only makes sense in a larger ensemble (3, 5, or 7 nodes are the most common sizes).</p><p>There is a command-line client to ZooKeeper called <code class="literal">zookeeper-client</code> in the Cloudera VM; note that in the vanilla ZooKeeper distribution it is called <code class="literal">zkCli.sh</code>. If you run it with no arguments, it will connect to the ZooKeeper server running on the local machine. From here, you can type <code class="literal">help</code> to get a list of commands.</p><p>The most immediately interesting commands will be <code class="literal">create</code>, <code class="literal">ls</code>, and <code class="literal">get</code>. As the names suggest, these create a ZNode, list the ZNodes at a particular point in the filesystem, and get the data stored at a particular ZNode. Here are some examples of usage.</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Create a ZNode with no data:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ create /zk-test '' </strong></span>
</pre></div></li><li style="list-style-type: disc"><p>Create a child of the first ZNode and store some text in it:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ create /zk-test/child1 'sampledata'</strong></span>
</pre></div></li><li style="list-style-type: disc"><p>Retrieve the data associated with a particular ZNode:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ get /zk-test/child1 </strong></span>
</pre></div></li></ul></div><p>The client can also register a watcher on a given ZNodeâ€”this will raise an alert if the ZNode in question changes, either its data or children being modified.</p><p>This might not <a id="id161" class="indexterm"></a>sound very useful, but ZNodes can additionally be created as both sequential and ephemeral nodes, and this is where the magic starts.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec30"></a>Implementing a distributed lock with sequential ZNodes</h3></div></div></div><p>If a <a id="id162" class="indexterm"></a>ZNode is created within the CLI with the <code class="literal">-s</code> option, it will be created as a sequential node. ZooKeeper will suffix the supplied name with a 10-digit integer guaranteed to be unique and greater than any other sequential children of the same ZNode. We can use this mechanism to create a distributed lock. ZooKeeper itself is not holding the actual lock; the client needs to understand what particular states in ZooKeeper mean in terms of their mapping to the application locks in question.</p><p>If we create a (non-sequential) ZNode at <code class="literal">/zk-lock</code>, then any client wishing to hold the lock will create a sequential child node. For example, the <code class="literal">create -s /zk-lock/locknode</code> command might create the node, <code class="literal">/zk-lock/locknode-0000000001</code>, in the first case, with increasing integer suffixes for subsequent calls. When a client creates a ZNode under the lock, it will then check if its sequential node has the lowest integer suffix. If it does, then it is treated as having the lock. If not, then it will need to wait until the node holding the lock is deleted. The client will usually put a watch on the node with the next lowest suffix and then be alerted when that node is deleted, indicating that it now holds the lock.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec31"></a>Implementing group membership and leader election using ephemeral ZNodes</h3></div></div></div><p>Any<a id="id163" class="indexterm"></a> ZooKeeper client will send heartbeats to the server throughout the session, showing that it is alive. For the ZNodes we have discussed until now, we can say that they are persistent and will survive across sessions. We <a id="id164" class="indexterm"></a>can, however, create a ZNode as ephemeral, meaning it will disappear once the client that created it either disconnects or is detected as being dead by the ZooKeeper server. Within the CLI an ephemeral ZNode is created by adding the <code class="literal">-e</code> flag to the create command.</p><p>Ephemeral ZNodes<a id="id165" class="indexterm"></a> are a good mechanism to implement group membership discovery within a distributed system. For any system where nodes can fail, join, and leave without notice, knowing which nodes are alive at any point in time is often a difficult task. Within ZooKeeper, we can provide the basis for such discovery by having each node create an ephemeral ZNode at a certain location in the ZooKeeper filesystem. The <a id="id166" class="indexterm"></a>ZNodes can hold data about the service nodes, such as host name, IP address, port number, and so on. To get a list of live nodes, we can simply list the child nodes of the parent group ZNode. Because of the nature of ephemeral nodes, we can have confidence that the list of live nodes retrieved at any time is up to date.</p><p>If we have <a id="id167" class="indexterm"></a>each service node create ZNode children that are not just ephemeral but also sequential, then we can also build a mechanism for leader election for services that need to have a single master node at any one time. The mechanism is the same for locks; the client service node creates the sequential and ephemeral ZNode and then checks if it has the lowest sequence number. If so, then it is the master. If not, then it will register a watcher on the next lowest sequence node to be alerted when it might become the master.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec32"></a>Java API</h3></div></div></div><p>The <code class="literal">org.apache.zookeeper.ZooKeeper</code> class<a id="id168" class="indexterm"></a> is the main programmatic client to access a ZooKeeper ensemble. Refer to the javadocs for the full details, but the basic interface is relatively <a id="id169" class="indexterm"></a>straightforward with obvious one-to-one correspondence to commands in the CLI. For example:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">create</code>: is equivalent to CLI <code class="literal">create</code>
</p></li><li style="list-style-type: disc"><p>
<code class="literal">getChildren</code>: is equivalent to CLI <code class="literal">ls</code>
</p></li><li style="list-style-type: disc"><p>
<code class="literal">getData</code>: is equivalent to CLI <code class="literal">get</code>
</p></li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec33"></a>Building blocks</h3></div></div></div><p>As can<a id="id170" class="indexterm"></a> be seen, ZooKeeper provides a small number of well-defined operations with very strong semantic guarantees that can be built into higher-level services, such as the locks, group membership, and leader election we discussed earlier. It's best to think of ZooKeeper as a toolkit of well-engineered and reliable functions critical to distributed systems that can be built upon without having to worry about the intricacies of their implementation. The provided ZooKeeper interface is quite low-level though, and there are a few higher-level interfaces emerging that provide more of the mapping of the low-level primitives into application-level logic. The <a id="id171" class="indexterm"></a>Curator project (<a class="ulink" href="http://curator.apache.org/" target="_blank">http://curator.apache.org/</a>) is a good example of this.</p><p>ZooKeeper was used sparingly within Hadoop 1, but it's now quite ubiquitous. It's used by both MapReduce and HDFS for the high availability of their JobTracker and NameNode components. Hive and Impala, which we will explore later, use it to place locks on data tables that are being accessed by multiple concurrent jobs. Kafka, which we'll discuss in the <a id="id172" class="indexterm"></a>context of Samza, uses ZooKeeper for node (broker in Kafka terminology), leader election, and state management.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec34"></a>Further reading</h3></div></div></div><p>We have not described ZooKeeper in much detail and have completely omitted aspects such as its ability to apply quotas and access control lists to ZNodes within the filesystem and the mechanisms to build callbacks. Our purpose here was to give enough of the details so that you would have some idea of how it was being used within the Hadoop services we explore in this book. For more information, consult the project home page.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec23"></a>Automatic NameNode failover</h2></div></div><hr /></div><p>Now that <a id="id173" class="indexterm"></a>we have introduced ZooKeeper, we can show how it is used to enable <span class="strong"><strong>automatic NameNode</strong></span> failover.</p><p>Automatic NameNode failover introduces two new components to the system, a <a id="id174" class="indexterm"></a>
<span class="strong"><strong>ZooKeeper quorum</strong></span>, and the <a id="id175" class="indexterm"></a>
<span class="strong"><strong>ZooKeeper Failover Controller</strong></span> (ZKFC), which runs on each NameNode host. The ZKFC creates an ephemeral ZNode in ZooKeeper and holds this ZNode for as long as it detects the local NameNode to be alive and functioning correctly. It <a id="id176" class="indexterm"></a>determines this by continuously sending simple health-check requests to the NameNode, and if the NameNode fails to respond correctly over a short period of time the ZKFC will assume the NameNode has failed. If a NameNode machine crashes or otherwise fails, the ZKFC session in ZooKeeper will be closed and the ephemeral ZNode will also be automatically removed.</p><p>The ZKFC processes are also monitoring the ZNodes of the other NameNodes in the cluster. If the ZKFC on the standby NameNode host sees the existing master ZNode disappear, it will assume the master has failed and will attempt a failover. It does this by trying to acquire the lock for the NameNode (through the protocol described in the ZooKeeper section) and if successful will initiate a failover through the same fencing/promotion mechanism described earlier.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec24"></a>HDFS snapshots</h2></div></div><hr /></div><p>We mentioned <a id="id177" class="indexterm"></a>earlier that HDFS replication alone is not a suitable backup strategy. In the Hadoop 2 filesystem, snapshots have been added, which brings another level of data protection to HDFS.</p><p>Filesystem snapshots have been used for some time across a variety of technologies. The basic idea is that it becomes possible to view the exact state of the filesystem at particular points in time. This is achieved by taking a copy of the filesystem metadata at the point the snapshot is made and making this available to be viewed in the future.</p><p>As changes to <a id="id178" class="indexterm"></a>the filesystem are made, any change that would affect the snapshot is treated specially. For example, if a file that exists in the snapshot is deleted then, even though it will be removed from the current state of the filesystem, its metadata will remain in the snapshot, and the blocks associated with its data will remain on the filesystem though not accessible through any view of the system other than the snapshot.</p><p>An example might illustrate this point. Say, you have a filesystem containing the following files:</p><div class="informalexample"><pre class="programlisting">/data1 (5 blocks)
/data2 (10 blocks)</pre></div><p>You take a snapshot and then delete the file <code class="literal">/data2</code>. If you view the current state of the filesystem, then only <code class="literal">/data1</code> will be visible. If you examine the snapshot, you will see both files. Behind the scenes, all 15 blocks still exist, but only those associated with the un-deleted file <code class="literal">/data1</code> are part of the current filesystem. The blocks for the file <code class="literal">/data2</code> will be released only when the snapshot is itself removedâ€”snapshots are read-only views.</p><p>Snapshots in Hadoop 2 can be applied at either the full filesystem level or only on particular paths. A path needs to be set as snapshottable, and note that you cannot have a path snapshottable if any of its children or parent paths are themselves snapshottable.</p><p>Let's take a simple example based on the directory we created earlier to illustrate the use of snapshots. The commands we are going to illustrate need to be executed with superuser privileges, which can be obtained with <code class="literal">sudo -u hdfs</code>.</p><p>First, use the <code class="literal">dfsadmin</code> subcommand of the <code class="literal">hdfs</code> CLI utility to enable snapshots of a directory, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo -u hdfs hdfs dfsadmin -allowSnapshot \</strong></span>
<span class="strong"><strong>/user/cloudera/testdir</strong></span>
<span class="strong"><strong>Allowing snapshot on testdir succeeded</strong></span>
</pre></div><p>Now, we create the snapshot and examine it; snapshots are available through the <code class="literal">.snapshot</code> subdirectory of the snapshottable directory. Note that the <code class="literal">.snapshot</code> directory will not be visible in a normal listing of the directory. Here's how we create a snapshot and examine it:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo -u hdfs hdfs dfs -createSnapshot \</strong></span>
<span class="strong"><strong>/user/cloudera/testdir sn1</strong></span>
<span class="strong"><strong>Created snapshot /user/cloudera/testdir/.snapshot/sn1</strong></span>

<span class="strong"><strong>$ sudo -u hdfs hdfs dfs -ls \</strong></span>
<span class="strong"><strong>/user/cloudera/testdir/.snapshot/sn1</strong></span>

<span class="strong"><strong>Found 1 items -rw-r--r--   1 cloudera cloudera         12 2014-11-13 11:21 /user/cloudera/testdir/.snapshot/sn1/testfile.txt</strong></span>
</pre></div><p>Now, we remove the test file from the main directory and verify that it is now empty:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo -u hdfs hdfs dfs -rm \</strong></span>
<span class="strong"><strong>/user/cloudera/testdir/testfile.txt</strong></span>
<span class="strong"><strong>14/11/13 13:13:51 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 1440 minutes, Emptier interval = 0 minutes. Moved: 'hdfs://localhost.localdomain:8020/user/cloudera/testdir/testfile.txt' to trash at: hdfs://localhost.localdomain:8020/user/hdfs/.Trash/Current</strong></span>
<span class="strong"><strong>$ hdfs dfs -ls /user/cloudera/testdir</strong></span>
<span class="strong"><strong>$</strong></span>
</pre></div><p>Note the mention <a id="id179" class="indexterm"></a>of trash directories; by default, HDFS will copy any deleted files into a <code class="literal">.Trash</code> directory in the user's home directory, which helps to defend against slipping fingers. These files can be removed through <code class="literal">hdfs dfs -expunge</code> or will be automatically purged in 7 days by default.</p><p>Now, we examine the snapshot where the now-deleted file is still available:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hdfs dfs -ls testdir/.snapshot/sn1</strong></span>
<span class="strong"><strong>Found 1 items drwxr-xr-x   - cloudera cloudera          0 2014-11-13 13:12 testdir/.snapshot/sn1</strong></span>
<span class="strong"><strong>$ hdfs dfs -tail testdir/.snapshot/sn1/testfile.txt</strong></span>
<span class="strong"><strong>Hello world</strong></span>
</pre></div><p>Then, we can delete the snapshot, freeing up any blocks held by it, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo -u hdfs hdfs dfs -deleteSnapshot \</strong></span>
<span class="strong"><strong>/user/cloudera/testdir sn1 </strong></span>
<span class="strong"><strong>$ hdfs dfs -ls testdir/.snapshot</strong></span>
<span class="strong"><strong>$</strong></span>
</pre></div><p>As can be seen, the files within a snapshot are fully available to be read and copied, providing access to the historical state of the filesystem at the point when the snapshot was made. Each directory can have up to 65,535 snapshots, and HDFS manages snapshots in such a way that they are quite efficient in terms of impact on normal filesystem operations. They are a great mechanism to use prior to any activity that might have adverse effects, such as trying a new version of an application that accesses the filesystem. If the new software corrupts files, the old state of the directory can be restored. If after a period of validation the software is accepted, then the snapshot can instead be deleted.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec25"></a>Hadoop filesystems</h2></div></div><hr /></div><p>Until<a id="id180" class="indexterm"></a> now, we referred to HDFS as<span class="emphasis"><em> the</em></span> Hadoop filesystem. In reality, Hadoop has a rather abstract notion of filesystem. HDFS is only one of several implementations of the <code class="literal">org.apache.hadoop.fs.FileSystem</code> Java abstract class. A list of available <a id="id181" class="indexterm"></a>filesystems can be found at <a class="ulink" href="https://hadoop.apache.org/docs/r2.5.0/api/org/apache/hadoop/fs/FileSystem.html" target="_blank">https://hadoop.apache.org/docs/r2.5.0/api/org/apache/hadoop/fs/FileSystem.html</a>. The following table summarizes some of these filesystems, along with the corresponding URI scheme and Java implementation class.</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Filesystem</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>URI scheme</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Java implementation</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>Local</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">file</code>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">org.apache.hadoop.fs.LocalFileSystem</code>
</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>HDFS</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">hdfs</code>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">org.apache.hadoop.hdfs.DistributedFileSystem</code>
</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>S3 (native)</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">s3n</code>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">org.apache.hadoop.fs.s3native.NativeS3FileSystem</code>
</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>S3 (block-based)</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">s3</code>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">org.apache.hadoop.fs.s3.S3FileSystem</code>
</p>
</td></tr></tbody></table></div><p>There exist two implementations of the S3 filesystem. Nativeâ€”<code class="literal">s3n</code>â€”is used to read and write regular files. Data stored using <code class="literal">s3n</code> can be accessed by any tool and conversely can be used to read data generated by other S3 tools. <code class="literal">s3n</code>
<a id="id182" class="indexterm"></a> cannot handle files larger than 5TB or rename operations.</p><p>Much like HDFS, the block-based S3 filesystem stores files in blocks and requires an S3 bucket to be dedicated to the filesystem. Files stored in an S3 filesystem can be larger than 5 TB, but they will not be interoperable with other S3 tools. Additionally block-based S3 supports rename operations.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec35"></a>Hadoop interfaces</h3></div></div></div><p>Hadoop <a id="id183" class="indexterm"></a>is written in Java, and not surprisingly, all interaction with the system happens via the Java API. The command-line interface <a id="id184" class="indexterm"></a>we used through the <code class="literal">hdfs</code> command in previous examples is a Java application that uses the <code class="literal">FileSystem</code> class to carry out input/output operations on the available filesystems.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec11"></a>Java FileSystem API</h4></div></div></div><p>The <a id="id185" class="indexterm"></a>Java API, provided by the <code class="literal">org.apache.hadoop.fs</code> package, exposes <a id="id186" class="indexterm"></a>Apache Hadoop filesystems.</p><p>
<code class="literal">org.apache.hadoop.fs.FileSystem</code> is the abstract class each filesystem implements and provides a general interface to interact with data in Hadoop. All code that uses HDFS should be written with the capability of handling a <code class="literal">FileSystem</code> object.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec12"></a>Libhdfs</h4></div></div></div><p>Libhdfs is <a id="id187" class="indexterm"></a>a C library that, despite <a id="id188" class="indexterm"></a>its name, can be used to access any Hadoop filesystem and not just HDFS. It is written using the Java Native Interface (JNI) and mimics the Java FileSystem class.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec13"></a>Thrift </h4></div></div></div><p>
<span class="strong"><strong>Apache Thrift</strong></span> (<a class="ulink" href="http://thrift.apache.org" target="_blank">http://thrift.apache.org</a>) is a <a id="id189" class="indexterm"></a>framework<a id="id190" class="indexterm"></a> for building cross-language <a id="id191" class="indexterm"></a>software through data serialization and remote method invocation mechanisms. The Hadoop Thrift API, available in <code class="literal">contrib</code>, exposes Hadoop filesystems as a Thrift service. This interface makes it easy for non-Java code to access data stored in a Hadoop filesystem.</p><p>Other than the aforementioned interfaces, there exist other interfaces that allow access to Hadoop filesystems via HTTP and FTPâ€”these for HDFS onlyâ€”as well as WebDAV.</p></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec26"></a>Managing and serializing data</h2></div></div><hr /></div><p>Having a<a id="id192" class="indexterm"></a> filesystem is all well and good, but we also need mechanisms to represent data and store it on the filesystems. We will explore some of these mechanisms now.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec36"></a>The Writable interface</h3></div></div></div><p>It is useful, <a id="id193" class="indexterm"></a>to us as developers, if we can manipulate higher-level <a id="id194" class="indexterm"></a>data types and have Hadoop look after the processes required to serialize them into bytes to write to a file system and reconstruct from a stream of bytes when it is read from the file system.</p><p>The <code class="literal">org.apache.hadoop.io package</code> contains the Writable interface, which provides this mechanism and is specified as follows: </p><div class="informalexample"><pre class="programlisting">   public interface Writable
   {
   void write(DataOutput out) throws IOException ;
   void readFields(DataInput in) throws IOException ;
   }</pre></div><p>The main purpose of this interface is to provide mechanisms for the serialization and deserialization of data as it is passed across the network or read and written from the disk.</p><p>When we <a id="id195" class="indexterm"></a>explore processing frameworks on Hadoop in later chapters, we will often see instances where the requirement is for a data argument to be of <a id="id196" class="indexterm"></a>the type Writable. If we use data structures that provide a suitable implementation of this interface, then the Hadoop machinery can automatically manage the serialization and deserialization of the data type without knowing anything about what it represents or how it is used.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec37"></a>Introducing the wrapper classes </h3></div></div></div><p>Fortunately, you<a id="id197" class="indexterm"></a> don't have to start from scratch and build<a id="id198" class="indexterm"></a> Writable variants of all the data types you will use. Hadoop provides classes that wrap the Java primitive types and implement the Writable interface. They are provided in the <code class="literal">org.apache.hadoop.io</code> package.</p><p>These classes are conceptually similar to the primitive wrapper classes, such as Integer and Long, found in <code class="literal">java.lang</code>. They hold a single primitive value that can be set either at construction or via a setter method. They are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">BooleanWritable</code>
</p></li><li style="list-style-type: disc"><p>
<code class="literal">ByteWritable</code>
</p></li><li style="list-style-type: disc"><p>
<code class="literal">DoubleWritable</code>
</p></li><li style="list-style-type: disc"><p>
<code class="literal">FloatWritable</code>
</p></li><li style="list-style-type: disc"><p>
<code class="literal">IntWritable</code>
</p></li><li style="list-style-type: disc"><p>
<code class="literal">LongWritable</code>
</p></li><li style="list-style-type: disc"><p>
<code class="literal">VIntWritable</code>: a variable length integer type </p></li><li style="list-style-type: disc"><p>
<code class="literal">VLongWritable</code>: a variable length long type </p></li><li style="list-style-type: disc"><p>There is also Text, which wraps <code class="literal">java.lang.String</code>.</p></li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec38"></a>Array wrapper classes </h3></div></div></div><p>Hadoop<a id="id199" class="indexterm"></a> also provides some collection-based wrapper classes. These<a id="id200" class="indexterm"></a> classes provide Writable wrappers for arrays of other Writable objects. For example, an instance could either hold an array of <code class="literal">IntWritable</code> or <code class="literal">DoubleWritable</code>, but not arrays of the raw int or float types. A specific subclass for the required Writable class will be required. They are as follows: </p><div class="informalexample"><pre class="programlisting">ArrayWritable
TwoDArrayWritable</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec39"></a>The Comparable and WritableComparable interfaces</h3></div></div></div><p>We were <a id="id201" class="indexterm"></a>slightly inaccurate when we said that the<a id="id202" class="indexterm"></a> wrapper classes<a id="id203" class="indexterm"></a> implement <code class="literal">Writable</code>; they actually implement a composite<a id="id204" class="indexterm"></a> interface called <code class="literal">WritableComparable</code> in the <code class="literal">org.apache.hadoop.io</code> package that combines <code class="literal">Writable</code> with the standard <code class="literal">java.lang.Comparable</code> interface:</p><div class="informalexample"><pre class="programlisting">   public interface WritableComparable extends Writable, Comparable
   {}</pre></div><p>The need for <code class="literal">Comparable</code> will only become apparent when we explore MapReduce in the next chapter, but for now, just remember that the wrapper classes provide mechanisms for them to be both serialized and sorted by Hadoop or any of its frameworks.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec27"></a>Storing data</h2></div></div><hr /></div><p>Until now, we<a id="id205" class="indexterm"></a> introduced the architecture of HDFS and how to programmatically store and retrieve data using the command-line tools and the Java API. In the examples seen until now, we have implicitly assumed that our data was stored as a text file. In reality, some applications and datasets will require ad hoc data structures to hold the file's contents. Over the years, file formats have been created to address both the requirements of MapReduce processingâ€”for instance, we want data to be splittableâ€”and to satisfy the need to model both structured and unstructured data. Currently, a lot of focus has been dedicated to better capture the use cases of relational data storage and modeling. In the remainder of this chapter, we will introduce some of the popular file format choices available within the Hadoop ecosystem.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec40"></a>Serialization and Containers</h3></div></div></div><p>When talking about file formats, we are assuming two types of scenarios, which are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Serialization:</strong></span> we <a id="id206" class="indexterm"></a>want to encode data structures generated and manipulated at processing time to a format we<a id="id207" class="indexterm"></a> can store to a file, transmit, and at a later stage, retrieve and translate back for further manipulation</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Containers</strong></span>: once <a id="id208" class="indexterm"></a>data is serialized to files, containers<a id="id209" class="indexterm"></a> provide means to group multiple files together and add additional metadata</p></li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec41"></a>Compression</h3></div></div></div><p>When working with data, file compression<a id="id210" class="indexterm"></a> can often lead to significant savings both in terms of the space necessary to store files as well as on the data I/O across the network and from/to local disks.</p><p>In broad terms, when using a processing framework, compression can occur at three points in the processing pipeline:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>input files to be processed</p></li><li style="list-style-type: disc"><p>output files that result after processing is completed</p></li><li style="list-style-type: disc"><p>intermediate/temporary files produced internally within the pipeline</p></li></ul></div><p>When we add compression at any of these stages, we have an opportunity to dramatically reduce the amount of data to be read or written to the disk or across the network. This is particularly useful with frameworks such as MapReduce that can, for example, produce volumes of temporary data that are larger than either the input or output datasets.</p><p>Apache Hadoop comes with a number of compression codecs: gzip, bzip2, LZO, snappyâ€”each with its own tradeoffs. Picking a codec is an educated choice that should consider both the kind of data being processed as well as the nature of the processing framework itself.</p><p>Other than the general space/time tradeoff, where the largest space savings come at the expense of compression and decompression speed (and vice versa), we need to take into account that data stored in HDFS will be accessed by parallel, distributed software; some of this software will also add its own particular requirements on file formats. MapReduce, for example, is most efficient on files that can be split into valid subfiles.</p><p>This can complicate decisions, such as the choice of whether to compress and which codec to use if so, as most compression codecs (such as gzip) do not support splittable files, whereas a few (such as LZO) do.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec42"></a>General-purpose file formats</h3></div></div></div><p>The first class <a id="id211" class="indexterm"></a>of file formats are those general-purpose <a id="id212" class="indexterm"></a>ones that can be applied to any application domain and make no assumptions on data structure or access patterns.</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Text</strong></span>: the <a id="id213" class="indexterm"></a>simplest approach to storing<a id="id214" class="indexterm"></a> data on HDFS is to use flat files. Text files can be used both to hold unstructured dataâ€”a web page or a tweetâ€”as well as structured dataâ€”a CSV file that is <a id="id215" class="indexterm"></a>a few million<a id="id216" class="indexterm"></a> rows long. Text files are splittable, though one needs to consider how to handle boundaries between multiple elements (for example, lines) in the file.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>SequenceFile</strong></span>: a<a id="id217" class="indexterm"></a> SequenceFile<a id="id218" class="indexterm"></a> is a flat data structure consisting of binary key/value pairs, introduced to address specific requirements of MapReduce-based processing. It is still extensively used in MapReduce as an input/output format. As we will see in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Processing â€“ MapReduce and Beyond</em></span>, internally, the temporary outputs of maps are stored using SequenceFile.</p></li></ul></div><p>SequenceFile provides <code class="literal">Writer</code>, <code class="literal">Reader</code>, and <code class="literal">Sorter</code> classes to write, read, and, sort data, respectively.</p><p>Depending on the compression mechanism in use, three variations of SequenceFile can be distinguished:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Uncompressed key/value records.</p></li><li style="list-style-type: disc"><p>Record compressed key/value records. Only 'values' are compressed.</p></li><li style="list-style-type: disc"><p>Block compressed key/value records. Keys and values are collected in blocks of arbitrary size and compressed separately.</p></li></ul></div><p>In each case, however, the SequenceFile remains splittable, which is one of its biggest strengths.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec43"></a>Column-oriented data formats</h3></div></div></div><p>In the<a id="id219" class="indexterm"></a> relational database world, column-oriented data stores<a id="id220" class="indexterm"></a> organize and store tables based on the columns; generally speaking, the data for each column will be stored together. This is a significantly different approach compared to most relational DBMS that organize data per row. Column-oriented storage has significant performance advantages; for example, if a query needs to read only two columns from a very wide table containing hundreds of columns, then only the required column data files are accessed. A traditional row-oriented database would have to read all columns for each row for which data was required. This has the greatest impact on workloads where aggregate functions are computed over large numbers of similar items, such as with OLAP workloads typical of data warehouse systems.</p><p>In <a class="link" href="#" linkend="ch07">Chapter 7</a>, <span class="emphasis"><em>Hadoop and SQL</em></span>, we will see how Hadoop is becoming a SQL backend for the data warehouse world thanks to projects such as Apache Hive and Cloudera Impala. As part of the expansion into this domain, a number of file formats have been developed to account for both <a id="id221" class="indexterm"></a>relational modeling and data warehousing needs.</p><p>RCFile, ORC, and Parquet are three state-of-the-art column-oriented file formats developed with these use cases in mind.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec14"></a>RCFile</h4></div></div></div><p>Row Columnar File (RCFile)<a id="id222" class="indexterm"></a> was<a id="id223" class="indexterm"></a> originally developed by Facebook to be used as the backend storage for their Hive data warehouse system that was the first mainstream SQL-on-Hadoop system available as open source.</p><p>RCFile aims to provide the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>fast data loading</p></li><li style="list-style-type: disc"><p>fast query processing</p></li><li style="list-style-type: disc"><p>efficient storage utilization</p></li><li style="list-style-type: disc"><p>adaptability to dynamic workloads </p></li></ul></div><p>More<a id="id224" class="indexterm"></a> information on RCFile can be found at <a class="ulink" href="http://www.cse.ohio-state.edu/hpcs/WWW/HTML/publications/abs11-4.html" target="_blank">http://www.cse.ohio-state.edu/hpcs/WWW/HTML/publications/abs11-4.html</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec15"></a>ORC</h4></div></div></div><p>The<a id="id225" class="indexterm"></a> Optimized Row Columnar file format (ORC) aims<a id="id226" class="indexterm"></a> to combine the performance of the RCFile with the flexibility of Avro. It is primarily intended to work with Apache Hive and has been initially developed by Hortonworks to overcome the perceived limitations of other available file formats.</p><p>More details can be <a id="id227" class="indexterm"></a>found at <a class="ulink" href="http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.0.0.2/ds_Hive/orcfile.html" target="_blank">http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.0.0.2/ds_Hive/orcfile.html</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec16"></a>Parquet</h4></div></div></div><p>Parquet, found at <a class="ulink" href="http://parquet.incubator.apache.org" target="_blank">http://parquet.incubator.apache.org</a>, was <a id="id228" class="indexterm"></a>originally a<a id="id229" class="indexterm"></a> joint effort of <a id="id230" class="indexterm"></a>Cloudera, Twitter, and Criteo, and now has been donated to the Apache Software Foundation. The goals of Parquet are to provide a modern, performant, columnar file format to be used with Cloudera Impala. As with Impala, Parquet has been inspired by the Dremel paper (<a class="ulink" href="http://research.google.com/pubs/pub36632.html" target="_blank">http://research.google.com/pubs/pub36632.html</a>). It allows complex, nested data structures and allows efficient encoding on a per-column level.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec17"></a>Avro</h4></div></div></div><p>Apache Avro (<a class="ulink" href="http://avro.apache.org" target="_blank">http://avro.apache.org</a>) is<a id="id231" class="indexterm"></a> a schema-oriented binary<a id="id232" class="indexterm"></a> data serialization<a id="id233" class="indexterm"></a> format and file container. Avro will be our preferred binary data format throughout this book. It is both splittable and compressible, making it an efficient format for data processing with frameworks such as MapReduce. </p><p>Numerous other projects also have built-in specific Avro support and integration, however, so it is very widely applicable. When data is stored in an Avro file, its schemaâ€”defined as a JSON objectâ€”is stored with it. A file can be later processed by a third party with no a priori notion of how data is encoded. This makes data self-describing and facilitates use with dynamic and scripting languages. The schema-on-read model also helps Avro records to be efficient to store as there is no need for the individual fields to be tagged.</p><p>In later chapters, you will see how these properties can make data life cycle management easier and allow non-trivial operations such as schema migrations.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec18"></a>Using the Java API</h4></div></div></div><p>We'll now <a id="id234" class="indexterm"></a>demonstrate the use of the Java API to parse Avro schemas, read and write Avro files, and use Avro's code generation facilities. Note that the format is intrinsically language independent; there are APIs for most languages, and files created by Java will seamlessly be read from any other language.</p><p>Avro schemas<a id="id235" class="indexterm"></a> are described as JSON documents and represented by the <code class="literal">org.apache.avro.Schema</code> class. To demonstrate the API for manipulating Avro documents, we'll look ahead to an Avro specification we use for a Hive table in <a class="link" href="#" linkend="ch07">Chapter 7</a>, <span class="emphasis"><em>Hadoop and SQL</em></span>. The following code can be found at <a class="ulink" href="https://github.com/learninghadoop2/book-examples/blob/master/ch2/src/main/java/com/learninghadoop2/avro/AvroParse.java" target="_blank">https://github.com/learninghadoop2/book-examples/blob/master/ch2/src/main/java/com/learninghadoop2/avro/AvroParse.java</a>.</p><p>In the following code, we will use the Avro Java API to create an Avro file containing a tweet record and then re-read the file, using the schema in the file to extract the details of the stored records:</p><div class="informalexample"><pre class="programlisting">    public static void testGenericRecord() {
        try {
            Schema schema = new Schema.Parser()
   .parse(new File("tweets_avro.avsc"));
            GenericRecord tweet = new GenericData
   .Record(schema);

            tweet.put("text", "The generic tweet text");

            File file = new File("tweets.avro");
            DatumWriter&lt;GenericRecord&gt; datumWriter = 
               new GenericDatumWriter&lt;&gt;(schema);
            DataFileWriter&lt;GenericRecord&gt; fileWriter = 
               new DataFileWriter&lt;&gt;( datumWriter );

            fileWriter.create(schema, file);
            fileWriter.append(tweet);
            fileWriter.close();

            DatumReader&lt;GenericRecord&gt; datumReader = 
                new GenericDatumReader&lt;&gt;(schema);
            DataFileReader&lt;GenericRecord&gt; fileReader = 
                new DataFileReader(file, datumReader);
            GenericRecord genericTweet = null;

            while (fileReader.hasNext()) {
                genericTweet = (GenericRecord) fileReader
                    .next(genericTweet);

                for (Schema.Field field : 
                    genericTweet.getSchema().getFields()) {
                    Object val = genericTweet.get(field.name());

                    if (val != null) {
                        System.out.println(val);
                    }
                }

            }
        } catch (IOException ie) {
            System.out.println("Error parsing or writing file.");
        }
    }</pre></div><p>The <code class="literal">tweets_avro.avsc</code> schema, found at <a class="ulink" href="https://github.com/learninghadoop2/book-examples/blob/master/ch2/tweets_avro.avsc" target="_blank">https://github.com/learninghadoop2/book-examples/blob/master/ch2/tweets_avro.avsc</a>, <a id="id236" class="indexterm"></a>describes a tweet with multiple fields. To create an Avro object of this type, we first parse the schema file. We then use Avro's concept of a <code class="literal">GenericRecord</code> to build an Avro document that complies with this schema. In this case, we only set a single attributeâ€”the tweet text itself.</p><p>To write this Avro fileâ€”containing a single objectâ€”we then use Avro's I/O capabilities. To read the file, we do not need to start with the schema, as we can extract this from the <code class="literal">GenericRecord</code> we read from the file. We then walk through the schema structure and dynamically process the document based on the discovered fields. This is particularly powerful, as it is the key enabler of clients remaining independent of the Avro schema and how it evolves over time.</p><p>If we have the schema file in advance, however, we can use Avro code generation to create a customized class that makes manipulating Avro records much easier. To generate the code, we will use the compile class in the <code class="literal">avro-tools.jar</code>, passing it the name of the schema file and the desired output directory:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ java -jar /opt/cloudera/parcels/CDH-5.0.0-1.cdh5.0.0.p0.47/lib/avro/avro-tools.jar compile schema tweets_avro.avsc src/main/java</strong></span>
</pre></div><p>The class will be <a id="id237" class="indexterm"></a>placed in a directory structure based on any namespace defined in the schema. Since we created this schema in the <code class="literal">com.learninghadoop2.avrotables</code> namespace, we see the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ls src/main/java/com/learninghadoop2/avrotables/tweets_avro.java</strong></span>
</pre></div><p>With this class, let's revisit the creation and the act of reading and writing Avro objects, as follows:</p><div class="informalexample"><pre class="programlisting">    public static void testGeneratedCode() {
        tweets_avro tweet = new tweets_avro();
        tweet.setText("The code generated tweet text");

        try {
            File file = new File("tweets.avro");
            DatumWriter&lt;tweets_avro&gt; datumWriter = 
                new SpecificDatumWriter&lt;&gt;(tweets_avro.class);
            DataFileWriter&lt;tweets_avro&gt; fileWriter = 
                new DataFileWriter&lt;&gt;(datumWriter);

            fileWriter.create(tweet.getSchema(), file);
            fileWriter.append(tweet);
            fileWriter.close();

            DatumReader&lt;tweets_avro&gt; datumReader = 
                new SpecificDatumReader&lt;&gt;(tweets_avro.class);
            DataFileReader&lt;tweets_avro&gt; fileReader = 
                new DataFileReader&lt;&gt;(file, datumReader);

            while (fileReader.hasNext()) {
                tweet = fileReader.next(tweet);
                System.out.println(tweet.getText());
            }
        } catch (IOException ie) {
            System.out.println("Error in parsing or writingfiles.");
        }
    }</pre></div><p>Because we used code generation, we now use the Avro <code class="literal">SpecificRecord</code> mechanism alongside the generated <a id="id238" class="indexterm"></a>class that represents the object in our domain model. Consequently, we can directly instantiate the object and access its attributes through familiar get/set methods.</p><p>Writing the file is similar to the action performed before, except that we use specific classes and also retrieve the schema directly from the tweet object when needed. Reading is similarly eased through the ability to create instances of a specific class and use get/set methods.</p></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec28"></a>Summary</h2></div></div><hr /></div><p>This chapter has given a whistle-stop tour through storage on a Hadoop cluster. In particular, we covered:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The high-level architecture of HDFS, the main filesystem used in Hadoop</p></li><li style="list-style-type: disc"><p>How HDFS works under the covers and, in particular, its approach to reliability</p></li><li style="list-style-type: disc"><p>How Hadoop 2 has added significantly to HDFS, particularly in the form of NameNode HA and filesystem snapshots</p></li><li style="list-style-type: disc"><p>What ZooKeeper is and how it is used by Hadoop to enable features such as NameNode automatic failover</p></li><li style="list-style-type: disc"><p>An overview of the command-line tools used to access HDFS</p></li><li style="list-style-type: disc"><p>The API for filesystems in Hadoop and how at a code level HDFS is just one implementation of a more flexible filesystem abstraction</p></li><li style="list-style-type: disc"><p>How data can be serialized onto a Hadoop filesystem and some of the support provided in the core classes</p></li><li style="list-style-type: disc"><p>The various file formats available in which data is most frequently stored in Hadoop and some of their particular use cases</p></li></ul></div><p>In the next chapter, we will look in detail at how Hadoop provides processing frameworks that can be used to process the data stored within it.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch03"></a>ChapterÂ 3.Â Processing â€“ MapReduce and Beyond</h2></div></div></div><p>In Hadoop 1, the platform had two clear components: HDFS for data storage and MapReduce for data processing. The previous chapter described the evolution of HDFS in Hadoop 2 and in this chapter we'll discuss data processing.</p><p>The picture with processing in Hadoop 2 has changed more significantly than has storage, and Hadoop now supports multiple processing models as first-class citizens. In this chapter we'll explore both MapReduce and other computational models in Hadoop2. In particular, we'll cover:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>What MapReduce is and the Java API required to write applications for it</p></li><li style="list-style-type: disc"><p>How MapReduce is implemented in practice</p></li><li style="list-style-type: disc"><p>How Hadoop reads data into and out of its processing jobs</p></li><li style="list-style-type: disc"><p>YARN, the Hadoop2 component that allows processing beyond MapReduce on the platform</p></li><li style="list-style-type: disc"><p>An introduction to several computational models implemented on YARN</p></li></ul></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec29"></a>MapReduce</h2></div></div><hr /></div><p>MapReduce<a id="id239" class="indexterm"></a> is the primary processing model supported in Hadoop 1. It follows a divide and conquer model for processing data made popular by a 2006 paper by Google (<a class="ulink" href="http://research.google.com/archive/mapreduce.html" target="_blank">http://research.google.com/archive/mapreduce.html</a>) and has foundations both in <a id="id240" class="indexterm"></a>functional programming and database research. The name itself refers to two distinct steps applied to all input data, a <code class="literal">map</code> function and a <code class="literal">reduce</code> function. </p><p>Every MapReduce application is a sequence of jobs that build atop this very simple model. Sometimes, the overall application may require multiple jobs, where the output of the <code class="literal">reduce</code> stage from one is the input to the <code class="literal">map</code> stage of another, and sometimes there might be multiple <code class="literal">map</code> or <code class="literal">reduce</code> functions, but the core concepts remain the same.</p><p>We will introduce the MapReduce model by looking at the nature of the <code class="literal">map</code> and <code class="literal">reduce</code> functions and then describe the Java API required to build implementations of the functions. After showing some examples, we will walk through a MapReduce execution to give more insight into how the actual MapReduce framework executes code at runtime.</p><p>Learning the <a id="id241" class="indexterm"></a>MapReduce model can be a little counter-intuitive; it's often difficult to appreciate how very simple functions can, when combined, provide very rich processing on enormous datasets. But it does work, trust us!</p><p>As we explore the nature of the <code class="literal">map</code> and <code class="literal">reduce</code> functions, think of them as being applied to a stream of records being retrieved from the source dataset. We'll describe how that happens later; for now, think of the source data being sliced into smaller chunks, each of which gets fed to a dedicated instance of the map function. Each record has the map function applied, producing a set of intermediary data. Records are retrieved from this temporary dataset and all associated records are fed together through the <code class="literal">reduce</code> function. The final output of the <code class="literal">reduce</code> function for all the sets of records is the overall result for the complete job.</p><p>From a functional perspective, MapReduce transforms data structures from one list of (key, value) pairs into another. During the<a id="id242" class="indexterm"></a> <span class="emphasis"><em>Map </em></span>phase, data is loaded from HDFS, and a function is applied in parallel to every input (key, value) and a new list of (key, value) pairs is the output:</p><div class="informalexample"><pre class="programlisting">map(k1,v1) -&gt; list(k2,v2)</pre></div><p>The framework then collects all pairs with the same key from all lists and groups them together, creating one group for each key. A <span class="emphasis"><em>Reduce </em></span>function<a id="id243" class="indexterm"></a> is applied in parallel to each group, which in turn produces a list of values:</p><div class="informalexample"><pre class="programlisting">reduce(k2, list (v2)) â†’ k3,list(v3)</pre></div><p>The output is then written back to HDFS in the following manner:</p><div class="mediaobject"><img src="graphics/5518OS_03_01.jpg" /><div class="caption"><p>Map and Reduce phases</p></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec30"></a>Java API to MapReduce</h2></div></div><hr /></div><p>The Java API to MapReduce<a id="id244" class="indexterm"></a> is exposed by the <code class="literal">org.apache.hadoop.mapreduce</code> package. Writing a MapReduce program, at its core, is a matter of subclassing Hadoop-provided <code class="literal">Mapper</code> and <code class="literal">Reducer</code> base classes, and overriding the <code class="literal">map()</code> and <code class="literal">reduce()</code> methods with our own implementation.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec44"></a>The Mapper class</h3></div></div></div><p>For <a id="id245" class="indexterm"></a>our <a id="id246" class="indexterm"></a>own <code class="literal">Mapper</code> implementations, we will subclass the <code class="literal">Mapper</code> base class and override the <code class="literal">map()</code> method, as follows:</p><div class="informalexample"><pre class="programlisting">   class Mapper&lt;K1, V1, K2, V2&gt;
   {
         void map(K1 key, V1 value Mapper.Context context)
               throws IOException, InterruptedException
         ...
   }</pre></div><p>The class is defined in terms of the key/value input and output types, and then the <code class="literal">map </code>method takes an input key/value pair as its parameter. The other parameter is an instance of the <code class="literal">Context</code> class that provides various mechanisms to communicate with the Hadoop framework, one of which is to output the results of a <code class="literal">map</code> or <code class="literal">reduce</code> method.</p><p>Notice that <a id="id247" class="indexterm"></a>the map method only refers to a single instance of K1 and V1 key/value pairs. This is a critical aspect of the MapReduce paradigm in which you write classes that process single records, and the framework is responsible for <a id="id248" class="indexterm"></a>all the work required to turn an enormous dataset into a stream of key/value pairs. You will never have to write map or reduce classes that try to deal with the full dataset. Hadoop also provides mechanisms through its <code class="literal">InputFormat</code> and <code class="literal">OutputFormat </code>classes that provide implementations of common file formats and likewise remove the need for having to write file parsers for any but custom file types.</p><p>There are three additional methods that sometimes may be required to be overridden:.</p><div class="informalexample"><pre class="programlisting">   protected void setup( Mapper.Context context)
         throws IOException, InterruptedException</pre></div><p>This method is called once before any key/value pairs are presented to the map method. The default implementation does nothing:</p><div class="informalexample"><pre class="programlisting">   protected void cleanup( Mapper.Context context)
         throws IOException, InterruptedException</pre></div><p>This method is called once after all key/value pairs have been presented to the map method. The default implementation does nothing:</p><div class="informalexample"><pre class="programlisting">   protected void run( Mapper.Context context)
         throws IOException, InterruptedException</pre></div><p>This method controls the overall flow of task processing within a JVM. The default implementation calls the setup method once before repeatedly calling the map method for each key/value pair in the split and then finally calls the cleanup method.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec45"></a>The Reducer class</h3></div></div></div><p>The <a id="id249" class="indexterm"></a>
<code class="literal">Reducer</code> base class works very similarly to <a id="id250" class="indexterm"></a>the <code class="literal">Mapper</code> class and usually requires only subclasses to override a single <code class="literal">reduce()</code> method. Here is the cut-down class definition:</p><div class="informalexample"><pre class="programlisting">   public class Reducer&lt;K2, V2, K3, V3&gt;
   {
      void reduce(K2 key, Iterable&lt;V2&gt; values,
         Reducer.Context context)
           throws IOException, InterruptedException
      ...
   }</pre></div><p>Again, notice the class definition in terms of the broader data flow (the <code class="literal">reduce</code> method accepts <code class="literal">K2</code>/<code class="literal">V2</code> as input and provides <code class="literal">K3</code>/<code class="literal">V3</code> as output), while the actual <code class="literal">reduce</code> method takes only a single key <a id="id251" class="indexterm"></a>and its associated list of values. The Context object is again the mechanism to output the result of the method.</p><p>This class <a id="id252" class="indexterm"></a>also has the setup, run and cleanup methods with similar default implementations as with the <code class="literal">Mapper</code> class that can optionally be overridden:</p><div class="informalexample"><pre class="programlisting">protected void setup(Reducer.Context context)
throws IOException, InterruptedException</pre></div><p>The <code class="literal">setup()</code> method is called once before any key/lists of values are presented to the <code class="literal">reduce</code> method. The default implementation does nothing:</p><div class="informalexample"><pre class="programlisting">protected void cleanup(Reducer.Context context)
throws IOException, InterruptedException</pre></div><p>The <code class="literal">cleanup()</code> method is called once after all key/lists of values have been presented to the <code class="literal">reduce</code> method. The default implementation does nothing:</p><div class="informalexample"><pre class="programlisting">protected void run(Reducer.Context context)
throws IOException, InterruptedException</pre></div><p>The <code class="literal">run()</code> method controls the overall flow of processing the task within the JVM. The default implementation calls the setup method before repeatedly and potentially concurrently calling the <code class="literal">reduce</code> method for as many key/value pairs provided to the <code class="literal">Reducer</code> class, and then finally calls the cleanup method.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec46"></a>The Driver class</h3></div></div></div><p>The <a id="id253" class="indexterm"></a>Driver class <a id="id254" class="indexterm"></a>communicates with the Hadoop framework and specifies the configuration elements needed to run a MapReduce job. This involves aspects such as telling Hadoop which <code class="literal">Mapper</code> and <code class="literal">Reducer</code> classes to use, where to find the input data and in what format, and where to place the output data and how to format it.</p><p>The driver logic usually exists in the main method of the class written to encapsulate a MapReduce job. There is no default parent Driver class to subclass:</p><div class="informalexample"><pre class="programlisting">public class ExampleDriver extends Configured implements Tool
   {
   ...
   public static void run(String[] args) throws Exception
   {
      // Create a Configuration object that is used to set other options
      Configuration conf = getConf();

      // Get command line arguments
      args = new GenericOptionsParser(conf, args)
      .getRemainingArgs();

      // Create the object representing the job
      Job job = new Job(conf, "ExampleJob");

      // Set the name of the main class in the job jarfile
      job.setJarByClass(ExampleDriver.class);
      // Set the mapper class
      job.setMapperClass(ExampleMapper.class);

      // Set the reducer class
      job.setReducerClass(ExampleReducer.class);

      // Set the types for the final output key and value
      job.setOutputKeyClass(Text.class);
      job.setOutputValueClass(IntWritable.class);

      // Set input and output file paths
      FileInputFormat.addInputPath(job, new Path(args[0]));
      FileOutputFormat.setOutputPath(job, new Path(args[1]));

      // Execute the job and wait for it to complete
      System.exit(job.waitForCompletion(true) ? 0 : 1);
   }

   public static void main(String[] args) throws Exception
   {
      int exitCode = ToolRunner.run(new ExampleDriver(), args);
      System.exit(exitCode);
    }
}</pre></div><p>In the preceding<a id="id255" class="indexterm"></a> lines of code, <code class="literal">org.apache.hadoop.util.Tool</code> is an interface for handling command-line options. The actual<a id="id256" class="indexterm"></a> handling is delegated to <code class="literal">ToolRunner.run</code>, which runs <code class="literal">Tool</code> with the given<code class="literal"> Configuration </code>used to get and set a job's configuration options. By subclassing <code class="literal">org.apache.hadoop.conf.Configured</code>, we can set the <code class="literal">Configuration</code> object directly from command-line options via<code class="literal"> GenericOptionsParser</code>.</p><p>Given our previous talk of jobs, it's not surprising that much of the setup involves operations on a job object. This includes setting the job name and specifying which classes are to be used for the mapper and reducer implementations.</p><p>Certain input/output configurations are set and, finally, the arguments passed to the main method are used to specify the input and output locations for the job. This is a very common model that you will see often.</p><p>There are a <a id="id257" class="indexterm"></a>number of default values for configuration options, and we are implicitly using some of them in the preceding class. Most notably, we don't say anything about the format of the <a id="id258" class="indexterm"></a>input files or how the output files are to be written. These are defined through the <code class="literal">InputFormat</code> and <code class="literal">OutputFormat</code> classes mentioned earlier; we will explore them in detail later. The default input and output formats are text files that suit our examples. There are multiple ways of expressing the format within text files in addition to particularly optimized binary formats.</p><p>A common model for less complex MapReduce jobs is to have the <code class="literal">Mapper</code> and <code class="literal">Reducer</code> classes as inner classes within the driver. This allows everything to be kept in a single file, which simplifies the code distribution.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec47"></a>Combiner</h3></div></div></div><p>Hadoop <a id="id259" class="indexterm"></a>allows the use of a combiner class<a id="id260" class="indexterm"></a> to perform some early sorting of the output from the <code class="literal">map</code> method before it's retrieved by the reducer.</p><p>Much of Hadoop's design is predicated on reducing the expensive parts of a job that usually equate to disk and network I/O. The output of the mapper is often large; it's not infrequent to see it many times the size of the original input. Hadoop does allow configuration options to help reduce the impact of the reducers transferring such large chunks of data across the network. The combiner takes a different approach where it's possible to perform early aggregation to require less data to be transferred in the first place.</p><p>The combiner does not have its own interface; a combiner must have the same signature as the reducer, and hence also subclasses the Reduce class from the <code class="literal">org.apache.hadoop.mapreduce</code> package. The effect of this is to basically perform a mini-reduce on the mapper for the output destined for each reducer.</p><p>Hadoop does not guarantee whether the combiner will be executed. Sometimes, it may not be executed at all, while at other times it may be used once, twice, or more times depending on the size and number of output files generated by the mapper for each reducer.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec48"></a>Partitioning</h3></div></div></div><p>One of the<a id="id261" class="indexterm"></a> implicit guarantees of the Reduce interface is that a single reducer will be given all the values associated with a given key. With<a id="id262" class="indexterm"></a> multiple reduce tasks running across a cluster, each mapper output must be partitioned into the separate outputs destined for each reducer. These partitioned files are stored on the local node filesystem.</p><p>The number of reduce tasks across the cluster is not as dynamic as that of mappers, and indeed we can specify the value as part of our job submission. Hadoop therefore, knows how many reducers will be needed to complete the job, and from this, it knows into how many partitions the mapper output should be split.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec19"></a>The optional partition function</h4></div></div></div><p>Within<a id="id263" class="indexterm"></a> the <code class="literal">org.apache.hadoop.mapreduce</code> package is the <code class="literal">Partitioner</code> class, an abstract class with the following signature:</p><div class="informalexample"><pre class="programlisting">public abstract class Partitioner&lt;Key, Value&gt;
{
  public abstract int getPartition(Key key, Value value, int numPartitions);
}</pre></div><p>By default, Hadoop will use a strategy that hashes the output key to perform the partitioning. This functionality is provided by the <code class="literal">HashPartitioner</code> class within the <code class="literal">org.apache.hadoop.mapreduce.lib.partition</code> package, but it's necessary in some cases to provide a custom subclass of <code class="literal">Partitioner</code> with application-specific partitioning logic. Notice that the <code class="literal">getPartition</code> function takes the key, value, and number of partitions as parameters, any of which can be used by the custom partitioning logic.</p><p>A custom partitioning strategy would be particularly necessary if, for example, the data provided a very uneven distribution when the standard hash function was applied. Uneven partitioning can result in some tasks having to perform significantly more work than others, leading to much longer overall job execution time.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec49"></a>Hadoop-provided mapper and reducer implementations</h3></div></div></div><p>We don't always <a id="id264" class="indexterm"></a>have to <a id="id265" class="indexterm"></a>write our own Mapper and Reducer classes from scratch. Hadoop provides several common Mapper and Reducer implementations that can be used in our jobs. If we don't override any of the methods in the Mapper and Reducer classes, the default implementations are the identity Mapper and Reducer classes, which simply output the input unchanged.</p><p>The mappers are <a id="id266" class="indexterm"></a>found at <code class="literal">org.apache.hadoop.mapreduce.lib.mapper</code> and include the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">InverseMapper</code>: <a id="id267" class="indexterm"></a>returns (value, key) as an output, that is, the input key is output as the value and the input value is output as the key</p></li><li style="list-style-type: disc"><p>
<code class="literal">TokenCounterMapper</code>: <a id="id268" class="indexterm"></a>counts the number of discrete tokens in each line of input</p></li><li style="list-style-type: disc"><p>
<code class="literal">IdentityMapper</code>: <a id="id269" class="indexterm"></a>implements the identity function, mapping inputs directly to outputs</p></li></ul></div><p>The <a id="id270" class="indexterm"></a>reducers are found at <code class="literal">org.apache.hadoop.mapreduce.lib.reduce</code> and currently include the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">IntSumReducer</code>: <a id="id271" class="indexterm"></a>outputs the sum of the list of integer values per key</p></li><li style="list-style-type: disc"><p>
<code class="literal">LongSumReducer</code>:<a id="id272" class="indexterm"></a> outputs the sum of the list of long values per key</p></li><li style="list-style-type: disc"><p>
<code class="literal">IdentityReducer</code>: <a id="id273" class="indexterm"></a>implements the identity function, mapping inputs directly to outputs</p></li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec50"></a>Sharing reference data</h3></div></div></div><p>Occasionally, we<a id="id274" class="indexterm"></a> might want to share data across tasks. For<a id="id275" class="indexterm"></a> instance, if we need to perform a lookup operation on an ID-to-string translation table, we might want such a data source to be accessible by the mapper or reducer. A straightforward approach is to store the data we want to access on HDFS and use the FileSystem API to query it as part of the Map or Reduce steps.</p><p>Hadoop gives us an alternative mechanism to achieve the goal of sharing reference data across all tasks in the job, the Distributed Cache defined by the <code class="literal">org.apache.hadoop.mapreduce.filecache.DistributedCache</code> class. This can be used to efficiently make available common read-only files that are used by the <code class="literal">map</code> or <code class="literal">reduce</code> tasks to all nodes. </p><p>The files can be text data as in this case, but could also be additional JARs, binary data, or archives; anything is possible. The files to be distributed are placed on HDFS and added to the DistributedCache within the job driver. Hadoop copies the files onto the local filesystem of each node prior to job execution, meaning every task has local access to the files.</p><p>An alternative <a id="id276" class="indexterm"></a>is <a id="id277" class="indexterm"></a>to bundle needed files into the job JAR submitted to Hadoop. This does tie the data to the job JAR, making it more difficult to share across jobs and requires the JAR to be rebuilt if the data changes.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec31"></a>Writing MapReduce programs</h2></div></div><hr /></div><p>In this chapter, we <a id="id278" class="indexterm"></a>will be focusing on batch workloads; given a set of historical data, we will look at properties of that dataset. In <a class="link" href="#" linkend="ch04">Chapter 4</a>, <span class="emphasis"><em>Real-time Computation with Samza</em></span>, and <a class="link" href="#" linkend="ch05">Chapter 5</a>, <span class="emphasis"><em>Iterative Computation with Spark</em></span>, we will show how a similar type of analysis can be performed over a stream of text collected in real time.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec51"></a>Getting started</h3></div></div></div><p>In the following examples, we will assume a dataset generated by collecting 1,000 tweets using the <code class="literal">stream.py</code> script, as shown in <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Introduction</em></span>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ python stream.py â€“t â€“n 1000 &gt; tweets.txt</strong></span>
</pre></div><p>We can then copy the dataset into HDFS with:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hdfs dfs -put tweets.txt &lt;destination&gt;</strong></span>
</pre></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip02"></a>Tip</h3><p>Note that until now we have been working only with the text of tweets. In the remainder of this book, we'll extend <code class="literal">stream.py</code> to output additional tweet metadata in JSON format. Keep this in mind before dumping terabytes of messages with <code class="literal">stream.py</code>.</p></div><p>Our first MapReduce <a id="id279" class="indexterm"></a>program will be the canonical WordCount example. A variation of this program will be used to determine trending topics. We will then analyze text associated with topics to determine whether it expresses a "positive" or "negative" sentiment. Finally, we will make use of a MapReduce patternâ€”ChainMapperâ€”to pull things together and present a data pipeline to clean and prepare the textual data we'll feed to the trending topic and sentiment analysis model.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec52"></a>Running the examples</h3></div></div></div><p>The full source code <a id="id280" class="indexterm"></a>of the examples described in this <a id="id281" class="indexterm"></a>section can be found at <a class="ulink" href="https://github.com/learninghadoop2/book-examples/tree/master/ch3" target="_blank">https://github.com/learninghadoop2/book-examples/tree/master/ch3</a>.</p><p>Before we run our job in Hadoop, we must compile our code and collect the required class files into a single JAR file that we will submit to the system. Using Gradle, you can build the needed JAR file with:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ./gradlew jar</strong></span>
</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec20"></a>Local cluster</h4></div></div></div><p>Jobs are<a id="id282" class="indexterm"></a> executed on Hadoop using the JAR option to the Hadoop command-line utility. To use this, we specify the name of the JAR file, the main class within it, and any arguments that will be passed to the main class, as shown in the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hadoop jar &lt;job jarfile&gt; &lt;main class&gt; &lt;argument 1&gt; â€¦ &lt;argument 2&gt;</strong></span>
</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec21"></a>Elastic MapReduce</h4></div></div></div><p>Recall from <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Introduction</em></span>, that <a id="id283" class="indexterm"></a>Elastic MapReduce expects the job JAR file and its input data to be located in an S3 bucket and conversely will dump its output back into S3.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note10"></a>Note</h3><p>Be careful: this will cost money! For this example, we will use the smallest possible cluster configuration available for EMR, a single-node cluster</p></div><p>First of all, we will copy the tweet dataset and the list of positive and negative words to S3 using the <code class="literal">aws</code> command-line utility:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ aws s3 put tweets.txt s3://&lt;bucket&gt;/input</strong></span>
<span class="strong"><strong>$ aws s3 put job.jar s3://&lt;bucket&gt;</strong></span>
</pre></div><p>We can execute a job using the EMR command-line tool as follows by uploading the JAR file to <code class="literal">s3://&lt;bucket&gt;</code> and adding <code class="literal">CUSTOM_JAR</code> steps with the aws CLI:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ aws emr add-steps --cluster-id &lt;cluster-id&gt; --steps \</strong></span>
<span class="strong"><strong>Type=CUSTOM_JAR,\</strong></span>
<span class="strong"><strong>Name=CustomJAR,\</strong></span>
<span class="strong"><strong>Jar=s3://&lt;bucket&gt;/job.jar,\</strong></span>
<span class="strong"><strong>MainClass=&lt;class name&gt;,\</strong></span>
<span class="strong"><strong>Args=arg1,arg2,â€¦argN</strong></span>
</pre></div><p>Here, <code class="literal">cluster-id</code> is the ID of a running EMR cluster, <code class="literal">&lt;class name&gt;</code> is the fully qualified name of the main class, and <code class="literal">arg1,arg2,â€¦,argN</code> are the job arguments.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec53"></a>WordCount, the Hello World of MapReduce</h3></div></div></div><p>WordCount<a id="id284" class="indexterm"></a> counts word occurrences in a dataset. The source code<a id="id285" class="indexterm"></a> of this example can be found at <a class="ulink" href="https://github.com/learninghadoop2/book-examples/blob/master/ch3/src/main/java/com/learninghadoop2/mapreduce/WordCount.java" target="_blank">https://github.com/learninghadoop2/book-examples/blob/master/ch3/src/main/java/com/learninghadoop2/mapreduce/WordCount.java</a>. Consider the following block of code for example:</p><div class="informalexample"><pre class="programlisting">public class WordCount extends Configured implements Tool
{
    public static class WordCountMapper
            extends Mapper&lt;Object, Text, Text, IntWritable&gt;
    {
        private final static IntWritable one = new IntWritable(1);
        private Text word = new Text();
        public void map(Object key, Text value, Context context
        ) throws IOException, InterruptedException {
            String[] words = value.toString().split(" ") ;
            for (String str: words)
            {
                word.set(str);
                context.write(word, one);
            }
        }
    }
    public static class WordCountReducer
            extends Reducer&lt;Text,IntWritable,Text,IntWritable&gt; {
        public void reduce(Text key, Iterable&lt;IntWritable&gt; values,
                           Context context
        ) throws IOException, InterruptedException {
            int total = 0;
            for (IntWritable val : values) {
                total++ ;
            }
            context.write(key, new IntWritable(total));
        }
    }

    public int run(String[] args) throws Exception {
        Configuration conf = getConf();

        args = new GenericOptionsParser(conf, args)
        .getRemainingArgs();

        Job job = Job.getInstance(conf);

        job.setJarByClass(WordCount.class);
        job.setMapperClass(WordCountMapper.class);
        job.setReducerClass(WordCountReducer.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);

        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));

        return (job.waitForCompletion(true) ? 0 : 1);
    }

    public static void main(String[] args) throws Exception {
        int exitCode = ToolRunner.run(new WordCount(), args);
        System.exit(exitCode);
    }
}</pre></div><p>This is our first <a id="id286" class="indexterm"></a>complete MapReduce job. Look at the structure, and you should recognize the elements we have previously <a id="id287" class="indexterm"></a>discussed: the overall <code class="literal">Job</code> class with the driver configuration in its main method and the Mapper and Reducer implementations defined as static nested classes.</p><p>We'll do a more detailed walkthrough of the mechanics of MapReduce in the next section, but for now, let's look at the preceding code and think of how it realizes the key/value transformations we discussed earlier.</p><p>The input to the Mapper class is arguably the hardest to understand, as the key is not actually used. The job specifies <code class="literal">TextInputFormat</code> as the format of the input data and, by default, this delivers to the mapper data where the key is the byte offset in the file and the value is the text of that line. In reality, you may never actually see a mapper that uses that byte offset key, but it's provided.</p><p>The mapper is executed once for each line of text in the input source, and every time it takes the line and breaks it into words. It then uses the Context object to output (more commonly known as emitting) each new key/value of the form (word, 1). These are our <code class="literal">K2</code>/<code class="literal">V2</code> values.</p><p>We said before that the input to the reducer is a key and a corresponding list of values, and there is some magic that happens between the <code class="literal">map</code> and <code class="literal">reduce</code> methods to collect the values for each key that facilitates thisâ€”called the shuffle stage, which we won't describe right now. Hadoop executes the reducer once for each key, and the preceding reducer implementation simply counts the numbers in the Iterable object and gives output for each word in the form of (word, count). These are our K3/V3 values.</p><p>Take a look at the signatures of our mapper and reducer classes: the <code class="literal">WordCountMapper</code> class accepts <code class="literal">IntWritable</code> and Text as input and provides Text and <code class="literal">IntWritable</code> as output. The <code class="literal">WordCountReducer</code> class has Text and <code class="literal">IntWritable</code> accepted as both input and output. This is again quite a common pattern, where the map method performs an inversion on the key and values, and instead emits a series of data pairs on which the reducer performs aggregation.</p><p>The driver is more meaningful here, as we have real values for the parameters. We use arguments passed to the class to specify the input and output locations.</p><p>Run the job with:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hadoop jar build/libs/mapreduce-example.jar com.learninghadoop2.mapreduce.WordCount \</strong></span>
<span class="strong"><strong> twitter.txt output</strong></span>
</pre></div><p>Examine the output <a id="id288" class="indexterm"></a>with a command such as the<a id="id289" class="indexterm"></a> following; the actual filename might be different, so just look inside the directory called output in your home directory on HDFS:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hdfs dfs -cat output/part-r-00000</strong></span>
</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec54"></a>Word co-occurrences</h3></div></div></div><p>Words <a id="id290" class="indexterm"></a>occurring together are likely to be phrases and commonâ€”frequently occurringâ€”phrases are likely to be important. In Natural Language Processing, a list of co-occurring terms is called an N-Gram. N-Grams are the foundation of several statistical methods for text analytics. We will give an example of the special case of an N-Gramâ€”and a metric often encountered in analytics applicationsâ€”composed of two terms (a bigram). </p><p>A naÃ¯ve implementation in MapReduce would be an extension of WordCount that emits a multi-field key composed of two tab-separated words.</p><div class="informalexample"><pre class="programlisting">public class BiGramCount extends Configured implements Tool
{
   public static class BiGramMapper
           extends Mapper&lt;Object, Text, Text, IntWritable&gt; {
       private final static IntWritable one = new IntWritable(1);
       private Text word = new Text();

       public void map(Object key, Text value, Context context
       ) throws IOException, InterruptedException {
           String[] words = value.toString().split(" ");

           Text bigram = new Text();
           String prev = null;

           for (String s : words) {
               if (prev != null) {
                   bigram.set(prev + "\t+\t" + s);
                   context.write(bigram, one);
               }

               prev = s;
           }
       }
   }

    @Override
    public int run(String[] args) throws Exception {
         Configuration conf = getConf();

         args = new GenericOptionsParser(conf, args).getRemainingArgs();
         Job job = Job.getInstance(conf);
         job.setJarByClass(BiGramCount.class);
         job.setMapperClass(BiGramMapper.class);
         job.setReducerClass(IntSumReducer.class);
         job.setOutputKeyClass(Text.class);
         job.setOutputValueClass(IntWritable.class);
         FileInputFormat.addInputPath(job, new Path(args[0]));
         FileOutputFormat.setOutputPath(job, new Path(args[1]));
         return (job.waitForCompletion(true) ? 0 : 1);
    }

    public static void main(String[] args) throws Exception {
        int exitCode = ToolRunner.run(new BiGramCount(), args);
        System.exit(exitCode);
    }
}</pre></div><p>In this job, we <a id="id291" class="indexterm"></a>replace <code class="literal">WordCountReducer</code> with <code class="literal">org.apache.hadoop.mapreduce.lib.reduce.IntSumReducer</code>, which implements the same logic. The<a id="id292" class="indexterm"></a> source code of this example can be found at <a class="ulink" href="https://github.com/learninghadoop2/book-examples/blob/master/ch3/src/main/java/com/learninghadoop2/mapreduce/BiGramCount.java" target="_blank">https://github.com/learninghadoop2/book-examples/blob/master/ch3/src/main/java/com/learninghadoop2/mapreduce/BiGramCount.java</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec55"></a>Trending topics</h3></div></div></div><p>The <code class="literal">#</code> symbol, called a <a id="id293" class="indexterm"></a>hashtag, is used to mark keywords or topics in a tweet. It was created organically by Twitter users as a way to categorize messages. Twitter Search (found at <a class="ulink" href="https://twitter.com/search-home" target="_blank">https://twitter.com/search-home</a>) popularized<a id="id294" class="indexterm"></a> the use of hashtags as a method to connect and find content related to specific topics as well as the people talking about such topics. By counting the frequency with which a hashtag is mentioned over a given time period, we can<a id="id295" class="indexterm"></a> determine which topics are trending in the social network.</p><div class="informalexample"><pre class="programlisting">public class HashTagCount extends Configured implements Tool
{
    public static class HashTagCountMapper
            extends Mapper&lt;Object, Text, Text, IntWritable&gt;
    {
        private final static IntWritable one = new IntWritable(1);
        private Text word = new Text();

        private String hashtagRegExp =
"(?:\\s|\\A|^)[##]+([A-Za-z0-9-_]+)";

        public void map(Object key, Text value, Context context)
                throws IOException, InterruptedException {
            String[] words = value.toString().split(" ") ;

            for (String str: words)
            {
                if (str.matches(hashtagRegExp)) {
                    word.set(str);
                    context.write(word, one);
                }
            }
        }
    }

    public int run(String[] args) throws Exception {
        Configuration conf = getConf();

        args = new GenericOptionsParser(conf, args)
        .getRemainingArgs();

        Job job = Job.getInstance(conf);

        job.setJarByClass(HashTagCount.class);
        job.setMapperClass(HashTagCountMapper.class);
        job.setCombinerClass(IntSumReducer.class);
        job.setReducerClass(IntSumReducer.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);

        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));

        return (job.waitForCompletion(true) ? 0 : 1);
    }

    public static void main(String[] args) throws Exception {
        int exitCode = ToolRunner.run(new HashTagCount(), args);
        System.exit(exitCode);
    }
}</pre></div><p>As in the <a id="id296" class="indexterm"></a>WordCount example, we tokenize text in the Mapper. We use a regular<a id="id297" class="indexterm"></a> expressionâ€” <code class="literal">hashtagRegExp</code>â€”to detect the presence of a hashtag in Twitter's text and emit the hashtag and the number 1 when a hashtag is found. In the Reducer step, we then count the total number of emitted hashtag occurrences using<code class="literal"> IntSumReducer</code>.</p><p>The full source<a id="id298" class="indexterm"></a> code of this example can be found at <a class="ulink" href="https://github.com/learninghadoop2/book-examples/blob/master/ch3/src/main/java/com/learninghadoop2/mapreduce/HashTagCount.java" target="_blank">https://github.com/learninghadoop2/book-examples/blob/master/ch3/src/main/java/com/learninghadoop2/mapreduce/HashTagCount.java</a>.</p><p>This compiled class will be in the JAR file we built with Gradle earlier, so now we execute HashTagCount with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hadoop jar build/libs/mapreduce-example.jar \</strong></span>
<span class="strong"><strong>com.learninghadoop2.mapreduce.HashTagCount twitter.txt output</strong></span>
</pre></div><p>Let's examine the output as before:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hdfs dfs -cat output/part-r-00000</strong></span>
</pre></div><p>You should see output similar to the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>#whey         1</strong></span>
<span class="strong"><strong>#willpower    1</strong></span>
<span class="strong"><strong>#win          2</strong></span>
<span class="strong"><strong>#winterblues  1</strong></span>
<span class="strong"><strong>#winterstorm  1</strong></span>
<span class="strong"><strong>#wipolitics   1</strong></span>
<span class="strong"><strong>#women        6</strong></span>
<span class="strong"><strong>#woodgrain    1</strong></span>
</pre></div><p>Each line is composed of a hashtag and the number of times it appears in the tweets dataset. As you can see, the MapReduce job orders results by key. If we want to find the most mentioned topics, we need to order the result set. The naÃ¯ve approach would be to perform a total order of the aggregated values and selecting the top 10.</p><p>If the output dataset is small, we can pipe it to standard output and sort it using the <code class="literal">sort</code> utility:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hdfs dfs -cat output/part-r-00000 | sort -k2 -n -r | head -n 10</strong></span>
</pre></div><p>Another solution <a id="id299" class="indexterm"></a>would be to write another MapReduce job to traverse the whole result set and sort by value. When data becomes large, this type of global sorting can become quite expensive. In the following section, we will illustrate an efficient design pattern to sort aggregated data</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec22"></a>The Top N pattern</h4></div></div></div><p>In the <a id="id300" class="indexterm"></a>Top N pattern, we keep data sorted in a local data structure. Each mapper calculates a list of the top N records in its split and sends its list to the reducer. A single reducer task finds the top N global records.</p><p>We will apply this design pattern to implement a <code class="literal">TopTenHashTag</code> job that finds the top ten topics in our dataset. The job takes as input the output data generated by <code class="literal">HashTagCount</code> and returns a list of the ten most frequently mentioned hashtags.</p><p>In <code class="literal">TopTenMapper</code> we use <code class="literal">TreeMap</code> to keep a sorted listâ€”in ascending orderâ€”of hashtags. The key of this map is the number of occurrences; the value is a tab-separated string of<code class="literal"> </code>hashtags and their frequency<code class="literal">.</code> In <code class="literal">map()</code>, for each value, we update the <code class="literal">topN</code> map. When topN has more than ten items, we remove the smallest:</p><div class="informalexample"><pre class="programlisting">public static class TopTenMapper extends Mapper&lt;Object, Text, 
  NullWritable, Text&gt; {

  private TreeMap&lt;Integer, Text&gt; topN = new TreeMap&lt;Integer, Text&gt;();
  private final static IntWritable one = new IntWritable(1);
  private Text word = new Text();
  public void map(Object key, Text value, Context context) throws 
    IOException, InterruptedException {

  String[] words = value.toString().split("\t") ;
  if (words.length &lt; 2) {
    return;
  }
  topN.put(Integer.parseInt(words[1]), new Text(value));
  if (topN.size() &gt; 10) {
    topN.remove(topN.firstKey());
  }
}

       @Override
       protected void cleanup(Context context) throws IOException, InterruptedException {
            for (Text t : topN.values()) {
                context.write(NullWritable.get(), t);
            }
        }
    }</pre></div><p>We don't emit <a id="id301" class="indexterm"></a>any key/value in the map function. We implement a <code class="literal">cleanup()</code> method that, once the mapper has consumed all its input, emits the (hashtag, count) values in <code class="literal">topN</code>. We use a <code class="literal">NullWritable</code> key because we want all values to be associated with the same key so that we can perform a global order over all mappers' top n lists. This implies that our job will execute only one reducer.</p><p>The reducer implements logic similar to what we have in <code class="literal">map()</code>. We instantiate <code class="literal">TreeMap</code> and use it to keep an ordered list of the top 10 values:</p><div class="informalexample"><pre class="programlisting">    public static class TopTenReducer extends
            Reducer&lt;NullWritable, Text, NullWritable, Text&gt; {

        private TreeMap&lt;Integer, Text&gt; topN = new TreeMap&lt;Integer, Text&gt;();

        @Override
        public void reduce(NullWritable key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException {
            for (Text value : values) {
                String[] words = value.toString().split("\t") ;

                topN.put(Integer.parseInt(words[1]),
                    new Text(value));

                if (topN.size() &gt; 10) {
                    topN.remove(topN.firstKey());
                }
            }

            for (Text word : topN.descendingMap().values()) {
                context.write(NullWritable.get(), word);
            }
        }
    }</pre></div><p>Finally, we traverse <code class="literal">topN</code> in descending order to generate the list of trending topics.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note11"></a>Note</h3><p>Note that in this implementation, we override hashtags that have a frequency value already present in <code class="literal">TreeMap</code> when calling <code class="literal">topN.put()</code>. Depending on the use case, it's advised to use a different data structureâ€”such as the ones offered by the <a id="id302" class="indexterm"></a>Guava library (<a class="ulink" href="https://code.google.com/p/guava-libraries/" target="_blank">https://code.google.com/p/guava-libraries/</a>)â€”or adjust the updating strategy.</p></div><p>In the driver, we <a id="id303" class="indexterm"></a>enforce a single reducer by setting <code class="literal">job.setNumReduceTasks(1)</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hadoop jar build/libs/mapreduce-example.jar \</strong></span>
<span class="strong"><strong>com.learninghadoop2.mapreduce.TopTenHashTag \</strong></span>
<span class="strong"><strong>output/part-r-00000 \</strong></span>
<span class="strong"><strong>top-ten</strong></span>
</pre></div><p>We can inspect the top ten to list trending topics:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hdfs dfs -cat top-ten/part-r-00000</strong></span>
<span class="strong"><strong>#Stalker48      150</strong></span>
<span class="strong"><strong>#gameinsight    55</strong></span>
<span class="strong"><strong>#12M    52</strong></span>
<span class="strong"><strong>#KCA    46</strong></span>
<span class="strong"><strong>#LORDJASONJEROME        29</strong></span>
<span class="strong"><strong>#Valencia       19</strong></span>
<span class="strong"><strong>#LesAnges6      16</strong></span>
<span class="strong"><strong>#VoteLuan       15</strong></span>
<span class="strong"><strong>#hadoop2    12</strong></span>
<span class="strong"><strong>#Gameinsight    11</strong></span>
</pre></div><p>The source code of this example<a id="id304" class="indexterm"></a> can be found at <a class="ulink" href="https://github.com/learninghadoop2/book-examples/blob/master/ch3/src/main/java/com/learninghadoop2/mapreduce/TopTenHashTag.java" target="_blank">https://github.com/learninghadoop2/book-examples/blob/master/ch3/src/main/java/com/learninghadoop2/mapreduce/TopTenHashTag.java</a>.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec56"></a>Sentiment of hashtags</h3></div></div></div><p>The<a id="id305" class="indexterm"></a> process of identifying subjective information <a id="id306" class="indexterm"></a>in a data source is commonly referred to as <a id="id307" class="indexterm"></a>sentiment analysis. In the previous example, we show how to detect trending topics in a social network; we'll now analyze the text shared around those topics to determine whether they express a mostly positive or negative sentiment.</p><p>A list of positive and negative words for the <a id="id308" class="indexterm"></a>English languageâ€”a so-called opinion lexiconâ€”can be found at <a class="ulink" href="http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar" target="_blank">http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar</a>.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note12"></a>Note</h3><p>These resourcesâ€”and many moreâ€”have been collected by Prof. Bing Liu's group at the University of Illinois at Chicago and have been used, among others, in <span class="emphasis"><em>Bing Liu, Minqing Hu and Junsheng Cheng. "Opinion Observer: Analyzing and Comparing Opinions on the Web." Proceedings of the 14th International World Wide Web conference (WWW-2005), May 10-14, 2005, Chiba, Japan</em></span>.
</p></div><p>In this example, we'll present a bag-of-words method that, although simplistic in nature, can be used as a <a id="id309" class="indexterm"></a>baseline to mine opinion in text. For each tweet and each hashtag, we will count the number of times a positive or a negative word appears and normalize this count by the text length.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note13"></a>Note</h3><p>The bag-of-words model is an approach used in Natural Language Processing and Information Retrieval to represent textual documents. In this model, text is represented as the set or bagâ€”with multiplicityâ€”of its words, disregarding grammar and morphological properties and even word order.</p></div><p>Uncompress the archive and place the word lists into HDFS with the following command line:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hdfs dfs â€“put positive-words.txt &lt;destination&gt;</strong></span>
<span class="strong"><strong>$ hdfs dfs â€“put negative-words.txt &lt;destination&gt;</strong></span>
</pre></div><p>In the Mapper class, we define two objects that will hold the word lists: <code class="literal">positiveWords</code> and <code class="literal">negativeWords</code> as <code class="literal">Set&lt;String&gt;</code>:</p><div class="informalexample"><pre class="programlisting">private Set&lt;String&gt; positiveWords =  null;
private Set&lt;String&gt; negativeWords = null;</pre></div><p>We override the default <code class="literal">setup()</code> method of the Mapper so that a list of positive and negative wordsâ€”specified by two configuration properties: <code class="literal">job.positivewords.path</code> and <code class="literal">job.negativewords.path</code>â€”is read from HDFS using the filesystem API we discussed in the previous chapter. We could have also used DistributedCache to share this data across the cluster. The helper method, <code class="literal">parseWordsList</code>, reads a list of word lists, strips out comments, and loads words into <code class="literal">HashSet&lt;String&gt;</code>:</p><div class="informalexample"><pre class="programlisting">private HashSet&lt;String&gt; parseWordsList(FileSystem fs, Path wordsListPath)
{
    HashSet&lt;String&gt; words = new HashSet&lt;String&gt;();
    try {

        if (fs.exists(wordsListPath)) {
            FSDataInputStream fi = fs.open(wordsListPath);

            BufferedReader br =
new BufferedReader(new InputStreamReader(fi));
            String line = null;
            while ((line = br.readLine()) != null) {
                if (line.length() &gt; 0 &amp;&amp; !line.startsWith(BEGIN_COMMENT)) {
                    words.add(line);
                }
            }

            fi.close();
        }
    }
    catch (IOException e) {
        e.printStackTrace();
    }

    return words;
}  </pre></div><p>In the Mapper step, we <a id="id310" class="indexterm"></a>emit for each hashtag in the tweet the overall sentiment of the tweet (simply the positive word count minus the negative word count) and the length of the tweet. </p><p>We'll use these in the reducer to calculate an overall sentiment ratio weighted by the length of the tweets to estimate the sentiment expressed by a tweet on a hashtag, as follows:</p><div class="informalexample"><pre class="programlisting">        public void map(Object key, Text value, Context context)
 throws IOException, InterruptedException {
            String[] words = value.toString().split(" ") ;
            Integer positiveCount = new Integer(0);
            Integer negativeCount = new Integer(0);

            Integer wordsCount = new Integer(0);

            for (String str: words)
            {
                if (str.matches(HASHTAG_PATTERN)) {
                    hashtags.add(str);
                }

                if (positiveWords.contains(str)) {
                    positiveCount += 1;
                } else if (negativeWords.contains(str)) {
                    negativeCount += 1;
                }

                wordsCount += 1;
            }

            Integer sentimentDifference = 0;
            if (wordsCount &gt; 0) {
              sentimentDifference = positiveCount - negativeCount;
            }

            String stats ;
            for (String hashtag : hashtags) {
                word.set(hashtag);
                stats = String.format("%d %d", sentimentDifference, wordsCount);
                context.write(word, new Text(stats));
            }
        }
    }</pre></div><p>In the Reducer step, we<a id="id311" class="indexterm"></a> add together the sentiment scores given to each instance of the hashtag and divide by the total size of all the tweets in which it occurred:</p><div class="informalexample"><pre class="programlisting">public static class HashTagSentimentReducer
            extends Reducer&lt;Text,Text,Text,DoubleWritable&gt; {
        public void reduce(Text key, Iterable&lt;Text&gt; values,
                           Context context
        ) throws IOException, InterruptedException {
            double totalDifference = 0;
            double totalWords = 0;
            for (Text val : values) {
                String[] parts = val.toString().split(" ") ;
                totalDifference += Double.parseDouble(parts[0]) ;
                totalWords += Double.parseDouble(parts[1]) ;
            }
            context.write(key,
new DoubleWritable(totalDifference/totalWords));
        }
    }</pre></div><p>The full source <a id="id312" class="indexterm"></a>code of this example can be found at <a class="ulink" href="https://github.com/learninghadoop2/book-examples/blob/master/ch3/src/main/java/com/learninghadoop2/mapreduce/HashTagSentiment.java" target="_blank">https://github.com/learninghadoop2/book-examples/blob/master/ch3/src/main/java/com/learninghadoop2/mapreduce/HashTagSentiment.java</a>.</p><p>After running the preceding code, execute <code class="literal">HashTagSentiment</code> with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hadoop jar build/libs/mapreduce-example.jar com.learninghadoop2.mapreduce.HashTagSentiment twitter.txt output-sentiment &lt;positive words&gt; &lt;negative words&gt;</strong></span>
</pre></div><p>You can examine the output with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hdfs dfs -cat output-sentiment/part-r-00</strong></span>
<span class="strong"><strong>000</strong></span>
</pre></div><p>You should see an output similar to the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>#1068   0.011861271213042056</strong></span>
<span class="strong"><strong>#10YearsOfLove  0.012285135487494233</strong></span>
<span class="strong"><strong>#11     0.011941109121333999</strong></span>
<span class="strong"><strong>#12     0.011938693593171155</strong></span>
<span class="strong"><strong>#12F    0.012339242266249566</strong></span>
<span class="strong"><strong>#12M    0.011864286953783268</strong></span>
<span class="strong"><strong>#12MCalleEnPazYaTeVasNicolas</strong></span>
</pre></div><p>In the preceding <a id="id313" class="indexterm"></a>output, each line is composed of a hashtag and the sentiment polarity associated with it. This number is a heuristic that tells us whether a hashtag is associated mostly with positive (polarity &gt; 0) or negative (polarity &lt; 0) sentiment and the magnitude of such a sentimentâ€”the higher or lower the number, the stronger the sentiment.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec57"></a>Text cleanup using chain mapper</h3></div></div></div><p>In the<a id="id314" class="indexterm"></a> examples presented until now, we ignored a key step of essentially every application built around text processing, which is the normalization and cleanup of the input data. Three common components of this normalization step are:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Changing the letter case to either lower or upper</p></li><li style="list-style-type: disc"><p>Removal of stopwords</p></li><li style="list-style-type: disc"><p>Stemming</p></li></ul></div><p>In this section, we will show how the <code class="literal">ChainMapper</code> classâ€”found at <code class="literal">org.apache.hadoop.mapreduce.lib.chain.ChainMapper</code>â€”allows us to sequentially combine a series of Mappers to put together as the first step of a data cleanup pipeline. Mappers are added to the configured job using the following:</p><div class="informalexample"><pre class="programlisting">ChainMapper.addMapper(
JobConf job,
Class&lt;? extends Mapper&lt;K1,V1,K2,V2&gt;&gt; klass,
Class&lt;? extends K1&gt; inputKeyClass,
Class&lt;? extends V1&gt; inputValueClass,
Class&lt;? extends K2&gt; outputKeyClass,
Class&lt;? extends V2&gt; outputValueClass, JobConf mapperConf)</pre></div><p>The static method, <code class="literal">addMapper</code>, requires the following arguments to be passed:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">job</code>: <a id="id315" class="indexterm"></a>JobConf to add the Mapper class</p></li><li style="list-style-type: disc"><p>
<code class="literal">class</code>:<a id="id316" class="indexterm"></a> Mapper class to add</p></li><li style="list-style-type: disc"><p>
<code class="literal">inputKeyClass</code>: <a id="id317" class="indexterm"></a>mapper input key class</p></li><li style="list-style-type: disc"><p>
<code class="literal">inputValueClass</code>: <a id="id318" class="indexterm"></a>mapper input value class</p></li><li style="list-style-type: disc"><p>
<code class="literal">outputKeyClass</code>: <a id="id319" class="indexterm"></a>mapper output key class</p></li><li style="list-style-type: disc"><p>
<code class="literal">outputValueClass</code>: <a id="id320" class="indexterm"></a>mapper output value class</p></li><li style="list-style-type: disc"><p>
<code class="literal">mapperConf</code>: a<a id="id321" class="indexterm"></a> JobConf with the configuration for the Mapper class</p></li></ul></div><p>In this<a id="id322" class="indexterm"></a> example, we will take care of the first item listed above: before computing the sentiment of each tweet, we will convert to lowercase each word present in its text. This will allow us to more accurately ascertain the sentiment of hashtags by ignoring differences in capitalization across tweets.</p><p>First of all, we define a new Mapperâ€”<code class="literal">LowerCaseMapper</code>â€”whose <code class="literal">map()</code> function calls Java String's <code class="literal">toLowerCase() </code>method on its input value and emits the lower cased text:</p><div class="informalexample"><pre class="programlisting">public class LowerCaseMapper extends Mapper&lt;LongWritable, Text, IntWritable, Text&gt; {
    private Text lowercased = new Text();
    public void map(LongWritable key, Text value, Context context)
throws IOException, InterruptedException {
        lowercased.set(value.toString().toLowerCase());
        context.write(new IntWritable(1), lowercased);
    }
}</pre></div><p>In the <code class="literal">HashTagSentimentChain</code> driver, we configure the Job object so that both Mappers will be <a id="id323" class="indexterm"></a>chained together and executed:</p><div class="informalexample"><pre class="programlisting">public class HashTagSentimentChain
extends Configured implements Tool
{

    public int run(String[] args) throws Exception {
        Configuration conf = getConf();
        args = new GenericOptionsParser(conf,args).getRemainingArgs();

        // location (on hdfs) of the positive words list
        conf.set("job.positivewords.path", args[2]);
        conf.set("job.negativewords.path", args[3]);

        Job job = Job.getInstance(conf);
        job.setJarByClass(HashTagSentimentChain.class);

        Configuration lowerCaseMapperConf = new Configuration(false);
        ChainMapper.addMapper(job,
                LowerCaseMapper.class,
                LongWritable.class, Text.class,
                IntWritable.class, Text.class,
                lowerCaseMapperConf);

        Configuration hashTagSentimentConf = new Configuration(false);
        ChainMapper.addMapper(job,
                HashTagSentiment.HashTagSentimentMapper.class,
                IntWritable.class,
                Text.class, Text.class,
                Text.class,
                hashTagSentimentConf);
        job.setReducerClass(HashTagSentiment.HashTagSentimentReducer.class);


        job.setInputFormatClass(TextInputFormat.class);
        FileInputFormat.addInputPath(job, new Path(args[0]));

        job.setOutputFormatClass(TextOutputFormat.class);
        FileOutputFormat.setOutputPath(job, new Path(args[1]));

        return (job.waitForCompletion(true) ? 0 : 1);
    }

    public static void main (String[] args) throws Exception {
        int exitCode = ToolRunner.run(
new HashTagSentimentChain(), args);
        System.exit(exitCode);
    }
}</pre></div><p>The <code class="literal">LowerCaseMapper</code> and <code class="literal">HashTagSentimentMapper</code> classes are invoked in a pipeline, where the output of the first becomes the input of the second. The output of the last Mapper will be written to the task's output. An immediate benefit of this design is a reduction of disk I/O operations. Mappers do not need to be aware that they are chained. It's therefore possible to reuse specialized Mappers that can be combined within a single task. Note that this pattern assumes that all Mappersâ€”and the Reduceâ€”use matching output and input (key, value) pairs. No casting or conversion is done by ChainMapper itself.</p><p>Finally, notice<a id="id324" class="indexterm"></a> that the <code class="literal">addMapper</code> call for the last mapper in the chain specifies the output key/value classes applicable to the whole mapper pipeline when used as a composite.</p><p>The full source code <a id="id325" class="indexterm"></a>of this example can be found at <a class="ulink" href="https://github.com/learninghadoop2/book-examples/blob/master/ch3/src/main/java/com/learninghadoop2/mapreduce/HashTagSentimentChain.java" target="_blank">https://github.com/learninghadoop2/book-examples/blob/master/ch3/src/main/java/com/learninghadoop2/mapreduce/HashTagSentimentChain.java</a>.</p><p>Execute <code class="literal">HashTagSentimentChain</code> with the command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hadoop jar build/libs/mapreduce-example.jar com.learninghadoop2.mapreduce.HashTagSentimentChain twitter.txt output &lt;positive words&gt; &lt;negative words&gt;</strong></span>
</pre></div><p>You should see an output similar to the previous example. Notice that this time, the hashtag in each line is lowercased.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec32"></a>Walking through a run of a MapReduce job</h2></div></div><hr /></div><p>To explore the<a id="id326" class="indexterm"></a> relationship between mapper and reducer in more detail, and to expose some of Hadoop's inner workings, we'll now go through how a MapReduce job is executed. This applies to both MapReduce in Hadoop 1 and Hadoop 2 even though the latter is implemented very differently using YARN, which we'll discuss later in this chapter. Additional information on the services described in this section, as well as suggestions for troubleshooting MapReduce applications, can be found in <a class="link" href="#" linkend="ch10">Chapter 10</a>, <span class="emphasis"><em>Running a Hadoop Cluster</em></span>.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec58"></a>Startup</h3></div></div></div><p>The driver is the <a id="id327" class="indexterm"></a>only piece of code that runs on our local machine, and the call to <code class="literal">Job.waitForCompletion()</code> starts the communication with the JobTracker, which is the master node in the MapReduce system. The JobTracker is responsible for all aspects of job scheduling and execution, so it becomes our primary interface when performing any task related to job management.</p><p>To share resources on the cluster the JobTracker can use one of several scheduling approaches to handle incoming jobs. The general model is to have a number of queues to which jobs can be submitted along with policies to assign resources across the queues. The most commonly used implementations for these policies are Capacity and Fair Scheduler.</p><p>The JobTracker communicates with the NameNode on our behalf and manages all interactions relating to the data stored on HDFS.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec59"></a>Splitting the input</h3></div></div></div><p>The first <a id="id328" class="indexterm"></a>of these interactions happens when the JobTracker looks at the input data and determines how to assign it to map tasks. Recall that HDFS files are usually split into blocks of at least 64 MB and the JobTracker will assign each block to one map task. Our WordCount example, of course, used a trivial amount of data that was well within a single block. Picture a much larger input file measured in terabytes, and the split model makes more sense. Each segment of the fileâ€”or split, in MapReduce terminologyâ€”is processed uniquely by one map task. Once it has computed the splits, the JobTracker places them and the JAR file containing the Mapper and Reducer classes into a job-specific directory on HDFS, whose path will be passed to each task as it starts.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec60"></a>Task assignment</h3></div></div></div><p>The TaskTracker service is responsible for allocating resources, executing and tracking the status of map and reduce tasks running on a node. Once the <a id="id329" class="indexterm"></a>JobTracker has determined how many map tasks will be needed, it looks at the number of hosts in the cluster, how many TaskTrackers are working, and how many map tasks each can concurrently execute (a user-definable configuration variable). The JobTracker also looks to see where the various input data blocks are located across the cluster and attempts to define an execution plan that maximizes the cases when the TaskTracker processes a split/block located on the same physical host, or, failing that, it processes at least one in the same hardware rack. This data locality optimization is a huge reason behind Hadoop's ability to efficiently process such large datasets. Recall also that, by default, each block is replicated across three different hosts, so the likelihood of producing a task/host plan that sees most blocks processed locally is higher than it might seem at first.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec61"></a>Task startup</h3></div></div></div><p>Each<a id="id330" class="indexterm"></a> TaskTracker then starts up a separate Java virtual machine to execute the tasks. This does add a startup time penalty, but it isolates the TaskTracker from problems caused by misbehaving <code class="literal">map</code> or <code class="literal">reduce</code> tasks, and it can be configured to be shared between subsequently executed tasks.</p><p>If the cluster has enough capacity to execute all the map tasks at once, they will all be started and given a reference to the split they are to process and the job JAR file. If there are more tasks than the cluster capacity, the JobTracker will keep a queue of pending tasks and assign them to nodes as they complete their initially assigned map tasks.</p><p>We are now ready to see the executed data of map tasks. If all this sounds like a lot of work, it is; it explains why, when running any MapReduce job, there is always a non-trivial amount of time taken as the system gets started and performs all these steps.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec62"></a>Ongoing JobTracker monitoring</h3></div></div></div><p>The JobTracker <a id="id331" class="indexterm"></a>doesn't just stop work now and wait for the <a id="id332" class="indexterm"></a>TaskTrackers to execute all the mappers and reducers. It's constantly exchanging heartbeat and status messages with the TaskTrackers, looking for evidence of progress or problems. It also collects metrics from the tasks throughout the job execution, some provided by Hadoop and others specified by the developer of the <code class="literal">map</code> and <code class="literal">reduce</code> tasks, although we don't use any in this example.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec63"></a>Mapper input</h3></div></div></div><p>The <a id="id333" class="indexterm"></a>driver class <a id="id334" class="indexterm"></a>specifies the format and structure of the input file using <code class="literal">TextInputFormat</code>, and from this, Hadoop knows to treat this as text with the byte offset as the key and line contents as the value. Assume that our dataset contains the following text:</p><div class="informalexample"><pre class="programlisting">This is a test
Yes it is</pre></div><p>The<a id="id335" class="indexterm"></a> two <a id="id336" class="indexterm"></a>invocations of the mapper will therefore be given the following output:</p><div class="informalexample"><pre class="programlisting">1 This is a test
2 Yes it is</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec64"></a>Mapper execution</h3></div></div></div><p>The <a id="id337" class="indexterm"></a>key/value pairs received by the mapper are the offset<a id="id338" class="indexterm"></a> in the file of the line and the line contents, respectively, because of how the job is configured. Our implementation of the map method in <code class="literal">WordCountMapper</code> discards the key, as we do not care where each line occurred in the file, and splits the provided value into words using the split method on the standard Java String class. Note<a id="id339" class="indexterm"></a> that better tokenization could be provided by use of regular expressions or the <code class="literal">StringTokenizer</code> class, but for our <a id="id340" class="indexterm"></a>purposes this simple approach will suffice. For each individual word, the mapper then emits a key comprised of the actual word itself, and a value of 1.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec65"></a>Mapper output and reducer input</h3></div></div></div><p>The <a id="id341" class="indexterm"></a>output of the mapper is a series of pairs of the form (word, 1); in our example, these will be:</p><div class="informalexample"><pre class="programlisting">(This,1), (is, 1), (a, 1), (test, 1), (Yes, 1), (it, 1), (is, 1)</pre></div><p>These output <a id="id342" class="indexterm"></a>pairs from the mapper are not passed directly to the reducer. Between mapping and reducing is the shuffle stage, where much of the magic of MapReduce occurs.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec66"></a>Reducer input</h3></div></div></div><p>The <a id="id343" class="indexterm"></a>reducer TaskTracker receives updates from the JobTracker<a id="id344" class="indexterm"></a> that tell it which nodes in the cluster hold <code class="literal">map</code> output partitions that need to be processed by its local <code class="literal">reduce</code> task. It then retrieves these from the various nodes and merges them into a single file that will be fed to the <code class="literal">reduce</code> task.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec67"></a>Reducer execution</h3></div></div></div><p>Our<a id="id345" class="indexterm"></a> <code class="literal">WordCountReducer</code> class is very simple; for each word, it <a id="id346" class="indexterm"></a>simply counts the number of elements in the array and emits the final (word, count) output for each word. For our invocation of WordCount on our sample input, all but one word has only one value in the list of values; <span class="emphasis"><em>is</em></span> has two.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec68"></a>Reducer output</h3></div></div></div><p>The<a id="id347" class="indexterm"></a> final set of reducer output for our example is therefore:</p><div class="informalexample"><pre class="programlisting">(This, 1), (is, 2), (a, 1), (test, 1), (Yes, 1), (it, 1)</pre></div><p>This data will be <a id="id348" class="indexterm"></a>output to partition files within the output directory specified in the driver that will be formatted using the specified OutputFormat implementation. Each <code class="literal">reduce</code> task writes to a single file with the filename <code class="literal">part-r-nnnnn</code>, where <code class="literal">nnnnn</code> starts at <code class="literal">00000</code> and is incremented.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec69"></a>Shutdown</h3></div></div></div><p>Once <a id="id349" class="indexterm"></a>all tasks have completed successfully, the JobTracker outputs the final state of the job to the client, along with the final aggregates of some of the more important counters that it has been aggregating along the way. The full job and task history is available in the log directory on each node or, more accessibly, via the JobTracker web UI; point your browser to port 50030 on the JobTracker node.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec70"></a>Input/Output</h3></div></div></div><p>We have<a id="id350" class="indexterm"></a> talked <a id="id351" class="indexterm"></a>about files being broken into splits as part of the job startup and the data in a split being sent to the mapper implementation. However, this overlooks two aspects: how the data is stored in the file and how the individual keys and values are passed to the mapper structure.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec71"></a>InputFormat and RecordReader</h3></div></div></div><p>Hadoop has<a id="id352" class="indexterm"></a> the concept <a id="id353" class="indexterm"></a>of InputFormat for the first of these responsibilities. The InputFormat abstract <a id="id354" class="indexterm"></a>class in the <code class="literal">org.apache.hadoop.mapreduce</code> package <a id="id355" class="indexterm"></a>provides two methods as shown in the following code:</p><div class="informalexample"><pre class="programlisting">public abstract class InputFormat&lt;K, V&gt;
{
    public abstract List&lt;InputSplit&gt; getSplits( JobContext context);
    RecordReader&lt;K, V&gt; createRecordReader(InputSplit split,
        TaskAttemptContext context) ;
}</pre></div><p>These methods display the two responsibilities of the InputFormat class:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>To provide details on how to divide an input file into the splits required for map processing</p></li><li style="list-style-type: disc"><p>To create a RecordReader that will generate the series of key/value pairs from a split</p></li></ul></div><p>The RecordReader class is also an abstract class within the <code class="literal">org.apache.hadoop.mapreduce</code> package:</p><div class="informalexample"><pre class="programlisting">public abstract class RecordReader&lt;Key, Value&gt; implements Closeable
{
  public abstract void initialize(InputSplit split,
    TaskAttemptContext  context);
  public abstract boolean nextKeyValue()
    throws IOException, InterruptedException;
  public abstract Key getCurrentKey()
    throws IOException, InterruptedException;
  public abstract Value getCurrentValue()
    throws IOException, InterruptedException;
  public abstract float getProgress()
    throws IOException, InterruptedException;
  public abstract close() throws IOException;
}</pre></div><p>A <code class="literal">RecordReader</code> instance<a id="id356" class="indexterm"></a> is created for each split and calls <code class="literal">getNextKeyValue</code> to return a Boolean indicating whether another key/value pair is available, and, if so, the <code class="literal">getKey</code> and <code class="literal">getValue</code> methods are used to access the key and value respectively.</p><p>The combination <a id="id357" class="indexterm"></a>of the <code class="literal">InputFormat</code> and <code class="literal">RecordReader</code> classes therefore are all that is required to bridge between any kind of input data and the key/value pairs required by MapReduce.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec72"></a>Hadoop-provided InputFormat</h3></div></div></div><p>There are some<a id="id358" class="indexterm"></a> Hadoop-provided<a id="id359" class="indexterm"></a> InputFormat implementations within the <code class="literal">org.apache.hadoop.mapreduce.lib.input</code> package:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">FileInputFormat</code>:<a id="id360" class="indexterm"></a> is an abstract base class that can be the parent of any file-based input.</p></li><li style="list-style-type: disc"><p>
<code class="literal">SequenceFileInputFormat</code>: <a id="id361" class="indexterm"></a>is an efficient binary file format that will be discussed in an upcoming section.</p></li><li style="list-style-type: disc"><p>
<code class="literal">TextInputFormat</code>: is<a id="id362" class="indexterm"></a> used for plain text files.</p></li><li style="list-style-type: disc"><p>
<code class="literal">KeyValueTextInputFormat</code>:<a id="id363" class="indexterm"></a> is used for plain text files. Each line is divided into key and value parts by a separator byte.</p></li></ul></div><p>Note that input formats are not restricted to reading from files; FileInputFormat is itself a subclass of InputFormat. It's possible to have Hadoop use data that is not based on files as the input to MapReduce jobs; common sources are relational databases or column-oriented databases, such as Amazon DynamoDB or HBase.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec73"></a>Hadoop-provided RecordReader</h3></div></div></div><p>Hadoop <a id="id364" class="indexterm"></a>provides a few common <code class="literal">RecordReader</code> implementations, which<a id="id365" class="indexterm"></a> are also present within the <code class="literal">org.apache.hadoop.mapreduce.lib.input</code> package:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">LineRecordReader</code>:<a id="id366" class="indexterm"></a> implementation is the default <code class="literal">RecordReader</code> class for text files that presents the byte offset in the file as the key and the line contents as the value</p></li><li style="list-style-type: disc"><p>
<code class="literal">SequenceFileRecordReader</code>:<a id="id367" class="indexterm"></a> implementation reads the key/value from the binary <code class="literal">SequenceFile</code> container</p></li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec74"></a>OutputFormat and RecordWriter</h3></div></div></div><p>There <a id="id368" class="indexterm"></a>is a <a id="id369" class="indexterm"></a>similar pattern for writing the output of a job coordinated by subclasses of <code class="literal">OutputFormat</code> and <code class="literal">RecordWriter</code> from the <code class="literal">org.apache.hadoop.mapreduce</code> package.<a id="id370" class="indexterm"></a> We won't explore these in any detail here, but the general approach is similar, although OutputFormat does have a more involved API, as it has methods for tasks such as validation of the output specification.</p><p>It's this step<a id="id371" class="indexterm"></a> that causes a job to fail if a specified output directory already exists. If you wanted different behavior, it would require a subclass of <code class="literal">OutputFormat</code> that overrides this method.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec75"></a>Hadoop-provided OutputFormat</h3></div></div></div><p>The <a id="id372" class="indexterm"></a>following<a id="id373" class="indexterm"></a> output formats are provided in the <code class="literal">org.apache.hadoop.mapreduce.output</code> package:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">FileOutputFormat</code>:<a id="id374" class="indexterm"></a> is the base class for all file-based OutputFormats</p></li><li style="list-style-type: disc"><p>
<code class="literal">NullOutputFormat</code>:<a id="id375" class="indexterm"></a> is a dummy implementation that discards the output and writes nothing to the file</p></li><li style="list-style-type: disc"><p>
<code class="literal">SequenceFileOutputFormat</code>:<a id="id376" class="indexterm"></a> writes to the binary SequenceFile format</p></li><li style="list-style-type: disc"><p>
<code class="literal">TextOutputFormat</code>: <a id="id377" class="indexterm"></a>writes a plain text file</p></li></ul></div><p>Note that these classes define their required <code class="literal">RecordWriter</code> implementations as static nested classes, so there are no separately provided <code class="literal">RecordWriter</code> implementations.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec76"></a>Sequence files</h3></div></div></div><p>The <code class="literal">SequenceFile</code> class<a id="id378" class="indexterm"></a> within the <code class="literal">org.apache.hadoop.io</code> package provides an efficient binary file format <a id="id379" class="indexterm"></a>that is often useful as an output from a MapReduce job. This is especially true if the output from the job is processed as the input of another job. Sequence files <a id="id380" class="indexterm"></a>have several <a id="id381" class="indexterm"></a>advantages, as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>As binary files, they are intrinsically more compact than text files</p></li><li style="list-style-type: disc"><p>They additionally support optional compression, which can also be applied at different levels, that is, they compress each record or an entire split</p></li><li style="list-style-type: disc"><p>They can be split and processed in parallel</p></li></ul></div><p>This last characteristic is important as most binary formatsâ€”particularly those that are compressed or encryptedâ€”cannot be split and must be read as a single linear stream of data. Using such files as input to a MapReduce job means that a single mapper will be used to process the entire file, causing a potentially large performance hit. In such a situation, it's preferable to use a splittable format, such as SequenceFile, or, if you cannot avoid receiving the file in another format, do a preprocessing step that converts it into a splittable format. This will be a tradeoff, as the conversion will take time, but in many casesâ€”especially with complex map tasksâ€”this will be outweighed by the time saved through increased parallelism.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec33"></a>YARN</h2></div></div><hr /></div><p>YARN started<a id="id382" class="indexterm"></a> out as part of the MapReduce v2 (MRv2) initiative but is now an independent sub-project within Hadoop (that is, it's at the same level as MapReduce). It grew out of a realization that MapReduce in Hadoop 1 conflated two related but distinct responsibilities: resource management and application execution.</p><p>Although it has enabled previously unimagined processing on enormous datasets, the MapReduce model at a conceptual level has an impact on performance and scalability. Implicit in the MapReduce model is that any application can only be composed of a series of largely linear MapReduce jobs, each of which follows a model of one or more maps followed by one or more reduces. This model is a great fit for some applications, but not all. In particular, it's a poor fit for workloads requiring very low-latency response times; the MapReduce startup times and sometimes lengthy job chains often greatly exceed the tolerance for a user-facing process. The model has also been found to be very inefficient for jobs that would more naturally be represented as a directed acyclic graph (DAG)<a id="id383" class="indexterm"></a> of tasks where the nodes on the graph are processing steps, and the edges are data flows. If analyzed and executed as a DAG then the application may be performed in one step with high parallelism across the processing steps, but when viewed through the MapReduce lens, the result is usually an inefficient series of interdependent MapReduce jobs.</p><p>Numerous projects have built different types of processing atop MapReduce and although many are wildly successful (Apache Hive and Pig are two standout examples), the close coupling of MapReduce as a processing paradigm with the job scheduling mechanism in Hadoop1 made it very difficult for any new project to tailor either of these areas to its specific needs.</p><p>The result is <span class="strong"><strong>Yet Another Resource Negotiator</strong></span> (<span class="strong"><strong>YARN</strong></span>), which provides a highly capable job scheduling mechanism within Hadoop and the well-defined interfaces for different processing models to be implemented within it.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec77"></a>YARN architecture</h3></div></div></div><p>To understand <a id="id384" class="indexterm"></a>how YARN works, it's important to stop thinking about MapReduce and how it processes data. YARN itself says nothing about the nature of the applications that run atop it, rather it's focused on providing the machinery for the scheduling and execution of these jobs. As we'll see, YARN is just as capable of hosting <a id="id385" class="indexterm"></a>long-running stream processing or low-latency, user-facing workloads as it is capable of hosting batch-processing workloads, such as MapReduce.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec23"></a>The components of YARN</h4></div></div></div><p>YARN<a id="id386" class="indexterm"></a> is comprised of <a id="id387" class="indexterm"></a>two main components, the <span class="strong"><strong>ResourceManager</strong></span> (<span class="strong"><strong>RM</strong></span>), which <a id="id388" class="indexterm"></a>manages resources across the cluster, and the<a id="id389" class="indexterm"></a> <span class="strong"><strong>NodeManager</strong></span> (<span class="strong"><strong>NM</strong></span>), which <a id="id390" class="indexterm"></a>runs on each host and manages the resources on the individual machine. The ResourceManager and NodeManagers deal with the scheduling and management of containers, an abstract notion of the memory, CPU, and I/O that will be dedicated to run a particular piece of application code. Using MapReduce as an example, when running atop YARN, the JobTracker and each TaskTracker all run in their own dedicated containers. Note though, that in YARN, each MapReduce job has its own dedicated JobTracker; there is no single instance that manages all jobs, as in Hadoop 1.</p><p>YARN itself is responsible only for the scheduling of tasks across the cluster; all notions of application-level progress, monitoring, and fault tolerance are handled in the application code. This is a very explicit design decision; by making YARN as independent as possible, it has a very clear set of responsibilities and does not artificially constrain the types of application that can be implemented on YARN.</p><p>As the arbiter<a id="id391" class="indexterm"></a> of all cluster resources, YARN has the ability to efficiently manage the cluster as a whole and not focus on application-level resource requirements. It has a pluggable scheduling policy with the provided implementations similar to the existing Hadoop Capacity and Fair Scheduler. YARN also treats all application code as inherently untrusted and all application management and control tasks are performed in user space.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec24"></a>Anatomy of a YARN application</h4></div></div></div><p>A submitted<a id="id392" class="indexterm"></a> YARN application has two components: the <a id="id393" class="indexterm"></a>
<span class="strong"><strong>ApplicationMaster</strong></span> (<span class="strong"><strong>AM</strong></span>), which <a id="id394" class="indexterm"></a>coordinates the overall application flow, and the specification of the code that will run on the worker nodes. For MapReduce atop YARN, the JobTracker implements the ApplicationMaster functionality and TaskTrackers are the application custom code deployed on the worker nodes.</p><p>As mentioned in the previous section, the responsibilities of application management, progress monitoring and fault tolerance are pushed to the application level in YARN. It's the ApplicationMaster that performs these tasks; YARN itself says nothing about the mechanisms for communication between the ApplicationMaster and the code running in the worker containers, for example.</p><p>This genericity <a id="id395" class="indexterm"></a>allows YARN applications to not be tied to Java classes. The ApplicationManager can instead request a NodeManager to execute shell scripts, native applications, or any other type of processing that is made available on each node.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec78"></a>Life cycle of a YARN application</h3></div></div></div><p>As <a id="id396" class="indexterm"></a>with MapReduce jobs in Hadoop 1, YARN applications are submitted to the cluster by a client. When a YARN application is started, the client first calls the ResourceManager (more specifically the ApplicationManager  portion of the ResourceManager) and requests the initial container within which to execute the ApplicationMaster. In most cases the ApplicationMaster will run from a hosted container in the cluster, just as will the rest of the application code. The ApplicationManager communicates with the other main component of the ResourceManager, the scheduler itself, which has the ultimate responsibility of managing all resources across the cluster.</p><p>The ApplicationMaster starts up in the provided container, registers itself with the ResourceManager, and begins the process of negotiating its required resources. The ApplicationMaster communicates with the ResourceManager and requests the containers it requires. The specification of the containers requested can also include additional information, such as desired placement within the cluster and concrete resource requirements, such as a particular amount of memory or CPU.</p><p>The ResourceManager provides the ApplicationMaster with the details of the containers it has been allocated, and the ApplicationMaster then communicates with the NodeManagers to start the application-specific task for each container. This is done by providing the NodeManager with the specification of the application to be executed, which as mentioned may be a JAR file, a script, a path to a local executable, or anything else that the NodeManager can invoke. Each NodeManager instantiates the container for the application code and starts the application based on the provided specification.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec25"></a>Fault tolerance and monitoring</h4></div></div></div><p>From<a id="id397" class="indexterm"></a> this point onward, the behavior is largely application specific. YARN will not manage application progress but does perform a number of ongoing tasks. The AMLivelinessMonitor within the ResourceManager receives heartbeats from all ApplicationMasters, and if it determines that an ApplicationMaster has failed or stopped working, it will de-register the failed ApplicationMaster and release all its allocated containers. The ResourceManager will then reschedule the application a configurable number of times.</p><p>Alongside<a id="id398" class="indexterm"></a> this process the NMLivelinessMonitor within the ResourceManager receives heartbeats from the NodeManagers and keeps track of the health of each NodeManager in the cluster. Similar to the monitoring of ApplicationMaster health, a NodeManager will be marked as dead after receiving no heartbeats for a default time of 10 minutes, after which all allocated containers are marked as dead, and the node is excluded from future resource allocation.</p><p>At the same time, the NodeManager will actively monitor resource utilization of each allocated container and, for those resources not constrained by hard limits, will kill containers that exceed their resource allocation.</p><p>At a higher <a id="id399" class="indexterm"></a>level, the YARN scheduler will always be looking to maximize the cluster utilization within the constraints of the sharing policy being employed. As with Hadoop 1, this will allow low-priority applications to use more cluster <a id="id400" class="indexterm"></a>resources if contention is low, but the scheduler will then preempt these additional containers (that is, request them to be terminated) if higher-priority applications are submitted.</p><p>The rest of the responsibility for application-level fault tolerance and progress monitoring must be implemented within the application code. For MapReduce on YARN, for example, all the management of task scheduling and retries is provided at the application level and is not in any way delivered by YARN.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec79"></a>Thinking in layers</h3></div></div></div><p>These last statements may suggest that writing applications to run on YARN is a lot of work, and this is true. The YARN API<a id="id401" class="indexterm"></a> is quite low-level and likely intimidating for most developers who just want to run some processing tasks on their data. If all we had was YARN and every new Hadoop application had to have its own ApplicationMaster implemented, then YARN would not look quite as interesting as it does.</p><p>What makes the picture better is that, in general, the requirement isn't to implement each and every application on YARN, but instead use it for a smaller number of <a id="id402" class="indexterm"></a>processing frameworks that provide much friendlier interfaces to be implemented. The first of these was MapReduce; with it hosted on YARN, the developer writes to the usual <code class="literal">map</code> and <code class="literal">reduce</code> interfaces and is largely unaware of the YARN mechanics.</p><p>But on the same cluster, another developer may be running a job that uses a different framework with significantly different processing characteristics, and YARN will manage both at the same time.</p><p>We'll give some more detail on several YARN processing models<a id="id403" class="indexterm"></a> currently available, but they run the gamut from batch processing through low-latency queries to stream and graph processing and beyond.</p><p>As the YARN experience grows, however, there are a number of initiatives to make the development of these processing frameworks easier. On the one hand there are higher-level interfaces, such<a id="id404" class="indexterm"></a> as <a id="id405" class="indexterm"></a>Cloudera Kitten (<a class="ulink" href="https://github.com/cloudera/kitten" target="_blank">https://github.com/cloudera/kitten</a>) or <a id="id406" class="indexterm"></a>Apache Twill (<a class="ulink" href="http://twill.incubator.apache.org/" target="_blank">http://twill.incubator.apache.org/</a>), that<a id="id407" class="indexterm"></a> give friendlier abstractions above the YARN APIs. Perhaps a more significant development model, though, is the emergence of frameworks that provide richer tools to more easily construct applications with a common general class of performance characteristics.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec80"></a>Execution models</h3></div></div></div><p>We <a id="id408" class="indexterm"></a>have mentioned different YARN applications having distinct processing characteristics, but an emerging pattern has seen their execution models in general being a source of differentiation. By this, we refer to how the YARN application life cycle is managed, and we identify three main types: per-job application, per-session, and always-on.</p><p>Batch processing, such as MapReduce on YARN, sees the life cycle of the MapReduce framework tied to that of the submitted application. If we submit a MapReduce job, then the JobTracker and TaskTrackers that execute it are created specifically for the job and are terminated when the job completes. This works well for batch, but if we wish to provide a more interactive model then the startup overhead of establishing the YARN application and all its resource allocations will severely impact the user experience if every command issued suffers this penalty. A more interactive, or session-based, life cycle will see the YARN application start and then be available to service a number of submitted requests/commands. The YARN application terminates only when the session is exited.</p><p>Finally, we have the concept of long-running applications that process continuous data streams independent of any interactive input. For these it makes most sense for the YARN application to start and continuously process data that is retrieved through some external mechanism. The application will only exit when explicitly shut down or if an abnormal situation occurs.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec34"></a>YARN in the real world â€“ Computation beyond MapReduce</h2></div></div><hr /></div><p>The previous discussions have been a little abstract, so in this section, we will explore a few existing YARN applications <a id="id409" class="indexterm"></a>to see just how they use the framework and how they provide a breadth of processing capability. Of particular interest is how the YARN frameworks take very different approaches to resource management, I/O pipelining, and fault tolerance.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec81"></a>The problem with MapReduce</h3></div></div></div><p>Until now, we <a id="id410" class="indexterm"></a>have looked at MapReduce in terms of API. MapReduce in Hadoop is more than that; up until Hadoop 2, it was the default execution engine for a number of tools, among which were Hive and Pig, which we will discuss in more detail later in this book. We have seen how MapReduce applications are, in fact, chains of jobs. This very aspect is one the biggest pain points and constraining factors of the frameworks. MapReduce checkpoints data to HDFS for intra-process communication:</p><div class="mediaobject"><img src="graphics/5518OS_03_02.jpg" /><div class="caption"><p>A chain of MapReduce jobs</p></div></div><p>At the end of each <code class="literal">reduce</code> phase, output is written to disk so that it can then be loaded by the mappers of the next job and <a id="id411" class="indexterm"></a>used as its input. This I/O overhead introduces latency, especially when we have applications that require multiple passes on a dataset (hence multiple writes). Unfortunately, this type of iterative computation is at the core of many analytics applications.</p><p>Apache Tez and Apache Spark are two frameworks that address this problem by generalizing the MapReduce paradigm. We will briefly discuss them in the remainder of this section, next to Apache Samza, a framework that takes an entirely different approach to real-time processing.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec82"></a>Tez</h3></div></div></div><p>Tez (<a class="ulink" href="http://tez.apache.org" target="_blank">http://tez.apache.org</a>) is <a id="id412" class="indexterm"></a>a low-level API <a id="id413" class="indexterm"></a>and execution <a id="id414" class="indexterm"></a>engine focused on providing low-latency processing, and is being used as the basis of the latest evolution of Hive, Pig and several other frameworks that implement standard join, filter, merge and group operations. Tez is an implementation and evolution of a programming model presented by Microsoft in the 2009 Dryad paper (<a class="ulink" href="http://research.microsoft.com/en-us/projects/dryad/" target="_blank">http://research.microsoft.com/en-us/projects/dryad/</a>). Tez is a generalization of MapReduce as dataflow that strives to achieve fast, interactive computing by pipelining I/O operations over a queue for intra-process communication. This avoids the expensive writes to disks that affect MapReduce. The API provides primitives expressing dependencies between jobs as a DAG. The full DAG is then submitted to a planner that can optimize the execution flow. The same application depicted in the preceding diagram would be executed in Tez as a single job, with I/O pipelined from reducers to reducers without HDFS writes and subsequent reads by mappers. An example can be seen in the following diagram:.</p><div class="mediaobject"><img src="graphics/5518OS_03_03.jpg" /><div class="caption"><p>A Tez DAG is a generalization of MapReduce</p></div></div><p>The<a id="id415" class="indexterm"></a> canonical WordCount example can be found at <a class="ulink" href="https://github.com/apache/incubator-tez/blob/master/tez-mapreduce-examples/src/main/java/org/apache/tez/mapreduce/examples/WordCount.java" target="_blank">https://github.com/apache/incubator-tez/blob/master/tez-mapreduce-examples/src/main/java/org/apache/tez/mapreduce/examples/WordCount.java</a>.</p><div class="informalexample"><pre class="programlisting">DAG dag = new DAG("WordCount");
dag.addVertex(tokenizerVertex)
.addVertex(summerVertex)
.addEdge(new Edge(tokenizerVertex, summerVertex,
edgeConf.createDefaultEdgeProperty()));</pre></div><p>Even though <a id="id416" class="indexterm"></a>the graph topology <code class="literal">dag </code>can be expressed with a few lines of code, the boilerplate required to execute the job is considerable. This code handles many of the low-level scheduling and execution responsibilities, including fault tolerance. When Tez detects a failed task, it walks back up the processing graph to find the point from which to re-execute the failed tasks.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec26"></a>Hive-on-tez</h4></div></div></div><p>Hive 0.13 is<a id="id417" class="indexterm"></a> the <a id="id418" class="indexterm"></a>first high-profile project to use Tez as its execution engine. We'll discuss Hive <a id="id419" class="indexterm"></a>in a lot more detail in <a class="link" href="#" linkend="ch07">Chapter 7</a>, <span class="emphasis"><em>Hadoop and SQL</em></span>, but for now we will just touch on how it's implemented on YARN.</p><p>Hive (<a class="ulink" href="http://hive.apache.org" target="_blank">http://hive.apache.org</a>) is an <a id="id420" class="indexterm"></a>engine for querying data stored on HDFS through <a id="id421" class="indexterm"></a>standard SQL syntax. It has been enormously successful, as this type of capability greatly reduces the barriers to start analytic exploration of data in Hadoop.</p><p>In Hadoop 1, Hive had no choice, but to implement its SQL statements as a series of MapReduce jobs. When SQL is submitted to Hive, it generates the required MapReduce jobs behind the scenes and executes these on the cluster. This approach has two main drawbacks: there is a non-trivial startup penalty each time, and the constrained MapReduce model means that seemingly simple SQL statements are often translated into a lengthy series of multiple dependent MapReduce jobs. This is an example of the type of processing more naturally conceptualized as a DAG of tasks, as described earlier in this chapter.</p><p>Although some benefits are achieved when Hive executes within MapReduce, within YARN, the major benefits come in Hive 0.13 when the project is fully re-implemented using Tez. By exploiting the Tez APIs, which are focused on providing low-latency processing, Hive gains even more performance while making its codebase simpler.</p><p>Since Tez treats its workloads as the DAGs which provide a much better fit to translated SQL queries, Hive on Tez can perform any SQL statement as a single job with maximized parallelism.</p><p>Tez helps Hive support interactive queries by providing an always-running service instead of requiring the application to be instantiated from scratch for each SQL submission. This is important because, even though queries that process huge data volumes will simply take some time, the goal is for Hive to become less of a batch tool and instead move to be as much of an interactive tool as possible.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec83"></a>Apache Spark</h3></div></div></div><p>Spark (<a class="ulink" href="http://spark.apache.org" target="_blank">.apache.org</a>) is a<a id="id422" class="indexterm"></a> processing framework <a id="id423" class="indexterm"></a>that excels at<a id="id424" class="indexterm"></a> iterative and near real-time processing. Created at UC Berkeley, it has been donated as an Apache project. Spark provides an abstraction that allows data in Hadoop to be viewed as a distributed data structure upon which a series of operations can be performed. The framework is based on the same concepts Tez draws inspiration from (Dryad), but excels with jobs that allow data to be held and processed in memory, and it can very efficiently schedule processing on the in-memory dataset across the cluster. Spark automatically controls replication of data across the cluster, ensuring that each element of the distributed dataset is held in memory on at least two machines, and provides replication-based fault tolerance somewhat akin to HDFS.</p><p>Spark started as a standalone system, but was ported to also run on YARN as of its 0.8 release. Spark is particularly interesting because, although its classic processing model is batch-oriented, with the Spark shell it provides an interactive frontend and with the Spark Streaming sub-project also offers near real-time processing of data streams. Spark is different things to different people; it's both a high-level API and an execution engine. At the time of writing, ports of Hive and Pig to Spark are in progress.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec84"></a>Apache Samza</h3></div></div></div><p>Samza (<a class="ulink" href="http://samza.apache.org" target="_blank">http://samza.apache.org</a>) is a <a id="id425" class="indexterm"></a>stream-processing<a id="id426" class="indexterm"></a> framework <a id="id427" class="indexterm"></a>developed at LinkedIn and donated to the Apache Software Foundation. Samza processes conceptually infinite streams of data, which are seen by the application as a series of messages.</p><p>Samza currently integrates most tightly with<a id="id428" class="indexterm"></a> Apache Kafka (<a class="ulink" href="http://kafka.apache.org" target="_blank">http://kafka.apache.org</a>) although it does have a pluggable architecture. Kafka itself is a messaging system that excels at large data volumes and provides a topic-based abstraction similar to most other messaging platforms, such as RabbitMQ. Publishers send messages to topics and interested clients consume messages from the topics as they arrive. Kafka has multiple aspects that set it apart from other messaging platforms, but for this discussion, the most interesting one is that Kafka stores messages for a period of time, which allows messages in topics to be replayed. Topics are partitioned across multiple hosts and partitions can be replicated across hosts to protect from node failure.</p><p>Samza builds its processing flow on its concept of streams, which when using Kafka map directly to Kafka partitions. A typical Samza job may listen to one topic for incoming messages, perform some transformations, and then write the output to a different topic. Multiple Samza jobs can then be composed to provide more complex processing structures.</p><p>As a YARN application, the Samza ApplicationMaster monitors the health of all running Samza tasks. If a task fails, then a replacement task is instantiated in a new container. Samza achieves fault tolerance by having each <a id="id429" class="indexterm"></a>task write its progress to a new stream (again modeled as a Kafka topic), so any replacement task just needs to read the latest task state from this checkpoint topic and then replay the main message topic from the last processed position. Samza additionally offers support for local task state, which can be very useful for join and aggregation type workloads. This local state is again built atop the stream abstraction and hence is intrinsically resilient to host failure.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec27"></a>YARN-independent frameworks</h4></div></div></div><p>An interesting <a id="id430" class="indexterm"></a>point to note is that two of the preceding projects (Samza and Spark) run atop YARN but are not specific to YARN. Spark started out as a standalone service and has implementations for other schedulers, such as Apache Mesos or to run on Amazon EC2. Though Samza runs only on YARN today, its architecture explicitly is not YARN-specific, and there are discussions about providing realizations on other platforms.</p><p>If the YARN model of pushing as much as possible into the application has its downsides through implementation complexity, then this decoupling is one of its major benefits. An application written to use YARN need not be tied to it; by definition, all the functionality for the actual application logic and management is encapsulated within the application code and is independent of YARN or another framework. This is, of course, not saying that designing a scheduler-independent application is a trivial task, but it's now a tractable task; this was absolutely not the case pre-YARN.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec85"></a>YARN today and beyond</h3></div></div></div><p>Though <a id="id431" class="indexterm"></a>YARN has <a id="id432" class="indexterm"></a>been used in production (at Yahoo! in particular) for some time, the final GA version was not released until late 2012. The interfaces to YARN were also somewhat fluid until quite late in the development cycle. Consequently, the fully forward compatible YARN as of Hadoop 2.2 is still relatively new.</p><p>YARN is fully functional today, and the future direction will see extensions to its current capabilities. Perhaps most notable among these will be the ability to specify and control container resources on more dimensions. Currently, only location, memory and CPU specifications are possible, and this will be expanded into areas such as storage and network I/O.</p><p>In addition, the ApplicationMaster  currently has little control over the management of how containers are co-located or not. Finer-grained control here will allow the ApplicationMaster to specify policies around when containers may or may not be scheduled on the same node. In addition, the current resource allocation model is quite static, and it will be useful to allow an application to dynamically change the resources allocated to a running container.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec35"></a>Summary</h2></div></div><hr /></div><p>This chapter explored how to process those large volumes of data that we discussed so much in the previous chapter. In particular we covered:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>How MapReduce was the only processing model available in Hadoop 1 and its conceptual model</p></li><li style="list-style-type: disc"><p>The Java API to MapReduce, and how to use this to build some examples, from a word count to sentiment analysis of Twitter hashtags</p></li><li style="list-style-type: disc"><p>The details of how MapReduce is implemented in practice, and we walked through the execution of a MapReduce job</p></li><li style="list-style-type: disc"><p>How Hadoop stores data and the classes involved to represent input and output formats and record readers and writers</p></li><li style="list-style-type: disc"><p>The limitations of MapReduce that led to the development of YARN, opening the door to multiple computational models on the Hadoop platform</p></li><li style="list-style-type: disc"><p>The YARN architecture and how applications are built atop it</p></li></ul></div><p>In the next two chapters, we will move away from strictly batch processing and delve into the world of near real-time and iterative processing, using two of the YARN-hosted frameworks we introduced in this chapter, namely Samza and Spark.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch04"></a>ChapterÂ 4.Â Real-time Computation with Samza</h2></div></div></div><p>The previous chapter discussed YARN, and frequently mentioned the breadth of computational models and processing frameworks outside of traditional batch-based MapReduce that it enables on the Hadoop platform. In this chapter and the next, we will explore two such projects in some depth, namely Apache Samza and Apache Spark. We chose these frameworks as they demonstrate the usage of stream and iterative processing and also provide interesting mechanisms to combine processing paradigms. In this chapter we will explore Samza and cover the following topics:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>What Samza is and how it integrates with YARN and other projects such as Apache Kafka</p></li><li style="list-style-type: disc"><p>How Samza provides a simple callback-based interface for stream processing</p></li><li style="list-style-type: disc"><p>How Samza composes multiple stream processing jobs into more complex workflows</p></li><li style="list-style-type: disc"><p>How Samza supports persistent local state within tasks and how this greatly enriches what it can enable</p></li></ul></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec36"></a>Stream processing with Samza</h2></div></div><hr /></div><p>To explore a <a id="id433" class="indexterm"></a>pure stream-processing platform, <a id="id434" class="indexterm"></a>we will use Samza, which is <a id="id435" class="indexterm"></a>available at <a class="ulink" href="https://samza.apache.org" target="_blank">https://samza.apache.org</a>. The code shown here was tested with the current 0.8 release and we'll keep the GitHub repository updated as the project continues to evolve.</p><p>Samza was built at LinkedIn and donated to the Apache Software Foundation in September 2013. Over the years, LinkedIn has built a model that conceptualizes much of their data as streams, and from this they saw the need for a framework that can provide a developer-friendly mechanism to process these ubiquitous data streams.</p><p>The team at<a id="id436" class="indexterm"></a> LinkedIn realized that when it came to data <a id="id437" class="indexterm"></a>processing, much of the attention went to the extreme ends of the spectrum, for example, RPC workloads are usually implemented as synchronous systems with very low latency requirements or batch systems where the periodicity of jobs is often measured in hours. The ground in between has been relatively poorly supported and this is the area that Samza is targeted at; most of its jobs expect response times ranging from milliseconds to minutes. They also assume that data arrives in a theoretically infinite stream of continuous messages.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec86"></a>How Samza works</h3></div></div></div><p>There are<a id="id438" class="indexterm"></a> numerous<a id="id439" class="indexterm"></a> stream-processing systems such as Storm (<a class="ulink" href="http://storm.apache.org" target="_blank">http://storm.apache.org</a>), in the open source world, and many other (mostly commercial) tools such as <span class="strong"><strong>complex event processing</strong></span> (<span class="strong"><strong>CEP</strong></span>) systems that also target <a id="id440" class="indexterm"></a>processing on continuous message streams. These <a id="id441" class="indexterm"></a>systems have many similarities but also some major differences.</p><p>For Samza, perhaps the most significant difference is its assumptions about message delivery. Many systems work very hard to reduce the latency of each message, sometimes with an assumption that the goal is to get the message into and out of the system as fast as possible. Samza assumes almost the opposite; its streams are persistent and resilient and any message written to a stream can be re-read for a period of time after its first arrival. As we will see, this gives significant capability around fault tolerance. Samza also builds on this model to allow each of its tasks to hold resilient local state.</p><p>Samza is mostly implemented in Scala even though its public APIs are written in Java. We'll show Java examples in this chapter, but any JVM language can be used to implement Samza applications. We'll discuss Scala when we explore Spark in the next chapter.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec87"></a>Samza high-level architecture</h3></div></div></div><p>Samza views the <a id="id442" class="indexterm"></a>world as having three main layers or components: the <a id="id443" class="indexterm"></a>streaming, execution, and processing layers.</p><div class="mediaobject"><img src="graphics/5518OS_04_01.jpg" /><div class="caption"><p>Samza architecture</p></div></div><p>The streaming layer<a id="id444" class="indexterm"></a> provides access to the data streams, both for <a id="id445" class="indexterm"></a>consumption and publication. The execution layer provides the<a id="id446" class="indexterm"></a> means by which Samza applications can be run, have resources such as CPU and memory allocated, and have their life cycles managed. The processing layer is the actual Samza framework itself, and its interfaces allow per-message functionality.</p><p>Samza provides pluggable interfaces to support the first two layers though the current main implementations use Kafka for streaming and YARN for execution. We'll discuss these further in the following sections.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec88"></a>Samza's best friend â€“ Apache Kafka</h3></div></div></div><p>Samza itself <a id="id447" class="indexterm"></a>does not implement the actual message stream. Instead, it<a id="id448" class="indexterm"></a> provides an interface for a message system with which <a id="id449" class="indexterm"></a>it then integrates. The default stream implementation is built upon <span class="strong"><strong>Apache Kafka</strong></span> (<a class="ulink" href="http://kafka.apache.org" target="_blank">http://kafka.apache.org</a>), a messaging system also built at LinkedIn but now a successful and widely adopted open source project.</p><p>Kafka can be viewed as a message broker akin to something like RabbitMQ or ActiveMQ, but as mentioned earlier, it writes all messages to disk and scales out across multiple hosts as a core part of its design. Kafka uses the concept of a publish/subscribe model through named topics to which producers write messages and from which consumers read messages. These work much like topics in any other messaging system.</p><p>Because Kafka writes all messages to disk, it might not have the same ultra-low latency message throughput as other messaging systems, which focus on getting the message processed <a id="id450" class="indexterm"></a>as fast as possible and don't aim to store the message<a id="id451" class="indexterm"></a> long term. Kafka can, however, scale exceptionally well and its ability to replay a message stream can be extremely useful. For example, if a consuming client fails, then it can re-read messages from a known good point in time, or if a downstream algorithm changes, then traffic can be replayed to utilize the new functionality.</p><p>When scaling across hosts, Kafka partitions topics and supports partition replication for fault tolerance. Each Kafka message has a key associated with the message and this is used to decide to which partition a given message is sent. This allows semantically useful partitioning, for example, if the key is a user ID in the system, then all messages for a given user will be sent to the same partition. Kafka guarantees ordered delivery within each partition so that any client reading a partition can know that they are receiving all messages for each key in that partition in the order in which they are written by the producer.</p><p>Samza periodically writes out checkpoints of the position upto which it has read in all the streams it is consuming. These checkpoint messages are themselves written to a Kafka topic. Thus, when a Samza job starts up, each task can reread its checkpoint stream to know from which position in the stream to start processing messages. This means that in effect Kafka also acts as a buffer; if a Samza job crashes or is taken down for upgrade, no messages will be lost. Instead, the job will just restart from the last checkpointed position when it restarts. This buffer functionality is also important, as it makes it easier for multiple Samza jobs to run as part of a complex workflow. When Kafka topics are the points of coordination between the jobs, one job might consume a topic being written to by another; in such cases, Kafka can help smooth out issues caused due to any given job running slower than others. Traditionally, the back pressure caused by a slow running job can be a real issue in a system comprised of multiple job stages, but Kafka as the resilient buffer allows each job to read and write at its own rate. Note that this is analogous to how multiple coordinating MapReduce jobs will use HDFS for similar purposes.</p><p>Kafka provides at-least once message delivery semantics, that is to say that any message written to Kafka will be guaranteed to be available to a client of the particular partition. Messages might be processed between checkpoints however; it is possible for duplicate messages to be received by the client. There are application-specific mechanisms to mitigate this, and both Kafka and Samza have exactly-once semantics on their roadmaps, but for now it is something you should take into consideration when designing jobs.</p><p>We won't <a id="id452" class="indexterm"></a>explain Kafka further beyond what we need to demonstrate <a id="id453" class="indexterm"></a>Samza. If you are interested, check out its website and wiki; there is a lot of good information, including some excellent papers and presentations.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec89"></a>YARN integration</h3></div></div></div><p>As mentioned earlier, just as Samza utilizes Kafka for its streaming layer implementation, it uses <a id="id454" class="indexterm"></a>YARN for the execution layer. Just like any YARN <a id="id455" class="indexterm"></a>application described in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Processing â€“ MapReduce and Beyond</em></span>, Samza provides an implementation of both an <code class="literal">ApplicationMaster</code>, which controls the life cycle of the overall job, plus implementations of Samza-specific functionality (called tasks) that are executed in each container. Just as Kafka partitions its topics, tasks are the mechanism by which Samza partitions its processing. Each Kafka partition will be read by a single Samza task. If a Samza job consumes multiple streams, then a given task will be the only consumer within the job for every stream partition assigned to it.</p><p>The Samza framework is told by each job configuration about the Kafka streams that are of interest to the job, and Samza continuously polls these streams to determine if any new messages have arrived. When a new message is available, the Samza task invokes a user-defined callback to process the message, a model that shouldn't look too alien to MapReduce developers. This method is defined in an interface called <code class="literal">StreamTask</code> and has the following signature:</p><div class="informalexample"><pre class="programlisting">public void process(IncomingMessageEnvelope envelope,
 MessageCollector collector, 
 TaskCoordinator coordinator)</pre></div><p>This is the core of each Samza task and defines the functionality to be applied to received messages. The received message that is to be processed is wrapped in the <code class="literal">IncomingMessageEnvelope</code>; output messages can be written to the <code class="literal">MessageCollector</code>, and task management (such as Shutdown) can be performed via the <code class="literal">TaskCoordinator</code>.</p><p>As mentioned, Samza creates one task instance for each partition in the underlying Kafka topic. Each YARN container will manage one or more of these tasks. The overall model then is of the Samza Application Master coordinating multiple containers, each of which is responsible for one or more <code class="literal">StreamTask</code> instances.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec90"></a>An independent model</h3></div></div></div><p>Though we will talk exclusively of Kafka and YARN as the providers of Samza's streaming and <a id="id456" class="indexterm"></a>execution layers in this chapter, it is important to remember that the core Samza system uses well-defined interfaces for both the stream and execution systems. There are implementations of multiple stream sources (we'll see one in the next section) and alongside the YARN support, Samza ships with a <code class="literal">LocalJobRunner</code> class. This alternative method of running tasks can execute <code class="literal">StreamTask</code> instances in-process on the JVM instead of requiring a full YARN cluster, which can sometimes be a useful testing and debugging tool. There is also a discussion of Samza implementations on top of other cluster manager or virtualization frameworks.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec91"></a>Hello Samza!</h3></div></div></div><p>Since not everyone already has ZooKeeper, Kafka, and YARN clusters ready to be used, the Samza<a id="id457" class="indexterm"></a> team has created a wonderful way to get started with the<a id="id458" class="indexterm"></a> product. Instead of just having a Hello world! program, there <a id="id459" class="indexterm"></a>is a repository called Hello Samza, which is available by cloning the repository at <a class="ulink" href="http://git://git.apache.org/samza-hello-samza.git" target="_blank">git://git.apache.org/samza-hello-samza.git</a>.</p><p>This will download and install dedicated instances of ZooKeeper, Kafka, and YARN (the 3 major prerequisites for Samza), creating a full stack upon which you can submit Samza jobs.</p><p>There are also a number of example Samza jobs that process data from Wikipedia edit notifications. Take a look at the page at <a class="ulink" href="http://samza.apache.org/startup/hello-samza/0.8/" target="_blank">http://samza.apache.org/startup/hello-samza/0.8/</a> and follow the instructions given there. (At the time of writing, Samza is still a relatively young project and we'd rather not include direct information about the examples, which might be subject to change).</p><p>For the remainder of the Samza examples in this chapter, we'll assume you are either using the Hello Samza package to provide the necessary components (ZooKeeper/Kafka/YARN) or you have integrated with other instances of each.</p><p>This example has three different Samza jobs that build upon each other. The first reads the Wikipedia edits, the second parses these records, and the third produces statistics based on the processed records. We'll build our own multistream workflow shortly.</p><p>One interesting point is the WikipediaFeed example here; it uses Wikipedia as its message source instead of Kafka. Specifically, it provides another implementation of the Samza <code class="literal">SystemConsumer</code> interface to allow Samza to read messages from an external system. As mentioned earlier, Samza is not tied to Kafka and, as this example shows, building a new stream implementation does not have to be against a generic infrastructure component; it can be quite job-specific, as the work required is not huge.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip03"></a>Tip</h3><p>Note <a id="id460" class="indexterm"></a>that the default configuration for both ZooKeeper<a id="id461" class="indexterm"></a> and Kafka will write system data to directories under <code class="literal">/tmp</code>, which will be what you have set if you use Hello Samza. Be careful if you are using a Linux distribution that purges the contents of this directory on a reboot. If you plan to carry out any significant testing, then it's best to reconfigure these components to use less ephemeral locations. Change the relevant config files for each service; they are located in the service directory under the <code class="literal">hello-samza/deploy</code> directory.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec92"></a>Building a tweet parsing job</h3></div></div></div><p>Let's build our own simple job implementation to show the full code required. We'll use parsing <a id="id462" class="indexterm"></a>of the Twitter stream as the examples in this chapter and will later set up a pipe from our client consuming messages from the Twitter API into a Kafka topic. So, we need a Samza task that will read the stream of JSON messages, extract the actual tweet text, and write these to a topic of tweets.</p><p>Here is the main code from <code class="literal">TwitterParseStreamTask.java</code>, available at <a class="ulink" href="https://github.com/learninghadoop2/book-examples/blob/master/ch4/src/main/java/com/learninghadoop2/samza/tasks/TwitterParseStreamTask.java" target="_blank">https://github.com/learninghadoop2/book-examples/blob/master/ch4/src/main/java/com/learninghadoop2/samza/tasks/TwitterParseStreamTask.java</a>:</p><div class="informalexample"><pre class="programlisting">package com.learninghadoop2.samza.tasks;
public class TwitterParseStreamTask implements StreamTask {
    @Override
    public void process(IncomingMessageEnvelope envelope, MessageCollector collector, TaskCoordinator coordinator) {
        String msg = ((String) envelope.getMessage());

        try {
            JSONParser parser  = new JSONParser();
            Object     obj     = parser.parse(msg);
            JSONObject jsonObj = (JSONObject) obj;
            String     text    = (String) jsonObj.get("text");

            collector.send(new OutgoingMessageEnvelope(new SystemStream("kafka", "tweets-parsed"), text));
        } catch (ParseException pe) {}
    }
  }
}</pre></div><p>The code is<a id="id463" class="indexterm"></a> largely self-explanatory, but there are a few points of interest. We use JSON Simple (<a class="ulink" href="http://code.google.com/p/json-simple/" target="_blank">http://code.google.com/p/json-simple/</a>) for our relatively straightforward JSON parsing requirements; we'll also use it later in this book.</p><p>The <code class="literal">IncomingMessageEnvelope</code> and its corresponding <code class="literal">OutputMessageEnvelope</code> are the main structures concerned with the actual message data. Along with the message payload, the<a id="id464" class="indexterm"></a> envelope will also have data concerning the system, topic name, and (optionally) partition number in addition to other metadata. For our purposes, we just extract the message body from the incoming message and send the tweet text we extract from it via a new <code class="literal">OutgoingMessageEnvelope</code> to a topic called <code class="literal">tweets-parsed</code> within a system called <code class="literal">kafka</code>. Note the lower case nameâ€”we'll explain this in a moment.</p><p>The type of message in the <code class="literal">IncomingMessageEnvelope</code> is <code class="literal">java.lang.Object</code>. Samza does not currently enforce a data model and hence does not have strongly-typed message bodies. Therefore, when extracting the message contents, an explicit cast is usually required. Since each task needs to know the expected message format of the streams it processes, this is not the oddity that it may appear to be.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec93"></a>The configuration file</h3></div></div></div><p>There<a id="id465" class="indexterm"></a> was nothing in the previous code that said where the messages <a id="id466" class="indexterm"></a>came from; the framework just presents them to the <code class="literal">StreamTask</code> implementation, but obviously Samza needs to know from where to fetch messages. There is a configuration file for each job that defines this and more. The following can be found as <code class="literal">twitter-parse.properties</code> at <a class="ulink" href="https://github.com/learninghadoop2/book-examples/blob/master/ch4/src/main/resources/twitter-parser.properties" target="_blank">https://github.com/learninghadoop2/book-examples/blob/master/ch4/src/main/resources/twitter-parser.properties</a>:</p><div class="informalexample"><pre class="programlisting"># Job
job.factory.class=org.apache.samza.job.yarn.YarnJobFactory
job.name=twitter-parser

# YARN
yarn.package.path=file:///home/gturkington/samza/build/distributions/learninghadoop2-0.1.tar.gz

# Task
task.class=com.learninghadoop2.samza.tasks.TwitterParseStreamTask
task.inputs=kafka.tweets
task.checkpoint.factory=org.apache.samza.checkpoint.kafka.KafkaCheckpointManagerFactory
task.checkpoint.system=kafka

# Normally, this would be 3, but we have only one broker.
task.checkpoint.replication.factor=1

# Serializers
serializers.registry.string.class=org.apache.samza.serializers.StringSerdeFactory

# Systems
systems.kafka.samza.factory=org.apache.samza.system.kafka.KafkaSystemFactory
systems.kafka.streams.tweets.samza.msg.serde=string
systems.kafka.streams.tweets-parsed.samza.msg.serde=string
systems.kafka.consumer.zookeeper.connect=localhost:2181/
systems.kafka.consumer.auto.offset.reset=largest
systems.kafka.producer.metadata.broker.list=localhost:9092
systems.kafka.producer.producer.type=sync
systems.kafka.producer.batch.num.messages=1</pre></div><p>This <a id="id467" class="indexterm"></a>may look like a lot, but for now we'll just consider the high-level <a id="id468" class="indexterm"></a>structure and some key settings. The job section sets YARN as the execution framework (as opposed to the local job runner class) and gives the job a name. If we were to run multiple copies of this same job, we would also give each copy a unique ID. The task section specifies the implementation class of our task and also the name of the streams for which it should receive messages. Serializers tell Samza how to read and write messages to and from the stream and the system section defines systems by name and associates implementation classes with them.</p><p>In our case, we define only one system called <code class="literal">kafka</code> and we refer to this system when sending our message in the preceding task. Note that this name is arbitrary and we could call it whatever we want. Obviously, for clarity it makes sense to call the Kafka system by the same name but this is only a convention. In particular, sometimes you will need to give different names when dealing with multiple systems that are similar to each other, or sometimes even when treating the same system differently in different parts of a configuration file.</p><p>In this section, we will also specify the SerDe to be associated with the streams used by the task. Recall that Kafka messages have a body and an optional key that is used to determine to <a id="id469" class="indexterm"></a>which partition the message is sent. Samza needs <a id="id470" class="indexterm"></a>to know how to treat the contents of the keys and messages for these streams. Samza has support to treat these as raw bytes or specific types such as string, integer, and JSON, as mentioned earlier.</p><p>The rest of the configuration will be mostly unchanged from job to job, as it includes things such as the location of the ZooKeeper ensemble and Kafka clusters, and specifies how streams are to be checkpointed. Samza allows a wide variety of customizations and the full <a id="id471" class="indexterm"></a>configuration options are detailed at <a class="ulink" href="http://samza.apache.org/learn/documentation/0.8/jobs/configuration-table.html" target="_blank">http://samza.apache.org/learn/documentation/0.8/jobs/configuration-table.html</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec94"></a>Getting Twitter data into Kafka</h3></div></div></div><p>Before <a id="id472" class="indexterm"></a>we run the job, we do need to get some tweets<a id="id473" class="indexterm"></a> into Kafka. Let's create a new Kafka topic called <span class="emphasis"><em>tweets</em></span> to which we'll write the tweets.</p><p>To perform this and other Kafka-related operations, we'll use command-line tools located within the <code class="literal">bin</code> directory of the Kafka distribution. If you are running a job from within the stack created as part of the Hello Samza application; this will be <code class="literal">deploy/kafka/bin</code>.</p><p>
<code class="literal">kafka-topics.sh</code> is a general-purpose tool that can be used to create, update, and describe topics. Most of its usages require arguments to specify the location of the local ZooKeeper cluster, where Kafka brokers store their details, and the name of the topic to be operated upon. To create a new topic, run the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ kafka-topics.sh  --zookeeper localhost:2181 --create â€“topic tweets --partitions 1 --replication-factor 1</strong></span>
</pre></div><p>This creates a topic called <span class="emphasis"><em>tweets</em></span> and explicitly sets its number of partitions and replication factor to 1. This is suitable if you are running Kafka within a local test VM, but clearly production deployments will have more partitions to scale out the load across multiple brokers and a replication factor of at least 2 to provide fault tolerance.</p><p>Use the list option of the <code class="literal">kafka-topics.sh</code> tool to simply show the topics in the system, or use <code class="literal">describe</code> to get more detailed information on specific topics:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ kafka-topics.sh  --zookeeper localhost:2181 --describe --topic tweets</strong></span>
<span class="strong"><strong>Topic:tweets    PartitionCount:1    ReplicationFactor:1    Configs:</strong></span>
<span class="strong"><strong>    Topic: tweets  Partition: 0    Leader: 0    Replicas: 0    Isr: 0</strong></span>
</pre></div><p>The <a id="id474" class="indexterm"></a>multiple 0s are possibly confusing as these are <a id="id475" class="indexterm"></a>labels and not counts. Each broker in the system has an ID that usually starts from 0, as do the partitions within each topic. The preceding output is telling us that the topic called <code class="literal">tweets</code> has a single partition with ID 0, the broker acting as the leader for that partition is broker 0, and the set of <span class="strong"><strong>in-sync replicas</strong></span> (<span class="strong"><strong>ISR</strong></span>) for this partition is again only broker 0. This last value is particularly<a id="id476" class="indexterm"></a> important when dealing with replication.</p><p>We'll use our Python utility from previous chapters to pull JSON tweets from the Twitter feed, and then use a Kafka CLI message producer to write the messages to a Kafka topic. This isn't a terribly efficient way of doing things, but it is suitable for illustration purposes. Assuming our Python script is in our home directory, run the following command from within the Kafka <code class="literal">bin</code> directory:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ python ~/stream.py â€“j | ./kafka-console-producer.sh  --broker-list localhost:9092 --topic tweets</strong></span>
</pre></div><p>This will run indefinitely so be careful not to leave it running overnight on a test VM with small disk space, not that the authors have ever done such a thing.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec95"></a>Running a Samza job</h3></div></div></div><p>To run a Samza job, we need our code to be packaged along with the Samza components required to <a id="id477" class="indexterm"></a>execute it into a <code class="literal">.tar.gz</code> archive that will be read by the YARN NodeManager. This is the file referred to by the <code class="literal">yarn.file.package</code> property in the Samza task configuration file.</p><p>When using the single node Hello Samza we can just use an absolute path on the filesystem, as seen in the previous configuration example. For jobs on larger YARN grids, the easiest way is to put the package onto HDFS and refer to it by an <code class="literal">hdfs://</code> URI or on a web server (Samza provides a mechanism to allow YARN to read the file via HTTP).</p><p>Because Samza has multiple subcomponents and each subcomponent has its own dependencies, the full YARN package can end up containing a lot of JAR files (over 100!). In addition, you need to include your custom code for the Samza task as well as some scripts from within the Samza distribution. It's not something to be done by hand. In the sample code for this chapter, found at <a class="ulink" href="https://github.com/learninghadoop2/book-examples/tree/master/ch4" target="_blank">https://github.com/learninghadoop2/book-examples/tree/master/ch4</a>, we have set up a sample structure to hold the code and config files and provided some automation via Gradle to build the necessary task archive and start the tasks.</p><p>When in the root of the Samza example code directory for this book, perform the following <a id="id478" class="indexterm"></a>command to build a single file archive containing all the classes of this chapter compiled together and bundled with all the other required files:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ./gradlew targz</strong></span>
</pre></div><p>This Gradle task will not only create the necessary <code class="literal">.tar.gz </code>archive in the <code class="literal">build/distributions</code> directory, but will also store an expanded version of the archive under <code class="literal">build/samza-package</code>. This will be useful, as we will use Samza scripts stored in the <code class="literal">bin</code> directory of the archive to actually submit the task to YARN.</p><p>So now, let's run our job. We need to have file paths for two things: the Samza <code class="literal">run-job.sh</code> script to submit a job to YARN and the configuration file for our job. Since our created job package has all the compiled tasks bundled together, it is by using a different configuration file that specifies a specific task implementation class in the <code class="literal">task.class</code> property that we tell Samza which task to run. To actually run the task, we can run the following command from within the exploded project archive under <code class="literal">build/samza-archives</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ bin/run-job.sh  --config-factory=org.apache.samza.config.factories.PropertiesConfigFactory --config-path=]config/twitter-parser.properties</strong></span>
</pre></div><p>For convenience, we added a Gradle task to run this job:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ./gradlew runTwitterParser</strong></span>
</pre></div><p>To see the output of the job, we'll use the Kafka CLI client to consume messages:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ./kafka-console-consumer.sh â€“zookeeper localhost:2181 â€“topic tweets-parsed</strong></span>
</pre></div><p>You should see a continuous stream of tweets appearing on the client.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note14"></a>Note</h3><p>Note that we did not explicitly create the topic called tweets-parsed. Kafka can allow topics to be created dynamically when either a producer or consumer tries to use the topic. In many situations, though the default partitioning and replication values may not be suitable, and explicit topic creation will be required to ensure these critical topic attributes are correctly defined.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec96"></a>Samza and HDFS</h3></div></div></div><p>You may have noticed that we just mentioned HDFS for the first time in our discussion of <a id="id479" class="indexterm"></a>Samza. Though Samza integrates tightly with YARN, it has<a id="id480" class="indexterm"></a> no direct integration with HDFS. At a logical level, Samza's stream-implementing systems (such as Kafka) are providing the storage layer that is usually provided by HDFS for traditional Hadoop workloads. In the terminology of Samza's architecture, as described earlier, YARN is the execution layer in both models, whereas Samza uses a streaming layer for its source and destination data, frameworks such as MapReduce use HDFS. This is a good example of how YARN enables alternative computational models that not only process data very differently than batch-oriented MapReduce, but that can also use entirely different storage systems for their source data.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec97"></a>Windowing functions</h3></div></div></div><p>It's frequently useful to generate some data based on the messages received on a stream over a<a id="id481" class="indexterm"></a> certain time window. An example of this may be<a id="id482" class="indexterm"></a> to record the top <span class="emphasis"><em>n</em></span> attribute values measured every minute. Samza supports this through the <code class="literal">WindowableTask</code> interface, which has the following single method to be implemented:</p><div class="informalexample"><pre class="programlisting">  public void window(MessageCollector collector, TaskCoordinator coordinator);</pre></div><p>This should look similar to the <code class="literal">process</code> method in the <code class="literal">StreamTask</code> interface. However, because the method is called on a time schedule, its invocation is not associated with a received message. The <code class="literal">MessageCollector</code> and <code class="literal">TaskCoordinator</code> parameters are still there, however, as most windowable tasks will produce output messages and may also wish to perform some task management actions.</p><p>Let's take our previous task and add a window function that will output the number of tweets received in each windowed time period. This is the main class implementation of <code class="literal">TwitterStatisticsStreamTask.java </code>found at <a class="ulink" href="https://github.com/learninghadoop2/book-examples/blob/master/ch4/src/main/java/com/learninghadoop2/samza/tasks/TwitterStatisticsStreamTask.java" target="_blank">https://github.com/learninghadoop2/book-examples/blob/master/ch4/src/main/java/com/learninghadoop2/samza/tasks/TwitterStatisticsStreamTask.java</a>:</p><div class="informalexample"><pre class="programlisting">public class TwitterStatisticsStreamTask implements StreamTask, WindowableTask {
    private int tweets = 0;

    @Override
    public void process(IncomingMessageEnvelope envelope, MessageCollector collector, TaskCoordinator coordinator) {
        tweets++;
    }

    @Override
    public void window(MessageCollector collector, TaskCoordinator coordinator) {
        collector.send(new OutgoingMessageEnvelope(new SystemStream("kafka", "tweet-stats"), "" + tweets));

        // Reset counts after windowing.
        tweets = 0;
    }
}</pre></div><p>The <code class="literal">TwitterStatisticsStreamTask</code> class has a private member variable called <code class="literal">tweets</code> that is initialized to <code class="literal">0</code> and is incremented in every call to the <code class="literal">process</code> method. We<a id="id483" class="indexterm"></a> therefore know that this variable will be incremented <a id="id484" class="indexterm"></a>for each message passed to the task from the underlying stream implementation. Each Samza container has a single thread running in a loop that executes the process and window methods on all the tasks within the container. This means that we do not need to guard instance variables against concurrent modifications; only one method on each task within a container will be executing simultaneously.</p><p>In our <code class="literal">window</code> method, we send a message to a new topic we call <code class="literal">tweet-stats</code> and then reset the <code class="literal">tweets</code> variable. This is pretty straightforward and the only missing piece is how Samza will know when to call the <code class="literal">window</code> method. We specify this in the configuration file: </p><div class="informalexample"><pre class="programlisting">task.window.ms=5000</pre></div><p>This tells Samza to call the <code class="literal">window</code> method on each task instance every 5 seconds. To run the <code class="literal">window</code> task, there is a Gradle task:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ./gradlew runTwitterStatistics</strong></span>
</pre></div><p>If we use <code class="literal">kafka-console-consumer.sh</code> to listen on the <code class="literal">tweet-stats</code> stream now, we will see the following output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Number of tweets: 5012</strong></span>
<span class="strong"><strong>Number of tweets: 5398</strong></span>
</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note15"></a>Note</h3><p>Note that the term <span class="emphasis"><em>window</em></span> in this context refers to Samza conceptually slicing the stream of messages into time ranges and providing a mechanism to perform processing<a id="id485" class="indexterm"></a> at each range boundary. Samza <a id="id486" class="indexterm"></a>does not directly provide an implementation of the other use of the term with regards to sliding windows, where a series of values is held and processed over time. However, the windowable task interface does provide the plumbing to implement such sliding windows.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec98"></a>Multijob workflows</h3></div></div></div><p>As we saw with the Hello Samza examples, some of the real power of Samza comes from composition <a id="id487" class="indexterm"></a>of multiple jobs and we'll use a text cleanup job to start demonstrating this capability. </p><p>In the following section, we'll perform tweet sentiment analysis by comparing tweets with a set of English positive and negative words. Simply applying this to the raw Twitter feed will have very patchy results, however, given how richly multilingual the Twitter stream is. We also need to consider things such as text cleanup, capitalization, frequent contractions, and so on. As anyone who has worked with any non-trivial dataset knows, the act of making the data fit for processing is usually where a large amount of effort (often the majority!) goes.</p><p>So before we try and detect tweet sentiments, let's do some simple text cleanup; in particular, we'll select only English language tweets and we will force their text to be lower case before sending them to a new output stream.</p><p>Language detection <a id="id488" class="indexterm"></a>is a difficult problem and for this we'll use a feature<a id="id489" class="indexterm"></a> of the Apache Tika library (<a class="ulink" href="http://tika.apache.org" target="_blank">http://tika.apache.org</a>). Tika provides a wide array of functionality to extract text from various sources and then to extract further information from that text. If using our Gradle scripts, the Tika dependency is already specified and will automatically be included in the generated job package. If building through another mechanism, you will need to download the Tika JAR file from the home page and add it to your YARN job package. The following code can be found as <code class="literal">TextCleanupStreamTask.java</code> at <a class="ulink" href="https://github.com/learninghadoop2/book-examples/blob/master/ch4/src/main/java/com/learninghadoop2/samza/tasks/TextCleanupStreamTask.java" target="_blank">https://github.com/learninghadoop2/book-examples/blob/master/ch4/src/main/java/com/learninghadoop2/samza/tasks/TextCleanupStreamTask.java</a>:</p><div class="informalexample"><pre class="programlisting">public class TextCleanupStreamTask implements StreamTask {
    @Override
    public void process(IncomingMessageEnvelope envelope, MessageCollector collector, TaskCoordinator coordinator) {
        String rawtext = ((String) envelope.getMessage());

        if ("en".equals(detectLanguage(rawtext))) {
            collector.send(new OutgoingMessageEnvelope(new SystemStream("kafka", "english-tweets"),
                    rawtext.toLowerCase()));
        }
    }

    private String detectLanguage(String text) {
        LanguageIdentifier li = new LanguageIdentifier(text);

        return li.getLanguage();
    }
}</pre></div><p>This task <a id="id490" class="indexterm"></a>is quite straightforward thanks to the heavy lifting performed by Tika. We create a utility method that wraps the creation and use of a Tika, <code class="literal">LanguageDetector</code>, and then we call this method on the message body of each incoming message in the <code class="literal">process</code> method. We only write to the output stream if the result of applying this utility method is <code class="literal">"en"</code>, that is, the two-letter code for English.</p><p>The configuration file for this task is similar to that of our previous task, with the specific values for the task name and implementing class. It is in the repository as <code class="literal">textcleanup.properties</code> at <a class="ulink" href="https://github.com/learninghadoop2/book-examples/blob/master/ch4/src/main/resources/textcleanup.properties" target="_blank">https://github.com/learninghadoop2/book-examples/blob/master/ch4/src/main/resources/textcleanup.properties</a>. We also need to specify the input stream:</p><div class="informalexample"><pre class="programlisting">task.inputs=kafka.tweets-parsed</pre></div><p>This is important because we need this task to parse the tweet text that was extracted in the earlier task and avoid duplicating the JSON parsing logic that is best encapsulated in one place. We can run this task with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ./gradlew runTextCleanup</strong></span>
</pre></div><p>Now, we can run all three tasks together; <code class="literal">TwitterParseStreamTask</code> and <code class="literal">TwitterStatisticsStreamTask</code> will consume the raw tweet stream, while <code class="literal">TextCleanupStreamTask</code> will consume the output from <code class="literal">TwitterParseStreamTask</code>.</p><div class="mediaobject"><img src="graphics/5518OS_04_02.jpg" /><div class="caption"><p>Data processing on streams</p></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec99"></a>Tweet sentiment analysis</h3></div></div></div><p>We'll now<a id="id491" class="indexterm"></a> implement a task to perform<a id="id492" class="indexterm"></a> tweet sentiment analysis similar to what we did using MapReduce in the previous chapter. This will also show us a useful mechanism offered by Samza: bootstrap streams.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl3sec28"></a>Bootstrap streams</h4></div></div></div><p>Generally speaking, most stream-processing jobs (in Samza or another framework) will start processing <a id="id493" class="indexterm"></a>messages that arrive after they start up and generally ignore historical messages. Because of its concept of replayable streams, Samza doesn't have this limitation.</p><p>In our sentiment analysis job, we had two sets of reference terms: positive and negative words. Though we've not shown it so far, Samza can consume messages from multiple streams and the underlying machinery will poll all named streams and provide their messages, one at a time, to the <code class="literal">process</code> method. We can therefore create streams for the positive and negative words and push the datasets onto those streams. At first glance, we could plan to rewind these two streams to the earliest point and read tweets as they arrive. The problem is that Samza won't guarantee ordering of messages from multiple streams, and even though there is a mechanism to give streams higher priority, we can't assume that all negative and positive words will be processed before the first tweet arrives.</p><p>For such types of scenarios, Samza has the concept of bootstrap streams. If a task has any bootstrap streams defined, then it will read these streams from the earliest offset until they are fully processed (technically, it will read the streams till they get caught up, so that any new words sent to either stream will be treated without priority and will arrive interleaved between tweets).</p><p>We'll now<a id="id494" class="indexterm"></a> create a new job called <code class="literal">TweetSentimentStreamTask</code> that reads two bootstrap streams, collects their contents into HashMaps, gathers running counts for sentiment trends, and uses a <code class="literal">window</code> function to output this data at intervals. This code can be found at <a class="ulink" href="https://github.com/learninghadoop2/book-examples/blob/master/ch4/src/main/java/com/learninghadoop2/samza/tasks/TwitterSentimentStreamTask.java" target="_blank">https://github.com/learninghadoop2/book-examples/blob/master/ch4/src/main/java/com/learninghadoop2/samza/tasks/TwitterSentimentStreamTask.java</a>:</p><div class="informalexample"><pre class="programlisting">public class TwitterSentimentStreamTask implements StreamTask, WindowableTask {
    private Set&lt;String&gt;          positiveWords  = new HashSet&lt;String&gt;();
    private Set&lt;String&gt;          negativeWords  = new HashSet&lt;String&gt;();
    private int                  tweets         = 0;
    private int                  positiveTweets = 0;
    private int                  negativeTweets = 0;
    private int                  maxPositive    = 0;
    private int                  maxNegative    = 0;

    @Override
    public void process(IncomingMessageEnvelope envelope, MessageCollector collector, TaskCoordinator coordinator) {
        if ("positive-words".equals(envelope.getSystemStreamPartition().getStream())) {
            positiveWords.add(((String) envelope.getMessage()));
        } else if ("negative-words".equals(envelope.getSystemStreamPartition().getStream())) {
            negativeWords.add(((String) envelope.getMessage()));
        } else if ("english-tweets".equals(envelope.getSystemStreamPartition().getStream())) {
            tweets++;

            int    positive = 0;
            int    negative = 0;
            String words    = ((String) envelope.getMessage());

            for (String word : words.split(" ")) {
                if (positiveWords.contains(word)) {
                    positive++;
                } else if (negativeWords.contains(word)) {
                    negative++;
                }
            }

            if (positive &gt; negative) {
                positiveTweets++;
            }

            if (negative &gt; positive) {
                negativeTweets++;
            }

            if (positive &gt; maxPositive) {
                maxPositive = positive;
            }

            if (negative &gt; maxNegative) {
                maxNegative = negative;
            }
        }
    }

    @Override
    public void window(MessageCollector collector, TaskCoordinator coordinator) {
        String msg = String.format("Tweets: %d Positive: %d Negative: %d MaxPositive: %d MinPositive: %d", tweets, positiveTweets, negativeTweets, maxPositive, maxNegative);


        collector.send(new OutgoingMessageEnvelope(new SystemStream("kafka", "tweet-sentiment-stats"), msg));

        // Reset counts after windowing.
        tweets         = 0;
        positiveTweets = 0;
        negativeTweets = 0;
        maxPositive    = 0;
        maxNegative    = 0;
    }

}</pre></div><p>In this task, we add a number of private member variables that we will use to keep a running count of the number of overall tweets, how many were positive and negative, and the maximum positive and negative counts seen in a single tweet.</p><p>This<a id="id495" class="indexterm"></a> task consumes from three Kafka topics. Even though we will configure two to be used as bootstrap streams, they are all still exactly the same type of Kafka topic from which messages are received; the only difference with bootstrap streams is that we tell Samza to use Kafka's rewinding capabilities to fully re-read each message in the stream. For the other stream of tweets, we just start reading new messages as they arrive.</p><p>As hinted earlier, if a task subscribes to multiple streams, the same <code class="literal">process</code> method will receive messages from each stream. That is why we use <code class="literal">envelope.getSystemStreamPartition().getStream()</code> to extract the stream name for each given message and then act accordingly. If the message is from either of the bootstrapped streams, we add its contents to the appropriate hashmap. We break a tweet message into its constituent words, test each word for positive or negative sentiment, and then update counts accordingly. As you can see, this task doesn't output the received tweets to another topic. </p><p>Since we don't perform any direct processing, there is no point in doing so; any other task that wishes to consume messages can just subscribe directly to the incoming tweets stream. However, a possible modification could be to write positive and negative sentiment tweets to dedicated streams for each.</p><p>The <code class="literal">window</code> method outputs a series of counts and then resets the variables (as it did before). Note that Samza does have support to directly expose metrics through JMX, which could possibly be a better fit for such simple windowing examples. However, we won't have space to cover that aspect of the project in this book.</p><p>To run this job, we need to modify the configuration file by setting the job and task names as usual, but we also need to specify multiple input streams now:</p><div class="informalexample"><pre class="programlisting">task.inputs=kafka.english-tweets,kafka.positive-words,kafka.negative-words</pre></div><p>Then, we need to specify that two of our streams are bootstrap streams that should be read from the earliest offset. Specifically, we set three properties for the streams. We say they are to be bootstrapped, that is, fully read before other streams, and this is achieved by specifying that the offset on each stream needs to be reset to the oldest (first) position:</p><div class="informalexample"><pre class="programlisting">systems.kafka.streams.positive-words.samza.bootstrap=true
systems.kafka.streams.positive-words.samza.reset.offset=true
systems.kafka.streams.positive-words.samza.offset.default=oldest

systems.kafka.streams.negative-words.samza.bootstrap=true
systems.kafka.streams.negative-words.samza.reset.offset=true
systems.kafka.streams.negative-words.samza.offset.default=oldest</pre></div><p>We <a id="id496" class="indexterm"></a>can run this job with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ./gradlew runTwitterSentiment</strong></span>
</pre></div><p>After starting the job, look at the output of the messages on the <code class="literal">tweet-sentiment-stats</code> topic.</p><p>The sentiment detection job will bootstrap the positive and negative word streams before reading any of our newly detected lower-case English tweets.</p><p>With the sentiment detection job, we can now visualize our four collaborating jobs as shown in the following diagram:</p><div class="mediaobject"><img src="graphics/5518OS_04_03.jpg" /><div class="caption"><p>Bootstrap streams and collaborating tasks</p></div></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip04"></a>Tip</h3><p>To<a id="id497" class="indexterm"></a> correctly run the jobs, it may seem necessary to start the JSON parser job followed by the cleanup job before finally starting the sentiment job, but this is not the case. Any unread messages remain buffered in Kafka, so it doesn't matter in which order the jobs of a multi-job workflow are started. Of course, the sentiment job will output counts of 0 tweets until it starts receiving data, but nothing will break if a stream job starts before those it depends on.</p></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec100"></a>Stateful tasks</h3></div></div></div><p>The final <a id="id498" class="indexterm"></a>aspect of Samza that we will explore is how it allows the <a id="id499" class="indexterm"></a>tasks processing stream partitions to have persistent local state. In the previous example, we used private variables to keep a track of running totals, but sometimes it is useful for a task to have richer local state. An example could be the act of performing a logical join on two streams, where it is useful to build up a state model from one stream and compare this with the other.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note16"></a>Note</h3><p>Note that Samza can utilize its concept of partitioned streams to greatly optimize the act of joining streams. If each stream to be joined uses the same partition key (for example, a user ID), then each task consuming these streams will receive all messages associated with each ID across all the streams.</p></div><p>Samza has another abstraction similar to its notion of the framework to manage its jobs and that which implements its tasks. It defines an abstract key-value store that can have multiple concrete implementations. Samza uses existing open source projects for the on-disk implementations and used LevelDB as of v0.7 and added RocksDB as of v0.8. There is also an in-memory store that does not persist the key-value data but that may be useful in testing or potentially very specific production workloads.</p><p>Each task can write to this key-value store and Samza manages its persistence to the local implementation. To support persistent states, the store is also modeled as a stream and all writes to the store are also pushed into a stream. If a task fails, then on restart, it can recover the state of its local key-value store by replaying the messages in the backing topic. An obvious concern here will be the number of messages that need to be replayed; however, when using Kafka, for example, it compacts messages with the same key so that only the latest update remains in the topic. </p><p>We'll modify our previous tweet sentiment example to add a lifetime count of the maximum positive and negative sentiment seen in any tweet. The following code can be found as <code class="literal">TwitterStatefulSentimentStateTask.java</code> at <a class="ulink" href="https://github.com/learninghadoop2/book-examples/blob/master/ch4/src/main/java/com/learninghadoop2/samza/tasks/TwitterStatefulSentimentStreamTask.java" target="_blank">https://github.com/learninghadoop2/book-examples/blob/master/ch4/src/main/java/com/learninghadoop2/samza/tasks/TwitterStatefulSentimentStreamTask.java</a>. Note that the process method is the same as <code class="literal">TwitterSentimentStateTask.java</code>, so we have omitted it here for space reasons:</p><div class="informalexample"><pre class="programlisting">public class TwitterStatefulSentimentStreamTask implements StreamTask, WindowableTask, InitableTask {
    private Set&lt;String&gt; positiveWords  = new HashSet&lt;String&gt;();
    private Set&lt;String&gt; negativeWords  = new HashSet&lt;String&gt;();
    private int tweets = 0;
    private int positiveTweets = 0;
    private int negativeTweets = 0;
    private int maxPositive = 0;
    private int maxNegative = 0;
    private KeyValueStore&lt;String, Integer&gt; store;

    @SuppressWarnings("unchecked")
    @Override
    public void init(Config config, TaskContext context) {
        this.store = (KeyValueStore&lt;String, Integer&gt;) context.getStore("tweet-store");
    }

    @Override
    public void process(IncomingMessageEnvelope envelope, MessageCollector collector, TaskCoordinator coordinator) {
...
    }

    @Override
    public void window(MessageCollector collector, TaskCoordinator coordinator) {
        Integer lifetimeMaxPositive = store.get("lifetimeMaxPositive");
        Integer lifetimeMaxNegative = store.get("lifetimeMaxNegative");

        if ((lifetimeMaxPositive == null) || (maxPositive &gt; lifetimeMaxPositive)) {
            lifetimeMaxPositive = maxPositive;
            store.put("lifetimeMaxPositive", lifetimeMaxPositive);
        }

        if ((lifetimeMaxNegative == null) || (maxNegative &gt; lifetimeMaxNegative)) {
            lifetimeMaxNegative = maxNegative;
            store.put("lifetimeMaxNegative", lifetimeMaxNegative);
        }

        String msg =
            String.format(
                "Tweets: %d Positive: %d Negative: %d MaxPositive: %d MaxNegative: %d LifetimeMaxPositive: %d LifetimeMaxNegative: %d",
                tweets, positiveTweets, negativeTweets, maxPositive, maxNegative, lifetimeMaxPositive,
                lifetimeMaxNegative);

        collector.send(new OutgoingMessageEnvelope(new SystemStream("kafka", "tweet-stateful-sentiment-stats"), msg));

        // Reset counts after windowing.
        tweets         = 0;
        positiveTweets = 0;
        negativeTweets = 0;
        maxPositive    = 0;
        maxNegative    = 0;
    }
}</pre></div><p>This <a id="id500" class="indexterm"></a>class implements a new interface called <code class="literal">InitableTask</code>. This has a single method called <code class="literal">init</code> and is used when a task needs to configure<a id="id501" class="indexterm"></a> aspects of its configuration before it begins execution. We use the <code class="literal">init()</code> method here to create an instance of the <code class="literal">KeyValueStore</code> class and store it in a private member variable. </p><p>
<code class="literal">KeyValueStore</code>, as the name suggests, provides a familiar <code class="literal">put</code>/<code class="literal">get</code> type interface. In this case, we specify that the keys are of the type String and the values are Integers. In our <code class="literal">window</code> method, we retrieve any previously stored values for the maximum positive and negative sentiment and if the count in the current window is higher, update the store accordingly. Then, we just output the results of the <code class="literal">window</code> method as before.</p><p>As you can see, the user does not need to deal with the details of either the local or remote persistence of the <code class="literal">KeyValueStore</code> instance; this is all handled by Samza. The efficiency of the mechanism also makes it tractable for tasks to hold sizeable amount of local state, which <a id="id502" class="indexterm"></a>can be particularly valuable in cases such as<a id="id503" class="indexterm"></a> long-running aggregations or stream joins.</p><p>The configuration file for the job can be found at <a class="ulink" href="https://github.com/learninghadoop2/book-examples/blob/master/ch4/src/main/resources/twitter-stateful-sentiment.properties" target="_blank">https://github.com/learninghadoop2/book-examples/blob/master/ch4/src/main/resources/twitter-stateful-sentiment.properties</a>. It needs to have a few entries added, which are as follows:</p><div class="informalexample"><pre class="programlisting">stores.tweet-store.factory=org.apache.samza.storage.kv.KeyValueStorageEngineFactory
stores.tweet-store.changelog=kafka.twitter-stats-state
stores.tweet-store.key.serde=string
stores.tweet-store.msg.serde=integer</pre></div><p>The first line specifies the implementation class for the store, the second line specifies the Kafka topic to be used for persistent state, and the last two lines specify the type of the store key and value.</p><p>To run this job, use the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ./gradlew runTwitterStatefulSentiment</strong></span>
</pre></div><p>For convenience, the following command will start up four jobs: the JSON parser, the text cleanup, the statistics job and the stateful sentiment jobs:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ./gradlew runTasks</strong></span>
</pre></div><p>Samza is a pure stream-processing system that provides pluggable implementations of its storage and execution layers. The most commonly used plugins are YARN and Kafka, and these demonstrate how Samza can integrate tightly with Hadoop YARN while using a completely different storage layer. Samza is still a relatively new project and the current features are only a subset of what is envisaged. It is recommended to consult its webpage to get the latest information on its current status.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec37"></a>Summary</h2></div></div><hr /></div><p>This chapter focused much more on what can be done on Hadoop 2, and in particular YARN, than the details of Hadoop internals. This is almost certainly a good thing, as it demonstrates that Hadoop is realizing its goal of becoming a much more flexible and generic data processing platform that is no longer tied to batch processing. In particular, we highlighted how Samza shows that the processing frameworks that can be implemented on YARN can innovate and enable functionality vastly different from that available in Hadoop 1.</p><p>In particular, we saw how Samza goes to the opposite end of the latency spectrum from batch processing and enables per-message processing of individual messages as they arrive.</p><p>We also saw how Samza provides a callback mechanism that MapReduce developers will be familiar with, but uses it for a very different processing model. We also discussed the ways in which Samza utilizes YARN as its main execution framework and how it implements the model described in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Processing â€“ MapReduce and Beyond</em></span>.</p><p>In the next chapter, we will switch gears and explore Apache Spark. Though it has a very different data model than Samza, we'll see that it does also have an extension that supports processing of real time data streams, including the option of Kafka integration. However, both projects are so different that they are complimentary more than in competition.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch05"></a>ChapterÂ 5.Â Iterative Computation with Spark</h2></div></div></div><p>In the previous chapter, we saw how Samza can enable near real-time stream data processing within Hadoop. This is quite a step away from the traditional batch processing model of MapReduce, but still keeps with the model of providing a well-defined interface against which business logic tasks can be implemented. In this chapter we will explore Apache Spark, which can be viewed both as a framework on which applications can be built as well as a processing framework in its own right. Not only are applications being built on Spark, but entire components within the Hadoop ecosystem are also being reimplemented to use Spark as their underlying processing framework. In particular, we will cover the following topics:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>What Spark is and how its core system can run on YARN</p></li><li style="list-style-type: disc"><p>The data model provided by Spark that enables hugely scalable and highly efficient data processing</p></li><li style="list-style-type: disc"><p>The breadth of additional Spark components and related projects</p></li></ul></div><p>It's important to note upfront that although Spark has its own mechanism to process streaming data, this is but one part of what Spark has to offer. It's best to think of it as a much broader initiative.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec38"></a>Apache Spark</h2></div></div><hr /></div><p>Apache <a id="id504" class="indexterm"></a>Spark (<a class="ulink" href="https://spark.apache.org/" target="_blank">https://spark.apache.org/</a>) is a data processing <a id="id505" class="indexterm"></a>framework based on a generalization of MapReduce. It <a id="id506" class="indexterm"></a>was originally developed by the AMPLab at UC Berkeley (<a class="ulink" href="https://amplab.cs.berkeley.edu/" target="_blank">https://amplab.cs.berkeley.edu/</a>). Like Tez, Spark acts as an execution engine that models data transformations as DAGs and strives to eliminate the I/O overhead of MapReduce in order to perform iterative computation at scale. While Tez's main goal was to provide a faster execution engine for MapReduce on Hadoop, Spark has been designed both as a standalone framework and an API for application development. The system is designed to perform general-purpose in-memory data processing, stream workflows, as well as interactive and iterative computation.</p><p>Spark is implemented in Scala, which is a statically typed programming language for the Java VM and exposes native programming interfaces for Java and Python in addition to Scala itself. Note that though Java code can call the Scala interface directly, there are some aspects of the type system that make such code pretty unwieldy, and hence we use the native Java API.</p><p>Scala ships with an interactive shell similar to that of Ruby and Python; this allows users to run Spark interactively from the interpreter to query any dataset.</p><p>The Scala interpreter <a id="id507" class="indexterm"></a>operates by compiling a class for each line typed by the user, loading it into the JVM, and invoking a function on it. This class includes a singleton object that contains the variables or functions on that line and runs the line's code in an initialize method. In addition to its rich programming interfaces, Spark is becoming established as an execution engine, with popular tools of the Hadoop ecosystem (such as Pig and Hive) being ported to the framework.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec101"></a>Cluster computing with working sets</h3></div></div></div><p>Spark's <a id="id508" class="indexterm"></a>architecture is centered around the concept of <span class="strong"><strong>Resilient Distributed Datasets</strong></span> (<span class="strong"><strong>RDDs</strong></span>), which is a read-only collection of Scala objects partitioned across a set of machines that can persist in memory. This abstraction was proposed <a id="id509" class="indexterm"></a>in a 2012 research paper, <span class="emphasis"><em>Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing</em></span>, which can be found at <a class="ulink" href="https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf" target="_blank">https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf</a>.</p><p>A Spark <a id="id510" class="indexterm"></a>application consists of a driver program that executes parallel operations on a cluster of workers and long-lived processes that can store data partitions in memory by dispatching functions that run as parallel tasks, as shown in the following diagram:</p><div class="mediaobject"><img src="graphics/5518OS_05_01.jpg" /><div class="caption"><p>Spark cluster architecture</p></div></div><p>Processes are coordinated via a SparkContext instance. SparkContext connects to a resource manager (such as YARN), requests executors on worker nodes, and sends tasks to be executed. Executors are responsible for running tasks and managing memory locally.</p><p>Spark <a id="id511" class="indexterm"></a>allows you to share variables between tasks, or between tasks and the driver, using an abstraction known as shared <a id="id512" class="indexterm"></a>variables. Spark supports two types of shared variables: broadcast variables, which can be used to cache a value in memory on all nodes, and accumulators, which are additive variables such as counters and sums.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl3sec29"></a>Resilient Distributed Datasets (RDDs)</h4></div></div></div><p>An RDD <a id="id513" class="indexterm"></a>is stored in memory, shared across machines and is used in MapReduce-like parallel operations. Fault <a id="id514" class="indexterm"></a>tolerance is achieved through the notion of <span class="emphasis"><em>lineage</em></span>: if a partition of an RDD is lost, the RDD has enough information about how it was derived from other RDDs to be able to rebuild just that partition. An RDD can be built in four ways:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>By reading data from a file stored in HDFS</p></li><li style="list-style-type: disc"><p>By dividing â€“ parallelizing â€“ a Scala collection into a number of partitions that are sent to workers</p></li><li style="list-style-type: disc"><p>By transforming an existing RDD using parallel operators</p></li><li style="list-style-type: disc"><p>By changing the persistence of an existing RDD</p></li></ul></div><p>Spark shines when RDDs can fit in memory and can be cached across operations. The API exposes <a id="id515" class="indexterm"></a>methods to persist RDDs <a id="id516" class="indexterm"></a>and allows for several persistence strategies and storage levels, allowing for spill to disk as well as space-efficient binary serialization.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl3sec30"></a>Actions</h4></div></div></div><p>Operations <a id="id517" class="indexterm"></a>are invoked by passing functions to Spark. The system deals with variables and side effects according to the functional programming paradigm. Closures can refer to variables in the scope where they are created. Examples of actions are <code class="literal">count</code> (returns the number of elements in the dataset), and <code class="literal">save</code> (outputs the dataset to storage). Other parallel operations on RDDs include the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">map</code>: applies <a id="id518" class="indexterm"></a>a function to each element of the dataset</p></li><li style="list-style-type: disc"><p>
<code class="literal">filter</code>: <a id="id519" class="indexterm"></a>selects elements from a dataset based on user-provided criteria</p></li><li style="list-style-type: disc"><p>
<code class="literal">reduce</code>: <a id="id520" class="indexterm"></a>combines dataset elements using an associative function</p></li><li style="list-style-type: disc"><p>
<code class="literal">collect</code>: <a id="id521" class="indexterm"></a>sends all elements of the dataset to the driver program</p></li><li style="list-style-type: disc"><p>
<code class="literal">foreach</code>: passes <a id="id522" class="indexterm"></a>each element through a user-provided function</p></li><li style="list-style-type: disc"><p>
<code class="literal">groupByKey</code>: <a id="id523" class="indexterm"></a>groups items together by a provided key</p></li><li style="list-style-type: disc"><p>
<code class="literal">sortByKey</code>: <a id="id524" class="indexterm"></a>sorts items by key</p></li></ul></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec102"></a>Deployment</h3></div></div></div><p>Spark can run <a id="id525" class="indexterm"></a>both in local mode, similar to a Hadoop single-node setup, or atop a resource manager. Currently supported resource managers include:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Spark Standalone Cluster Mode</p></li><li style="list-style-type: disc"><p>YARN</p></li><li style="list-style-type: disc"><p>Apache Mesos</p></li></ul></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl3sec31"></a>Spark on YARN</h4></div></div></div><p>An <a id="id526" class="indexterm"></a>ad-hoc-consolidated JAR needs to be built in order to deploy Spark on <a id="id527" class="indexterm"></a>YARN. Spark launches an instance of the standalone deployed cluster within the ResourceManager. Cloudera and MapR both ship with Spark on YARN as part of their software distribution. At the time of writing, Spark is <a id="id528" class="indexterm"></a>available for Hortonworks's HDP as a technology preview (<a class="ulink" href="http://hortonworks.com/hadoop/spark/" target="_blank">http://hortonworks.com/hadoop/spark/</a>).</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl3sec32"></a>Spark on EC2</h4></div></div></div><p>Spark comes with a <a id="id529" class="indexterm"></a>deployment script, <code class="literal">spark-ec2</code>, located in the <code class="literal">ec2</code> directory. This script automatically sets up Spark and HDFS on a cluster of <a id="id530" class="indexterm"></a>EC2 instances. In order to launch a Spark cluster on the Amazon cloud, go to the <code class="literal">ec2</code> directory and run the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>./spark-ec2 -k &lt;keypair&gt; -i &lt;key-file&gt; -s &lt;num-slaves&gt; launch &lt;cluster-name&gt;</strong></span>
</pre></div><p>Here, <code class="literal">&lt;keypair&gt;</code> is the name of your EC2 key pair, <code class="literal">&lt;key-file&gt;</code> is the private key file for the key pair, <code class="literal">&lt;num-slaves&gt;</code> is the number of slave nodes to be launched, and <code class="literal">&lt;cluster-name&gt;</code> is the name to be given to your cluster. See <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Introduction</em></span>, for more details regarding the setup of key pairs, and verify that the cluster scheduler is up and sees all the slaves by going to its web UI, the address of which will be printed once the script completes.</p><p>You can specify a path in S3 as the input through a URI of the form <code class="literal">s3n://&lt;bucket&gt;/path</code>. You will also need to set your Amazon security credentials, either by setting the environment variables <code class="literal">AWS_ACCESS_KEY_ID</code> and <code class="literal">AWS_SECRET_ACCESS_KEY</code> before your program is executed, or through <code class="literal">SparkContext.hadoopConfiguration</code>.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec103"></a>Getting started with Spark</h3></div></div></div><p>Spark binaries and <a id="id531" class="indexterm"></a>source code are available on the project website at <a class="ulink" href="http://spark.apache.org/" target="_blank">http://spark.apache.org/</a>. The examples in the following section have been tested using <a id="id532" class="indexterm"></a>Spark 1.1.0 built from source on the Cloudera CDH 5.0 QuickStart VM.</p><p>Download and uncompress the <code class="literal">gzip</code> archive with the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ wget http://d3kbcqa49mib13.cloudfront.net/spark-1.1.0.tgz </strong></span>
<span class="strong"><strong>$ tar xvzf spark-1.1.0.tgz</strong></span>
<span class="strong"><strong>$ cd spark-1.1.0</strong></span>
</pre></div><p>Spark is <a id="id533" class="indexterm"></a>built on Scala 2.10 and uses <code class="literal">sbt</code> (<a class="ulink" href="https://github.com/sbt/sbt" target="_blank">https://github.com/sbt/sbt</a>) to build the source core and related examples:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ./sbt/sbt -Dhadoop.version=2.2.0  -Pyarn  assembly</strong></span>
</pre></div><p>With the <code class="literal">-Dhadoop.version=2.2.0</code> and <code class="literal">-Pyarn</code> options, we instruct <code class="literal">sbt</code> to build against Hadoop versions 2.2.0 or higher and enable YARN support.</p><p>Start Spark in standalone mode with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ./sbin/start-all.sh </strong></span>
</pre></div><p>This command will launch a local master instance at <code class="literal">spark://localhost:7077</code> as well as a worker node.</p><p>A web interface to the <a id="id534" class="indexterm"></a>master node can be accessed at <code class="literal">http://localhost:8080/</code> and can be seen in the following screenshot:</p><div class="mediaobject"><img src="graphics/5518OS_05_02.jpg" /><div class="caption"><p>Master node web interface</p></div></div><p>Spark can run interactively through <code class="literal">spark-shell</code>, which is a modified version of the Scala shell. As a first example, we will implement a word count of the Twitter dataset we used in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Processing - MapReduce and Beyond</em></span>, using the Scala API.</p><p>Start an interactive <code class="literal">spark-shell</code> session by running the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ./bin/spark-shell</strong></span>
</pre></div><p>The shell instantiates a <code class="literal">SparkContext</code> object, <code class="literal">sc</code>, that is responsible for handling driver connections to workers. We will describe its semantics later in this chapter.</p><p>To make things a bit easier, let's create a sample textual dataset that contains one status update per line:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ stream.py -t -n 1000 &gt; sample.txt</strong></span>
</pre></div><p>Then, copy it to HDFS:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hdfs dfs -put sample.txt /tmp</strong></span>
</pre></div><p>Within <code class="literal">spark-shell</code>, we first create an <code class="literal">RDD - file</code> - from the sample data:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>val file = sc.textFile("/tmp/sample.txt")</strong></span>
</pre></div><p>Then, we apply a series of transformations to count the word occurrences in the file. Note that the output of the transformation chain - <code class="literal">counts</code> - is still an RDD:</p><div class="informalexample"><pre class="programlisting">val counts = file.flatMap(line =&gt; line.split(" "))
.map(word =&gt; (word, 1))
.reduceByKey((m, n) =&gt; m + n)  </pre></div><p>This chain of transformations corresponds to the map and reduce phases that we are familiar with. In the map phase, we load each line of the dataset (<code class="literal">flatMap</code>), tokenize each tweet into a sequence of words, count the occurrence of each word (<code class="literal">map</code>), and emit (<code class="literal">key, value</code>) pairs. In the reduce phase, we group by key (<code class="literal">word</code>) and sum values (<code class="literal">m, n</code>) together to obtain word counts.</p><p>Finally, we print the first <a id="id535" class="indexterm"></a>ten elements, <code class="literal">counts.take(10)</code>, to the console:</p><div class="informalexample"><pre class="programlisting">counts.take(10).foreach(println)</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec104"></a>Writing and running standalone applications</h3></div></div></div><p>Spark allows <a id="id536" class="indexterm"></a>standalone applications <a id="id537" class="indexterm"></a>to be written using three <a id="id538" class="indexterm"></a>APIs: Scala, Java, and Python.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl3sec33"></a>Scala API</h4></div></div></div><p>The first thing a <a id="id539" class="indexterm"></a>Spark driver must do is to create a <code class="literal">SparkContext</code> object, which tells Spark how to access a cluster. After importing classes and implicit <a id="id540" class="indexterm"></a>conversions into a program, as in the following:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark.SparkContext 
import org.apache.spark.SparkContext._</pre></div><p>The <code class="literal">SparkContext</code> <a id="id541" class="indexterm"></a>object can be created with the following constructor:</p><div class="informalexample"><pre class="programlisting">new SparkContext(master, appName, [sparkHome]) </pre></div><p>It can also be created through <code class="literal">SparkContext(conf)</code>, which takes a <code class="literal">SparkConf</code> object.</p><p>The master parameter is a string that specifies a cluster URI to connect to (such as <code class="literal">spark://localhost:7077</code>) or a<code class="literal"> local</code> string to run in local mode. The <code class="literal">appName</code> term is the application name that will be shown in the cluster web UI.</p><p>It is not possible to override the default <code class="literal">SparkContext</code> class, nor is it possible to create a new one within a <a id="id542" class="indexterm"></a>running Spark shell. It is however possible to specify which master the context connects to using the <code class="literal">MASTER</code> environment variable. For example, to run<code class="literal"> spark-shell</code> on four cores, use the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ MASTER=local[4] ./bin/spark-shell </strong></span>
</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl3sec34"></a>Java API</h4></div></div></div><p>The <a id="id543" class="indexterm"></a>
<code class="literal">org.apache.spark.api.java</code> package exposes all the Spark <a id="id544" class="indexterm"></a>features available in the Scala version to Java. The Java API has a <code class="literal">JavaSparkContext</code> class that returns instances of <code class="literal">org.apache.spark.api.java.JavaRDD</code> and works with Java collections instead of Scala ones.</p><p>There are a <a id="id545" class="indexterm"></a>few key differences between the Java and Scala APIs:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Java 7 does not support anonymous or first-class functions; therefore, functions must be implemented by extending the <code class="literal">org.apache.spark.api.java.function.Function</code>, <code class="literal">Function2</code>, and other classes. As of Spark version 1.0 the API has been refactored to support Java 8 lambda expressions. With Java 8, Function classes can be replaced with inline expressions that act as a shorthand for anonymous functions.</p></li><li style="list-style-type: disc"><p>The RDD methods return Java collections</p></li><li style="list-style-type: disc"><p>Key-value pairs, which are simply written as (<code class="literal">key</code>, <code class="literal">value</code>) in Scala, are represented by the <code class="literal">scala.Tuple2</code> class.</p></li><li style="list-style-type: disc"><p>To maintain type safety, some RDD and function methods, such as those that handle key pairs <a id="id546" class="indexterm"></a>and doubles, are implemented as specialized <a id="id547" class="indexterm"></a>classes.</p></li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl3sec35"></a>WordCount in Java</h4></div></div></div><a id="id548" class="indexterm"></a><p>An example of WordCount in Java is included with the <a id="id549" class="indexterm"></a>Spark source code distribution at <code class="literal">examples/src/main/java/org/apache/spark/examples/JavaWordCount.java</code>.</p><p>First of all, we <a id="id550" class="indexterm"></a>create a context using the <code class="literal">JavaSparkContext</code> class:</p><div class="informalexample"><pre class="programlisting">   JavaSparkContext sc = new JavaSparkContext(master, "JavaWordCount",
     System.getenv("SPARK_HOME"), JavaSparkContext.jarOfClass(JavaWordCount.class));

    JavaRDD&lt;String&gt; data = sc.textFile(infile, 1);
    JavaRDD&lt;String&gt; words = data.flatMap(new FlatMapFunction&lt;String, String&gt;() {
      @Override
      public Iterable&lt;String&gt; call(String s) {
        return Arrays.asList(s.split(" "));
      }
    });

    JavaPairRDD&lt;String, Integer&gt; ones = words.map(new PairFunction&lt;String, String, Integer&gt;() {
      @Override
      public Tuple2&lt;String, Integer&gt; call(String s) {
        return new Tuple2&lt;String, Integer&gt;(s, 1);
      }
    });
    
    JavaPairRDD&lt;String, Integer&gt; counts = ones.reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() {
      @Override
      public Integer call(Integer i1, Integer i2) {
        return i1 + i2;
      }
    });</pre></div><p>We then build an RDD from the HDFS location <code class="literal">infile</code>. In the first step of the transformation chain, we tokenize each tweet in the dataset and return a list of words. We use an instance <a id="id551" class="indexterm"></a>of <code class="literal">JavaPairRDD&lt;String, Integer&gt;</code> to count <a id="id552" class="indexterm"></a>occurrences of each word. Finally, we reduce the RDD to a new <code class="literal">JavaPairRDD&lt;String, Integer&gt;</code> instance that contains a list of tuples, each <a id="id553" class="indexterm"></a>representing a word and the number of times it was found in the dataset.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl3sec36"></a>Python API</h4></div></div></div><p>PySpark requires <a id="id554" class="indexterm"></a>Python version 2.6 or higher. RDDs support the same methods as their Scala counterparts but take Python functions and return Python <a id="id555" class="indexterm"></a>collection types. Lambda syntax (<a class="ulink" href="https://docs.python.org/2/reference/expressions.html" target="_blank">https://docs.python.org/2/reference/expressions.html</a>) is used to pass functions <a id="id556" class="indexterm"></a>to RDDs.</p><p>The word count in <code class="literal">pyspark</code> is relatively similar to its Scala counterpart:</p><div class="informalexample"><pre class="programlisting">tweets = sc.textFile("/tmp/sample.txt")
counts = tweets.flatMap(lambda tweet: tweet.split(' ')) \
                  .map(lambda word: (word, 1)) \
                  .reduceByKey(lambda m,n:m+n)</pre></div><p>The <code class="literal">lambda</code> construct creates anonymous functions at runtime. <code class="literal">lambda tweet: tweet.split(' ')</code> creates a function that takes a string <code class="literal">tweet</code> as the input and outputs a list of strings split by whitespace. Spark's <code class="literal">flatMap</code> applies this function to each line of the <code class="literal">tweets</code> dataset. In the <code class="literal">map</code> phase, for each <code class="literal">word</code> token, <code class="literal">lambda word: (word, 1)</code> returns <code class="literal">(word, 1)</code> tuples that indicate the occurrence of a word in the dataset. In <code class="literal">reduceByKey</code>, we group these tuples by key - word - and sum the values together to obtain the word count with <code class="literal">lambda m,n:m+n</code>.</p></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec39"></a>The Spark ecosystem</h2></div></div><hr /></div><p>Apache Spark <a id="id557" class="indexterm"></a>powers a number of tools, both as a library and as an execution engine.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec105"></a>Spark Streaming</h3></div></div></div><p>Spark <a id="id558" class="indexterm"></a>Streaming (found at <a class="ulink" href="http://spark.apache.org/docs/latest/streaming-programming-guide.html" target="_blank">http://spark.apache.org/docs/latest/streaming-programming-guide.html</a>) is an extension of the <a id="id559" class="indexterm"></a>Scala API that allows data ingestion from streams such as Kafka, Flume, Twitter, ZeroMQ, and TCP sockets.</p><p>Spark Streaming <a id="id560" class="indexterm"></a>receives live input data streams and divides the data into batches (arbitrarily sized time windows), which are then processed by the Spark core engine to generate the final stream of results in batches. This high-level abstraction is called DStream (<code class="literal">org.apache.spark.streaming.dstream.DStreams</code>) and is implemented as a sequence of RDDs. DStream allows for two kinds of operations: <span class="emphasis"><em>transformations</em></span> and <span class="emphasis"><em>output operations</em></span>. Transformations work on one or more DStreams to create new DStreams. As part of a chain of transformations, data can be persisted either to a storage layer (HDFS) or an output channel. Spark Streaming allows for transformations over a sliding window of data. A window-based operation needs to specify two parameters: the window length, the duration of the window and the slide interval,  the interval at which the window-based operation is performed.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec106"></a>GraphX</h3></div></div></div><p>GraphX (found at <a class="ulink" href="https://spark.apache.org/docs/latest/graphx-programming-guide.html" target="_blank">https://spark.apache.org/docs/latest/graphx-programming-guide.html</a>) is <a id="id561" class="indexterm"></a>an API for graph <a id="id562" class="indexterm"></a>computation that exposes a set of operators and algorithms for graph-oriented <a id="id563" class="indexterm"></a>computation as well as an optimized variant of Pregel.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec107"></a>MLlib</h3></div></div></div><p>MLlib (found at <a class="ulink" href="http://spark.apache.org/docs/latest/mllib-guide.html" target="_blank">http://spark.apache.org/docs/latest/mllib-guide.html</a>) provides common <a id="id564" class="indexterm"></a>
<span class="strong"><strong>Machine Learning</strong></span> (<span class="strong"><strong>ML</strong></span>) functionality, including <a id="id565" class="indexterm"></a>tests and data generators. MLlib <a id="id566" class="indexterm"></a>currently supports four types of algorithms: binary classification, regression, clustering, and collaborative filtering.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec108"></a>Spark SQL</h3></div></div></div><p>Spark SQL is derived <a id="id567" class="indexterm"></a>from Shark, which is an implementation of the Hive data warehousing system that uses Spark as an execution engine. We <a id="id568" class="indexterm"></a>will discuss Hive in <a class="link" href="#" linkend="ch07">Chapter 7</a>, <span class="emphasis"><em>Hadoop and SQL</em></span>. With Spark SQL, it is possible to mix SQL-like queries with Scala or Python code. The result sets returned by a query are themselves RDDs, and as such, they can be manipulated by Spark core methods or MLlib and GraphX.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec40"></a>Processing data with Apache Spark</h2></div></div><hr /></div><p>In this section, we <a id="id569" class="indexterm"></a>will implement the examples from <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Processing â€“ MapReduce and Beyond</em></span>, using the Scala API. We will consider <a id="id570" class="indexterm"></a>both the batch and real-time processing scenarios. We will show you how Spark Streaming can be used to compute statistics on the live Twitter stream.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec109"></a>Building and running the examples</h3></div></div></div><p>Scala <a id="id571" class="indexterm"></a>source code for the examples can be found at <a class="ulink" href="https://github.com/learninghadoop2/book-examples/tree/master/ch5" target="_blank">https://github.com/learninghadoop2/book-examples/tree/master/ch5</a>. We <a id="id572" class="indexterm"></a>will be using <code class="literal">sbt</code> to build, manage, and execute code.</p><p>The <code class="literal">build.sbt</code> file <a id="id573" class="indexterm"></a>controls the codebase metadata and software dependencies; these include the version of the Scala interpreter that Spark links to, a link to the Akka package repository used to resolve implicit dependencies, as well as dependencies on Spark and Hadoop libraries.</p><p>The source code for all examples can be compiled with:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sbt compile</strong></span>
</pre></div><p>Or, it can be packaged into a JAR file with:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sbt package</strong></span>
</pre></div><p>A helper script to execute compiled classes can be generated with:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sbt add-start-script-tasks</strong></span>
<span class="strong"><strong>$ sbt start-script</strong></span>
</pre></div><p>The helper can be invoked as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ target/start &lt;class name&gt; &lt;master&gt; &lt;param1&gt; â€¦ &lt;param n&gt;</strong></span>
</pre></div><p>Here, <code class="literal">&lt;master&gt;</code> is the URI of the master node. An interactive Scala session can be invoked via <code class="literal">sbt</code> with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sbt console</strong></span>
</pre></div><p>This console is not the same as the Spark interactive shell; rather, it is an alternative way to execute code. In order to run Spark code in it we will need to manually import and instantiate a <code class="literal">SparkContext</code> object. All examples presented in this section expect a <code class="literal">twitter4j.properties</code> file containing the consumer key and secret and the access tokens to be present in the same directory where <code class="literal">sbt</code> or <code class="literal">spark-shell</code> is being invoked:</p><div class="informalexample"><pre class="programlisting">oauth.consumerKey=
oauth.consumerSecret=
oauth.accessToken=
oauth.accessTokenSecret=</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl3sec37"></a>Running the examples on YARN</h4></div></div></div><p>To <a id="id574" class="indexterm"></a>run the examples on a <a id="id575" class="indexterm"></a>YARN grid, we first build a JAR file using:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sbt package</strong></span>
</pre></div><p>Then, we ship it to the resource manager using the <code class="literal">spark-submit</code> command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>./bin/spark-submit --class application.to.execute --master yarn-cluster [options] target/scala-2.10/chapter-4_2.10-1.0.jar [&lt;param1&gt; â€¦ &lt;param n&gt;]</strong></span>
</pre></div><p>Unlike the standalone mode, we don't need to specify a <code class="literal">&lt;master&gt;</code> URI. In YARN, the ResourceManager <a id="id576" class="indexterm"></a>is selected from the cluster configuration. More information on <a id="id577" class="indexterm"></a>launching spark in YARN can be found at <a class="ulink" href="http://spark.apache.org/docs/latest/running-on-yarn.html" target="_blank">http://spark.apache.org/docs/latest/running-on-yarn.html</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl3sec38"></a>Finding popular topics</h4></div></div></div><p>Unlike the <a id="id578" class="indexterm"></a>earlier examples with the Spark shell we initialize a <code class="literal">SparkContext</code> as part of the program. We pass three arguments to the <code class="literal">SparkContext</code> constructor: the type of scheduler we want to use, a name for the application, and the directory where Spark is installed:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark.SparkContext._
import org.apache.spark.SparkContext
import scala.util.matching.Regex

object HashtagCount {
  def main(args: Array[String]) {
[â€¦]
  val sc = new SparkContext(master, 
"HashtagCount", 
System.getenv("SPARK_HOME"))

    val file = sc.textFile(inputFile)
    val pattern = new Regex("(?:\\s|\\A|^)[##]+([A-Za-z0-9-_]+)")
    
    val counts = file.flatMap(line =&gt; 
      (pattern findAllIn line).toList)
        .map(word =&gt; (word, 1))
        .reduceByKey((m, n) =&gt; m + n)  
    
    counts.saveAsTextFile(outputPath)
  }
}</pre></div><p>We create an initial RDD from a dataset stored in HDFS - inputFile - and apply logic that is similar to the WordCount example.</p><p>For each tweet in the dataset, we extract an array of strings that match the hashtag pattern <code class="literal">(pattern findAllIn line).toArray</code>, and we count an occurrence of each string using the map operator. This generates a new RDD as a list of tuples in the form:</p><div class="informalexample"><pre class="programlisting">(word, 1), (word2, 1), (word, 1) </pre></div><p>Finally, we combine together elements of this RDD using the <code class="literal">reduceByKey()</code> method. We store the RDD generated by this last step back into HDFS with <code class="literal">saveAsTextFile</code>.</p><p>The code <a id="id579" class="indexterm"></a>for the standalone driver can be found at <a class="ulink" href="https://github.com/learninghadoop2/book-examples/blob/master/ch5/src/main/scala/com/learninghadoop2/spark/HashTagCount.scala" target="_blank">https://github.com/learninghadoop2/book-examples/blob/master/ch5/src/main/scala/com/learninghadoop2/spark/HashTagCount.scala</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl3sec39"></a>Assigning a sentiment to topics </h4></div></div></div><p>The <a id="id580" class="indexterm"></a>source code of this example can be found at <a class="ulink" href="https://github.com/learninghadoop2/book-examples/blob/master/ch5/src/main/scala/com/learninghadoop2/spark/HashTagSentiment.scala" target="_blank">https://github.com/learninghadoop2/book-examples/blob/master/ch5/src/main/scala/com/learninghadoop2/spark/HashTagSentiment.scala</a>, and the code is as follows:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark.SparkContext._
import org.apache.spark.SparkContext
import scala.util.matching.Regex
import scala.io.Source
    
object HashtagSentiment {
  def main(args: Array[String]) {
   [â€¦]
    val sc = new SparkContext(master, 
"HashtagSentiment", 
System.getenv("SPARK_HOME"))

    val file = sc.textFile(inputFile)

    val positive = Source.fromFile(positiveWordsPath)
      .getLines
      .filterNot(_ startsWith ";")
      .toSet
    val negative = Source.fromFile(negativeWordsPath)
      .getLines
      .filterNot(_ startsWith ";")
      .toSet

    val pattern = new Regex("(?:\\s|\\A|^)[##]+([A-Za-z0-9-_]+)")
    val counts = file.flatMap(line =&gt; (pattern findAllIn line).map({
    word =&gt; (word, sentimentScore(line, positive, negative)) 
    })).reduceByKey({ (m, n) =&gt; (m._1 + n._1, m._2 + n._2) })

    val sentiment = counts.map({hashtagScore =&gt;
    val hashtag = hashtagScore._1
    val score = hashtagScore._2
    val normalizedScore = score._1 / score._2
    (hashtag, normalizedScore)
    })


    sentiment.saveAsTextFile(outputPath)
  }
}</pre></div><p>First, we read a list of positive and negative words into Scala <code class="literal">Set</code> objects and filter out comments (strings beginning with <code class="literal">;</code>).</p><p>When a hashtag is found, we call a function - <code class="literal">sentimentScore</code> - to estimate the sentiment expressed by that given text. This function implements the same logic we used in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Processing â€“ MapReduce and Beyond</em></span>, to estimate the sentiment of a tweet. It takes as input parameters the tweet's text, <code class="literal">str</code>, and a list of positive and negative words as <code class="literal">Set[String]</code> objects. The return value is the difference between the positive and negative <a id="id581" class="indexterm"></a>scores and the number of words in the tweets. In Spark, we represent this return value as a pair of <code class="literal">Double</code> and <code class="literal">Integer</code> objects:</p><div class="informalexample"><pre class="programlisting">def sentimentScore(str: String, positive: Set[String], 
         negative: Set[String]): (Double, Int) = {
   var positiveScore = 0; var negativeScore = 0;
    str.split("""\s+""").foreach { w =&gt;
      if (positive.contains(w)) { positiveScore+=1; }
      if (negative.contains(w)) { negativeScore+=1; }
    } 
    ((positiveScore - negativeScore).toDouble, 
           str.split("""\s+""").length)
}</pre></div><p>We reduce the map output by aggregating by the key (the hashtag). In this phase, we emit a triple made of the hashtag, the sum of the difference between positive and negative scores, and the number of words per tweet. We use an additional map step to normalize the sentiment <a id="id582" class="indexterm"></a>score and store the resulting list of hashtag and sentiment pairs to HDFS.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec110"></a>Data processing on streams</h3></div></div></div><p>The previous <a id="id583" class="indexterm"></a>example can be easily adjusted to <a id="id584" class="indexterm"></a>work on a real-time stream of data. In this and the following section, we will use <code class="literal">spark-streaming-twitter</code> to perform some simple analytics tasks on the real-time firehose:</p><div class="informalexample"><pre class="programlisting">  val window = 10
  val ssc = new StreamingContext(master, "TwitterStreamEcho", Seconds(window), System.getenv("SPARK_HOME"))

  val stream = TwitterUtils.createStream(ssc, auth)

  val tweets = stream.map(tweet =&gt; (tweet.getText()))
  tweets.print()

  ssc.start()
  ssc.awaitTermination()
}   </pre></div><p>The Scala <a id="id585" class="indexterm"></a>source code for this example can be found at <a class="ulink" href="https://github.com/learninghadoop2/book-examples/blob/master/ch5/src/main/scala/com/learninghadoop2/spark/TwitterStreamEcho.scala" target="_blank">https://github.com/learninghadoop2/book-examples/blob/master/ch5/src/main/scala/com/learninghadoop2/spark/TwitterStreamEcho.scala</a>.</p><p>The two key packages we need to import are:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.streaming.twitter._</pre></div><p>We initialize a new <code class="literal">StreamingContext ssc</code> on a local cluster using a 10-second window and use this context to create a <code class="literal">DStream</code> of tweets whose text we print.</p><p>Upon successful execution, Twitter's real-time firehose will be echoed in the terminal in batches of 10 seconds worth of data. Notice that the computation will continue indefinitely but can be interrupted at any moment by pressing <span class="emphasis"><em>Ctrl</em></span> + <span class="emphasis"><em>C</em></span>.</p><p>The <code class="literal">TwitterUtils</code> object is a wrapper to the <code class="literal">Twitter4j</code> library (<a class="ulink" href="http://twitter4j.org/en/index.html" target="_blank">http://twitter4j.org/en/index.html</a>) that ships with <code class="literal">spark-streaming-twitter</code>. A successful call to <code class="literal">TwitterUtils.createStream</code> will return a DStream of <code class="literal">Twitter4j</code> objects (<code class="literal">TwitterInputDStream</code>). In the preceding example, we used the <code class="literal">getText()</code> method to extract the tweet text; however, notice that the <code class="literal">twitter4j</code> object exposes the full <a id="id586" class="indexterm"></a>Twitter API. For instance, we can <a id="id587" class="indexterm"></a>print a stream of users with the following call:</p><div class="informalexample"><pre class="programlisting">val users = stream.map(tweet =&gt; (tweet.getUser().getId(), tweet.getUser().getName()))
users.print()</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl3sec40"></a>State management</h4></div></div></div><p>Spark <a id="id588" class="indexterm"></a>Streaming provides an ad hoc DStream to keep the state of each key in an RDD and the <code class="literal">updateStateByKey</code> method to mutate state.</p><p>We can reuse the code of the batch example to assign and update sentiment scores on streams:</p><div class="informalexample"><pre class="programlisting">object StreamingHashTagSentiment {
[â€¦]
    
    val counts = text.flatMap(line =&gt; (pattern findAllIn line)
      .toList
      .map(word =&gt; (word, sentimentScore(line, positive, negative))))
      .reduceByKey({ (m, n) =&gt; (m._1 + n._1, m._2 + n._2) })

    val sentiment = counts.map({hashtagScore =&gt;
        val hashtag = hashtagScore._1
        val score = hashtagScore._2
        val normalizedScore = score._1 / score._2
        (hashtag, normalizedScore)
    })
    
    val stateDstream = sentiment
         .updateStateByKey[Double](updateFunc)
 
    stateDstream.print

    ssc.checkpoint("/tmp/checkpoint")
    ssc.start()
}</pre></div><p>A state DStream is created by calling <code class="literal">hashtagSentiment.updateStateByKey</code>.</p><p>The <code class="literal">updateFunc</code> <a id="id589" class="indexterm"></a>function implements the state mutation logic, which is a cumulative sum of sentiment scores over a period of time:</p><div class="informalexample"><pre class="programlisting">    val updateFunc = (values: Seq[Double], state: Option[Double]) =&gt; {
      val currentScore = values.sum

      val previousScore = state.getOrElse(0.0)

      Some( (currentScore + previousScore) * decayFactor)
    }   </pre></div><p>
<code class="literal">decayFactor</code> <a id="id590" class="indexterm"></a>is a constant value, less than or equal to zero, that we use to proportionally decrease the score over time. Intuitively, this will fade hashtags if they are not trending anymore. Spark Streaming writes intermediate data for stateful operations to HDFS, so <a id="id591" class="indexterm"></a>we need to checkpoint the Streaming context with <code class="literal">ssc.checkpoint</code>.</p><p>The source code for this example can be found at <a class="ulink" href="https://github.com/learninghadoop2/book-examples/blob/master/ch5/src/main/scala/com/learninghadoop2/spark/StreamingHashTagSentiment.scala" target="_blank">https://github.com/learninghadoop2/book-examples/blob/master/ch5/src/main/scala/com/learninghadoop2/spark/StreamingHashTagSentiment.scala</a>.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec111"></a>Data analysis with Spark SQL</h3></div></div></div><p>Spark <a id="id592" class="indexterm"></a>SQL can ease the <a id="id593" class="indexterm"></a>task of representing and manipulating structured data. We will load a JSON file into a temporary table and calculate simple statistics by blending SQL statements and Scala code:</p><div class="informalexample"><pre class="programlisting">object SparkJson {
   [â€¦]
   val file = sc.textFile(inputFile)
   
   val sqlContext = new org.apache.spark.sql.SQLContext(sc)
   import sqlContext._
   
   val tweets = sqlContext.jsonFile(inFile)
   tweets.printSchema()
   
   // Register the SchemaRDD as a table
   tweets.registerTempTable("tweets")
   val text = sqlContext.sql("SELECT text, user.id FROM tweets")
   
   // Find the ten most popular hashtags
   val pattern = new Regex("(?:\\s|\\A|^)[##]+([A-Za-z0-9-_]+)")
    
   val counts = text.flatMap(sqlRow =&gt; (pattern findAllIn sqlRow(0).toString).toList)
            .map(word =&gt; (word, 1))
            .reduceByKey( (m, n) =&gt; m+n)
   counts.registerTempTable("hashtag_frequency")

counts.printSchema

val top10 = sqlContext.sql("SELECT _1 as hashtag, _2 as frequency FROM hashtag_frequency order by frequency desc limit 10")

top10.foreach(println)
}</pre></div><p>As with previous examples, we instantiate a <code class="literal">SparkContext sc</code> and load the dataset of JSON tweets. We then create an instance of <code class="literal">org.apache.spark.sql.SQLContext</code> based on the existing <code class="literal">sc</code>. The <code class="literal">import sqlContext._</code> gives access to all functions and implicit conventions for <code class="literal">sqlContext</code>. We load the tweets' JSON dataset using <code class="literal">sqlContext.jsonFile</code>. The resulting <code class="literal">tweets</code> object is an instance of <code class="literal">SchemaRDD</code>, which is a new <a id="id594" class="indexterm"></a>type of RDD introduced by <a id="id595" class="indexterm"></a>Spark SQL. The <code class="literal">SchemaRDD</code> class is conceptually similar to a table in a relational database; it is composed of <code class="literal">Row</code> objects and a schema that describes the content in each <code class="literal">Row</code>. We can see the schema for a tweet by calling <code class="literal">tweets.printSchema()</code>. Before we're able to manipulate tweets with SQL statements, we need to register <code class="literal">SchemaRDD</code> as a table in the <code class="literal">SQLContext</code>. We then extract the text field of a JSON tweet with an SQL query. Note that the output of <code class="literal">sqlContext.sql</code> is an RDD again. As such, we can manipulate it using Spark core methods. In our case, we reuse the logic used in previous examples to extract hashtags and <a id="id596" class="indexterm"></a>count their occurrences. Finally, we register the resulting RDD as a table, <code class="literal">hashtag_frequency</code>, and order hashtags by frequency with a SQL query.</p><p>The source code of <a id="id597" class="indexterm"></a>this example can be found at <a class="ulink" href="https://github.com/learninghadoop2/book-examples/blob/master/ch5/src/main/scala/com/learninghadoop2/spark/SparkJson.scala" target="_blank">https://github.com/learninghadoop2/book-examples/blob/master/ch5/src/main/scala/com/learninghadoop2/spark/SparkJson.scala</a>.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl3sec41"></a>SQL on data streams</h4></div></div></div><p>At the time <a id="id598" class="indexterm"></a>of writing, a <code class="literal">SQLContext</code> cannot be directly instantiated from a <code class="literal">StreamingContext</code> object. It is, however, possible to query a DStream by registering a <code class="literal">SchemaRDD</code> for each RDD in a given <a id="id599" class="indexterm"></a>stream:</p><div class="informalexample"><pre class="programlisting">object SqlOnStream {
[â€¦]

    val ssc = new StreamingContext(sc, Seconds(window))

    val gson = new Gson()

    val dstream = TwitterUtils
   .createStream(ssc, auth)
   .map(gson.toJson(_))

    val sqlContext = new org.apache.spark.sql.SQLContext(sc)
    import sqlContext._

   dstream.foreachRDD( rdd =&gt; {
      rdd.foreach(println)
        val jsonRDD = sqlContext.jsonRDD(rdd)
        jsonRDD.registerTempTable("tweets")
        jsonRDD.printSchema 

         sqlContext.sql(query)
    })

    ssc.checkpoint("/tmp/checkpoint")
    ssc.start() 
    ssc.awaitTermination() 
}</pre></div><p>In order to get the two working together, we first create a <code class="literal">SparkContext sc</code> that we use to initialize both a <code class="literal">StreamingContext ssc</code> and a <code class="literal">sqlContext</code>. As in previous examples, we use <code class="literal">TwitterUtils.createStream</code> to create a DStream RDD <code class="literal">dstream</code>. In this example, we use Google's Gson JSON parser to serialize each <code class="literal">twitter4j</code> object to a JSON string. To execute Spark SQL queries on the stream, we register a <code class="literal">SchemaRDD jsonRDD</code> within a <code class="literal">dstream.foreachRDD</code> loop. We use the <code class="literal">sqlContext.jsonRDD</code> method to create an RDD from a batch of JSON tweets. At this point, we can query the <code class="literal">SchemaRDD</code> <a id="id600" class="indexterm"></a>using the <code class="literal">sqlContext.sql</code> method.</p><p>The source <a id="id601" class="indexterm"></a>code of this example can be <a id="id602" class="indexterm"></a>found at <a class="ulink" href="https://github.com/learninghadoop2/book-examples/blob/master/ch5/src/main/scala/com/learninghadoop2/spark/SqlOnStream.scala" target="_blank">https://github.com/learninghadoop2/book-examples/blob/master/ch5/src/main/scala/com/learninghadoop2/spark/SqlOnStream.scala</a>.</p></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec41"></a>Comparing Samza and Spark Streaming</h2></div></div><hr /></div><p>It is useful to <a id="id603" class="indexterm"></a>compare Samza and Spark Streaming to help identify the areas in which each can best be applied. As it has been hopefully made clear <a id="id604" class="indexterm"></a>in this book, these technologies are very much complimentary. Even though Spark Streaming might appear competitive with Samza, we feel both products offer compelling advantages in certain areas.</p><p>Samza shines when the input data is truly a stream of discrete events and you wish to build processing that operates on this type of input. Samza jobs running on Kafka can have latencies in the order of milliseconds. This provides a programming model focused on the individual messages and is the better fit for true near real-time processing applications. Though it lacks support to build topologies of collaborating jobs, its simple model allows similar constructs to be built and, perhaps more importantly, be easily reasoned about. Its model of partitioning and scaling also focuses on simplicity, which again makes a Samza application very easy to understand and gives it a significant advantage when dealing with something as intrinsically complex as real-time data.</p><p>Spark is much more than a streaming product. Its support for building distributed data structures from existing datasets and using powerful primitives to manipulate these gives it the ability to process large datasets at a higher level of granularity. Other products in the Spark ecosystem build additional interfaces or abstractions upon this common batch processing core. This is very much a different focus to the message stream model of Samza.</p><p>This batch model is also demonstrated when we look at Spark Streaming; instead of a per-message processing model, it slices the message stream into a series of RDDs. With a fast execution engine, this means latencies as low as 1 second (<a class="ulink" href="http://www.cs.berkeley.edu/~matei/papers/2012/hotcloud_spark_streaming.pdf" target="_blank">http://www.cs.berkeley.edu/~matei/papers/2012/hotcloud_spark_streaming.pdf</a>). For <a id="id605" class="indexterm"></a>workloads that wish to analyze the stream in such a way, this will be a better fit than Samza's per-message model, which <a id="id606" class="indexterm"></a>requires additional logic to provide such windowing.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec42"></a>Summary</h2></div></div><hr /></div><p>This chapter explored Spark and showed you how it adds iterative processing as a new rich framework upon which applications can be built atop YARN. In particular, we highlighted:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The distributed data-structure-based processing model of Spark and how it allows very efficient in-memory data processing</p></li><li style="list-style-type: disc"><p>The broader Spark ecosystem and how multiple additional projects are built atop it to specialize the computational model even further</p></li></ul></div><p>In the next chapter we will explore Apache Pig and its programming language, Pig Latin. We will see how this tool can greatly simplify software development for Hadoop by abstracting away some of the MapReduce and Spark complexity.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch06"></a>ChapterÂ 6.Â Data Analysis with Apache Pig</h2></div></div></div><p>In the previous chapters, we explored a number of APIs for data processing. MapReduce, Spark, Tez and Samza are rather low-level, and writing non-trivial business logic with them often requires significant Java development. Moreover, different users will have different needs. It might be impractical for an analyst to write MapReduce code or build a DAG of inputs and outputs to answer some simple queries. At the same time, a software engineer or a researcher might want to prototype ideas and algorithms using high-level abstractions before jumping into low-level implementation details.</p><p>In this chapter and the following one, we will explore some tools that provide a way to process data on HDFS using higher-level abstractions. In this chapter we will explore Apache Pig, and, in particular, we will cover the following topics:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>What Apache Pig is and the dataflow model it provides</p></li><li style="list-style-type: disc"><p>Pig Latin's data types and functions</p></li><li style="list-style-type: disc"><p>How Pig can be easily enhanced using custom user code</p></li><li style="list-style-type: disc"><p>How we can use Pig to analyze the Twitter stream</p></li></ul></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec43"></a>An overview of Pig</h2></div></div><hr /></div><p>Historically, the<a id="id607" class="indexterm"></a> Pig toolkit consisted of a compiler that generated MapReduce programs, bundled their dependencies, and executed them on Hadoop. Pig jobs are written in a language called <span class="strong"><strong>Pig Lat</strong></span>
<span class="strong"><strong>in</strong></span><a id="id608" class="indexterm"></a> and can be executed in both interactive and batch fashions. Furthermore, Pig Latin can be extended using <a id="id609" class="indexterm"></a>
<span class="strong"><strong>User Defined Functions</strong></span> (<span class="strong"><strong>UDFs</strong></span>) written in Java, Python, Ruby, Groovy, or JavaScript.</p><p>Pig use cases<a id="id610" class="indexterm"></a> include the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Data processing</p></li><li style="list-style-type: disc"><p>Ad hoc analytical queries</p></li><li style="list-style-type: disc"><p>Rapid prototyping of algorithms</p></li><li style="list-style-type: disc"><p>Extract Transform Load pipelines</p></li></ul></div><p>Following a <a id="id611" class="indexterm"></a>trend we have seen in previous chapters, Pig is moving towards a general-purpose computing architecture. As of version 0.13 the <a id="id612" class="indexterm"></a>
<span class="strong"><strong>ExecutionEngine</strong></span> interface (<code class="literal">org.apache.pig.backend.executionengine</code>) acts as a bridge between the frontend and the backend of Pig, allowing Pig Latin scripts to be compiled and executed on frameworks other than MapReduce. At the time of writing, version 0.13 ships <a id="id613" class="indexterm"></a>with <span class="strong"><strong>MRExecutionEngine</strong></span> (<code class="literal">org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRExecutionEngine</code>) and work on a low-latency backend based on <a id="id614" class="indexterm"></a>
<span class="strong"><strong>Tez</strong></span> (<code class="literal">org.apache.pig.backend.hadoop.executionengine.tez.*</code>) is expected to be included in version 0.14 (see <a class="ulink" href="https://issues.apache.org/jira/browse/PIG-3446" target="_blank">https://issues.apache.org/jira/browse/PIG-3446</a>). Work on integrating Spark is currently in progress in the development branch (see <a class="ulink" href="https://issues.apache.org/jira/browse/PIG-4059" target="_blank">https://issues.apache.org/jira/browse/PIG-4059</a>).</p><p>Pig 0.13 comes with a number of performance enhancements for the MapReduce backend, in particular two features to reduce latency of small jobs: <span class="emphasis"><em>direct HDFS access</em></span> (<a class="ulink" href="https://issues.apache.org/jira/browse/PIG-3642" target="_blank">https://issues.apache.org/jira/browse/PIG-3642</a>) and auto local <span class="emphasis"><em>mode</em></span> (<a class="ulink" href="https://issues.apache.org/jira/browse/PIG-3463" target="_blank">https://issues.apache.org/jira/browse/PIG-3463</a>). Direct HDFS, the <code class="literal">opt.fetch</code> property, is turned on by default. When doing a <code class="literal">DUMP</code> in a simple (map-only) script that contains only <code class="literal">LIMIT</code>, <code class="literal">FILTER</code>, <code class="literal">UNION</code>, <code class="literal">STREAM</code>, or <code class="literal">FOREACH</code> operators, input data is fetched from HDFS, and the query is executed directly in Pig, bypassing MapReduce. With auto local, the <code class="literal">pig.auto.local.enabled</code> property, Pig will run a query in the Hadoop local mode when the data size is smaller than <code class="literal">pig.auto.local.input.maxbytes</code>. Auto local is off by default.</p><p>Pig will launch MapReduce jobs if both modes are off or if the query is not eligible for either. If both modes are on, Pig will check whether the query is eligible for direct access and, if not, fall back to auto local. Failing that, it will execute the query on MapReduce.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec44"></a>Getting started</h2></div></div><hr /></div><p>We will use<a id="id615" class="indexterm"></a> the <code class="literal">stream.py</code> script options to extract JSON data and retrieve a specific number of tweets; we can run this with a command such as the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ python stream.py -j -n 10000 &gt; tweets.json</strong></span>
</pre></div><p>The <code class="literal">tweets.json</code> file will contain one JSON string on each line representing a tweet.</p><p>Remember that the Twitter API credentials need to be made available as environment variables or hardcoded in the script itself.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec45"></a>Running Pig</h2></div></div><hr /></div><p>Pig is<a id="id616" class="indexterm"></a> a tool that translates statements written in Pig Latin and executes them either on a single machine in standalone mode or on a full Hadoop cluster when in distributed mode. Even in the latter, Pig's role is to translate Pig Latin statements into MapReduce jobs and therefore it doesn't require the installation of additional services or daemons. It is used as a command-line tool with its associated libraries.</p><p>Cloudera CDH ships with Apache Pig version 0.12. Alternatively, the Pig source code and binary distributions<a id="id617" class="indexterm"></a> can be obtained at <a class="ulink" href="https://pig.apache.org/releases.html" target="_blank">https://pig.apache.org/releases.html</a>.</p><p>As can be expected, the MapReduce mode requires access to a Hadoop cluster and HDFS installation. MapReduce mode is the default mode executed when running the Pig command at the command-line prompt. Scripts can be executed with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ pig -f &lt;script&gt;</strong></span>
</pre></div><p>Parameters can be passed via the command line using <code class="literal">-param &lt;param&gt;=&lt;val&gt;</code>, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ pig â€“param input=tweets.txt</strong></span>
</pre></div><p>Parameters can also be specified in a <code class="literal">param</code> file that can be passed to Pig using the <code class="literal">-param_file &lt;file&gt;</code> option. Multiple files can be specified. If a parameter is present multiple times in the file, the last value will be used and a warning will be displayed. A parameter file contains one parameter per line. Empty lines and comments (specified by starting a line with <code class="literal">#</code>) are allowed. Within a Pig script, parameters are in the form <code class="literal">$&lt;parameter&gt;</code>. The default value can be assigned using the <code class="literal">default</code> statement: <code class="literal">%default input tweets.json'</code>. The <code class="literal">default</code> command will not work within a Grunt session; we'll discuss Grunt in the next section.</p><p>In local mode, all files are installed and run using the local host and filesystem. Specify local mode using the <code class="literal">-x</code> flag:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ pig -x local</strong></span>
</pre></div><p>In both execution modes, Pig programs can be run either in an interactive shell or in batch mode.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec112"></a>Grunt â€“ the Pig interactive shell</h3></div></div></div><p>Pig can <a id="id618" class="indexterm"></a>run <a id="id619" class="indexterm"></a>in an interactive mode using the Grunt shell, which is invoked when we use the <code class="literal">pig</code> command at the terminal prompt. In the rest of this chapter, we will assume that examples are executed within a Grunt session. Other than executing Pig Latin statements, Grunt offers a number of utilities and access to shell commands:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">fs</code>: allows users to manipulate Hadoop filesystem objects and has the same semantics as the Hadoop CLI</p></li><li style="list-style-type: disc"><p>
<code class="literal">sh</code>: <a id="id620" class="indexterm"></a>executes commands via the operating system shell</p></li><li style="list-style-type: disc"><p>
<code class="literal">exec</code>: launches a Pig script within an interactive Grunt session</p></li><li style="list-style-type: disc"><p>
<code class="literal">kill</code>: kills a MapReduce job</p></li><li style="list-style-type: disc"><p>
<code class="literal">help</code>: <a id="id621" class="indexterm"></a>prints a list of all available commands</p></li></ul></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec42"></a>Elastic MapReduce</h4></div></div></div><p>Pig scripts <a id="id622" class="indexterm"></a>can be executed on EMR by creating a cluster with <code class="literal">--applications Name=Pig,Args=--version,&lt;version&gt;</code>, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ aws emr create-cluster \</strong></span>
<span class="strong"><strong>--name "Pig cluster" \</strong></span>
<span class="strong"><strong>--ami-version &lt;ami version&gt; \</strong></span>
<span class="strong"><strong>--instance-type &lt;EC2 instance&gt; \</strong></span>
<span class="strong"><strong>--instance-count &lt;number of nodes&gt; \</strong></span>
<span class="strong"><strong>--applications Name=Pig,Args=--version,&lt;version&gt;\</strong></span>
<span class="strong"><strong>--log-uri &lt;S3 bucket&gt; \</strong></span>
<span class="strong"><strong>--steps Type=PIG,\ </strong></span>
<span class="strong"><strong>Name="Pig script",\</strong></span>
<span class="strong"><strong>Args=[-f,s3://&lt;script location&gt;,\</strong></span>
<span class="strong"><strong>-p,input=&lt;input param&gt;,\</strong></span>
<span class="strong"><strong>-p,output=&lt;output param&gt;]</strong></span>
</pre></div><p>The preceding command will provision a new EMR cluster and execute <code class="literal">s3://&lt;script location&gt;</code>. Notice that the scripts to be executed and the input (<code class="literal">-p input</code>) and output (<code class="literal">-p output</code>) paths are expected to be located on S3.</p><p>As an alternative to creating a new EMR cluster, it is possible to add Pig steps to an already-instantiated EMR cluster using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ aws emr add-steps \</strong></span>
<span class="strong"><strong>--cluster-id &lt;cluster id&gt;\</strong></span>
<span class="strong"><strong>--steps Type=PIG,\ </strong></span>
<span class="strong"><strong>Name= "Other Pig script",\</strong></span>
<span class="strong"><strong>Args=[-f,s3://&lt;script location&gt;,\</strong></span>
<span class="strong"><strong>-p,input=&lt;input param&gt;,\</strong></span>
<span class="strong"><strong>-p,output=&lt;output param&gt;]</strong></span>
</pre></div><p>In the preceding command, <code class="literal">&lt;cluster id&gt;</code> is the ID of the instantiated cluster.</p><p>It is also possible to ssh into the master node and run Pig Latin statements within a Grunt session with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ aws emr ssh --cluster-id &lt;cluster id&gt; --key-pair-file &lt;key pair&gt;</strong></span>
</pre></div></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec46"></a>Fundamentals of Apache Pig</h2></div></div><hr /></div><p>The <a id="id623" class="indexterm"></a>primary interface to program Apache Pig is Pig Latin, a procedural language that implements ideas of the dataflow paradigm.</p><p>Pig Latin programs are generally organized as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>A <code class="literal">LOAD</code> statement reads data from HDFS</p></li><li style="list-style-type: disc"><p> A series of statements aggregates and manipulates data</p></li><li style="list-style-type: disc"><p>A <code class="literal">STORE</code> statement writes output to the filesystem</p></li><li style="list-style-type: disc"><p>Alternatively, a <code class="literal">DUMP</code> statement displays the output to the terminal</p></li></ul></div><p>The following example shows a sequence of statements that outputs the top 10 hashtags ordered by the frequency, extracted from the dataset of tweets:</p><div class="informalexample"><pre class="programlisting">tweets = LOAD 'tweets.json' 
  USING JsonLoader('created_at:chararray, 
    id:long, 
    id_str:chararray, 
    text:chararray');

hashtags = FOREACH tweets {
  GENERATE FLATTEN(
    REGEX_EXTRACT(
      text, 
      '(?:\\s|\\A|^)[##]+([A-Za-z0-9-_]+)', 1)
    ) as tag;
}

hashtags_grpd = GROUP hashtags BY tag;
hashtags_count = FOREACH hashtags_grpd {
  GENERATE 
    group, 
    COUNT(hashtags) as occurrencies; 
}
hashtags_count_sorted = ORDER hashtags_count BY occurrencies DESC;
top_10_hashtags = LIMIT hashtags_count_sorted 10;
DUMP top_10_hashtags;</pre></div><p>First, we load the <code class="literal">tweets.json</code> dataset from HDFS, de-serialize the JSON file, and map it to a four-column schema that contains a tweet's creation time, its ID in numerical and string form, and the text. For each tweet, we extract hashtags from its text using a regular expression. We aggregate on hashtag, count the number of occurrences, and order by frequency. Finally, we limit the ordered records to the top 10 most frequent hashtags.</p><p>A series of statements like the previous one is picked up by the Pig compiler, transformed into MapReduce jobs, and executed on a Hadoop cluster. The planner and optimizer will resolve dependencies on input and output relations and parallelize the execution of statements wherever possible.</p><p>
<span class="strong"><strong>Statements</strong></span><a id="id624" class="indexterm"></a> are the building blocks of processing data with Pig. They take a relation as input and produce another relation as output. In Pig Latin terms, a relation can be defined as a bag of <a id="id625" class="indexterm"></a>
<span class="strong"><strong>tuples</strong></span>, two data types we will use throughout the remainder of this chapter.</p><p>Users<a id="id626" class="indexterm"></a> experienced with SQL and the relational data model might find Pig Latin's syntax somewhat familiar. While there are indeed similarities in the syntax itself, Pig Latin implements an entirely different computational model. Pig Latin is procedural, it specifies the actual data transforms to be performed, whereas SQL is declarative and describes the nature of the problem but does not specify the actual runtime processing. In terms of organizing data, a relation can be thought of as a table in a relational database, where tuples in a bag correspond to the rows in a table. Relations are unordered and therefore easily parallelizable, and they are less constrained than relational tables. Pig relations can contain tuples with different numbers of fields, and those with the same field count can have fields of different types in corresponding positions.</p><p>A key difference between SQL and the dataflow model adopted by Pig Latin lies in how splits in a data pipeline are managed. In the relational world, a declarative language such as SQL implements and executes queries that will generate a single result. The dataflow model sees data transformations as a graph where input and output are nodes connected by an operator. For instance, intermediate steps of a query might require the input to be grouped by a number of keys and result in multiple outputs (<code class="literal">GROUP BY</code>). Pig has built-in mechanisms to manage multiple data flows in such a graph by executing operators as soon as inputs are readily available and potentially apply different operators to each flow. For instance, Pig's implementation of the <code class="literal">GROUP BY</code> operator uses the <a id="id627" class="indexterm"></a>parallel feature (<a class="ulink" href="http://pig.apache.org/docs/r0.12.0/perf.html#parallel" target="_blank">http://pig.apache.org/docs/r0.12.0/perf.html#parallel</a>) to allow a user to increase the number of reduce tasks for the MapReduce jobs generated and hence increases concurrency. An additional side effect of this property is that when multiple operators can be executed in parallel in the same program, Pig does so (more details on Pig's multi-query implementation<a id="id628" class="indexterm"></a> can be found at <a class="ulink" href="http://pig.apache.org/docs/r0.12.0/perf.html#multi-query-execution" target="_blank">http://pig.apache.org/docs/r0.12.0/perf.html#multi-query-execution</a>). Another consequence of Pig Latin's approach to computation is that it allows the persistence of data at any point in the pipeline. It allows the developer to select specific operator implementations and execution plans when necessary, effectively overriding the optimizer.</p><p>Pig Latin allows and even encourages developers to insert their own code almost anywhere in a pipeline by means of <span class="strong"><strong>User Defined Functions</strong></span> (<span class="strong"><strong>UDFs</strong></span>)<a id="id629" class="indexterm"></a> as well as by utilizing Hadoop streaming. UDFs allow users to specify custom business logic on how data is loaded, how it is stored, and how it is processed, whereas streaming allows users to launch executables at any point in the data flow.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec47"></a>Programming Pig</h2></div></div><hr /></div><p>Pig Latin <a id="id630" class="indexterm"></a>comes with a number of built-in functions (the eval, load/store, math, string, bag, and tuple functions) and a number of scalar and complex data types. Additionally, Pig allows function and data-type extension by means of UDFs and dynamic invocation of Java methods.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec113"></a>Pig data types</h3></div></div></div><p>Pig<a id="id631" class="indexterm"></a> supports the following <a id="id632" class="indexterm"></a>scalar data types:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">int</code>:<a id="id633" class="indexterm"></a> a signed 32-bit integer</p></li><li style="list-style-type: disc"><p>
<code class="literal">long</code>: <a id="id634" class="indexterm"></a>a signed 64-bit integer</p></li><li style="list-style-type: disc"><p>
<code class="literal">float</code>:<a id="id635" class="indexterm"></a> a 32-bit floating point</p></li><li style="list-style-type: disc"><p>
<code class="literal">double</code>:<a id="id636" class="indexterm"></a> a 64-bit floating point</p></li><li style="list-style-type: disc"><p>
<code class="literal">chararray</code>:<a id="id637" class="indexterm"></a> a character array (string) in Unicode UTF-8 format</p></li><li style="list-style-type: disc"><p>
<code class="literal">bytearray</code>:<a id="id638" class="indexterm"></a> a byte array (blob)</p></li><li style="list-style-type: disc"><p>
<code class="literal">boolean</code>:<a id="id639" class="indexterm"></a> a boolean</p></li><li style="list-style-type: disc"><p>
<code class="literal">datetime</code>:<a id="id640" class="indexterm"></a> a datetime</p></li><li style="list-style-type: disc"><p>
<code class="literal">biginteger</code>:<a id="id641" class="indexterm"></a> a Java BigInteger</p></li><li style="list-style-type: disc"><p>
<code class="literal">bigdecimal</code>:<a id="id642" class="indexterm"></a> a Java BigDecimal</p></li></ul></div><p>Pig supports the following <a id="id643" class="indexterm"></a>complex data types:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">map</code>:<a id="id644" class="indexterm"></a> an associative array enclosed by <code class="literal">[]</code>, with the key and value separated by <code class="literal">#</code>, and items separated by <code class="literal">,</code>
</p></li><li style="list-style-type: disc"><p>
<code class="literal">tuple</code>:<a id="id645" class="indexterm"></a> an ordered list of data, where elements can be of any scalar or complex type enclosed by <code class="literal">()</code>, with items separated by <code class="literal">,</code>
</p></li><li style="list-style-type: disc"><p>
<code class="literal">bag</code>:<a id="id646" class="indexterm"></a> an unordered collection of tuples enclosed by <code class="literal">{}</code> and separated by <code class="literal">,</code>
</p></li></ul></div><p>By default, Pig treats data as untyped. The user can declare the types of data at load time or manually cast it when necessary. If a data type is not declared, but a script implicitly treats a value as a certain type, Pig will assume it is of that type and cast it accordingly. The fields of a bag or tuple can be referred to by the name <code class="literal">tuple.field</code> or by the position <code class="literal">$&lt;index&gt;</code>. Pig counts from 0 and hence the first element will be denoted as <code class="literal">$0</code>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec114"></a>Pig functions</h3></div></div></div><a id="id647" class="indexterm"></a><p>Built-in functions<a id="id648" class="indexterm"></a> are implemented in Java, and<a id="id649" class="indexterm"></a> they try to follow standard Java conventions. There are however a number of differences to keep in mind, which are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Function names are case sensitive and uppercase</p></li><li style="list-style-type: disc"><p>If the result value is null, empty, or <span class="strong"><strong>not a number</strong></span> (<span class="strong"><strong>NaN</strong></span>), Pig returns null</p></li><li style="list-style-type: disc"><p>If Pig is unable to process the expression, it returns an exception</p></li></ul></div><p>A list of all <a id="id650" class="indexterm"></a>built-in functions can be found at <a class="ulink" href="http://pig.apache.org/docs/r0.12.0/func.html" target="_blank">http://pig.apache.org/docs/r0.12.0/func.html</a>.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec43"></a>Load/store</h4></div></div></div><p>Load/store functions<a id="id651" class="indexterm"></a> determine how data goes into and comes out of Pig. The <code class="literal">PigStorage</code>, <code class="literal">TextLoader</code>, and <code class="literal">BinStorage</code> functions can be used to read and write UTF-8 delimited, unstructured text, and binary data respectively. Support for compression is determined by the load/store function. The <code class="literal">PigStorage</code> and <code class="literal">TextLoader</code> functions support gzip and bzip2 compression for both read (load) and write (store). The <code class="literal">BinStorage</code> function does not support compression.</p><p>As of version 0.12, Pig includes built-in support for loading and storing Avro and JSON data via the <code class="literal">AvroStorage</code> (load/store), <code class="literal">JsonStorage</code> (store), and <code class="literal">JsonLoader</code> (load). At the time of writing, JSON support is still somewhat limited. In particular, Pig expects a schema for the data to be provided as an argument to <code class="literal">JsonLoader/JsonStorage</code>, or it assumes that <code class="literal">.pig_schema</code> (produced by <code class="literal">JsonStorage</code>) is present in the directory containing the input data. In practice, this makes it difficult to work with JSON dumps not generated by Pig itself.</p><p>As seen in our following example, we can load the JSON dataset with <code class="literal">JsonLoader</code>:</p><div class="informalexample"><pre class="programlisting">tweets = LOAD 'tweets.json' USING JsonLoader(
'created_at:chararray,  
id:long, 
id_str:chararray, 
text:chararray,
source:chararray');</pre></div><p>We provide <a id="id652" class="indexterm"></a>a schema so that the first five elements of a JSON object <code class="literal">created_id</code>, <code class="literal">id</code>, <code class="literal">id_str</code>, <code class="literal">text</code>, and <code class="literal">source</code> are mapped. We can look at the schema of tweets by using <code class="literal">describe tweets</code>, which returns the following:</p><div class="informalexample"><pre class="programlisting"> tweets: {created_at: chararray,id: long,id_str: chararray,text: chararray,source: chararray} </pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec44"></a>Eval</h4></div></div></div><p>Eval functions<a id="id653" class="indexterm"></a> implement a set of operations to be applied on an expression that returns a bag or map data type. The expression result is evaluated within the function context.</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">AVG(expression)</code>: <a id="id654" class="indexterm"></a>computes the average of the numeric values in a single-column bag</p></li><li style="list-style-type: disc"><p>
<code class="literal">COUNT(expression)</code>:<a id="id655" class="indexterm"></a> counts all elements with non-null values in the first position in a bag</p></li><li style="list-style-type: disc"><p>
<code class="literal">COUNT_STAR(expression)</code>: <a id="id656" class="indexterm"></a>counts all elements in a bag</p></li><li style="list-style-type: disc"><p>
<code class="literal">IsEmpty(expression)</code>: <a id="id657" class="indexterm"></a>checks whether a bag or map is empty</p></li><li style="list-style-type: disc"><p>
<code class="literal">MAX(expression)</code>, <code class="literal">MIN(expression)</code>, and <code class="literal">SUM(expression)</code>: return the<a id="id658" class="indexterm"></a> max, min, or the sum<a id="id659" class="indexterm"></a> of <a id="id660" class="indexterm"></a>elements in a bag</p></li><li style="list-style-type: disc"><p>
<code class="literal">TOKENIZE(exp</code>
<code class="literal">ression)</code>: <a id="id661" class="indexterm"></a>splits a string and outputs a bag of words</p></li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec45"></a>The tuple, bag, and map functions</h4></div></div></div><p>These<a id="id662" class="indexterm"></a> functions <a id="id663" class="indexterm"></a>allow <a id="id664" class="indexterm"></a>conversion from and to the bag, tuple, and map types. They include the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">TOTUPLE(expression)</code>, <code class="literal">TOMAP(expression)</code>, and <code class="literal">TOBAG(expression)</code>: These <a id="id665" class="indexterm"></a>coerce <a id="id666" class="indexterm"></a>
<code class="literal">expression</code> <a id="id667" class="indexterm"></a>to a tuple, map, or bag</p></li><li style="list-style-type: disc"><p>
<code class="literal">TOP(n, column, relation)</code>: This returns <a id="id668" class="indexterm"></a>the top <code class="literal">n</code> tuples from a bag of tuples</p></li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec46"></a>The math, string, and datetime functions</h4></div></div></div><p>Pig <a id="id669" class="indexterm"></a>exposes a<a id="id670" class="indexterm"></a> number of functions <a id="id671" class="indexterm"></a>provided by the <code class="literal">java.lang.Math</code>, <code class="literal">java.lang.String</code>, <code class="literal">java.util.Date</code>, and Joda-Time <code class="literal">DateTime</code> class (found at <a class="ulink" href="http://www.joda.org/joda-time/" target="_blank">http://www.joda.org/joda-time/</a>).</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec47"></a>Dynamic invokers</h4></div></div></div><p>Dynamic invokers<a id="id672" class="indexterm"></a> allow<a id="id673" class="indexterm"></a> the execution of Java functions without having to wrap them in a UDF. They can be used for any static function that:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>accepts no arguments or accepts a combination of <code class="literal">string</code>, <code class="literal">int</code>, <code class="literal">long</code>, <code class="literal">double</code>, <code class="literal">float</code>, or <code class="literal">array</code> with these same types</p></li><li style="list-style-type: disc"><p>returns a <code class="literal">string</code>, <code class="literal">int</code>, <code class="literal">long</code>, <code class="literal">double</code>, or <code class="literal">float</code> value</p></li></ul></div><p>Only primitives can be used for numbers and Java boxed classes (such as Integer) cannot be used as arguments. Depending on the return type, a specific kind of invoker must be used: <code class="literal">InvokeForString</code>, <code class="literal">InvokeForInt</code>, <code class="literal">InvokeForLong</code>, <code class="literal">InvokeForDouble</code>, or <code class="literal">InvokeForFloat</code>. More details <a id="id674" class="indexterm"></a>regarding dynamic invokers can be found at <a class="ulink" href="http://pig.apache.org/docs/r0.12.0/func.html#dynamic-invokers" target="_blank">http://pig.apache.org/docs/r0.12.0/func.html#dynamic-invokers</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec48"></a>Macros</h4></div></div></div><p>As of <a id="id675" class="indexterm"></a>version 0.9, Pig Latin's preprocessor supports macro expansion. Macros are defined using the <code class="literal">DEFINE</code> statement:</p><div class="informalexample"><pre class="programlisting">DEFINE macro_name(param1, ..., paramN) RETURNS output_bag { 
  pig_latin_statements 
};</pre></div><p>The macro <a id="id676" class="indexterm"></a>is expanded inline, and its parameters are referenced in the Pig Latin block within <code class="literal">{ }</code>.</p><p>The macro output relation is given in the <code class="literal">RETURNS</code> statements (<code class="literal">output_bag</code>). <code class="literal">RETURNS void</code> is used for a macro with no output relation.</p><p>We can define a macro to count the number of rows in a relation, as follows:</p><div class="informalexample"><pre class="programlisting">DEFINE count_rows(X) RETURNS cnt { 
  grpd = group $X all; 
  $cnt = foreach grpd generate COUNT($X); 
};</pre></div><p>We can use it in a Pig script or Grunt session to count the number of tweets:</p><div class="informalexample"><pre class="programlisting">tweets_count = count_rows(tweets);
DUMP tweets_count;</pre></div><p>Macros <a id="id677" class="indexterm"></a>allow us to make scripts modular by housing code in separate files and importing them where needed. For example, we can save <code class="literal">count_rows</code> in a file called <code class="literal">count_rows.macro</code> and later on import it with the command <code class="literal">import 'count_rows.macro'</code>.</p><p>Macros have a number of limitations; in particular, only Pig Latin statements are allowed inside a macro. It is not possible to use <code class="literal">REGISTER</code> statements and shell commands, UDFs are not allowed, and parameter substitution inside the macro is not supported.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec115"></a>Working with data</h3></div></div></div><p>Pig Latin provides <a id="id678" class="indexterm"></a>a number of relational operators to combine functions and apply transformations on data. Typical operations in a data pipeline<a id="id679" class="indexterm"></a> consist of filtering relations (<code class="literal">FILTER</code>), aggregating inputs based on keys (<code class="literal">GROUP</code>), generating transformations based on columns of data (<code class="literal">FOREACH</code>), and joining relations (<code class="literal">JOIN</code>) based on shared keys.</p><p>In the following sections, we will illustrate such operators on a dataset of tweets generated by loading JSON data.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec49"></a>Filtering</h4></div></div></div><p>The<a id="id680" class="indexterm"></a> <code class="literal">FILTER</code> operator<a id="id681" class="indexterm"></a> selects tuples from a relation based on an expression, as follows:</p><div class="informalexample"><pre class="programlisting">relation = FILTER relation BY expression;</pre></div><p>We can use this operator to filter tweets whose text matches the hashtag regular expression, as follows:</p><div class="informalexample"><pre class="programlisting">tweets_with_tag = FILTER tweets BY 
    (text 
       MATCHES '(?:\\s|\\A|^)[##]+([A-Za-z0-9-_]+)'
);</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec50"></a>Aggregation</h4></div></div></div><p>The <a id="id682" class="indexterm"></a>
<code class="literal">GROUP</code> operator <a id="id683" class="indexterm"></a>groups together data in one or more relations based on an expression or a key, as follows:</p><div class="informalexample"><pre class="programlisting">relation = GROUP relation BY expression;</pre></div><p>We can group tweets by the <code class="literal">source</code> field into a new relation <code class="literal">grpd</code>, as follows:</p><div class="informalexample"><pre class="programlisting">grpd = GROUP tweets BY source;</pre></div><p>It is possible to group on multiple dimensions by specifying a tuple as the key, as follows:</p><div class="informalexample"><pre class="programlisting">grpd = GROUP tweets BY (created_at, source);</pre></div><p>The result <a id="id684" class="indexterm"></a>of a <code class="literal">GROUP</code> operation is a relation that includes one tuple per unique value of the group expression. This tuple contains two fields. The first field is named <code class="literal">group</code> and <a id="id685" class="indexterm"></a>is of the same type as the group key. The second field takes the name of the original relation and is of the type bag. The names of both fields are generated by the system.</p><p>Using the <code class="literal">ALL</code> keyword, Pig will aggregate across the whole relation. The <code class="literal">GROUP tweets ALL</code> scheme will aggregate all tuples in the same group.</p><p>As previously mentioned, Pig allows explicit handling of the concurrency level of the <code class="literal">GROUP</code> operator using the<a id="id686" class="indexterm"></a> <code class="literal">PARALLEL</code> operator:</p><div class="informalexample"><pre class="programlisting">grpd = GROUP tweets BY (created_at, id) PARALLEL 10;</pre></div><p>In the preceding example, the MapReduce job generated by the compiler will run 10 concurrent reduce tasks. Pig has a heuristic estimate of how many reducers to use. Another way of globally enforcing the number of reduce tasks is to use the <code class="literal">set default_parallel &lt;n&gt;</code> command.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec51"></a>Foreach</h4></div></div></div><p>The <a id="id687" class="indexterm"></a>
<code class="literal">FOREACH</code> operator<a id="id688" class="indexterm"></a> applies functions on columns, as follows:</p><div class="informalexample"><pre class="programlisting">relation = FOREACH relation GENERATE transformation;</pre></div><p>The output of <code class="literal">FOREACH</code> depends on the transformation applied.</p><p>We can use the operator to project the text of all tweets that contain a hashtag, as follows:</p><div class="informalexample"><pre class="programlisting"> t = FOREACH tweets_with_tag GENERATE text;</pre></div><p>We can also apply a function to the projected columns. For instance, we can use the <code class="literal">REGEX_TOKENIZE</code> function to split each tweet into words, as follows:</p><div class="informalexample"><pre class="programlisting">t = FOREACH tweets_with_tag GENERATE FLATTEN(TOKENIZE(text)) as word;</pre></div><p>The <code class="literal">FLATTEN</code> modifier further un-nests the bag generated by <code class="literal">TOKENIZE</code> into a tuple of words.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec52"></a>Join</h4></div></div></div><p>The <a id="id689" class="indexterm"></a>
<code class="literal">JOIN</code> operator<a id="id690" class="indexterm"></a> performs an inner join of two or more relations based on common field values. Its syntax is as follows:</p><div class="informalexample"><pre class="programlisting">relation = JOIN relation1 BY expression1, relation2 BY expression2;</pre></div><p>We can <a id="id691" class="indexterm"></a>use a join operation to detect tweets that contain positive words, as follows:</p><div class="informalexample"><pre class="programlisting">positive = LOAD 'positive-words.txt' USING PigStorage() as (w:chararray);</pre></div><p>Filter out the comments, as follows:</p><div class="informalexample"><pre class="programlisting">positive_words = FILTER positive BY NOT w MATCHES '^;.*';</pre></div><p>
<code class="literal">positive_words</code>
<a id="id692" class="indexterm"></a> is <a id="id693" class="indexterm"></a>a bag of tuples, each containing a word. We then tokenize the tweets' text and create a new bag of (id_str, word) tuples as follows:</p><div class="informalexample"><pre class="programlisting">id_words = FOREACH tweets {
   GENERATE 
      id_str, 
      FLATTEN(TOKENIZE(text)) as word;
}</pre></div><p>We join the two relations on the <code class="literal">word</code> field and obtain a relation of all tweets that contain one or more positive words, as follows:</p><div class="informalexample"><pre class="programlisting">positive_tweets = JOIN positive_words BY w, id_words BY word;</pre></div><p>In this statement, we join <code class="literal">positive_words</code> and <code class="literal">id_words</code> on the condition that <code class="literal">id_words.word</code> is a positive word. The <code class="literal">positive_tweets</code> operator is a bag in the form of <code class="literal">{w:chararray,id_str:chararray, word:chararray}</code> that contains all elements of <code class="literal">positive_words</code> and <code class="literal">id_words</code> that match the join condition.</p><p>We can combine the <code class="literal">GROUP</code> and <code class="literal">FOREACH</code> operator to calculate the number of positive words per tweet (with at least one positive word). First, we group the relation of positive tweets by the tweet ID, and then we count the number of occurrences of each ID in the relation, as follows:</p><div class="informalexample"><pre class="programlisting">grpd = GROUP positive_tweets BY id_str;
score = FOREACH grpd GENERATE FLATTEN(group), COUNT(positive_tweets);</pre></div><p>The <code class="literal">JOIN</code> operator can make use of the parallelize feature as well, as follows:</p><div class="informalexample"><pre class="programlisting">positive_tweets = JOIN positive_words BY w, id_words BY word PARALLEL 10</pre></div><p>The preceding command will execute the join with 10 reducer tasks.</p><p>It is possible to specify the operator's behavior with the <code class="literal">USING</code> keyword followed by the ID of a<a id="id694" class="indexterm"></a> specialized join. More details can be found at <a class="ulink" href="http://pig.apache.org/docs/r0.12.0/perf.html#specialized-joins" target="_blank">http://pig.apache.org/docs/r0.12.0/perf.html#specialized-joins</a>.</p></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec48"></a>Extending Pig (UDFs)</h2></div></div><hr /></div><p>Functions<a id="id695" class="indexterm"></a> can be a part of almost every operator in Pig. There are two main differences between UDFs and built-in functions. First, UDFs need to be registered using the <code class="literal">REGISTER</code> keyword in order to make them available to Pig. Secondly, they need to be qualified when used. Pig UDFs can currently be implemented in Java, Python, Ruby, JavaScript, and Groovy. The most extensive support is provided for Java functions, which allow you to customize all parts of the process including data load/store, transformation, and aggregation. Additionally, Java functions are also more efficient because they are implemented in the same language as Pig and because additional interfaces are supported, such as the Algebraic and Accumulator interfaces. On the other hand, Ruby and Python APIs allow more rapid prototyping.</p><p>The integration of UDFs with the Pig environment is mainly managed by the following two statements <code class="literal">REGISTER</code> and <code class="literal">DEFINE</code>:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">REGISTER</code>
<a id="id696" class="indexterm"></a> registers a JAR file so that the UDFs in the file can be used, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>REGISTER 'piggybank.jar'</strong></span>
</pre></div></li><li style="list-style-type: disc"><p>
<code class="literal">DEFINE</code>
<a id="id697" class="indexterm"></a> creates an alias to a function or a streaming command, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>DEFINE MyFunction my.package.uri.MyFunction</strong></span>
</pre></div></li></ul></div><p>The version 0.12 of Pig introduced the streaming of UDFs as a mechanism for writing functions using languages with no JVM implementation.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec116"></a>Contributed UDFs</h3></div></div></div><p>Pig's code <a id="id698" class="indexterm"></a>base hosts<a id="id699" class="indexterm"></a> a UDF repository called <span class="strong"><strong>Piggybank</strong></span>. Other popular contributed repositories are <span class="strong"><strong>Twitter's Elephant Bird</strong></span> <a id="id700" class="indexterm"></a>(found at <a class="ulink" href="https://github.com/kevinweil/elephant-bird/" target="_blank">https://github.com/kevinweil/elephant-bird/</a>) and <a id="id701" class="indexterm"></a>
<span class="strong"><strong>Apache DataFu</strong></span> (found at <a class="ulink" href="http://datafu.incubator.apache.org/" target="_blank">http://datafu.incubator.apache.org/</a>).</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec53"></a>Piggybank</h4></div></div></div><p>Piggybank<a id="id702" class="indexterm"></a> is a place for Pig users to share their functions. Shared <a id="id703" class="indexterm"></a>code is located in the official Pig Subversion repository found at <a class="ulink" href="http://svn.apache.org/viewvc/pig/trunk/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/" target="_blank">http://svn.apache.org/viewvc/pig/trunk/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/</a>. The API documentation can be found at <a class="ulink" href="http://pig.apache.org/docs/r0.12.0/api/" target="_blank">http://pig.apache.org/docs/r0.12.0/api/</a> under the <span class="strong"><strong>contrib</strong></span> section. Piggybank UDFs can be obtained by checking out and compiling the sources from the Subversion repository or by using the JAR file that ships with binary releases of Pig. In Cloudera CDH, <code class="literal">piggybank.jar</code> is available <a id="id704" class="indexterm"></a>at <code class="literal">/opt/cloudera/parcels/CDH/lib/pig/piggybank.jar</code>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec54"></a>Elephant Bird</h4></div></div></div><p>Elephant Bird is an <a id="id705" class="indexterm"></a>open source<a id="id706" class="indexterm"></a> library of all things Hadoop used in production at Twitter. This library contains a number of serialization tools, custom input and output formats, writables, Pig load/store functions, and more miscellanea.</p><p>Elephant Bird ships with an extremely flexible JSON loader function, which at the time of writing, is the go-to resource for manipulating JSON data in Pig.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec55"></a>Apache DataFu</h4></div></div></div><p>Apache DataFu Pig collects <a id="id707" class="indexterm"></a>a number <a id="id708" class="indexterm"></a>of analytical<a id="id709" class="indexterm"></a> functions developed and contributed by LinkedIn. These include statistical and estimation functions, bag and set operations, sampling, hashing, and link analysis.</p></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec49"></a>Analyzing the Twitter stream</h2></div></div><hr /></div><p>In the <a id="id710" class="indexterm"></a>following examples, we will use the implementation of JsonLoader provided by Elephant Bird to load and manipulate JSON data. We will use Pig to explore tweet metadata and analyze trends in the dataset. Finally, we will model the interaction between users as a graph and use Apache DataFu to analyze this social network.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec117"></a>Prerequisites</h3></div></div></div><p>Download the <code class="literal">elephant-bird-pig</code> (<a class="ulink" href="http://central.maven.org/maven2/com/twitter/elephantbird/elephant-bird-pig/4.5/elephant-bird-pig-4.5.jar" target="_blank">http://central.maven.org/maven2/com/twitter/elephantbird/elephant-bird-pig/4.5/elephant-bird-pig-4.5.jar</a>), <code class="literal">elephant-bird-hadoop-compat</code> (<a class="ulink" href="http://central.maven.org/maven2/com/twitter/elephantbird/elephant-bird-hadoop-compat/4.5/elephant-bird-hadoop-compat-4.5.jar" target="_blank">http://central.maven.org/maven2/com/twitter/elephantbird/elephant-bird-hadoop-compat/4.5/elephant-bird-hadoop-compat-4.5.jar</a>)<code class="literal">,</code> and <code class="literal">elephant-bird-core</code> (<a class="ulink" href="http://central.maven.org/maven2/com/twitter/elephantbird/elephant-bird-core/4.5/elephant-bird-core-4.5.jar" target="_blank">http://central.maven.org/maven2/com/twitter/elephantbird/elephant-bird-core/4.5/elephant-bird-core-4.5.jar</a>) JAR files from the Maven central repository and copy <a id="id711" class="indexterm"></a>them onto HDFS using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hdfs dfs -put target/elephant-bird-pig-4.5.jar hdfs:///jar/</strong></span>
<span class="strong"><strong>$ hdfs dfs â€“put target/elephant-bird-hadoop-compat-4.5.jar hdfs:///jar/</strong></span>
<span class="strong"><strong>$ hdfs dfs â€“put elephant-bird-core-4.5.jar hdfs:///jar/ </strong></span>
</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec118"></a>Dataset exploration</h3></div></div></div><p>Before diving <a id="id712" class="indexterm"></a>deeper into the dataset, we need to register the dependencies to Elephant Bird and DataFu, as follows:</p><div class="informalexample"><pre class="programlisting">REGISTER /opt/cloudera/parcels/CDH/lib/pig/datafu-1.1.0-cdh5.0.0.jar
REGISTER /opt/cloudera/parcels/CDH/lib/pig/lib/json-simple-1.1.jar
REGISTER hdfs:///jar/elephant-bird-pig-4.5.jar
REGISTER hdfs:///jar/elephant-bird-hadoop-compat-4.5.jar
REGISTER hdfs:///jar/elephant-bird-core-4.5.jar</pre></div><p>Then, load the JSON dataset of tweets using <code class="literal">com.twitter.elephantbird.pig.load.JsonLoader</code>, as follows:</p><div class="informalexample"><pre class="programlisting">tweets = LOAD 'tweets.json' using  com.twitter.elephantbird.pig.load.JsonLoader('-nestedLoad');</pre></div><p>
<code class="literal">com.twitter.elephantbird.pig.load.JsonLoader</code> decodes each line of the input file to JSON and passes the resulting map of values to Pig as a single-element tuple. This enables access to elements of the JSON object without having to specify a schema upfront. The <code class="literal">â€“nestedLoad</code> argument instructs the class to load nested data structures.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec119"></a>Tweet metadata</h3></div></div></div><p>In the remainder<a id="id713" class="indexterm"></a> of the chapter, we will use metadata from the JSON dataset to model the tweet stream. One example of metadata attached to a tweet is the <code class="literal">Place</code> object, which contains geographical information about the user's location. <code class="literal">Place</code> contains fields that describe its name, ID, country, country code, and more. A full description can be found at <a class="ulink" href="https://dev.twitter.com/docs/platform-objects/places" target="_blank">https://dev.twitter.com/docs/platform-objects/places</a>.</p><div class="informalexample"><pre class="programlisting">place = FOREACH tweets GENERATE (chararray)$0#'place' as place;</pre></div><p>Entities<a id="id714" class="indexterm"></a> give information such as structured data from tweets, URLs, hashtags, and mentions, without having to extract them from text. A description of entities can be found at <a class="ulink" href="https://dev.twitter.com/docs/entities" target="_blank">https://dev.twitter.com/docs/entities</a>. The hashtag entity is an array of tags extracted from a tweet. Each entity has the following two attributes:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Text</strong></span>:<a id="id715" class="indexterm"></a> is the hashtag text</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Indices</strong></span>: <a id="id716" class="indexterm"></a>is the character position from which the hashtag was extracted</p></li></ul></div><p>The following code uses entities:</p><div class="informalexample"><pre class="programlisting">hashtags_bag = FOREACH tweets {
    GENERATE 
      FLATTEN($0#'entities'#'hashtags') as tag;
}</pre></div><p>We then<a id="id717" class="indexterm"></a> flatten <code class="literal">hashtags_bag</code> to extract each hashtag's text:</p><div class="informalexample"><pre class="programlisting">hashtags = FOREACH hashtags_bag GENERATE tag#'text' as topic;</pre></div><p>Entities for user objects contain information that appears in the user profile and description fields. We can extract the tweet author's ID via the <code class="literal">user</code> field in the tweet map:</p><div class="informalexample"><pre class="programlisting">users = FOREACH tweets GENERATE $0#'user'#'id' as id;</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec120"></a>Data preparation</h3></div></div></div><p>The <code class="literal">SAMPLE</code> built-in operator selects a <a id="id718" class="indexterm"></a>set of <span class="emphasis"><em>n</em></span> tuples with probability <span class="emphasis"><em>p</em></span> out of the dataset, as follows:</p><div class="informalexample"><pre class="programlisting">sampled = SAMPLE tweets 0.01;</pre></div><p>The preceding command will select approximately 1 percent of the dataset. Given that <code class="literal">SAMPLE</code> is probabilistic (<a class="ulink" href="http://en.wikipedia.org/wiki/Bernoulli_sampling" target="_blank">http://en.wikipedia.org/wiki/Bernoulli_sampling</a>), there is no guarantee that the sample size will be exact. Moreover the function samples with replacement, which means that each item might appear more than once.</p><p>Apache DataFu implements a number of sampling methods for cases where having an exact sample size and no replacement is desired (<code class="literal">SimpleRandomSampling</code>), sampling with replacement (<code class="literal">SimpleRandomSampleWithReplacementVote</code> and <code class="literal">SimpleRandomSampleWithReplacementElect</code>), when we want to account for sample bias (<code class="literal">WeightedRandomSampling</code>), or to sample across multiple relations (<code class="literal">SampleByKey</code>).</p><p>We can create a sample of exactly 1 percent of the dataset, with each item having the same probability of being selected, using <code class="literal">SimpleRandomSample</code>.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note17"></a>Note</h3><p>The actual guarantee is a sample of size <span class="emphasis"><em>ceil (p*n)</em></span> with a probability of at least 99 percent.</p></div><p>First, we pass a sampling probability 0.01 to the UDF constructor:</p><div class="informalexample"><pre class="programlisting">DEFINE SRS datafu.pig.sampling.SimpleRandomSample('0.01');</pre></div><p>and the bag, created with <code class="literal">(GROUP tweets ALL),</code> to be sampled:</p><div class="informalexample"><pre class="programlisting">sampled = FOREACH (GROUP tweets ALL) GENERATE FLATTEN(SRS(tweets));</pre></div><p>The <code class="literal">SimpleRandomSample</code> UDF <a id="id719" class="indexterm"></a>selects without replacement, which means that each item will appear only once.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note18"></a>Note</h3><p>Which sampling method to use depends both on the data we are working with, assumptions on how items are distributed, the size of the dataset, and what we practically want to achieve. In general, when we want to explore a dataset to formulate hypotheses, <code class="literal">SimpleRandomSample</code> can be a good choice. However, in several analytics applications, it is common to use methods that assume replacement (for example, bootstrapping).</p><p>Note that when working with very large datasets, sampling with replacement and sampling without replacement tend to behave similarly. The probability of an item being selected twice out of a population of billions of items will be low.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec121"></a>Top n statistics</h3></div></div></div><p>One of the <a id="id720" class="indexterm"></a>first questions we might want to ask is how frequent certain things are. For instance, we might want to create a histogram of the top 10 topics by the number of mentions. Similarly, we might want to find the top 50 countries or the top 10 users. Before looking at tweets data, we will define a macro so that we can apply the same selection logic to different collections of items:</p><div class="informalexample"><pre class="programlisting">DEFINE top_n(rel, col, n) 
  RETURNS top_n_items {
    grpd = GROUP $rel BY $col;
    cnt_items = FOREACH grpd 
        GENERATE FLATTEN(group), COUNT($rel) AS cnt;
    cnt_items_sorted = ORDER cnt_items BY cnt DESC;
    $top_n_items = LIMIT cnt_items_sorted $n;
  }</pre></div><p>The<code class="literal"> top_n</code> method takes a relation <code class="literal">rel</code>, the column <code class="literal">col</code> we want to count, and the number of items to return <code class="literal">n</code> as parameters. In the Pig Latin block, we first group <code class="literal">rel</code> by items in <code class="literal">col</code>, count the number of occurrences of each item, sort them, and select the most frequent <code class="literal">n</code>.</p><p>To find the top 10 English hashtags, we filter them by language, and extract their text:</p><div class="informalexample"><pre class="programlisting">tweets_en = FILTER tweets by $0#'lang' == 'en';
hashtags_bag = FOREACH tweets { 
    GENERATE
        FLATTEN($0#'entities'#'hashtags') AS tag;
}
hashtags = FOREACH hashtags_bag GENERATE tag#'text' AS tag;</pre></div><p>And apply the <code class="literal">top_n</code> macro:</p><div class="informalexample"><pre class="programlisting">top_10_hashtags = top_n(hashtags, tag, 10);</pre></div><p>In order to better characterize what is trending and make this information more relevant to users, we can drill down into the dataset and look at hashtags per geographic location. </p><p>First, we generate bag of (<code class="literal">place</code>, <code class="literal">hashtag</code>) tuples, as follows:</p><div class="informalexample"><pre class="programlisting">hashtags_country_bag = FOREACH tweets generate {
    0#'place' as place, 
    FLATTEN($0#'entities'#'hashtags') as tag;
}</pre></div><p>And then, we <a id="id721" class="indexterm"></a>extract the country code and hashtag text, as follows:</p><div class="informalexample"><pre class="programlisting">hashtags_country = FOREACH hashtags_country_bag {
  GENERATE 
    place#'country_code' as co, 
    tag#'text' as tag;
}</pre></div><p>Then, we count how many times each country code and hashtag appear together, as follows:</p><div class="informalexample"><pre class="programlisting">hashtags_country_frequency = FOREACH (GROUP hashtags_country ALL) {
  GENERATE 
    FLATTEN(group), 
    COUNT(hashtags_country) as count;
}</pre></div><p>Finally, we count the top 10 countries per hashtag with the <code class="literal">TOP</code> function, as follows:</p><div class="informalexample"><pre class="programlisting">hashtags_country_regrouped= GROUP hashtags_country_frequency BY cnt; 
top_results = FOREACH hashtags_country_regrouped {
    result = TOP(10, 1, hashtags_country_frequency);
    GENERATE FLATTEN(result);
} </pre></div><p>
<code class="literal">TOP</code>'s parameters are the number of tuples to return, the column to compare, and the relation containing said column:</p><div class="informalexample"><pre class="programlisting">top_results = FOREACH D {
  result = TOP(10, 1, C);
  GENERATE FLATTEN(result);
}</pre></div><p>The source code for this example can be found at <a class="ulink" href="https://github.com/learninghadoop2/book-examples/blob/master/ch6/topn.pig" target="_blank">https://github.com/learninghadoop2/book-examples/blob/master/ch6/topn.pig</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec122"></a>Datetime manipulation</h3></div></div></div><p>The <code class="literal">created_at</code> field <a id="id722" class="indexterm"></a>in the JSON tweets gives us time-stamped information about when the tweet was posted. Unfortunately, its format is not compatible with Pig's built-in <code class="literal">datetime</code> type.</p><p>Piggybank comes to the rescue with a number of time manipulation UDFs contained in <code class="literal">org.apache.pig.piggybank.evaluation.datetime.convert</code>. One of them is <code class="literal">CustomFormatToISO</code>, which converts an arbitrarily formatted timestamp into an ISO 8601 datetime string.</p><p>In order to access these UDFs, we first need to register the <code class="literal">piggybank.jar</code> file, as follows:</p><div class="informalexample"><pre class="programlisting">REGISTER /opt/cloudera/parcels/CDH/lib/pig/piggybank.jar</pre></div><p>To make our code less verbose, we create an alias for the <code class="literal">CustomFormatToISO</code> class's fully qualified Java name:</p><div class="informalexample"><pre class="programlisting">DEFINE CustomFormatToISO org.apache.pig.piggybank.evaluation.datetime.convert.CustomFormatToISO();</pre></div><p>By knowing how to manipulate timestamps, we can calculate statistics at different time intervals. For instance, we can look at how many tweets are created per hour. Pig has a built-in <code class="literal">GetHour</code> function that extracts the hour out of a <code class="literal">datetime</code> type. To use this, we first convert the timestamp string to ISO 8601 with <code class="literal">CustomFormatToISO</code> and then the resulting <code class="literal">chararray</code> to <code class="literal">datetime</code> using the built-in <code class="literal">ToDate</code> function, as follows:</p><div class="informalexample"><pre class="programlisting">hourly_tweets = FOREACH tweets {
  GENERATE 
    GetHour(
      ToDate(
      CustomFormatToISO(
$0#'created_at', 'EEE MMMM d HH:mm:ss Z y')
      )
    ) as hour;
}</pre></div><p>Now, it is just a matter of grouping <code class="literal">hourly_tweets</code> by hour and then generating a count of tweets per group, as follows:</p><div class="informalexample"><pre class="programlisting">hourly_tweets_count =  FOREACH (GROUP hourly_tweets BY hour) { 
  GENERATE FLATTEN(group), COUNT(hourly_tweets);
}</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec56"></a>Sessions</h4></div></div></div><p>DataFu's <code class="literal">Sessionize</code> class <a id="id723" class="indexterm"></a>can help us to better capture user activity over time. A session represents the activity of a user within a given period of time. For instance, we can look at each user's tweet stream at intervals of 15 minutes and measure these sessions to determine both network volumes as well as user activity:</p><div class="informalexample"><pre class="programlisting">DEFINE Sessionize datafu.pig.sessions.Sessionize('15m');
users_activity = FOREACH tweets {
      GENERATE 
        CustomFormatToISO($0#'created_at', 
                      'EEE MMMM d HH:mm:ss Z y') AS dt,
        (chararray)$0#'user'#'id' as user_id;
}
users_activity_sessionized = FOREACH 
    (GROUP users_activity BY user_id) {
    ordered = ORDER users_activity BY dt;
    GENERATE FLATTEN(Sessionize(ordered)) 
                    AS (dt, user_id, session_id);
}</pre></div><p>
<code class="literal">user_activity</code> simply records the time <code class="literal">dt</code> a given <code class="literal">user_id</code> posted a status update.</p><p>
<code class="literal">Sessionize</code> takes the session timeout and a bag as input. The first element of the input bag is an ISO 8601 timestamp, and the bag must be sorted by this timestamp. Events that are within 15 minutes from each other will belong to the same session.</p><p>It returns the input bag with a new field, <code class="literal">session_id</code>, that uniquely identifies a session. With this data, we can calculate the session's length and some other statistics. More examples of <code class="literal">Sessionize</code> usage can be found at <a class="ulink" href="http://datafu.incubator.apache.org/docs/datafu/guide/sessions.html" target="_blank">http://datafu.incubator.apache.org/docs/datafu/guide/sessions.html</a>.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec123"></a>Capturing user interactions</h3></div></div></div><p>In the<a id="id724" class="indexterm"></a> remainder of the chapter, we will look at how to capture patterns from user interactions. As a first step in this direction, we will create a dataset suitable to model a social network. This dataset will contain a timestamp, the ID of the tweet, the user who posted the tweet, the user and tweet she's replying to, and the hashtag in the tweet.</p><p>Twitter considers as a reply (<code class="literal">in_reply_to_status_id_str</code>) any message beginning with the <code class="literal">@</code> character. Such tweets are interpreted as a direct message to that person. Placing an <code class="literal">@</code> character anywhere else in the tweet is interpreted as a mention (<code class="literal">'entities'#'user_mentions</code>') and not a reply. The difference is that mentions are immediately broadcast to a person's followers, whereas replies are not. Replies are, however, considered as mentions.</p><p>When working with personally identifiable information, it is a good idea to anonymize if not remove entirely sensitive data such as IP addresses, names, and user IDs. A commonly used technique involves a <code class="literal">hash</code> function that takes as input the data we want to anonymize, concatenated<a id="id725" class="indexterm"></a> with additional random data called salt. The following code shows an example of such anonymization:</p><div class="informalexample"><pre class="programlisting">DEFINE SHA datafu.pig.hash.SHA();
from_to_bag = FOREACH tweets {
  dt = $0#'created_at';
  user_id = (chararray)$0#'user'#'id';
  tweet_id = (chararray)$0#'id_str';
  reply_to_tweet = (chararray)$0#'in_reply_to_status_id_str';
  reply_to = (chararray)$0#'in_reply_to_user_id_str';
  place = $0#'place';
  topics = $0#'entities'#'hashtags';

  GENERATE
    CustomFormatToISO(dt, 'EEE MMMM d HH:mm:ss Z y') AS dt,
    SHA((chararray)CONCAT('SALT', user_id)) AS source,  
    SHA(((chararray)CONCAT('SALT', tweet_id))) AS tweet_id,
    ((reply_to_tweet IS NULL) 
         ? NULL 
         : SHA((chararray)CONCAT('SALT', reply_to_tweet))) 
               AS  reply_to_tweet_id,
    ((reply_to IS NULL) 
         ? NULL 
         : SHA((chararray)CONCAT('SALT', reply_to))) 
                AS destination,
    (chararray)place#'country_code' as country,
    FLATTEN(topics) AS topic;
}

-- extract the hashtag text
from_to = FOREACH from_to_bag { 
  GENERATE 
    dt, 
    tweet_id, 
    reply_to_tweet_id, 
    source, 
    destination, 
    country,
    (chararray)topic#'text' AS topic;
}</pre></div><p>In this example, we use <code class="literal">CONCAT</code> to append a (not so random) salt string to personal data. We then generate a hash of the salted IDs with DataFu's <code class="literal">SHA</code> function. The <code class="literal">SHA</code> function requires its input parameters to be non null. We enforce this condition using <code class="literal">if-then-else</code> statements. In Pig Latin, this is expressed as <code class="literal">&lt;condition is true&gt; ? &lt;true branch&gt; : &lt;false branch&gt;</code> . If the string is null, we return <code class="literal">NULL</code>, and if not, we return the salted hash. To<a id="id726" class="indexterm"></a> make code more readable, we use aliases for the tweet JSON fields and reference them in the <code class="literal">GENERATE</code> block.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec124"></a>Link analysis</h3></div></div></div><p>We can <a id="id727" class="indexterm"></a>redefine our approach to determine trending topics to include users' reactions. A first, naÃ¯ve, approach could be to consider a topic as important if it caused a number of replies larger than a threshold value.</p><p>A problem with this approach is that tweets generate relatively few replies, so the volume of the resulting dataset will be low. Hence, it requires a very large amount of data to contain tweets being replied to and produce any result. In practice, we would likely want to combine this metric with other ones (for example, mentions) in order to perform more meaningful analyses.</p><p>To satisfy this query, we will create a new dataset that includes the hashtags extracted from both the tweet and the one a user is replying to:</p><div class="informalexample"><pre class="programlisting">tweet_hashtag = FOREACH from_to GENERATE tweet_id, topic;
from_to_self_joined = JOIN from_to BY reply_to_tweet_id LEFT, 
tweet_hashtag BY tweet_id;

twitter_graph = FOREACH from_to_self_joined  { 
    GENERATE
        from_to::dt AS dt,
        from_to::tweet_id AS tweet_id,
        from_to::reply_to_tweet_id AS reply_to_tweet_id,
        from_to::source AS source,
        from_to::destination AS destination,
        from_to::topic AS topic,
        from_to::country AS country,
        tweet_hashtag::topic AS topic_replied;
}</pre></div><p>Note that Pig does not allow a cross join on the same relation, hence we have to create <code class="literal">tweet_hashtag</code> for the right-hand side of the join. Here, we use the <code class="literal">::</code> operator to disambiguate from which relation and column we want to select records.</p><p>Once again, we can look for the top 10 topics by number of replies using the <code class="literal">top_n</code> macro:</p><div class="informalexample"><pre class="programlisting">top_10_topics = top_n(twitter_graph, topic_replied, 10);</pre></div><p>Counting things will only take us so far. We can compute more descriptive statistics on this dataset with DataFu. Using the <code class="literal">Quantile</code> function, we can calculate the median, the 90th, 95th, and the 99th percentiles of the number of hashtag reactions, as follows:</p><div class="informalexample"><pre class="programlisting">DEFINE Quantile datafu.pig.stats.Quantile('0.5','0.90','0.95','0.99');</pre></div><p>Since the UDF <a id="id728" class="indexterm"></a>expects an ordered bag of integer values as input, we first count the frequency of each <code class="literal">topic_replied</code> entry, as follows. </p><div class="informalexample"><pre class="programlisting">topics_with_replies_grpd = GROUP twitter_graph BY topic_replied;
topics_with_replies_cnt = FOREACH topics_with_replies_grpd {
  GENERATE
COUNT(twitter_graph) as cnt;
}</pre></div><p>Then, we apply <code class="literal">Quantile</code> on the bag of frequencies, as follows:</p><div class="informalexample"><pre class="programlisting">quantiles = FOREACH (GROUP topics_with_replies_cnt ALL) {
    sorted = ORDER topics_with_replies_cnt BY cnt;
    GENERATE Quantile(sorted);
}</pre></div><p>The source code for this example can be found at <a class="ulink" href="https://github.com/learninghadoop2/book-examples/blob/master/ch6/graph.pig" target="_blank">https://github.com/learninghadoop2/book-examples/blob/master/ch6/graph.pig</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec125"></a>Influential users</h3></div></div></div><p>We will <a id="id729" class="indexterm"></a>use PageRank, an algorithm developed by Google to rank web pages (<a class="ulink" href="http://ilpubs.stanford.edu:8090/422/1/1999-66.pdf" target="_blank">http://ilpubs.stanford.edu:8090/422/1/1999-66.pdf</a>), to identify influential users in the Twitter graph we generated in the previous section.</p><p>This type of analysis has a number of use cases, such as targeted and contextual advertisement, recommendation systems, spam detection, and obviously measuring the importance of web pages. A similar approach, used by Twitter to implement the Who to Follow feature, is described in the research paper <span class="emphasis"><em>WTF: The</em></span><a id="id730" class="indexterm"></a>
<span class="emphasis"><em> Who to Follow service at Twitter</em></span> found at <a class="ulink" href="http://stanford.edu/~rezab/papers/wtf_overview.pdf" target="_blank">http://stanford.edu/~rezab/papers/wtf_overview.pdf</a>.</p><p>Informally, PageRank determines the importance of a page based on the importance of other pages linking to it and assigns it a score between 0 and 1. A high PageRank score indicates that a lot of pages point to it. Intuitively, being linked by pages with a high PageRank is a quality endorsement. In terms of the Twitter graph, we assume that users receiving a lot of replies are important or influential within the social network. In Twitter's case, we consider an extended definition of PageRank, where the link between two users is given by a direct reply and labeled by any eventual hashtag present in the message. Heuristically, we want to identify influential users on a given topic.</p><p>In DataFu's implementation, each graph is represented as a bag of <code class="literal">(source, edges)</code> tuples. The <code class="literal">source</code> tuple is an integer ID representing the source node. The edges are a bag of <code class="literal">(destination, weight)</code> tuples. <code class="literal">destination</code> is an integer ID representing the destination node. <code class="literal">weight</code> is a double representing how much the edge should be weighted. The output of the UDF is a bag of <code class="literal">(source, rank)</code> pairs, where <code class="literal">rank</code> is the PageRank value for the source user in the graph. Notice that we talked about nodes, edges, and graphs as abstract concepts. In Google's case, nodes are web pages, edges are links from one page to the other, and graphs are groups of pages connected directly and indirectly.</p><p>In our case, nodes <a id="id731" class="indexterm"></a>represent users, edges represent <code class="literal">in_reply_to_user_id_str</code> mentions, and edges are labeled by hashtags in tweets. The output of PageRank should suggest which users are influential on a given topic given their interaction patterns.</p><p>In this section, we will write a pipeline to:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Represent data as a graph where each node is a user and a hashtag labels the edge</p></li><li style="list-style-type: disc"><p>Map IDs and hashtags to integers so that they can be consumed by PageRank</p></li><li style="list-style-type: disc"><p>Apply PageRank</p></li><li style="list-style-type: disc"><p>Store the results into HDFS in an interoperable format (Avro)</p></li></ul></div><p>We represent the graph as a bag of tuples in the form <code class="literal">(source, destination, topic)</code>, where each tuple represents the interaction between nodes. The source code for this example can be found at <a class="ulink" href="https://github.com/learninghadoop2/book-examples/blob/master/ch6/pagerank.pig" target="_blank">https://github.com/learninghadoop2/book-examples/blob/master/ch6/pagerank.pig</a>.</p><p>We will map users' and hashtags' text to numerical IDs. We use the Java String <code class="literal">hashCode()</code> method to perform this conversion step and wrap the logic in an <code class="literal">Eval</code> UDF.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note19"></a>Note</h3><p>The size of an integer is effectively the upper bound for the number of nodes and edges in the graph. For production code, it is recommended that you use a more robust hash function.</p></div><p>The <code class="literal">StringToInt</code> class takes a string as input, calls the <code class="literal">hashCode()</code> method, and returns the method output to Pig. The UDF code can be found at <a class="ulink" href="https://github.com/learninghadoop2/book-examples/blob/master/ch6/udf/com/learninghadoop2/pig/udf/StringToInt.java" target="_blank">https://github.com/learninghadoop2/book-examples/blob/master/ch6/udf/com/learninghadoop2/pig/udf/StringToInt.java</a>.</p><div class="informalexample"><pre class="programlisting">package com.learninghadoop2.pig.udf;
import java.io.IOException;
import org.apache.pig.EvalFunc;
import org.apache.pig.data.Tuple;

public class StringToInt extends EvalFunc&lt;Integer&gt; {
    public Integer exec(Tuple input) throws IOException {
        if (input == null || input.size() == 0)
            return null;
        try {
            String str = (String) input.get(0);
            return str.hashCode();
        } catch(Exception e) {
          throw 
             new IOException("Cannot convert String to Int", e);
        }
    }
}</pre></div><p>We extend <code class="literal">org.apache.pig.EvalFunc</code> and override the <code class="literal">exec</code> method to return <code class="literal">str.hashCode()</code> on the function input. The <code class="literal">EvalFunc&lt;Integer&gt;</code> class is parameterized with the return type of the UDF (<code class="literal">Integer</code>).</p><p>Next, we compile <a id="id732" class="indexterm"></a>the class and archive it into a JAR, as follows:</p><div class="informalexample"><pre class="programlisting">$ javac -classpath /opt/cloudera/parcels/CDH/lib/pig/pig.jar:$(hadoop classpath) com/learninghadoop2/pig/udf/StringToInt.java
$ jar cvf myudfs-pig.jar com/learninghadoop2/pig/udf/StringToInt.class</pre></div><p>We can now register the UDF in Pig and create an alias to <code class="literal">StringToInt</code>, as follows:</p><div class="informalexample"><pre class="programlisting">REGISTER myudfs-pig.jar
DEFINE StringToInt com.learninghadoop2.pig.udf.StringToInt();</pre></div><p>We filter out tweets with no <code class="literal">destination</code> and no <code class="literal">topic</code>, as follows:</p><div class="informalexample"><pre class="programlisting">tweets_graph_filtered = FILTER twitter_graph by 
(destination IS NOT NULL) AND 
(topic IS NOT null);</pre></div><p>Then, we convert the <code class="literal">source</code>, <code class="literal">destination</code>, and <code class="literal">topic</code> to integer IDs:</p><div class="informalexample"><pre class="programlisting">from_to = foreach tweets_graph_filtered {
  GENERATE 
    StringToInt(source) as source_id, 
    StringToInt(destination) as destination_id, 
    StringToInt(topic) as topic_id;
}</pre></div><p>Once data is in the appropriate format, we can reuse the implementation of PageRank and the example code (found at <a class="ulink" href="https://github.com/apache/incubator-datafu/blob/master/datafu-pig/src/main/java/datafu/pig/linkanalysis/PageRank.java" target="_blank">https://github.com/apache/incubator-datafu/blob/master/datafu-pig/src/main/java/datafu/pig/linkanalysis/PageRank.java</a>) provided by DataFu, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">DEFINE PageRank datafu.pig.linkanalysis.PageRank('dangling_nodes','true');</pre></div><p>We begin by creating a bag of <code class="literal">(source_id, destination_id, topic_id)</code> tuples, as follows:</p><div class="informalexample"><pre class="programlisting">reply_to = group from_to by (source_id, destination_id, topic_id); </pre></div><p>We count the occurrences of each tuple, that is, how many times two people talked about a topic, as follows:</p><div class="informalexample"><pre class="programlisting">topic_edges = foreach reply_to {
  GENERATE flatten(group), ((double)COUNT(from_to.topic_id)) as w;
}</pre></div><p>Remember that <a id="id733" class="indexterm"></a>topic is the edge of our graph; we begin by creating an association between the source node and the topic edge, as follows:</p><div class="informalexample"><pre class="programlisting">topic_edges_grouped = GROUP topic_edges by (topic_id, source_id);</pre></div><p>Then we regroup it with the purpose of adding a destination node and the edge weight, as follows:</p><div class="informalexample"><pre class="programlisting">topic_edges_grouped = FOREACH topic_edges_grouped {
  GENERATE
    group.topic_id as topic,
    group.source_id as source,
    topic_edges.(destination_id,w) as edges;
}</pre></div><p>Once we create the Twitter graph, we calculate the PageRank of all users (<code class="literal">source_id</code>):</p><div class="informalexample"><pre class="programlisting">topic_rank = FOREACH (GROUP topic_edges_grouped BY topic) {
  GENERATE
    group as topic,
    FLATTEN(PageRank(topic_edges_grouped.(source,edges))) as (source,rank);
}
topic_rank = FOREACH topic_rank GENERATE topic, source, rank;</pre></div><p>We store the result in HDFS in Avro format. If Avro dependencies are not present in the classpath, we need to add the Avro MapReduce jar file to our environment before accessing individual fields. Within Pig, for example, on the Cloudera CDH5 VM:</p><div class="informalexample"><pre class="programlisting">REGISTER /opt/cloudera/parcels/CDH/lib/avro/avro.jar
REGISTER /opt/cloudera/parcels/CDH/lib/avro/avro-mapred-hadoop2.jar 
STORE topic_rank INTO 'replies-pagerank' using AvroStorage();    </pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note20"></a>Note</h3><p>In these last <a id="id734" class="indexterm"></a>two sections, we made a number of implicit assumptions on what a Twitter graph might look like and what the concepts of topic and user interaction mean. Given the constraints that we posed, the resulting social network we analyzed will be relatively small and not necessarily representative of the entire Twitter social network. Extrapolating results from this dataset is discouraged. In practice, there are many other factors that should be taken into account to generate a robust model of social interaction.</p></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec50"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we introduced Apache Pig, a platform for large-scale data analysis on Hadoop. In particular, we covered the following topics:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The goals of Pig as a way of providing a dataflow-like abstraction that does not require hands-on MapReduce development</p></li><li style="list-style-type: disc"><p>How Pig's approach to processing data compares to SQL, where Pig is procedural while SQL is declarative</p></li><li style="list-style-type: disc"><p>Getting started with Pig â€” an easy task, as it is a library that generates custom code and doesn't require additional services</p></li><li style="list-style-type: disc"><p>An overview of the data types, core functions, and extension mechanisms provided by Pig</p></li><li style="list-style-type: disc"><p>Examples of applying Pig to analyze the Twitter dataset in detail, which demonstrated its ability to express complex concepts in a very concise fashion</p></li><li style="list-style-type: disc"><p>How libraries such as Piggybank, Elephant Bird, and DataFu provide repositories for numerous useful prewritten Pig functions</p></li><li style="list-style-type: disc"><p>In the next chapter, we will revisit the SQL comparison by exploring tools that expose a SQL-like abstraction over data stored in HDFS</p></li></ul></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch07"></a>ChapterÂ 7.Â Hadoop and SQL</h2></div></div></div><p>MapReduce<a id="id735" class="indexterm"></a> is a powerful paradigm that enables complex data processing that can reveal valuable insights. As discussed in earlier chapters however, it does require a different mindset and some training and experience on the model of breaking processing analytics into a series of map and reduce steps. There are several products that are built atop Hadoop to provide higher-level or more familiar views of the data held within HDFS, and Pig is a very popular one. This chapter will explore the other most common abstraction implemented atop Hadoop: SQL.</p><p>In this chapter, we will cover the following topics:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>What the use cases for SQL on Hadoop are and why it is so popular</p></li><li style="list-style-type: disc"><p>HiveQL, the SQL dialect introduced by Apache Hive</p></li><li style="list-style-type: disc"><p>Using HiveQL to perform SQL-like analysis of the Twitter dataset</p></li><li style="list-style-type: disc"><p>How HiveQL can approximate common features of relational databases such as joins and views</p></li><li style="list-style-type: disc"><p>How HiveQL allows the incorporation of user-defined functions into its queries</p></li><li style="list-style-type: disc"><p>How SQL on Hadoop complements Pig</p></li><li style="list-style-type: disc"><p>Other SQL-on-Hadoop products such as Impala and how they differ from Hive</p></li></ul></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec51"></a>Why SQL on Hadoop</h2></div></div><hr /></div><p>So far we have seen <a id="id736" class="indexterm"></a>how to write Hadoop programs using the MapReduce APIs and how Pig Latin provides a scripting abstraction and a wrapper for custom business logic by means of UDFs. Pig<a id="id737" class="indexterm"></a> is a very powerful tool, but its dataflow-based programming model is not familiar to most developers or business analysts. The traditional tool of choice for such people to explore data is SQL.</p><p>Back in 2008 Facebook released Hive, the first widely used implementation of SQL on Hadoop.</p><p>Instead of providing a way of more quickly developing map and reduce tasks, Hive offers an implementation of <span class="emphasis"><em>HiveQL</em></span>, a query <a id="id738" class="indexterm"></a>language based on SQL. Hive takes HiveQL statements and immediately and automatically translates the queries into one or more MapReduce jobs. It then executes the overall MapReduce program and returns the results to the user.</p><p>This interface to <a id="id739" class="indexterm"></a>Hadoop not only reduces the time required to produce results from data analysis, it also significantly widens the net as to who can use Hadoop. Instead of requiring software development skills, anyone who's familiar with SQL can use Hive.</p><p>The combination of these attributes is that HiveQL is often used as a tool for business and data analysts to perform ad hoc queries on the data stored on HDFS. With Hive, the data analyst can work on refining queries without the involvement of a software developer. Just as with Pig, Hive also allows HiveQL to be extended by means of User Defined Functions, enabling the base SQL dialect to be customized with business-specific functionality.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec126"></a>Other SQL-on-Hadoop solutions</h3></div></div></div><p>Though Hive was the<a id="id740" class="indexterm"></a> first product to introduce and support HiveQL, it is no longer the only one. Later in this chapter, we will also discuss Impala, released in 2013 and already a very popular tool, particularly for low-latency queries. There are others, but we will mostly discuss Hive and Impala as they have been the most successful.</p><p>While introducing the core features and capabilities of SQL on Hadoop however, we will give examples using Hive; even though Hive and Impala share many SQL features, they also have numerous differences. We don't want to constantly have to caveat each new feature with exactly how it is supported in Hive compared to Impala. We'll generally be looking at aspects of the feature set that are common to both, but if you use both products, it's important to read the latest release notes to understand the differences.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec52"></a>Prerequisites</h2></div></div><hr /></div><p>Before diving into specific technologies, let's<a id="id741" class="indexterm"></a> generate some data that we'll use in the examples throughout this chapter. We'll create a modified version of a former Pig script as the main functionality for this. The script in this chapter assumes that the Elephant Bird JARs used previously are available in the <code class="literal">/jar</code> directory on HDFS. The full source code is at <a class="ulink" href="https://github.com/learninghadoop2/book-examples/blob/master/ch7/extract_for_hive.pig" target="_blank">https://github.com/learninghadoop2/book-examples/blob/master/ch7/extract_for_hive.pig</a>, but the core<a id="id742" class="indexterm"></a> of <code class="literal">extract_for_hive.pig</code> is as <a id="id743" class="indexterm"></a>follows:</p><div class="informalexample"><pre class="programlisting">-- load JSON data
tweets = load '$inputDir' using  com.twitter.elephantbird.pig.load.JsonLoader('-nestedLoad');
-- Tweets
tweets_tsv = foreach tweets {
generate 
    (chararray)CustomFormatToISO($0#'created_at', 
'EEE MMMM d HH:mm:ss Z y') as dt, 
    (chararray)$0#'id_str', 
(chararray)$0#'text' as text, 
    (chararray)$0#'in_reply_to', 
(boolean)$0#'retweeted' as is_retweeted, 
(chararray)$0#'user'#'id_str' as user_id, (chararray)$0#'place'#'id' as place_id;
}
store tweets_tsv into '$outputDir/tweets' 
using PigStorage('\u0001');
-- Places
needed_fields = foreach tweets {
   generate 
(chararray)CustomFormatToISO($0#'created_at', 
'EEE MMMM d HH:mm:ss Z y') as dt, 
     (chararray)$0#'id_str' as id_str, 
$0#'place' as place;
}
place_fields = foreach needed_fields {
generate 
    (chararray)place#'id' as place_id, 
    (chararray)place#'country_code' as co, 
    (chararray)place#'country' as country, 
    (chararray)place#'name' as place_name, 
    (chararray)place#'full_name' as place_full_name, 
    (chararray)place#'place_type' as place_type;
}
filtered_places = filter place_fields by co != '';
unique_places = distinct filtered_places;
store unique_places into '$outputDir/places' 
using PigStorage('\u0001');

-- Users
users = foreach tweets {
   generate 
(chararray)CustomFormatToISO($0#'created_at', 
'EEE MMMM d HH:mm:ss Z y') as dt, 
(chararray)$0#'id_str' as id_str, 
$0#'user' as user;
}
user_fields = foreach users {
   generate 
    (chararray)CustomFormatToISO(user#'created_at', 
'EEE MMMM d HH:mm:ss Z y') as dt,
  (chararray)user#'id_str' as user_id, 
  (chararray)user#'location' as user_location, 
  (chararray)user#'name' as user_name, 
  (chararray)user#'description' as user_description, 
  (int)user#'followers_count' as followers_count, 
  (int)user#'friends_count' as friends_count, 
  (int)user#'favourites_count' as favourites_count, 
  (chararray)user#'screen_name' as screen_name, 
  (int)user#'listed_count' as listed_count;

}
unique_users = distinct user_fields;
store unique_users into '$outputDir/users' 
using PigStorage('\u0001');</pre></div><p>Run this script as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ pig â€“f extract_for_hive.pig â€“param inputDir=&lt;json input&gt; -param outputDir=&lt;output path&gt;</strong></span>
</pre></div><p>The preceding<a id="id744" class="indexterm"></a> code writes data into three separate TSV files for the tweet, user, and place information. Notice that in the <code class="literal">store</code> command, we pass an argument when calling <code class="literal">PigStorage</code>. This single argument changes the default field separator from a tab character to unicode value U0001, or you can also use <span class="emphasis"><em>Ctrl</em></span> +<span class="emphasis"><em>C</em></span> + <span class="emphasis"><em>A</em></span>. This is often used as a separator in Hive tables and will be particularly useful to us as our tweet data could contain tabs in other fields.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec127"></a>Overview of Hive</h3></div></div></div><p>We will now show how you can import data into Hive and run a query against the table abstraction Hive provides over the data. In this example, and in the remainder of the chapter, we will assume that queries are typed into the shell that can be invoked by executing the <code class="literal">hive</code> command.</p><a id="id745" class="indexterm"></a><p>Recently a client called Beeline also became available and will likely be the preferred CLI client in the near future.</p><p>When importing any new data into Hive, there is generally a three-stage process:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Create the specification of the table into which the data is to be imported</p></li><li style="list-style-type: disc"><p>Import the data into the created table</p></li><li style="list-style-type: disc"><p>Execute HiveQL queries against the table</p></li></ul></div><p>Most of the HiveQL statements are direct analogues to similarly named statements in standard SQL. We assume only a passing knowledge of SQL throughout this chapter, but if you need a refresher, there are numerous good online learning resources.</p><p>Hive gives a structured query view of our data, and to enable that, we must first define the specification of the table's columns and import the data into the table before we can execute any queries. A table specification is generated using a <code class="literal">CREATE</code> statement that specifies the table name, the name and types of its columns, and some metadata about how the table is stored:</p><div class="informalexample"><pre class="programlisting">CREATE table tweets (
created_at string,
tweet_id string,
text string,
in_reply_to string,
retweeted boolean,
user_id string,
place_id string
) ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\u0001'
STORED AS TEXTFILE;</pre></div><p>The statement creates a new table <code class="literal">tweets</code> defined by a list of names for columns in the dataset and their data type. We specify that fields are delimited by the Unicode U0001 character  and that the format used to store data is <code class="literal">TEXTFILE</code>.</p><p>Data can be imported from a location in HDFS <code class="literal">tweets/</code> using the <code class="literal">LOAD DATA</code> statement:</p><div class="informalexample"><pre class="programlisting">LOAD DATA INPATH 'tweets' OVERWRITE INTO TABLE tweets;</pre></div><p>By default, data for Hive tables is stored on HDFS under <code class="literal">/user/hive/warehouse</code>. If a <code class="literal">LOAD</code> statement is given a path to data on HDFS, it will not simply copy the data into <code class="literal">/user/hive/warehouse</code>, but will move it there instead. If you want to analyze data on HDFS that is used by other applications, then either create a copy or use the <code class="literal">EXTERNAL</code> mechanism that will be described later.</p><p>Once data has been imported into Hive, we can run queries against it. For instance:</p><div class="informalexample"><pre class="programlisting">SELECT COUNT(*) FROM tweets;</pre></div><p>The preceding<a id="id746" class="indexterm"></a> code will return the total number of tweets present in the dataset. HiveQL, like SQL, is not case sensitive in terms of keywords, columns, or table names. By convention, SQL statements use uppercase for SQL language keywords, and we will generally follow this when using HiveQL within files, as will be shown later. However, when typing interactive commands, we will frequently take the line of least resistance and use lowercase.</p><p>If you look closely at the time taken by the various commands in the preceding example, you'll notice that loading data into a table takes about as long as creating the table specification, but even the simple count of all rows takes significantly longer. The output also shows that table creation and the loading of data do not actually cause MapReduce jobs to be executed, which explains the very short execution times.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec128"></a>The nature of Hive tables</h3></div></div></div><p>Although Hive<a id="id747" class="indexterm"></a> copies the data file into its working directory, it does not actually process the input data into rows at that point. </p><p>Both the <code class="literal">CREATE TABLE</code> and <code class="literal">LOAD DATA</code> statements do not truly create concrete table data as such; instead, they produce the metadata that will be used when Hive  generates MapReduce jobs to access the data conceptually stored in the table but actually residing on HDFS. Even though the HiveQL statements refer to a specific table structure, it is Hive's responsibility to generate code that correctly maps this to the actual on-disk format in which the data files are stored.</p><p>This might seem to suggest that Hive isn't a <span class="emphasis"><em>real</em></span> database; this is true, it isn't. Whereas a relational database will require a table schema to be defined before data is ingested and then ingest only data that conforms to that specification, Hive is much more flexible. The less concrete nature of Hive tables means that schemas can be defined based on the data as it has already arrived and not on some assumption of how the data should be, which might prove to be wrong. Though changeable data formats are troublesome regardless of technology, the Hive model provides an additional degree of freedom in handling the problem when, not if, it arises.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec53"></a>Hive architecture</h2></div></div><hr /></div><p>Until version 2, Hadoop was primarily <a id="id748" class="indexterm"></a>a batch system. As we saw in previous chapters, MapReduce jobs tend to have high latency and overhead derived from submission and scheduling. Internally, Hive compiles HiveQL statements into MapReduce jobs. Hive queries have traditionally been characterized by high latency. This has changed with the Stinger initiative and the improvements introduced in Hive 0.13 that we will discuss later.</p><p>Hive runs as a client application that processes HiveQL queries, converts them into MapReduce jobs, and submits these to a Hadoop cluster either to native MapReduce in Hadoop 1 or to the MapReduce Application Master running on YARN in Hadoop 2.</p><p>Regardless of the model, Hive uses a component called the metastore, in which it holds all its metadata about the tables defined in the system. Ironically, this is stored in a relational database dedicated to Hive's usage. In the earliest versions of Hive, all clients communicated directly with the metastore, but this meant that every user of the Hive CLI tool needed to know the metastore username and password.</p><p>HiveServer was created to act as a point of entry for remote clients, which could also act as a single access-control point and which controlled all access to the underlying metastore. Because of limitations in HiveServer, the newest way to access Hive is through the multi-client HiveServer2.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note21"></a>Note</h3><p>HiveServer2<a id="id749" class="indexterm"></a> introduces a number of improvements over its predecessor, including user authentication and support for multiple connections from the same client. More <a id="id750" class="indexterm"></a>information can be found at <a class="ulink" href="https://cwiki.apache.org/confluence/display/Hive/Setting+Up+HiveServer2" target="_blank">https://cwiki.apache.org/confluence/display/Hive/Setting+Up+HiveServer2</a>.</p></div><p>Instances of <code class="literal">HiveServer</code> and <code class="literal">HiveServer2</code> can be manually executed with the <code class="literal">hive --service hiveserver</code> and <code class="literal">hive --service hiveserver2</code> commands, respectively.</p><p>In the examples we saw before and in the remainder of this chapter, we implicitly use HiveServer to submit queries via the Hive command-line tool. HiveServer2 comes with Beeline. For compatibility and maturity reasons, Beeline being relatively new, both tools are available on Cloudera and most other major distributions. The Beeline client is part of the core Apache Hive distribution and so is also fully open source. Beeline can be executed in embedded version with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ beeline -u jdbc:hive2://</strong></span>
</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec129"></a>Data types</h3></div></div></div><p>HiveQL supports many of the common data types <a id="id751" class="indexterm"></a>provided by standard database systems. These include<a id="id752" class="indexterm"></a> primitive types, such as <code class="literal">float</code>, <code class="literal">double</code>, <code class="literal">int,</code> and <code class="literal">string</code>, through<a id="id753" class="indexterm"></a> to structured collection types that provide the SQL<a id="id754" class="indexterm"></a> analogues to types such as <code class="literal">arrays</code>, <code class="literal">structs</code>, and <code class="literal">unions</code> (<code class="literal">structs</code> with options for some fields). Since Hive is implemented in Java, primitive <a id="id755" class="indexterm"></a>types will behave like their Java counterparts. We can distinguish Hive data types<a id="id756" class="indexterm"></a> into the following five<a id="id757" class="indexterm"></a> broad categories:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Numeric</strong></span>: <code class="literal">tinyint</code>, <code class="literal">smallint</code>, <code class="literal">int</code>, <code class="literal">bigint</code>, <code class="literal">float</code>, <code class="literal">double</code>, and <code class="literal">decimal</code>
</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Date and time</strong></span>: <code class="literal">timestamp</code> and <code class="literal">date</code>
</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>String</strong></span>: <code class="literal">string</code>, <code class="literal">varchar</code>, and <code class="literal">char</code>
</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Collections</strong></span>: <code class="literal">array</code>, <code class="literal">map</code>, <code class="literal">struct</code>, and <code class="literal">uniontype</code>
</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Misc</strong></span>: <code class="literal">boolean</code>, <code class="literal">binary</code>, and <code class="literal">NULL</code>
</p></li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec130"></a>DDL statements</h3></div></div></div><p>HiveQL provides <a id="id758" class="indexterm"></a>a number of<a id="id759" class="indexterm"></a> statements to create, delete, and alter databases, tables, and views. The <code class="literal">CREATE DATABASE &lt;name&gt;</code> statement creates a new database with the given name. A database represents a namespace where table and view metadata is contained. If multiple databases are present, the <code class="literal">USE &lt;database name&gt;</code> statement specifies which one to use to query tables or create new metadata. If no database is explicitly specified, Hive will run all statements against the <code class="literal">default</code> database. <code class="literal">SHOW [DATABASES, TABLES, VIEWS]</code> displays the databases currently available within a data warehouse and which table and view metadata is present within the database currently in use:</p><div class="informalexample"><pre class="programlisting">CREATE DATABASE twitter;
SHOW databases;
USE twitter;
SHOW TABLES;</pre></div><p>The <code class="literal">CREATE TABLE [IF NOT EXISTS] &lt;name&gt;</code> statement creates a table with the given name. As alluded to earlier, what is really created is the metadata representing the table and its mapping to files on HDFS as well as a directory in which to store the data files. If a table or view with the same name already exists, Hive will raise an exception.</p><p>Both table and <a id="id760" class="indexterm"></a>column names<a id="id761" class="indexterm"></a> are case insensitive. In older versions of Hive (0.12 and earlier), only alphanumeric and underscore characters were allowed in table and column names. As of Hive 0.13, the system supports unicode characters in column names. Reserved words, such as <code class="literal">load</code> and <code class="literal">create</code>, need to be escaped by backticks (the ` character) to be treated literally.</p><p>The <code class="literal">EXTERNAL</code> keyword <a id="id762" class="indexterm"></a>specifies that the table exists in resources out of Hive's control, which can be a useful mechanism to extract data from another source at the beginning of a Hadoop-based <a id="id763" class="indexterm"></a>
<span class="strong"><strong>Extract-Transform-Load</strong></span> (<span class="strong"><strong>ETL</strong></span>) pipeline. The <code class="literal">LOCATION</code> clause specifies where the source file (or directory) is to be found. The <code class="literal">EXTERNAL</code> keyword and <code class="literal">LOCATION</code> clause have been used in the following code:</p><div class="informalexample"><pre class="programlisting">CREATE EXTERNAL TABLE tweets (
created_at string,
tweet_id string,
text string,
in_reply_to string,
retweeted boolean,
user_id string,
place_id string
) ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\u0001'
STORED AS TEXTFILE
LOCATION '${input}/tweets';</pre></div><p>This table will be created in the metastore but the data will not be copied into the <code class="literal">/user/hive/warehouse</code> directory.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip05"></a>Tip</h3><p>Note that Hive has no concept of primary key or unique identifier. Uniqueness and data normalization are aspects to be addressed before loading data into the data warehouse.</p></div><p>The <code class="literal">CREATE VIEW &lt;view name&gt; â€¦ AS SELECT</code> statement creates a view with the given name. For example, we can create a view to isolate retweets from other messages, as follows:</p><div class="informalexample"><pre class="programlisting">CREATE VIEW retweets 
COMMENT 'Tweets that have been retweeted'
AS SELECT * FROM tweets WHERE retweeted = true;</pre></div><p>Unless otherwise specified, column names are derived from the defining <code class="literal">SELECT</code> statement. Hive does not currently support materialized views.</p><p>The <code class="literal">DROP TABLE</code> and <code class="literal">DROP VIEW</code> statements remove both metadata and data for a given table or view. When dropping an <code class="literal">EXTERNAL</code> table or a view, only metadata will be removed and the actual data files will not be affected.</p><p>Hive allows table metadata to be altered via the <code class="literal">ALTER TABLE</code> statement, which can be used to change a column type, name, position, and comment or to add and replace columns.</p><p>When adding columns, it is<a id="id764" class="indexterm"></a> important to remember that only metadata will be <a id="id765" class="indexterm"></a>changed and not the dataset itself. This means that if we were to add a column in the middle of the table which didn't exist in older files, then while selecting from older data, we might get wrong values in the wrong columns. This is because we would be looking at old files with a new format. We will discuss data and schema migrations in <a class="link" href="#" linkend="ch08">Chapter 8</a>, <span class="emphasis"><em>Data Lifecycle Management</em></span>, when discussing Avro.</p><p>Similarly, <code class="literal">ALTER VIEW &lt;view name&gt; AS &lt;select statement&gt;</code> changes the definition of an existing view.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec131"></a>File formats and storage</h3></div></div></div><p>The data files underlying a<a id="id766" class="indexterm"></a> Hive table are no different from any other file on HDFS. Users can<a id="id767" class="indexterm"></a> directly read the HDFS files in the Hive tables using other tools. They <a id="id768" class="indexterm"></a>can also use other tools to write to HDFS files that can be loaded<a id="id769" class="indexterm"></a> into Hive through <code class="literal">CREATE EXTERNAL TABLE</code> or through <code class="literal">LOAD DATA INPATH</code>.</p><p>Hive uses <code class="literal">the Serializer</code> and <code class="literal">Deserializer</code> classes, SerDe, as well as <code class="literal">FileFormat</code> to read and write table rows. A native SerDe is used if <code class="literal">ROW FORMAT</code> is not specified or <code class="literal">ROW FORMAT DELIMITED</code> is specified in a <code class="literal">CREATE TABLE</code> statement. The <code class="literal">DELIMITED</code> clause instructs the system to read delimited files. Delimiter characters can be escaped using the <code class="literal">ESCAPED BY</code> clause.</p><p>Hive currently uses the following <code class="literal">FileFormat</code> classes to read and write HDFS files:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">TextInputFormat</code> <a id="id770" class="indexterm"></a>and <code class="literal">HiveIgnoreKeyTextOutputFormat</code>: will <a id="id771" class="indexterm"></a>read/write data in plain text file format</p></li><li style="list-style-type: disc"><p>
<code class="literal">SequenceFileInputFormat</code> and <code class="literal">SequenceFileOutputFormat</code>: classes read/write <a id="id772" class="indexterm"></a>data in the Hadoop <code class="literal">SequenceFile</code> <a id="id773" class="indexterm"></a>format</p></li></ul></div><p>Additionally, the following SerDe classes can be used to serialize and deserialize data:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">MetadataTypedColumnsetSerDe</code>: <a id="id774" class="indexterm"></a>will read/write delimited records such as CSV or tab-separated records</p></li><li style="list-style-type: disc"><p>
<code class="literal">ThriftSerDe</code>, and <code class="literal">DynamicSerDe</code>: will <a id="id775" class="indexterm"></a>read/write <a id="id776" class="indexterm"></a>Thrift objects</p></li></ul></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl3sec57"></a>JSON</h4></div></div></div><p>As of version 0.13, Hive<a id="id777" class="indexterm"></a> ships<a id="id778" class="indexterm"></a> with the native <code class="literal">org.apache.hive.hcatalog.data.JsonSerDe</code>. For older versions of Hive, Hive-JSON-Serde<a id="id779" class="indexterm"></a> (found at <a class="ulink" href="https://github.com/rcongiu/Hive-JSON-Serde" target="_blank">https://github.com/rcongiu/Hive-JSON-Serde</a>) is arguably one of the most feature-rich JSON serialization/deserialization modules.</p><p>We can use either module to load JSON tweets without any need for preprocessing and just define a Hive schema that matches the content of a JSON document. In the following example, we use Hive-JSON-Serde.</p><p>As with any third-party module, we load the SerDe JARs into Hive with the following code:</p><div class="informalexample"><pre class="programlisting">ADD JAR JAR json-serde-1.3-jar-with-dependencies.jar;</pre></div><p>Then, we issue the usual <code class="literal">CREATE</code> statement, as follows:</p><div class="informalexample"><pre class="programlisting">CREATE EXTERNAL TABLE tweets (
   contributors string,
   coordinates struct &lt;
      coordinates: array &lt;float&gt;,
      type: string&gt;,
   created_at string,
   entities struct &lt;
      hashtags: array &lt;struct &lt;
            indices: array &lt;tinyint&gt;,
            text: string&gt;&gt;,
â€¦
)
ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'
STORED AS TEXTFILE
LOCATION 'tweets';  </pre></div><p>With this SerDe, we can map nested documents (such as entities or users) to the <code class="literal">struct</code> or <code class="literal">map</code> types. We tell Hive that the data stored at <code class="literal">LOCATION 'tweets'</code> is text (<code class="literal">STORED AS TEXTFILE</code>) and that each row is a JSON object (<code class="literal">ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe</code>'). In Hive 0.13 and later, we can express this property as <code class="literal">ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe'</code>.</p><p>Manually <a id="id780" class="indexterm"></a>specifying the schema for complex documents can be a tedious and<a id="id781" class="indexterm"></a> error-prone process. The <code class="literal">hive-json</code> module<a id="id782" class="indexterm"></a> (found at <a class="ulink" href="https://github.com/hortonworks/hive-json" target="_blank">https://github.com/hortonworks/hive-json</a>) is a handy utility to analyze large documents and generate an appropriate <a id="id783" class="indexterm"></a>Hive schema. Depending on the document collection, further refinement might be necessary.</p><p>In our example, we used a schema generated with <code class="literal">hive-json</code> that maps the tweets JSON to a number of <code class="literal">struct</code> data types. This allows us to query the data using a handy dot notation. For instance, we can extract the screen name and description fields of a user object with the following code:</p><div class="informalexample"><pre class="programlisting">SELECT user.screen_name, user.description FROM tweets_json LIMIT 10;</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl3sec58"></a>Avro</h4></div></div></div><p>
<span class="strong"><strong>AvroSerde</strong></span> (<a class="ulink" href="https://cwiki.apache.org/confluence/display/Hive/AvroSerDe" target="_blank">https://cwiki.apache.org/confluence/display/Hive/AvroSerDe</a>) allows <a id="id784" class="indexterm"></a>us to<a id="id785" class="indexterm"></a> read and write data in<a id="id786" class="indexterm"></a> Avro format. Starting from 0.14, Avro-backed tables can be created using the <code class="literal">STORED AS AVRO</code> statement, and Hive will take care of creating an appropriate Avro schema for the table. Prior versions of Hive are a bit more verbose.</p><p>As an example, let's load into Hive the PageRank dataset we generated in <a class="link" href="#" linkend="ch06">Chapter 6</a>, <span class="emphasis"><em>Data Analysis with Apache Pig</em></span>. This dataset was created using Pig's <code class="literal">AvroStorage</code> class, and has the following schema:</p><div class="informalexample"><pre class="programlisting">{
  "type":"record",
  "name":"record",
  "fields": [
    {"name":"topic","type":["null","int"]},
    {"name":"source","type":["null","int"]},
    {"name":"rank","type":["null","float"]}
  ]
}  </pre></div><a id="id787" class="indexterm"></a><p>The table structure is captured in an Avro record, which contains header information (a name and optional namespace to qualify the name) and an array of the fields. Each field is specified with its name and type as well as an optional documentation string.</p><p>For a few of the fields, the type is not a single value, but instead a pair of values, one of which is null. This is an Avro union, and this is the idiomatic way of handling columns that might have a null value. Avro specifies null as a concrete type, and any location where another type might have a null value needs to be specified in this way. This will be handled transparently for us when we use the following schema.</p><p>With this definition, we can now create a Hive table that uses this schema for its table specification, as follows:</p><div class="informalexample"><pre class="programlisting">CREATE EXTERNAL TABLE tweets_pagerank
ROW FORMAT SERDE
  'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
WITH SERDEPROPERTIES ('avro.schema.literal'='{
    "type":"record",
    "name":"record",
    "fields": [
        {"name":"topic","type":["null","int"]},
        {"name":"source","type":["null","int"]},
        {"name":"rank","type":["null","float"]}
    ]
}')
STORED AS INPUTFORMAT
  'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
OUTPUTFORMAT
  'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
LOCATION '${data}/ch5-pagerank';</pre></div><p>Then, look at the following table definition from within Hive (note also that HCatalog, which we'll introduce in <a class="link" href="#" linkend="ch08">Chapter 8</a>, <span class="emphasis"><em>Data Life Cycle Management</em></span>, also supports such definitions):</p><div class="informalexample"><pre class="programlisting">DESCRIBE tweets_pagerank;
OK
topic                 int                   from deserializer   
source                int                   from deserializer   
rank                  float                 from deserializer  </pre></div><p>In the DDL, we told Hive that data is stored in Avro format using <code class="literal">AvroContainerInputFormat</code> and <code class="literal">AvroContainerOutputFormat</code>. Each row needs to be serialized and deserialized using <code class="literal">org.apache.hadoop.hive.serde2.avro.AvroSerDe</code>. The table schema is inferred by Hive from the Avro schema embedded in <code class="literal">avro.schema.literal</code>.</p><p>Alternatively, we<a id="id788" class="indexterm"></a> can store a schema on HDFS and have Hive read it to determine the table structure. Create the preceding schema in a file called <code class="literal">pagerank.avsc</code>â€”this is the standard file extension for Avro schemas. Then place it on HDFS; we prefer to have a common location for schema files such as <code class="literal">/schema/avro</code>. Finally, define the table using the <code class="literal">avro.schema.url</code> SerDe property <code class="literal">WITH SERDEPROPERTIES ('avro.schema.url'='hdfs://&lt;namenode&gt;/schema/avro/pagerank.avsc')</code>.</p><p>If Avro dependencies are not present in the classpath, we need to add the Avro <code class="literal">MapReduce</code> JAR to our environment before accessing individual fields. Within Hive, on the Cloudera CDH5 VM:</p><div class="informalexample"><pre class="programlisting">ADD JAR /opt/cloudera/parcels/CDH/lib/avro/avro-mapred-hadoop2.jar; </pre></div><p>We can also use this table like any other. For instance, we can query the data to select the user and topic pairs with a high PageRank:</p><div class="informalexample"><pre class="programlisting">SELECT source, topic from tweets_pagerank WHERE rank &gt;= 0.9;</pre></div><p>In <a class="link" href="#" linkend="ch08">Chapter 8</a>, <span class="emphasis"><em>Data Lifecycle Management</em></span>, we will see how Avro and <code class="literal">avro.schema.url</code> play an instrumental role in enabling schema migrations.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl3sec59"></a>Columnar stores</h4></div></div></div><p>Hive can also take <a id="id789" class="indexterm"></a>advantage of <a id="id790" class="indexterm"></a>columnar storage via <a id="id791" class="indexterm"></a>the <code class="literal">ORC</code> (<a class="ulink" href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC" target="_blank">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC</a>) and<a id="id792" class="indexterm"></a> <code class="literal">Parquet</code> (<a class="ulink" href="https://cwiki.apache.org/confluence/display/Hive/Parquet" target="_blank">https://cwiki.apache.org/confluence/display/Hive/Parquet</a>) formats.</p><p>If a table is defined with very many <a id="id793" class="indexterm"></a>columns, it is not unusual for any given query to only process a small subset of these columns. But even in a SequenceFile each full row and all its columns will be read from disk, decompressed, and processed. This consumes a lot of system resources for data that we know in advance is not of interest.</p><p>Traditional relational databases also store data on a row basis, and a type of database called <span class="strong"><strong>columnar</strong></span><a id="id794" class="indexterm"></a>
<span class="strong"><strong> </strong></span>changed <a id="id795" class="indexterm"></a>this to be column-focused. In the simplest model, instead of one file for each table, there would be one file for each column in the table. If a query only needed to access five columns in a table with 100 columns in total, then only the files for those five columns will be read. Both ORC and Parquet use this principle as well as other optimizations to enable much faster queries.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec132"></a>Queries</h3></div></div></div><p>Tables can be queried using the<a id="id796" class="indexterm"></a> familiar <code class="literal">SELECT â€¦ FROM</code> statement. The <code class="literal">WHERE</code> statement<a id="id797" class="indexterm"></a> allows the specification of filtering conditions, <code class="literal">GROUP BY</code> aggregates records, <code class="literal">ORDER BY</code> specifies sorting criteria, and <code class="literal">LIMIT</code> specifies the number of records to retrieve. Aggregate functions, such as <code class="literal">count</code> and <code class="literal">sum</code>, can be applied to aggregated records. For instance, the following code returns the top 10 most prolific users in the dataset:</p><div class="informalexample"><pre class="programlisting">SELECT user_id, COUNT(*) AS cnt FROM tweets GROUP BY user_id ORDER BY cnt DESC LIMIT 10</pre></div><p>This returns the top 10 most prolific users in the dataset:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>2263949659 4</strong></span>
<span class="strong"><strong>1332188053  4</strong></span>
<span class="strong"><strong>959468857  3</strong></span>
<span class="strong"><strong>1367752118  3</strong></span>
<span class="strong"><strong>362562944  3</strong></span>
<span class="strong"><strong>58646041  3</strong></span>
<span class="strong"><strong>2375296688  3</strong></span>
<span class="strong"><strong>1468188529  3</strong></span>
<span class="strong"><strong>37114209  3</strong></span>
<span class="strong"><strong>2385040940  3</strong></span>
</pre></div><p>We can improve the readability of the <code class="literal">hive</code> output by setting the following:</p><div class="informalexample"><pre class="programlisting">SET hive.cli.print.header=true;</pre></div><p>This will instruct <code class="literal">hive</code>, though not <code class="literal">beeline</code>, to print column names as part of the output.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip06"></a>Tip</h3><p>You can add the command to the .<code class="literal">hiverc</code> file usually found in the root of the executing user's home directory to have it apply to all <code class="literal">hive</code> CLI sessions.</p></div><p>HiveQL implements a <code class="literal">JOIN</code> operator<a id="id798" class="indexterm"></a> that enables us to combine tables together. In the <span class="emphasis"><em>Prerequisites</em></span> section, we generated separate datasets for the user and place objects. Let's now <a id="id799" class="indexterm"></a>load them into <a id="id800" class="indexterm"></a>hive using external tables.</p><p>We first create a <code class="literal">user</code> table to store user data, as follows:</p><div class="informalexample"><pre class="programlisting">CREATE EXTERNAL TABLE user (
created_at string,
user_id string,
`location` string,
name string,
description string,
followers_count bigint,
friends_count bigint,
favourites_count bigint,
screen_name string,
listed_count bigint
) ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\u0001'
STORED AS TEXTFILE
LOCATION '${input}/users';</pre></div><p>We then create a <code class="literal">place</code> table to store location data, as follows:</p><div class="informalexample"><pre class="programlisting">CREATE EXTERNAL TABLE place (
place_id string,
country_code string,
country string,
`name` string,
full_name string,
place_type string
) ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\u0001'
STORED AS TEXTFILE
LOCATION '${input}/places';</pre></div><p>We can use the <code class="literal">JOIN</code> operator to display the names of the 10 most prolific users, as follows:</p><div class="informalexample"><pre class="programlisting">SELECT tweets.user_id, user.name, COUNT(tweets.user_id) AS cnt 
FROM tweets 
JOIN user ON user.user_id  = tweets.user_id
GROUP BY tweets.user_id, user.user_id, user.name 
ORDER BY cnt DESC LIMIT 10; </pre></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip07"></a>Tip</h3><p>Only equality, outer, and left (semi) joins are supported in Hive.</p></div><p>Notice that there might be <a id="id801" class="indexterm"></a>multiple entries with a given user ID but different values for the <code class="literal">followers_count</code>, <code class="literal">friends_count</code>, and <code class="literal">favourites_count</code> columns. To avoid duplicate<a id="id802" class="indexterm"></a> entries, we count only <code class="literal">user_id</code> from the <code class="literal">tweets</code> table.</p><p>We can rewrite the previous query as follows:</p><div class="informalexample"><pre class="programlisting">SELECT tweets.user_id, u.name, COUNT(*) AS cnt 
FROM tweets 
join (SELECT user_id, name FROM user GROUP BY user_id, name) u
ON u.user_id = tweets.user_id
GROUP BY tweets.user_id, u.name 
ORDER BY cnt DESC LIMIT 10;   </pre></div><p>Instead of directly joining the <code class="literal">user</code> table, we execute a subquery, as follows:</p><div class="informalexample"><pre class="programlisting">SELECT user_id, name FROM user GROUP BY user_id, name;</pre></div><p>The subquery extracts unique user IDs and names. Note that Hive has limited support for subqueries, historically only permitting a subquery in the <code class="literal">FROM</code> clause of a <code class="literal">SELECT</code> statement. Hive 0.13 has added limited support for subqueries within the <code class="literal">WHERE</code> clause also.</p><p>HiveQL<a id="id803" class="indexterm"></a> is an ever-evolving rich language, a full exposition of which is beyond the scope of this chapter. A description of its query and ddl capabilities can be found at <a class="ulink" href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual" target="_blank">https://cwiki.apache.org/confluence/display/Hive/LanguageManual</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec133"></a>Structuring Hive tables for given workloads</h3></div></div></div><p>Often Hive isn't<a id="id804" class="indexterm"></a> used in isolation, instead tables are <a id="id805" class="indexterm"></a>created with particular workloads in mind or needs invoked in ways that are suitable for inclusion in automated processes. We'll now explore some of these scenarios.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec134"></a>Partitioning a table</h3></div></div></div><p>With columnar file<a id="id806" class="indexterm"></a> formats, we explained the benefits of excluding unneeded data as early as possible when processing a query. A similar concept has been used in SQL for some time: table partitioning.</p><p>When creating a partitioned table, a column is specified as the partition key. All values with that key are then stored together. In Hive's case, different subdirectories for each partition key are created under the table directory in the warehouse location on HDFS.</p><p>It's important to understand the cardinality of the partition column. With too few distinct values, the benefits are reduced as the files are still very large. If there are too many values, then queries might need a large number of files to be scanned to access all the required data. Perhaps the most common partition key is one based on date. We could, for example, partition our <code class="literal">user</code> table from earlier based on the <code class="literal">created_at</code> column, that is, the date the user was first registered. Note that since partitioning a table by definition affects its file structure, we create this table now as a non-external one, as follows:</p><div class="informalexample"><pre class="programlisting">CREATE TABLE partitioned_user (
created_at string,
user_id string,
`location` string,
name string,
description string,
followers_count bigint,
friends_count bigint,
favourites_count bigint,
screen_name string,
listed_count bigint
)  PARTITIONED BY (created_at_date string)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\u0001'
STORED AS TEXTFILE;</pre></div><p>To load data into a partition, we can explicitly give a value for the partition into which to insert the data, as follows:</p><div class="informalexample"><pre class="programlisting">INSERT INTO TABLE partitioned_user
PARTITION( created_at_date = '2014-01-01')
SELECT 
created_at,
user_id,
location,
name,
description,
followers_count,
friends_count,
favourites_count,
screen_name,
listed_count
FROM user;</pre></div><p>This is at best verbose, as we need a statement for each partition key value; if a single <code class="literal">LOAD</code> or <code class="literal">INSERT</code> statement contains data for multiple partitions, it just won't work. Hive also has a feature called dynamic partitioning, which can help us here. We set the following three variables:</p><div class="informalexample"><pre class="programlisting">SET hive.exec.dynamic.partition = true;
SET hive.exec.dynamic.partition.mode = nonstrict;
SET hive.exec.max.dynamic.partitions.pernode=5000;</pre></div><p>The first two statements <a id="id807" class="indexterm"></a>enable all partitions (<code class="literal">nonstrict</code> option) to be dynamic. The third one allows 5,000 distinct partitions to be created on each mapper and reducer node.</p><p>We can then simply use the name of the column to be used as the partition key, and Hive will insert data into partitions depending on the value of the key for a given row:</p><div class="informalexample"><pre class="programlisting">INSERT INTO TABLE partitioned_user
PARTITION( created_at_date )
SELECT 
created_at,
user_id,
location,
name,
description,
followers_count,
friends_count,
favourites_count,
screen_name,
listed_count,
to_date(created_at) as created_at_date
FROM user;</pre></div><p>Even though we use only a single partition column here, we can partition a table by multiple column keys; just have them as a comma-separated list in the <code class="literal">PARTITIONED BY</code> clause.</p><p>Note that the partition key columns need to be included as the last columns in any statement being used to insert into a partitioned table. In the preceding code we use Hive's <code class="literal">to_date</code> function to convert the <code class="literal">created_at</code> timestamp to a <code class="literal">YYYY-MM-DD</code> formatted string.</p><p>Partitioned data is stored in HDFS as <code class="literal">/path/to/warehouse/&lt;database&gt;/&lt;table&gt;/key=&lt;value&gt;</code>. In our example, the <code class="literal">partitioned_user</code> table structure will look like <code class="literal">/user/hive/warehouse/default/partitioned_user/created_at=2014-04-01</code>.</p><p>If data is added directly to the filesystem, for instance by some third-party processing tool or by <code class="literal">hadoop fs -put</code>, the metastore won't automatically detect the new partitions. The user will need to manually run an <code class="literal">ALTER TABLE</code> statement such as the following for each newly added partition:</p><div class="informalexample"><pre class="programlisting">ALTER TABLE &lt;table_name&gt; ADD PARTITION &lt;location&gt;;</pre></div><p>To add metadata for all partitions not currently present in the metastore we can use: <code class="literal">MSCK REPAIR TABLE &lt;table_name&gt;;</code> statement. On EMR, this is equivalent to executing the following statement:</p><div class="informalexample"><pre class="programlisting">ALTER TABLE &lt;table_name&gt; RECOVER PARTITIONS; </pre></div><p>Notice that both statements will work also with <code class="literal">EXTERNAL</code> tables. In the following chapter, we will see how this pattern can be exploited to create flexible and interoperable pipelines.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl3sec60"></a>Overwriting and updating data</h4></div></div></div><p>Partitioning is <a id="id808" class="indexterm"></a>also useful when we need to update a portion <a id="id809" class="indexterm"></a>of a table. Normally a statement of the following form will replace all the data for the destination table:</p><div class="informalexample"><pre class="programlisting">INSERT OVERWRITE INTO &lt;table&gt;â€¦</pre></div><p>If <code class="literal">OVERWRITE</code> is omitted, then each <code class="literal">INSERT</code> statement will add additional data to the table. Sometimes, this is desirable, but often, the source data being ingested into a Hive table is intended to fully update a subset of the data and keep the rest untouched.</p><p>If we perform an <code class="literal">INSERT OVERWRITE</code> statement (or a <code class="literal">LOAD OVERWRITE</code> statement) into a partition of a table, then only the specified partition will be affected. Thus, if we were inserting user data and<a id="id810" class="indexterm"></a> only wanted to affect the partitions with data in<a id="id811" class="indexterm"></a> the source file, we could achieve this by adding the <code class="literal">OVERWRITE</code> keyword to our previous <code class="literal">INSERT</code> statement.</p><p>We can also add caveats to the <code class="literal">SELECT</code> statement. Say, for example, we only wanted to update data for a certain month:</p><div class="informalexample"><pre class="programlisting">INSERT INTO TABLE partitioned_user
PARTITION (created_at_date)
SELECT created_at ,
user_id,
location,
name,
description,
followers_count,
friends_count,
favourites_count,
screen_name,
listed_count,
to_date(created_at) as created_at_date
FROM user 
WHERE to_date(created_at) BETWEEN '2014-03-01' and '2014-03-31';</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl3sec61"></a>Bucketing and sorting</h4></div></div></div><p>Partitioning a table is a <a id="id812" class="indexterm"></a>construct that you take explicit advantage of by <a id="id813" class="indexterm"></a>using the partition column (or columns) in the <code class="literal">WHERE</code> clause of queries against the tables. There is another mechanism called bucketing that can further segment how a table is stored and does so in a way that allows Hive itself to optimize its internal query plans to take advantage of the structure.</p><p>Let's create bucketed versions of our tweets and user tables; note the following additional <code class="literal">CLUSTER BY</code> and <code class="literal">SORT BY</code> statements in the <code class="literal">CREATE TABLE</code> statements:</p><div class="informalexample"><pre class="programlisting">CREATE table bucketed_tweets (
tweet_id string,
text string,
in_reply_to string,
retweeted boolean,
user_id string,
place_id string
)  PARTITIONED BY (created_at string)
CLUSTERED BY(user_ID) into 64 BUCKETS
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\u0001'
STORED AS TEXTFILE;

CREATE TABLE bucketed_user (
user_id string,
`location` string,
name string,
description string,
followers_count bigint,
friends_count bigint,
favourites_count bigint,
screen_name string,
listed_count bigint
)  PARTITIONED BY (created_at string)
CLUSTERED BY(user_ID) SORTED BY(name) into 64 BUCKETS
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\u0001'
STORED AS TEXTFILE;</pre></div><p>Note that we changed the<a id="id814" class="indexterm"></a> tweets table to also be partitioned; you can only bucket <a id="id815" class="indexterm"></a>a table that is partitioned.</p><p>Just as we need to specify a partition column when inserting into a partitioned table, we must also take care to ensure that data inserted into a bucketed table is correctly clustered. We do this by setting the following flag before inserting the data into the table:</p><div class="informalexample"><pre class="programlisting">SET hive.enforce.bucketing=true;</pre></div><p>Just as with partitioned tables, you cannot apply the bucketing function when using the <code class="literal">LOAD DATA</code> statement; if you wish to load external data into a bucketed table, first insert it into a temporary table, and then use the <code class="literal">INSERTâ€¦SELECTâ€¦</code> syntax to populate the bucketed table.</p><p>When data is inserted into a bucketed table, rows are allocated to a bucket based on the result of a hash function applied to the column specified in the <code class="literal">CLUSTERED BY</code> clause.</p><p>One of the greatest advantages of bucketing a table comes when we need to join two tables that are similarly bucketed, as in the previous example. So, for example, any query of the following form would be vastly improved:</p><div class="informalexample"><pre class="programlisting">SET hive.optimize.bucketmapjoin=true;
SELECT â€¦
FROM bucketed_user u JOIN bucketed_tweet t
ON u.user_id = t.user_id;</pre></div><p>With the join being <a id="id816" class="indexterm"></a>performed on the column used to bucket the table, Hive <a id="id817" class="indexterm"></a>can optimize the amount of processing as it knows that each bucket contains the same set of <code class="literal">user_id</code> columns in both tables. While determining which rows against which to match, only those in the bucket need to be compared against, and not the whole table. This does require that the tables are both clustered on the same column and that the bucket numbers are either identical or one is a multiple of the other. In the latter case, with say one table clustered into 32 buckets and another into 64, the nature of the default hash function used to allocate data to a bucket means that the IDs in bucket 3 in the first table will cover those in both buckets 3 and 35 in the second.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl3sec62"></a>Sampling data</h4></div></div></div><p>Bucketing a<a id="id818" class="indexterm"></a> table can also help while using Hive's ability to sample data in a table. Sampling allows a query to gather only a specified subset of the overall rows in the table. This is useful when you have an extremely large table with moderately consistent data patterns. In such a case, applying a query to a small fraction of the data will be much faster and will still give a broadly representative result. Note, of course, that this only applies to queries where you are looking to determine table characteristics, such as pattern ranges in the data; if you are trying to count anything, then the result needs to be scaled to the full table size.</p><p>For a non-bucketed table, you can sample in a mechanism similar to what we saw earlier by specifying that the query should only be applied to a certain subset of the table:</p><div class="informalexample"><pre class="programlisting">SELECT max(friends_count)
FROM user TABLESAMPLE(BUCKET 2 OUT OF 64 ON name);</pre></div><p>In this query, Hive will effectively hash the rows in the table into 64 buckets based on the name column. It will then only use the second bucket for the query. Multiple buckets can be specified, and if <code class="literal">RAND()</code> is given as the <code class="literal">ON</code> clause, then the entire row is used by the bucketing function.</p><p>Though successful, this is highly inefficient as the full table needs to be scanned to generate the required subset of data. If we sample on a bucketed table and ensure the number of buckets sampled is equal to or a multiple of the buckets in the table, then Hive will only read the buckets in question. For example:</p><div class="informalexample"><pre class="programlisting">SELECT MAX(friends_count)
FROM bucketed_user TABLESAMPLE(BUCKET 2 OUT OF 32 on user_id);</pre></div><p>In the preceding query against the <code class="literal">bucketed_user</code> table, which is created with 64 buckets on the <code class="literal">user_id</code> column, the sampling, since it is using the same column, will only read the required buckets. In this case, these will be buckets 2 and 34 from each partition.</p><p>A final form of sampling is<a id="id819" class="indexterm"></a> block sampling. In this case, we can specify the required amount of the table to be sampled, and Hive will use an approximation of this by only reading enough source data blocks on HDFS to meet the required size. Currently, the data size can be specified as either a percentage of the table, as an absolute data size, or as a number of rows (in each block). The syntax for <code class="literal">TABLESAMPLE</code> is as follows, which will sample 0.5 percent of the table, 1 GB of data or 100 rows per split, respectively:</p><div class="informalexample"><pre class="programlisting">TABLESAMPLE(0.5 PERCENT)
TABLESAMPLE(1G)
TABLESAMPLE(100 ROWS)</pre></div><p>If these latter forms of sampling are of interest, then consult the documentation, as there are some specific limitations on the input format and file formats that are supported.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec135"></a>Writing scripts</h3></div></div></div><p>We can place Hive<a id="id820" class="indexterm"></a> commands in a file and run them with the <code class="literal">-f</code> option in the <code class="literal">hive</code> CLI utility:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cat show_tables.hql</strong></span>
<span class="strong"><strong>show tables;</strong></span>
<span class="strong"><strong>$ hive -f show_tables.hql  </strong></span>
</pre></div><p>We can parameterize HiveQL statements by means of the <code class="literal">hiveconf</code> mechanism. This allows us to specify an environment variable name at the point it is used rather than at the point of invocation. For example:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cat show_tables2.hql</strong></span>
<span class="strong"><strong>show tables like '${hiveconf:TABLENAME}';</strong></span>
<span class="strong"><strong>$ hive -hiveconf TABLENAME=user -f show_tables2.hql</strong></span>
</pre></div><p>The variable can also be set within the Hive script or an interactive session:</p><div class="informalexample"><pre class="programlisting">SET TABLE_NAME='user';</pre></div><p>The preceding <code class="literal">hiveconf</code> argument will add any new variables in the same namespace as the Hive configuration options. As of Hive 0.8, there is a similar option called <code class="literal">hivevar</code> that adds any user variables into a distinct namespace. Using <code class="literal">hivevar</code>, the preceding command would be as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cat show_tables3.hql</strong></span>
<span class="strong"><strong>show tables like '${hivevar:TABLENAME}';</strong></span>
<span class="strong"><strong>$ hive -hivevar TABLENAME=user â€“f show_tables3.hql</strong></span>
</pre></div><p>Or we can write the <a id="id821" class="indexterm"></a>command interactively:</p><div class="informalexample"><pre class="programlisting">SET hivevar:TABLE_NAME='user';</pre></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec54"></a>Hive and Amazon Web Services</h2></div></div><hr /></div><p>With <a id="id822" class="indexterm"></a>Elastic MapReduce as the <a id="id823" class="indexterm"></a>AWS Hadoop-on-demand service, it is of course possible to run Hive on an EMR cluster. But it is also possible to use Amazon storage services, particularly S3, from any Hadoop cluster be it within EMR or your own local cluster.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec136"></a>Hive and S3</h3></div></div></div><p>As mentioned in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Storage</em></span>, it is possible<a id="id824" class="indexterm"></a> to specify a default filesystem other than HDFS for<a id="id825" class="indexterm"></a> Hadoop and S3 is one option. But, it doesn't have to be an all-or-nothing thing; it is possible to have specific tables stored in S3. The data for these tables will be retrieved into the cluster to be processed, and any resulting data can either be written to a different S3 location (the same table cannot be the source and destination of a single query) or onto HDFS.</p><p>We can take a file of our tweet data and place it onto a location in S3 with a command such as the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ aws s3 put tweets.tsv s3://&lt;bucket-name&gt;/tweets/</strong></span>
</pre></div><p>We firstly need to specify the access key and secret access key that can access the bucket. This can be done in three ways:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Set <code class="literal">fs.s3n.awsAccessKeyId</code> and <code class="literal">fs.s3n.awsSecretAccessKey</code> to the appropriate values in the Hive CLI</p></li><li style="list-style-type: disc"><p>Set the same values in <code class="literal">hive-site.xml</code> though note this limits use of S3 to a single set of credentials</p></li><li style="list-style-type: disc"><p>Specify the table location explicitly in the table URL, that is, <code class="literal">s3n://&lt;access key&gt;:&lt;secret access key&gt;@&lt;bucket&gt;/&lt;path&gt;</code>
</p></li></ul></div><p>Then we can create a table referencing this data, as follows:</p><div class="informalexample"><pre class="programlisting">CREATE table remote_tweets (
created_at string,
tweet_id string,
text string,
in_reply_to string,
retweeted boolean,
user_id string,
place_id string
)  CLUSTERED BY(user_ID) into 64 BUCKETS
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
LOCATION 's3n://&lt;bucket-name&gt;/tweets'</pre></div><p>This can be an incredibly effective way of pulling S3 data into a local Hadoop cluster for processing.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note22"></a>Note</h3><p>In order to use AWS credentials in the URI of an S3 location regardless of how the parameters are passed, the secret and access keys must not contain <code class="literal">/</code>, <code class="literal">+</code>, <code class="literal">=</code>, or <code class="literal">\</code> characters. If necessary, a new set of credentials can be generated from the IAM console<a id="id826" class="indexterm"></a> at <a class="ulink" href="https://console.aws.amazon.com/iam/" target="_blank">https://console.aws.amazon.com/iam/</a>.</p></div><p>In theory, you<a id="id827" class="indexterm"></a> can just leave the data in the external table and refer to it<a id="id828" class="indexterm"></a> when needed to avoid WAN data transfer latencies (and costs), even though it often makes sense to pull the data into a local table and do future processing from there. If the table is partitioned, then you might find yourself retrieving a new partition each day, for example.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec137"></a>Hive on Elastic MapReduce</h3></div></div></div><p>On one level, using<a id="id829" class="indexterm"></a> Hive within Amazon Elastic MapReduce is just the<a id="id830" class="indexterm"></a> same as everything discussed in this chapter. You can create a persistent cluster, log in to the master node, and use the Hive CLI to create tables and submit queries. Doing all this will use the local storage on the EC2 instances for the table data.</p><p>Not surprisingly, jobs on EMR clusters can also refer to tables whose data is stored on S3 (or DynamoDB). And also not surprisingly, Amazon has made extensions to its version of Hive to make all this very seamless. It is quite simple from within an EMR job to pull data from a table stored in S3, process it, write any intermediate data to the EMR local storage, and then write the output results into S3, DynamoDB, or one of a growing list of other AWS services.</p><p>The pattern mentioned earlier <a id="id831" class="indexterm"></a>where new data is added to a new partition <a id="id832" class="indexterm"></a>directory for a table each day has proved very effective in S3; it is often the storage location of choice for large and incrementally growing datasets. There is a syntax difference when using EMR; instead of the MSCK command mentioned earlier, the command to update a Hive table with new data added to a partition directory is as follows:</p><div class="informalexample"><pre class="programlisting">ALTER TABLE &lt;table-name&gt; RECOVER PARTITIONS;</pre></div><p>Consult the EMR documentation<a id="id833" class="indexterm"></a> for the latest enhancements at <a class="ulink" href="http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-hive-additional-features.html" target="_blank">http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-hive-additional-features.html</a>. Also, consult the broader EMR documentation. In particular, the integration points with other AWS services is an area of rapid growth.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec55"></a>Extending HiveQL</h2></div></div><hr /></div><p>The HiveQL language can be<a id="id834" class="indexterm"></a> extended by means of plugins and third-party functions. In Hive, there are three types of functions characterized by the number of rows they take as input and produce as output:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>User Defined Functions</strong></span> (<span class="strong"><strong>UDFs</strong></span>): are<a id="id835" class="indexterm"></a> simpler functions that act on one row at a time.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>User Defined Aggregate Functions</strong></span> (<span class="strong"><strong>UDAFs</strong></span>): take multiple rows as input and <a id="id836" class="indexterm"></a>generate multiple rows as output. These are aggregate functions to be used in conjunction with a <code class="literal">GROUP BY</code> statement (similar to <code class="literal">COUNT()</code>, <code class="literal">AVG()</code>, <code class="literal">MIN()</code>, <code class="literal">MAX()</code>, and so on).</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>User Defined Table Functions</strong></span> (<span class="strong"><strong>UDTFs</strong></span>): take<a id="id837" class="indexterm"></a> multiple rows as input and generate a logical table comprised of multiple rows that can be used in join expressions.</p></li></ul></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip08"></a>Tip</h3><p>These APIs are provided only in Java. For other languages, it is possible to stream data through a user-defined script using the <code class="literal">TRANSFORM</code>, <code class="literal">MAP</code>, and <code class="literal">REDUCE</code> clauses that act as a frontend to Hadoop's streaming capabilities.</p></div><p>Two APIs are available to write UDFs. A simple API <code class="literal">org.apache.hadoop.hive.ql.exec.UDF</code> can be used for functions that take and return basic writable types. A richer API, which provides support for data types other than writable is available in the <code class="literal">org.apache.hadoop.hive.ql.udf.generic.GenericUDF</code> package. We'll now illustrate how <code class="literal">org.apache.hadoop.hive.ql.exec.UDF</code> can be used to implement a string to ID function similar to the one we used in <a class="link" href="#" linkend="ch05">Chapter 5</a>, <span class="emphasis"><em>Iterative Computation with Spark</em></span>, to map hashtags to integers in Pig. Building a UDF with this API only requires extending the UDF class and <a id="id838" class="indexterm"></a>writing an <code class="literal">evaluate()</code> method, as follows:</p><div class="informalexample"><pre class="programlisting">public class StringToInt extends UDF {
    public Integer evaluate(Text input) {
        if (input == null)
            return null;

         String str = input.toString();
         return str.hashCode();
    }
}</pre></div><p>The function takes a <code class="literal">Text</code> object as input and maps it to an integer value with the <code class="literal">hashCode()</code> method. The source code of this function can be found at <a class="ulink" href="https://github.com/learninghadoop2/book-examples/blob/master/ch7/udf/com/learninghadoop2/hive/udf/StringToInt.java" target="_blank">https://github.com/learninghadoop2/book-examples/blob/master/ch7/udf/com/learninghadoop2/hive/udf/StringToInt.java</a>.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip09"></a>Tip</h3><p>As noted in <a class="link" href="#" linkend="ch06">Chapter 6</a>, <span class="emphasis"><em>Data Analysis with Apache Pig</em></span>, a more robust hash function should be used in production.</p></div><p>We compile the class and archive it into a JAR file, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ javac -classpath $(hadoop classpath):/opt/cloudera/parcels/CDH/lib/hive/lib/* com/learninghadoop2/hive/udf/StringToInt.java </strong></span>
<span class="strong"><strong>$ jar cvf myudfs-hive.jar com/learninghadoop2/hive/udf/StringToInt.class</strong></span>
</pre></div><p>Before being able to use it, a UDF must be registered in Hive with the following commands:</p><div class="informalexample"><pre class="programlisting">ADD JAR myudfs-hive.jar;
CREATE TEMPORARY FUNCTION string_to_int AS 'com.learninghadoop2.hive.udf.StringToInt'; </pre></div><p>The <code class="literal">ADD JAR</code> statement <a id="id839" class="indexterm"></a>adds a JAR file to the distributed cache. The <code class="literal">CREATE TEMPORARY FUNCTION &lt;function&gt; AS &lt;class&gt;</code> statement registers a function in Hive that implements a given Java class. The function will be dropped once the Hive session is closed. As of Hive 0.13, it is possible to create permanent functions whose definition is kept in the metastore using <code class="literal">CREATE FUNCTION â€¦ </code>.</p><p>Once registered, <code class="literal">StringToInt</code> can be used in a query just like any other function. In the following example, we first extract a list of hashtags from the tweet's text by applying <code class="literal">regexp_extract</code>. Then, we use <code class="literal">string_to_int</code> to map each tag to a numerical ID:</p><div class="informalexample"><pre class="programlisting">SELECT unique_hashtags.hashtag, string_to_int(unique_hashtags.hashtag) AS tag_id FROM
    (
        SELECT regexp_extract(text, 
            '(?:\\s|\\A|^)[##]+([A-Za-z0-9-_]+)') as hashtag  
        FROM tweets 
        GROUP BY regexp_extract(text, 
        '(?:\\s|\\A|^)[##]+([A-Za-z0-9-_]+)')
) unique_hashtags GROUP BY unique_hashtags.hashtag, string_to_int(unique_hashtags.hashtag);</pre></div><p>Just as we did in the previous chapter, we can use the preceding query to create a lookup table:</p><div class="informalexample"><pre class="programlisting">CREATE TABLE lookuptable (tag string, tag_id bigint);
INSERT OVERWRITE TABLE lookuptable 
SELECT unique_hashtags.hashtag, 
    string_to_int(unique_hashtags.hashtag) as tag_id
FROM 
  (
    SELECT regexp_extract(text, 
        '(?:\\s|\\A|^)[##]+([A-Za-z0-9-_]+)') AS hashtag  
         FROM tweets 
         GROUP BY regexp_extract(text, 
            '(?:\\s|\\A|^)[##]+([A-Za-z0-9-_]+)')
   ) unique_hashtags 
GROUP BY unique_hashtags.hashtag, string_to_int(unique_hashtags.hashtag);</pre></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec56"></a>Programmatic interfaces</h2></div></div><hr /></div><p>In addition <a id="id840" class="indexterm"></a>to the <code class="literal">hive</code> and <code class="literal">beeline</code> command-line tools, it is possible to submit HiveQL queries to the system via the JDBC and Thrift programmatic interfaces. Support for ODBC was bundled in older versions of Hive, but as of Hive 0.12, it needs to be built from scratch. More information on this process can be found at <a class="ulink" href="https://cwiki.apache.org/confluence/display/Hive/HiveODBC" target="_blank">https://cwiki.apache.org/confluence/display/Hive/HiveODBC</a>.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec138"></a>JDBC</h3></div></div></div><p>A Hive client written <a id="id841" class="indexterm"></a>using <a id="id842" class="indexterm"></a>JDBC APIs looks exactly the same as a client program written for other database systems (for example MySQL). The following is a sample Hive client program using JDBC APIs. The source code for this <a id="id843" class="indexterm"></a>example can be found at <a class="ulink" href="https://github.com/learninghadoop2/book-examples/blob/master/ch7/clients/com/learninghadoop2/hive/client/HiveJdbcClient.java" target="_blank">https://github.com/learninghadoop2/book-examples/blob/master/ch7/clients/com/learninghadoop2/hive/client/HiveJdbcClient.java</a>.</p><div class="informalexample"><pre class="programlisting">public class HiveJdbcClient {
     private static String driverName = " org.apache.hive.jdbc.HiveDriver";
     
     // connection string
     public static String URL = "jdbc:hive2://localhost:10000";

     // Show all tables in the default database
     public static String QUERY = "show tables";

     public static void main(String[] args) throws SQLException {
          try {
               Class.forName (driverName);
          } 
          catch (ClassNotFoundException e) {
               e.printStackTrace();
               System.exit(1);
          }
          Connection con = DriverManager.getConnection (URL);
          Statement stmt = con.createStatement();
          
          ResultSet resultSet = stmt.executeQuery(QUERY);
          while (resultSet.next()) {
               System.out.println(resultSet.getString(1));
          }
    }
}</pre></div><p>The <code class="literal">URL</code> part is the JDBC URI that describes the connection end point. The format for establishing a remote connection is <code class="literal">jdbc:hive2:&lt;host&gt;:&lt;port&gt;/&lt;database&gt;</code>. Connections in embedded mode can be established by not specifying a host or port, like <code class="literal">jdbc:hive2://</code>.</p><p>
<code class="literal">hive</code> and <code class="literal">hive2</code> are the drivers to be used when connecting to <code class="literal">HiveServer</code> and <code class="literal">HiveServer2</code>. <code class="literal">QUERY</code> contains the HiveQL query to be executed.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip10"></a>Tip</h3><p>Hive's JDBC interface exposes only the default database. In order to access other databases, you need to reference them explicitly in the underlying queries using the <code class="literal">&lt;database&gt;.&lt;table&gt;</code> notation.</p></div><p>First we load the <code class="literal">HiveServer2</code> JDBC driver <code class="literal">org.apache.hive.jdbc.HiveDriver</code>.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip11"></a>Tip</h3><p>Use <code class="literal">org.apache.hadoop.hive.jdbc.HiveDriver</code> to connect to HiveServer.</p></div><p>Then, like with any other <a id="id844" class="indexterm"></a>JDBC program, we establish a connection to <code class="literal">URL</code> and use it to instantiate a <code class="literal">Statement</code> class. We execute <code class="literal">QUERY</code>, with no authentication, and store the output dataset into the <code class="literal">ResultSet</code> object. Finally, we scan <code class="literal">resultSet</code> and print its content to the command line.</p><p>Compile and execute the example with the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ javac HiveJdbcClient.java</strong></span>
<span class="strong"><strong>$ java -cp $(hadoop classpath):/opt/cloudera/parcels/CDH/lib/hive/lib/*:/opt/cloudera/parcels/CDH/lib/hive/lib/hive-jdbc.jar: com.learninghadoop2.hive.client.HiveJdbcClient</strong></span>
</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec139"></a>Thrift</h3></div></div></div><p>Thrift <a id="id845" class="indexterm"></a>provides lower-level access to Hive and has a number of advantages over the JDBC<a id="id846" class="indexterm"></a> implementation of HiveServer. Primarily, it allows multiple connections from the same client, and it allows programming languages other than Java to be used with ease. With HiveServer2, it is a less commonly used option but still worth mentioning for compatibility. A sample Thrift client implemented using the Java API can be found at <a class="ulink" href="https://github.com/learninghadoop2/book-examples/blob/master/ch7/clients/com/learninghadoop2/hive/client/HiveThriftClient.java" target="_blank">https://github.com/learninghadoop2/book-examples/blob/master/ch7/clients/com/learninghadoop2/hive/client/HiveThriftClient.java</a>. This client can be used to connect to HiveServer, but <a id="id847" class="indexterm"></a>due to protocol differences, the client won't work with HiveServer2.</p><p>In the example we define a <code class="literal">getClient()</code> method that takes as input the host and port of a HiveServer service and returns an instance of <code class="literal">org.apache.hadoop.hive.service.ThriftHive.Client</code>.</p><p>A client is obtained by first instantiating a socket connection, <code class="literal">org.apache.thrift.transport.TSocket</code>, to the HiveServer service, and by specifying a protocol, <code class="literal">org.apache.thrift.protocol.TBinaryProtocol</code>, to serialize and transmit data, as follows:</p><div class="informalexample"><pre class="programlisting">        TSocket transport = new TSocket(host, port);
        transport.setTimeout(TIMEOUT);
        transport.open();
        TBinaryProtocol protocol = new TBinaryProtocol(transport);
        client = new ThriftHive.Client(protocol);</pre></div><p>We call <code class="literal">getClient()</code> from the main method and use the client to execute a query against an instance of HiveServer running on localhost on port <code class="literal">11111</code>, as follows:</p><div class="informalexample"><pre class="programlisting">     public static void main(String[] args) throws Exception {
          Client client = getClient("localhost", 11111);
          client.execute("show tables");
          List&lt;String&gt; results = client.fetchAll();           
for (String result : results) {
System.out.println(result);           
} 
     }</pre></div><p>Make sure that <a id="id848" class="indexterm"></a>HiveServer is running on port <code class="literal">11111</code>, and if not, start an<a id="id849" class="indexterm"></a> instance with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sudo hive --service hiveserver -p 11111</strong></span>
</pre></div><p>Compile and execute the <code class="literal">HiveThriftClient.java</code> example with:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ javac $(hadoop classpath):/opt/cloudera/parcels/CDH/lib/hive/lib/* com/learninghadoop2/hive/client/HiveThriftClient.java</strong></span>
<span class="strong"><strong>$ java -cp $(hadoop classpath):/opt/cloudera/parcels/CDH/lib/hive/lib/*: com.learninghadoop2.hive.client.HiveThriftClient</strong></span>
</pre></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec57"></a>Stinger initiative</h2></div></div><hr /></div><p>Hive has remained <a id="id850" class="indexterm"></a>very successful and capable since its earliest releases, particularly in its ability to provide SQL-like processing on enormous datasets. But other technologies did not stand still, and Hive acquired a reputation of being relatively slow, particularly in regard to lengthy startup times on large jobs and its inability to give quick responses to conceptually simple queries.</p><p>These perceived limitations were less due to Hive itself and more a consequence of how translation of SQL queries into the MapReduce model has much built-in inefficiency when compared to other ways of implementing a SQL query. Particularly in regard to very large datasets, MapReduce saw lots of I/O (and consequently time) spent writing out the results of one MapReduce job just to have them read by another. As discussed in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Processing â€“ MapReduce and Beyond</em></span>, this is a major driver in the design of Tez, which can schedule jobs on a Hadoop cluster as a graph of tasks that does not require inefficient writes and reads between them.</p><p>The following is a query on the MapReduce framework versus Tez:</p><div class="informalexample"><pre class="programlisting">SELECT a.country, COUNT(b.place_id) FROM place a JOIN tweets b ON (a. place_id = b.place_id) GROUP BY a.country;</pre></div><p>The following figure contrasts the execution plan for the preceding query on the MapReduce framework versus Tez:</p><div class="mediaobject"><img src="graphics/5518OS_07_01.jpg" /><div class="caption"><p>Hive on MapReduce versus Tez</p></div></div><p>In plain MapReduce, two <a id="id851" class="indexterm"></a>jobs are created for the <code class="literal">GROUP BY</code> and <code class="literal">JOIN</code> clauses. The first job is composed of a set of MapReduce tasks that read data from the disk to carry out grouping. The reducers write intermediate results to the disk so that output can be synchronized. The mappers in the second job read the intermediate results from the disk as well as data from table b. The combined dataset is then passed to the reducer where shared keys are joined. Were we to execute an <code class="literal">ORDER BY</code> statement, this would have resulted in a third job and further MapReduce passes. The same query is executed on Tez as a single job by a single set of Map tasks that read data from the disk. I/O grouping and joining are pipelined across reducers.</p><p>Alongside these architectural limitations, there were quite a few areas around SQL language support that could also provide better efficiency, and in early 2013, the Stinger initiative was launched with an explicit goal of making Hive over 100 times as fast and with much richer SQL support. Hive 0.13 has all the features of the three phases of Stinger, resulting in a much more complete SQL dialect. Also, Tez is offered as an execution framework in addition to a MapReduce-based implementation atop YARN which is more efficient than previous implementations on Hadoop 1 MapReduce.</p><p>With Tez as the execution<a id="id852" class="indexterm"></a> engine, Hive is no longer limited to a series of linear MapReduce jobs and can instead build a processing graph where any given step can, for example, stream results to multiple sub-steps.</p><p>To take advantage of the Tez framework, there is a new <code class="literal">hive</code> variable setting:</p><div class="informalexample"><pre class="programlisting">set hive.execution.engine=tez;</pre></div><p>This setting relies on Tez<a id="id853" class="indexterm"></a> being installed on the cluster; it is available in source form from <a class="ulink" href="http://tez.apache.org" target="_blank">http://tez.apache.org</a> or in several distributions, though at the time of writing, not Cloudera.</p><p>The alternative value is <code class="literal">mr</code>, which uses the classic MapReduce model (atop YARN), so it is possible in a single installation to compare with the performance of Hive using Tez.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec58"></a>Impala</h2></div></div><hr /></div><p>Hive is not the<a id="id854" class="indexterm"></a> only product providing SQL-on-Hadoop capability. The second most widely used is likely Impala, announced in late 2012 and released in spring 2013. Though originally developed internally within Cloudera, its source code is periodically<a id="id855" class="indexterm"></a> pushed to an open source Git repository (<a class="ulink" href="https://github.com/cloudera/impala" target="_blank">https://github.com/cloudera/impala</a>).</p><p>Impala was created out of the same perception of Hive's weaknesses that led to the Stinger initiative.</p><p>Impala also took some inspiration from Google Dremel (<a class="ulink" href="http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36632.pdf" target="_blank">http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36632.pdf</a>) which was first openly described by a paper published in 2009. Dremel was built at Google to address the gap between the need for very fast queries on very large datasets and the high latency inherent in the existing MapReduce model underpinning Hive at the time. Dremel was a sophisticated approach to this problem that, rather than building mitigations atop MapReduce such as implemented by Hive, instead created a new service that accessed the same data stored in HDFS. Dremel also benefited from significant work to optimize the storage format of the data in a way that made it more amenable to very fast analytic queries.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec140"></a>The architecture of Impala</h3></div></div></div><p>The basic architecture has<a id="id856" class="indexterm"></a> three main components; the Impala daemons, the state store, and the clients. Recent versions have added additional components that improve the service, but we'll focus on the high-level architecture.</p><p>The Impala daemon (<code class="literal">impalad</code>) should be run on each host where a DataNode process is managing HDFS data. Note that <code class="literal">impalad</code> does not access the filesystem blocks through the full HDFS FileSystem API; instead, it uses a feature called short-circuit reads to make data access more efficient.</p><p>When a client submits a query, it can do so to any of the running <code class="literal">impalad</code> processes, and this one will become the coordinator for the execution of that query. The key aspect of Impala's performance is that for each query, it generates custom native code, which is then pushed to and executed by all the <code class="literal">impalad</code> processes on the system. This highly optimized code performs the query on the local data, and each <code class="literal">impalad</code> then returns its subset of the result set to the coordinator node, which performs the final data consolidation to produce the final result. This type of architecture should be familiar to anyone who has worked with any of the (usually commercial and expensive) <span class="strong"><strong>Massively Parallel Processing</strong></span> (<span class="strong"><strong>MPP</strong></span>) (the <a id="id857" class="indexterm"></a>term used for this type of shared scale-out architecture) data warehouse solutions available today. As the cluster runs, the state store daemon ensures that each <code class="literal">impalad</code> process is aware of all the others and provides a view of the overall cluster health.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec141"></a>Co-existing with Hive</h3></div></div></div><p>Impala, as a newer<a id="id858" class="indexterm"></a> product, tends to have a more restricted set of SQL data types and supports a more constrained dialect of SQL than Hive. It is, however, expanding this support with each new release. Refer to the Impala<a id="id859" class="indexterm"></a> documentation (<a class="ulink" href="http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/Impala/impala.html" target="_blank">http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH5/latest/Impala/impala.html</a>) to get an overview of the current level of support.</p><p>Impala supports the Hive metastore mechanism used by Hive to persistently store the metadata surrounding its table structure and storage. This means that on a cluster with an existing Hive setup, it should be immediately possible to use Impala as it will access the same metastore and therefore provide access to the same tables available in Hive.</p><p>But be warned that the differences in SQL dialect and data types might cause unexpected results when working in a combined Hive and Impala environment. Some queries might work on one but not the other, they might show very different performance characteristics (more on this later), or they might actually give different results. This last point might become apparent when using data types such as float and double that are simply treated differently in the underlying systems (Hive is implemented on Java while Impala is written in C++).</p><p>As of version 1.2, it supports <a id="id860" class="indexterm"></a>UDFs written both in C++ and Java, although C++ is strongly recommended as a much faster solution. Keep this in mind if you are looking to share custom functions between Hive and Impala.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec142"></a>A different philosophy</h3></div></div></div><p>When Impala was first released, its greatest benefit was in how it truly enabled what is often called<a id="id861" class="indexterm"></a> <span class="emphasis"><em>speed of thought</em></span> analysis. Queries could be returned sufficiently fast that an analyst could explore a thread of analysis in a completely interactive fashion without having to wait for minutes at a time for each query to complete. It's fair to say that most adopters of Impala were at times stunned by its performance, especially when compared to the version of Hive shipping at the time.</p><p>The Impala focus has remained mostly on these shorter queries, and this does impose some limitations on the system. Impala tends to be quite memory-heavy as it relies on in-memory processing to achieve much of its performance. If a query requires a dataset to be held in memory rather than being available on the executing node, then that query will simply fail in versions of Impala before 2.0.</p><p>Comparing the work on Stinger to Impala, it could be argued that Impala has a much stronger focus on excelling in the shorter (and arguably more common) queries that support interactive data analysis. Many business intelligence tools and services are now certified to directly run on Impala. The Stinger initiative has put less effort into making Hive just as fast in the area where Impala excels but has instead improved Hive (to varying degrees) for all workloads. Impala is still developing at a fast pace and Stinger has put additional momentum into Hive, so it is most likely wise to consider both products and determine which best meets the performance and functionality requirements of your projects and workflows.</p><p>It should also be kept in mind that there are competitive commercial pressures shaping the direction of Impala and Hive. Impala was created and is still driven by Cloudera, the most popular vendor of Hadoop distributions. The Stinger initiative, though contributed to by many companies as diverse as Microsoft (yes, really!) and Intel, was lead by Hortonworks, probably the second largest vendor of Hadoop distributions. The fact is that if you are using the Cloudera distribution of Hadoop, then some of the core features of Hive might be slower to arrive, whereas Impala will always be up-to-date. Conversely, if you use another distribution, you might get the latest Hive release, but that might either have an older Impala or, as is currently the case, you might have to download and install it yourself.</p><p>A similar situation has arisen with the Parquet and ORC file formats mentioned earlier. Parquet is preferred by Impala and developed by a group of companies led by Cloudera, while ORC is preferred by Hive and is championed by Hortonworks.</p><p>Unfortunately, the reality is that Parquet support is often very quick to arrive in the Cloudera distribution but less so in say the Hortonworks distribution, where the ORC file format is preferred.</p><p>These themes are a little concerning since, although competition in this space is a good thing, and arguably the announcement of Impala helped energize the Hive community, there is a greater risk that your choice of distribution might have a larger impact on the tools and file formats that will be fully supported, unlike in the past. Hopefully, the current situation is just an artifact of where we are in the development cycles of all these new and improved technologies, but do consider your choice of distribution carefully in relation to your SQL-on-Hadoop needs.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec143"></a>Drill, Tajo, and beyond</h3></div></div></div><p>You should also consider that SQL on Hadoop no longer only refers to Hive or Impala. Apache <a id="id862" class="indexterm"></a>Drill (<a class="ulink" href="http://drill.apache.org" target="_blank">http://drill.apache.org</a>) is a fuller implementation of the Dremel <a id="id863" class="indexterm"></a>model first described by Google. Although Impala implements the Dremel architecture across HDFS data, Drill looks to provide similar functionality across multiple data sources. It is still in its early stages, but if your needs are broader than what Hive or Impala provides, it might be worth considering.</p><p>Tajo (<a class="ulink" href="http://tajo.apache.org" target="_blank">http://tajo.apache.org</a>) is<a id="id864" class="indexterm"></a> another Apache project that seeks to be a full data warehouse <a id="id865" class="indexterm"></a>system on Hadoop data. With an architecture similar to that of Impala, it offers a much richer system with components such as multiple optimizers and ETL tools that are commonplace in traditional data warehouses but less frequently bundled in the Hadoop world. It has a much smaller user base but has been used by certain companies very successfully for a significant length of time, and might be worth considering if you need a fuller data warehousing solution.</p><p>Other products are also emerging in this space, and it's a good idea to do some research. Hive and Impala are awesome tools, but if you find that they don't meet your needs, then look aroundâ€”something else might.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec59"></a>Summary</h2></div></div><hr /></div><p>In its early days, Hadoop was sometimes erroneously seen as the latest supposed relational database killer. Over time, it has become more apparent that the more sensible approach is to view it as a complement to RDBMS technologies and that, in fact, the RDBMS community has developed tools such as SQL that are also valuable in the Hadoop world.</p><p>HiveQL is an implementation of SQL on Hadoop and was the primary focus of this chapter. In regard to HiveQL and its implementations, we covered the following topics:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>How HiveQL provides a logical model atop data stored in HDFS in contrast to relational databases where the table structure is enforced in advance</p></li><li style="list-style-type: disc"><p>How HiveQL supports many standard SQL data types and commands including joins and views</p></li><li style="list-style-type: disc"><p>The ETL-like features offered by HiveQL, including the ability to import data into tables and optimize the table structure through partitioning and similar mechanisms</p></li><li style="list-style-type: disc"><p>How HiveQL offers the ability to extend its core set of operators with user-defined code and how this contrasts to the Pig UDF mechanism</p></li><li style="list-style-type: disc"><p>The recent history of Hive developments, such as the Stinger initiative, that have seen Hive transition to an updated implementation that uses Tez</p></li><li style="list-style-type: disc"><p>The broader ecosystem around HiveQL that now includes products such as Impala, Tajo and Drill and how each of these focuses on specific areas in which to excel</p></li></ul></div><p>With Pig and Hive, we've introduced alternative models to process MapReduce data, but so far we've not looked at another question: what approaches and tools are required to actually allow this massive dataset being collected in Hadoop to remain useful and manageable over time? In the next chapter, we'll take a slight step up the abstraction hierarchy and look at how to manage the life cycle of this enormous data asset.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch08"></a>ChapterÂ 8.Â Data Lifecycle Management</h2></div></div></div><p>Our previous chapters were quite technology focused, describing particular tools or techniques and how they can be used. In this and the next chapter, we are going to take a more top-down approach whereby we will describe a problem space you are likely to encounter and then explore how to address it. In particular, we'll cover the following topics:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>What we mean by the term data life cycle management</p></li><li style="list-style-type: disc"><p>Why data life cycle management is something to think about</p></li><li style="list-style-type: disc"><p>The categories of tools that can be used to address the problem</p></li><li style="list-style-type: disc"><p>How to use these tools to build the first half of a Twitter sentiment analysis pipeline</p></li></ul></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec60"></a>What data lifecycle management is</h2></div></div><hr /></div><p>Data<a id="id866" class="indexterm"></a> doesn't exist only at a point in time. Particularly for long-running production workflows, you are likely to acquire a significant quantity of data in a Hadoop cluster. Requirements rarely stay static for long, so alongside new logic you might also see the format of that data change or require multiple data sources to be used to provide the dataset processed in your application. We use the term <span class="strong"><strong>data lifecycle management</strong></span> to describe an approach to handling the collection, storage, and transformation of data that ensures that data is where it needs to be, in the format it needs to be in, in a way that allows data and system evolution over time.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec144"></a>Importance of data lifecycle management</h3></div></div></div><p>If you build<a id="id867" class="indexterm"></a> data processing applications, you are by definition reliant on the data that is processed. Just as we consider the reliability of applications and systems, it becomes necessary to ensure that the data is also production-ready.</p><p>Data at some point needs to be ingested into Hadoop. It is one part of an enterprise and often has multiple points of integration with external systems. If the ingest of data coming from those systems is not reliable, then the impact on the jobs that process that data is often as disruptive as a major system failure. Data ingest becomes a critical component in its <a id="id868" class="indexterm"></a>own right. And when we say the ingest needs to be reliable, we don't just mean that data is arriving; it also has to be arriving in a format that is usable and through a mechanism that can handle evolution over time.</p><p>The problem with many of these issues is that they do not arise in a significant fashion until the flows are large, the system is critical, and the business impact of any problems is non-trivial. Ad hoc approaches that worked for a less critical dataflow often will simply not scale, but will be very painful to replace on a live system.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec145"></a>Tools to help</h3></div></div></div><p>But don't panic! There are a number of categories of tools<a id="id869" class="indexterm"></a> that can help with the data life cycle management problem. We'll give examples of the following three broad categories in this chapter:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Orchestration services</strong></span>: building<a id="id870" class="indexterm"></a> an ingest pipeline usually has multiple discrete stages, and we will use an orchestration tool to allow these to be described, executed, and managed</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Connectors</strong></span>: given<a id="id871" class="indexterm"></a> the importance of integration with external systems, we will look at how we can use connectors to simplify the abstractions provided by Hadoop storage</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>File formats</strong></span>: how <a id="id872" class="indexterm"></a>we store the data impacts how we manage format evolution over time, and several rich storage formats have ways of supporting this</p></li></ul></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec61"></a>Building a tweet analysis capability</h2></div></div><hr /></div><p>In <a id="id873" class="indexterm"></a>earlier chapters, we used various implementations of Twitter data analysis to describe several concepts. We will take this capability to a deeper level and approach it as a major case study.</p><p>In this chapter, we will build a data ingest pipeline, constructing a production-ready dataflow that is designed with reliability and future evolution in mind. </p><p>We'll build out the pipeline incrementally throughout the chapter. At each stage, we'll highlight what has changed but can't include full listings at each stage without trebling the size of<a id="id874" class="indexterm"></a> the chapter. The source code for this chapter, however, has every iteration in its full glory.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec146"></a>Getting the tweet data</h3></div></div></div><p>The <a id="id875" class="indexterm"></a>first thing we need to do is get the actual tweet data. As in previous examples, we can pass the <code class="literal">-j</code> and <code class="literal">-n</code> arguments to <code class="literal">stream.py</code> to dump JSON tweets to stdout:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ stream.py -j -n 10000 &gt; tweets.json</strong></span>
</pre></div><p>Since we have this tool that can create a batch of sample tweets on demand, we could start our ingest pipeline by having this job run on a periodic basis. But how?</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec147"></a>Introducing Oozie</h3></div></div></div><p>We could, of course, bang <a id="id876" class="indexterm"></a>rocks together and use something like cron for simple job scheduling, but recall that we want an ingest pipeline that is built with reliability in mind. So, we really want a scheduling tool that we can use to detect failures and otherwise respond to exceptional situations.</p><p>The tool <a id="id877" class="indexterm"></a>we will use <a id="id878" class="indexterm"></a>here is Oozie (<a class="ulink" href="http://oozie.apache.org" target="_blank">http://oozie.apache.org</a>), a workflow engine and scheduler built with a focus on the Hadoop ecosystem.</p><p>Oozie provides <a id="id879" class="indexterm"></a>a means to define a workflow as a series of nodes with configurable parameters and controlled transition from one node to the next. It is installed as part of the Cloudera QuickStart VM, and the main command-line client is, not surprisingly, called <code class="literal">oozie</code>.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note23"></a>Note</h3><p>We've tested the workflows in this chapter against version 5.0 of the Cloudera QuickStart VM, and at the time of writing Oozie in the latest version, 5.1, has some issues. There's nothing particularly version-specific in our workflows, however, so they should be compatible with any correctly working Oozie v4 implementation.</p></div><p>Though powerful and flexible, Oozie can take a little getting used to, so we'll give some examples and describe what we are doing along the way.</p><p>The most common node in an Oozie workflow is an action. It is within action nodes<a id="id880" class="indexterm"></a> that the steps of the workflow are actually executed; the other node types handle management of the workflow in terms of decisions, parallelism, and failure detection. Oozie has multiple types of actions that it can perform. One of these is the shell action, which can be used to execute any command on the system, such as native binaries, shell scripts, or any other command-line utility. Let's create a script to generate a file of tweets and copy this to HDFS:</p><div class="informalexample"><pre class="programlisting">set -e
source twitter.keys
python stream.py -j -n 500 &gt; /tmp/tweets.out
hdfs dfs -put /tmp/tweets.out /tmp/tweets/tweets.out
rm -f /tmp/tweets.out</pre></div><p>Note that the first line will cause the entire script to fail should any of the included commands fail. We use an environment file to provide the Twitter keys to our script in <code class="literal">twitter.keys</code>, which is of the following form:</p><div class="informalexample"><pre class="programlisting">export TWITTER_CONSUMER_KEY=&lt;value&gt;
export TWITTER_CONSUMER_SECRET=&lt;value&gt;
export TWITTER_ACCESS_KEY=&lt;value&gt; 
export TWITTER_ACCESS_SECRET=&lt;value&gt;</pre></div><p>Oozie uses XML to describe its workflows, usually stored in a file called <code class="literal">workflow.xml</code>. Let's walk through the definition for an Oozie workflow that calls a shell command.</p><p>The schema for an<a id="id881" class="indexterm"></a> Oozie workflow is called <span class="strong"><strong>workflow-app</strong></span>, and <a id="id882" class="indexterm"></a>we can give the workflow a specific name. This is useful when viewing job history in the CLI or Oozie web UI. In the examples in this book, we'll use an increasing version number to allow us to more easily separate the iterations within the source repository. This is how we give the workflow-app a specific name:</p><div class="informalexample"><pre class="programlisting">&lt;workflow-app xmlns="uri:oozie:workflow:0.4" name="v1"&gt;</pre></div><p>Oozie workflows are made up of a series of connected nodes, each of which represents a step in the process, and which are represented by XML nodes in the workflow definition. Oozie has a number of nodes that deal with the transition of the workflow from one step to the next. The first of these is the start node, which simply states the name of the first node to be executed as part of the workflow, as follows:</p><div class="informalexample"><pre class="programlisting">    &lt;start to="fs-node"/&gt;</pre></div><p>We then have the definition for the named start node. In this case, it is an action node, which is the generic node type for most Oozie nodes that actually perform some processing, as follows:</p><div class="informalexample"><pre class="programlisting">    &lt;action name="fs-node"&gt;</pre></div><p>Action is a<a id="id883" class="indexterm"></a> broad category of nodes, and we will typically then specialize it with the particular processing for this given node. In this case, we are using the fs node type, which allows us to perform filesystem operations:</p><div class="informalexample"><pre class="programlisting">    &lt;fs&gt;</pre></div><p>We want to ensure that the directory on HDFS to which we wish to copy the file of tweet data, exists, is empty, and has suitable permissions. We do this by trying to delete the directory if it exists, then creating it, and finally applying the required permissions, as follows:</p><div class="informalexample"><pre class="programlisting">    &lt;delete path="${nameNode}/tmp/tweets"/&gt;
    &lt;mkdir path="${nameNode}/tmp/tweets"/&gt;
    &lt;chmod path="${nameNode}/tmp/tweets" permissions="777"/&gt;
    &lt;/fs&gt;</pre></div><p>We'll see an alternative way of setting up directories later. After performing the functionality of the node, Oozie needs know how to proceed with the workflow. In most cases, this will comprise moving to another action node if this node was successful and aborting the workflow otherwise. This is specified by the next elements. The ok node gives the name of the node to which to transition if the execution was successful; the error node names the destination node for failure scenarios. Here's how the ok and fail nodes are used:</p><div class="informalexample"><pre class="programlisting">    &lt;ok to="shell-node"/&gt;
    &lt;error to="fail"/&gt;
    &lt;/action&gt;
    &lt;action name="shell-node"&gt;</pre></div><p>The second action node is again specialized with its specific processing type; in this case, we have a shell node:</p><div class="informalexample"><pre class="programlisting">&lt;shell xmlns="uri:oozie:shell-action:0.2"&gt;</pre></div><p>The shell action then has the Hadoop JobTracker and NameNode locations specified. Note that the actual values are given by variables; we'll explain where they come from later. The JobTracker and NameNode are specified as follows:</p><div class="informalexample"><pre class="programlisting">            &lt;job-tracker&gt;${jobTracker}&lt;/job-tracker&gt;
            &lt;name-node&gt;${nameNode}&lt;/name-node&gt;</pre></div><p>As mentioned in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Processing â€“ MapReduce and Beyond</em></span>, MapReduce uses multiple queues to provide support for different approaches to resource scheduling. The next element specifies the MapReduce queue to which the workflow should be submitted:</p><div class="informalexample"><pre class="programlisting">             &lt;configuration&gt;
                &lt;property&gt;
                    &lt;name&gt;mapred.job.queue.name&lt;/name&gt;
                    &lt;value&gt;${queueName}&lt;/value&gt;
                &lt;/property&gt;
             &lt;/configuration&gt;</pre></div><p>Now that the shell node is fully configured, we can specify the command to invoke, again via a variable, as follows:</p><div class="informalexample"><pre class="programlisting">              &lt;exec&gt;${EXEC}&lt;/exec&gt;</pre></div><p>The various steps of Oozie workflows are executed as MapReduce jobs. This shell action will, therefore, be executed as a specific task instance on a particular TaskTracker. We, therefore, need to specify which files need to be copied to the local working directory on the TaskTracker machine before the action can be performed. In this case, we need to copy the main shell script, the Python tweet generator, and the Twitter config file, as follows:</p><div class="informalexample"><pre class="programlisting">&lt;file&gt;${workflowRoot}/${EXEC}&lt;/file&gt;
&lt;file&gt;${workflowRoot}/twitter.keys&lt;/file&gt;
&lt;file&gt;${workflowRoot}/stream.py&lt;/file&gt;</pre></div><p>After closing the shell element, we again specify what to do depending on whether the action completed successfully or not. Because MapReduce is used for job execution, the majority of node types by definition have built-in retry and recovery logic, though this is not the case for shell nodes:</p><div class="informalexample"><pre class="programlisting">       &lt;/shell&gt;
      &lt;ok to="end"/&gt;
      &lt;error to="fail"/&gt;
&lt;/action&gt;</pre></div><p>If the workflow fails, let's just kill it in this case. The <code class="literal">kill</code> node type does exactly thatâ€” terminate the workflow from proceeding to any further steps, usually logging error messages along the way. Here's how the <code class="literal">kill</code> node type is used:</p><div class="informalexample"><pre class="programlisting">&lt;kill name="fail"&gt;
   &lt;message&gt;Shell action failed, error message[${wf:errorMessage(wf:lastErrorNode())}]&lt;/message&gt;
&lt;/kill&gt;</pre></div><p>The <code class="literal">end</code> node on the other hand simply halts the workflow and logs it as a successful completion within Oozie:</p><div class="informalexample"><pre class="programlisting">   &lt;end name="end"/&gt;
&lt;/workflow-app&gt;</pre></div><p>The obvious question is what the preceding variables represent and from where they get their concrete values. The preceding variables are examples of the Oozie Expression Language often referred to as EL.</p><p>Alongside the workflow definition file (<code class="literal">workflow.xml</code>), which describes the steps in the flow, we also need to create a configuration file that gives the specific values for a given execution of the workflow. This separation of functionality and configuration allows us to write workflows that can be used on different clusters, on different file locations, or with different variable values without having to recreate the workflow itself. By convention, this file is usually named <code class="literal">job.properties</code>. For the preceding workflow, here's a sample <code class="literal">job.properties</code> file.</p><p>Firstly, we specify the location of the JobTracker, the NameNode, and the MapReduce queue to which to submit the workflow. The following should work on the Cloudera 5.0 QuickStart VM, though in v 5.1 the hostname has been changed to <code class="literal">quickstart.cloudera</code>. The important thing is that the specified NameNode and JobTracker addresses need to be in the Oozie whitelistâ€”the local services on the VM are added automatically:</p><div class="informalexample"><pre class="programlisting">jobTracker=localhost.localdomain:8032
nameNode=hdfs://localhost.localdomain:8020
queueName=default</pre></div><p>Next, we set some values for where the workflow definitions and associated files can be found on the HDFS filesystem. Note the use of a variable representing the username running the job. This allows a single workflow to be applied to different paths depending on the submitting user, as follows:</p><div class="informalexample"><pre class="programlisting">tasksRoot=book
workflowRoot=${nameNode}/user/${user.name}/${tasksRoot}/v1
oozie.wf.application.path=${nameNode}/user/${user.name}/${tasksRoot}/v1</pre></div><p>Next, we name the command to be executed in the workflow as <code class="literal">${EXEC}</code>:</p><div class="informalexample"><pre class="programlisting">EXEC=gettweets.sh</pre></div><p>More complex workflows will require additional entries in the <code class="literal">job.properties</code> file; the preceding workflow is as simple as it gets.</p><p>The <code class="literal">oozie</code> command-line tool needs to know where the Oozie server is running. This can be added as an argument to every Oozie shell command, but that gets unwieldy very quickly. Instead, you can set the shell environment variable, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ export OOZIE_URL='http://localhost:11000/oozie'</strong></span>
</pre></div><p>After all that work, we can now actually run an Oozie workflow. Create a directory on HDFS as specified in the values in the <code class="literal">job.properties</code> file. In the preceding command, we'd be creating this as <code class="literal">book/v1</code> under our home directory on HDFS. Copy the <code class="literal">stream.py</code>, <code class="literal">gettweets.sh</code> and <code class="literal">twitter.properties</code> files to that directory; these are the files required to perform the actual execution of the shell command. Then, add the <code class="literal">workflow.xml</code> file to the same directory.</p><p>To run the workflow then, we do the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ oozie job -run -config &lt;path-to-job.properties&gt;</strong></span>
</pre></div><p>If submitted successfully, Oozie will print the job name to the screen. You can see the current status of this workflow with:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ oozie job -info &lt;job-id&gt;</strong></span>
</pre></div><p>You can also check the logs for the job:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ oozie job -log &lt;job-id&gt;  </strong></span>
</pre></div><p>In addition, all current and recent jobs can be viewed with:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ oozie jobs  </strong></span>
</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl3sec63"></a>A note on HDFS file permissions</h4></div></div></div><p>There is a <a id="id884" class="indexterm"></a>subtle aspect in the shell command that can catch the unwary. As an alternative to having the <code class="literal">fs</code> node, we could instead include a preparation element within the shell node to create the directory we need on the filesystem. It would look like the following:</p><div class="informalexample"><pre class="programlisting">&lt;prepare&gt;
     &lt;mkdir path="${nameNode}/tmp/tweets"/&gt;
&lt;/prepare&gt;</pre></div><p>The prepare stage is executed by the user who submitted the workflow, but since the actual script execution is performed on YARN, it is usually executed as the yarn user. You might hit a problem where the script generates the tweets, the <code class="literal">/tmp/tweets</code> directory is created on HDFS, but the script then fails to have permission to write to that directory. You can either resolve this through assigning permissions more precisely or, as shown earlier, you add a filesystem node to encapsulate the needed operations. We'll use a mixture of both techniques in this chapter; for non-shell nodes, we'll use prepare elements, particularly if the needed directory is manipulated only by that node. For cases where a shell node<a id="id885" class="indexterm"></a> is involved or where the created directories will be used across multiple nodes, we'll be safe and use the more explicit <code class="literal">fs</code> node.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl3sec64"></a>Making development a little easier</h4></div></div></div><p>It can<a id="id886" class="indexterm"></a> sometimes get awkward to manage the files and resources for an Oozie job during development. Some need to be on HDFS, while some need to be local, and changes to some files require changes to others. The easiest approach is often to develop or make changes in a complete clone of the workflow directory on the local filesystem and push changes from there to the similarly named directory in HDFS, not forgetting, of course, to ensure that all changes are under revision control! For operational execution of the workflow, the <code class="literal">job.properties</code> file is the only thing that needs to be on the local filesystem and, conversely, all the other files need to be on HDFS. Always remember this: it's all too easy to make changes to a local copy of a workflow, forget to push the changes to HDFS, and then be confused as to why the workflow isn't reflecting the changes.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl3sec65"></a>Extracting data and ingesting into Hive</h4></div></div></div><p>With <a id="id887" class="indexterm"></a>our data on HDFS, we can now extract the separate datasets for<a id="id888" class="indexterm"></a> tweets and users, and place data as in previous chapters. We can reuse <code class="literal">extract_for_hive.pig</code> to parse the raw tweet JSON into separate files, store them again on HDFS, and then follow up with a Hive step that ingests these new files into Hive tables for tweets, users, and places.</p><p>To do this within Oozie, we'll need to add two new nodes to our workflow, a Pig action for the first step and a Hive action for the second.</p><p>For our Hive action, we'll just create three external tables that point to the files generated by Pig. This would then allow us to follow our previously described model of ingesting into temporary or external tables and using HiveQL <code class="literal">INSERT</code> statements from there to insert into the operational, and often partitioned, tables. This <a id="id889" class="indexterm"></a>
<code class="literal">create.hql</code> script can be found at <a class="ulink" href="https://github.com/learninghadoop2/book-examples/blob/master/ch8/v2/hive/create.hql" target="_blank">https://github.com/learninghadoop2/book-examples/blob/master/ch8/v2/hive/create.hql</a> but is simply of the following form:</p><div class="informalexample"><pre class="programlisting">CREATE DATABASE IF NOT EXISTS twttr ;
USE twttr;
DROP TABLE IF EXISTS tweets;
CREATE EXTERNAL TABLE tweets (
...
) ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\u0001'
STORED AS TEXTFILE
LOCATION '${ingestDir}/tweets';

DROP TABLE IF EXISTS user;
CREATE EXTERNAL TABLE user (
...
) ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\u0001'
STORED AS TEXTFILE
LOCATION '${ingestDir}/users';

DROP TABLE IF EXISTS place;
CREATE EXTERNAL TABLE place (
...
) ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\u0001'
STORED AS TEXTFILE
LOCATION '${ingestDir}/places';</pre></div><p>Note that the file separator on each table is also explicitly set to match what we are outputting from Pig. In <a id="id890" class="indexterm"></a>addition to this, locations in both scripts are specified by variables for which we will provide concrete values in our <code class="literal">job.properties</code> file.</p><p>With the preceding <a id="id891" class="indexterm"></a>statements, we can create the Pig node for our workflow found in the source code as v2 of the pipeline. Much of the node definition looks similar to the shell node used previously, as we set the same configuration elements; also notice our use of the <code class="literal">prepare</code> element to create the needed output directory. We can create the Pig node for our workflow as shown in the following <code class="literal">action</code>:</p><div class="informalexample"><pre class="programlisting">&lt;action name="pig-node"&gt;
   &lt;pig&gt;
       &lt;job-tracker&gt;${jobTracker}&lt;/job-tracker&gt;
       &lt;name-node&gt;${nameNode}&lt;/name-node&gt;
       &lt;prepare&gt;
           &lt;delete path="${nameNode}/${outputDir}"/&gt;
           &lt;mkdir path="${nameNode}/${outputDir}"/&gt;
       &lt;/prepare&gt;
       &lt;configuration&gt;
           &lt;property&gt;
               &lt;name&gt;mapred.job.queue.name&lt;/name&gt;
               &lt;value&gt;${queueName}&lt;/value&gt;
           &lt;/property&gt;
       &lt;/configuration&gt;</pre></div><p>Similarly as with the shell command, we need to tell the Pig action the location of the actual Pig script. This is specified in the following <code class="literal">script</code> element:</p><div class="informalexample"><pre class="programlisting">          &lt;script&gt;${workflowRoot}/pig/extract_for_hive.pig&lt;/script&gt;</pre></div><p>We also need to<a id="id892" class="indexterm"></a> modify the command line used to invoke the Pig script to add <a id="id893" class="indexterm"></a>several parameters. The following elements do this; note the construction pattern wherein one element adds the actual parameter name and the next its value (we'll see an alternative mechanism for passing arguments in the next section):</p><div class="informalexample"><pre class="programlisting">       &lt;argument&gt;-param&lt;/argument&gt;
       &lt;argument&gt;inputDir=${inputDir}&lt;/argument&gt;
       &lt;argument&gt;-param&lt;/argument&gt;
       &lt;argument&gt;outputDir=${outputDir}&lt;/argument&gt;
  &lt;/pig&gt;</pre></div><p>Because we want to move from this step to the Hive node, we need to set the following elements appropriately:</p><div class="informalexample"><pre class="programlisting">       &lt;ok to="hive-node"/&gt;
       &lt;error to="fail"/&gt;
   &lt;/action&gt;</pre></div><p>The Hive action itself is a little different than the previous nodes; even though it starts in a similar fashion, it specifies the Hive action-specific namespace, as follows:</p><div class="informalexample"><pre class="programlisting">&lt;action name="hive-node"&gt;
       &lt;hive xmlns="uri:oozie:hive-action:0.2"&gt;
        &lt;job-tracker&gt;${jobTracker}&lt;/job-tracker&gt;
        &lt;name-node&gt;${nameNode}&lt;/name-node&gt;</pre></div><p>The Hive action needs many of the configuration elements used by Hive itself and, in most cases, we copy the <code class="literal">hive-site.xml</code> file into the workflow directory and specify its location, as shown in the following xml; note that this mechanism is not Hive-specific and can also be used for custom actions:</p><div class="informalexample"><pre class="programlisting">        &lt;job-xml&gt;${workflowRoot}/hive-site.xml&lt;/job-xml&gt;</pre></div><p>In addition, we might need to override some MapReduce default configuration properties, as shown in the following xml, where we specify that intermediate compression should be used for our job:</p><div class="informalexample"><pre class="programlisting">        &lt;configuration&gt;
             &lt;property&gt;
                 &lt;name&gt;mapred.compress.map.output&lt;/name&gt;
                 &lt;value&gt;true&lt;/value&gt;
             &lt;/property&gt;
        &lt;/configuration&gt;</pre></div><p>After configuring the Hive environment, we now specify the location of the Hive script:</p><div class="informalexample"><pre class="programlisting">        &lt;script&gt;${workflowRoot}/hive/create.hql&lt;/script&gt;</pre></div><p>We also have <a id="id894" class="indexterm"></a>to <a id="id895" class="indexterm"></a>provide the mechanism to pass arguments to the Hive script. But instead of building out the command line one component at a time, we'll add the <code class="literal">param</code> elements that map the name of a configuration element in the <code class="literal">job.properties</code> file to variables specified in the Hive script; this mechanism is also supported with Pig actions:</p><div class="informalexample"><pre class="programlisting">        &lt;param&gt;dbName=${dbName}&lt;/param&gt;
        &lt;param&gt;ingestDir=${ingestDir}&lt;/param&gt;
   &lt;/hive&gt;</pre></div><p>The Hive node then closes as the others, as follows:</p><div class="informalexample"><pre class="programlisting">     &lt;ok to="end"/&gt;
     &lt;error to="fail"/&gt;
&lt;/action&gt;</pre></div><p>We now need to put all this together to run the multistage workflow in Oozie. The full <code class="literal">workflow.xml</code> file<a id="id896" class="indexterm"></a>
<a id="id897" class="indexterm"></a> can be found at <a class="ulink" href="https://github.com/learninghadoop2/book-examples/tree/master/ch8/v2" target="_blank">https://github.com/learninghadoop2/book-examples/tree/master/ch8/v2</a> and the workflow is visualized in the following diagram:</p><div class="mediaobject"><img src="graphics/5518OS_08_01.jpg" /><div class="caption"><p>Data ingestion workflow v2</p></div></div><p>This <a id="id898" class="indexterm"></a>workflow <a id="id899" class="indexterm"></a>performs all the steps discussed before; it generates tweet data, extracts subsets of data via Pig, and then ingests these into Hive.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl3sec66"></a>A note on workflow directory structure</h4></div></div></div><p>We <a id="id900" class="indexterm"></a>now have quite a few files in our workflow directory and it is best to adopt some structure and naming conventions. For the current workflow, our directory on HDFS looks like the following:</p><div class="informalexample"><pre class="programlisting">/hive/
/hive/create.hql
/lib/
/pig/
/pig/extract_for_hive.pig
/scripts/
/scripts/gettweets.sh
/scripts/stream-json-batch.py
/scripts/twitter-keys
/hive-site.xml
/job.properties
/workflow.xml</pre></div><p>The model <a id="id901" class="indexterm"></a>we follow is to keep configuration files in the top-level directory but to keep files related to a given action type in dedicated subdirectories. Note that it is useful to have a <code class="literal">lib</code> directory even if empty, as some node types look for it.</p><p>With the preceding structure, the <code class="literal">job.properties</code> file for our combined job is now the following:</p><div class="informalexample"><pre class="programlisting">jobTracker=localhost.localdomain:8032
nameNode=hdfs://localhost.localdomain:8020
queueName=default
tasksRoot=book

workflowRoot=${nameNode}/user/${user.name}/${tasksRoot}/v2
oozie.wf.application.path=${nameNode}/user/${user.name}/${tasksRoot}/v2
oozie.use.system.libpath=true
EXEC=gettweets.sh
inputDir=/tmp/tweets
outputDir=/tmp/tweetdata
ingestDir=/tmp/tweetdata
dbName=twttr</pre></div><p>In the preceding code, we've fully updated the <code class="literal">workflow.xml</code> definition to include all the steps described so farâ€”including an initial <code class="literal">fs</code> node to create the required directory without worrying about user permissions.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl3sec67"></a>Introducing HCatalog</h4></div></div></div><p>If we<a id="id902" class="indexterm"></a> look <a id="id903" class="indexterm"></a>at our current workflow, there is inefficiency in how we use HDFS as the interface between Pig and Hive. We need to output the result of our Pig script onto HDFS, where the Hive script can then use it as the location of some new tables. What this highlights is that it is often very useful to have data stored in Hive, but this is limited, as few tools (primarily Hive) can access the Hive metastore and hence read and write such data. If we think about it, Hive has two main layers: its tools for accessing and manipulating its data plus the execution framework to run queries on that data.</p><p>The <a id="id904" class="indexterm"></a>HCatalog <a id="id905" class="indexterm"></a>subproject of Hive effectively provides an independent implementation of the first of these layersâ€”the means to access and manipulate data in the Hive metastore. HCatalog provides mechanisms for other tools, such as Pig and MapReduce, to natively read and write table-structured data that is stored on HDFS.</p><p>Remember, of course, that the data is stored on HDFS in one format or another. The Hive metastore provides the models to abstract these files into the relational table structure familiar from Hive. So when we say we are storing data in HCatalog, what we really mean is that we are storing data on HDFS in such a way that this data can then be exposed by table structures specified within the Hive metastore. Conversely, when we refer to Hive data, what we really mean is data whose metadata is stored in the Hive metastore, and which can be accessed by any metastore-aware tool, such as HCatalog.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch08lvl4sec01"></a>Using HCatalog</h5></div></div></div><p>The<a id="id906" class="indexterm"></a> HCatalog command-line tool is called<a id="id907" class="indexterm"></a> <span class="strong"><strong>hcat</strong></span> and will be preinstalled on the Cloudera QuickStart VMâ€”it is installed, in fact, with any version of Hive later than 0.11 inclusive.</p><p>The <code class="literal">hcat</code> utility doesn't have an interactive mode, so generally you will use it with explicit command-line arguments or by pointing it at a file of commands, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hcat â€“e "use default; show tables"</strong></span>
<span class="strong"><strong>$ hcat â€“f commands.hql</strong></span>
</pre></div><p>Though the hcat tool is useful and can be incorporated into scripts, the more interesting element of HCatalog for our purposes here is its integration with Pig. HCatalog defines a new Pig loader called <code class="literal">HCatLoader</code> and a storer called <code class="literal">HCatStorer</code>. As the names suggest, these allow Pig scripts to read from or write to Hive tables directly. We can use this mechanism to replace our previous Pig and Hive actions in our Oozie workflow with a single HCatalog-based Pig action that writes the output of the Pig job directly into our tables in Hive.</p><p>For clarity, we'll create new tables named <code class="literal">tweets_hcat</code>, <code class="literal">places_hcat</code>, and <code class="literal">users_hcat</code> into which we'll insert this data; note that these are no longer external tables:</p><div class="informalexample"><pre class="programlisting">CREATE TABLE tweets_hcatâ€¦ 
CREATE TABLE places_hcat â€¦
CREATE TABLE users_hcat â€¦</pre></div><p>Note that if we had these commands in a script file, we could use the hcat CLI tool to execute them, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hcat â€“f create.hql</strong></span>
</pre></div><p>The HCat CLI tool<a id="id908" class="indexterm"></a> does not, however, offer an interactive shell akin to the Hive CLI. We can now use our previous Pig script and need to only change the store commands, replacing the use of <code class="literal">PigStorage</code> with <code class="literal">HCatStorer</code>. Our updated Pig script, <code class="literal">extract_to_hcat.pig</code>, therefore includes <code class="literal">store</code> commands such as the following:</p><div class="informalexample"><pre class="programlisting">store tweets_tsv into 'twttr.tweets_hcat' using org.apache.hive.hcatalog.pig.HCatStorer();</pre></div><p>Note that the package name for the <code class="literal">HCatStorer</code> class has the <code class="literal">org.apache.hive.hcatalog</code> prefix; when HCatalog was in the Apache incubator, it used <code class="literal">org.apache.hcatalog</code> for its package prefix. This older form is now deprecated, and the new form that explicitly shows HCatalog as a subproject of Hive should be used instead.</p><p>With this <a id="id909" class="indexterm"></a>new Pig script, we can now replace our previous Pig and Hive action with an updated Pig action using HCatalog. This also requires the first usage of the Oozie sharelib, which we'll discuss in the next section. In our workflow definition, the <code class="literal">pig</code> element of this action will be defined as shown in the following xml and can be found as v3 of the pipeline in the source bundle; in v3, we've also added a utility Hive node to run before the Pig node to ensure that all necessary tables exist before the Pig script that requires them is executed.</p><div class="informalexample"><pre class="programlisting">&lt;pig&gt;
   &lt;job-tracker&gt;${jobTracker}&lt;/job-tracker&gt;
   &lt;name-node&gt;${nameNode}&lt;/name-node&gt;
   &lt;job-xml&gt;${workflowRoot}/hive-site.xml&lt;/job-xml&gt;
    &lt;configuration&gt;
          &lt;property&gt;
              &lt;name&gt;mapred.job.queue.name&lt;/name&gt;
              &lt;value&gt;${queueName}&lt;/value&gt;
          &lt;/property&gt;
          &lt;property&gt;
             &lt;name&gt;oozie.action.sharelib.for.pig&lt;/name&gt;
             &lt;value&gt;pig,hcatalog&lt;/value&gt;
          &lt;/property&gt;
    &lt;/configuration&gt;
    &lt;script&gt;${workflowRoot}/pig/extract_to_hcat.pig
    &lt;/script&gt;
    &lt;argument&gt;-param&lt;/argument&gt;
    &lt;argument&gt;inputDir=${inputDir}&lt;/argument&gt;
&lt;/pig&gt;</pre></div><p>The two <a id="id910" class="indexterm"></a>changes of note are the addition of the explicit reference to the <code class="literal">hive-site.xml</code> file; this is required by HCatalog, and the new configuration element that tells Oozie to include the required <code class="literal">HCatalog</code> JARs.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl3sec68"></a>The Oozie sharelib</h4></div></div></div><p>That <a id="id911" class="indexterm"></a>last addition touched on an important aspect of Oozie we've not mentioned thus far: the Oozie <code class="literal">sharelib</code>. When <a id="id912" class="indexterm"></a>Oozie runs all its various action types, it requires multiple JARs to access Hadoop and to invoke various tools, such as Hive and Pig. As part of the Oozie installation, a large number of dependent JARs have been placed on HDFS to be used by Oozie and its various action types: this is the Oozie <code class="literal">sharelib</code>.</p><p>For most usages of Oozie, it's enough to know the <code class="literal">sharelib</code> exists, usually under <code class="literal">/user/oozie/share/lib on HDFS</code>, and when, as in the previous example, some explicit configuration values need to be added. When using a Pig action, the Pig JARs will automatically get picked up, but when the Pig script uses something like HCatalog, then this dependency will not be explicitly known to Oozie.</p><p>The Oozie CLI allows manipulation of the <code class="literal">sharelib</code>, though the scenarios where this will be required are outside of the scope of this book. The following command can be useful though to see which components are included in the Oozie <code class="literal">sharelib</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ oozie admin -shareliblist</strong></span>
</pre></div><p>The following command is useful to see the individual JARs comprising a particular component within the <code class="literal">sharelib</code>, in this case HCatalog:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ oozie admin -shareliblist hcat</strong></span>
</pre></div><p>These commands can be useful to verify that the required JARs are being included and to see which specific versions are being used.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl3sec69"></a>HCatalog and partitioned tables</h4></div></div></div><p>If you rerun<a id="id913" class="indexterm"></a> the previous workflow a second time, it will fail; dig into the logs, and you will see HCatalog complaining that it cannot write to a table that already contains data. This is a current limitation of HCatalog; it views tables and partitions within tables as immutable by default. Hive, on the other hand, will add new data to a table or partition; its default view of a table is that it is mutable.</p><p>Upcoming changes to Hive and HCatalog will see the support of a new table property that will control this behavior in either tool; for example, the following added to a table definition would allow table appends as supported in Hive today:</p><div class="informalexample"><pre class="programlisting">TBLPROPERTIES("immutable"="false")</pre></div><p>This is<a id="id914" class="indexterm"></a> currently not available in the shipping version of Hive and HCatalog, however. For us to have a workflow that adds more and more data into our tables, we therefore need to create a new partition for each new run of the workflow. We've made these changes in v4 of our pipeline, where we first recreate the tables with an integer partition key, as follows:</p><div class="informalexample"><pre class="programlisting">CREATE  TABLE tweets_hcat (
â€¦)
PARTITIONED BY (partition_key int)
ROW FORMAT DELIMITED
  FIELDS TERMINATED BY '\u0001'
STORED AS SEQUENCEFILE;

CREATE  TABLE `places_hcat`(
â€¦ )
partitioned by(partition_key int)
ROW FORMAT DELIMITED
  FIELDS TERMINATED BY '\u0001'
STORED AS SEQUENCEFILE
TBLPROPERTIES("immutable"="false") ;

CREATE  TABLE `users_hcat`(
â€¦)
partitioned by(partition_key int)
ROW FORMAT DELIMITED
  FIELDS TERMINATED BY '\u0001'
STORED AS SEQUENCEFILE
TBLPROPERTIES("immutable"="false") ;</pre></div><p>The Pig <code class="literal">HCatStorer</code> takes an optional partition definition and we modify the <code class="literal">store</code> statements in our Pig script accordingly; for example:</p><div class="informalexample"><pre class="programlisting">store tweets_tsv into 'twttr.tweets_hcat' 
using org.apache.hive.hcatalog.pig.HCatStorer(
'partition_key=$partitionKey');</pre></div><p>We then modify our Pig action in the <code class="literal">workflow.xml</code> file to include this additional parameter:</p><div class="informalexample"><pre class="programlisting">&lt;script&gt;${workflowRoot}/pig/extract_to_hcat.pig&lt;/script&gt;
          &lt;param&gt;inputDir=${inputDir}&lt;/param&gt;
          &lt;param&gt;partitionKey=${partitionKey}&lt;/param&gt;</pre></div><p>The question is then <a id="id915" class="indexterm"></a>how we pass this partition key to the workflow. We could specify it in the <code class="literal">job.properties</code> file, but by doing so we would hit the same problem with trying to write to an existing partition on the next re-run.</p><div class="mediaobject"><img src="graphics/5518OS_08_02.jpg" /><div class="caption"><p>Ingestion workflow v4</p></div></div><p>For now, we'll pass this as an explicit argument to the invocation of the Oozie CLI and explore better <a id="id916" class="indexterm"></a>ways to do this later:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ oozie job â€“run â€“config v4/job.properties â€“DpartitionKey=12345</strong></span>
</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note24"></a>Note</h3><p>Note that a consequence of this behavior is that rerunning an HCat workflow with the same arguments will fail. Be aware of this when testing workflows or playing with the sample code from this book.</p></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec148"></a>Producing derived data</h3></div></div></div><p>Now that<a id="id917" class="indexterm"></a> we have our main data pipeline established, there is most likely a series of actions that we wish to take after we add each new<a id="id918" class="indexterm"></a> additional dataset. As a simple example, note that with our previous mechanism of adding each set of user data to a separate partition, the <code class="literal">users_hcat</code> table will contain users multiple times. Let's create a new table for unique users and regenerate this each time we add new user data.</p><p>Note that given the aforementioned limitations of HCatalog, we'll use a Hive action for this purpose, as we need to replace the data in a table.</p><p>First, we'll create a new table for unique user information, as follows:</p><div class="informalexample"><pre class="programlisting">CREATE TABLE IF NOT EXISTS `unique_users`(
  `user_id` string ,
  `name` string ,
  `description` string ,
  `screen_name` string )
ROW FORMAT DELIMITED
  FIELDS TERMINATED BY '\t'
STORED AS sequencefile ;</pre></div><p>In this table, we'll only store the attributes of a user that either never change (ID) or change rarely (the screen name, and so on). We can then write a simple Hive statement to populate this table from the full <code class="literal">users_hcat</code> table:</p><div class="informalexample"><pre class="programlisting">USE twttr;
INSERT OVERWRITE TABLE unique_users
SELECT DISTINCT user_id, name, description, screen_name
FROM users_hcat;</pre></div><p>We can then add an additional Hive action node that comes after our previous Pig node in the workflow. When doing this, we discover that our pattern of simply giving nodes names such as hive-node is a really bad idea, as we now have two Hive-based nodes. In v5 of the <a id="id919" class="indexterm"></a>workflow, we add this new node and also change our nodes to have more descriptive <a id="id920" class="indexterm"></a>names:</p><div class="mediaobject"><img src="graphics/5518OS_08_03.jpg" /><div class="caption"><p>Ingestion workflow v5</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl3sec70"></a>Performing multiple actions in parallel</h4></div></div></div><p>Our workflow<a id="id921" class="indexterm"></a> has two types of activity: initial setup with the nodes that initialize the filesystem and Hive tables, and the functional nodes that perform actual processing. If we look at the two setup nodes we have been using, it is obvious that they are quite distinct and not interdependent. We can therefore take advantage of an Oozie feature called <code class="literal">fork</code> and <code class="literal">join</code> nodes to execute these actions in parallel. The start of our <code class="literal">workflow.xml</code> file now becomes:</p><div class="informalexample"><pre class="programlisting"> &lt;start to="setup-fork-node"/&gt;</pre></div><p>The Oozie <code class="literal">fork</code> node<a id="id922" class="indexterm"></a> contains a number of <code class="literal">path</code> elements, each of which specifies a starting node. Each of these will be launched in parallel:</p><div class="informalexample"><pre class="programlisting">&lt;fork name="setup-fork-node"&gt;
   &lt;path start="setup-filesystem-node" /&gt;
   &lt;path start="create-tables-node" /&gt;
&lt;/fork&gt;</pre></div><p>Each of the <a id="id923" class="indexterm"></a>specified action nodes is no different from any we have used previously. An action node can link to a series of other nodes; the only requirement is that each parallel series of actions must end with a transition to the <code class="literal">join</code> node<a id="id924" class="indexterm"></a> associated with the <code class="literal">fork</code> node, as follows:</p><div class="informalexample"><pre class="programlisting">    &lt;action name="setup-filesystem-node"&gt;
â€¦
        &lt;ok to="setup-join-node"/&gt;
        &lt;error to="fail"/&gt;
    &lt;/action&gt;
    &lt;action name="create-tables-node"&gt;
â€¦
        &lt;ok to="setup-join-node"/&gt;
        &lt;error to="fail"/&gt;
    &lt;/action&gt;</pre></div><p>The <code class="literal">join</code> node itself acts as the point of coordination; any workflow that has completed will wait until all the paths specified in the <code class="literal">fork</code> node reach this point. At that point, the workflow continues at the node specified within the <code class="literal">join</code> node. Here's how the <code class="literal">join</code> node is used:</p><div class="informalexample"><pre class="programlisting">&lt;join name="create-join-node" to="gettweets-node"/&gt;</pre></div><p>In the preceding code we omitted the action definitions for space purposes, but the full workflow definition is in v6:</p><div class="mediaobject"><img src="graphics/5518OS_08_05.jpg" /><div class="caption"><p>Ingestion workflow v6</p></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl3sec71"></a>Calling a subworkflow</h4></div></div></div><p>Though<a id="id925" class="indexterm"></a> the <code class="literal">fork/join</code> mechanism makes the process of parallel actions more efficient, it does still add significant verbosity if we include it in our main <code class="literal">workflow.xml</code> definition. Conceptually, we have a series of actions that are performing related tasks required by our workflow but not necessarily part of it. For this and similar cases, Oozie offers the ability to invoke a subworkflow. The parent workflow will execute the child and wait for it to complete, with the ability to pass configuration elements from one workflow to the other.</p><p>The child workflow will be a full workflow in its own right, usually stored in a directory on HDFS with all the usual structure we expect for a workflow, the main <code class="literal">workflow.xml</code> file, and any required Hive, Pig, or similar files.</p><p>We can create <a id="id926" class="indexterm"></a>a new directory on HDFS called setup-workflow, and in this create the files required only for our filesystem and Hive creation actions. The subworkflow configuration file will look like the following:</p><div class="informalexample"><pre class="programlisting">&lt;workflow-app xmlns="uri:oozie:workflow:0.4" name="create-workflow"&gt;
    &lt;start to="setup-fork-node"/&gt;
    &lt;fork name="setup-fork-node"&gt;
          &lt;path start="setup-filesystem-node" /&gt;
      &lt;path start="create-tables-node" /&gt;
    &lt;/fork&gt;
    &lt;action name="setup-filesystem-node"&gt;
    â€¦
    &lt;/action&gt;
    &lt;action name="create-tables-node"&gt;
    â€¦
    &lt;/action&gt;
    &lt;join name="create-join-node" to="end"/&gt;
    &lt;kill name="fail"&gt;
        &lt;message&gt;Action failed, error message[${wf:errorMessage(wf:lastErrorNode())}]&lt;/message&gt;
    &lt;/kill&gt;
    &lt;end name="end"/&gt;
&lt;/workflow-app&gt;</pre></div><p>With this subworkflow defined, we then modify the first nodes of our main workflow to use a subworkflow node, as in the following:</p><div class="informalexample"><pre class="programlisting">    &lt;start to="create-subworkflow-node"/&gt;
    &lt;action name="create-subworkflow-node"&gt;
        &lt;sub-workflow&gt;
            &lt;app-path&gt;${subWorkflowRoot}&lt;/app-path&gt;
            &lt;propagate-configuration/&gt;
        &lt;/sub-workflow&gt;
        &lt;ok to="gettweets-node"/&gt;
        &lt;error to="fail"/&gt;
    &lt;/action&gt;</pre></div><p>We will specify the <code class="literal">subWorkflowPath</code> in the <code class="literal">job.properties</code> of our parent workflow, and the <code class="literal">propagate-configuration</code> element <a id="id927" class="indexterm"></a>will pass the configuration of the parent workflow to the child.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl3sec72"></a>Adding global settings</h4></div></div></div><p>By <a id="id928" class="indexterm"></a>extracting utility nodes into subworkflows, we can significantly reduce clutter and complexity in our main workflow definition. In v7 of our ingest pipeline, we'll make one additional simplification and add a global configuration section, as in the following:</p><div class="informalexample"><pre class="programlisting">&lt;workflow-app xmlns="uri:oozie:workflow:0.4" name="v7"&gt;
    &lt;global&gt;
            &lt;job-tracker&gt;${jobTracker}&lt;/job-tracker&gt;
            &lt;name-node&gt;${nameNode}&lt;/name-node&gt;
            &lt;job-xml&gt;${workflowRoot}/hive-site.xml&lt;/job-xml&gt;
            &lt;configuration&gt;
                &lt;property&gt;
                    &lt;name&gt;mapred.job.queue.name&lt;/name&gt;
                    &lt;value&gt;${queueName}&lt;/value&gt;
                &lt;/property&gt;
            &lt;/configuration&gt;
&lt;/global&gt;
&lt;start to="create-subworkflow-node"/&gt;</pre></div><p>By adding this global configuration section, we remove the need to specify any of these values in the Hive and Pig nodes in the remaining workflow (note that currently the shell node does not support the global configuration mechanism). This can dramatically simplify some of our nodes; for example, our Pig node is now as follows:</p><div class="informalexample"><pre class="programlisting">&lt;action name="hcat-ingest-node"&gt;
   &lt;pig&gt;
     &lt;configuration&gt;
       &lt;property&gt;
         &lt;name&gt;oozie.action.sharelib.for.pig&lt;/name&gt;
         &lt;value&gt;pig,hcatalog&lt;/value&gt;
         &lt;/property&gt;
       &lt;/configuration&gt;
       &lt;script&gt;${workflowRoot}/pig/extract_to_hcat.pig&lt;/script&gt;
          &lt;param&gt;inputDir=${inputDir}&lt;/param&gt;
          &lt;param&gt;dbName=${dbName}&lt;/param&gt;
          &lt;param&gt;partitionKey=${partitionKey}&lt;/param&gt;
   &lt;/pig&gt;
   &lt;ok to="derived-data-node"/&gt;
   &lt;error to="fail"/&gt;
&lt;/action&gt;</pre></div><p>As can be seen, we <a id="id929" class="indexterm"></a>can add additional configuration elements, or indeed override those specified in the global section, resulting in a much clearer action definition that focuses only on the information specific to the action in question. Our workflow v7 has had both a global section added as well as the addition of the subworkflow, and this makes a significant improvement in the workflow readability:</p><div class="mediaobject"><img src="graphics/5518OS_08_06.jpg" /><div class="caption"><p>Ingestion workflow v7</p></div></div></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec62"></a>Challenges of external data</h2></div></div><hr /></div><p>When we<a id="id930" class="indexterm"></a> rely on external data to drive our application, we are implicitly dependent on the quality and stability of that data. This is, of course, true for any data, but when the data is generated by an external source over which we do not have control, the risks are most likely higher. Regardless, when building what we expect to be reliable applications on top of such data feeds, and especially when our data volumes grow, we need to think about how to mitigate these risks.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec149"></a>Data validation</h3></div></div></div><p>We use <a id="id931" class="indexterm"></a>the general term data validation to refer to the act of ensuring that incoming data complies with our expectations and potentially applying normalization to modify it accordingly or to even delete malformed or corrupt input. What this actually involves will be very application-specific. In some cases, the important thing is ensuring the system only ingests data that conforms to a given definition of accurate or clean. For our tweet data, we don't care about every single record and could very easily adopt a policy such as dropping records that don't have values in particular fields we care about. For other applications, however, it is imperative to capture every input record, and this might drive the implementation of logic to reformat every record to make sure it complies with the requirements. In yet other cases, only correct records will be ingested, but the rest, instead of being discarded, might be stored elsewhere for later analysis.</p><p>The bottom line is that trying to define a generic approach to data validation is vastly beyond the scope of this chapter.</p><p>However, we can offer some thoughts on where in the pipeline to incorporate various types of validation logic.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl3sec73"></a>Validation actions</h4></div></div></div><p>Logic to <a id="id932" class="indexterm"></a>do any necessary validation or cleanup can be incorporated directly into other actions. A shell node running a script to gather data can have commands added to handle malformed records differently. Pig and Hive actions that load data into tables can either perform filtering on ingest (easier done in Pig) or add caveats when copying data from an ingest table to the operational store.</p><p>There is an argument though for the addition of a validation node into the workflow, even if initially it performs no actual logic. This could, for instance, be a Pig action that reads the data, applies the validation, and writes the validated data to a new location to be read by follow-on nodes. The advantage here is that we can later update the validation logic without altering our other actions, which should reduce the risk of accidentally breaking the rest of the pipeline and also make nodes more cleanly defined in terms of responsibilities. The natural extension of this train of thought is that a new subworkflow for validation is most likely a good model as well, as it not only provides separation of responsibilities, but also makes the validation logic easier to test and update.</p><p>The obvious <a id="id933" class="indexterm"></a>disadvantage of this approach is that it adds additional processing and another cycle of reading the data and writing it all again. This is, of course, directly working against one of the advantages we highlighted when considering the use of HCatalog from Pig.</p><p>In the end, it will come down to a trade-off of performance against workflow complexity and maintainability. When considering how to perform validation and just what that means for your workflow, take all these elements into account before deciding on an implementation.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec150"></a>Handling format changes</h3></div></div></div><p>We can't declare <a id="id934" class="indexterm"></a>victory just because we have data flowing into our system and are confident the data is sufficiently validated. Particularly when the data comes from an external source we have to think about how the structure of the data might change over time.</p><p>Remember that systems such as Hive only apply the table schema when the data is being read. This is a huge benefit in enabling flexible data storage and ingest, but can lead to user-facing queries or workloads failing suddenly when the ingested data no longer matches the queries being executed against it. A relational database, which applies schemas on write, would not even allow such data to be ingested into the system.</p><p>The obvious approach to handling changes made to the data format would be to reprocess existing data into the new format. Though this is tractable on smaller datasets, it quickly becomes infeasible on the sort of volumes seen in large Hadoop clusters.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec151"></a>Handling schema evolution with Avro</h3></div></div></div><p>Avro has <a id="id935" class="indexterm"></a>some features with respect to its integration with Hive that help us with this problem. If we take our table for tweets data, we could represent the structure of a tweet record by the following Avro schema:</p><div class="informalexample"><pre class="programlisting">{
 "namespace": "com.learninghadoop2.avrotables",
 "type":"record",
 "name":"tweets_avro",
 "fields":[
   {"name": "created_at", "type": ["null" ,"string"]},
   {"name": "tweet_id_str", "type": ["null","string"]},
   {"name": "text","type":["null","string"]},
   {"name": "in_reply_to", "type": ["null","string"]},
   {"name": "is_retweeted", "type": ["null","string"]},
   {"name": "user_id", "type": ["null","string"]},
  {"name": "place_id", "type": ["null","string"]}
  ]
}</pre></div><p>Create the preceding schema in a file called <code class="literal">tweets_avro.avsc</code>â€”this is the standard file extension for Avro schemas. Then, place it on HDFS; we like to have a common location for schema files such as <code class="literal">/schema/avro</code>.</p><p>With this definition, we can now create a Hive table that uses this schema for its table specification, as follows:</p><div class="informalexample"><pre class="programlisting">CREATE TABLE tweets_avro
PARTITIONED BY ( `partition_key` int)
ROW FORMAT SERDE
  'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
WITH SERDEPROPERTIES (
'avro.schema.url'='hdfs://localhost.localdomain:8020/schema/avro/tweets_avro.avsc'
)
STORED AS INPUTFORMAT
  'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
OUTPUTFORMAT
  'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat';</pre></div><p>Then, look <a id="id936" class="indexterm"></a>at the table definition from within Hive (or HCatalog, which also supports such definitions):</p><div class="informalexample"><pre class="programlisting">describe tweets_avro
OK
created_at              string                  from deserializer
tweet_id_str            string                  from deserializer
text                    string                  from deserializer
in_reply_to             string                  from deserializer
is_retweeted            string                  from deserializer
user_id                 string                  from deserializer
place_id                string                  from deserializer
partition_key           int                   None</pre></div><p>We can also use this table like any other, for example, to copy the data from partition 3 from the non-Avro table into the Avro table, as follows:</p><div class="informalexample"><pre class="programlisting">SET hive.exec.dynamic.partition.mode=nonstrict
INSERT INTO TABLE tweets_avro
PARTITION (partition_key)
SELECT  FROM tweets_hcat</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note25"></a>Note</h3><p>Just as in previous examples, if Avro dependencies are not present in the classpath, we need to add the Avro MapReduce JAR to our environment before being able to select from the table.</p></div><p>We now have a new tweets table specified by an Avro schema; so far it just looks like other tables. But the real benefits for our purposes in this chapter are in how we can use the Avro mechanism to handle schema evolution. Let's add a new field to our table schema, as follows:</p><div class="informalexample"><pre class="programlisting">{
 "namespace": "com.learninghadoop2.avrotables",
 "type":"record",
 "name":"tweets_avro",
 "fields":[
   {"name": "created_at", "type": ["null" ,"string"]},
   {"name": "tweet_id_str", "type": ["null","string"]},
   {"name": "text","type":["null","string"]},
   {"name": "in_reply_to", "type": ["null","string"]},
   {"name": "is_retweeted", "type": ["null","string"]},
   {"name": "user_id", "type": ["null","string"]},
  {"name": "place_id", "type": ["null","string"]},
  {"name": "new_feature", "type": "string", "default": "wow!"}
  ]
}</pre></div><p>With this new<a id="id937" class="indexterm"></a> schema in place, we can validate that the table definition has also been updated, as follows:</p><div class="informalexample"><pre class="programlisting">describe tweets_avro;
OK
created_at              string                  from deserializer
tweet_id_str            string                  from deserializer
text                    string                  from deserializer
in_reply_to             string                  from deserializer
is_retweeted            string                  from deserializer
user_id                 string                  from deserializer
place_id                string                  from deserializer
new_feature             string                  from deserializer
partition_key           int                     None</pre></div><p>Without adding any new data, we can run queries on the new field that will return the default value for our existing data, as follows:</p><div class="informalexample"><pre class="programlisting">SELECT new_feature FROM tweets_avro LIMIT 5;
...
OK
wow!
wow!
wow!
wow!
wow!</pre></div><p>Even more impressive is the fact that the new column doesn't need to be added at the end; it can be anywhere in the record. With this mechanism, we can now update our Avro schemas to represent the new data structure and see these changes automatically reflected in our Hive table definitions. Any queries that refer to the new column will retrieve the default value for all our existing data that does not have that field present.</p><p>Note that the default mechanism we are using here is core to Avro and is not specific to Hive. Avro is a very powerful and flexible format that has applications in many areas and is definitely worth deeper examination than we are giving it here.</p><p>Technically, what this provides us with is forward compatibility. We can make changes to our table schema and have all our existing data remain automatically compliant with the new structure we can't, however, continue to ingest data of the old format into the updated tables since the mechanism does not provide backward compatibility:</p><div class="informalexample"><pre class="programlisting">INSERT INTO TABLE tweets_avro 
PARTITION (partition_key)
SELECT * FROM tweets_hcat;
FAILED: SemanticException [Error 10044]: Line 1:18 Cannot insert into target table because column number/types are different 'tweets_avro': Table insclause-0 has 8 columns, but query has 7 columns.</pre></div><p>Supporting<a id="id938" class="indexterm"></a> schema evolution with Avro allows data changes to be something that is handled as part of normal business instead of the firefighting emergency they all too often turn into. But plainly, it's not for free; there is still a need to make the changes in the pipeline and roll these into production. Having Hive tables that provide forward compatibility does, however, allow the process to be performed in more manageable steps; otherwise, you would need to synchronize changes across every stage of the pipeline. If the changes are made from ingest up to the point they are inserted into Avro-backed Hive tables, then all users of those tables can remain unchanged (as long as they don't do things like <code class="literal">select *</code>, which is usually a terrible idea anyway) and continue to run existing queries against the new data. These applications can then be changed on a different timetable to the ingestion mechanism. In our v8 of the ingest pipeline, we show how to fully use Avro tables for all of our existing functionality.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note26"></a>Note</h3><p>Note that Hive 0.14, currently unreleased at the time of writing this, will likely include more built-in support for Avro that might simplify the process of schema evolution even further. If Hive 0.14 is available when you read this, then do check out the final implementation.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl3sec74"></a>Final thoughts on using Avro schema evolution</h4></div></div></div><p>With this <a id="id939" class="indexterm"></a>discussion of Avro, we have touched on some aspects of much broader topics, in particular of data management on a broader scale and policies around data versioning and retention. Much of this area becomes very specific to an organization, but here are a few parting thoughts that we feel are more broadly applicable.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch08lvl4sec02"></a>Only make additive changes</h5></div></div></div><p>We <a id="id940" class="indexterm"></a>discussed adding columns in the preceding example. Sometimes, though more rarely, your source data drops columns or you discover you no longer need a new column. Avro doesn't really provide tools to help with this, and we feel it is often undesirable. Instead of dropping old columns, we tend to maintain the old data and simply do not use the empty columns in all the new data. This is much easier to manage if you control the data format; if you are ingesting external sources, then to follow this approach you will either need to reprocess data to remove the old column or change the ingest mechanism to add a default value for all new data.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch08lvl4sec03"></a>Manage schema versions explicitly</h5></div></div></div><p>In the<a id="id941" class="indexterm"></a> preceding examples, we had a single schema file to which we made changes directly. This is likely a very bad idea, as it removes our ability to track schema changes over time. In addition to treating schemas as artifacts to be kept under version control (your schemas are in Git too, aren't they?) it is often useful to tag each schema with an explicit version. This is particularly useful when the incoming data is also explicitly versioned. Then, instead of overwriting the existing schema file, you can add the new file and use an <code class="literal">ALTER TABLE</code> statement to point the Hive table definition at the new schema. We are, of course, assuming here that you don't have the option of using a different query for the old data with the different format. Though there is no automatic mechanism for Hive to select schema, there might be cases where you can control this manually and sidestep the evolution question.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch08lvl4sec04"></a>Think about schema distribution</h5></div></div></div><p>When <a id="id942" class="indexterm"></a>using a schema file, think about how it will be distributed to the clients. If, as in the previous example, the file is on HDFS, then it likely makes sense to give it a high replication factor. The file will be retrieved by each mapper in every MapReduce job that queries the table.</p><p>The Avro URL can also be specified as a local filesystem location (<code class="literal">file://</code>), which is useful for development and also as a web resource (<code class="literal">http://</code>). Though the latter is very useful as it is a convenient mechanism to distribute the schema to non-Hadoop clients, remember that the load on the web server might be high. With modern hardware and efficient web servers, this is most likely not a huge concern, but if you have a cluster of thousands of machines running many parallel jobs where each mapper needs to hit the web server, then be careful.</p></div></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec63"></a>Collecting additional data</h2></div></div><hr /></div><p>Many<a id="id943" class="indexterm"></a> data processing systems don't have a single data ingest source; often, one primary source is enriched by other secondary sources. We will now look at how to incorporate the retrieval of such reference data into our data warehouse.</p><p>At a high level, the problem isn't very different from our retrieval of the raw tweet data, as we wish to pull data from an external source, possibly do some processing on it, and store it somewhere where it can be used later. But this does highlight an aspect we need to consider; do we really want to retrieve this data every time we ingest new tweets? The answer is certainly no. The reference data changes very rarely, and we could easily fetch it much less frequently than new tweet data. This raises a question we've skirted until now: just how do we schedule Oozie workflows?</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec152"></a>Scheduling workflows</h3></div></div></div><p>Until now, we've <a id="id944" class="indexterm"></a>run all our Oozie workflows on demand from the CLI. Oozie also has a scheduler that allows jobs to be started either on a timed basis or when external criteria such as data appearing in HDFS are met. It would be a good fit for our workflows to have our main tweet pipeline run, say, every 10 minutes but the reference data only refreshed daily.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip12"></a>Tip</h3><p>Regardless of when data is retrieved, think carefully how to handle datasets that perform a delete/replace operation. In particular, don't do the delete before retrieving and validating the new data; otherwise, any jobs that require the reference data will fail until the next run of the retrieval succeeds. It could be a good option to include the destructive operations in a subworkflow that is only triggered after successful completion of the retrieval steps.</p></div><p>Oozie actually defines two types of applications that it can run: workflows such as we've used so far and coordinators, which schedule workflows to be executed based on various criteria. A coordinator job is conceptually similar to our other workflows; we push an XML configuration file onto HDFS and use a parameterized properties file to configure it at runtime. In addition, coordinator jobs have the facility to receive additional parameterization from the events that trigger their execution.</p><p>This is possibly best described by an example. Let's say, we wish to do as previously mentioned and create a coordinator that executes v7 of our ingest workflow every 10 minutes. Here's the <code class="literal">coordinator.xml</code> file (the standard name for the coordinator XML definition):</p><div class="informalexample"><pre class="programlisting">&lt;coordinator-app name="tweets-10min-coordinator"  frequency="${freq}" start="${startTime}" end="${endTime}"  timezone="UTC" xmlns="uri:oozie:coordinator:0.2"&gt;</pre></div><p>The main <a id="id945" class="indexterm"></a>action node in a coordinator is the workflow, for which we need to specify its root location on HDFS and all required properties, as follows:</p><div class="informalexample"><pre class="programlisting">    &lt;action&gt;
        &lt;workflow&gt;
           &lt;app-path&gt;${workflowPath}&lt;/app-path&gt;
                &lt;configuration&gt;
                     &lt;property&gt;
                        &lt;name&gt;workflowRoot&lt;/name&gt;
                        &lt;value&gt;${workflowRoot}&lt;/value&gt;
                    &lt;/property&gt;
â€¦</pre></div><p>We also need to include any properties required by any action in the workflow or by any subworkflow it triggers; in effect, this means that any user-defined variables present in any of the workflows to be triggered need to be included here, as follows:</p><div class="informalexample"><pre class="programlisting">                    &lt;property&gt;
                        &lt;name&gt;dbName&lt;/name&gt;
                        &lt;value&gt;${dbName}&lt;/value&gt;
                   &lt;/property&gt;
                   &lt;property&gt;
                        &lt;name&gt;partitionKey&lt;/name&gt;&lt;value&gt;${coord:formatTime(coord:nominalTime(), 'yyyyMMddhhmm')}
                        &lt;/value&gt;
                   &lt;/property&gt;
                   &lt;property&gt;
                        &lt;name&gt;exec&lt;/name&gt;
                        &lt;value&gt;gettweets.sh&lt;/value&gt;
                   &lt;/property&gt;
                   &lt;property&gt;
                        &lt;name&gt;inputDir&lt;/name&gt;
                        &lt;value&gt;/tmp/tweets&lt;/value&gt;
                   &lt;/property&gt;
                   &lt;property&gt;
                        &lt;name&gt;subWorkflowRoot&lt;/name&gt;
                        &lt;value&gt;${subWorkflowRoot}&lt;/value&gt;
                   &lt;/property&gt;
             &lt;/configuration&gt;
          &lt;/workflow&gt;
      &lt;/action&gt;
&lt;/coordinator-app&gt;</pre></div><p>We used a few<a id="id946" class="indexterm"></a> coordinator-specific features in the preceding xml. Note the specification of the starting and ending time of the coordinator and also its frequency (in minutes). We are using the simplest form here; Oozie also has a set of functions to allow quite rich specifications of the frequency.</p><p>We use coordinator EL functions in our definition of the <code class="literal">partitionKey</code> variable. Earlier, when running workflows from the CLI, we specified these explicitly but mentioned there was a better wayâ€”this is it. The following expression generates a formatted output containing the year, month, day, hour, and minute:</p><div class="informalexample"><pre class="programlisting">${coord:formatTime(coord:nominalTime(), 'yyyyMMddhhmm')}</pre></div><p>If we then use this as the value for our partition key, we can ensure that each invocation of the workflow correctly creates a unique partition in our <code class="literal">HCatalog</code> tables.</p><p>The corresponding <code class="literal">job.properties</code> for the coordinator job looks much like our previous config files with the usual entries for the NameNode and similar variables as well as having values for the application-specific variables, such as <code class="literal">dbName</code>. In addition, we need to specify the root of the coordinator location on HDFS, as follows:</p><div class="informalexample"><pre class="programlisting">oozie.coord.application.path=${nameNode}/user/${user.name}/${tasksRoot}/tweets_10min</pre></div><p>Note the <code class="literal">oozie.coord</code> namespace prefix instead of the previously used <code class="literal">oozie.wf</code>. With the coordinator definition on HDFS, we can submit the file to Oozie just as with the previous jobs. But in this case, the job will only run for a given time period. Specifically, it will run every five minutes (the frequency is variable) when the system clock is between <code class="literal">startTime</code> and <code class="literal">endTime</code>.</p><p>We've included the full configuration in the <code class="literal">tweets_10min</code> directory in the source code for this chapter.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec153"></a>Other Oozie triggers</h3></div></div></div><p>The preceding <a id="id947" class="indexterm"></a>coordinator has a very simple trigger; it starts periodically within a specified time range. Oozie has an additional capability called datasets, where it can be triggered by the availability of new data.</p><p>This isn't a <a id="id948" class="indexterm"></a>great fit for how we've defined our pipeline until now, but imagine that, instead of our workflow collecting tweets as its first step, an external system was pushing new files of tweets onto HDFS on a continuous basis. Oozie can be configured to either look for the presence of new data based on a directory pattern or to specifically trigger when a ready file appears on HDFS. This latter configuration provides a very convenient mechanism with which to integrate the output of MapReduce jobs, which by default, write a <code class="literal">_SUCCESS</code> file into their output directory.</p><p>Oozie datasets are arguably one of the most powerful parts of the whole system, and we cannot do them justice here for space reasons. But we do strongly recommend that you consult the Oozie home page for more information.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec64"></a>Pulling it all together</h2></div></div><hr /></div><p>Let's review what we've discussed until now and how <a id="id949" class="indexterm"></a>we can use Oozie to build a sophisticated series of <a id="id950" class="indexterm"></a>workflows that implement an approach to data life cycle management by putting together all the discussed techniques.</p><p>First, it's important to define clear responsibilities and implement parts of the system using good design and separation of concern principles. By applying this, we end up with several different workflows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>A subworkflow to ensure the environment (mainly HDFS and Hive metadata) is correctly configured</p></li><li style="list-style-type: disc"><p>A subworkflow to perform data validation</p></li><li style="list-style-type: disc"><p>The main workflow that triggers both the preceding subworkflows and then pulls new data through a multistep ingest pipeline</p></li><li style="list-style-type: disc"><p>A coordinator that executes the preceding workflows every 10 minutes</p></li><li style="list-style-type: disc"><p>A second coordinator that ingests reference data that will be useful to the application pipeline</p></li></ul></div><p>We also define all our tables with Avro schemas and use them wherever possible to help manage schema evolution and changing data formats over time.</p><p>We present the full source code of these components in the final version of the workflow in the source code of this chapter.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec154"></a>Other tools to help</h3></div></div></div><p>Though Oozie is a very powerful tool, sometimes it can be somewhat difficult to correctly write workflow definition files. As pipelines get sizeable, managing complexity becomes a challenge even with good functional partitioning into multiple workflows. At a simpler level, XML is just never fun for a human to write! There are a few tools that can help. Hue, the tool calling itself the <a id="id951" class="indexterm"></a>Hadoop UI<a id="id952" class="indexterm"></a> (<a class="ulink" href="http://gethue.com/" target="_blank">http://gethue.com/</a>), provides some graphical tools to help compose, execute, and manage Oozie workflows. Though powerful, Hue is not a beginner tool; we'll mention it a little more in <a class="link" href="#" linkend="ch11">Chapter 11</a>, <span class="emphasis"><em>Where to Go Next</em></span>.</p><p>A new Apache project called <a id="id953" class="indexterm"></a>Falcon (<a class="ulink" href="http://falcon.incubator.apache.org" target="_blank">http://falcon.incubator.apache.org</a>) might also be of interest. Falcon<a id="id954" class="indexterm"></a> uses Oozie to build a range of much higher-level data flows and actions. For example, Falcon provides recipes to enable and ensure cross-site replication across multiple Hadoop clusters. The Falcon team is working on much better interfaces to build their workflows, so the project might well be worth watching.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec65"></a>Summary</h2></div></div><hr /></div><p>Hopefully, this chapter presented the topic of data life cycle management as something other than a dry abstract concept. We covered a lot, particularly:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The definition of data life cycle management and how it covers a number of issues and techniques that usually become important with large data volumes</p></li><li style="list-style-type: disc"><p>The concept of building a data ingest pipeline along good data life cycle management principles that can then be utilized by higher-level analytic tools</p></li><li style="list-style-type: disc"><p>Oozie as a Hadoop-focused workflow manager and how we can use it to compose a series of actions into a unified workflow</p></li><li style="list-style-type: disc"><p>Various Oozie tools, such as subworkflows, parallel action execution, and global variables, that allow us to apply true design principles to our workflows</p></li><li style="list-style-type: disc"><p>HCatalog and how it provides the means for tools other than Hive to read and write table-structured data; we showed its great promise and integration with tools such as Pig but also highlighted some current weaknesses</p></li><li style="list-style-type: disc"><p>Avro as our tool of choice to handle schema evolution over time</p></li><li style="list-style-type: disc"><p>Using Oozie coordinators to build scheduled workflows based either on time intervals or data availability to drive the execution of multiple ingest pipelines</p></li><li style="list-style-type: disc"><p>Some other tools that can make these tasks easier, namely, Hue and Falcon</p></li></ul></div><p>In the next chapter, we'll look at several of the higher-level analytic tools and frameworks that can build sophisticated application logic upon the data collected in an ingest pipeline.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch09"></a>ChapterÂ 9.Â Making Development Easier</h2></div></div></div><p>In this chapter, we will look at how, depending on use cases and end goals, application development in Hadoop can be simplified using a number of abstractions and frameworks built on top of the Java APIs. In particular, we will learn about the following topics:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>How the streaming API allows us to write MapReduce jobs using dynamic languages such as Python and Ruby</p></li><li style="list-style-type: disc"><p>How frameworks such as Apache Crunch and Kite Morphlines allow us to express data transformation pipelines using higher-level abstractions</p></li><li style="list-style-type: disc"><p>How Kite Data, a promising framework developed by Cloudera, provides us with the ability to apply design patterns and boilerplate to ease integration and interoperability of different components within the Hadoop ecosystem</p></li></ul></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec66"></a>Choosing a framework</h2></div></div><hr /></div><p>In the<a id="id955" class="indexterm"></a> previous chapters, we looked at the MapReduce and Spark programming APIs to write distributed applications. Although very powerful and flexible, these APIs come with a certain level of complexity and possibly require significant development time.</p><p>In an effort to reduce verbosity, we introduced the Pig and Hive frameworks, which compile domain-specific languages, Pig Latin and Hive QL, into a number of MapReduce jobs or Spark DAGs, effectively abstracting the APIs away. Both languages can be extended with UDFs, which is a way of mapping complex logic to the Pig and Hive data models.</p><p>At times when we need a certain degree of flexibility and modularity, things can get tricky. Depending on the use case and developer needs, the Hadoop ecosystem presents a vast choice of APIs, frameworks, and libraries. In this chapter, we identify four categories of users and match them with the following relevant tools:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Developers that want to avoid Java in favor of scripting MapReduce jobs using dynamic languages, or use languages not implemented on the JVM. A typical use case would be upfront analysis and rapid prototyping: Hadoop streaming</p></li><li style="list-style-type: disc"><p>Java <a id="id956" class="indexterm"></a>developers that need to integrate components of the Hadoop ecosystem and could benefit from codified design patterns and boilerplate: Kite Data</p></li><li style="list-style-type: disc"><p>Java developers who want to write modular data pipelines using a familiar API: Apache Crunch</p></li><li style="list-style-type: disc"><p>Developers who would rather configure chains of data transformations. For instance, a data engineer that wants to embed existing code in an ETL pipeline: Kite Morphlines</p></li></ul></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec67"></a>Hadoop streaming</h2></div></div><hr /></div><p>We have <a id="id957" class="indexterm"></a>mentioned previously that MapReduce programs don't have to be written in Java. There are several reasons why you might want or need to write your map and reduce tasks in another language. Perhaps you have existing code to leverage or need to use third-party binariesâ€”the reasons are varied and valid.</p><p>Hadoop provides a number of mechanisms to aid non-Java development, primary amongst which are Hadoop pipes that provide a native C++ interface and Hadoop streaming that allows any program that uses standard input and output to be used for map and reduce tasks. With the MapReduce Java API, both map and reduce tasks provide implementations for methods that contain the task functionality. These methods receive the input to the task as method arguments and then output results via the <code class="literal">Context</code> object. This is a clear and type-safe interface, but it is by definition Java-specific.</p><p>Hadoop streaming takes a different approach. With streaming, you write a map task that reads its input from standard input, one line at a time, and gives the output of its results to standard output. The reduce task then does the same, again using only standard input and output for its data flow.</p><p>Any program that reads and writes from standard input and output can be used in streaming, such as compiled binaries, Unix shell scripts, or programs written in a dynamic language such as Python or Ruby. The biggest advantage to streaming is that it can allow you to try ideas and iterate them more quickly than using Java. Instead of a compile/JAR/submit cycle, you just write the scripts and pass them as arguments to the streaming JAR file. Especially when doing initial analysis on a new dataset or trying out new ideas, this can significantly speed up development.</p><p>The classic debate regarding dynamic versus static languages balances the benefits of swift development against runtime performance and type checking. These dynamic downsides also apply <a id="id958" class="indexterm"></a>when using streaming. Consequently, we favor the use of streaming for upfront analysis and Java for the implementation of jobs that will be executed on the production cluster.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec155"></a>Streaming word count in Python</h3></div></div></div><p>We'll demonstrate <a id="id959" class="indexterm"></a>Hadoop streaming by re-implementing our familiar word count example using Python. First, we create a script that will be our mapper. It consumes UTF-8 encoded rows of text from standard input with a <code class="literal">for</code> loop, splits this into words, and uses the <code class="literal">print</code> function to write each word to standard output, as follows:</p><div class="informalexample"><pre class="programlisting">#!/bin/env python
import sys

for line in sys.stdin:
    # skip empty lines
    if line == '\n':
        continue

    # preserve utf-8 encoding
    try:
        line = line.encode('utf-8')
    except UnicodeDecodeError:
        continue
    # newline characters can appear within the text
    line = line.replace('\n', '')

    # lowercase and tokenize
    line = line.lower().split()

    for term in line:
        if not term:
          continue
        try:
            print(
                u"%s" % (
                    term.decode('utf-8')))
        except UnicodeEncodeError:
            continue</pre></div><p>The reducer counts the number of occurrences of each word from standard input, and gives the output as the final value to standard output, as follows:</p><div class="informalexample"><pre class="programlisting">#!/bin/env python
import sys

count = 1
current = None

for word in sys.stdin:
    word = word.strip()

    if word == current:
        count += 1
    else:
        if current:
            print "%s\t%s" % (current.decode('utf-8'), count)
        current = word
        count = 1
if current == word:
    print "%s\t%s" % (current.decode('utf-8'), count)</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note27"></a>Note</h3><p>In both cases, we are implicitly using Hadoop input and output formats discussed in the earlier chapters. It is the <code class="literal">TextInputFormat</code> that processes the source file and provides each line one at a time to the map script. Conversely, the <code class="literal">TextOutputFormat</code> will ensure that the output of reduce tasks is also correctly written as text.</p></div><p>Copy <code class="literal">map.py</code> and <code class="literal">reduce.py</code> to HDFS, and <a id="id960" class="indexterm"></a>execute the scripts as a streaming job using the sample data from the previous chapters, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \</strong></span>
<span class="strong"><strong>-file map.py \</strong></span>
<span class="strong"><strong>-mapper "python map.py" \</strong></span>
<span class="strong"><strong>-file reduce.py \</strong></span>
<span class="strong"><strong>-reducer "python reduce.py" \</strong></span>
<span class="strong"><strong>-input sample.txt \</strong></span>
<span class="strong"><strong>-output output.txt </strong></span>
</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note28"></a>Note</h3><p>Tweets are <code class="literal">UTF-8</code> encoded. Make sure that <code class="literal">PYTHONIOENCODING</code> is set accordingly in order to pipe data in a UNIX terminal:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ export PYTHONIOENCODING='UTF-8'</strong></span>
</pre></div></div><p>The same code can <a id="id961" class="indexterm"></a>be executed from the command-line prompt:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cat sample.txt | python map.py| python reduce.py &gt; out.txt</strong></span>
</pre></div><p>The mapper and reducer code can be found at <a class="ulink" href="https://github.com/learninghadoop2/book-examples/blob/master/ch9/streaming/wc/python/map.py" target="_blank">https://github.com/learninghadoop2/book-examples/blob/master/ch9/streaming/wc/python/map.py</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec156"></a>Differences in jobs when using streaming</h3></div></div></div><p>In Java, we<a id="id962" class="indexterm"></a> know that our <code class="literal">map()</code> method will be invoked once for each input key/value pair and our <code class="literal">reduce()</code> method will be invoked for each key and its set of values.</p><p>With streaming, we don't have the concept of the map or reduce methods anymore; instead we have written scripts that process streams of received data. This changes how we need to write our reducer. In Java, the grouping of values to each key was performed by Hadoop; each invocation of the reduce method would receive a single, tab separated key and all its values. In streaming, each instance of the reduce task is given the individual ungathered values one at a time.</p><p>Hadoop streaming does sort the keys, for example, if a mapper emitted the following data:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>First 1</strong></span>
<span class="strong"><strong>Word 1</strong></span>
<span class="strong"><strong>Word 1</strong></span>
<span class="strong"><strong>A 1</strong></span>
<span class="strong"><strong>First 1</strong></span>
</pre></div><p>The streaming reducer would receive it in the following order:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>A 1</strong></span>
<span class="strong"><strong>First 1</strong></span>
<span class="strong"><strong>First 1</strong></span>
<span class="strong"><strong>Word 1</strong></span>
<span class="strong"><strong>Word 1</strong></span>
</pre></div><p>Hadoop still collects the values for each key and ensures that each key is passed only to a single reducer. In other words, a reducer gets all the values for a number of keys, and they are grouped <a id="id963" class="indexterm"></a>together; however, they are not packaged into individual executions of the reducer, that is, one per key, as with the Java API. Since Hadoop streaming uses the <code class="literal">stdin</code> and <code class="literal">stdout</code> channels to exchange data between tasks, debug and error messages should not be printed to standard output. In the following example, we will use the Python <code class="literal">logging</code> (<a class="ulink" href="https://docs.python.org/2/library/logging.html" target="_blank">https://docs.python.org/2/library/logging.html</a>) package to log warning statements to a file.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec157"></a>Finding important words in text</h3></div></div></div><p>We will now implement a metric, <span class="strong"><strong>Term Frequency-Inverse Document Frequency</strong></span> (<span class="strong"><strong>TF-IDF</strong></span>), that will <a id="id964" class="indexterm"></a>help us to determine the<a id="id965" class="indexterm"></a> importance of words based on how frequently they appear across a set of documents (tweets, in our case).</p><p>Intuitively, if a word appears frequently in a document it is important and should be given a high score. However, if a word appears in many documents, we should penalize it with a lower score, as it is a common word and its frequency is not unique to this document.</p><p>Therefore, common words such as <span class="emphasis"><em>the</em></span>, and <span class="emphasis"><em>for</em></span>, which appear in many documents, will be scaled down. Words that appear frequently in a single tweet will be scaled up. Uses of TF-IDF, often in combination with other metrics and techniques, include stop word removal and text classification. Note that this technique will have shortcomings when dealing with short documents, such as tweets. In such cases, the term frequency component will tend to become one. Conversely, one could exploit this property to detect outliers.</p><p>The definition<a id="id966" class="indexterm"></a> of TF-IDF we will use in our example is the following:</p><div class="informalexample"><pre class="programlisting">tf = # of times term appears in a document (raw frequency)
idf = 1+log(#  of documents / # documents with term in it)
tf-idf = tf * idf</pre></div><p>We will implement the algorithm in Python using three MapReduce jobs:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The first one calculates term frequency</p></li><li style="list-style-type: disc"><p>The second one calculates document frequency (the denominator of IDF)</p></li><li style="list-style-type: disc"><p>The third one calculates per-tweet TF-IDF</p></li></ul></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl3sec75"></a>Calculate term frequency</h4></div></div></div><p>The<a id="id967" class="indexterm"></a> term frequency <a id="id968" class="indexterm"></a>part is very similar to the word count example. The main difference is that we will be using a <a id="id969" class="indexterm"></a>multi-field, tab-separated, key to keep track of co-occurrences of terms and document IDs. For each tweetâ€”in JSON formatâ€”the mapper extracts the <code class="literal">id_str</code> and <code class="literal">text</code> fields, tokenizes <code class="literal">text</code>, and emits a <code class="literal">term</code>, <code class="literal">doc_id</code> tuple:</p><div class="informalexample"><pre class="programlisting">for tweet in sys.stdin:
    # skip empty lines
    if tweet == '\n':
        continue
    try:
        tweet = json.loads(tweet)
    except:
        logger.warn("Invalid input %s " % tweet)
        continue
    # In our example one tweet corresponds to one document.
    doc_id = tweet['id_str']
    if not doc_id:
        continue

    # preserve utf-8 encoding
    text = tweet['text'].encode('utf-8')
    # newline characters can appear within the text
    text = text.replace('\n', '')

    # lowercase and tokenize
    text = text.lower().split()

    for term in text:
        try:
            print(
                u"%s\t%s" % (
                    term.decode('utf-8'), doc_id.decode('utf-8'))
                )
        except UnicodeEncodeError:
            logger.warn("Invalid term %s " % term)</pre></div><p>In the reducer, we emit <a id="id970" class="indexterm"></a>the frequency of each term in a document as a tab-separated string:</p><div class="informalexample"><pre class="programlisting">freq = 1
cur_term, cur_doc_id = sys.stdin.readline().split()
for line in sys.stdin:
    line = line.strip()
    try:
        term, doc_id = line.split('\t')
    except:
        logger.warn("Invalid record %s " % line)

    # the key is a (doc_id, term) pair
    if (doc_id == cur_doc_id) and (term == cur_term):
        freq += 1

    else:
        print(
            u"%s\t%s\t%s" % (
                cur_term.decode('utf-8'), cur_doc_id.decode('utf-8'), freq))
        cur_doc_id = doc_id
        cur_term = term
        freq = 1

print(
    u"%s\t%s\t%s" % (
        cur_term.decode('utf-8'), cur_doc_id.decode('utf-8'), freq))</pre></div><p>For this<a id="id971" class="indexterm"></a> implementation to work, it is crucial that the reducer input is sorted by term. We can test both scripts from the command line with the following pipe:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cat tweets.json  |  python map-tf.py  | sort -k1,2  | \</strong></span>
<span class="strong"><strong>python reduce-tf.py</strong></span>
</pre></div><p>Whereas at the command line we use the <code class="literal">sort</code> utility, in MapReduce we will use <code class="literal">org.apache.hadoop.mapreduce.lib.KeyFieldBasedComparator</code>. This comparator implements a subset of features provided by the <code class="literal">sort</code> command. In particular, ordering by field can be specified with the <code class="literal">â€“k&lt;position&gt;</code> option. To filter by term, the first field of our key, we set <code class="literal">-D mapreduce.text.key.comparator.options=-k1</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>/usr/bin/hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \</strong></span>
<span class="strong"><strong>-D map.output.key.field.separator=\t \</strong></span>
<span class="strong"><strong>-D stream.num.map.output.key.fields=2 \</strong></span>
<span class="strong"><strong>-Dmapreduce.output.key.comparator.class=\</strong></span>
<span class="strong"><strong>org.apache.hadoop.mapreduce.lib.KeyFieldBasedComparator \</strong></span>
<span class="strong"><strong>-D mapreduce.text.key.comparator.options=-k1,2 \</strong></span>
<span class="strong"><strong>-input tweets.json \</strong></span>
<span class="strong"><strong>-output /tmp/tf-out.tsv \</strong></span>
<span class="strong"><strong>-file map-tf.py \</strong></span>
<span class="strong"><strong>-mapper "python map-tf.py" \</strong></span>
<span class="strong"><strong>-file reduce-tf.py \</strong></span>
<span class="strong"><strong>-reducer "python reduce-tf.py" </strong></span>
</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note29"></a>Note</h3><p>We specify which fields belong to the key (for shuffling) in the comparator options.</p></div><p>The mapper <a id="id972" class="indexterm"></a>and <a id="id973" class="indexterm"></a>reducer code can be found at <a class="ulink" href="https://github.com/learninghadoop2/book-examples/blob/master/ch9/streaming/tf-idf/python/map-tf.py" target="_blank">https://github.com/learninghadoop2/book-examples/blob/master/ch9/streaming/tf-idf/python/map-tf.py</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl3sec76"></a>Calculate document frequency</h4></div></div></div><p>The main <a id="id974" class="indexterm"></a>logic <a id="id975" class="indexterm"></a>to calculate document frequency is in the reducer, while the mapper is just an identity function <a id="id976" class="indexterm"></a>that loads and pipes the (ordered by term) output of the TF job. In the reducer, for each term, we count how many times it occurs across all documents. For each term, we keep a buffer <code class="literal">key_cache</code> of (<code class="literal">term</code>, <code class="literal">doc_id</code>, <code class="literal">tf</code>) tuples, and when a new term is found we flush the buffer to standard output, together with the accumulated document frequency <code class="literal">df</code>:</p><div class="informalexample"><pre class="programlisting"># Cache the (term,doc_id, tf) tuple. 
key_cache = []

line = sys.stdin.readline().strip()
cur_term, cur_doc_id, cur_tf = line.split('\t')
cur_tf = int(cur_tf)
cur_df = 1

for line in sys.stdin:
    line = line.strip()

    try:
        term, doc_id, tf = line.strip().split('\t')
        tf = int(tf)
    except:
        logger.warn("Invalid record: %s " % line)
        continue

    # term is the only key for this input
    if (term == cur_term):
        # increment document frequency
        cur_df += 1

        key_cache.append(
            u"%s\t%s\t%s" % (term.decode('utf-8'), doc_id.decode('utf-8'), tf))

    else:
        for key in key_cache:
            print("%s\t%s" % (key, cur_df))

        print (
            u"%s\t%s\t%s\t%s" % (
                cur_term.decode('utf-8'),
                cur_doc_id.decode('utf-8'),
                cur_tf, cur_df)
            )

        # flush the cache
        key_cache = []
        cur_doc_id = doc_id
        cur_term = term
        cur_tf = tf
        cur_df = 1

for key in key_cache:
    print(u"%s\t%s" % (key.decode('utf-8'), cur_df))
print(
    u"%s\t%s\t%s\t%s\n" % (
        cur_term.decode('utf-8'),
        cur_doc_id.decode('utf-8'),
        cur_tf, cur_df))</pre></div><p>We can test<a id="id977" class="indexterm"></a> the scripts from the command line with:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cat /tmp/tf-out.tsv  |  python map-df.py  | python reduce-df.py &gt; /tmp/df-out.tsv</strong></span>
</pre></div><p>And we can<a id="id978" class="indexterm"></a> test the scripts on Hadoop streaming with:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>/usr/bin/hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \</strong></span>
<span class="strong"><strong>-D map.output.key.field.separator=\t \</strong></span>
<span class="strong"><strong>-D stream.num.map.output.key.fields=3 \</strong></span>
<span class="strong"><strong>-D mapreduce.output.key.comparator.class=\</strong></span>
<span class="strong"><strong>org.apache.hadoop.mapreduce.lib.KeyFieldBasedComparator \</strong></span>
<span class="strong"><strong>-D mapreduce.text.key.comparator.options=-k1 \</strong></span>
<span class="strong"><strong>-input /tmp/tf-out.tsv/part-00000 \</strong></span>
<span class="strong"><strong>-output /tmp/df-out.tsv \</strong></span>
<span class="strong"><strong>-mapper org.apache.hadoop.mapred.lib.IdentityMapper \</strong></span>
<span class="strong"><strong>-file reduce-df.py \</strong></span>
<span class="strong"><strong>-reducer "python reduce-df.py"</strong></span>
</pre></div><p>On Hadoop we use <code class="literal">org.apache.hadoop.mapred.lib.IdentityMapper</code>, which provides the same logic as the <code class="literal">map-df.py</code> script.</p><p>The mapper and reducer code can be found at <a class="ulink" href="https://github.com/learninghadoop2/book-examples/blob/master/ch9/streaming/tf-idf/python/map-df.py" target="_blank">https://github.com/learninghadoop2/book-examples/blob/master/ch9/streaming/tf-idf/python/map-df.py</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl3sec77"></a>Putting it all together â€“ TF-IDF</h4></div></div></div><p>To calculate TF-IDF, we<a id="id979" class="indexterm"></a> only need a mapper that consumes the output of the previous step:</p><div class="informalexample"><pre class="programlisting">num_doc = sys.argv[1]

for line in sys.stdin:
    line = line.strip()

    try:
        term, doc_id, tf, df = line.split('\t')

        tf = float(tf)
        df = float(df)
        num_doc = float(num_doc)
    except:
        logger.warn("Invalid record %s" % line)

    # idf = num_doc / df
    tf_idf = tf * (1+math.log(num_doc / df))
    print("%s\t%s\t%s" % (term, doc_id, tf_idf))</pre></div><p>The number of <a id="id980" class="indexterm"></a>documents in the collection is passed as a parameter to <code class="literal">tf-idf.py</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>/usr/bin/hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \</strong></span>
<span class="strong"><strong>-D mapreduce.reduce.tasks=0 \</strong></span>
<span class="strong"><strong>-input /tmp/df-out.tsv/part-00000 \</strong></span>
<span class="strong"><strong>-output /tmp/tf-idf.out \</strong></span>
<span class="strong"><strong>-file tf-idf.py \</strong></span>
<span class="strong"><strong>-mapper "python tf-idf.py 15578"</strong></span>
</pre></div><p>To calculate the total number of tweets, we can use the <code class="literal">cat</code> and <code class="literal">wc</code> Unix utilities in combination with Hadoop streaming:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>/usr/bin/hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar \</strong></span>
<span class="strong"><strong>-input tweets.json \</strong></span>
<span class="strong"><strong>-output tweets.cnt \</strong></span>
<span class="strong"><strong>-mapper /bin/cat \</strong></span>
<span class="strong"><strong>-reducer /usr/bin/wc</strong></span>
</pre></div><p>The mapper source code can be found at <a class="ulink" href="https://github.com/learninghadoop2/book-examples/blob/master/ch9/streaming/tf-idf/python/tf-idf.py" target="_blank">https://github.com/learninghadoop2/book-examples/blob/master/ch9/streaming/tf-idf/python/tf-idf.py</a>.</p></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec68"></a>Kite Data</h2></div></div><hr /></div><p>The Kite SDK (<a class="ulink" href="http://www.kitesdk.org" target="_blank">http://www.kitesdk.org</a>) is a <a id="id981" class="indexterm"></a>collection of classes, command-line tools, and examples that aims at easing the process of building applications on top of Hadoop.</p><p>In this section we will look at how <a id="id982" class="indexterm"></a>Kite Data, a subproject of Kite, can ease integration with several components of a Hadoop data warehouse. Kite examples<a id="id983" class="indexterm"></a> can be found at <a class="ulink" href="https://github.com/kite-sdk/kite-examples" target="_blank">https://github.com/kite-sdk/kite-examples</a>.</p><p>On Cloudera's QuickStart VM, Kite JARs<a id="id984" class="indexterm"></a> can be found at <code class="literal">/opt/cloudera/parcels/CDH/lib/kite/</code>.</p><p>Kite Data is organized in a number of subprojects, some of which we'll describe in the following sections.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec158"></a>Data Core</h3></div></div></div><p>As the <a id="id985" class="indexterm"></a>name suggests, the <a id="id986" class="indexterm"></a>core is the building block for all capabilities provided in the Data module. Its principal abstractions are datasets and repositories.</p><p>The <code class="literal">org.kitesdk.data.Dataset</code> interface is used to represent an immutable set of data:</p><div class="informalexample"><pre class="programlisting">@Immutable
<span class="strong"><strong>public interface Dataset&lt;E&gt; extends</strong></span> RefinableView&lt;E&gt; {
  String <span class="strong"><strong>getName();</strong></span>
  DatasetDescriptor <span class="strong"><strong>getDescriptor();</strong></span>
  Dataset&lt;E&gt; getPartition(PartitionKey key, <span class="strong"><strong>boolean autoCreate);</strong></span>
  void dropPartition(PartitionKey key);
  Iterable&lt;Dataset&lt;E&gt;&gt; <span class="strong"><strong>getPartitions();</strong></span>
  URI <span class="strong"><strong>getUri();</strong></span>
}</pre></div><p>Each dataset is identified by a name and an instance of the <code class="literal">org.kitesdk.data.DatasetDescriptor</code> interface, that is the structural description of a dataset and provides its schema (<code class="literal">org.apache.avro.Schema</code>) and partitioning strategy.</p><p>Implementations of the <code class="literal">Reader&lt;E&gt;</code> interface are used to read data from an underlying storage system and produce deserialized entities of type <code class="literal">E</code>. The <code class="literal">newReader()</code> method can be used to get an appropriate implementation for a given dataset:</p><div class="informalexample"><pre class="programlisting">public interface DatasetReader&lt;E&gt; extends Iterator&lt;E&gt;, Iterable&lt;E&gt;, Closeable {
  void open();

  boolean hasNext();
 
  E next();
    void remove();
    void close();
    boolean isOpen();
}</pre></div><p>An instance<a id="id987" class="indexterm"></a> of <code class="literal">DatasetReader</code> will provide methods to read and iterate over streams of data. Similarly, <code class="literal">org.kitesdk.data.DatasetWriter</code> provides an interface to write streams of data to the <code class="literal">Dataset</code> objects:</p><div class="informalexample"><pre class="programlisting">public interface DatasetWriter&lt;E&gt; extends Flushable, Closeable {
  void open();
  void write(E entity);
  void flush();
  void close();
  boolean isOpen();
}</pre></div><p>Like readers, writers<a id="id988" class="indexterm"></a> are use-once objects. They serialize instances of entities of type <code class="literal">E</code> and write them to the underlying storage system. Writers are usually not instantiated directly; rather, an appropriate implementation can be created by the <code class="literal">newWriter()</code> factory method. Implementations of <code class="literal">DatasetWriter</code> will hold resources until <code class="literal">close()</code> is called and expect the caller to invoke <code class="literal">close()</code> in a <code class="literal">finally</code> block when the writer is no longer in use. Finally, note that implementations of <code class="literal">DatasetWriter</code> are typically not thread-safe. The behavior of a writer being accessed from multiple threads is undefined.</p><p>A particular case of a dataset is the <code class="literal">View</code> interface, which is as follows:</p><div class="informalexample"><pre class="programlisting">public interface View&lt;E&gt; {
   Dataset&lt;E&gt; getDataset();
   DatasetReader&lt;E&gt; newReader();
   DatasetWriter&lt;E&gt; newWriter();
   boolean includes(E entity);
   public boolean deleteAll();
}</pre></div><p>Views carry subsets of the keys and partitions of an existing dataset; they are conceptually similar to the notion of "view" in the relational model.</p><p>A <code class="literal">View</code> interface can be created from ranges of data, or ranges of keys, or as a union between other views.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec159"></a>Data HCatalog</h3></div></div></div><p>Data HCatalog is a <a id="id989" class="indexterm"></a>module that enables the accessing of <a id="id990" class="indexterm"></a>HCatalog repositories. The core abstractions of this module are <code class="literal">org.kitesdk.data.hcatalog.HCatalogAbstractDatasetRepository</code> and its concrete implementation, <code class="literal">org.kitesdk.data.hcatalog.HCatalogDatasetRepository</code>. </p><p>They describe a <code class="literal">DatasetRepository</code> that uses HCatalog to manage metadata and HDFS for storage, as follows:</p><div class="informalexample"><pre class="programlisting">public class HCatalogDatasetRepository extends HCatalogAbstractDatasetRepository {
   HCatalogDatasetRepository(Configuration conf) {
    super(conf, new HCatalogManagedMetadataProvider(conf));
  }
   HCatalogDatasetRepository(Configuration conf, MetadataProvider provider) {
    super(conf, provider);
  }
   public &lt;E&gt; Dataset&lt;E&gt; create(String name, DatasetDescriptor descriptor) {
    getMetadataProvider().create(name, descriptor);
    return load(name);
  }
   public boolean delete(String name) {
    return getMetadataProvider().delete(name);
  }
   public static class Builder {
   â€¦
  }
}</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note30"></a>Note</h3><p>As of Kite 0.17, Data HCatalog is deprecated in favor of the new Data Hive module.</p></div><p>The location of <a id="id991" class="indexterm"></a>the data directory is either chosen by <code class="literal">Hive/HCatalog</code> (so-called "managed tables"), or specified when creating an instance of this class by providing a filesystem <a id="id992" class="indexterm"></a>and a root directory in the constructor (external tables).</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec160"></a>Data Hive</h3></div></div></div><p>The <a id="id993" class="indexterm"></a>kite-data-module  exposes Hive schemas via the <code class="literal">Dataset</code> interface. As of Kite 0.17, this <a id="id994" class="indexterm"></a>package supersedes Data HCatalog.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec161"></a>Data MapReduce</h3></div></div></div><p>The <code class="literal">org.kitesdk.data.mapreduce</code> package<a id="id995" class="indexterm"></a> provides<a id="id996" class="indexterm"></a> interfaces to read and write data to and from a Dataset with MapReduce.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec162"></a>Data Spark</h3></div></div></div><p>The <code class="literal">org.kitesdk.data.spark</code> package<a id="id997" class="indexterm"></a> provides interfaces <a id="id998" class="indexterm"></a>for reading and writing data to and from a Dataset with Apache Spark.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec163"></a>Data Crunch</h3></div></div></div><p>The<a id="id999" class="indexterm"></a> <code class="literal">org.kitesdk.data.crunch.CrunchDatasets</code> package is a helper class to expose datasets and <a id="id1000" class="indexterm"></a>views as Crunch <code class="literal">ReadableSource</code> or <code class="literal">Target</code> classes:</p><div class="informalexample"><pre class="programlisting">public class CrunchDatasets {
public static &lt;E&gt; ReadableSource&lt;E&gt; asSource(View&lt;E&gt; view, Class&lt;E&gt; type) {
    return new DatasetSourceTarget&lt;E&gt;(view, type);
  }
public static &lt;E&gt; ReadableSource&lt;E&gt; asSource(URI uri, Class&lt;E&gt; type) {
    return new DatasetSourceTarget&lt;E&gt;(uri, type);
  }
public static &lt;E&gt; ReadableSource&lt;E&gt; asSource(String uri, Class&lt;E&gt; type) {
    return asSource(URI.create(uri), type);
  }

public static &lt;E&gt; Target asTarget(View&lt;E&gt; view) {
    return new DatasetTarget&lt;E&gt;(view);
  }
 public static Target asTarget(String uri) {
    return asTarget(URI.create(uri));
  }
public static Target asTarget(URI uri) {
    return new DatasetTarget&lt;Object&gt;(uri);
  }
}</pre></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec69"></a>Apache Crunch</h2></div></div><hr /></div><p>Apache Crunch (<a class="ulink" href="http://crunch.apache.org" target="_blank">http://crunch.apache.org</a>) is a <a id="id1001" class="indexterm"></a>Java and Scala library to <a id="id1002" class="indexterm"></a>create pipelines of MapReduce jobs. It is based on Google's <a id="id1003" class="indexterm"></a>FlumeJava (<a class="ulink" href="http://dl.acm.org/citation.cfm?id=1806638" target="_blank">http://dl.acm.org/citation.cfm?id=1806638</a>) paper and library. The project goal is to make the task of writing MapReduce jobs as straightforward as possible for anybody familiar with the Java programming language by exposing a number of patterns that implement operations such as aggregating, joining, filtering, and sorting records. </p><p>Similar to tools such as Pig, Crunch pipelines are created by composing immutable, distributed data structures and running all processing operations on such structures; they are expressed and implemented as user-defined functions. Pipelines are compiled into a DAG of MapReduce jobs, whose execution is managed by the library's planner. Crunch allows us to write iterative code and abstracts away the complexity of thinking <a id="id1004" class="indexterm"></a>in terms of map and reduce operations, while at the same time avoiding the need of an ad hoc programming language such as PigLatin. In addition, Crunch offers a highly customizable type system that allows us to work with, and mix, Hadoop Writables, HBase, and Avro serialized objects.</p><p>FlumeJava's main assumption is that MapReduce is the wrong level of abstraction for several classes of problems, where computations are often made up of multiple, chained jobs. Frequently, we need to compose logically independent operations (for example, filtering, projecting, grouping, and other transformations) into a single physical MapReduce job for performance reasons. This aspect also has implications for code testability. Although we won't cover this aspect in this chapter, the reader is encouraged to look further into it by consulting Crunch's documentation.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec164"></a>Getting started</h3></div></div></div><p>Crunch JARs<a id="id1005" class="indexterm"></a> are already installed on the QuickStart VM. By default, the JARs are found in <code class="literal">/opt/cloudera/parcels/CDH/lib/crunch</code>.</p><p>Alternatively, recent Crunch libraries<a id="id1006" class="indexterm"></a> can be downloaded from <a class="ulink" href="https://crunch.apache.org/download.html" target="_blank">https://crunch.apache.org/download.html</a>, from Maven Central or Cloudera-specific repositories.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec165"></a>Concepts</h3></div></div></div><p>Crunch pipelines<a id="id1007" class="indexterm"></a> are created by composing two abstractions: <code class="literal">PCollection</code> and <code class="literal">PTable</code>.</p><p>The <a id="id1008" class="indexterm"></a>
<code class="literal">PCollection&lt;T&gt;</code> interface<a id="id1009" class="indexterm"></a> is a distributed, immutable collection of objects of type <code class="literal">T</code>. The <a id="id1010" class="indexterm"></a>
<code class="literal">PTable&lt;Key, Value&gt;</code> interface<a id="id1011" class="indexterm"></a> is a distributed, immutable hashtableâ€”a sub-interface of PCollectionâ€”of keys of the <code class="literal">Key</code> type and values of the <code class="literal">Value</code> type that exposes methods to work with the key-value pairs.</p><p>These two abstractions support the following four primitive operations:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">parallelDo</code>: <a id="id1012" class="indexterm"></a>applies a user-defined function, <code class="literal">DoFn</code>, to a given <code class="literal">PCollection</code> and returns a new <code class="literal">PCollection</code>
</p></li><li style="list-style-type: disc"><p>
<code class="literal">union</code>: <a id="id1013" class="indexterm"></a>merges two or more <code class="literal">PCollections</code> into a single virtual <code class="literal">PCollection</code>
</p></li><li style="list-style-type: disc"><p>
<code class="literal">groupByKey</code>: <a id="id1014" class="indexterm"></a>sorts and groups the elements of a <code class="literal">PTable</code> by their keys</p></li><li style="list-style-type: disc"><p>
<code class="literal">combineValues</code>: <a id="id1015" class="indexterm"></a>aggregates the values from a <code class="literal">groupByKey</code> operation</p></li></ul></div><p>The <a class="ulink" href="https://github.com/learninghadoop2/book-examples/blob/master/ch9/crunch/src/main/java/com/learninghadoop2/crunch/HashtagCount.java" target="_blank">https://github.com/learninghadoop2/book-examples/blob/master/ch9/crunch/src/main/java/com/learninghadoop2/crunch/HashtagCount.java</a> implements a Crunch MapReduce pipeline that counts hashtag occurrences:</p><div class="informalexample"><pre class="programlisting">Pipeline pipeline = new MRPipeline(HashtagCount.class, getConf());

pipeline.enableDebug();

PCollection&lt;String&gt; lines = pipeline.readTextFile(args[0]);

PCollection&lt;String&gt; words = lines.parallelDo(new DoFn&lt;String, String&gt;() {
  public void process(String line, Emitter&lt;String&gt; emitter) {
    for (String word : line.split("\\s+")) {
        if (word.matches("(?:\\s|\\A|^)[##]+([A-Za-z0-9-_]+)")) {
            emitter.emit(word);
        }
    }
  }
}, Writables.strings());

PTable&lt;String, Long&gt; counts = words.count();

pipeline.writeTextFile(counts, args[1]);
// Execute the pipeline as a MapReduce.
pipeline.done();</pre></div><p>In this example, we <a id="id1016" class="indexterm"></a>first create a <code class="literal">MRPipeline</code> pipeline and use it to first read the content of <code class="literal">sample.txt</code> created with <code class="literal">stream.py -t</code> into a collection of strings, where each element of the collection represents a tweet. We tokenize each tweet into words with <code class="literal">tweet.split("\\s+")</code>, and we emit each word that matches the hashtag regular expression, serialized as Writable. Note that the tokenizing and filtering operations are executed in parallel by MapReduce jobs created by the <code class="literal">parallelDo</code> call. We create a <code class="literal">PTable</code> that associates each hashtag, represented as a string, with the number of times it occurred in the datasets. Finally, we write the <code class="literal">PTable</code> counts into HDFS as a textfile. The pipeline is executed with <code class="literal">pipeline.done()</code>.</p><p>To compile and execute the pipeline, we can use Gradle to manage the needed dependencies, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ./gradlew jar</strong></span>
<span class="strong"><strong>$ ./gradlew copyJars</strong></span>
</pre></div><p>Add the Crunch and Avro dependencies downloaded with <code class="literal">copyJars</code> to the <code class="literal">LIBJARS</code> environment variable:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ export CRUNCH_DEPS=build/libjars/crunch-example/lib</strong></span>
<span class="strong"><strong>$ export LIBJARS=${LIBJARS},${CRUNCH_DEPS}/crunch-core-0.9.0-cdh5.0.3.jar,${CRUNCH_DEPS}/avro-1.7.5-cdh5.0.3.jar,${CRUNCH_DEPS}/avro-mapred-1.7.5-cdh5.0.3-hadoop2.jar</strong></span>
</pre></div><p>Then, run the <a id="id1017" class="indexterm"></a>example on Hadoop:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hadoop jar build/libs/crunch-example.jar \</strong></span>
<span class="strong"><strong>com.learninghadoop2.crunch.HashtagCount \</strong></span>
<span class="strong"><strong>tweets.json count-out \</strong></span>
<span class="strong"><strong>-libjars $LIBJARS</strong></span>
</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec166"></a>Data serialization</h3></div></div></div><p>One of the<a id="id1018" class="indexterm"></a> framework's goals is to make it easy to process complex<a id="id1019" class="indexterm"></a> records containing nested and repeated data structures, such as protocol buffers and Thrift records.</p><p>The <code class="literal">org.apache.crunch.types.PType</code> interface defines the mapping between a data type that is used in a Crunch pipeline and a serialization and storage format that is used to read/write data from/to HDFS. Every <code class="literal">PCollection</code> has an associated <code class="literal">PType</code> that tells Crunch how to read/write data.</p><p>The <code class="literal">org.apache.crunch.types.PTypeFamily</code> interface provides an abstract factory to implement instances of <code class="literal">PType</code> that share the same serialization format. Currently, Crunch supports two type families: one based on the Writable interface and the other on Apache Avro.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note31"></a>Note</h3><p>Although Crunch permits mixing and matching <code class="literal">PCollection</code> interfaces that use different instances of <code class="literal">PType</code> in the same pipeline, each <code class="literal">PCollection</code> interfaces's <code class="literal">PType</code> must belong to a unique family. For instance, it is not possible to have a <code class="literal">PTable</code> with a key serialized as Writable and its value serialized using Avro.</p></div><p>Both type families support a common set of primitive types (strings, longs, integers, floats, doubles, booleans, and bytes) as well as more complex <code class="literal">PType</code> interfaces that can be constructed out of other <code class="literal">PTypes</code>. These include tuples and collections of other <code class="literal">PType</code>. A particularly important, complex, <code class="literal">PType</code> is <code class="literal">tableOf</code>, which determines whether the return type of <code class="literal">paralleDo</code> will be a <code class="literal">PCollection</code> or <code class="literal">PTable</code>.</p><p>New <code class="literal">PTypes</code> can be created by inheriting and extending the built-ins of the Avro and Writable families. This requires implementing input <code class="literal">MapFn&lt;S, T&gt;</code> and output<code class="literal"> MapFn&lt;T, S&gt;</code> classes. We are implementing <code class="literal">PType</code> for instances where <code class="literal">S</code> is the original type and <code class="literal">T</code> is the new type .</p><p>Derived <code class="literal">PTypes</code> can be found in the <code class="literal">PTypes</code> class. These include serialization support for protocol buffers, Thrift records, Java Enums, BigInteger, and UUIDs. The Elephant Bird <a id="id1020" class="indexterm"></a>library <a id="id1021" class="indexterm"></a>we discussed in <a class="link" href="#" linkend="ch06">Chapter 6</a>, <span class="emphasis"><em>Data Analysis with Apache Pig</em></span>, contains additional examples.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec167"></a>Data processing patterns</h3></div></div></div><a id="id1022" class="indexterm"></a><p>
<code class="literal">org.apache.crunch.lib</code> <a id="id1023" class="indexterm"></a>implements a number of design patterns for common data manipulation operations.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl3sec78"></a>Aggregation and sorting</h4></div></div></div><p>Most of the<a id="id1024" class="indexterm"></a> data processing patterns provided by <code class="literal">org.apache.crunch.lib</code> rely on the <code class="literal">PTable</code>'s <code class="literal">groupByKey</code> method. The method has three different overloaded forms:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">groupByKey()</code>: <a id="id1025" class="indexterm"></a>lets the planner determine the number of partitions</p></li><li style="list-style-type: disc"><p>
<code class="literal">groupByKey(int numPartitions)</code>:<a id="id1026" class="indexterm"></a> is used to set the number of partitions specified by the developer</p></li><li style="list-style-type: disc"><p>
<code class="literal">groupByKey(GroupingOptions options)</code>: <a id="id1027" class="indexterm"></a>allows us to specify custom partitions and comparators for shuffling</p></li></ul></div><p>The <code class="literal">org.apache.crunch.GroupingOptions</code> class takes instances of Hadoop's <code class="literal">Partitioner</code> and <code class="literal">RawComparator</code> classes to implement custom partitioning and sorting operations.</p><p>The <code class="literal">groupByKey</code> method returns an instance of <code class="literal">PGroupedTable</code>, Crunch's representation of a grouped table. It corresponds to the output of the shuffle phase of a MapReduce job and allows values to be combined with the <code class="literal">combineValue</code> method.</p><p>The <code class="literal">org.apache.crunch.lib.Aggregate</code> package exposes methods to perform simple aggregations (count, max, top, and length) on the <code class="literal">PCollection</code> instances.</p><p>Sort provides an API to sort <code class="literal">PCollection</code> and <code class="literal">PTable</code> instances whose contents implement the <code class="literal">Comparable</code> interface.</p><p>By default, Crunch sorts data using one reducer. This behavior can be modified by passing the number of partitions required to the <code class="literal">sort</code> method. The <code class="literal">Sort.Order</code> method signals the order in which a sort should be done.</p><p>The following are how different sort options can be specified for collections:</p><div class="informalexample"><pre class="programlisting">public static &lt;T&gt; PCollection&lt;T&gt; sort(PCollection&lt;T&gt; collection)
public static &lt;T&gt; PCollection&lt;T&gt; sort(PCollection&lt;T&gt; collection, Sort.Order order)
public static &lt;T&gt; PCollection&lt;T&gt; sort(PCollection&lt;T&gt; collection, int numReducers,                                       Sort.Order order)</pre></div><p>The following are how different sort options can be specified for tables:</p><div class="informalexample"><pre class="programlisting">public static &lt;K,V&gt; PTable&lt;K,V&gt; sort(PTable&lt;K,V&gt; table)

public static &lt;K,V&gt; PTable&lt;K,V&gt; sort(PTable&lt;K,V&gt; table, Sort.Order key)
public static &lt;K,V&gt; PTable&lt;K,V&gt; sort(PTable&lt;K,V&gt; table, int numReducers, Sort.Order key)</pre></div><p>Finally, <code class="literal">sortPairs</code> sorts the <code class="literal">PCollection</code> of pairs using the specified column order in <code class="literal">Sort.ColumnOrder</code>:</p><div class="informalexample"><pre class="programlisting">sortPairs(PCollection&lt;Pair&lt;U,V&gt;&gt; collection, Sort.ColumnOrder... columnOrders)</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl3sec79"></a>Joining data</h4></div></div></div><p>The <a id="id1028" class="indexterm"></a>
<code class="literal">org.apache.crunch.lib.Join</code> package is an API to join <code class="literal">PTables</code> based on a common key. The following four join operations are supported:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">fullJoin</code>
</p></li><li style="list-style-type: disc"><p>
<code class="literal">join</code> (defaults to <code class="literal">innerJoin</code>)</p></li><li style="list-style-type: disc"><p>
<code class="literal">leftJoin</code>
</p></li><li style="list-style-type: disc"><p>
<code class="literal">rightJoin</code>
</p></li></ul></div><p>The methods have a common return type and signature. For reference, we will describe the commonly used <code class="literal">join</code> method that implements an inner join:</p><div class="informalexample"><pre class="programlisting">public static &lt;K,U,V&gt; PTable&lt;K,Pair&lt;U,V&gt;&gt; join(PTable&lt;K,U&gt; left, PTable&lt;K,V&gt; right)</pre></div><p>The <code class="literal">org.apache.crunch.lib.Join.JoinStrategy</code> package provides an interface to define custom join strategies. Crunch's default strategy (<code class="literal">defaultStrategy</code>) is to join data reduce-side.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec168"></a>Pipelines implementation and execution</h3></div></div></div><p>Crunch comes <a id="id1029" class="indexterm"></a>with three implementations <a id="id1030" class="indexterm"></a>of the pipeline interface. The oldest one, implicitly <a id="id1031" class="indexterm"></a>used in this chapter, is <code class="literal">org.apache.crunch.impl.mr.MRPipeline</code>, which uses Hadoop's MapReduce as its execution engine. <code class="literal">org.apache.crunch.impl.mem.MemPipeline</code> allows all operations to be performed in memory, with no serialization to disk performed. Crunch 0.10 introduced <code class="literal">org.apache.crunch.impl.spark.SparkPipeline</code> which compiles and runs a DAG of <code class="literal">PCollections</code> to Apache Spark.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl3sec80"></a>SparkPipeline</h4></div></div></div><p>With<a id="id1032" class="indexterm"></a> SparkPipeline, Crunch delegates<a id="id1033" class="indexterm"></a> much of the execution to Spark and does relatively little of the planning tasks, with the following exceptions:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Multiple inputs</p></li><li style="list-style-type: disc"><p>Multiple outputs</p></li><li style="list-style-type: disc"><p>Data serialization</p></li><li style="list-style-type: disc"><p>Checkpointing</p></li></ul></div><p>At the time of writing, SparkPipeline is still heavily under development and might not handle all of the use cases of a standard MRPipeline. The Crunch community is actively working to ensure complete compatibility between the two implementations.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl3sec81"></a>MemPipeline</h4></div></div></div><p>MemPipeline <a id="id1034" class="indexterm"></a>executes<a id="id1035" class="indexterm"></a> in-memory on a client. Unlike MRPipeline, MemPipeline is not explicitly created but referenced by calling the static method <code class="literal">MemPipeline.getInstance()</code>. All operations are in memory, and the use of PTypes is minimal.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec169"></a>Crunch examples</h3></div></div></div><p>We will<a id="id1036" class="indexterm"></a> now use <a id="id1037" class="indexterm"></a>Apache Crunch to reimplement some of the MapReduce code written so far in a more modular fashion.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl3sec82"></a>Word co-occurrence</h4></div></div></div><p>In <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Processing â€“ MapReduce and Beyond</em></span>, we showed a MapReduce job, BiGramCount, to<a id="id1038" class="indexterm"></a> count co-occurrences of words in tweets. That same logic can be implemented as a <code class="literal">DoFn</code>. Instead of emitting a multi-field key and having to parse it at a later stage, with Crunch we can use a complex type <code class="literal">Pair&lt;String, String&gt;</code>, as follows:</p><div class="informalexample"><pre class="programlisting">class BiGram extends DoFn&lt;String, Pair&lt;String, String&gt;&gt; {
    @Override
    public void process(String tweet, 
Emitter&lt;Pair&lt;String, String&gt;&gt; emitter) {
        String[] words = tweet.split(" ") ;
                
        Text bigram = new Text();
        String prev = null;
                 
        for (String s : words) {
          if (prev != null) {
              emitter.emit(Pair.of(prev, s));
            }       
            prev = s;
        }
    }   
}       </pre></div><p>Notice how, compared to MapReduce, the <code class="literal">BiGram</code> Crunch implementation is a standalone class, easily reusable in any other codebase. The code for this example is included in <a class="ulink" href="https://github.com/learninghadoop2/book-examples/blob/master/ch9/crunch/src/main/java/com/learninghadoop2/crunch/DataPreparationPipeline.java" target="_blank">https://github.com/learninghadoop2/book-examples/blob/master/ch9/crunch/src/main/java/com/learninghadoop2/crunch/DataPreparationPipeline.java</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl3sec83"></a>TF-IDF</h4></div></div></div><p>We can<a id="id1039" class="indexterm"></a> implement the TF-IDF chain of jobs with a <code class="literal">MRPipeline</code>, as follows:</p><div class="informalexample"><pre class="programlisting">public class CrunchTermFrequencyInvertedDocumentFrequency 
         extends Configured implements Tool, Serializable {
   
   private Long numDocs;
   
   @SuppressWarnings("deprecation")
   
   public static class TF {
        String term;
        String docId;
        int frequency;

        public TF() {}

        public TF(String term, 
               String docId, Integer frequency) {
           this.term = term;
           this.docId = docId;
           this.frequency = (int) frequency;
           
        }
   }

   public int run(String[] args) throws Exception {
       if(args.length != 2) {
         System.err.println();
         System.err.println("Usage: " + this.getClass().getName() + " [generic options] input output");

         return 1;
       }
       // Create an object to coordinate pipeline creation and execution.
       Pipeline pipeline = 
new MRPipeline(TermFrequencyInvertedDocumentFrequency.class, getConf());
            
       // enable debug options
       pipeline.enableDebug();
       
       // Reference a given text file as a collection of Strings.
       PCollection&lt;String&gt; tweets = pipeline.readTextFile(args[0]);
       numDocs = tweets.length().getValue();
       
       // We use Avro reflections to map the TF POJO to avsc 
       PTable&lt;String, TF&gt; tf = tweets.parallelDo(new TermFrequencyAvro(), Avros.tableOf(Avros.strings(), Avros.reflects(TF.class)));
       
       // Calculate DF
       PTable&lt;String, Long&gt; df = Aggregate.count(tf.parallelDo( new DocumentFrequencyString(), Avros.strings()));
       
       
       // Finally we calculate TF-IDF 
       PTable&lt;String, Pair&lt;TF, Long&gt;&gt; tfDf = Join.join(tf, df);
       PCollection&lt;Tuple3&lt;String, String, Double&gt;&gt; tfIdf = tfDf.parallelDo(new TermFrequencyInvertedDocumentFrequency(),
                Avros.triples(
                      Avros.strings(), 
                      Avros.strings(), 
                      Avros.doubles()));
  

       // Serialize as avro 
       tfIdf.write(To.avroFile(args[1]));
       
       // Execute the pipeline as a MapReduce.
       PipelineResult result = pipeline.done();
       return result.succeeded() ? 0 : 1;
   }
   â€¦
}</pre></div><p>The approach<a id="id1040" class="indexterm"></a> that we follow here has a number of advantages compared to streaming. First of all, we don't need to manually chain MapReduce jobs using a separate script. This task is Crunch's main purpose. Secondly, we can express each component of the metric as a distinct class, making it easier to reuse in future applications.</p><p>To implement term frequency, we create a <code class="literal">DoFn</code> class that takes as input a tweet and emits <code class="literal">Pair&lt;String, TF&gt;</code>. The first element is a term, and the second is an instance of the POJO class that will be serialized using Avro. The <code class="literal">TF</code> part contains three variables:<code class="literal"> term</code>, <code class="literal">documentId</code>, and <code class="literal">frequency</code>. In the reference implementation, we expect input data to be a JSON string that we deserialize and parse. We also include tokenizing as a subtask of the process method. </p><p>Depending on the use cases, we could abstract both operations in<a id="id1041" class="indexterm"></a> separate <code class="literal">DoFns</code>, as follows:</p><div class="informalexample"><pre class="programlisting">class TermFrequencyAvro extends DoFn&lt;String,Pair&lt;String, TF&gt;&gt; {
    public void process(String JSONTweet, 
Emitter&lt;Pair &lt;String, TF&gt;&gt; emitter) {
        Map&lt;String, Integer&gt; termCount = new HashMap&lt;&gt;();

        String tweet;
        String docId;

        JSONParser parser = new JSONParser();

        try {
            Object obj = parser.parse(JSONTweet);

            JSONObject jsonObject = (JSONObject) obj;

            tweet = (String) jsonObject.get("text");
            docId = (String) jsonObject.get("id_str");
        
            for (String term : tweet.split("\\s+")) {
                if (termCount.containsKey(term.toLowerCase())) {
                    termCount.put(term, 
termCount.get(term.toLowerCase()) + 1);
                } else {
                    termCount.put(term.toLowerCase(), 1);
                }
            }
        
            for (Entry&lt;String, Integer&gt; entry : termCount.entrySet()) {
                emitter.emit(Pair.of(entry.getKey(), new TF(entry.getKey(), docId, entry.getValue())));
            }
        } catch (ParseException e) {
            e.printStackTrace();
        }
    }
  }
}</pre></div><p>Document frequency is straightforward. For each <code class="literal">Pair&lt;String, TF&gt;</code> generated in the term frequency step, we emit the termâ€”the first element of the pair. We aggregate and count the resulting <code class="literal">PCollection</code> of terms to obtain document frequency, as follows:</p><div class="informalexample"><pre class="programlisting">class DocumentFrequencyString extends DoFn&lt;Pair&lt;String, TF&gt;, String&gt; {
@Override
   public void process(Pair&lt;String, TF&gt; tfAvro,
      Emitter&lt;String&gt; emitter) {
      emitter.emit(tfAvro.first());
   }
}</pre></div><p>We finally<a id="id1042" class="indexterm"></a> join the <code class="literal">PTable</code> TF with the <code class="literal">PTable</code> DF on the shared key (term) and feed the resulting <code class="literal">Pair&lt;String, Pair&lt;TF, Long&gt;&gt;</code> object to <code class="literal">TermFrequencyInvertedDocumentFrequency</code>.</p><p>For each term and document, we calculate TF-IDF and return a <code class="literal">term</code>, <code class="literal">docIf</code>, and <code class="literal">tfIdf</code> triple:</p><div class="informalexample"><pre class="programlisting">   class TermFrequencyInvertedDocumentFrequency extends MapFn&lt;Pair&lt;String, Pair&lt;TF, Long&gt;&gt;, Tuple3&lt;String, String, Double&gt; &gt;  {      
      @Override
      public Tuple3&lt;String, String, Double&gt; map(
            Pair&lt;String, Pair&lt;TF, Long&gt;&gt; input) {

         Pair&lt;TF, Long&gt; tfDf = input.second();
         Long df = tfDf.second();
         
         TF tf = tfDf.first();
         double idf = 1.0+Math.log(numDocs / df);
         double tfIdf = idf * tf.frequency;
         
         return  Tuple3.of(tf.term, tf.docId, tfIdf);
      }
      
   }   </pre></div><p>We use <code class="literal">MapFn</code> because we are going to output one record for each input. The source code for this example can be found at <a class="ulink" href="https://github.com/learninghadoop2/book-examples/blob/master/ch9/crunch/src/main/java/com/learninghadoop2/crunch/CrunchTermFrequencyInvertedDocumentFrequency.java" target="_blank">https://github.com/learninghadoop2/book-examples/blob/master/ch9/crunch/src/main/java/com/learninghadoop2/crunch/CrunchTermFrequencyInvertedDocumentFrequency.java</a>.</p><p>The example can be compiled and executed with the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ./gradlew jar</strong></span>
<span class="strong"><strong>$ ./gradlew copyJars</strong></span>
</pre></div><p>If not already <a id="id1043" class="indexterm"></a>done, add the Crunch and Avro dependencies downloaded with <code class="literal">copyJars</code> to the <code class="literal">LIBJARS</code> environment variable, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ export CRUNCH_DEPS=build/libjars/crunch-example/lib</strong></span>
<span class="strong"><strong>$ export LIBJARS=${LIBJARS},${CRUNCH_DEPS}/crunch-core-0.9.0-cdh5.0.3.jar,${CRUNCH_DEPS}/avro-1.7.5-cdh5.0.3.jar,${CRUNCH_DEPS}/avro-mapred-1.7.5-cdh5.0.3-hadoop2.jar</strong></span>
</pre></div><p>Furthermore, add the <code class="literal">json-simple</code> JAR to <code class="literal">LIBJARS</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ export LIBJARS=${LIBJARS},${CRUNCH_DEPS}/json-simple-1.1.1.jar</strong></span>
</pre></div><p>Finally, run <code class="literal">CrunchTermFrequencyInvertedDocumentFrequency</code> as a MapReduce job, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hadoop jar build/libs/crunch-example.jar \</strong></span>
<span class="strong"><strong>com.learninghadoop2.crunch.CrunchTermFrequencyInvertedDocumentFrequency  \</strong></span>
<span class="strong"><strong>-libjars ${LIBJARS} \</strong></span>
<span class="strong"><strong>tweets.json tweets.avro-out</strong></span>
</pre></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec170"></a>Kite Morphlines</h3></div></div></div><p>Kite Morphlines<a id="id1044" class="indexterm"></a> is a <a id="id1045" class="indexterm"></a>data transformation library, inspired by Unix pipes, originally developed as part of Cloudera Search. A morphline is an in-memory chain of transformation commands that relies on a plugin structure to tap heterogeneous data sources. It uses declarative commands to carry out ETL operations on records. Commands are defined in a configuration file, which is later fed to a driver class.</p><p>The goal is to make embedding ETL logic into any Java codebase a trivial task by providing a library that allows developers to replace programming with a series of configuration settings.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl3sec84"></a>Concepts</h4></div></div></div><p>Morphlines <a id="id1046" class="indexterm"></a>are built around two abstractions: <code class="literal">Command</code> and <code class="literal">Record</code>.</p><p>Records<a id="id1047" class="indexterm"></a> are implementations<a id="id1048" class="indexterm"></a> of the <code class="literal">org.kitesdk.morphline.api.Record</code> interface:</p><div class="informalexample"><pre class="programlisting">public final class Record {  
  private ArrayListMultimap&lt;String, Object&gt; fields;  
â€¦
    private Record(ArrayListMultimap&lt;String, Object&gt; fields) {â€¦}
  public ListMultimap&lt;String, Object&gt; getFields() {â€¦}
  public List get(String key) {â€¦}
  public void put(String key, Object value) {â€¦}
   â€¦
}</pre></div><p>A record is a set of <a id="id1049" class="indexterm"></a>named fields, where each field has a list of one or more values. A <code class="literal">Record</code> is implemented on top of Google Guava's <code class="literal">ListMultimap</code> and <code class="literal">ArrayListMultimap</code> classes. Note that a value can be any Java object, fields can be multivalued, and two records don't need to use common field names. A record can contain an <code class="literal">_attachment_body</code> field that can be a <code class="literal">java.io.InputStream</code> or a byte array.</p><p>Commands implement the <code class="literal">org.kitesdk.morphline.api.Command</code> interface:</p><div class="informalexample"><pre class="programlisting">public interface Command {
   void notify(Record notification);
   boolean process(Record record);
   Command getParent();
}</pre></div><p>A command transforms a record into zero or more records. Commands can call the methods on the <code class="literal">Record</code> instance provided for read and write operations as well as for adding or removing fields.</p><p>Commands are chained together, and at each step of a morphline the parent command sends records to its child, which in turn processes them. Information between parents and children is exchanged using two communication channels (planes); notifications are sent via a control plane, and records are sent over a data plane. Records are processed by the <code class="literal">process()</code> method, which returns a Boolean value to indicate whether a morphline should proceed or not. </p><p>Commands are not instantiated directly, but via an implementation of the <code class="literal">org.kitesdk.morphline.api.CommandBuilder</code> interface:</p><div class="informalexample"><pre class="programlisting">public interface CommandBuilder {
   Collection&lt;String&gt; getNames();
   Command build(Config config, 
      Command parent, 
      Command child, 
      MorphlineContext context);
}</pre></div><p>The <code class="literal">getNames</code> method returns the names with which the command can be invoked. Multiple names are supported to allow backwards compatible name changes. The <code class="literal">build()</code> method creates and returns a command rooted at the given morphline configuration.</p><p>The <code class="literal">org.kitesdk.morphline.api.MorphlineContext</code> interface allows additional parameters to be passed to all morphline commands.</p><p>The data model of <a id="id1050" class="indexterm"></a>morphlines is structured following a source-pipe-sink pattern, where data is captured from a source, piped through a number of processing steps, and its output is then delivered into a sink.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl3sec85"></a>Morphline commands</h4></div></div></div><p>Kite Morphlines<a id="id1051" class="indexterm"></a> comes with a number of default commands that implement data transformations on common serialization formats (plaintext, Avro, JSON). Currently available commands are organized as subprojects of morphlines and include:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">kite-morphlines-core-stdio</code>:<a id="id1052" class="indexterm"></a> will <a id="id1053" class="indexterm"></a>read data from binary large objects (BLOBs) and text</p></li><li style="list-style-type: disc"><p>
<code class="literal">kite-morphlines-core-stdlib</code>:<a id="id1054" class="indexterm"></a> wraps <a id="id1055" class="indexterm"></a>around Java data types for data manipulation and representation</p></li><li style="list-style-type: disc"><p>
<code class="literal">kite-morphlines-avro</code>:<a id="id1056" class="indexterm"></a> is<a id="id1057" class="indexterm"></a> used for serialization into and deserialization from data in the Avro format</p></li><li style="list-style-type: disc"><p>
<code class="literal">kite-morphlines-json</code>: <a id="id1058" class="indexterm"></a>will<a id="id1059" class="indexterm"></a> serialize and deserialize data in JSON format</p></li><li style="list-style-type: disc"><p>
<code class="literal">kite-morphlines-hadoop-core</code>: is <a id="id1060" class="indexterm"></a>used to access HDFS</p></li><li style="list-style-type: disc"><p>
<code class="literal">kite-morphlines-hadoop-parquet-avro</code>:<a id="id1061" class="indexterm"></a> is<a id="id1062" class="indexterm"></a> used to serialize and deserialize data in the Parquet format</p></li><li style="list-style-type: disc"><p>
<code class="literal">kite-morphlines-hadoop-sequencefile</code>: <a id="id1063" class="indexterm"></a>is used to serialize and deserialize data in<a id="id1064" class="indexterm"></a> the Sequencefile format</p></li><li style="list-style-type: disc"><p>
<code class="literal">kite-morphlines-hadoop-rcfile</code>:<a id="id1065" class="indexterm"></a> is used<a id="id1066" class="indexterm"></a> to serialize and deserialize data in RCfile format</p></li></ul></div><p>A list of all available <a id="id1067" class="indexterm"></a>commands can be found at <a class="ulink" href="http://kitesdk.org/docs/0.17.0/kite-morphlines/morphlinesReferenceGuide.html" target="_blank">http://kitesdk.org/docs/0.17.0/kite-morphlines/morphlinesReferenceGuide.html</a>.</p><p>Commands are defined by declaring a chain of transformations in a configuration file, <code class="literal">morphline.conf</code>, which is then <a id="id1068" class="indexterm"></a>compiled and executed by a driver program. For instance, we can specify a <code class="literal">read_tweets</code> morphline that will load tweets stored as JSON data, serialize and deserialize them using Jackson, and print the first 10, by combining the default <code class="literal">readJson</code> and <code class="literal">head</code> commands contained in the <code class="literal">org.kitesdk.morphline</code> package, as follows:</p><div class="informalexample"><pre class="programlisting">morphlines : [{
  id : read_tweets
  importCommands : ["org.kitesdk.morphline.**"]
    
  commands : [{
    readJson {
      outputClass : com.fasterxml.jackson.databind.JsonNode
    }}
    {
      head { 
      limit : 10
    }}
  ]
}]</pre></div><p>We will now show how this morphline can be executed both from a standalone Java program as well as from MapReduce.</p><p>
<code class="literal">MorphlineDriver.java</code> shows how to use the library embedded into a host system. The first step that we carry out in the<code class="literal"> main</code> method is to load morphline's JSON configuration, build a <code class="literal">MorphlineContext</code> object, and compile it into an instance of <code class="literal">Command</code> that acts as the starting node of the morphline. Note that <code class="literal">Compiler.compile()</code> takes a <code class="literal">finalChild</code> parameter; in this case, it is <code class="literal">RecordEmitter</code>. We use <code class="literal">RecordEmitter</code> to act as a sink for the morphline, by either printing a record to stdout or storing it into HDFS. In the <code class="literal">MorphlineDriver</code> example, we use <code class="literal">org.kitesdk.morphline.base.Notifications</code> to manage and monitor the morphline life cycle in a <a id="id1069" class="indexterm"></a>transactional fashion.</p><p>A call to <code class="literal">Notifications.notifyStartSession(morphline)</code> starts the transformation chain within a transaction defined by calling <code class="literal">Notifications.notifyBeginTransaction</code>. Upon success, we terminate the pipeline with <code class="literal">Notifications.notifyShutdown(morphline)</code>. In the event of failure, we roll back the transaction, <code class="literal">Notifications.notifyRollbackTransaction(morphline)</code>, and pass an exception handler from the morphline context to the calling Java code:</p><div class="informalexample"><pre class="programlisting">public class MorphlineDriver {
    private static final class RecordEmitter implements Command {
       private final Text line = new Text();

      @Override
      public Command getParent() {
         return null;
      }

      @Override
      public void notify(Record record) {
         
      }

      @Override
      public boolean process(Record record) {
         line.set(record.get("_attachment_body").toString());
         
         System.out.println(line);
         
         return true;
      }
       }  
    
   public static void main(String[] args) throws IOException {
       /* load a morphline conf and set it up */
       File morphlineFile = new File(args[0]);
       String morphlineId = args[1];
       MorphlineContext morphlineContext = new MorphlineContext.Builder().build();
       Command morphline = new Compiler().compile(morphlineFile, morphlineId, morphlineContext, new RecordEmitter());
          
       /* Prepare the morphline for execution
        * 
        * Notifications are sent through the communication channel  
        * */
       
       Notifications.notifyBeginTransaction(morphline);
       
       /* Note that we are using the local filesystem, not hdfs*/
       InputStream in = new BufferedInputStream(new FileInputStream(args[2]));
       
       /* fill in a record and pass  it over */
       Record record = new Record();
       record.put(Fields.ATTACHMENT_BODY, in); 
       
       try {

            Notifications.notifyStartSession(morphline);
            boolean success = morphline.process(record);
            if (!success) {
              System.out.println("Morphline failed to process record: " + record);
            }
        /* Commit the morphline */
       } catch (RuntimeException e) {
           Notifications.notifyRollbackTransaction(morphline);
           morphlineContext.getExceptionHandler().handleException(e, null);
         }
       finally {
            in.close();
        }
       
        /* shut it down */
        Notifications.notifyShutdown(morphline);     
    }
}</pre></div><p>In this example, we load data in JSON format from the local filesystem into an <code class="literal">InputStream</code> object and use it to initialize a new <code class="literal">Record</code> instance. The <code class="literal">RecordEmitter</code> class contains the last processed record instance of the chain, on which we extract <code class="literal">_attachment_body</code> and print it to standard output. The source code for <code class="literal">MorphlineDriver</code>
<a id="id1070" class="indexterm"></a> can be found at <a class="ulink" href="https://github.com/learninghadoop2/book-examples/blob/master/ch9/kite/src/main/java/com/learninghadoop2/kite/morphlines/MorphlineDriver.java" target="_blank">https://github.com/learninghadoop2/book-examples/blob/master/ch9/kite/src/main/java/com/learninghadoop2/kite/morphlines/MorphlineDriver.java</a>.</p><p>Using the same <a id="id1071" class="indexterm"></a>morphline from a MapReduce job is straightforward. During the setup phase of the Mapper, we build a context that contains the instantiation logic, while the map method sets the <code class="literal">Record</code> object up and fires off the processing logic, as follows:</p><div class="informalexample"><pre class="programlisting">public static class ReadTweets
        extends Mapper&lt;Object, Text, Text, NullWritable&gt; {
    private final Record record = new Record();
    private Command morphline;

    @Override
    protected void setup(Context context)
            throws IOException, InterruptedException {
        File morphlineConf = new File(context.getConfiguration()
                .get(MORPHLINE_CONF));
        String morphlineId = context.getConfiguration()
                .get(MORPHLINE_ID);
        MorphlineContext morphlineContext = 
new MorphlineContext.Builder()
                .build();

        morphline = new org.kitesdk.morphline.base.Compiler()
                .compile(morphlineConf,
                        morphlineId,
                        morphlineContext,
                        new RecordEmitter(context));
    }

    public void map(Object key, Text value, Context context)
            throws IOException, InterruptedException {
        record.put(Fields.ATTACHMENT_BODY,
                new ByteArrayInputStream(
value.toString().getBytes("UTF8")));
        if (!morphline.process(record)) {
              System.out.println(
"Morphline failed to process record: " + record);
        }

        record.removeAll(Fields.ATTACHMENT_BODY);
    }
}</pre></div><p>In the MapReduce <a id="id1072" class="indexterm"></a>code we modify <code class="literal">RecordEmitter</code> to extract the <code class="literal">Fields</code> payload from post-processed records and store it into context. This allows us to write data into HDFS by specifying a <code class="literal">FileOutputFormat</code> in the MapReduce configuration boilerplate:</p><div class="informalexample"><pre class="programlisting">private static final class RecordEmitter implements Command {
    private final Text line = new Text();
    private final Mapper.Context context;

    private RecordEmitter(Mapper.Context context) {
        this.context = context;
    }

    @Override
    public void notify(Record notification) {
    }

    @Override
    public Command getParent() {
        return null;
    }

    @Override
    public boolean process(Record record) {
        line.set(record.get(Fields.ATTACHMENT_BODY).toString());
        try {
            context.write(line, null);
        } catch (Exception e) {
            e.printStackTrace();
            return false;
        }
        return true;
    }
}   </pre></div><p>Notice that we can now change the processing pipeline behavior and add further data transformations by modifying <code class="literal">morphline.conf</code> without the explicit need to alter the instantiation and processing logic. The MapReduce driver source code<a id="id1073" class="indexterm"></a> can be found at <a class="ulink" href="https://github.com/learninghadoop2/book-examples/blob/master/ch9/kite/src/main/java/com/learninghadoop2/kite/morphlines/MorphlineDriverMapReduce.java" target="_blank">https://github.com/learninghadoop2/book-examples/blob/master/ch9/kite/src/main/java/com/learninghadoop2/kite/morphlines/MorphlineDriverMapReduce.java</a>.</p><p>Both examples can be compiled from <code class="literal">ch9/kite/</code> with the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ./gradlew jar</strong></span>
<span class="strong"><strong>$ ./gradlew copyJar</strong></span>
</pre></div><p>We add the <code class="literal">runtime</code> dependencies to <code class="literal">LIBJARS</code>, as follows</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ export KITE_DEPS=/home/cloudera/review/hadoop2book-private-reviews-gabriele-ch8/src/ch8/kite/build/libjars/kite-example/lib</strong></span>
<span class="strong"><strong>export LIBJARS=${LIBJARS},${KITE_DEPS}/kite-morphlines-core-0.17.0.jar,${KITE_DEPS}/kite-morphlines-json-0.17.0.jar,${KITE_DEPS}/metrics-core-3.0.2.jar,${KITE_DEPS}/metrics-healthchecks-3.0.2.jar,${KITE_DEPS}/config-1.0.2.jar,${KITE_DEPS}/jackson-databind-2.3.1.jar,${KITE_DEPS}/jackson-core-2.3.1.jar,${KITE_DEPS}/jackson-annotations-2.3.0.jar</strong></span>
</pre></div><p>We can run the <a id="id1074" class="indexterm"></a>MapReduce driver with the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hadoop jar build/libs/kite-example.jar \</strong></span>
<span class="strong"><strong>com.learninghadoop2.kite.morphlines.MorphlineDriverMapReduce \</strong></span>
<span class="strong"><strong>-libjars ${LIBJARS} \</strong></span>
<span class="strong"><strong>morphline.conf \</strong></span>
<span class="strong"><strong>read_tweets \</strong></span>
<span class="strong"><strong>tweets.json \</strong></span>
<span class="strong"><strong>morphlines-out</strong></span>
</pre></div><p>The Java standalone driver can be executed with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ export CLASSPATH=${CLASSPATH}:${KITE_DEPS}/kite-morphlines-core-0.17.0.jar:${KITE_DEPS}/kite-morphlines-json-0.17.0.jar:${KITE_DEPS}/metrics-core-3.0.2.jar:${KITE_DEPS}/metrics-healthchecks-3.0.2.jar:${KITE_DEPS}/config-1.0.2.jar:${KITE_DEPS}/jackson-databind-2.3.1.jar:${KITE_DEPS}/jackson-core-2.3.1.jar:${KITE_DEPS}/jackson-annotations-2.3.0.jar:${KITE_DEPS}/slf4j-api-1.7.5.jar:${KITE_DEPS}/guava-11.0.2.jar:${KITE_DEPS}/hadoop-common-2.3.0-cdh5.0.3.jar</strong></span>
<span class="strong"><strong>$ java -cp $CLASSPATH:./build/libs/kite-example.jar \</strong></span>
<span class="strong"><strong>com.learninghadoop2.kite.morphlines.MorphlineDriver \</strong></span>
<span class="strong"><strong>morphline.conf \</strong></span>
<span class="strong"><strong>read_tweets tweets.json \</strong></span>
<span class="strong"><strong>morphlines-out</strong></span>
</pre></div></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec70"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we introduced four tools to ease development on Hadoop. In particular, we covered:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>How Hadoop streaming allows the writing of MapReduce jobs using dynamic languages</p></li><li style="list-style-type: disc"><p>How Kite Data simplifies interfacing with heterogeneous data sources</p></li><li style="list-style-type: disc"><p>How Apache Crunch provides a high-level abstraction to write pipelines of Spark and MapReduce jobs that implement common design patterns</p></li><li style="list-style-type: disc"><p>How Morphlines allows us to declare chains of commands and data transformations that can then be embedded in any Java codebase</p></li></ul></div><p>In <a class="link" href="#" linkend="ch10">Chapter 10</a>, <span class="emphasis"><em>Running a Hadoop 2 Cluster</em></span>, we will shift our focus from the domain of software development to system administration. We will discuss how to set up, manage, and scale a Hadoop cluster, while taking aspects such as monitoring and security into consideration.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch10"></a>ChapterÂ 10.Â Running a Hadoop Cluster</h2></div></div></div><p>In this chapter, we will change our focus a little and look at some of the considerations you will face when running an operational Hadoop cluster. In particular, we will cover the following topics:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Why a developer should care about operations and why Hadoop operations are different</p></li><li style="list-style-type: disc"><p>More detail on Cloudera Manager and its capabilities and limitations</p></li><li style="list-style-type: disc"><p>Designing a cluster for use on both physical hardware and EMR</p></li><li style="list-style-type: disc"><p>Securing a Hadoop cluster</p></li><li style="list-style-type: disc"><p>Hadoop monitoring</p></li><li style="list-style-type: disc"><p>Troubleshooting problems with an application running on Hadoop</p></li></ul></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec71"></a>I'm a developer â€“ I don't care about operations!</h2></div></div><hr /></div><p>Before going any<a id="id1075" class="indexterm"></a> further, we need to explain why we are putting a chapter about systems operations in a book squarely aimed at developers. For anyone who has developed for more traditional platforms (for example, web apps, database programming, and so on) then the norm might well have been for a very clear delineation between development and operations. The first group builds the code and packages it up, and the second group controls and operates the environment in which it runs.</p><p>In recent years, the DevOps movement has gained momentum with a belief that it is best for everyone if these silos are removed and that the teams work more closely together. When it comes to running systems and services based on Hadoop, we believe this is absolutely essential.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec171"></a>Hadoop and DevOps practices</h3></div></div></div><p>Even though a<a id="id1076" class="indexterm"></a> developer can <a id="id1077" class="indexterm"></a>conceptually build an application ready to be dropped into YARN and forgotten about, the reality is often more nuanced. How many resources are allocated to the application at runtime is most likely something the developer wishes to influence. Once the application is running, the operations staff likely want some insight into the application when they are trying to optimize the cluster. There really isn't the same clear-cut split of responsibilities seen in traditional enterprise IT. And that's likely a really good thing.</p><p>In other words, developers need to be more aware of the operations aspects, and the operations staff need to be more aware of what the developers are doing. So consider this chapter our contribution to help you have those discussions with your operations staff. We don't intend to make you an expert Hadoop administrator by the end of this chapter; that really is emerging as a dedicated role and skillset in itself. Instead, we will give a whistle-stop tour of issues you do need some awareness of and that will make your life easier once your applications are running on live clusters.</p><p>By the nature of this coverage, we will be touching on a lot of topics and going into them only lightly; if any are of deeper interest, then we provide links for further investigation. Just make sure you keep your operations staff involved!</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec72"></a>Cloudera Manager</h2></div></div><hr /></div><p>In this book, we used as the most <a id="id1078" class="indexterm"></a>common platform the <span class="strong"><strong>Cloudera Hadoop Distribution</strong></span> (<span class="strong"><strong>CDH</strong></span>)<a id="id1079" class="indexterm"></a> with its convenient QuickStart virtual machine and the powerful Cloudera Manager application. With a Cloudera-based cluster, Cloudera Manager will become (at least initially) your primary interface into the system to manage and monitor the cluster, so let's explore it a little.</p><p>Note that Cloudera Manager has extensive and high-quality online documentation. We won't duplicate this documentation here; instead we'll attempt to highlight where Cloudera Manager fits into your development and operational workflows and how it might or might not be something you want to embrace. Documentation for the latest and previous versions of Cloudera Manager can be accessed via the main <a id="id1080" class="indexterm"></a>Cloudera documentation page at <a class="ulink" href="http://www.cloudera.com/content/support/en/documentation.html" target="_blank">http://www.cloudera.com/content/support/en/documentation.html</a>.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec172"></a>To pay or not to pay</h3></div></div></div><p>Before getting all <a id="id1081" class="indexterm"></a>excited about Cloudera Manager, it's important to consult the current documentation concerning what features are available in the free version and which ones require subscription to a paid-for Cloudera offering. If you absolutely want some of the features offered only in the paid-for version but either can't or don't wish to pay for subscription services, then Cloudera Manager, and possibly the entire Cloudera distribution, might not be a good fit for you. We'll return to this topic in <a class="link" href="#" linkend="ch11">Chapter 11</a>, <span class="emphasis"><em>Where to Go Next</em></span>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec173"></a>Cluster management using Cloudera Manager</h3></div></div></div><p>Using the <a id="id1082" class="indexterm"></a>QuickStart VM, it won't be obvious, but<a id="id1083" class="indexterm"></a> Cloudera Manager is the primary tool to be used for management of all services in the cluster. If you want to enable a new service, you'll use Cloudera Manager. To change a configuration, you will need Cloudera Manager. To upgrade to the latest release, you will again require Cloudera Manager.</p><p>Even if the primary management of the cluster is handled by operational staff, as a developer you'll likely still want to become familiar with the Cloudera Manager interface just to look to see exactly how the cluster is configured. If your jobs are running slowly, then looking into Cloudera Manager to see just how things are currently configured will likely be your first start. The default port for the Cloudera Manager web interface is <code class="literal">7180</code>, so the home page will usually be connected to via a URL such as <code class="literal">http://&lt;hostname&gt;:7180/cmf/home</code>, and can be seen in the following screenshot:</p><div class="mediaobject"><img src="graphics/5518OS_10_01.jpg" /><div class="caption"><p>Cloudera Manager home page</p></div></div><p>It's worth <a id="id1084" class="indexterm"></a>poking around the interface; however, if you are<a id="id1085" class="indexterm"></a> connecting with a user account with admin privileges, be careful!</p><p>Click on the <span class="strong"><strong>Clusters</strong></span> link, and this will expand to give a list of the clusters currently managed by this instance of Cloudera Manager. This should tell you that a single Cloudera Manager instance can manage multiple clusters. This is very useful, especially if you have many clusters spread across development and production.</p><p>For each expanded cluster, there will be a list of the services currently running on the cluster. Click on a service, and then you will see a list of additional choices. Select <span class="strong"><strong>Configuration</strong></span>, and you can start browsing the detailed configuration of that particular service. Click on <span class="strong"><strong>Actions</strong></span>, and you will get some service-specific options; this will usually include stopping, starting, restarting, and otherwise managing the service.</p><p>Click on the <span class="strong"><strong>Hosts</strong></span> option instead of <span class="strong"><strong>Clusters</strong></span>, and you can start drilling down into the servers managed by Cloudera Manager, and from there, see which service components are deployed on each.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch10lvl3sec86"></a>Cloudera Manager and other management tools</h4></div></div></div><p>That last comment<a id="id1086" class="indexterm"></a> might raise a question: how does <a id="id1087" class="indexterm"></a>Cloudera Manager integrate with other systems management tools? Given our earlier comments regarding the importance of DevOps philosophies, how well does it integrate with the tools favored in DevOps environments?</p><p>The honest answer: not always very well. Though the main Cloudera Manager server can itself be managed by automation tools, such as Puppet or Chef, there is an explicit assumption that Cloudera Manager will control the installation and configuration of all the software Cloudera Manager needs on all the hosts that will be included in its clusters. To some administrators, this makes the hardware behind Cloudera Manager look like a big, black box; they might control the installation of the base operating system, but the management of the configuration baseline going forward is entirely managed by Cloudera Manager. There's nothing much to be done here; it is what it isâ€”to get the benefits of Cloudera Manager, it will add itself as a new management system in your infrastructure, and how well that fits in with your broader environment will be determined on a case-by-case basis.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec174"></a>Monitoring with Cloudera Manager</h3></div></div></div><p>A similar point can be made<a id="id1088" class="indexterm"></a> regarding systems monitoring as Cloudera Manager is also conceptually a point of duplication here. But start clicking around the interface, and it will become apparent very quickly that Cloudera Manager provides an exceptionally rich set of tools to assess the health and performance of managed clusters. </p><p>From graphing the relative performance of Impala queries through showing the job status for YARN applications and giving low-level data on the blocks stored on HDFS, it is all there in a single interface. We'll discuss later in this chapter how troubleshooting on Hadoop can be challenging, but the single point of visibility provided by Cloudera Manager is a great tool when looking to assess cluster health or performance. We'll discuss monitoring in a little more detail later in this chapter.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch10lvl3sec87"></a>Finding configuration files</h4></div></div></div><p>One of the first confusions faced when<a id="id1089" class="indexterm"></a> running a cluster managed by Cloudera Manager is trying to find the configuration files used by the cluster. In the vanilla Apache releases of products, such as the core Hadoop, there would be files typically stored in <code class="literal">/etc/hadoop</code>, similarly <code class="literal">/etc/hive</code> for Hive, <code class="literal">/etc/oozie</code> for Oozie, and so on.</p><p>In a Cloudera Manager managed cluster, however, the config files are regenerated each time a service is restarted, and instead of sitting in the <code class="literal">/etc</code> locations on the filesystem, will be found at <code class="literal">/var/run/cloudera-scm-agent-process/&lt;pid&gt;-&lt;task name&gt;/</code>, where the last directory might have a name such as <code class="literal">7007-yarn-NODEMANAGER</code>. This might seem odd to anyone used to working on earlier Hadoop clusters or other distributions that don't do such a thing. But in a Cloudera Manager-controlled cluster, it might often be easier to use the web interface to browse the configuration instead of looking for the underlying config files. Which approach is best? This is a little philosophical, and each team needs to decide which works best for them.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec175"></a>Cloudera Manager API</h3></div></div></div><p>We've only given the<a id="id1090" class="indexterm"></a> highest level of overview of Cloudera Manager, and in doing so, have completely ignored one area that might be very useful for some organizations: Cloudera Manager offers an API that allows integration of its capabilities into other systems and tools. Consult the documentation if this might be of interest to you.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec176"></a>Cloudera Manager lock-in</h3></div></div></div><p>This brings us to the point that is <a id="id1091" class="indexterm"></a>implicit in the whole discussion around Cloudera Manager: it does cause a degree of lock-in to Cloudera and their distribution. That lock-in might only exist in certain ways; code, for example, should be portable across clusters modulo the usual caveats about different underlying versionsâ€”but the cluster itself might not easily be reconfigured to use a different distribution. Assume that switching distributions would be a complete remove/reformat/reinstall activity.</p><p>We aren't saying don't use it, rather that you need to be aware of the lock-in that comes with the use of Cloudera Manager. For small teams with little dedicated operations support or existing infrastructure, the impact of such a lock-in is likely outweighed by the significant capabilities that Cloudera Manager gives you.</p><p>For larger teams or ones working in an environment where integration with existing tools and processes has more weight, the decision might be less clear. Look at Cloudera Manager, discuss with your operations people, and determine what is right for you.</p><p>Note that it is possible to manually <a id="id1092" class="indexterm"></a>download and install the various components of the Cloudera distribution without using Cloudera Manager to manage the cluster and its hosts. This might be an attractive middle ground for some users as the Cloudera software can be used, but deployment and management can be built into the existing deployment and management tools. This is also potentially a way of avoiding the additional expense of the paid-for levels of Cloudera support mentioned earlier.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec73"></a>Ambari â€“ the open source alternative</h2></div></div><hr /></div><p>Ambari is<a id="id1093" class="indexterm"></a> an Apache <a id="id1094" class="indexterm"></a>project (<a class="ulink" href="http://ambari.apache.org" target="_blank">http://ambari.apache.org</a>), which in theory, provides an open source alternative to Cloudera Manager. It is the administration console for the Hortonworks distribution. At the time of writing Hortonworks employees are also the vast majority of the project contributors.</p><p>Ambari, as one would expect given its open source nature, relies on other open source products, such as Puppet and Nagios, to provide the management and monitoring of its managed clusters. It also has high-level functionality similar to Cloudera Manager, that is, the installation, configuration, management, and monitoring of a Hadoop cluster, and the component services within it.</p><p>It is good to be aware of the Ambari project as the choice is not just between full lock-in to Cloudera and Cloudera Manager or a manually managed cluster. Ambari provides a graphical tool that might be worth consideration, or indeed involvement, as it matures. On an HDP cluster, the Ambari UI equivalent to the Cloudera Manager home page shown earlier can be reached at <code class="literal">http://&lt;hostname&gt;:8080/#/main/dashboard</code> and looks like the following screenshot:</p><div class="mediaobject"><img src="graphics/5518OS_10_17.jpg" /><div class="caption"><p>Ambari</p></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec74"></a>Operations in the Hadoop 2 world</h2></div></div><hr /></div><p>As mentioned in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Storage</em></span>, some of the most significant changes made to HDFS in Hadoop 2<a id="id1095" class="indexterm"></a> involve its fault tolerance and better integration with external systems. This is<a id="id1096" class="indexterm"></a> not just a curiosity, but the NameNode High Availability features, in particular, have made a massive difference in the management of clusters since Hadoop 1. In the bad old days of 2012 or so, a significant part of the operational preparedness of a Hadoop cluster was built around mitigations for, and restoration processes around failure of the NameNode. If the NameNode died in Hadoop 1, and you didn't have a backup of the HDFS <code class="literal">fsimage</code> metadata file, then you basically lost access to all your data. If the metadata was permanently lost, then so was the data.</p><p>Hadoop 2 has added the in-built NameNode HA and the machinery to make it work. In addition, there are components such as the NFS gateway into HDFS, which make it a much more flexible system. But this additional capability does come at the expense of more moving parts. To enable NameNode HA, there are additional components in the JournalManager and FailoverController, and the NFS gateway requires Hadoop-specific implementations of the portmap and nfsd services.</p><p>Hadoop 2 also now has <a id="id1097" class="indexterm"></a>extensive other integration points with external services as well as a much broader selection of applications and services that run atop it. Consequently, it might be useful to view Hadoop 2 in terms of operations as having traded the simplicity of Hadoop 1 for additional complexity, which delivers a substantially more capable platform.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec75"></a>Sharing resources</h2></div></div><hr /></div><p>In Hadoop 1, the only time one had to consider<a id="id1098" class="indexterm"></a> resource sharing was in considering which scheduler to use for the MapReduce JobTracker. Since all jobs were eventually translated into MapReduce code having  a policy for resource sharing at the MapReduce level was usually sufficient to manage cluster workloads in the large.</p><p>Hadoop 2 and YARN changed this picture. As well as running many MapReduce jobs, a cluster might also be running many other applications atop other YARN ApplicationMasters. Tez and Spark are frameworks in their own right that run additional applications atop their provided interfaces.</p><p>If everything runs on YARN, then it provides ways of configuring the maximum resource allocation (in terms of CPU, memory, and soon I/O) consumed by each container allocated to an application. The primary goal here is to ensure that enough resources are allocated to keep the hardware fully utilized without either having unused capacity or overloading it.</p><p>Things get somewhat more interesting when non-YARN applications, such as Impala, are running on the cluster and want to grab allocated slices of capacity (particularly memory in the case of Impala). This could also happen if, say, you were running Spark on the same hosts in its non-YARN mode or indeed any other distributed application that might benefit from co-location on the Hadoop machines.</p><p>Basically, in Hadoop 2, you need to think of the cluster as much more of a multi-tenancy environment that requires more attention given to the allocation of resources to the various tenants.</p><p>There really is no silver bullet recommendation here; the right configuration will be entirely dependent on the services co-located and the workloads they are running. This is another example where you want to work closely with your operations team to do a series of load tests with thresholds to determine just what the resource requirements of the various clients are and which approach will give the maximum utilization and performance. The following <a id="id1099" class="indexterm"></a>blog post from Cloudera engineers gives a good overview of how they approach this very issue in having Impala and MapReduce coexist <a id="id1100" class="indexterm"></a>effectively: <a class="ulink" href="http://blog.cloudera.com/blog/2013/06/configuring-impala-and-mapreduce-for-multi-tenant-performance/" target="_blank">http://blog.cloudera.com/blog/2013/06/configuring-impala-and-mapreduce-for-multi-tenant-performance/</a>.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec76"></a>Building a physical cluster</h2></div></div><hr /></div><p>There is one minor <a id="id1101" class="indexterm"></a>requirement before thinking about allocation of hardware resources: defining and selecting the hardware used for your cluster. In this section, we'll discuss a physical cluster and move on to Amazon EMR in the next.</p><p>Any specific hardware advice will be out of date the moment it is written. We advise perusing the websites of the various Hadoop distribution vendors as they regularly write new articles on the currently recommended configurations.</p><p>Instead of telling you how many cores or GB of memory you need, we'll look at hardware selection at a slightly higher level. The first thing to realize is that the hosts running your Hadoop cluster will most likely look very different from the rest of your enterprise. Hadoop is optimized for low(er) cost hardware, so instead of seeing a small number of very large servers, expect to see a larger number of machines with fewer enterprise reliability features. But don't think that Hadoop will run great on any junk you have lying around. It might, but recently the profile of typical Hadoop servers has been moving away from the bottom-end of the market, and instead, the sweet spot would seem to be in mid-range servers where the maximum cores/disks/memory can be achieved at a price point.</p><p>You should also expect to have different resource requirements for the hosts running services such as the HDFS NameNode or the YARN ResourceManager, as opposed to the worker nodes storing data and executing the application logic. For the former, there is usually much less requirement for lots of storage, but frequently, a need for more memory and possibly faster disks.</p><p>For Hadoop worker nodes, the ratio between the three main hardware categories of cores, memory, and I/O is often the most important thing to get right. And this will directly inform the decisions you make regarding workload and resource allocation.</p><p>For example, many workloads tend to become I/O bound and having many times as many containers allocated on a host than there are physical disks might actually cause an overall slowdown due to contention for the spinning disks. At the time of writing, current recommendations here are for the number of YARN containers to be no more than 1.8 times the number of disks. If you have workloads that are I/O bound, then you will most likely get much better performance by adding more hosts to the cluster instead of trying to get more containers running or indeed faster processors or more memory on the current hosts.</p><p>Conversely, if you <a id="id1102" class="indexterm"></a>expect to run lots of concurrent Impala, Spark, and other memory-hungry jobs, then memory might quickly become the resource most under pressure. This is why even though you can get current hardware recommendations for general-purpose clusters from the distribution vendors, you still need to validate against your expected workloads and tailor accordingly. There is really no substitute for benchmarking on a small test cluster or indeed on EMR, which can be a great platform to explore the resource requirements of multiple applications that can inform hardware acquisition decisions. Perhaps EMR might be your main environment; if so, we'll discuss that in a later section.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec177"></a>Physical layout</h3></div></div></div><p>If you do use a physical cluster, there <a id="id1103" class="indexterm"></a>are a few things you will need to consider that are largely transparent on EMR.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch10lvl3sec88"></a>Rack awareness</h4></div></div></div><p>The first of these aspects for <a id="id1104" class="indexterm"></a>clusters large enough to consume more than one rack of data center space is building rack awareness. As mentioned in <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Storage</em></span>, when HDFS places replicas of new files, it attempts to place the second replica on a different host than the first, and the third in a different rack of equipment in a multi-rack system. This heuristic is aimed at maximizing resilience; there will be at least one replica available even if an entire rack of equipment fails. MapReduce uses similar logic to attempt to get a better-balanced task spread.</p><p>If you do nothing, then each host will be specified as being in the single default rack. But, if the cluster grows beyond this point, you will need to update the rack name.</p><p>Under the covers, Hadoop discovers a node's rack by executing a user-supplied script that maps node hostname to rack names. Cloudera Manager allows rack names to be set on a given host, and this is then retrieved when its rack awareness scripts are called by Hadoop. To set the rack for a host, click on <span class="strong"><strong>Hosts-&gt;&lt;hostname&gt;-&gt;Assign Rack</strong></span>, and then assign the rack from the Cloudera Manager home page.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch10lvl3sec89"></a>Service layout</h4></div></div></div><p>As mentioned earlier, you are likely<a id="id1105" class="indexterm"></a> to have two types of hardware in your cluster: the machines running the workers and those running the servers. When deploying a physical cluster, you will need to decide which services and which subcomponents of the services run on which physical machines.</p><p>For the workers, this is usually pretty straightforward; most, though not all, services have a model of a worker agent on all worker hosts. But, for the master/server components, it requires a little thought. If you have three master nodes, then how do you spread your primary and backup NameNodes: the YARN ResourceManager, maybe Hue, a few Hive servers, and an Oozie manager? Some of these features are highly available, while others are not. As you add more and more services to your cluster, you'll also see this list of master services grow substantially.</p><p>In an ideal world, you might have a host per service master but that is only tractable for very large clusters; in smaller installations it is prohibitively expensive. Plus it might always be a little wasteful. There are no hard-and-fast rules here either, but do look at your available hardware, and try to spread the services across the nodes as much as possible. Don't, for example, have two nodes for the two NameNodes and then put everything else on a third. Think about the impact of a single host failure and manage the layout to minimize it. As the cluster grows across multiple racks of equipment, the considerations will also need to consider how to survive single-rack failures. Hadoop itself helps with this since HDFS will attempt to ensure each block of data has replicas across at least two racks. But, this type of resilience is undermined if, for example, all the master nodes reside in a single rack.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch10lvl3sec90"></a>Upgrading a service</h4></div></div></div><p>Upgrading Hadoop has historically been a <a id="id1106" class="indexterm"></a>time-consuming and somewhat risky task. This remains the case on a manually deployed cluster, that is, one not managed by a tool such as Cloudera Manager.</p><p>If you are using Cloudera Manager, then it takes the time-consuming part out of the activity, but not necessarily the risk. Any upgrade should always be viewed as an activity with a high chance of unexpected issues, and you should arrange enough cluster downtime to account for this surprise excitement. There's really no substitute for doing a test upgrade on a test cluster, which underlines the importance of thinking about Hadoop as a component of your environment that needs to be treated with a deployment life cycle like any other.</p><p>Sometimes an upgrade requires modification to the HDFS metadata or might otherwise affect the filesystem. This is, of course, where the real risks lie. In addition to running a test upgrade, be aware of the ability to set HDFS in upgrade mode, which effectively makes a snapshot of the filesystem state prior to the upgrade and which will be retained until the upgrade is finalized. This<a id="id1107" class="indexterm"></a> can be really helpful as even an upgrade that goes badly wrong and corrupts data can potentially be fully rolled back.</p></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec77"></a>Building a cluster on EMR</h2></div></div><hr /></div><p>Elastic MapReduce is a<a id="id1108" class="indexterm"></a> flexible solution that, depending on requirements <a id="id1109" class="indexterm"></a>and workloads, can sit next to, or replace, a physical Hadoop cluster. As we've seen so far, EMR provides clusters preloaded and configured with Hive, Streaming, and Pig as well as with custom JAR clusters that allow the execution of MapReduce applications.</p><p>A second distinction to make is between transient and long-running life cycles. A transient EMR cluster is generated on demand; data is loaded in S3 or HDFS, some processing workflow is executed, output results are stored, and the cluster is automatically shut down. A long-running cluster is kept alive once the workflow terminates, and the cluster remains available for new data to be copied over and new workflows to be executed. Long-running clusters are typically well-suited for data warehousing or working with datasets large enough that loading and processing data would be inefficient compared to a transient instance.</p><p>In a must-read white paper for <a id="id1110" class="indexterm"></a>prospective users (found at <a class="ulink" href="https://media.amazonwebservices.com/AWS_Amazon_EMR_Best_Practices.pdf" target="_blank">https://media.amazonwebservices.com/AWS_Amazon_EMR_Best_Practices.pdf</a>), Amazon gives a heuristic to estimate which cluster type is a better fit as follows:</p><p><span class="emphasis"><em>If number of jobs per day * (time to setup cluster including Amazon S3 data load time if using Amazon S3 + data processing time) &lt; 24 hours, consider transient Amazon EMR clusters or physical instances. Long-running instances are instantiated by passing the â€“alive argument to the ElasticMapreduce command, which enables the Keep Alive option and disables auto termination.</em></span></p><p>Note that transient and long-running clusters share the same properties and limitations; in particular, data on HDFS is not persisted once the cluster is shut down.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec178"></a>Considerations about filesystems</h3></div></div></div><p>In our examples so far we assumed <a id="id1111" class="indexterm"></a>data to be available in S3. In this case, a bucket is mounted in EMR as an <code class="literal">s3n</code> filesystem, and it is used as input source as well as a temporary filesystem to store intermediate data in computations. With S3 we introduce potential I/O overhead, operations such as reads and writes fire off <code class="literal">GET</code> and <code class="literal">PUT HTTP</code> requests.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note32"></a>Note</h3><p>Note that EMR does not support S3 block storage. The s3 URI maps to s3n.</p></div><p>Another option would be to load data into the cluster HDFS and run processing from there. In this case, we do have faster I/O and data locality, but we would lose persistence. When the cluster is shut down, our data disappears. As a rule of thumb, if you are running a transient cluster, it makes sense to use S3 as a backend. In practice, one should monitor and take decisions based on the workflow characteristics. Iterative, multi-pass MapReduce jobs would greatly benefit from HDFS; one could argue that for those types of workflows, an execution engine like Tez or Spark would be more appropriate.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec179"></a>Getting data into EMR</h3></div></div></div><p>When copying data from <a id="id1112" class="indexterm"></a>HDFS to S3, it is recommended to use<a id="id1113" class="indexterm"></a> s3distcp (<a class="ulink" href="http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/UsingEMR_s3distcp.html" target="_blank">http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/UsingEMR_s3distcp.html</a>) instead of Apache distcp or Hadoop distcp. This approach is suitable also to transfer data within EMR and from S3 to HDFS. To move very large amounts of data from the local disk into S3, Amazon recommends parallelizing the workload using Jets3t or GNU Parallel. In general, it's important to be aware that PUT requests to S3 are capped at 5 GB per file. To upload larger files, one needs to rely on <a id="id1114" class="indexterm"></a>Multipart Upload (<a class="ulink" href="https://aws.amazon.com/about-aws/whats-new/2010/11/10/Amazon-S3-Introducing-Multipart-Upload/" target="_blank">https://aws.amazon.com/about-aws/whats-new/2010/11/10/Amazon-S3-Introducing-Multipart-Upload/</a>), an API that allows splitting large files into smaller parts and reassembles them when uploaded. Files can also be copied with tools such as the AWS CLI or the popular S3CMD utility, but these do not have the parallelism advantages of as s3distcp.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec180"></a>EC2 instances and tuning</h3></div></div></div><p>The size of an EMR cluster <a id="id1115" class="indexterm"></a>depends on the dataset size, the number of files and<a id="id1116" class="indexterm"></a> blocks (determines the number of splits) and the type of workload (try to avoid spilling to disk when a task runs out of memory). As a rule of thumb, a good size is one that maximizes parallelism. The number of mappers and reducers per instance as well as heap size per JVM daemon is generally configured by EMR when the cluster is provisioned and tuned in the event of changes in the available resources.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec78"></a>Cluster tuning</h2></div></div><hr /></div><p>In addition to the previous comments <a id="id1117" class="indexterm"></a>specific to a cluster run on EMR, there are some general thoughts to keep in mind when running workloads on any type of cluster. This will, of course, be more explicit when running outside of EMR as it often abstracts some of the details.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec181"></a>JVM considerations</h3></div></div></div><p>You should be running the<a id="id1118" class="indexterm"></a> 64-bit version of a JVM and using the server mode. This<a id="id1119" class="indexterm"></a> can take longer to produce optimized code, but it also uses more aggressive strategies and will re-optimize code over time. This makes it a much better fit for long-running services, such as Hadoop processes.</p><p>Ensure that you allocate enough memory to the JVM to prevent overly-frequent <span class="strong"><strong>Garbage Collection</strong></span> (<span class="strong"><strong>GC</strong></span>)<a id="id1120" class="indexterm"></a> pauses. The concurrent mark-and-sweep collector is currently the most tested and recommended for Hadoop. The <span class="strong"><strong>Garbage First</strong></span> (<span class="strong"><strong>G1</strong></span>) collector<a id="id1121" class="indexterm"></a> has become the GC option of choice in numerous other workloads since its introduction with JDK7, so it's worth monitoring recommended best practice as it evolves. These options can be configured as custom Java arguments within each service's configuration section of Cloudera Manager.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch10lvl3sec91"></a>The small files problem</h4></div></div></div><p>Heap allocation to<a id="id1122" class="indexterm"></a> Java processes on worker nodes will be something you consider when thinking about service co-location. But there is a particular situation regarding the NameNode you should be aware of: the small files problem.</p><p>Hadoop is optimized for very large files with large block sizes. But sometimes particular workloads or data sources push many small files onto HDFS. This is most likely suboptimal as it suggests each task processing a block at a time will read only a small amount of data before completing, causing inefficiency. </p><p>Having many small files also consumes more NameNode memory; it holds in-memory the mapping from files to blocks and consequently holds metadata for each file and block. If the number of files and hence blocks increases quickly, then so will the NameNode memory usage. This is likely to only hit a subset of systems as, at the time of writing this, 1 GB of memory can support 2 million files or blocks, but with a default heap size of 2 or 4 GB, this limit can easily be reached. If the NameNode needs to start very aggressively running garbage collection or eventually runs out of memory, then your cluster will be very unhealthy. The mitigation is to assign more heap to the JVM; the longer-term approach is to combine many small files into a smaller number of larger ones. Ideally, compressed with a splittable <a id="id1123" class="indexterm"></a>compression codec.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec182"></a>Map and reduce optimizations</h3></div></div></div><p>Mappers and reducers both <a id="id1124" class="indexterm"></a>provide areas for optimizing performance; here<a id="id1125" class="indexterm"></a> are a few pointers to consider:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The number of mappers <a id="id1126" class="indexterm"></a>depends on <a id="id1127" class="indexterm"></a>the number of splits. When files are smaller than the default block size or compressed using a non splittable format, the number of mappers will equal the number of files. Otherwise, the number of mappers is given by the total size of each file divided by the block size.</p></li><li style="list-style-type: disc"><p>Compress mappers output to reduce writes to disk and increase I/O. LZO is a good format for this task.</p></li><li style="list-style-type: disc"><p>Avoid spill to disk: the mappers should have enough memory to retain as much data as possible.</p></li><li style="list-style-type: disc"><p>Number of Reducers: it is recommended that you use fewer reducers than the total reducer capacity (this avoids execution waits).</p></li></ul></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec79"></a>Security</h2></div></div><hr /></div><p>Once you built a cluster, the first<a id="id1128" class="indexterm"></a> thing you thought about was how to secure it, right? Don't worry, most people don't. But, as Hadoop has moved on from being something running in-house analysis in the research department to directly driving critical systems, it's not something to ignore for too long.</p><p>Securing Hadoop is not something to be done on a whim or without significant testing. We cannot give detailed advice on this topic and cannot stress strongly enough the need to take this topic seriously and do it properly. It might consume time, it might cost money, but weigh this against the cost of having your cluster compromised.</p><p>Security is also a much bigger topic than just the Hadoop cluster. We'll explore some of the security features available in Hadoop, but you do need a coherent security strategy into which these discrete components fit.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec183"></a>Evolution of the Hadoop security model</h3></div></div></div><p>In Hadoop 1, there was <a id="id1129" class="indexterm"></a>effectively no security protection as the provided security model had obvious attack vectors. The Unix user ID with which you connected to the cluster was assumed to be valid, and you had all the privileges of that user. Plainly, this meant that anyone with administrative access on a host that could access the cluster could effectively impersonate any other user.</p><p>This led to the development of the so-called "head node" access model, whereby the Hadoop cluster was firewalled off from every host except one, the head node, and all access to the cluster was mediated through this centrally-controlled node. This was an effective mitigation for the lack of a real security model and can still be useful in situations even when richer security schemes are utilized.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec184"></a>Beyond basic authorization</h3></div></div></div><p>Core Hadoop has had additional security features<a id="id1130" class="indexterm"></a> added, which address the previous concerns. In particular, they address the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>A cluster can require a user to authenticate via Kerberos and prove they are who they say they are.</p></li><li style="list-style-type: disc"><p>In secure mode, the cluster can also use Kerberos for all node-node communications, ensuring that all communicating nodes are authenticated and preventing malicious nodes from attempting to join the cluster.</p></li><li style="list-style-type: disc"><p>To ease management, users can be collected into groups against which data-access privileges can be defined. This is called <span class="strong"><strong>Role Based Access Control</strong></span> (<span class="strong"><strong>RBAC</strong></span>)<a id="id1131" class="indexterm"></a> and is a prerequisite for a secure cluster with more than a handful of users. The user-group mappings can be retrieved from corporate systems, such as LDAP or active directory.</p></li><li style="list-style-type: disc"><p>HDFS can apply ACLs to replace the current Unix-inspired owner/group/world model.</p></li></ul></div><p>These capabilities give <a id="id1132" class="indexterm"></a>Hadoop a significantly stronger security posture than in the past, but the community is moving fast and additional dedicated Apache projects have emerged to address specific areas of security.</p><p>Apache<a id="id1133" class="indexterm"></a> Sentry <a class="ulink" href="https://sentry.incubator.apache.org" target="_blank">https://sentry.incubator.apache.org</a> is a system to provide much finer-grained authorization to Hadoop data and services. Other services build Sentry mappings, and this allows, for example, specific restrictions to be placed not only on particular HDFS directories, but also on entities such as Hive tables.</p><p>Whereas Sentry focuses on providing much richer tools for the internal, fine-grained aspects of Hadoop security, Apache Knox (<a class="ulink" href="http://knox.apache.org" target="_blank">http://knox.apache.org</a>) provides<a id="id1134" class="indexterm"></a> a secure gateway <a id="id1135" class="indexterm"></a>to Hadoop that integrates with external identity management systems and provides access control mechanisms to allow or disallow access to specific Hadoop services and operations. It does this by presenting a REST-only interface to Hadoop and securing all calls to this API.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec185"></a>The future of Hadoop security</h3></div></div></div><p>There are many other<a id="id1136" class="indexterm"></a> developments happening in the Hadoop world. Core Hadoop 2.5 added extended file attributes to HDFS, which can be used as the basis of additional access control mechanisms. Future versions will incorporate capabilities for better support of encryption for data in transit as well as at rest, and the Project Rhino<a id="id1137" class="indexterm"></a> initiative led by Intel (<a class="ulink" href="https://github.com/intel-hadoop/project-rhino/" target="_blank">https://github.com/intel-hadoop/project-rhino/</a>) is building out richer support for filesystem cryptographic modules, a secure filesystem, and, at some point, a fuller key-management infrastructure.</p><p>The Hadoop distribution vendors are moving fast to add these capabilities to their releases, so if you care about security (you do, don't you!), then consult the documentation for the latest release of your distribution. New security features are being added even in point updates and aren't being delayed until major upgrades.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec186"></a>Consequences of using a secured cluster</h3></div></div></div><p>After teasing you with all the <a id="id1138" class="indexterm"></a>security goodness that is now available and that which is coming, it's only fair to give some words of warning. Security is often hard to do correctly, and often the feeling of security wrongly employed with a buggy deployment is worse than knowing you have no security.</p><p>However, even if you do it right, there are consequences to running a secure cluster. It makes things harder for the administrators certainly and often the users, so there is definitely an overhead. Specific Hadoop tools and services will also work differently depending on what security is employed on a cluster.</p><p>Oozie, which we discussed in <a class="link" href="#" linkend="ch08">Chapter 8</a>, <span class="emphasis"><em>Data Lifecycle Management</em></span>, uses its own delegation tokens behind the scenes. This allows the oozie user to submit jobs that are then executed on behalf of the originally submitting user. In a cluster using only the basic authorization mechanism, this is very easily configured, but using Oozie in a secure cluster will require additional logic to be added to the workflow definitions and the general Oozie configuration. This isn't a problem with Hadoop or Oozie; however, similarly as with the additional complexity resulting from the much better HA features of HDFS in Hadoop 2, better security mechanisms will simply have costs and consequences that you need take into consideration.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec80"></a>Monitoring</h2></div></div><hr /></div><p>Earlier in this chapter, we discussed Cloudera Manager as a visual<a id="id1139" class="indexterm"></a> monitoring tool and hinted that it could also be programmatically integrated with other monitoring systems. But before plugging Hadoop into any monitoring framework, it's worth considering just what it means to operationally monitor a Hadoop cluster.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec187"></a>Hadoop â€“ where failures don't matter</h3></div></div></div><p>Traditional systems monitoring<a id="id1140" class="indexterm"></a> tends to be quite a binary tool; generally speaking, either something is working or it isn't. A host is alive or dead, and a web server is responding or it isn't. But in the Hadoop world, things are a little different; the important thing is service availability, and this can still be treated as live even if particular pieces of hardware or software have failed. No Hadoop cluster should be in trouble if a single worker node fails. As of Hadoop 2, even the failure of the server processes, such as the NameNode shouldn't really be a concern if HA is configured. So, any monitoring of Hadoop needs to take into account the service health and not that of specific host machines, which should be unimportant. Operations people on 24/7 pager are not going to be happy getting paged at 3 AM to discover that one worker node in a cluster of 10,000 has failed. Indeed, once the scale of the cluster increases beyond a certain point, the failure of individual pieces of hardware becomes an almost commonplace occurrence.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec188"></a>Monitoring integration</h3></div></div></div><p>You won't be building your own <a id="id1141" class="indexterm"></a>monitoring tools; instead, you might likely want to integrate with existing tools and frameworks. For popular open source monitoring tools, such as Nagios and Zabbix, there are multiple sample templates to integrate Hadoop's service-wide and node-specific metrics. </p><p>This can give the sort of separation hinted previously; the failure of the YARN ResourceManager would be a high-criticality event that should most likely cause alerts to be sent to operations staff, but a high load on specific hosts should only be captured and not cause alerts to be fired. This then provides the duality of firing alerts when bad things happen in addition to capturing and providing the information needed to delve into system data over time to do trend analysis.</p><p>Cloudera Manager provides a REST interface, which is another point of integration against which tools such as Nagios can integrate and pull the Cloudera Manager-defined service-level metrics instead of having to define its own.</p><p>For heavier-weight enterprise-monitoring infrastructure built on frameworks, such as IBM Tivoli or HP OpenView, Cloudera Manager can also deliver events via SNMP traps that will be collected by these systems.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec189"></a>Application-level metrics</h3></div></div></div><p>At times, you might also want your<a id="id1142" class="indexterm"></a> applications to gather metrics that can be centrally captured within the system. The mechanisms for this will differ from one computational model to another, but the most well-known are the application counters available within MapReduce.</p><p>When a MapReduce job completes, it outputs a number of counters, gathered by the system throughout the job execution, that deal with metrics such as the number of map tasks, bytes written, failed tasks, and so on. You can also write application-specific metrics that will be available alongside the system counters and which are automatically aggregated across the map/reduce execution. First define a Java enum, and name your desired metrics within it, as follows:</p><div class="informalexample"><pre class="programlisting">public enum AppMetrics{
  MAX_SEEN,
  MIN_SEEN,
  BAD_RECORDS 
};</pre></div><p>Then, within <a id="id1143" class="indexterm"></a>the map, reduce, setup, and cleanup methods of your Map or Reduce implementations, you can do something like the following to increment a counter by one:</p><div class="informalexample"><pre class="programlisting">Context.getCounter(AppMetrics.BAD_RECORDS).increment(1);</pre></div><p>Refer to the JavaDoc of the <code class="literal">org.apache.hadoop.mapreduce.Counter</code> interface for more details of this mechanism.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec81"></a>Troubleshooting</h2></div></div><hr /></div><p>Monitoring and logging counters or <a id="id1144" class="indexterm"></a>additional information is all well and good, but it can be intimidating to know how to actually find the information you need when troubleshooting a problem with an application. In this section, we will look at how Hadoop stores logs and system information. We can distinguish three typologies of logs, as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>YARN applications, including MapReduce jobs</p></li><li style="list-style-type: disc"><p>Daemon logs (NameNode and ResourceManager)</p></li><li style="list-style-type: disc"><p>Services that log non-distributed workloads, for example, HiveServer2 logging to <code class="literal">/var/log</code>
</p></li></ul></div><p>Next to these log typologies, Hadoop exposes a number of metrics at filesystem (the storage availability, replication factor, and number of blocks) and system level. As mentioned, both Apache Ambari and Cloudera Manager, which centralize access to debug information, do a nice job as the frontend. However, under the hood, each service logs to either HDFS or the single-node filesystem. Furthermore, YARN, MapReduce, and HDFS expose their logfiles and metrics via web interfaces and programmatic APIs.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec190"></a>Logging levels</h3></div></div></div><p>Hadoop logs messages to Log4j by default. Log4j<a id="id1145" class="indexterm"></a> is configured via <code class="literal">log4j.properties</code> in the classpath. This file<a id="id1146" class="indexterm"></a> defines what is logged and with which layout:</p><div class="informalexample"><pre class="programlisting">log4j.rootLogger=${root.logger}
root.logger=INFO,console
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.err
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n</pre></div><p>The default root logger is <code class="literal">INFO,console</code>, which logs all messages at the level <code class="literal">INFO</code> and above to the console's <code class="literal">stderr</code>. Single applications deployed on Hadoop can ship their own <code class="literal">log4j.properties</code> and set the level and other properties of their emitted logs as required.</p><p>Hadoop daemons have a web page to get and set the log level for any Log4j property. This interface is exposed by the <code class="literal">/LogLevel</code> endpoint in each service web UI. To enable debug logging for the <code class="literal">ResourceManager</code> class, we will visit <code class="literal">http://resourcemanagerhost:8088/LogLevel</code>, and the screenshot can be seen as follows:</p><div class="mediaobject"><img src="graphics/5518OS_10_02.jpg" /><div class="caption"><p>Getting and setting the log level on ResourceManager</p></div></div><p>Alternatively, the <a id="id1147" class="indexterm"></a>YARN <code class="literal">daemonlog &lt;host:port&gt;</code> command interfaces with the <code class="literal">service /LogLevel</code> endpoint. We can inspect the level associated with <code class="literal">mapreduce.map.log.level</code> for the <code class="literal">ResourceManager</code> class using the <code class="literal">â€“getlevel &lt;property&gt;</code> parameter, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hadoop daemonlog -getlevel localhost.localdomain:8088  mapreduce.map.log.level </strong></span>
<span class="strong"><strong>Connecting to http://localhost.localdomain:8088/logLevel?log=mapreduce.map.log.level Submitted Log Name: mapreduce.map.log.level Log Class: org.apache.commons.logging.impl.Log4JLogger Effective level: INFO </strong></span>
</pre></div><p>The effective level can be modified using the <code class="literal">-setlevel &lt;property&gt; &lt;level&gt;</code> option:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hadoop daemonlog -setlevel localhost.localdomain:8088  mapreduce.map.log.level  DEBUG</strong></span>
<span class="strong"><strong>Connecting to http://localhost.localdomain:8088/logLevel?log=mapreduce.map.log.level&amp;level=DEBUG</strong></span>
<span class="strong"><strong>Submitted Log Name: mapreduce.map.log.level</strong></span>
<span class="strong"><strong>Log Class: org.apache.commons.logging.impl.Log4JLogger</strong></span>
<span class="strong"><strong>Submitted Level: DEBUG</strong></span>
<span class="strong"><strong>Setting Level to DEBUG ...</strong></span>
<span class="strong"><strong>Effective level: DEBUG</strong></span>
</pre></div><p>Note that this setting will affect all logs produced by the <code class="literal">ResourceManager</code> class. This includes system-generated entries as well as the ones generated by applications running on YARN.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec191"></a>Access to logfiles</h3></div></div></div><p>Logfile locations<a id="id1148" class="indexterm"></a> and naming conventions are likely to differ based on the distribution. Apache Ambari and Cloudera Manager centralize access to logfiles, both for services and single applications. On Cloudera's QuickStart VM, an overview of the currently running processes and links to their logfiles, the <code class="literal">stderr</code> and <code class="literal">stdout</code> channels can be found at <code class="literal">http://localhost.localdomain:7180/cmf/hardware/hosts/1/processes</code>, and the screenshot can be seen as follows:</p><div class="mediaobject"><img src="graphics/5518OS_10_03.jpg" /><div class="caption"><p>Access to log resources in Cloudera Manager</p></div></div><p>Ambari provides a similar overview via the <span class="strong"><strong>Services</strong></span> dashboard found at <code class="literal">http://127.0.0.1:8080/#/main/services</code> on the HDP Sandbox, and the screenshot can be seen as follows:</p><div class="mediaobject"><img src="graphics/5518OS_10_18.jpg" /><div class="caption"><p>Access to log resources on Apache Ambari</p></div></div><p>Non-distributed logs <a id="id1149" class="indexterm"></a>are usually found under <code class="literal">/var/log/&lt;service&gt;</code> on each cluster node. YARN containers and MRv2 logs locations also depend on the distribution. On CDH5 these resources are available in HDFS under <code class="literal">/tmp/logs/&lt;user&gt;</code>.</p><p>The standard modality to access distributed logs is either via command-line tools or using services web UIs.</p><p>For instance, the command is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ yarn application -list -appStates ALL </strong></span>
</pre></div><p>The preceding command will list all running and retried YARN applications. The URL in the task column points to a web interface that exposes the task log, as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>14/08/03 14:44:38 INFO client.RMProxy: Connecting to ResourceManager at localhost.localdomain/127.0.0.1:8032 Total number of applications (application-types: [] and states: [NEW, NEW_SAVING, SUBMITTED, ACCEPTED, RUNNING, FINISHED, FAILED, KILLED]):4                 Application-Id      Application-Name      Application-Type        User       Queue               State         Final-State         Progress                         Tracking-URL application_1405630696162_0002  PigLatin:DefaultJobName             MAPREDUCE    cloudera  root.cloudera            FINISHED           SUCCEEDED             100%  http://localhost.localdomain:19888/jobhistory/job/job_1405630696162_0002 application_1405630696162_0004  PigLatin:DefaultJobName             MAPREDUCE    cloudera  root.cloudera            FINISHED           SUCCEEDED             100%  http://localhost.localdomain:19888/jobhistory/job/job_1405630696162_0004 application_1405630696162_0003  PigLatin:DefaultJobName             MAPREDUCE    cloudera  root.cloudera            FINISHED           SUCCEEDED             100%  http://localhost.localdomain:19888/jobhistory/job/job_1405630696162_0003 application_1405630696162_0005  PigLatin:DefaultJobName             MAPREDUCE    cloudera  root.cloudera            FINISHED           SUCCEEDED             100%  http://localhost.localdomain:19888/jobhistory/job/job_1405630696162_0005 </strong></span>
</pre></div><p>For instance, <code class="literal">http://localhost.localdomain:19888/jobhistory/job/job_1405630696162_0002</code>, a link to a task belonging to user cloudera, is a frontend to the content stored under <code class="literal">hdfs:///tmp/logs/cloudera/logs/application_1405630696162_0002/</code>.</p><p>In the following sections, we <a id="id1150" class="indexterm"></a>will give an overview of the available UIs for different services.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note33"></a>Note</h3><p>Provisioning an EMR cluster with the <code class="literal">â€“log-uri s3://&lt;bucket&gt;</code> option will ensure that Hadoop logs are copied into the <code class="literal">s3://&lt;bucket&gt;</code> location.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec192"></a>ResourceManager, NodeManager, and Application Manager</h3></div></div></div><p>On YARN the<a id="id1151" class="indexterm"></a> ResourceManager web UI provides information and general<a id="id1152" class="indexterm"></a> job statistics <a id="id1153" class="indexterm"></a>of the Hadoop cluster, running/completed/failed jobs, and a job history logfile. By default, the UI is exposed at <code class="literal">http://&lt;resourcemanagerhost&gt;:8088/</code> and can be seen in the following screenshot:</p><div class="mediaobject"><img src="graphics/5518OS_10_04.jpg" /><div class="caption"><p>Resource Manager</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch10lvl3sec92"></a>Applications</h4></div></div></div><p>On the left-hand sidebar, it is <a id="id1154" class="indexterm"></a>possible to review the application status of interest: <code class="literal">NEW</code>, <code class="literal">SUBMITTED</code>, <code class="literal">ACCEPTED</code>, <code class="literal">RUNNING</code>, <code class="literal">FINISHING</code>, <code class="literal">FINISHED</code>, <code class="literal">FAILED</code>, or <code class="literal">KILLED</code>. Depending on the application status, the following information is available:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The application ID</p></li><li style="list-style-type: disc"><p>The submitting user</p></li><li style="list-style-type: disc"><p>The application name</p></li><li style="list-style-type: disc"><p>The scheduler queue in which the application is placed</p></li><li style="list-style-type: disc"><p>Start/finish times and state</p></li><li style="list-style-type: disc"><p>Link to the Tracking UI for application history</p></li></ul></div><p>In addition, the <code class="literal">Cluster Metrics</code> view gives you information on the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Overall application status</p></li><li style="list-style-type: disc"><p>Number of running containers</p></li><li style="list-style-type: disc"><p>Memory usage</p></li><li style="list-style-type: disc"><p>Node status</p></li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch10lvl3sec93"></a>Nodes</h4></div></div></div><p>The <code class="literal">Nodes</code> view<a id="id1155" class="indexterm"></a> is a frontend to the NodeManager service menu, which shows health and location information on the node's running applications, as follows:</p><div class="mediaobject"><img src="graphics/5518OS_10_05.jpg" /><div class="caption"><p>Nodes status</p></div></div><p>Each individual node of the cluster exposes further information and statistics at host level via its own UI. These include which version of Hadoop is running on the node, how much memory is available on the node, the node status, and a list of running applications and containers, as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/5518OS_10_06.jpg" /><div class="caption"><p>Single node info</p></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch10lvl3sec94"></a>Scheduler</h4></div></div></div><p>The following screenshot shows the<a id="id1156" class="indexterm"></a> Scheduler window:</p><div class="mediaobject"><img src="graphics/5518OS_10_07.jpg" /><div class="caption"><p>Scheduler</p></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch10lvl3sec95"></a>MapReduce</h4></div></div></div><p>Though the <a id="id1157" class="indexterm"></a>same information and logging details are available in MapReduce v1 and MapReduce v2, the access modality is slightly different.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch10lvl3sec96"></a>MapReduce v1</h4></div></div></div><p>The following screenshot<a id="id1158" class="indexterm"></a> shows the MapReduce JobTracker UI:</p><div class="mediaobject"><img src="graphics/5518OS_10_08.jpg" /><div class="caption"><p>The Job Tracker UI</p></div></div><p>The Job Tracker UI, available by default at <code class="literal">http://&lt;jobtracker&gt;:50070</code>, exposes information on all currently running as well as retired MapReduce jobs, a summary of the cluster resources and health, as well as scheduling information and completion percentage, as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/5518OS_10_09.jpg" /><div class="caption"><p>Job details</p></div></div><p>For each running and<a id="id1159" class="indexterm"></a> retired job, details are available, including its ID, owner, priority, task assignment, and task launch for the mapper. Clicking on a <code class="literal">jobid</code> link will lead to a job details pageâ€”the same URL exposed by the <code class="literal">mapred job â€“list</code> command. This resource gives details about both the map and reduce tasks as well as general counter statistics at the job, filesystem, and MapReduce levels; these include the memory used, number of read/write operations, and the number of bytes read and written. </p><p>For each Map and Reduce operation, the JobTracker exposes the total, pending, running, completed, and failed tasks, as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/5518OS_10_10.jpg" /><div class="caption"><p>Job tasks overview</p></div></div><p>Clicking on the links in the <a id="id1160" class="indexterm"></a>Job table will lead to a further overview at the task and task-attempt levels, as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/5518OS_10_11.jpg" /><div class="caption"><p>Task attempts</p></div></div><p>From this last page, we <a id="id1161" class="indexterm"></a>can access the logs of each task attempt, both for successful and failed/killed tasks on each individual TaskTracker host. This log contains the most granular information about the status of the MapReduce job, including the output of Log4j appenders as well as output piped to the <code class="literal">stdout</code> and <code class="literal">stderr</code> channels and <code class="literal">syslog</code>, as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/5518OS_10_12.jpg" /><div class="caption"><p>TaskTracker logs</p></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch10lvl3sec97"></a>MapReduce v2 (YARN)</h4></div></div></div><p>As we have <a id="id1162" class="indexterm"></a>seen in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Processing â€“ MapReduce and Beyond</em></span>, with YARN, MapReduce is only one of many processing frameworks that can be deployed. Recall from previous chapters that the JobTracker and TaskTracker services have been replaced by the ResourceManager and NodeManager, respectively. As such, both the service UIs and the logfiles from YARN are more generic than MapReduce v1. </p><p>The <code class="literal">application_1405630696162_0002</code> name shown in Resource Manager corresponds to a MapReduce job with the <code class="literal">job_1405630696162_0002</code> ID. That application ID belongs to the task running inside the container, and clicking on it will reveal an overview of the MapReduce job and allow a drill-down to the individual tasks from either phase until the single-task log is reached, as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/5518OS_10_13.jpg" /><div class="caption"><p>A YARN application containing a MapReduce job</p></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch10lvl3sec98"></a>JobHistory Server</h4></div></div></div><p>YARN ships with a JobHistory <a id="id1163" class="indexterm"></a>REST service that exposes details on finished applications. Currently, it only supports MapReduce and provides information on finished jobs. This includes the job final status <code class="literal">SUCCESSFUL</code> or <code class="literal">FAILED</code>, who submitted the job, the total number of map and reduce tasks, and timing information. </p><p>A UI is available at <code class="literal">http://&lt;jobhistoryhost&gt;:19888/jobhistory</code>, as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/5518OS_10_14.jpg" /><div class="caption"><p>JobHistory UI</p></div></div><p>Clicking on each job ID will lead to the MapReduce job UI shown in the YARN application screenshot.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec193"></a>NameNode and DataNode</h3></div></div></div><p>The web interface for the <span class="strong"><strong>Hadoop Distributed File System</strong></span> (<span class="strong"><strong>HDFS</strong></span>)<a id="id1164" class="indexterm"></a> shows<a id="id1165" class="indexterm"></a> information about the NameNode itself as well as the<a id="id1166" class="indexterm"></a> filesystem generally. </p><p>By default, it is located at <code class="literal">http://&lt;namenodehost&gt;:50070/</code>, as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/5518OS_10_15.jpg" /><div class="caption"><p>NameNode UI</p></div></div><p>The <span class="strong"><strong>Overview</strong></span> menu exposes<a id="id1167" class="indexterm"></a> NameNode information about DFS capacity and usage and the block pool status, and it gives a summary of the status of DataNode health and availability. The information contained in this page is for the most part equivalent to what is shown at the command-line prompt:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hdfs dfsadmin â€“report</strong></span>
</pre></div><p>The <a id="id1168" class="indexterm"></a>DataNodes menu gives more detailed information about the status of each node and offers a drill-down at the single-host level, both for available and decommissioned nodes, as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/5518OS_10_16.jpg" /><div class="caption"><p>Datanode UI</p></div></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec82"></a>Summary</h2></div></div><hr /></div><p>This has been quite a whistle-stop tour around the considerations of running an operational Hadoop cluster. We didn't try to turn developers into administrators, but hopefully, the broader perspective will help you to help your operations staff. In particular, we covered the following topics:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>How Hadoop is a natural fit for DevOps approaches as its multilayered complexity means it's not possible or desirable to have substantial knowledge gaps between development and operations staff</p></li><li style="list-style-type: disc"><p>Cloudera Manager, and how it can be a great management and monitoring tool; it might cause integration problems though, if you have other enterprise tools, and it comes with a vendor lock-in risk</p></li><li style="list-style-type: disc"><p>Ambari, the Apache open source alternative to Cloudera Manager, and how it is used in the Hortonworks distribution</p></li><li style="list-style-type: disc"><p>How to think about selecting hardware for a physical Hadoop cluster, and how this naturally fits into the considerations of how the multiple workloads possible in the world of Hadoop 2 can peacefully coexist on shared resources</p></li><li style="list-style-type: disc"><p>The different considerations for firing up and using EMR clusters and how this can be both an adjunct to, as well as an alternative to, a physical cluster</p></li><li style="list-style-type: disc"><p>The Hadoop security ecosystem, how it is a very fast moving area, and how the features available today are vastly better than some years ago and there is still much around the corner</p></li><li style="list-style-type: disc"><p>Monitoring of a Hadoop cluster, considering what events are important in the Hadoop model of embracing failure, and how these alerts and metrics can be integrated into other enterprise-monitoring frameworks</p></li><li style="list-style-type: disc"><p>How to troubleshoot issues with a Hadoop cluster, both in terms of what might have happened and how to find the information to inform your analysis</p></li><li style="list-style-type: disc"><p>A quick tour of the various web UIs provided by Hadoop, which can give very good overviews of happenings within various components in the system</p></li></ul></div><p>This concludes our treatment of Hadoop in depth. In the final chapter, we will express some thoughts on the broader Hadoop ecosystem, give some pointers for useful and interesting tools and products that we didn't have a chance to cover in the book, and suggest how to get involved with the community.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch11"></a>ChapterÂ 11.Â Where to Go Next</h2></div></div></div><p>In the previous chapters we have examined many parts of Hadoop 2 and the ecosystem around it. However, we have necessarily been limited by page count; some areas we didn't get into as much depth as was possible, other areas we referred to only in passing or did not mention at all.</p><p>The Hadoop ecosystem, with distributions, Apache and non-Apache projects, is an incredibly vibrant and healthy place to be right now. In this chapter, we hope to complement the previously discussed more detailed material with a travel guide, if you will, for other interesting destinations. In this chapter, we will discuss the following topics:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Hadoop distributions</p></li><li style="list-style-type: disc"><p>Other significant Apache and non-Apache projects</p></li><li style="list-style-type: disc"><p>Sources of information and help</p></li></ul></div><p>Of course, note that any overview of the ecosystem is both skewed by our interests and preferences, and is outdated the moment it is written. In other words, don't for a moment think this is all that's available, consider it instead a whetting of the appetite.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch11lvl1sec83"></a>Alternative distributions</h2></div></div><hr /></div><p>We've <a id="id1169" class="indexterm"></a>generally used the Cloudera distribution for Hadoop in this book, but<a id="id1170" class="indexterm"></a> have attempted to keep the coverage distribution independent as much as possible. We've also mentioned the <span class="strong"><strong>Hortonworks Data Platform</strong></span> (<span class="strong"><strong>HDP</strong></span>)<a id="id1171" class="indexterm"></a> throughout this book but these are certainly not the only distribution choices available to you.</p><p>Before taking a look around, let's consider whether you need a distribution at all. It is completely possible to go to the Apache website, download the source tarballs of the projects in which you are interested, then work to build them all together. However, given version dependencies, this is likely to consume more time than you would expect. Potentially, vastly more so. In addition, the end product will likely lack some polish in terms of tools or scripts for operational deployment and management. For most users, these areas are why employing an existing Hadoop distribution is the natural choice.</p><p>A note on free and commercial extensionsâ€”being an open source project with a quite liberal license, distribution creators are also free to enhance Hadoop with proprietary extensions that are made available either as free open source or commercial products.</p><p>This can be a <a id="id1172" class="indexterm"></a>controversial issue as some open source advocates dislike any commercialization of successful open source projects; to them, it appears that the commercial entity is freeloading by taking the fruits of the open source community without having to build it for themselves. Others see this as a healthy aspect of the flexible Apache license; the base product will always be free, and individuals and companies can choose whether to go with commercial extensions or not. We don't give judgment either way, but be aware that this is another of the controversies you will almost certainly encounter.</p><p>So you need to decide if you need a distribution and if so for what reasons, which specific aspects will benefit you most above rolling your own? Do you wish for a fully open source product or are you willing to pay for commercial extensions? With these questions in mind, let's look at a few of the main distributions.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec194"></a>Cloudera Distribution for Hadoop</h3></div></div></div><p>You will be<a id="id1173" class="indexterm"></a> familiar with <a id="id1174" class="indexterm"></a>the Cloudera distribution (<a class="ulink" href="http://www.cloudera.com" target="_blank">http://www.cloudera.com</a>) as it has been used throughout this book. CDH <a id="id1175" class="indexterm"></a>was the first widely available alternative distribution and its breadth of available software, proven level of quality, and its free cost has made it a very popular choice.</p><p>Recently, Cloudera has been actively extending the products it adds to its distribution beyond the core Hadoop projects. In addition to Cloudera Manager and Impala (both Cloudera-developed products), it has also added other tools such as Cloudera Search (based on Apache Solr) and Cloudera Navigator (a data governance solution). While CDH versions prior to 5 were focused more on the integration benefits of a distribution, version 5 (and presumably beyond) is adding more and more capability atop the base Apache Hadoop projects.</p><p>Cloudera also offers commercial support for its products in addition to training and consultancy services. Details can be found on the company web page.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec195"></a>Hortonworks Data Platform</h3></div></div></div><p>In 2011, the<a id="id1176" class="indexterm"></a> Yahoo! division <a id="id1177" class="indexterm"></a>responsible for so much of the development of Hadoop was spun off into a new company called Hortonworks. They have also produced their own pre-integrated Hadoop distribution called the <a id="id1178" class="indexterm"></a>
<span class="strong"><strong>Hortonworks Data Platform</strong></span> (<span class="strong"><strong>HDP</strong></span>), available at <a class="ulink" href="http://hortonworks.com/products/hortonworksdataplatform/" target="_blank">http://hortonworks.com/products/hortonworksdataplatform/</a>.</p><p>HDP is conceptually similar to CDH but both products have differences in their focus. Hortonworks makes much of the fact HDP is fully open source, including the management tool Ambari, which we discussed briefly in <a class="link" href="#" linkend="ch10">Chapter 10</a>, <span class="emphasis"><em>Running a Hadoop Cluster</em></span>. They have also positioned HDP as a key integration platform through its support for tools such as Talend Open Studio. Hortonworks does not offer proprietary software; its business model focuses instead on offering professional services and support for the platform.</p><p>Both Cloudera and Hortonworks are venture-backed companies with significant engineering expertise; both companies employ many of the most prolific contributors to Hadoop. The underlying technology is, however, comprised of the same Apache projects; the distinguishing factors are how they are packaged, the versions employed, and the additional value-added offerings provided by the companies.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec196"></a>MapR</h3></div></div></div><p>A different<a id="id1179" class="indexterm"></a> type of distribution is offered by MapR Technologies, although the company and distribution are usually referred to simply as MapR. The<a id="id1180" class="indexterm"></a> distribution available<a id="id1181" class="indexterm"></a> from <a class="ulink" href="http://www.mapr.com" target="_blank">http://www.mapr.com</a> is based on Hadoop, but has added a number of changes and enhancements.</p><p>The focus of the MapR distribution is on performance and availability. For example, it was the first distribution to offer a high-availability solution for the Hadoop NameNode and JobTracker, which you will remember from <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Storage</em></span>, was a significant weakness in core Hadoop 1. It also offered native integration with NFS filesystems long before Hadoop 2, which makes processing of existing data much easier. To achieve these features, MapR replaced HDFS with a full POSIX compliant filesystem that also features no NameNode, resulting in a true distributed system with no master, and a claim of much better hardware utilization than Apache HDFS.</p><p>MapR provides both a community and enterprise edition of its distribution; not all the extensions are available in the free product. The company also offers support services as part of the enterprise product subscription in addition to training and consultancy.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec197"></a>And the restâ€¦</h3></div></div></div><p>Hadoop distributions are not just the territory of young start-ups, nor are they a static marketplace. Intel had its own distribution until early 2014 when it decided to fold its changes into CDH instead. IBM has its own distribution called IBM Infosphere Big Insights<a id="id1182" class="indexterm"></a> available in both free and commercial editions. There are also various stories of numerous large enterprises rolling their own distributions, some of which are made openly available while others are not. You will have no shortage of options with so many high-quality distributions available.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec198"></a>Choosing a distribution</h3></div></div></div><p>This<a id="id1183" class="indexterm"></a> raises the question: how to choose a distribution? As can be seen, the available distributions (and we didn't cover them all) range from convenient packaging and integration of fully open source products through to entire bespoke integration and analysis layers atop them. There is no overall best distribution; think carefully about your requirements and consider the alternatives. Since all these offer a free download of at least a basic version, it's good to simply play and experience the options for yourself.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch11lvl1sec84"></a>Other computational frameworks</h2></div></div><hr /></div><p>We've <a id="id1184" class="indexterm"></a>frequently discussed the myriad possibilities <a id="id1185" class="indexterm"></a>brought to the Hadoop platform by YARN. We went into details of two new models, Samza and Spark. Additionally, other more established frameworks such as Pig are also being ported to the framework.</p><p>To give a view of the much bigger picture in this section, we will illustrate the breadth of processing possible using YARN by presenting a set of computational models that are currently being ported to Hadoop on top of YARN.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec199"></a>Apache Storm</h3></div></div></div><p>Storm (<a class="ulink" href="http://storm.apache.org" target="_blank">http://storm.apache.org</a>) is a<a id="id1186" class="indexterm"></a> distributed <a id="id1187" class="indexterm"></a>computation framework written (mainly) in the<a id="id1188" class="indexterm"></a> Clojure programming language. It uses custom-created spouts and bolts to define information sources and manipulations to allow distributed processing of streaming data. A Storm application is designed as a topology of interfaces that creates a stream of transformations. It provides similar functionality to a MapReduce job with the exception that the topology will theoretically run indefinitely until it is manually terminated.</p><p>Though <a id="id1189" class="indexterm"></a>initially built distinct from Hadoop, a YARN port is being developed by Yahoo! and can be found at <a class="ulink" href="https://github.com/yahoo/storm-yarn" target="_blank">https://github.com/yahoo/storm-yarn</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec200"></a>Apache Giraph</h3></div></div></div><p>Giraph<a id="id1190" class="indexterm"></a> originated as the open source implementation of<a id="id1191" class="indexterm"></a> Google's Pregel paper (which can be found at <a class="ulink" href="http://kowshik.github.io/JPregel/pregel_paper.pdf" target="_blank">http://kowshik.github.io/JPregel/pregel_paper.pdf</a>). Both Giraph and Pregel are inspired by<a id="id1192" class="indexterm"></a> the <span class="strong"><strong>Bulk Synchronous Parallel</strong></span> (<span class="strong"><strong>BSP</strong></span>) model<a id="id1193" class="indexterm"></a> of distributed computation introduced by Valiant in 1990. Giraph adds several features including master computation, sharded aggregators, edge-oriented input, and out-of-core computation. The YARN port can be found at <a class="ulink" href="https://issues.apache.org/jira/browse/GIRAPH-13" target="_blank">https://issues.apache.org/jira/browse/GIRAPH-13</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec201"></a>Apache HAMA</h3></div></div></div><p>Hama<a id="id1194" class="indexterm"></a> is a top-level Apache project that aims, like <a id="id1195" class="indexterm"></a>other methods we've encountered so far, to address the weakness of MapReduce with regard to iterative programming. Similar to the aforementioned Giraph, Hama implements the BSP techniques and has been heavily inspired by the Pregel paper. The YARN port can be found at <a class="ulink" href="https://issues.apache.org/jira/browse/HAMA-431" target="_blank">https://issues.apache.org/jira/browse/HAMA-431</a>.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch11lvl1sec85"></a>Other interesting projects</h2></div></div><hr /></div><p>Whether you use a <a id="id1196" class="indexterm"></a>bundled distribution or stick with the base Apache Hadoop download, you will encounter many references to other related projects. We've covered several of these such as Hive, Samza, and Crunch in this book; we'll now highlight some of the others.</p><p>Note that this coverage seeks to point out the highlights (from the authors' perspective) as well as give a taste of the breadth of types of projects available. As mentioned earlier, keep looking out, as there will be new ones launching all the time.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec202"></a>HBase</h3></div></div></div><p>Perhaps the<a id="id1197" class="indexterm"></a> most popular Apache Hadoop-related project that we didn't cover in this <a id="id1198" class="indexterm"></a>book is HBase (<a class="ulink" href="http://hbase.apache.org" target="_blank">http://hbase.apache.org</a>). Based on the BigTable model of data storage publicized by Google in an academic paper (sound familiar?), HBase is a nonrelational data store sitting atop HDFS.</p><p>While both MapReduce and Hive focus on batch-like data access patterns, HBase instead seeks to provide very low-latency access to data. Consequently HBase can, unlike the aforementioned technologies, directly support user-facing services.</p><p>The HBase data model is not the relational approach that was used in Hive and all other RDBMSs, nor does it offer the full ACID guarantees that are taken for granted with relational stores. Instead, it is a key-value schema-less solution that takes a column-oriented view of data; columns can be added at runtime and depend on the values inserted into HBase. Each lookup operation is then very fast, as it is effectively a key-value mapping from the row key to the desired column. HBase also treats timestamps as another dimension on the data so one can directly retrieve data from a point in time.</p><p>The data model is very powerful but does not suit all use cases just as the relational model isn't universally applicable. But if you have a requirement for structured low-latency views on large-scale data stored in Hadoop, then HBase is absolutely something you should look at.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec203"></a>Sqoop</h3></div></div></div><p>In <a class="link" href="#" linkend="ch07">Chapter 7</a>, <span class="emphasis"><em>Hadoop and SQL</em></span>, we looked at <a id="id1199" class="indexterm"></a>tools for presenting a relational-like interface to data stored on HDFS. Often, such data either needs to be retrieved from an existing relational database or the output of its processing needs to be stored back.</p><p>Apache Sqoop (<a class="ulink" href="http://sqoop.apache.org" target="_blank">http://sqoop.apache.org</a>) provides <a id="id1200" class="indexterm"></a>a mechanism for declaratively specifying data movement between relational databases and Hadoop. It takes a task definition and from this generates MapReduce jobs to execute the required data retrieval or storage. It will also generate code to help manipulate relational records with custom Java classes. In addition, it can integrate with HBase and Hcatalog/Hive and it provides a very rich set of integration possibilities.</p><p>At the time of writing, Sqoop is slightly in flux. Its <a id="id1201" class="indexterm"></a>original version, Sqoop 1, was a pure client-side application. Much like the original Hive command-line tool, Sqoop 1 has no server and generates all code on the client. This unfortunately means that each client needs to know a lot of details about physical data sources, including exact host names as well as authentication credentials.</p><p>Sqoop 2<a id="id1202" class="indexterm"></a> provides a centralized Sqoop server that encapsulates all these details and offers the various configured data sources to the connecting clients. It is a superior model but at the time of writing, the general community recommendation is to stick with Sqoop 1 until the new version evolves further. Check on the current status if you are interested in this type of tool.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec204"></a>Whir</h3></div></div></div><p>When<a id="id1203" class="indexterm"></a> looking to use cloud services such as Amazon AWS for Hadoop deployments, it is usually a lot easier to use a higher level service such as Elastic MapReduce as opposed to setting up your own cluster on EC2. Though there are scripts to help, the fact is that the overhead of Hadoop-based deployments on cloud infrastructures can be involved. That's where<a id="id1204" class="indexterm"></a> Apache Whir (<a class="ulink" href="https://whirr.apache.org/" target="_blank">https://whirr.apache.org/</a>) comes in.</p><p>Whir isn't focused on Hadoop; it's about supplier-independent instantiation of cloud services of which Hadoop is a single example. Whir aims to provide a programmatic way of specifying and creating Hadoop-based deployments on cloud infrastructures in a way that handles all the underlying service aspects for you. It does this in a provider-independent fashion so that once you've launched on say EC2 then you can use the same code to create the identical setup on another provider such as Rightscale or Eucalyptus. This makes vendor lock-in, often a concern with cloud deployments, less of an issue.</p><p>Whir isn't quite there yet. Today, it is limited in services it can create and providers it supports, however, if you are interested in cloud deployment with less pain then it's worth watching its progress.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note34"></a>Note</h3><p>If you are building out your full infrastructure on Amazon Web Services then you might find cloud formation gives much of the same ability to define application requirements, though obviously in an AWS-specific fashion.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec205"></a>Mahout</h3></div></div></div><p>Apache Mahout (<a class="ulink" href="http://mahout.apache.org/" target="_blank">http://mahout.apache.org/</a>) is a <a id="id1205" class="indexterm"></a>collection of distributed algorithms, Java <a id="id1206" class="indexterm"></a>classes, and tools for performing advanced analytics on top of Hadoop. Similar to Spark's MLLib briefly mentioned in <a class="link" href="#" linkend="ch05">Chapter 5</a>, <span class="emphasis"><em>Iterative Computation with Spark</em></span>, Mahout ships with a number of algorithms for common use cases: recommendation, clustering, regression, and feature engineering. Although the system is focused on natural language processing and text-mining tasks, its building blocks (linear algebra operations) are suitable to be applied to a number of domains. As of Version 0.9, the <a id="id1207" class="indexterm"></a>project is being decoupled from the MapReduce framework in favor of richer programming models such as Spark. The community end goal is to obtain a platform-independent library based on a Scala DSL.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec206"></a>Hue</h3></div></div></div><p>Initially developed by <a id="id1208" class="indexterm"></a>Cloudera and marketed as the "User Interface for Hadoop", Hue (<a class="ulink" href="http://gethue.com/" target="_blank">http://gethue.com/</a>) is <a id="id1209" class="indexterm"></a>a collection of applications, bundled together under a common web interface, that act as clients for core services and a number of components of the Hadoop ecosystem:</p><div class="mediaobject"><img src="graphics/5518_11_01.jpg" /><div class="caption"><p>The Hue Query Editor for Hive</p></div></div><p>Hue leverages many <a id="id1210" class="indexterm"></a>of the tools we discussed in previous chapters and provides an integrated interface for analyzing and visualizing data. There are two components that are remarkably interesting. On one hand, there is a query editor that allows the user to create and save Hive (or Impala) queries, export the result set in CSV or Microsoft Office Excel format as well as plot it in the browser. The editor features the capability of sharing both HiveQL and result sets, thus facilitating collaboration within an organization. On the other hand, there is an Oozie workflow and coordinator editor that allows a user to create and deploy Oozie jobs manually, automating the generation of XML configurations and boilerplate.</p><p>Both Cloudera and Hortonworks distributions ship with Hue and typically include the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>A file manager for HDFS</p></li><li style="list-style-type: disc"><p>A Job Browser for YARN (MapReduce)</p></li><li style="list-style-type: disc"><p>An Apache HBase browser</p></li><li style="list-style-type: disc"><p>A Hive metastore explorer</p></li><li style="list-style-type: disc"><p>Query editors for Hive and Impala</p></li><li style="list-style-type: disc"><p>A script editor for Pig</p></li><li style="list-style-type: disc"><p>A job editor for MapReduce and Spark</p></li><li style="list-style-type: disc"><p>An editor for Sqoop 2 jobs</p></li><li style="list-style-type: disc"><p>An Oozie workflow editor and dashboard</p></li><li style="list-style-type: disc"><p>An Apache ZooKeeper browser</p></li></ul></div><p>On top of this, Hue is a <a id="id1211" class="indexterm"></a>framework with an SDK that contains a number of web assets, APIs, and patterns for developing third-party applications that interact with Hadoop.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch11lvl1sec86"></a>Other programming abstractions</h2></div></div><hr /></div><p>Hadoop<a id="id1212" class="indexterm"></a> isn't just extended by additional functionality, there are tools to provide entirely different paradigms for writing the code used to process your data within Hadoop.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec207"></a>Cascading</h3></div></div></div><p>Developed by Concurrent, and open sourced under <a id="id1213" class="indexterm"></a>an Apache license, Cascading (<a class="ulink" href="http://www.cascading.org/" target="_blank">http://www.cascading.org/</a>) is a popular framework that abstracts <a id="id1214" class="indexterm"></a>the complexity of MapReduce away and allows us to create complex workflows on top of Hadoop. Cascading jobs can compile to, and be executed on, MapReduce, Tez, and Spark. Conceptually, the framework is similar to Apache Crunch, covered in <a class="link" href="#" linkend="ch09">Chapter 9</a>, <span class="emphasis"><em>Making Development Easier</em></span>, though practically there are differences in terms of data abstractions and end goals. Cascading adopts a tuple data model (similar to Pig) rather than arbitrary objects, and encourages the user to rely on a higher level DSL, powerful built-in types, and tools to manipulate data.</p><p>Put in simple terms, Cascading<a id="id1215" class="indexterm"></a> is to PigLatin and HiveQL what Crunch is to a user-defined function.</p><p>Like Morphlines, which we also saw in <a class="link" href="#" linkend="ch09">Chapter 9</a>, <span class="emphasis"><em>Making Development Easier</em></span>, the Cascading data model follows a source-pipe-sink approach, where data is captured from a source, piped through a number of processing steps, and its output is then delivered into a sink, ready to be picked up by another application.</p><p>Cascading encourages developers to write code in a number of JVM languages. Ports of the framework exist for Python (PyCascading), JRuby (Cascading.jruby), Clojure (Cascalog), and Scala (Scalding). Cascalog and Scalding in particular have gained a lot of traction and spawned off their very own ecosystems.</p><p>An area where Cascading<a id="id1216" class="indexterm"></a> excels is documentation. The project provides comprehensive javadocs of the API, extensive tutorials (<a class="ulink" href="http://www.cascading.org/documentation/tutorials/" target="_blank">http://www.cascading.org/documentation/tutorials/</a>) and an interactive exercise-based learning environment (<a class="ulink" href="https://github.com/Cascading/Impatient" target="_blank">https://github.com/Cascading/Impatient</a>).</p><p>Another strong selling point of Cascading is its integration with third-party environments. Amazon EMR supports Cascading as a first-class processing framework and allows us to launch Cascading clusters both with the command line and web interfaces (<a class="ulink" href="http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/CreateCascading.html" target="_blank">http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/CreateCascading.html</a>). Plugins for the SDK exist for both the IntelliJ IDEA and Eclipse integrated development environments. One of the framework's top projects, Cascading Patterns, a collection of machine-learning algorithms, features a utility for translating <span class="strong"><strong>Predictive Model Markup Language</strong></span> (<span class="strong"><strong>PMML</strong></span>)<a id="id1217" class="indexterm"></a> documents into applications on Apache Hadoop, thus facilitating interoperability with popular statistical environments and scientific tools such as R (<a class="ulink" href="http://cran.r-project.org/web/packages/pmml/index.html" target="_blank">http://cran.r-project.org/web/packages/pmml/index.html</a>).</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch11lvl1sec87"></a>AWS resources</h2></div></div><hr /></div><p>Many<a id="id1218" class="indexterm"></a> Hadoop technologies<a id="id1219" class="indexterm"></a> can be deployed on AWS as part of a self-managed cluster. However, just as Amazon offers support for Elastic MapReduce, which handles Hadoop as a managed service, there are a few other services that are worth mentioning.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec208"></a>SimpleDB and DynamoDB</h3></div></div></div><p>For some time, AWS has offered SimpleDB<a id="id1220" class="indexterm"></a> as a hosted service <a id="id1221" class="indexterm"></a>providing<a id="id1222" class="indexterm"></a> an HBase-like data model.</p><p>It has, however, largely been superseded by a more recent service from AWS, DynamoDB<a id="id1223" class="indexterm"></a>, located at <a class="ulink" href="http://aws.amazon.com/dynamodb" target="_blank">http://aws.amazon.com/dynamodb</a>. Though its data model is very similar to that of SimpleDB and HBase, it is aimed at a very different type of application. Where SimpleDB has quite a rich search API but is very limited in terms of size, DynamoDB provides a more constrained though constantly evolving API, but with a service guarantee of near-unlimited scalability.</p><p>The DynamoDB <a id="id1224" class="indexterm"></a>pricing model is particularly interesting; instead of paying for a certain number of servers hosting the service, you allocate a certain capacity for read-and-write operations, and DynamoDB manages the resources required to meet this provisioned capacity. This is an interesting development as it is a more pure service model, where the mechanism of delivering the desired performance is kept completely opaque to the service user. Have a look at DynamoDB but if you need a much larger scale of data store than SimpleDB can offer; however, do consider the pricing model carefully as provisioning too much capacity can become very expensive very quickly. Amazon provides some good best practices for DynamoDB at the following URL that illustrate that minimizing the service costs can result in additional application-layer complexity: <a class="ulink" href="http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/BestPractices.html" target="_blank">http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/BestPractices.html</a>.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note35"></a>Note</h3><p>Of course the discussion of DynamoDB and SimpleDB assumes a non-relational data model; there is the <a id="id1225" class="indexterm"></a>
<span class="strong"><strong>Amazon Relational Database Service</strong></span> (<span class="strong"><strong>Amazon RDS</strong></span>) for a relational database in the cloud service.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec209"></a>Kinesis</h3></div></div></div><p>Just as EMR is hosted Hadoop and DynamoDB has similarities to a hosted HBase, it wasnâ€™t surprising to see AWS announce Kinesis, a hosted streaming data service in 2013. This can be found at <a class="ulink" href="http://aws.amazon.com/kinesis" target="_blank">http://aws.amazon.com/kinesis</a> and it has very similar conceptual building blocks to the stack of Samza atop Kafka. Kinesis provides a partitioned view of messages as a stream of data and an API to have callbacks that execute when messages arrive. As with most AWS services, there is tight integration with other services making it easy to get data into and out of locations such as S3.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec210"></a>Data Pipeline</h3></div></div></div><p>The final <a id="id1226" class="indexterm"></a>AWS service <a id="id1227" class="indexterm"></a>that we'll mention is Data Pipeline, which <a id="id1228" class="indexterm"></a>can <a id="id1229" class="indexterm"></a>be found at <a class="ulink" href="http://aws.amazon.com/datapipeline" target="_blank">http://aws.amazon.com/datapipeline</a>. As the name suggests, it is a framework for building up data-processing jobs that involve multiple steps, data movements, and transformations. It has quite a conceptual overlap with Oozie, but with a few twists. Firstly, Data Pipeline has the expected deep integration with many other AWS services, enabling easy definition of data workflows that incorporate diverse repositories such as RDS, S3, and DynamoDB. In addition however, Data Pipeline does have the ability to integrate agents installed on local infrastructure, providing an interesting avenue for building workflows that span across the AWS and on-premises environments.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch11lvl1sec88"></a>Sources of information</h2></div></div><hr /></div><p>You don't just <a id="id1230" class="indexterm"></a>need new technologies and toolsâ€”even if they are cool. Sometimes, a little help from a more experienced source can pull you out of a hole. In this regard, you are well covered, as the Hadoop<a id="id1231" class="indexterm"></a> community is extremely strong in many areas.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec211"></a>Source code</h3></div></div></div><p>It's sometimes easy to<a id="id1232" class="indexterm"></a> overlook, but Hadoop and all the other Apache projects are after all fully open source. The actual source code is the ultimate source (pardon the pun) of information about how the system works. Becoming familiar with the source and tracing through some of the functionality can be hugely informative. Not to mention helpful when you are hitting unexpected behavior.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec212"></a>Mailing lists and forums</h3></div></div></div><p>Almost all the <a id="id1233" class="indexterm"></a>projects and services listed in this chapter have <a id="id1234" class="indexterm"></a>their own mailing lists and/or forums; check out the home pages for the specific links. Most distributions also have their own forums and other mechanisms to share knowledge and get (non-commercial) help from the community. Additionally, if using AWS, make sure to check out the AWS developer forums<a id="id1235" class="indexterm"></a> at <a class="ulink" href="https://forums.aws.amazon.com" target="_blank">https://forums.aws.amazon.com</a>.</p><p>Always <a id="id1236" class="indexterm"></a>remember<a id="id1237" class="indexterm"></a> to read posting guidelines carefully and understand the expected etiquette. These are tremendous sources of information; the lists and forums are often frequently visited by the developers of the particular project. Expect to see the core Hadoop developers on the Hadoop lists, Hive developers on the Hive list, EMR developers on the EMR forums, and so on.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec213"></a>LinkedIn groups</h3></div></div></div><p>There are <a id="id1238" class="indexterm"></a>a <a id="id1239" class="indexterm"></a>number of Hadoop and related groups on the professional social network LinkedIn. Do a search for your particular <a id="id1240" class="indexterm"></a>areas of interest, but a good starting point might be the general Hadoop users' group at <a class="ulink" href="http://www.linkedin.com/groups/Hadoop-Users-988957" target="_blank">http://www.linkedin.com/groups/Hadoop-Users-988957</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec214"></a>HUGs</h3></div></div></div><p>If you <a id="id1241" class="indexterm"></a>want more face-to-face interaction then look for a <span class="strong"><strong>Hadoop User Group</strong></span> (<span class="strong"><strong>HUG</strong></span>)<a id="id1242" class="indexterm"></a> in your area, most<a id="id1243" class="indexterm"></a> of <a id="id1244" class="indexterm"></a>which will be listed at <a class="ulink" href="http://wiki.apache.org/hadoop/HadoopUserGroups" target="_blank">http://wiki.apache.org/hadoop/HadoopUserGroups</a>. These tend to arrange semi-regular get-togethers that combine things such as quality presentations, the ability to discuss technology with like-minded individuals, and often pizza and drinks.</p><p>No HUG near where you live? Consider starting one.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl2sec215"></a>Conferences</h3></div></div></div><p>Though <a id="id1245" class="indexterm"></a>some industries take decades to build up a<a id="id1246" class="indexterm"></a> conference circuit, Hadoop already has some significant conference action involving the open source, academic, and commercial worlds. Events such as the Hadoop Summit and Strata are pretty big; these and some other are linked <a id="id1247" class="indexterm"></a>from <a class="ulink" href="http://wiki.apache.org/hadoop/Conferences" target="_blank">http://wiki.apache.org/hadoop/Conferences</a>.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch11lvl1sec89"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we took a quick gallop around the broader Hadoop ecosystem, looking at the following topics:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Why alternative Hadoop distributions exist and some of the more popular ones</p></li><li style="list-style-type: disc"><p>Other projects that provide capabilities, extensions, or Hadoop supporting tools</p></li><li style="list-style-type: disc"><p>Alternative ways of writing or creating Hadoop jobs</p></li><li style="list-style-type: disc"><p>Sources of information and how to connect with other enthusiasts</p></li></ul></div><p>Now, go have fun and build something amazing!</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">

    <div id="backindex">
      <h1 class="title">Index</h1>
      <h2>A</h2>
      <ul>
        <li>additional data, collecting<ul><li>about / <a href="#ch08lvl1sec63" title="Collecting additional data" class="link">Collecting additional data</a></li><li>workflows, scheduling / <a href="#ch08lvl1sec63" title="Scheduling workflows" class="link">Scheduling workflows</a></li><li>Oozie triggers / <a href="#ch08lvl1sec63" title="Other Oozie triggers" class="link">Other Oozie triggers</a></li></ul></li>
        <li>addMapper method, arguments<ul><li>job / <a href="#ch03lvl1sec31" title="Text cleanup using chain mapper" class="link">Text cleanup using chain mapper</a></li><li>class / <a href="#ch03lvl1sec31" title="Text cleanup using chain mapper" class="link">Text cleanup using chain mapper</a></li><li>inputKeyClass / <a href="#ch03lvl1sec31" title="Text cleanup using chain mapper" class="link">Text cleanup using chain mapper</a></li><li>inputValueClass / <a href="#ch03lvl1sec31" title="Text cleanup using chain mapper" class="link">Text cleanup using chain mapper</a></li><li>outputKeyClass / <a href="#ch03lvl1sec31" title="Text cleanup using chain mapper" class="link">Text cleanup using chain mapper</a></li><li>outputValueClass / <a href="#ch03lvl1sec31" title="Text cleanup using chain mapper" class="link">Text cleanup using chain mapper</a></li><li>mapperConf / <a href="#ch03lvl1sec31" title="Text cleanup using chain mapper" class="link">Text cleanup using chain mapper</a></li></ul></li>
        <li>alternative distributions<ul><li>about / <a href="#ch11lvl1sec83" title="Alternative distributions" class="link">Alternative distributions</a></li><li>Cloudera Distribution / <a href="#ch11lvl1sec83" title="Cloudera Distribution for Hadoop" class="link">Cloudera Distribution for Hadoop</a></li><li>Hortonworks Data Platform (HDP) / <a href="#ch11lvl1sec83" title="Hortonworks Data Platform" class="link">Hortonworks Data Platform</a></li><li>MapR / <a href="#ch11lvl1sec83" title="MapR" class="link">MapR</a></li><li>selecting / <a href="#ch11lvl1sec83" title="Choosing a distribution" class="link">Choosing a distribution</a></li></ul></li>
        <li>Amazon account<ul><li>reference link / <a href="#ch01lvl1sec15" title="Creating an AWS account" class="link">Creating an AWS account</a></li></ul></li>
        <li>Amazon CLI<ul><li>reference link / <a href="#ch01lvl1sec15" title="The AWS command-line interface" class="link">The AWS command-line interface</a></li></ul></li>
        <li>Amazon EMR<ul><li>about / <a href="#ch01lvl1sec15" title="Amazon EMR" class="link">Amazon EMR</a></li><li>AWS account, creating / <a href="#ch01lvl1sec15" title="Creating an AWS account" class="link">Creating an AWS account</a></li><li>required services, signing up / <a href="#ch01lvl1sec15" title="Signing up for the necessary services" class="link">Signing up for the necessary services</a></li></ul></li>
        <li>Amazon Relational Database Service (Amazon RDS) / <a href="#ch11lvl1sec87" title="SimpleDB and DynamoDB" class="link">SimpleDB and DynamoDB</a></li>
        <li>Amazon Web Services<ul><li>Hive, working with / <a href="#ch07lvl1sec54" title="Hive and Amazon Web Services" class="link">Hive and Amazon Web Services</a></li></ul></li>
        <li>Ambari<ul><li>about / <a href="#ch10lvl1sec73" title="Ambari â€“ the open source alternative" class="link">Ambari â€“ the open source alternative</a></li><li>URL / <a href="#ch10lvl1sec73" title="Ambari â€“ the open source alternative" class="link">Ambari â€“ the open source alternative</a></li></ul></li>
        <li>AMPLab<ul><li>at UC Berkeley, URL / <a href="#ch05lvl1sec38" title="Apache Spark" class="link">Apache Spark</a></li></ul></li>
        <li>Apache Avro<ul><li>about / <a href="#ch02lvl1sec27" title="Avro" class="link">Avro</a></li><li>URL / <a href="#ch02lvl1sec27" title="Avro" class="link">Avro</a></li></ul></li>
        <li>Apache Crunch<ul><li>about / <a href="#ch09lvl1sec69" title="Apache Crunch" class="link">Apache Crunch</a></li><li>URL / <a href="#ch09lvl1sec69" title="Apache Crunch" class="link">Apache Crunch</a></li><li>JARs / <a href="#ch09lvl1sec69" title="Getting started" class="link">Getting started</a></li><li>libraries / <a href="#ch09lvl1sec69" title="Getting started" class="link">Getting started</a></li><li>concepts / <a href="#ch09lvl1sec69" title="Concepts" class="link">Concepts</a></li><li>PCollection&lt;T&gt; interface / <a href="#ch09lvl1sec69" title="Concepts" class="link">Concepts</a></li><li>PTable&lt;Key, Value&gt; interface / <a href="#ch09lvl1sec69" title="Concepts" class="link">Concepts</a></li><li>data serialization / <a href="#ch09lvl1sec69" title="Data serialization" class="link">Data serialization</a></li><li>data processing patterns / <a href="#ch09lvl1sec69" title="Data processing patterns" class="link">Data processing patterns</a></li><li>Pipelines implementation / <a href="#ch09lvl1sec69" title="Pipelines implementation and execution" class="link">Pipelines implementation and execution</a></li><li>execution / <a href="#ch09lvl1sec69" title="Pipelines implementation and execution" class="link">Pipelines implementation and execution</a></li><li>examples / <a href="#ch09lvl1sec69" title="Crunch examples" class="link">Crunch examples</a></li><li>Kite Morphlines / <a href="#ch09lvl1sec69" title="Kite Morphlines" class="link">Kite Morphlines</a></li></ul></li>
        <li>Apache DataFu<ul><li>reference link / <a href="#ch06lvl1sec48" title="Contributed UDFs" class="link">Contributed UDFs</a>, <a href="#ch06lvl1sec48" title="Apache DataFu" class="link">Apache DataFu</a></li><li>about / <a href="#ch06lvl1sec48" title="Apache DataFu" class="link">Apache DataFu</a></li></ul></li>
        <li>Apache Giraph<ul><li>about / <a href="#ch11lvl1sec84" title="Apache Giraph" class="link">Apache Giraph</a></li><li>URL / <a href="#ch11lvl1sec84" title="Apache Giraph" class="link">Apache Giraph</a></li></ul></li>
        <li>Apache HAMA<ul><li>about / <a href="#ch11lvl1sec84" title="Apache HAMA" class="link">Apache HAMA</a></li></ul></li>
        <li>Apache Kafka<ul><li>URL / <a href="#ch03lvl1sec34" title="Apache Samza" class="link">Apache Samza</a>, <a href="#ch04lvl1sec36" title="Samza's best friend â€“ Apache Kafka" class="link">Samza's best friend â€“ Apache Kafka</a></li><li>about / <a href="#ch04lvl1sec36" title="Samza's best friend â€“ Apache Kafka" class="link">Samza's best friend â€“ Apache Kafka</a></li><li>Twitter data, getting into / <a href="#ch04lvl1sec36" title="Getting Twitter data into Kafka" class="link">Getting Twitter data into Kafka</a></li></ul></li>
        <li>Apache Knox<ul><li>about / <a href="#ch10lvl1sec79" title="Beyond basic authorization" class="link">Beyond basic authorization</a></li><li>URL / <a href="#ch10lvl1sec79" title="Beyond basic authorization" class="link">Beyond basic authorization</a></li></ul></li>
        <li>Apache Sentry<ul><li>URL / <a href="#ch10lvl1sec79" title="Beyond basic authorization" class="link">Beyond basic authorization</a></li></ul></li>
        <li>Apache Spark<ul><li>about / <a href="#ch05lvl1sec38" title="Apache Spark" class="link">Apache Spark</a>, <a href="#ch05lvl1sec38" title="Getting started with Spark" class="link">Getting started with Spark</a></li><li>URL / <a href="#ch05lvl1sec38" title="Apache Spark" class="link">Apache Spark</a>, <a href="#ch05lvl1sec38" title="Getting started with Spark" class="link">Getting started with Spark</a></li><li>cluster computing, with working sets / <a href="#ch05lvl1sec38" title="Cluster computing with working sets" class="link">Cluster computing with working sets</a></li><li>Resilient Distributed Datasets (RDDs) / <a href="#ch05lvl1sec38" title="Resilient Distributed Datasets (RDDs)" class="link">Resilient Distributed Datasets (RDDs)</a></li><li>actions / <a href="#ch05lvl1sec38" title="Actions" class="link">Actions</a></li><li>deployment / <a href="#ch05lvl1sec38" title="Deployment" class="link">Deployment</a></li><li>on YARN / <a href="#ch05lvl1sec38" title="Spark on YARN" class="link">Spark on YARN</a></li><li>on EC2 / <a href="#ch05lvl1sec38" title="Spark on EC2" class="link">Spark on EC2</a></li><li>standalone applications, writing / <a href="#ch05lvl1sec38" title="Writing and running standalone applications" class="link">Writing and running standalone applications</a></li><li>Scala API / <a href="#ch05lvl1sec38" title="Scala API" class="link">Scala API</a></li><li>Java API / <a href="#ch05lvl1sec38" title="Java API" class="link">Java API</a></li><li>WordCount, in Java / <a href="#ch05lvl1sec38" title="WordCount in Java" class="link">WordCount in Java</a></li><li>Python API / <a href="#ch05lvl1sec38" title="Python API" class="link">Python API</a></li><li>data, processing / <a href="#ch05lvl1sec40" title="Processing data with Apache Spark" class="link">Processing data with Apache Spark</a></li></ul></li>
        <li>Apache Spark, ecosystem<ul><li>about / <a href="#ch05lvl1sec39" title="The Spark ecosystem" class="link">The Spark ecosystem</a></li><li>Spark Streaming / <a href="#ch05lvl1sec39" title="Spark Streaming" class="link">Spark Streaming</a></li><li>GraphX / <a href="#ch05lvl1sec39" title="GraphX" class="link">GraphX</a></li><li>MLLib / <a href="#ch05lvl1sec39" title="MLlib" class="link">MLlib</a></li><li>Spark SQL / <a href="#ch05lvl1sec39" title="Spark SQL" class="link">Spark SQL</a></li></ul></li>
        <li>Apache Storm<ul><li>about / <a href="#ch11lvl1sec84" title="Apache Storm" class="link">Apache Storm</a></li><li>URL / <a href="#ch11lvl1sec84" title="Apache Storm" class="link">Apache Storm</a></li></ul></li>
        <li>Apache Thrift<ul><li>about / <a href="#ch02lvl1sec25" title="Thrift " class="link">Thrift </a></li><li>URL / <a href="#ch02lvl1sec25" title="Thrift " class="link">Thrift </a></li></ul></li>
        <li>Apache Tika<ul><li>about / <a href="#ch04lvl1sec36" title="Multijob workflows" class="link">Multijob workflows</a></li><li>URL / <a href="#ch04lvl1sec36" title="Multijob workflows" class="link">Multijob workflows</a></li></ul></li>
        <li>Apache Twill<ul><li>URL / <a href="#ch03lvl1sec33" title="Thinking in layers" class="link">Thinking in layers</a></li></ul></li>
        <li>Apache ZooKeeper<ul><li>about / <a href="#ch02lvl1sec22" title="Apache ZooKeeper â€“ a different type of filesystem" class="link">Apache ZooKeeper â€“ a different type of filesystem</a></li><li>URL / <a href="#ch02lvl1sec22" title="Apache ZooKeeper â€“ a different type of filesystem" class="link">Apache ZooKeeper â€“ a different type of filesystem</a></li><li>distributed lock, implementing with sequential ZNodes / <a href="#ch02lvl1sec22" title="Implementing a distributed lock with sequential ZNodes" class="link">Implementing a distributed lock with sequential ZNodes</a></li><li>group membership, implementing / <a href="#ch02lvl1sec22" title="Implementing group membership and leader election using ephemeral ZNodes" class="link">Implementing group membership and leader election using ephemeral ZNodes</a></li><li>leader election, implementing with ephemeral ZNodes / <a href="#ch02lvl1sec22" title="Implementing group membership and leader election using ephemeral ZNodes" class="link">Implementing group membership and leader election using ephemeral ZNodes</a></li><li>Java API / <a href="#ch02lvl1sec22" title="Java API" class="link">Java API</a></li><li>blocks, building / <a href="#ch02lvl1sec22" title="Building blocks" class="link">Building blocks</a></li><li>used, for enabling automatic NameNode failover / <a href="#ch02lvl1sec23" title="Automatic NameNode failover" class="link">Automatic NameNode failover</a></li></ul></li>
        <li>application development<ul><li>framework, selecting / <a href="#ch09lvl1sec66" title="Choosing a framework" class="link">Choosing a framework</a></li></ul></li>
        <li>Application Manager<ul><li>about / <a href="#ch10lvl1sec81" title="ResourceManager, NodeManager, and Application Manager" class="link">ResourceManager, NodeManager, and Application Manager</a></li></ul></li>
        <li>ApplicationMaster (AM)<ul><li>about / <a href="#ch03lvl1sec33" title="Anatomy of a YARN application" class="link">Anatomy of a YARN application</a></li></ul></li>
        <li>architectural principles, HDFS and MapReduce / <a href="#ch01lvl1sec10" title="Common building blocks" class="link">Common building blocks</a></li>
        <li>Array wrapper classes<ul><li>about / <a href="#ch02lvl1sec26" title="Array wrapper classes " class="link">Array wrapper classes </a></li></ul></li>
        <li>automatic NameNode failover<ul><li>enabling / <a href="#ch02lvl1sec23" title="Automatic NameNode failover" class="link">Automatic NameNode failover</a></li></ul></li>
        <li>Avro<ul><li>about / <a href="#ch07lvl1sec53" title="Avro" class="link">Avro</a></li></ul></li>
        <li>Avro schema evolution, using<ul><li>thoughts / <a href="#ch08lvl1sec62" title="Final thoughts on using Avro schema evolution" class="link">Final thoughts on using Avro schema evolution</a></li><li>additive changes, making / <a href="#ch08lvl1sec62" title="Only make additive changes" class="link">Only make additive changes</a></li><li>schema versions, managing explicitly / <a href="#ch08lvl1sec62" title="Manage schema versions explicitly" class="link">Manage schema versions explicitly</a></li><li>schema distribution / <a href="#ch08lvl1sec62" title="Think about schema distribution" class="link">Think about schema distribution</a></li></ul></li>
        <li>Avro schemas<ul><li>about / <a href="#ch02lvl1sec27" title="Using the Java API" class="link">Using the Java API</a></li></ul></li>
        <li>AvroSerde<ul><li>URL / <a href="#ch07lvl1sec53" title="Avro" class="link">Avro</a></li><li>about / <a href="#ch07lvl1sec53" title="Avro" class="link">Avro</a></li></ul></li>
        <li>AWS<ul><li>about / <a href="#ch01lvl1sec12" title="Distributions of Apache Hadoop" class="link">Distributions of Apache Hadoop</a>, <a href="#ch01lvl1sec14" title="AWS â€“ infrastructure on demand from Amazon" class="link">AWS â€“ infrastructure on demand from Amazon</a></li><li>Simple Storage Service (S3) / <a href="#ch01lvl1sec14" title="Simple Storage Service (S3)" class="link">Simple Storage Service (S3)</a></li><li>Elastic MapReduce (EMR) / <a href="#ch01lvl1sec14" title="Elastic MapReduce (EMR)" class="link">Elastic MapReduce (EMR)</a></li></ul></li>
        <li>AWS command-line interface<ul><li>about / <a href="#ch01lvl1sec15" title="The AWS command-line interface" class="link">The AWS command-line interface</a></li><li>reference link / <a href="#ch01lvl1sec15" title="The AWS command-line interface" class="link">The AWS command-line interface</a></li></ul></li>
        <li>AWS credentials<ul><li>about / <a href="#ch01lvl1sec15" title="AWS credentials" class="link">AWS credentials</a></li><li>account ID / <a href="#ch01lvl1sec15" title="AWS credentials" class="link">AWS credentials</a></li><li>access key / <a href="#ch01lvl1sec15" title="AWS credentials" class="link">AWS credentials</a></li><li>secret access key / <a href="#ch01lvl1sec15" title="AWS credentials" class="link">AWS credentials</a></li><li>key pairs / <a href="#ch01lvl1sec15" title="AWS credentials" class="link">AWS credentials</a></li><li>reference link / <a href="#ch01lvl1sec15" title="AWS credentials" class="link">AWS credentials</a></li></ul></li>
        <li>AWS developer forums<ul><li>URL / <a href="#ch11lvl1sec88" title="Mailing lists and forums" class="link">Mailing lists and forums</a></li></ul></li>
        <li>AWS resources<ul><li>about / <a href="#ch11lvl1sec87" title="AWS resources" class="link">AWS resources</a></li><li>SimpleDB / <a href="#ch11lvl1sec87" title="SimpleDB and DynamoDB" class="link">SimpleDB and DynamoDB</a></li><li>DynamoDB / <a href="#ch11lvl1sec87" title="SimpleDB and DynamoDB" class="link">SimpleDB and DynamoDB</a></li><li>Data Pipeline / <a href="#ch11lvl1sec87" title="Data Pipeline" class="link">Data Pipeline</a></li></ul></li>
      </ul>
      <h2>B</h2>
      <ul>
        <li>block replication<ul><li>about / <a href="#ch02lvl1sec19" title="Block replication" class="link">Block replication</a></li></ul></li>
        <li>Bulk Synchronous Parallel (BSP) model<ul><li>about / <a href="#ch11lvl1sec84" title="Apache Giraph" class="link">Apache Giraph</a></li></ul></li>
      </ul>
      <h2>C</h2>
      <ul>
        <li>Cascading<ul><li>about / <a href="#ch11lvl1sec86" title="Cascading" class="link">Cascading</a></li><li>URL / <a href="#ch11lvl1sec86" title="Cascading" class="link">Cascading</a></li><li>reference links / <a href="#ch11lvl1sec86" title="Cascading" class="link">Cascading</a></li></ul></li>
        <li>Cloudera<ul><li>URL / <a href="#ch01lvl1sec12" title="Distributions of Apache Hadoop" class="link">Distributions of Apache Hadoop</a></li><li>URL, for documentation / <a href="#ch10lvl1sec72" title="Cloudera Manager" class="link">Cloudera Manager</a></li><li>URL, for blog post / <a href="#ch10lvl1sec75" title="Sharing resources" class="link">Sharing resources</a></li></ul></li>
        <li>Cloudera distribution<ul><li>about / <a href="#ch11lvl1sec83" title="Cloudera Distribution for Hadoop" class="link">Cloudera Distribution for Hadoop</a></li><li>URL / <a href="#ch11lvl1sec83" title="Cloudera Distribution for Hadoop" class="link">Cloudera Distribution for Hadoop</a></li></ul></li>
        <li>Cloudera Hadoop Distribution (CDH)<ul><li>about / <a href="#ch10lvl1sec72" title="Cloudera Manager" class="link">Cloudera Manager</a></li></ul></li>
        <li>Cloudera Kitten<ul><li>URL / <a href="#ch03lvl1sec33" title="Thinking in layers" class="link">Thinking in layers</a></li></ul></li>
        <li>Cloudera Manager<ul><li>about / <a href="#ch10lvl1sec72" title="Cloudera Manager" class="link">Cloudera Manager</a></li><li>payment, for subscription services / <a href="#ch10lvl1sec72" title="To pay or not to pay" class="link">To pay or not to pay</a></li><li>cluster management, performing / <a href="#ch10lvl1sec72" title="Cluster management using Cloudera Manager" class="link">Cluster management using Cloudera Manager</a></li><li>integrating, with systems management tools / <a href="#ch10lvl1sec72" title="Cloudera Manager and other management tools" class="link">Cloudera Manager and other management tools</a></li><li>monitoring with / <a href="#ch10lvl1sec72" title="Monitoring with Cloudera Manager" class="link">Monitoring with Cloudera Manager</a></li><li>logfiles, finding / <a href="#ch10lvl1sec72" title="Finding configuration files" class="link">Finding configuration files</a></li></ul></li>
        <li>Cloudera Manager API<ul><li>about / <a href="#ch10lvl1sec72" title="Cloudera Manager API" class="link">Cloudera Manager API</a></li></ul></li>
        <li>Cloudera Manager lock-in<ul><li>about / <a href="#ch10lvl1sec72" title="Cloudera Manager lock-in" class="link">Cloudera Manager lock-in</a></li></ul></li>
        <li>Cloudera Quickstart VM<ul><li>about / <a href="#ch01lvl1sec15" title="Cloudera QuickStart VM" class="link">Cloudera QuickStart VM</a></li><li>advantages / <a href="#ch01lvl1sec15" title="Cloudera QuickStart VM" class="link">Cloudera QuickStart VM</a></li></ul></li>
        <li>cluster<ul><li>building, on EMR / <a href="#ch10lvl1sec77" title="Building a cluster on EMR" class="link">Building a cluster on EMR</a></li></ul></li>
        <li>cluster, APache Spark<ul><li>computing, with working sets / <a href="#ch05lvl1sec38" title="Cluster computing with working sets" class="link">Cluster computing with working sets</a></li></ul></li>
        <li>cluster, on EMR<ul><li>filesystem, considerations / <a href="#ch10lvl1sec77" title="Considerations about filesystems" class="link">Considerations about filesystems</a></li><li>data, obtaining into EMR / <a href="#ch10lvl1sec77" title="Getting data into EMR" class="link">Getting data into EMR</a></li><li>EC2 instances / <a href="#ch10lvl1sec77" title="EC2 instances and tuning" class="link">EC2 instances and tuning</a></li><li>EC2 tuning / <a href="#ch10lvl1sec77" title="EC2 instances and tuning" class="link">EC2 instances and tuning</a></li></ul></li>
        <li>cluster management<ul><li>performing, Cloudera Manager used / <a href="#ch10lvl1sec72" title="Cluster management using Cloudera Manager" class="link">Cluster management using Cloudera Manager</a></li></ul></li>
        <li>cluster startup, HDFS<ul><li>about / <a href="#ch02lvl1sec19" title="Cluster startup" class="link">Cluster startup</a></li><li>NameNode startup / <a href="#ch02lvl1sec19" title="NameNode startup" class="link">NameNode startup</a></li><li>DataNode startup / <a href="#ch02lvl1sec19" title="DataNode startup" class="link">DataNode startup</a></li></ul></li>
        <li>cluster tuning<ul><li>about / <a href="#ch10lvl1sec78" title="Cluster tuning" class="link">Cluster tuning</a></li><li>JVM considerations / <a href="#ch10lvl1sec78" title="JVM considerations" class="link">JVM considerations</a></li><li>map optimization / <a href="#ch10lvl1sec78" title="Map and reduce optimizations" class="link">Map and reduce optimizations</a></li><li>reduce optimization / <a href="#ch10lvl1sec78" title="Map and reduce optimizations" class="link">Map and reduce optimizations</a></li></ul></li>
        <li>column-oriented data formats<ul><li>about / <a href="#ch02lvl1sec27" title="Column-oriented data formats" class="link">Column-oriented data formats</a></li><li>RCFile / <a href="#ch02lvl1sec27" title="RCFile" class="link">RCFile</a></li><li>ORC / <a href="#ch02lvl1sec27" title="ORC" class="link">ORC</a></li><li>Parquet / <a href="#ch02lvl1sec27" title="Parquet" class="link">Parquet</a></li><li>Avro / <a href="#ch02lvl1sec27" title="Avro" class="link">Avro</a></li><li>Java API, using / <a href="#ch02lvl1sec27" title="Using the Java API" class="link">Using the Java API</a></li></ul></li>
        <li>columnar<ul><li>about / <a href="#ch07lvl1sec53" title="Columnar stores" class="link">Columnar stores</a></li></ul></li>
        <li>columnar stores / <a href="#ch07lvl1sec53" title="Columnar stores" class="link">Columnar stores</a></li>
        <li>combiner class, Java API to MapReduce<ul><li>about / <a href="#ch03lvl1sec30" title="Combiner" class="link">Combiner</a></li></ul></li>
        <li>combineValues operation<ul><li>about / <a href="#ch09lvl1sec69" title="Concepts" class="link">Concepts</a></li></ul></li>
        <li>command-line access, HDFS filesystem<ul><li>about / <a href="#ch02lvl1sec20" title="Command-line access to the HDFS filesystem" class="link">Command-line access to the HDFS filesystem</a></li><li>hdfs command / <a href="#ch02lvl1sec20" title="Command-line access to the HDFS filesystem" class="link">Command-line access to the HDFS filesystem</a></li><li>dfs command / <a href="#ch02lvl1sec20" title="Command-line access to the HDFS filesystem" class="link">Command-line access to the HDFS filesystem</a></li><li>dfsadmin command / <a href="#ch02lvl1sec20" title="Command-line access to the HDFS filesystem" class="link">Command-line access to the HDFS filesystem</a></li></ul></li>
        <li>Comparable interface<ul><li>about / <a href="#ch02lvl1sec26" title="The Comparable and WritableComparable interfaces" class="link">The Comparable and WritableComparable interfaces</a></li></ul></li>
        <li>complex data types<ul><li>map / <a href="#ch06lvl1sec47" title="Pig data types" class="link">Pig data types</a></li><li>tuple / <a href="#ch06lvl1sec47" title="Pig data types" class="link">Pig data types</a></li><li>bag / <a href="#ch06lvl1sec47" title="Pig data types" class="link">Pig data types</a></li></ul></li>
        <li>complex event processing (CEP)<ul><li>about / <a href="#ch04lvl1sec36" title="How Samza works" class="link">How Samza works</a></li></ul></li>
        <li>components, Hadoop<ul><li>about / <a href="#ch01lvl1sec10" title="Components of Hadoop" class="link">Components of Hadoop</a></li><li>common building blocks / <a href="#ch01lvl1sec10" title="Common building blocks" class="link">Common building blocks</a></li><li>storage / <a href="#ch01lvl1sec10" title="Storage" class="link">Storage</a></li><li>computation / <a href="#ch01lvl1sec10" title="Computation" class="link">Computation</a></li></ul></li>
        <li>components, YARN<ul><li>about / <a href="#ch03lvl1sec33" title="The components of YARN" class="link">The components of YARN</a></li><li>ResourceManager (RM) / <a href="#ch03lvl1sec33" title="The components of YARN" class="link">The components of YARN</a></li><li>NodeManager (NM) / <a href="#ch03lvl1sec33" title="The components of YARN" class="link">The components of YARN</a></li></ul></li>
        <li>computation<ul><li>about / <a href="#ch01lvl1sec10" title="Computation" class="link">Computation</a></li></ul></li>
        <li>computation, Hadoop 2<ul><li>about / <a href="#ch01lvl1sec11" title="Computation in Hadoop 2" class="link">Computation in Hadoop 2</a></li></ul></li>
        <li>computational frameworks<ul><li>about / <a href="#ch11lvl1sec84" title="Other computational frameworks" class="link">Other computational frameworks</a></li><li>Apache Storm / <a href="#ch11lvl1sec84" title="Apache Storm" class="link">Apache Storm</a></li><li>Apache Giraph / <a href="#ch11lvl1sec84" title="Apache Giraph" class="link">Apache Giraph</a>, <a href="#ch11lvl1sec84" title="Apache HAMA" class="link">Apache HAMA</a></li></ul></li>
        <li>conferences<ul><li>about / <a href="#ch11lvl1sec88" title="Conferences" class="link">Conferences</a></li><li>reference link / <a href="#ch11lvl1sec88" title="Conferences" class="link">Conferences</a></li></ul></li>
        <li>configuration file, Samza<ul><li>about / <a href="#ch04lvl1sec36" title="The configuration file" class="link">The configuration file</a></li></ul></li>
        <li>containers<ul><li>about / <a href="#ch02lvl1sec27" title="Serialization and Containers" class="link">Serialization and Containers</a></li></ul></li>
        <li>contributed UDFs<ul><li>about / <a href="#ch06lvl1sec48" title="Contributed UDFs" class="link">Contributed UDFs</a></li><li>Piggybank / <a href="#ch06lvl1sec48" title="Piggybank" class="link">Piggybank</a></li><li>Elephant Bird / <a href="#ch06lvl1sec48" title="Elephant Bird" class="link">Elephant Bird</a></li><li>Apache DataFu / <a href="#ch06lvl1sec48" title="Apache DataFu" class="link">Apache DataFu</a></li></ul></li>
        <li>create.hql script<ul><li>reference link / <a href="#ch08lvl1sec61" title="Extracting data and ingesting into Hive" class="link">Extracting data and ingesting into Hive</a></li></ul></li>
        <li>Crunch examples<ul><li>about / <a href="#ch09lvl1sec69" title="Crunch examples" class="link">Crunch examples</a></li><li>word co-occurrence / <a href="#ch09lvl1sec69" title="Word co-occurrence" class="link">Word co-occurrence</a></li><li>TF-IDF / <a href="#ch09lvl1sec69" title="TF-IDF" class="link">TF-IDF</a></li></ul></li>
        <li>Curator project <ul><li>reference link / <a href="#ch02lvl1sec22" title="Building blocks" class="link">Building blocks</a></li></ul></li>
      </ul>
      <h2>D</h2>
      <ul>
        <li>data, managing<ul><li>about / <a href="#ch02lvl1sec26" title="Managing and serializing data" class="link">Managing and serializing data</a></li><li>Writable interface / <a href="#ch02lvl1sec26" title="The Writable interface" class="link">The Writable interface</a></li><li>wrapper classes / <a href="#ch02lvl1sec26" title="Introducing the wrapper classes " class="link">Introducing the wrapper classes </a></li><li>Array wrapper classes / <a href="#ch02lvl1sec26" title="Array wrapper classes " class="link">Array wrapper classes </a></li><li>Comparable interface / <a href="#ch02lvl1sec26" title="The Comparable and WritableComparable interfaces" class="link">The Comparable and WritableComparable interfaces</a></li><li>WritableComparable interface / <a href="#ch02lvl1sec26" title="The Comparable and WritableComparable interfaces" class="link">The Comparable and WritableComparable interfaces</a></li></ul></li>
        <li>data, Pig<ul><li>working with / <a href="#ch06lvl1sec47" title="Working with data" class="link">Working with data</a></li><li>FILTER operator / <a href="#ch06lvl1sec47" title="Filtering" class="link">Filtering</a></li><li>aggregation / <a href="#ch06lvl1sec47" title="Aggregation" class="link">Aggregation</a></li><li>FOREACH operator / <a href="#ch06lvl1sec47" title="Foreach" class="link">Foreach</a></li><li>JOIN operator / <a href="#ch06lvl1sec47" title="Join" class="link">Join</a></li></ul></li>
        <li>data, storing<ul><li>about / <a href="#ch02lvl1sec27" title="Storing data" class="link">Storing data</a></li><li>serialization file format / <a href="#ch02lvl1sec27" title="Serialization and Containers" class="link">Serialization and Containers</a></li><li>containers file format / <a href="#ch02lvl1sec27" title="Serialization and Containers" class="link">Serialization and Containers</a></li><li>file compression / <a href="#ch02lvl1sec27" title="Compression" class="link">Compression</a></li><li>general-purpose file formats / <a href="#ch02lvl1sec27" title="General-purpose file formats" class="link">General-purpose file formats</a></li><li>column-oriented data formats / <a href="#ch02lvl1sec27" title="Column-oriented data formats" class="link">Column-oriented data formats</a></li></ul></li>
        <li>Data core<ul><li>about / <a href="#ch09lvl1sec68" title="Data Core" class="link">Data Core</a></li></ul></li>
        <li>Data Crunch<ul><li>about / <a href="#ch09lvl1sec68" title="Data Crunch" class="link">Data Crunch</a></li></ul></li>
        <li>Data HCatalog<ul><li>about / <a href="#ch09lvl1sec68" title="Data HCatalog" class="link">Data HCatalog</a></li></ul></li>
        <li>Data Hive<ul><li>about / <a href="#ch09lvl1sec68" title="Data Hive" class="link">Data Hive</a></li></ul></li>
        <li>data life cycle management<ul><li>about / <a href="#ch08lvl1sec60" title="What data lifecycle management is" class="link">What data lifecycle management is</a></li><li>importance / <a href="#ch08lvl1sec60" title="Importance of data lifecycle management" class="link">Importance of data lifecycle management</a></li><li>tools / <a href="#ch08lvl1sec60" title="Tools to help" class="link">Tools to help</a></li></ul></li>
        <li>Data MapReduce<ul><li>about / <a href="#ch09lvl1sec68" title="Data MapReduce" class="link">Data MapReduce</a></li></ul></li>
        <li>DataNode / <a href="#ch10lvl1sec81" title="NameNode and DataNode" class="link">NameNode and DataNode</a></li>
        <li>DataNodes<ul><li>about / <a href="#ch01lvl1sec11" title="Storage in Hadoop 2" class="link">Storage in Hadoop 2</a></li></ul></li>
        <li>DataNode startup<ul><li>about / <a href="#ch02lvl1sec19" title="DataNode startup" class="link">DataNode startup</a></li></ul></li>
        <li>Data Pipeline<ul><li>about / <a href="#ch11lvl1sec87" title="Data Pipeline" class="link">Data Pipeline</a></li><li>reference link / <a href="#ch11lvl1sec87" title="Data Pipeline" class="link">Data Pipeline</a></li></ul></li>
        <li>data processing<ul><li>about / <a href="#ch01lvl1sec17" title="Data processing with Hadoop" class="link">Data processing with Hadoop</a></li><li>dataset, generating from Twitter / <a href="#ch01lvl1sec17" title="Why Twitter?" class="link">Why Twitter?</a></li><li>dataset, building / <a href="#ch01lvl1sec17" title="Building our first dataset" class="link">Building our first dataset</a></li><li>programmatic access, with Python / <a href="#ch01lvl1sec17" title="Programmatic access with Python" class="link">Programmatic access with Python</a></li></ul></li>
        <li>data processing, Apache Spark<ul><li>about / <a href="#ch05lvl1sec40" title="Processing data with Apache Spark" class="link">Processing data with Apache Spark</a></li><li>examples, running / <a href="#ch05lvl1sec40" title="Building and running the examples" class="link">Building and running the examples</a></li><li>examples, building / <a href="#ch05lvl1sec40" title="Building and running the examples" class="link">Building and running the examples</a></li><li>examples, running on YARN / <a href="#ch05lvl1sec40" title="Running the examples on YARN" class="link">Running the examples on YARN</a></li><li>popular topics, finding / <a href="#ch05lvl1sec40" title="Finding popular topics" class="link">Finding popular topics</a></li><li>sentiment, assigning to topics / <a href="#ch05lvl1sec40" title="Assigning a sentiment to topics " class="link">Assigning a sentiment to topics </a></li><li>on streams / <a href="#ch05lvl1sec40" title="Data processing on streams" class="link">Data processing on streams</a></li><li>state management / <a href="#ch05lvl1sec40" title="State management" class="link">State management</a></li><li>data analysis, with Spark SQL / <a href="#ch05lvl1sec40" title="Data analysis with Spark SQL" class="link">Data analysis with Spark SQL</a></li><li>SQL, on data streams / <a href="#ch05lvl1sec40" title="SQL on data streams" class="link">SQL on data streams</a></li></ul></li>
        <li>data processing patterns, Crunch<ul><li>about / <a href="#ch09lvl1sec69" title="Data processing patterns" class="link">Data processing patterns</a></li><li>aggregation and sorting / <a href="#ch09lvl1sec69" title="Aggregation and sorting" class="link">Aggregation and sorting</a></li><li>joining data / <a href="#ch09lvl1sec69" title="Joining data" class="link">Joining data</a></li></ul></li>
        <li>data serialization, Crunch<ul><li>about / <a href="#ch09lvl1sec69" title="Data serialization" class="link">Data serialization</a></li></ul></li>
        <li>dataset, building with Twitter<ul><li>about / <a href="#ch01lvl1sec17" title="Building our first dataset" class="link">Building our first dataset</a></li><li>multiple APIs, using / <a href="#ch01lvl1sec17" title="One service, multiple APIs" class="link">One service, multiple APIs</a></li><li>anatomy, of Tweet / <a href="#ch01lvl1sec17" title="Anatomy of a Tweet" class="link">Anatomy of a Tweet</a></li><li>Twitter credentials / <a href="#ch01lvl1sec17" title="Twitter credentials" class="link">Twitter credentials</a></li></ul></li>
        <li>Data Spark<ul><li>about / <a href="#ch09lvl1sec68" title="Data Spark" class="link">Data Spark</a></li></ul></li>
        <li>data types, Hive<ul><li>numeric / <a href="#ch07lvl1sec53" title="Data types" class="link">Data types</a></li><li>date and time / <a href="#ch07lvl1sec53" title="Data types" class="link">Data types</a></li><li>string / <a href="#ch07lvl1sec53" title="Data types" class="link">Data types</a></li><li>collections / <a href="#ch07lvl1sec53" title="Data types" class="link">Data types</a></li><li>misc / <a href="#ch07lvl1sec53" title="Data types" class="link">Data types</a></li></ul></li>
        <li>data types, Pig<ul><li>scalar data types / <a href="#ch06lvl1sec47" title="Pig data types" class="link">Pig data types</a></li><li>complex data types / <a href="#ch06lvl1sec47" title="Pig data types" class="link">Pig data types</a></li></ul></li>
        <li>DDL statements, Hive / <a href="#ch07lvl1sec53" title="DDL statements" class="link">DDL statements</a></li>
        <li>decayFactor function / <a href="#ch05lvl1sec40" title="State management" class="link">State management</a></li>
        <li>DEFINE operator<ul><li>about / <a href="#ch06lvl1sec48" title="Extending Pig (UDFs)" class="link">Extending Pig (UDFs)</a></li></ul></li>
        <li>derived data, producing<ul><li>about / <a href="#ch08lvl1sec61" title="Producing derived data" class="link">Producing derived data</a></li><li>multiple actions, performing in parallel / <a href="#ch08lvl1sec61" title="Performing multiple actions in parallel" class="link">Performing multiple actions in parallel</a></li><li>subworkflow, calling / <a href="#ch08lvl1sec61" title="Calling a subworkflow" class="link">Calling a subworkflow</a></li><li>global settings, adding / <a href="#ch08lvl1sec61" title="Adding global settings" class="link">Adding global settings</a></li></ul></li>
        <li>DevOps<ul><li>practices / <a href="#ch10lvl1sec71" title="Hadoop and DevOps practices" class="link">Hadoop and DevOps practices</a></li></ul></li>
        <li>directed acyclic graph (DAG)<ul><li>about / <a href="#ch03lvl1sec33" title="YARN" class="link">YARN</a></li></ul></li>
        <li>document frequency<ul><li>about / <a href="#ch09lvl1sec67" title="Calculate document frequency" class="link">Calculate document frequency</a></li><li>calculating, TF-IDF used / <a href="#ch09lvl1sec67" title="Calculate document frequency" class="link">Calculate document frequency</a></li></ul></li>
        <li>Drill<ul><li>URL / <a href="#ch07lvl1sec58" title="Drill, Tajo, and beyond" class="link">Drill, Tajo, and beyond</a></li><li>about / <a href="#ch07lvl1sec58" title="Drill, Tajo, and beyond" class="link">Drill, Tajo, and beyond</a></li></ul></li>
        <li>Driver class, Java API to MapReduce<ul><li>about / <a href="#ch03lvl1sec30" title="The Driver class" class="link">The Driver class</a></li></ul></li>
        <li>dynamic invokers<ul><li>about / <a href="#ch06lvl1sec47" title="Dynamic invokers" class="link">Dynamic invokers</a></li><li>reference link / <a href="#ch06lvl1sec47" title="Dynamic invokers" class="link">Dynamic invokers</a></li></ul></li>
        <li>DynamoDB<ul><li>URL / <a href="#ch11lvl1sec87" title="SimpleDB and DynamoDB" class="link">SimpleDB and DynamoDB</a></li><li>about / <a href="#ch11lvl1sec87" title="SimpleDB and DynamoDB" class="link">SimpleDB and DynamoDB</a></li></ul></li>
      </ul>
      <h2>E</h2>
      <ul>
        <li>EC2<ul><li>Apache Spark on / <a href="#ch05lvl1sec38" title="Spark on EC2" class="link">Spark on EC2</a></li></ul></li>
        <li>EC2 key-value pair<ul><li>reference link / <a href="#ch01lvl1sec15" title="The AWS command-line interface" class="link">The AWS command-line interface</a></li></ul></li>
        <li>ElasticMapReduce<ul><li>Hive, using with / <a href="#ch07lvl1sec54" title="Hive on Elastic MapReduce" class="link">Hive on Elastic MapReduce</a></li></ul></li>
        <li>Elastic MapReduce (EMR)<ul><li>about / <a href="#ch01lvl1sec12" title="Distributions of Apache Hadoop" class="link">Distributions of Apache Hadoop</a>, <a href="#ch01lvl1sec14" title="Elastic MapReduce (EMR)" class="link">Elastic MapReduce (EMR)</a></li><li>URL / <a href="#ch01lvl1sec14" title="Elastic MapReduce (EMR)" class="link">Elastic MapReduce (EMR)</a></li><li>using / <a href="#ch01lvl1sec15" title="Using Elastic MapReduce" class="link">Using Elastic MapReduce</a></li></ul></li>
        <li>Elephant Bird<ul><li>reference link / <a href="#ch06lvl1sec48" title="Contributed UDFs" class="link">Contributed UDFs</a>, <a href="#ch06lvl1sec48" title="Elephant Bird" class="link">Elephant Bird</a></li></ul></li>
        <li>EMR<ul><li>cluster, building on / <a href="#ch10lvl1sec77" title="Building a cluster on EMR" class="link">Building a cluster on EMR</a></li><li>URL, for best practices / <a href="#ch10lvl1sec77" title="Building a cluster on EMR" class="link">Building a cluster on EMR</a></li></ul></li>
        <li>EMR documentation<ul><li>URL / <a href="#ch07lvl1sec54" title="Hive on Elastic MapReduce" class="link">Hive on Elastic MapReduce</a></li></ul></li>
        <li>entities<ul><li>about / <a href="#ch06lvl1sec49" title="Tweet metadata" class="link">Tweet metadata</a></li></ul></li>
        <li>ephemeral ZNodes<ul><li>about / <a href="#ch02lvl1sec22" title="Implementing group membership and leader election using ephemeral ZNodes" class="link">Implementing group membership and leader election using ephemeral ZNodes</a></li></ul></li>
        <li>eval functions, Pig<ul><li>AVG(expression) / <a href="#ch06lvl1sec47" title="Eval" class="link">Eval</a></li><li>COUNT(expression) / <a href="#ch06lvl1sec47" title="Eval" class="link">Eval</a></li><li>COUNT_STAR(expression) / <a href="#ch06lvl1sec47" title="Eval" class="link">Eval</a></li><li>IsEmpty(expression) / <a href="#ch06lvl1sec47" title="Eval" class="link">Eval</a></li><li>MAX(expression) / <a href="#ch06lvl1sec47" title="Eval" class="link">Eval</a></li><li>MIN(expression) / <a href="#ch06lvl1sec47" title="Eval" class="link">Eval</a></li><li>SUM(expression) / <a href="#ch06lvl1sec47" title="Eval" class="link">Eval</a></li><li>TOKENIZE(expression) / <a href="#ch06lvl1sec47" title="Eval" class="link">Eval</a></li></ul></li>
        <li>examples<ul><li>running / <a href="#ch01lvl1sec16" title="Running the examples" class="link">Running the examples</a></li></ul></li>
        <li>examples, MapReduce programs<ul><li>reference link / <a href="#ch03lvl1sec31" title="Running the examples" class="link">Running the examples</a></li><li>local cluster / <a href="#ch03lvl1sec31" title="Local cluster" class="link">Local cluster</a></li><li>Elastic MapReduce / <a href="#ch03lvl1sec31" title="Elastic MapReduce" class="link">Elastic MapReduce</a></li></ul></li>
        <li>examples and source code<ul><li>download link / <a href="#ch01lvl1sec15" title="Getting started" class="link">Getting started</a></li></ul></li>
        <li>ExecutionEngine interface / <a href="#ch06lvl1sec43" title="An overview of Pig" class="link">An overview of Pig</a></li>
        <li>external data, challenges<ul><li>about / <a href="#ch08lvl1sec62" title="Challenges of external data" class="link">Challenges of external data</a></li><li>data validation / <a href="#ch08lvl1sec62" title="Data validation" class="link">Data validation</a></li><li>validation actions / <a href="#ch08lvl1sec62" title="Validation actions" class="link">Validation actions</a></li><li>format changes, handling / <a href="#ch08lvl1sec62" title="Handling format changes" class="link">Handling format changes</a></li><li>schema evolution, handling with Avro / <a href="#ch08lvl1sec62" title="Handling schema evolution with Avro" class="link">Handling schema evolution with Avro</a></li></ul></li>
        <li>EXTERNAL keyword / <a href="#ch07lvl1sec53" title="DDL statements" class="link">DDL statements</a></li>
        <li>Extract-Transform-Load (ETL) / <a href="#ch07lvl1sec53" title="DDL statements" class="link">DDL statements</a></li>
        <li>extract_for_hive.pig<ul><li>URL, for source code / <a href="#ch07lvl1sec52" title="Prerequisites" class="link">Prerequisites</a></li></ul></li>
      </ul>
      <h2>F</h2>
      <ul>
        <li>Falcon<ul><li>URL / <a href="#ch08lvl1sec64" title="Other tools to help" class="link">Other tools to help</a></li><li>about / <a href="#ch08lvl1sec64" title="Other tools to help" class="link">Other tools to help</a></li></ul></li>
        <li>file format, Hive<ul><li>about / <a href="#ch07lvl1sec53" title="File formats and storage" class="link">File formats and storage</a></li><li>JSON / <a href="#ch07lvl1sec53" title="JSON" class="link">JSON</a></li></ul></li>
        <li>FileFormat classes, Hive<ul><li>TextInputFormat / <a href="#ch07lvl1sec53" title="File formats and storage" class="link">File formats and storage</a></li><li>HiveIgnoreKeyTextOutputFormat / <a href="#ch07lvl1sec53" title="File formats and storage" class="link">File formats and storage</a></li><li>SequenceFileInputFormat / <a href="#ch07lvl1sec53" title="File formats and storage" class="link">File formats and storage</a></li><li>SequenceFileOutputFormat / <a href="#ch07lvl1sec53" title="File formats and storage" class="link">File formats and storage</a></li></ul></li>
        <li>filesystem metadata, HDFS<ul><li>protecting / <a href="#ch02lvl1sec21" title="Protecting the filesystem metadata" class="link">Protecting the filesystem metadata</a></li><li>Secondary NameNode, demerits / <a href="#ch02lvl1sec21" title="Secondary NameNode not to the rescue" class="link">Secondary NameNode not to the rescue</a></li><li>Hadoop 2 NameNode HA / <a href="#ch02lvl1sec21" title="Hadoop 2 NameNode HA" class="link">Hadoop 2 NameNode HA</a></li><li>client configuration / <a href="#ch02lvl1sec21" title="Client configuration" class="link">Client configuration</a></li><li>failover, working / <a href="#ch02lvl1sec21" title="How a failover works" class="link">How a failover works</a></li></ul></li>
        <li>FILTER operator<ul><li>about / <a href="#ch06lvl1sec47" title="Filtering" class="link">Filtering</a></li></ul></li>
        <li>FlumeJava<ul><li>reference link / <a href="#ch09lvl1sec69" title="Apache Crunch" class="link">Apache Crunch</a></li></ul></li>
        <li>FOREACH operator<ul><li>about / <a href="#ch06lvl1sec47" title="Foreach" class="link">Foreach</a></li></ul></li>
        <li>fork node<ul><li>about / <a href="#ch08lvl1sec61" title="Performing multiple actions in parallel" class="link">Performing multiple actions in parallel</a></li></ul></li>
        <li>functions, Pig<ul><li>about / <a href="#ch06lvl1sec47" title="Pig functions" class="link">Pig functions</a></li><li>built-in functions / <a href="#ch06lvl1sec47" title="Pig functions" class="link">Pig functions</a></li><li>reference link, for built-in functions / <a href="#ch06lvl1sec47" title="Pig functions" class="link">Pig functions</a></li><li>load/store functions / <a href="#ch06lvl1sec47" title="Load/store" class="link">Load/store</a></li><li>eval / <a href="#ch06lvl1sec47" title="Eval" class="link">Eval</a></li><li>tuple / <a href="#ch06lvl1sec47" title="The tuple, bag, and map functions" class="link">The tuple, bag, and map functions</a></li><li>bag / <a href="#ch06lvl1sec47" title="The tuple, bag, and map functions" class="link">The tuple, bag, and map functions</a></li><li>map / <a href="#ch06lvl1sec47" title="The tuple, bag, and map functions" class="link">The tuple, bag, and map functions</a></li><li>string / <a href="#ch06lvl1sec47" title="The math, string, and datetime functions" class="link">The math, string, and datetime functions</a></li><li>math / <a href="#ch06lvl1sec47" title="The math, string, and datetime functions" class="link">The math, string, and datetime functions</a></li><li>datetime / <a href="#ch06lvl1sec47" title="The math, string, and datetime functions" class="link">The math, string, and datetime functions</a></li><li>dynamic invokers / <a href="#ch06lvl1sec47" title="Dynamic invokers" class="link">Dynamic invokers</a></li><li>macros / <a href="#ch06lvl1sec47" title="Macros" class="link">Macros</a></li></ul></li>
      </ul>
      <h2>G</h2>
      <ul>
        <li>Garbage Collection (GC) / <a href="#ch10lvl1sec78" title="JVM considerations" class="link">JVM considerations</a></li>
        <li>Garbage First (G1) collector / <a href="#ch10lvl1sec78" title="JVM considerations" class="link">JVM considerations</a></li>
        <li>general-purpose file formats<ul><li>about / <a href="#ch02lvl1sec27" title="General-purpose file formats" class="link">General-purpose file formats</a></li><li>Text files / <a href="#ch02lvl1sec27" title="General-purpose file formats" class="link">General-purpose file formats</a></li><li>SequenceFile / <a href="#ch02lvl1sec27" title="General-purpose file formats" class="link">General-purpose file formats</a></li></ul></li>
        <li>general availability (GA) / <a href="#ch01lvl1sec08" title="A note on versioning" class="link">A note on versioning</a></li>
        <li>Google Chubby system<ul><li>reference link / <a href="#ch02lvl1sec22" title="Apache ZooKeeper â€“ a different type of filesystem" class="link">Apache ZooKeeper â€“ a different type of filesystem</a></li></ul></li>
        <li>Google File System (GFS)<ul><li>reference link / <a href="#ch01lvl1sec09" title="The background of Hadoop" class="link">The background of Hadoop</a></li></ul></li>
        <li>Gradle<ul><li>URL / <a href="#ch01lvl1sec16" title="Running the examples" class="link">Running the examples</a></li></ul></li>
        <li>GraphX<ul><li>about / <a href="#ch05lvl1sec39" title="GraphX" class="link">GraphX</a></li><li>URL / <a href="#ch05lvl1sec39" title="GraphX" class="link">GraphX</a></li></ul></li>
        <li>groupByKey() method / <a href="#ch09lvl1sec69" title="Aggregation and sorting" class="link">Aggregation and sorting</a></li>
        <li>groupByKey(GroupingOptions options) method / <a href="#ch09lvl1sec69" title="Aggregation and sorting" class="link">Aggregation and sorting</a></li>
        <li>groupByKey(int numPartitions) method / <a href="#ch09lvl1sec69" title="Aggregation and sorting" class="link">Aggregation and sorting</a></li>
        <li>groupByKey operation<ul><li>about / <a href="#ch09lvl1sec69" title="Concepts" class="link">Concepts</a></li></ul></li>
        <li>GROUP operator<ul><li>about / <a href="#ch06lvl1sec47" title="Aggregation" class="link">Aggregation</a></li></ul></li>
        <li>Grunt<ul><li>about / <a href="#ch06lvl1sec45" title="Grunt â€“ the Pig interactive shell" class="link">Grunt â€“ the Pig interactive shell</a></li><li>sh command / <a href="#ch06lvl1sec45" title="Grunt â€“ the Pig interactive shell" class="link">Grunt â€“ the Pig interactive shell</a></li><li>help command / <a href="#ch06lvl1sec45" title="Grunt â€“ the Pig interactive shell" class="link">Grunt â€“ the Pig interactive shell</a></li></ul></li>
        <li>Guava library<ul><li>URL / <a href="#ch03lvl1sec31" title="The Top N pattern" class="link">The Top N pattern</a></li></ul></li>
      </ul>
      <h2>H</h2>
      <ul>
        <li>Hadoop<ul><li>versioning / <a href="#ch01lvl1sec08" title="A note on versioning" class="link">A note on versioning</a></li><li>background / <a href="#ch01lvl1sec09" title="The background of Hadoop" class="link">The background of Hadoop</a></li><li>components / <a href="#ch01lvl1sec10" title="Components of Hadoop" class="link">Components of Hadoop</a></li><li>dual approach / <a href="#ch01lvl1sec13" title="A dual approach" class="link">A dual approach</a></li><li>about / <a href="#ch01lvl1sec15" title="Getting started" class="link">Getting started</a></li><li>using / <a href="#ch01lvl1sec15" title="Getting Hadoop up and running" class="link">Getting Hadoop up and running</a></li><li>EMR, using / <a href="#ch01lvl1sec15" title="How to use EMR" class="link">How to use EMR</a></li><li>AWS credentials / <a href="#ch01lvl1sec15" title="AWS credentials" class="link">AWS credentials</a></li><li>data processing / <a href="#ch01lvl1sec17" title="Data processing with Hadoop" class="link">Data processing with Hadoop</a></li><li>practices / <a href="#ch10lvl1sec71" title="Hadoop and DevOps practices" class="link">Hadoop and DevOps practices</a></li><li>alternative distributions / <a href="#ch11lvl1sec83" title="Alternative distributions" class="link">Alternative distributions</a></li><li>computational frameworks / <a href="#ch11lvl1sec84" title="Other computational frameworks" class="link">Other computational frameworks</a></li><li>interesting projects / <a href="#ch11lvl1sec85" title="Other interesting projects" class="link">Other interesting projects</a></li><li>programming abstractions / <a href="#ch11lvl1sec86" title="Other programming abstractions" class="link">Other programming abstractions</a></li><li>AWS resources / <a href="#ch11lvl1sec87" title="AWS resources" class="link">AWS resources</a></li><li>sources of information / <a href="#ch11lvl1sec88" title="Sources of information" class="link">Sources of information</a></li></ul></li>
        <li>Hadoop-provided InputFormat, MapReduce job<ul><li>about / <a href="#ch03lvl1sec32" title="Hadoop-provided InputFormat" class="link">Hadoop-provided InputFormat</a></li><li>FileInputFormat / <a href="#ch03lvl1sec32" title="Hadoop-provided InputFormat" class="link">Hadoop-provided InputFormat</a></li><li>SequenceFileInputFormat / <a href="#ch03lvl1sec32" title="Hadoop-provided InputFormat" class="link">Hadoop-provided InputFormat</a></li><li>TextInputFormat / <a href="#ch03lvl1sec32" title="Hadoop-provided InputFormat" class="link">Hadoop-provided InputFormat</a></li><li>KeyValueTextInputFormat / <a href="#ch03lvl1sec32" title="Hadoop-provided InputFormat" class="link">Hadoop-provided InputFormat</a></li></ul></li>
        <li>Hadoop-provided Mapper and Reducer implementations, Java API to MapReduce<ul><li>about / <a href="#ch03lvl1sec30" title="Hadoop-provided mapper and reducer implementations" class="link">Hadoop-provided mapper and reducer implementations</a></li><li>mappers / <a href="#ch03lvl1sec30" title="Hadoop-provided mapper and reducer implementations" class="link">Hadoop-provided mapper and reducer implementations</a></li><li>reducers / <a href="#ch03lvl1sec30" title="Hadoop-provided mapper and reducer implementations" class="link">Hadoop-provided mapper and reducer implementations</a></li></ul></li>
        <li>Hadoop-provided OutputFormat, MapReduce job<ul><li>about / <a href="#ch03lvl1sec32" title="Hadoop-provided OutputFormat" class="link">Hadoop-provided OutputFormat</a></li><li>FileOutputFormat / <a href="#ch03lvl1sec32" title="Hadoop-provided OutputFormat" class="link">Hadoop-provided OutputFormat</a></li><li>NullOutputFormat / <a href="#ch03lvl1sec32" title="Hadoop-provided OutputFormat" class="link">Hadoop-provided OutputFormat</a></li><li>SequenceFileOutputFormat / <a href="#ch03lvl1sec32" title="Hadoop-provided OutputFormat" class="link">Hadoop-provided OutputFormat</a></li><li>TextOutputFormat / <a href="#ch03lvl1sec32" title="Hadoop-provided OutputFormat" class="link">Hadoop-provided OutputFormat</a></li></ul></li>
        <li>Hadoop-provided RecordReader, MapReduce job<ul><li>about / <a href="#ch03lvl1sec32" title="Hadoop-provided RecordReader" class="link">Hadoop-provided RecordReader</a></li><li>LineRecordReader / <a href="#ch03lvl1sec32" title="Hadoop-provided RecordReader" class="link">Hadoop-provided RecordReader</a></li><li>SequenceFileRecordReader / <a href="#ch03lvl1sec32" title="Hadoop-provided RecordReader" class="link">Hadoop-provided RecordReader</a></li></ul></li>
        <li>Hadoop 2<ul><li>about / <a href="#ch01lvl1sec11" title="Hadoop 2 â€“ what's the big deal?" class="link">Hadoop 2 â€“ what's the big deal?</a></li><li>storage / <a href="#ch01lvl1sec11" title="Storage in Hadoop 2" class="link">Storage in Hadoop 2</a></li><li>computation / <a href="#ch01lvl1sec11" title="Computation in Hadoop 2" class="link">Computation in Hadoop 2</a></li><li>diagrammatic representation, architecture / <a href="#ch01lvl1sec11" title="Computation in Hadoop 2" class="link">Computation in Hadoop 2</a></li><li>reference link / <a href="#ch01lvl1sec15" title="Getting started" class="link">Getting started</a></li><li>operations / <a href="#ch10lvl1sec74" title="Operations in the Hadoop 2 world" class="link">Operations in the Hadoop 2 world</a></li></ul></li>
        <li>Hadoop 2 NameNode HA<ul><li>about / <a href="#ch02lvl1sec21" title="Hadoop 2 NameNode HA" class="link">Hadoop 2 NameNode HA</a></li><li>enabling / <a href="#ch02lvl1sec21" title="Hadoop 2 NameNode HA" class="link">Hadoop 2 NameNode HA</a></li><li>keeping, in sync / <a href="#ch02lvl1sec21" title="Keeping the HA NameNodes in sync" class="link">Keeping the HA NameNodes in sync</a></li></ul></li>
        <li>Hadoop Distributed File System (HDFS) / <a href="#ch10lvl1sec81" title="NameNode and DataNode" class="link">NameNode and DataNode</a></li>
        <li>Hadoop distributions<ul><li>about / <a href="#ch01lvl1sec12" title="Distributions of Apache Hadoop" class="link">Distributions of Apache Hadoop</a></li><li>Hortonworks / <a href="#ch01lvl1sec12" title="Distributions of Apache Hadoop" class="link">Distributions of Apache Hadoop</a></li><li>Cloudera / <a href="#ch01lvl1sec12" title="Distributions of Apache Hadoop" class="link">Distributions of Apache Hadoop</a></li><li>MapR / <a href="#ch01lvl1sec12" title="Distributions of Apache Hadoop" class="link">Distributions of Apache Hadoop</a></li><li>reference link / <a href="#ch01lvl1sec12" title="Distributions of Apache Hadoop" class="link">Distributions of Apache Hadoop</a></li></ul></li>
        <li>Hadoop filesystems<ul><li>about / <a href="#ch02lvl1sec25" title="Hadoop filesystems" class="link">Hadoop filesystems</a></li><li>reference link / <a href="#ch02lvl1sec25" title="Hadoop filesystems" class="link">Hadoop filesystems</a></li><li>Hadoop interfaces / <a href="#ch02lvl1sec25" title="Hadoop interfaces" class="link">Hadoop interfaces</a></li></ul></li>
        <li>Hadoop interfaces<ul><li>about / <a href="#ch02lvl1sec25" title="Hadoop interfaces" class="link">Hadoop interfaces</a></li><li>Java FileSystem API / <a href="#ch02lvl1sec25" title="Java FileSystem API" class="link">Java FileSystem API</a></li><li>Libhdfs / <a href="#ch02lvl1sec25" title="Libhdfs" class="link">Libhdfs</a></li><li>Apache Thrift / <a href="#ch02lvl1sec25" title="Thrift " class="link">Thrift </a></li></ul></li>
        <li>Hadoop operations<ul><li>about / <a href="#ch10lvl1sec71" title="I'm a developer â€“ I don't care about operations!" class="link">I'm a developer â€“ I don't care about operations!</a></li></ul></li>
        <li>Hadoop security<ul><li>future / <a href="#ch10lvl1sec79" title="The future of Hadoop security" class="link">The future of Hadoop security</a></li></ul></li>
        <li>Hadoop security model<ul><li>evolution / <a href="#ch10lvl1sec79" title="Evolution of the Hadoop security model" class="link">Evolution of the Hadoop security model</a></li><li>additional security features / <a href="#ch10lvl1sec79" title="Beyond basic authorization" class="link">Beyond basic authorization</a></li></ul></li>
        <li>Hadoop streaming<ul><li>about / <a href="#ch09lvl1sec67" title="Hadoop streaming" class="link">Hadoop streaming</a></li><li>word count, streaming in Python / <a href="#ch09lvl1sec67" title="Streaming word count in Python" class="link">Streaming word count in Python</a></li><li>differences in jobs / <a href="#ch09lvl1sec67" title="Differences in jobs when using streaming" class="link">Differences in jobs when using streaming</a></li><li>importance of words, determining / <a href="#ch09lvl1sec67" title="Finding important words in text" class="link">Finding important words in text</a></li></ul></li>
        <li>Hadoop UI<ul><li>URL / <a href="#ch08lvl1sec64" title="Other tools to help" class="link">Other tools to help</a></li><li>about / <a href="#ch08lvl1sec64" title="Other tools to help" class="link">Other tools to help</a></li></ul></li>
        <li>Hadoop User Group (HUG) / <a href="#ch11lvl1sec88" title="HUGs" class="link">HUGs</a></li>
        <li>hashtagRegExp / <a href="#ch03lvl1sec31" title="Trending topics" class="link">Trending topics</a></li>
        <li>hashtags<ul><li>about / <a href="#ch03lvl1sec31" title="Sentiment of hashtags" class="link">Sentiment of hashtags</a></li></ul></li>
        <li>HBase<ul><li>about / <a href="#ch11lvl1sec85" title="HBase" class="link">HBase</a></li><li>URL / <a href="#ch11lvl1sec85" title="HBase" class="link">HBase</a></li></ul></li>
        <li>HCatalog<ul><li>about / <a href="#ch08lvl1sec61" title="Introducing HCatalog" class="link">Introducing HCatalog</a></li><li>using / <a href="#ch08lvl1sec61" title="Using HCatalog" class="link">Using HCatalog</a></li></ul></li>
        <li>HCat CLI tool<ul><li>about / <a href="#ch08lvl1sec61" title="Using HCatalog" class="link">Using HCatalog</a></li></ul></li>
        <li>hcat utility<ul><li>about / <a href="#ch08lvl1sec61" title="Using HCatalog" class="link">Using HCatalog</a></li></ul></li>
        <li>HDFS<ul><li>about / <a href="#ch01lvl1sec10" title="Components of Hadoop" class="link">Components of Hadoop</a>, <a href="#ch01lvl1sec10" title="Storage" class="link">Storage</a>, <a href="#ch04lvl1sec36" title="Samza and HDFS" class="link">Samza and HDFS</a></li><li>characteristics / <a href="#ch01lvl1sec10" title="Storage" class="link">Storage</a></li><li>architecture / <a href="#ch02lvl1sec19" title="The inner workings of HDFS" class="link">The inner workings of HDFS</a></li><li>NameNode / <a href="#ch02lvl1sec19" title="The inner workings of HDFS" class="link">The inner workings of HDFS</a></li><li>DataNodes / <a href="#ch02lvl1sec19" title="The inner workings of HDFS" class="link">The inner workings of HDFS</a></li><li>cluster startup / <a href="#ch02lvl1sec19" title="Cluster startup" class="link">Cluster startup</a></li><li>block replication / <a href="#ch02lvl1sec19" title="Block replication" class="link">Block replication</a></li></ul></li>
        <li>HDFS and MapReduce<ul><li>merits / <a href="#ch01lvl1sec10" title="Better together" class="link">Better together</a></li></ul></li>
        <li>HDFS filesystem<ul><li>command-line access / <a href="#ch02lvl1sec20" title="Command-line access to the HDFS filesystem" class="link">Command-line access to the HDFS filesystem</a></li><li>exploring / <a href="#ch02lvl1sec20" title="Exploring the HDFS filesystem" class="link">Exploring the HDFS filesystem</a></li></ul></li>
        <li>HDFS snapshots<ul><li>about / <a href="#ch02lvl1sec24" title="HDFS snapshots" class="link">HDFS snapshots</a></li></ul></li>
        <li>Hello Samza<ul><li>about / <a href="#ch04lvl1sec36" title="Hello Samza!" class="link">Hello Samza!</a></li><li>URL / <a href="#ch04lvl1sec36" title="Hello Samza!" class="link">Hello Samza!</a></li></ul></li>
        <li>high-availability (HA)<ul><li>about / <a href="#ch01lvl1sec11" title="Storage in Hadoop 2" class="link">Storage in Hadoop 2</a></li></ul></li>
        <li>High Performance Computing (HPC) / <a href="#ch01lvl1sec11" title="Computation in Hadoop 2" class="link">Computation in Hadoop 2</a></li>
        <li>Hive<ul><li>about / <a href="#ch03lvl1sec34" title="Hive-on-tez" class="link">Hive-on-tez</a></li><li>URL / <a href="#ch03lvl1sec34" title="Hive-on-tez" class="link">Hive-on-tez</a></li><li>overview / <a href="#ch07lvl1sec52" title="Overview of Hive" class="link">Overview of Hive</a></li><li>data types / <a href="#ch07lvl1sec53" title="Data types" class="link">Data types</a></li><li>DDL statements / <a href="#ch07lvl1sec53" title="DDL statements" class="link">DDL statements</a></li><li>file formats / <a href="#ch07lvl1sec53" title="File formats and storage" class="link">File formats and storage</a></li><li>storage / <a href="#ch07lvl1sec53" title="File formats and storage" class="link">File formats and storage</a></li><li>queries / <a href="#ch07lvl1sec53" title="Queries" class="link">Queries</a></li><li>scripts, writing / <a href="#ch07lvl1sec53" title="Writing scripts" class="link">Writing scripts</a></li><li>working, with Amazon Web Services / <a href="#ch07lvl1sec54" title="Hive and Amazon Web Services" class="link">Hive and Amazon Web Services</a></li><li>using, with S3 / <a href="#ch07lvl1sec54" title="Hive and S3" class="link">Hive and S3</a></li><li>using, with ElasticMapReduce / <a href="#ch07lvl1sec54" title="Hive on Elastic MapReduce" class="link">Hive on Elastic MapReduce</a></li><li>URL, for source code of JDBC client / <a href="#ch07lvl1sec56" title="JDBC" class="link">JDBC</a></li><li>URL, for source code of Thrift client / <a href="#ch07lvl1sec56" title="Thrift" class="link">Thrift</a></li></ul></li>
        <li>Hive-JSON-Serde<ul><li>URL / <a href="#ch07lvl1sec53" title="JSON" class="link">JSON</a></li></ul></li>
        <li>hive-json module<ul><li>URL / <a href="#ch07lvl1sec53" title="JSON" class="link">JSON</a></li><li>about / <a href="#ch07lvl1sec53" title="JSON" class="link">JSON</a></li></ul></li>
        <li>Hive-on-tez<ul><li>about / <a href="#ch03lvl1sec34" title="Hive-on-tez" class="link">Hive-on-tez</a></li></ul></li>
        <li>Hive 0.13<ul><li>about / <a href="#ch03lvl1sec34" title="Hive-on-tez" class="link">Hive-on-tez</a></li></ul></li>
        <li>Hive architecture<ul><li>about / <a href="#ch07lvl1sec53" title="Hive architecture" class="link">Hive architecture</a></li></ul></li>
        <li>HiveQL<ul><li>about / <a href="#ch07lvl1sec51" title="Why SQL on Hadoop" class="link">Why SQL on Hadoop</a>, <a href="#ch07lvl1sec53" title="Queries" class="link">Queries</a></li><li>extending / <a href="#ch07lvl1sec55" title="Extending HiveQL" class="link">Extending HiveQL</a></li></ul></li>
        <li>HiveServer2<ul><li>about / <a href="#ch07lvl1sec53" title="Hive architecture" class="link">Hive architecture</a></li><li>URL / <a href="#ch07lvl1sec53" title="Hive architecture" class="link">Hive architecture</a></li></ul></li>
        <li>Hive tables<ul><li>about / <a href="#ch07lvl1sec52" title="The nature of Hive tables" class="link">The nature of Hive tables</a></li><li>structuring, from workloads / <a href="#ch07lvl1sec53" title="Structuring Hive tables for given workloads" class="link">Structuring Hive tables for given workloads</a></li></ul></li>
        <li>Hortonwork's HDP<ul><li>URL / <a href="#ch05lvl1sec38" title="Spark on YARN" class="link">Spark on YARN</a></li></ul></li>
        <li>Hortonworks<ul><li>URL / <a href="#ch01lvl1sec12" title="Distributions of Apache Hadoop" class="link">Distributions of Apache Hadoop</a></li></ul></li>
        <li>Hortonworks Data Platform (HDP)<ul><li>about / <a href="#ch11lvl1sec83" title="Alternative distributions" class="link">Alternative distributions</a>, <a href="#ch11lvl1sec83" title="Hortonworks Data Platform" class="link">Hortonworks Data Platform</a></li><li>URL / <a href="#ch11lvl1sec83" title="Hortonworks Data Platform" class="link">Hortonworks Data Platform</a></li></ul></li>
        <li>Hue<ul><li>about / <a href="#ch11lvl1sec85" title="Hue" class="link">Hue</a></li><li>URL / <a href="#ch11lvl1sec85" title="Hue" class="link">Hue</a></li></ul></li>
        <li>HUGs<ul><li>about / <a href="#ch11lvl1sec88" title="HUGs" class="link">HUGs</a></li><li>reference link / <a href="#ch11lvl1sec88" title="HUGs" class="link">HUGs</a></li></ul></li>
      </ul>
      <h2>I</h2>
      <ul>
        <li>IAM console<ul><li>URL / <a href="#ch07lvl1sec54" title="Hive and S3" class="link">Hive and S3</a></li></ul></li>
        <li>IBM Infosphere Big Insights<ul><li>about / <a href="#ch11lvl1sec83" title="And the restâ€¦" class="link">And the restâ€¦</a></li></ul></li>
        <li>Identity and Access Management (IAM) / <a href="#ch01lvl1sec15" title="AWS credentials" class="link">AWS credentials</a></li>
        <li>Impala<ul><li>about / <a href="#ch07lvl1sec58" title="Impala" class="link">Impala</a></li><li>references / <a href="#ch07lvl1sec58" title="Impala" class="link">Impala</a>, <a href="#ch07lvl1sec58" title="Co-existing with Hive" class="link">Co-existing with Hive</a></li><li>architecture / <a href="#ch07lvl1sec58" title="The architecture of Impala" class="link">The architecture of Impala</a></li><li>co-existing, with Hive / <a href="#ch07lvl1sec58" title="Co-existing with Hive" class="link">Co-existing with Hive</a></li></ul></li>
        <li>in-sync replicas (ISR)<ul><li>about / <a href="#ch04lvl1sec36" title="Getting Twitter data into Kafka" class="link">Getting Twitter data into Kafka</a></li></ul></li>
        <li>indices attribute, entity<ul><li>about / <a href="#ch06lvl1sec49" title="Tweet metadata" class="link">Tweet metadata</a></li></ul></li>
        <li>input/output, MapReduce job<ul><li>about / <a href="#ch03lvl1sec32" title="Input/Output" class="link">Input/Output</a></li></ul></li>
        <li>InputFormat, MapReduce job<ul><li>about / <a href="#ch03lvl1sec32" title="InputFormat and RecordReader" class="link">InputFormat and RecordReader</a></li></ul></li>
      </ul>
      <h2>J</h2>
      <ul>
        <li>Java<ul><li>WordCount / <a href="#ch05lvl1sec38" title="WordCount in Java" class="link">WordCount in Java</a></li></ul></li>
        <li>Java API<ul><li>about / <a href="#ch05lvl1sec38" title="Java API" class="link">Java API</a></li><li>and Scala API, differences / <a href="#ch05lvl1sec38" title="Java API" class="link">Java API</a></li></ul></li>
        <li>Java API to MapReduce<ul><li>about / <a href="#ch03lvl1sec30" title="Java API to MapReduce" class="link">Java API to MapReduce</a></li><li>Mapper class / <a href="#ch03lvl1sec30" title="The Mapper class" class="link">The Mapper class</a></li><li>Reducer class / <a href="#ch03lvl1sec30" title="The Reducer class" class="link">The Reducer class</a></li><li>Driver class / <a href="#ch03lvl1sec30" title="The Driver class" class="link">The Driver class</a></li><li>combiner class / <a href="#ch03lvl1sec30" title="Combiner" class="link">Combiner</a></li><li>partitioning / <a href="#ch03lvl1sec30" title="Partitioning" class="link">Partitioning</a></li><li>Hadoop-provided Mapper and Reducer implementations / <a href="#ch03lvl1sec30" title="Hadoop-provided mapper and reducer implementations" class="link">Hadoop-provided mapper and reducer implementations</a></li><li>reference data, sharing / <a href="#ch03lvl1sec30" title="Sharing reference data" class="link">Sharing reference data</a></li></ul></li>
        <li>Java FileSystem API<ul><li>about / <a href="#ch02lvl1sec25" title="Java FileSystem API" class="link">Java FileSystem API</a></li></ul></li>
        <li>JDBC<ul><li>about / <a href="#ch07lvl1sec56" title="JDBC" class="link">JDBC</a></li></ul></li>
        <li>JobTracker monitoring, MapReduce job<ul><li>about / <a href="#ch03lvl1sec32" title="Ongoing JobTracker monitoring" class="link">Ongoing JobTracker monitoring</a></li></ul></li>
        <li>join node<ul><li>about / <a href="#ch08lvl1sec61" title="Performing multiple actions in parallel" class="link">Performing multiple actions in parallel</a></li></ul></li>
        <li>JOIN operator<ul><li>about / <a href="#ch06lvl1sec47" title="Join" class="link">Join</a></li></ul> / <a href="#ch07lvl1sec53" title="Queries" class="link">Queries</a></li>
        <li>JSON<ul><li>about / <a href="#ch07lvl1sec53" title="JSON" class="link">JSON</a></li></ul></li>
        <li>JSON Simple<ul><li>URL / <a href="#ch04lvl1sec36" title="Building a tweet parsing job" class="link">Building a tweet parsing job</a></li></ul></li>
        <li>JVM considerations, cluster tuning<ul><li>about / <a href="#ch10lvl1sec78" title="JVM considerations" class="link">JVM considerations</a></li><li>small files problem / <a href="#ch10lvl1sec78" title="The small files problem" class="link">The small files problem</a></li></ul></li>
      </ul>
      <h2>K</h2>
      <ul>
        <li>kite-morphlines-avro command / <a href="#ch09lvl1sec69" title="Morphline commands" class="link">Morphline commands</a></li>
        <li>kite-morphlines-core-stdio command / <a href="#ch09lvl1sec69" title="Morphline commands" class="link">Morphline commands</a></li>
        <li>kite-morphlines-core-stdlib command / <a href="#ch09lvl1sec69" title="Morphline commands" class="link">Morphline commands</a></li>
        <li>kite-morphlines-hadoop-core command / <a href="#ch09lvl1sec69" title="Morphline commands" class="link">Morphline commands</a></li>
        <li>kite-morphlines-hadoop-parquet-avro command / <a href="#ch09lvl1sec69" title="Morphline commands" class="link">Morphline commands</a></li>
        <li>kite-morphlines-hadoop-rcfile command / <a href="#ch09lvl1sec69" title="Morphline commands" class="link">Morphline commands</a></li>
        <li>kite-morphlines-hadoop-sequencefile command / <a href="#ch09lvl1sec69" title="Morphline commands" class="link">Morphline commands</a></li>
        <li>kite-morphlines-json command / <a href="#ch09lvl1sec69" title="Morphline commands" class="link">Morphline commands</a></li>
        <li>Kite Data<ul><li>about / <a href="#ch09lvl1sec68" title="Kite Data" class="link">Kite Data</a></li><li> Data core / <a href="#ch09lvl1sec68" title="Data Core" class="link">Data Core</a></li><li>Data core / <a href="#ch09lvl1sec68" title="Data Core" class="link">Data Core</a></li><li>Data HCatalog / <a href="#ch09lvl1sec68" title="Data HCatalog" class="link">Data HCatalog</a></li><li>Data Hive / <a href="#ch09lvl1sec68" title="Data Hive" class="link">Data Hive</a></li><li>Data MapReduce / <a href="#ch09lvl1sec68" title="Data MapReduce" class="link">Data MapReduce</a></li><li>Data Spark / <a href="#ch09lvl1sec68" title="Data Spark" class="link">Data Spark</a></li><li>Data Crunch / <a href="#ch09lvl1sec68" title="Data Crunch" class="link">Data Crunch</a></li></ul></li>
        <li>Kite examples<ul><li>reference link / <a href="#ch09lvl1sec68" title="Kite Data" class="link">Kite Data</a></li></ul></li>
        <li>Kite JARs<ul><li>reference link / <a href="#ch09lvl1sec68" title="Kite Data" class="link">Kite Data</a></li></ul></li>
        <li>Kite Morphlines<ul><li>about / <a href="#ch09lvl1sec69" title="Kite Morphlines" class="link">Kite Morphlines</a></li><li>concepts / <a href="#ch09lvl1sec69" title="Concepts" class="link">Concepts</a></li><li>Record abstractions / <a href="#ch09lvl1sec69" title="Concepts" class="link">Concepts</a></li><li>commands / <a href="#ch09lvl1sec69" title="Morphline commands" class="link">Morphline commands</a></li></ul></li>
        <li>Kite SDK<ul><li>URL / <a href="#ch09lvl1sec68" title="Kite Data" class="link">Kite Data</a></li></ul></li>
        <li>KVM<ul><li>reference link / <a href="#ch01lvl1sec15" title="Cloudera QuickStart VM" class="link">Cloudera QuickStart VM</a></li></ul></li>
      </ul>
      <h2>L</h2>
      <ul>
        <li>Lambda syntax<ul><li>URL / <a href="#ch05lvl1sec38" title="Python API" class="link">Python API</a></li></ul></li>
        <li>Libhdfs<ul><li>about / <a href="#ch02lvl1sec25" title="Libhdfs" class="link">Libhdfs</a></li></ul></li>
        <li>LinkedIn groups<ul><li>about / <a href="#ch11lvl1sec88" title="LinkedIn groups" class="link">LinkedIn groups</a></li><li>URL / <a href="#ch11lvl1sec88" title="LinkedIn groups" class="link">LinkedIn groups</a></li></ul></li>
        <li>Log4j<ul><li>about / <a href="#ch10lvl1sec81" title="Logging levels" class="link">Logging levels</a></li></ul></li>
        <li>logfiles<ul><li>accessing to / <a href="#ch10lvl1sec81" title="Access to logfiles" class="link">Access to logfiles</a></li></ul></li>
        <li>logging levels<ul><li>about / <a href="#ch10lvl1sec81" title="Logging levels" class="link">Logging levels</a></li></ul></li>
      </ul>
      <h2>M</h2>
      <ul>
        <li>Machine Learning (ML)<ul><li>about / <a href="#ch05lvl1sec39" title="MLlib" class="link">MLlib</a></li></ul></li>
        <li>macros<ul><li>about / <a href="#ch06lvl1sec47" title="Macros" class="link">Macros</a></li></ul></li>
        <li>Mahout<ul><li>about / <a href="#ch11lvl1sec85" title="Mahout" class="link">Mahout</a></li><li>URL / <a href="#ch11lvl1sec85" title="Mahout" class="link">Mahout</a></li></ul></li>
        <li>map optimization, cluster tuning<ul><li>considerations / <a href="#ch10lvl1sec78" title="Map and reduce optimizations" class="link">Map and reduce optimizations</a></li></ul></li>
        <li>Mapper class, Java API to MapReduce<ul><li>about / <a href="#ch03lvl1sec30" title="The Mapper class" class="link">The Mapper class</a></li></ul></li>
        <li>mapper execution, MapReduce job<ul><li>about / <a href="#ch03lvl1sec32" title="Mapper execution" class="link">Mapper execution</a></li></ul></li>
        <li>mapper input, MapReduce job<ul><li>about / <a href="#ch03lvl1sec32" title="Mapper input" class="link">Mapper input</a></li></ul></li>
        <li>mapper output, MapReduce job<ul><li>about / <a href="#ch03lvl1sec32" title="Mapper output and reducer input" class="link">Mapper output and reducer input</a></li></ul></li>
        <li>mappers, Mapper and Reducer implementations<ul><li>InverseMapper / <a href="#ch03lvl1sec30" title="Hadoop-provided mapper and reducer implementations" class="link">Hadoop-provided mapper and reducer implementations</a></li><li>TokenCounterMapper / <a href="#ch03lvl1sec30" title="Hadoop-provided mapper and reducer implementations" class="link">Hadoop-provided mapper and reducer implementations</a></li><li>IdentityMapper / <a href="#ch03lvl1sec30" title="Hadoop-provided mapper and reducer implementations" class="link">Hadoop-provided mapper and reducer implementations</a></li></ul></li>
        <li>MapR<ul><li>URL / <a href="#ch01lvl1sec12" title="Distributions of Apache Hadoop" class="link">Distributions of Apache Hadoop</a>, <a href="#ch11lvl1sec83" title="MapR" class="link">MapR</a></li><li>about / <a href="#ch11lvl1sec83" title="MapR" class="link">MapR</a></li></ul></li>
        <li>MapReduce<ul><li>reference link / <a href="#ch01lvl1sec09" title="The background of Hadoop" class="link">The background of Hadoop</a>, <a href="#ch03lvl1sec29" title="MapReduce" class="link">MapReduce</a></li><li>about / <a href="#ch03lvl1sec29" title="MapReduce" class="link">MapReduce</a></li><li>Map phase / <a href="#ch03lvl1sec29" title="MapReduce" class="link">MapReduce</a></li></ul></li>
        <li>MapReduce API<ul><li>about / <a href="#ch01lvl1sec10" title="Components of Hadoop" class="link">Components of Hadoop</a>, <a href="#ch01lvl1sec10" title="Computation" class="link">Computation</a></li></ul></li>
        <li>MapReduce driver source code<ul><li>reference link / <a href="#ch09lvl1sec69" title="Morphline commands" class="link">Morphline commands</a></li></ul></li>
        <li>MapReduce job<ul><li>about / <a href="#ch03lvl1sec32" title="Walking through a run of a MapReduce job" class="link">Walking through a run of a MapReduce job</a></li><li>startup / <a href="#ch03lvl1sec32" title="Startup" class="link">Startup</a></li><li>input, splitting / <a href="#ch03lvl1sec32" title="Splitting the input" class="link">Splitting the input</a></li><li>task assignment / <a href="#ch03lvl1sec32" title="Task assignment" class="link">Task assignment</a></li><li>task startup / <a href="#ch03lvl1sec32" title="Task startup" class="link">Task startup</a></li><li>JobTracker monitoring / <a href="#ch03lvl1sec32" title="Ongoing JobTracker monitoring" class="link">Ongoing JobTracker monitoring</a></li><li>mapper input / <a href="#ch03lvl1sec32" title="Mapper input" class="link">Mapper input</a></li><li>mapper execution / <a href="#ch03lvl1sec32" title="Mapper execution" class="link">Mapper execution</a></li><li>mapper output / <a href="#ch03lvl1sec32" title="Mapper output and reducer input" class="link">Mapper output and reducer input</a></li><li>reducer input / <a href="#ch03lvl1sec32" title="Reducer input" class="link">Reducer input</a></li><li>reducer execution / <a href="#ch03lvl1sec32" title="Reducer execution" class="link">Reducer execution</a></li><li>reducer output / <a href="#ch03lvl1sec32" title="Reducer output" class="link">Reducer output</a></li><li>shutdown / <a href="#ch03lvl1sec32" title="Shutdown" class="link">Shutdown</a></li><li>input/output / <a href="#ch03lvl1sec32" title="Input/Output" class="link">Input/Output</a></li><li>InputFormat / <a href="#ch03lvl1sec32" title="InputFormat and RecordReader" class="link">InputFormat and RecordReader</a></li><li>RecordReader / <a href="#ch03lvl1sec32" title="InputFormat and RecordReader" class="link">InputFormat and RecordReader</a></li><li>Hadoop-provided InputFormat / <a href="#ch03lvl1sec32" title="Hadoop-provided InputFormat" class="link">Hadoop-provided InputFormat</a></li><li>Hadoop-provided RecordReader / <a href="#ch03lvl1sec32" title="Hadoop-provided RecordReader" class="link">Hadoop-provided RecordReader</a></li><li>OutputFormat / <a href="#ch03lvl1sec32" title="OutputFormat and RecordWriter" class="link">OutputFormat and RecordWriter</a></li><li>RecordWriter / <a href="#ch03lvl1sec32" title="OutputFormat and RecordWriter" class="link">OutputFormat and RecordWriter</a></li><li>Hadoop-provided OutputFormat / <a href="#ch03lvl1sec32" title="Hadoop-provided OutputFormat" class="link">Hadoop-provided OutputFormat</a></li><li>sequence files / <a href="#ch03lvl1sec32" title="Sequence files" class="link">Sequence files</a></li></ul></li>
        <li>MapReduce programs<ul><li>writing / <a href="#ch03lvl1sec31" title="Writing MapReduce programs" class="link">Writing MapReduce programs</a>, <a href="#ch03lvl1sec31" title="Getting started" class="link">Getting started</a></li><li>examples, running / <a href="#ch03lvl1sec31" title="Running the examples" class="link">Running the examples</a></li><li>WordCount example / <a href="#ch03lvl1sec31" title="WordCount, the Hello World of MapReduce" class="link">WordCount, the Hello World of MapReduce</a></li><li>word co-occurrences / <a href="#ch03lvl1sec31" title="Word co-occurrences" class="link">Word co-occurrences</a></li><li>social network topics / <a href="#ch03lvl1sec31" title="Trending topics" class="link">Trending topics</a></li><li>reference link, for HashTagCount example source code / <a href="#ch03lvl1sec31" title="Trending topics" class="link">Trending topics</a></li><li>Top N pattern / <a href="#ch03lvl1sec31" title="The Top N pattern" class="link">The Top N pattern</a></li><li>reference link, for TopTenHashTag source code / <a href="#ch03lvl1sec31" title="The Top N pattern" class="link">The Top N pattern</a></li><li>hashtags / <a href="#ch03lvl1sec31" title="Sentiment of hashtags" class="link">Sentiment of hashtags</a></li><li>reference link, for HashTagSentiment source code / <a href="#ch03lvl1sec31" title="Sentiment of hashtags" class="link">Sentiment of hashtags</a></li><li>text cleanup, chain mapper used / <a href="#ch03lvl1sec31" title="Text cleanup using chain mapper" class="link">Text cleanup using chain mapper</a></li><li>reference link, for HashTagSentimentChain source code / <a href="#ch03lvl1sec31" title="Text cleanup using chain mapper" class="link">Text cleanup using chain mapper</a></li></ul></li>
        <li>Massively Parallel Processing (MPP)<ul><li>about / <a href="#ch07lvl1sec58" title="The architecture of Impala" class="link">The architecture of Impala</a></li></ul></li>
        <li>MemPipeline<ul><li>about / <a href="#ch09lvl1sec69" title="MemPipeline" class="link">MemPipeline</a></li></ul></li>
        <li>Message Passing Interface (MPI) / <a href="#ch01lvl1sec11" title="Computation in Hadoop 2" class="link">Computation in Hadoop 2</a></li>
        <li>MLLib<ul><li>about / <a href="#ch05lvl1sec39" title="MLlib" class="link">MLlib</a></li></ul></li>
        <li>monitoring<ul><li>about / <a href="#ch10lvl1sec80" title="Monitoring" class="link">Monitoring</a></li><li>Hadoop / <a href="#ch10lvl1sec80" title="Hadoop â€“ where failures don't matter" class="link">Hadoop â€“ where failures don't matter</a></li><li>application-level metrics / <a href="#ch10lvl1sec80" title="Application-level metrics" class="link">Application-level metrics</a></li></ul></li>
        <li>monitoring tools<ul><li>about / <a href="#ch10lvl1sec80" title="Monitoring integration" class="link">Monitoring integration</a></li></ul></li>
        <li>MoprhlineDrvier source code<ul><li>reference link / <a href="#ch09lvl1sec69" title="Morphline commands" class="link">Morphline commands</a></li></ul></li>
        <li>Morphline commands<ul><li>kite-morphlines-core-stdio / <a href="#ch09lvl1sec69" title="Morphline commands" class="link">Morphline commands</a></li><li>kite-morphlines-core-stdlib / <a href="#ch09lvl1sec69" title="Morphline commands" class="link">Morphline commands</a></li><li>kite-morphlines-avro / <a href="#ch09lvl1sec69" title="Morphline commands" class="link">Morphline commands</a></li><li>kite-morphlines-json / <a href="#ch09lvl1sec69" title="Morphline commands" class="link">Morphline commands</a></li><li>kite-morphlines-hadoop-parquet-avro / <a href="#ch09lvl1sec69" title="Morphline commands" class="link">Morphline commands</a></li><li>kite-morphlines-hadoop-sequencefile / <a href="#ch09lvl1sec69" title="Morphline commands" class="link">Morphline commands</a></li><li>kite-morphlines-hadoop-rcfile / <a href="#ch09lvl1sec69" title="Morphline commands" class="link">Morphline commands</a></li><li>reference link / <a href="#ch09lvl1sec69" title="Morphline commands" class="link">Morphline commands</a></li></ul></li>
        <li>MRExecutionEngine / <a href="#ch06lvl1sec43" title="An overview of Pig" class="link">An overview of Pig</a></li>
        <li>Multipart Upload<ul><li>URL / <a href="#ch10lvl1sec77" title="Getting data into EMR" class="link">Getting data into EMR</a></li></ul></li>
      </ul>
      <h2>N</h2>
      <ul>
        <li>NameNode<ul><li>about / <a href="#ch01lvl1sec11" title="Storage in Hadoop 2" class="link">Storage in Hadoop 2</a></li></ul> / <a href="#ch10lvl1sec81" title="NameNode and DataNode" class="link">NameNode and DataNode</a></li>
        <li>NameNode HA<ul><li>about / <a href="#ch01lvl1sec11" title="Storage in Hadoop 2" class="link">Storage in Hadoop 2</a></li></ul></li>
        <li>NameNode startup<ul><li>about / <a href="#ch02lvl1sec19" title="NameNode startup" class="link">NameNode startup</a></li></ul></li>
        <li>NFS share / <a href="#ch02lvl1sec21" title="Keeping the HA NameNodes in sync" class="link">Keeping the HA NameNodes in sync</a></li>
        <li>NodeManager<ul><li>about / <a href="#ch10lvl1sec81" title="ResourceManager, NodeManager, and Application Manager" class="link">ResourceManager, NodeManager, and Application Manager</a></li></ul></li>
        <li>NodeManager (NM)<ul><li>about / <a href="#ch03lvl1sec33" title="The components of YARN" class="link">The components of YARN</a></li></ul></li>
      </ul>
      <h2>O</h2>
      <ul>
        <li>Oozie<ul><li>about / <a href="#ch08lvl1sec61" title="Introducing Oozie" class="link">Introducing Oozie</a></li><li>URL / <a href="#ch08lvl1sec61" title="Introducing Oozie" class="link">Introducing Oozie</a></li><li>features / <a href="#ch08lvl1sec61" title="Introducing Oozie" class="link">Introducing Oozie</a></li><li>action nodes / <a href="#ch08lvl1sec61" title="Introducing Oozie" class="link">Introducing Oozie</a></li><li>HDFS file permissions / <a href="#ch08lvl1sec61" title="A note on HDFS file permissions" class="link">A note on HDFS file permissions</a></li><li>development, making easier / <a href="#ch08lvl1sec61" title="Making development a little easier" class="link">Making development a little easier</a></li><li>data, extracting / <a href="#ch08lvl1sec61" title="Extracting data and ingesting into Hive" class="link">Extracting data and ingesting into Hive</a></li><li>data, ingesting into Hive / <a href="#ch08lvl1sec61" title="Extracting data and ingesting into Hive" class="link">Extracting data and ingesting into Hive</a></li><li>workflow directory structure / <a href="#ch08lvl1sec61" title="A note on workflow directory structure" class="link">A note on workflow directory structure</a></li><li>HCatalog / <a href="#ch08lvl1sec61" title="Introducing HCatalog" class="link">Introducing HCatalog</a></li><li>sharelib / <a href="#ch08lvl1sec61" title="The Oozie sharelib" class="link">The Oozie sharelib</a></li><li>HCatalog and partitioned tables / <a href="#ch08lvl1sec61" title="HCatalog and partitioned tables" class="link">HCatalog and partitioned tables</a></li><li>using / <a href="#ch08lvl1sec64" title="Pulling it all together" class="link">Pulling it all together</a></li></ul></li>
        <li>Oozie triggers / <a href="#ch08lvl1sec63" title="Other Oozie triggers" class="link">Other Oozie triggers</a></li>
        <li>Oozie workflow<ul><li>about / <a href="#ch08lvl1sec61" title="Introducing Oozie" class="link">Introducing Oozie</a></li></ul> / <a href="#ch08lvl1sec61" title="Introducing Oozie" class="link">Introducing Oozie</a></li>
        <li>operations, Hadoop 2<ul><li>about / <a href="#ch10lvl1sec74" title="Operations in the Hadoop 2 world" class="link">Operations in the Hadoop 2 world</a></li></ul></li>
        <li>opinion lexicon<ul><li>URL / <a href="#ch03lvl1sec31" title="Sentiment of hashtags" class="link">Sentiment of hashtags</a></li></ul></li>
        <li>Optimized Row Columnar file format (ORC)<ul><li>about / <a href="#ch02lvl1sec27" title="ORC" class="link">ORC</a></li><li>reference link / <a href="#ch02lvl1sec27" title="ORC" class="link">ORC</a></li></ul></li>
        <li>ORC<ul><li>URL / <a href="#ch07lvl1sec53" title="Columnar stores" class="link">Columnar stores</a></li></ul></li>
        <li>org.apache.zookeeper.ZooKeeper class<ul><li>about / <a href="#ch02lvl1sec22" title="Java API" class="link">Java API</a></li></ul></li>
        <li>OutputFormat, MapReduce job<ul><li>about / <a href="#ch03lvl1sec32" title="OutputFormat and RecordWriter" class="link">OutputFormat and RecordWriter</a></li></ul></li>
      </ul>
      <h2>P</h2>
      <ul>
        <li>parallelDo operation<ul><li>about / <a href="#ch09lvl1sec69" title="Concepts" class="link">Concepts</a></li></ul></li>
        <li>PARALLEL operator<ul><li>about / <a href="#ch06lvl1sec47" title="Aggregation" class="link">Aggregation</a></li></ul></li>
        <li>Parquet<ul><li>reference link / <a href="#ch02lvl1sec27" title="Parquet" class="link">Parquet</a></li><li>about / <a href="#ch02lvl1sec27" title="Parquet" class="link">Parquet</a></li><li>URL / <a href="#ch07lvl1sec53" title="Columnar stores" class="link">Columnar stores</a></li></ul></li>
        <li>partitioning, Java API to MapReduce<ul><li>about / <a href="#ch03lvl1sec30" title="Partitioning" class="link">Partitioning</a></li><li>optional partition function / <a href="#ch03lvl1sec30" title="The optional partition function" class="link">The optional partition function</a></li></ul></li>
        <li>PCollection&lt;T&gt; interface, Crunch<ul><li>about / <a href="#ch09lvl1sec69" title="Concepts" class="link">Concepts</a></li></ul></li>
        <li>physical cluster<ul><li>building / <a href="#ch10lvl1sec76" title="Building a physical cluster" class="link">Building a physical cluster</a></li></ul></li>
        <li>physical cluster, considerations<ul><li>about / <a href="#ch10lvl1sec76" title="Physical layout" class="link">Physical layout</a></li><li>rack awareness / <a href="#ch10lvl1sec76" title="Rack awareness" class="link">Rack awareness</a></li><li>service layout / <a href="#ch10lvl1sec76" title="Service layout" class="link">Service layout</a></li><li>service, upgrading / <a href="#ch10lvl1sec76" title="Upgrading a service" class="link">Upgrading a service</a></li></ul></li>
        <li>Pig<ul><li>overview / <a href="#ch06lvl1sec43" title="An overview of Pig" class="link">An overview of Pig</a></li><li>use cases / <a href="#ch06lvl1sec43" title="An overview of Pig" class="link">An overview of Pig</a></li><li>about / <a href="#ch06lvl1sec44" title="Getting started" class="link">Getting started</a>, <a href="#ch07lvl1sec51" title="Why SQL on Hadoop" class="link">Why SQL on Hadoop</a></li><li>running / <a href="#ch06lvl1sec45" title="Running Pig" class="link">Running Pig</a></li><li>reference link, for source code and binary distributions / <a href="#ch06lvl1sec45" title="Running Pig" class="link">Running Pig</a></li><li>Grunt / <a href="#ch06lvl1sec45" title="Grunt â€“ the Pig interactive shell" class="link">Grunt â€“ the Pig interactive shell</a></li><li>Elastic MapReduce / <a href="#ch06lvl1sec45" title="Elastic MapReduce" class="link">Elastic MapReduce</a></li><li>fundamentals / <a href="#ch06lvl1sec46" title="Fundamentals of Apache Pig" class="link">Fundamentals of Apache Pig</a></li><li>reference link, for parallel feature / <a href="#ch06lvl1sec46" title="Fundamentals of Apache Pig" class="link">Fundamentals of Apache Pig</a></li><li>reference link, for multi-query implementation / <a href="#ch06lvl1sec46" title="Fundamentals of Apache Pig" class="link">Fundamentals of Apache Pig</a></li><li>programming / <a href="#ch06lvl1sec47" title="Programming Pig" class="link">Programming Pig</a></li><li>data types / <a href="#ch06lvl1sec47" title="Pig data types" class="link">Pig data types</a></li><li>functions / <a href="#ch06lvl1sec47" title="Pig functions" class="link">Pig functions</a></li><li>data, working with / <a href="#ch06lvl1sec47" title="Working with data" class="link">Working with data</a></li></ul></li>
        <li>Piggybank<ul><li>about / <a href="#ch06lvl1sec48" title="Piggybank" class="link">Piggybank</a></li></ul></li>
        <li>Pig Latin / <a href="#ch06lvl1sec43" title="An overview of Pig" class="link">An overview of Pig</a></li>
        <li>Pig UDFs<ul><li>extending / <a href="#ch06lvl1sec48" title="Extending Pig (UDFs)" class="link">Extending Pig (UDFs)</a></li><li>contributed UDFs / <a href="#ch06lvl1sec48" title="Contributed UDFs" class="link">Contributed UDFs</a></li></ul></li>
        <li>pipelines implementation, Apache Crunch<ul><li>about / <a href="#ch09lvl1sec69" title="Pipelines implementation and execution" class="link">Pipelines implementation and execution</a></li><li>SparkPipeline / <a href="#ch09lvl1sec69" title="SparkPipeline" class="link">SparkPipeline</a></li><li>MemPipeline / <a href="#ch09lvl1sec69" title="MemPipeline" class="link">MemPipeline</a></li></ul></li>
        <li>positive_words operator<ul><li>about / <a href="#ch06lvl1sec47" title="Join" class="link">Join</a></li></ul></li>
        <li>pre-requisites<ul><li>about / <a href="#ch07lvl1sec52" title="Prerequisites" class="link">Prerequisites</a></li></ul></li>
        <li>Predictive Model Markup Language (PMML) / <a href="#ch11lvl1sec86" title="Cascading" class="link">Cascading</a></li>
        <li>processing models, YARN<ul><li>Cloudera Kitten / <a href="#ch03lvl1sec33" title="Thinking in layers" class="link">Thinking in layers</a></li><li>Apache Twill / <a href="#ch03lvl1sec33" title="Thinking in layers" class="link">Thinking in layers</a></li></ul></li>
        <li>programmatic interfaces<ul><li>about / <a href="#ch07lvl1sec56" title="Programmatic interfaces" class="link">Programmatic interfaces</a></li><li>JDBC / <a href="#ch07lvl1sec56" title="JDBC" class="link">JDBC</a></li><li>Thrift / <a href="#ch07lvl1sec56" title="Thrift" class="link">Thrift</a></li></ul></li>
        <li>Project Rhino<ul><li>URL / <a href="#ch10lvl1sec79" title="The future of Hadoop security" class="link">The future of Hadoop security</a></li></ul></li>
        <li>PTable&lt;Key, Value&gt; interface, Crunch<ul><li>about / <a href="#ch09lvl1sec69" title="Concepts" class="link">Concepts</a></li></ul></li>
        <li>Python<ul><li>used, for programmatic access / <a href="#ch01lvl1sec17" title="Programmatic access with Python" class="link">Programmatic access with Python</a></li></ul></li>
        <li>Python API<ul><li>about / <a href="#ch05lvl1sec38" title="Python API" class="link">Python API</a></li></ul></li>
      </ul>
      <h2>Q</h2>
      <ul>
        <li>QJM mechanism<ul><li>about / <a href="#ch02lvl1sec21" title="Keeping the HA NameNodes in sync" class="link">Keeping the HA NameNodes in sync</a></li></ul></li>
        <li>queries, Hive / <a href="#ch07lvl1sec53" title="Queries" class="link">Queries</a></li>
      </ul>
      <h2>R</h2>
      <ul>
        <li>RDDs<ul><li>about / <a href="#ch05lvl1sec38" title="Cluster computing with working sets" class="link">Cluster computing with working sets</a>, <a href="#ch05lvl1sec38" title="Resilient Distributed Datasets (RDDs)" class="link">Resilient Distributed Datasets (RDDs)</a></li></ul></li>
        <li>RDDs, operations<ul><li>map / <a href="#ch05lvl1sec38" title="Actions" class="link">Actions</a></li><li>filter / <a href="#ch05lvl1sec38" title="Actions" class="link">Actions</a></li><li>reduce / <a href="#ch05lvl1sec38" title="Actions" class="link">Actions</a></li><li>collect / <a href="#ch05lvl1sec38" title="Actions" class="link">Actions</a></li><li>for each / <a href="#ch05lvl1sec38" title="Actions" class="link">Actions</a></li><li>groupByKey / <a href="#ch05lvl1sec38" title="Actions" class="link">Actions</a></li><li>sortByKey / <a href="#ch05lvl1sec38" title="Actions" class="link">Actions</a></li></ul></li>
        <li>Record abstractions<ul><li>implementing / <a href="#ch09lvl1sec69" title="Concepts" class="link">Concepts</a></li></ul></li>
        <li>RecordReader, MapReduce job<ul><li>about / <a href="#ch03lvl1sec32" title="InputFormat and RecordReader" class="link">InputFormat and RecordReader</a></li></ul></li>
        <li>RecordWriter, MapReduce job<ul><li>about / <a href="#ch03lvl1sec32" title="OutputFormat and RecordWriter" class="link">OutputFormat and RecordWriter</a></li></ul></li>
        <li>Reduce function<ul><li>about / <a href="#ch03lvl1sec29" title="MapReduce" class="link">MapReduce</a></li></ul></li>
        <li>reduce optimization, cluster tuning<ul><li>considerations / <a href="#ch10lvl1sec78" title="Map and reduce optimizations" class="link">Map and reduce optimizations</a></li></ul></li>
        <li>Reducer class, Java API to MapReduce<ul><li>about / <a href="#ch03lvl1sec30" title="The Reducer class" class="link">The Reducer class</a></li></ul></li>
        <li>reducer execution, MapReduce job<ul><li>about / <a href="#ch03lvl1sec32" title="Reducer execution" class="link">Reducer execution</a></li></ul></li>
        <li>reducer input, MapReduce job<ul><li>about / <a href="#ch03lvl1sec32" title="Reducer input" class="link">Reducer input</a></li></ul></li>
        <li>reducer output, MapReduce job<ul><li>about / <a href="#ch03lvl1sec32" title="Reducer output" class="link">Reducer output</a></li></ul></li>
        <li>reducers, Mapper and Reducer implementations<ul><li>IntSumReducer / <a href="#ch03lvl1sec30" title="Hadoop-provided mapper and reducer implementations" class="link">Hadoop-provided mapper and reducer implementations</a></li><li>LongSumReducer / <a href="#ch03lvl1sec30" title="Hadoop-provided mapper and reducer implementations" class="link">Hadoop-provided mapper and reducer implementations</a></li><li>IdentityReducer / <a href="#ch03lvl1sec30" title="Hadoop-provided mapper and reducer implementations" class="link">Hadoop-provided mapper and reducer implementations</a></li></ul></li>
        <li>reference data, Java API to MapReduce<ul><li>sharing / <a href="#ch03lvl1sec30" title="Sharing reference data" class="link">Sharing reference data</a></li></ul></li>
        <li>REGISTER operator<ul><li>about / <a href="#ch06lvl1sec48" title="Extending Pig (UDFs)" class="link">Extending Pig (UDFs)</a></li></ul></li>
        <li>required services, AWS<ul><li>Simple Storage Service (S3) / <a href="#ch01lvl1sec15" title="Signing up for the necessary services" class="link">Signing up for the necessary services</a></li><li>Elastic MapReduce / <a href="#ch01lvl1sec15" title="Signing up for the necessary services" class="link">Signing up for the necessary services</a></li><li>Elastic Compute Cloud (EC2) / <a href="#ch01lvl1sec15" title="Signing up for the necessary services" class="link">Signing up for the necessary services</a></li></ul></li>
        <li>ResourceManager<ul><li>about / <a href="#ch10lvl1sec81" title="ResourceManager, NodeManager, and Application Manager" class="link">ResourceManager, NodeManager, and Application Manager</a></li><li>applications / <a href="#ch10lvl1sec81" title="Applications" class="link">Applications</a></li><li>Nodes view / <a href="#ch10lvl1sec81" title="Nodes" class="link">Nodes</a></li><li>Scheduler window / <a href="#ch10lvl1sec81" title="Scheduler" class="link">Scheduler</a></li><li>MapReduce / <a href="#ch10lvl1sec81" title="MapReduce" class="link">MapReduce</a></li><li>MapReduce v1 / <a href="#ch10lvl1sec81" title="MapReduce v1" class="link">MapReduce v1</a></li><li>MapReduce v2 (YARN) / <a href="#ch10lvl1sec81" title="MapReduce v2 (YARN)" class="link">MapReduce v2 (YARN)</a></li><li>JobHistory Server / <a href="#ch10lvl1sec81" title="JobHistory Server" class="link">JobHistory Server</a></li></ul></li>
        <li>resources<ul><li>sharing / <a href="#ch10lvl1sec75" title="Sharing resources" class="link">Sharing resources</a></li></ul></li>
        <li>Role Based Access Control (RBAC) / <a href="#ch10lvl1sec79" title="Beyond basic authorization" class="link">Beyond basic authorization</a></li>
        <li>Row Columnar File (RCFile)<ul><li>about / <a href="#ch02lvl1sec27" title="RCFile" class="link">RCFile</a></li><li>reference link / <a href="#ch02lvl1sec27" title="RCFile" class="link">RCFile</a></li></ul></li>
      </ul>
      <h2>S</h2>
      <ul>
        <li>S3<ul><li>Hive, using with / <a href="#ch07lvl1sec54" title="Hive and S3" class="link">Hive and S3</a></li></ul></li>
        <li>s3distcp<ul><li>URL / <a href="#ch10lvl1sec77" title="Getting data into EMR" class="link">Getting data into EMR</a></li></ul></li>
        <li>s3n / <a href="#ch02lvl1sec25" title="Hadoop filesystems" class="link">Hadoop filesystems</a></li>
        <li>Samza<ul><li>about / <a href="#ch03lvl1sec34" title="Apache Samza" class="link">Apache Samza</a></li><li>URL / <a href="#ch03lvl1sec34" title="Apache Samza" class="link">Apache Samza</a>, <a href="#ch04lvl1sec36" title="Stream processing with Samza" class="link">Stream processing with Samza</a></li><li>YARN-independent frameworks / <a href="#ch03lvl1sec34" title="YARN-independent frameworks" class="link">YARN-independent frameworks</a></li><li>used, for stream processing / <a href="#ch04lvl1sec36" title="Stream processing with Samza" class="link">Stream processing with Samza</a></li><li>working / <a href="#ch04lvl1sec36" title="How Samza works" class="link">How Samza works</a></li><li>architecture / <a href="#ch04lvl1sec36" title="Samza high-level architecture" class="link">Samza high-level architecture</a></li><li>Apache Kafka / <a href="#ch04lvl1sec36" title="Samza's best friend â€“ Apache Kafka" class="link">Samza's best friend â€“ Apache Kafka</a></li><li>integrating, with YARN / <a href="#ch04lvl1sec36" title="YARN integration" class="link">YARN integration</a></li><li>independent model / <a href="#ch04lvl1sec36" title="An independent model" class="link">An independent model</a></li><li>Hello Samza / <a href="#ch04lvl1sec36" title="Hello Samza!" class="link">Hello Samza!</a></li><li>tweet parsing job, building / <a href="#ch04lvl1sec36" title="Building a tweet parsing job" class="link">Building a tweet parsing job</a></li><li>configuration file / <a href="#ch04lvl1sec36" title="The configuration file" class="link">The configuration file</a></li><li>URL, for configuration options / <a href="#ch04lvl1sec36" title="The configuration file" class="link">The configuration file</a></li><li>Twitter data, getting into Apache Kafka / <a href="#ch04lvl1sec36" title="Getting Twitter data into Kafka" class="link">Getting Twitter data into Kafka</a></li><li>HDFS / <a href="#ch04lvl1sec36" title="Samza and HDFS" class="link">Samza and HDFS</a></li><li>window function, adding / <a href="#ch04lvl1sec36" title="Windowing functions" class="link">Windowing functions</a></li><li>multijob workflows / <a href="#ch04lvl1sec36" title="Multijob workflows" class="link">Multijob workflows</a></li><li>tweet sentiment analysis, performing / <a href="#ch04lvl1sec36" title="Tweet sentiment analysis" class="link">Tweet sentiment analysis</a></li><li>tasks processing / <a href="#ch04lvl1sec36" title="Stateful tasks" class="link">Stateful tasks</a></li><li>and Spark Streaming, comparing / <a href="#ch05lvl1sec41" title="Comparing Samza and Spark Streaming" class="link">Comparing Samza and Spark Streaming</a></li></ul></li>
        <li>Samza, layers<ul><li>streaming / <a href="#ch04lvl1sec36" title="Samza high-level architecture" class="link">Samza high-level architecture</a></li><li>execution / <a href="#ch04lvl1sec36" title="Samza high-level architecture" class="link">Samza high-level architecture</a></li><li>processing / <a href="#ch04lvl1sec36" title="Samza high-level architecture" class="link">Samza high-level architecture</a></li></ul></li>
        <li>Samza job<ul><li>executing / <a href="#ch04lvl1sec36" title="Running a Samza job" class="link">Running a Samza job</a></li></ul></li>
        <li>sbt<ul><li>URL / <a href="#ch05lvl1sec38" title="Getting started with Spark" class="link">Getting started with Spark</a></li></ul></li>
        <li>Scala<ul><li>and Java source code, examples URL / <a href="#ch05lvl1sec40" title="Building and running the examples" class="link">Building and running the examples</a></li></ul></li>
        <li>Scala API<ul><li>about / <a href="#ch05lvl1sec38" title="Scala API" class="link">Scala API</a></li></ul></li>
        <li>scalar data types<ul><li>int / <a href="#ch06lvl1sec47" title="Pig data types" class="link">Pig data types</a></li><li>long / <a href="#ch06lvl1sec47" title="Pig data types" class="link">Pig data types</a></li><li>float / <a href="#ch06lvl1sec47" title="Pig data types" class="link">Pig data types</a></li><li>double / <a href="#ch06lvl1sec47" title="Pig data types" class="link">Pig data types</a></li><li>chararray / <a href="#ch06lvl1sec47" title="Pig data types" class="link">Pig data types</a></li><li>bytearray / <a href="#ch06lvl1sec47" title="Pig data types" class="link">Pig data types</a></li><li>boolean / <a href="#ch06lvl1sec47" title="Pig data types" class="link">Pig data types</a></li><li>datetime / <a href="#ch06lvl1sec47" title="Pig data types" class="link">Pig data types</a></li><li>biginteger / <a href="#ch06lvl1sec47" title="Pig data types" class="link">Pig data types</a></li><li>bigdecimal / <a href="#ch06lvl1sec47" title="Pig data types" class="link">Pig data types</a></li></ul></li>
        <li>Scala source code<ul><li>URL / <a href="#ch05lvl1sec40" title="Data processing on streams" class="link">Data processing on streams</a></li></ul></li>
        <li>Secondary NameNode<ul><li>about / <a href="#ch02lvl1sec21" title="Secondary NameNode not to the rescue" class="link">Secondary NameNode not to the rescue</a></li><li>demerits / <a href="#ch02lvl1sec21" title="Secondary NameNode not to the rescue" class="link">Secondary NameNode not to the rescue</a></li></ul></li>
        <li>secured cluster<ul><li>using, consequences / <a href="#ch10lvl1sec79" title="Consequences of using a secured cluster" class="link">Consequences of using a secured cluster</a></li></ul></li>
        <li>security<ul><li>about / <a href="#ch10lvl1sec79" title="Security" class="link">Security</a></li></ul></li>
        <li>sentiment analysis<ul><li>about / <a href="#ch03lvl1sec31" title="Sentiment of hashtags" class="link">Sentiment of hashtags</a></li></ul></li>
        <li>SequenceFile<ul><li>about / <a href="#ch02lvl1sec27" title="General-purpose file formats" class="link">General-purpose file formats</a></li></ul></li>
        <li>SequenceFile class, MapReduce job<ul><li>about / <a href="#ch03lvl1sec32" title="Sequence files" class="link">Sequence files</a></li></ul></li>
        <li>sequence files, MapReduce job<ul><li>about / <a href="#ch03lvl1sec32" title="Sequence files" class="link">Sequence files</a></li><li>advantages / <a href="#ch03lvl1sec32" title="Sequence files" class="link">Sequence files</a></li></ul></li>
        <li>SerDe classes, Hive<ul><li>MetadataTypedColumnsetSerDe / <a href="#ch07lvl1sec53" title="File formats and storage" class="link">File formats and storage</a></li><li>ThriftSerDe / <a href="#ch07lvl1sec53" title="File formats and storage" class="link">File formats and storage</a></li><li>DynamicSerDe / <a href="#ch07lvl1sec53" title="File formats and storage" class="link">File formats and storage</a></li></ul></li>
        <li>serialization<ul><li>about / <a href="#ch02lvl1sec27" title="Serialization and Containers" class="link">Serialization and Containers</a></li></ul></li>
        <li>sharelib, Oozie<ul><li>about / <a href="#ch08lvl1sec61" title="The Oozie sharelib" class="link">The Oozie sharelib</a></li></ul></li>
        <li>SimpleDB<ul><li>about / <a href="#ch11lvl1sec87" title="SimpleDB and DynamoDB" class="link">SimpleDB and DynamoDB</a></li></ul></li>
        <li>Simple Storage Service (S3), AWS<ul><li>about / <a href="#ch01lvl1sec14" title="Simple Storage Service (S3)" class="link">Simple Storage Service (S3)</a></li><li>URL / <a href="#ch01lvl1sec14" title="Simple Storage Service (S3)" class="link">Simple Storage Service (S3)</a></li></ul></li>
        <li>sources of information, Hadoop<ul><li>about / <a href="#ch11lvl1sec88" title="Sources of information" class="link">Sources of information</a></li><li>source code / <a href="#ch11lvl1sec88" title="Source code" class="link">Source code</a></li><li>mailing lists / <a href="#ch11lvl1sec88" title="Mailing lists and forums" class="link">Mailing lists and forums</a></li><li>forums / <a href="#ch11lvl1sec88" title="Mailing lists and forums" class="link">Mailing lists and forums</a></li><li>LinkedIn groups / <a href="#ch11lvl1sec88" title="LinkedIn groups" class="link">LinkedIn groups</a></li><li>HUGs / <a href="#ch11lvl1sec88" title="HUGs" class="link">HUGs</a></li><li>conferences / <a href="#ch11lvl1sec88" title="Conferences" class="link">Conferences</a></li></ul></li>
        <li>Spark<ul><li>about / <a href="#ch03lvl1sec34" title="Apache Spark" class="link">Apache Spark</a></li><li>URL / <a href="#ch03lvl1sec34" title="Apache Spark" class="link">Apache Spark</a></li></ul></li>
        <li>SparkContext object / <a href="#ch05lvl1sec38" title="Scala API" class="link">Scala API</a></li>
        <li>SparkPipeline<ul><li>about / <a href="#ch09lvl1sec69" title="SparkPipeline" class="link">SparkPipeline</a></li></ul></li>
        <li>Spark SQL<ul><li>about / <a href="#ch05lvl1sec39" title="Spark SQL" class="link">Spark SQL</a></li><li>data analysis with / <a href="#ch05lvl1sec40" title="Data analysis with Spark SQL" class="link">Data analysis with Spark SQL</a></li></ul></li>
        <li>Spark Streaming<ul><li>URL / <a href="#ch05lvl1sec39" title="Spark Streaming" class="link">Spark Streaming</a></li><li>about / <a href="#ch05lvl1sec39" title="Spark Streaming" class="link">Spark Streaming</a></li><li>and Samza, comparing / <a href="#ch05lvl1sec41" title="Comparing Samza and Spark Streaming" class="link">Comparing Samza and Spark Streaming</a></li></ul></li>
        <li>specialized join<ul><li>reference link / <a href="#ch06lvl1sec47" title="Join" class="link">Join</a></li></ul></li>
        <li>speed of thought analysis / <a href="#ch07lvl1sec58" title="A different philosophy" class="link">A different philosophy</a></li>
        <li>SQL<ul><li>on data streams / <a href="#ch05lvl1sec40" title="SQL on data streams" class="link">SQL on data streams</a></li><li>on data streams, URL / <a href="#ch05lvl1sec40" title="SQL on data streams" class="link">SQL on data streams</a></li></ul></li>
        <li>SQL-on-Hadoop<ul><li>need for / <a href="#ch07lvl1sec51" title="Why SQL on Hadoop" class="link">Why SQL on Hadoop</a></li><li>solutions / <a href="#ch07lvl1sec51" title="Other SQL-on-Hadoop solutions" class="link">Other SQL-on-Hadoop solutions</a></li></ul></li>
        <li>Sqoop<ul><li>about / <a href="#ch11lvl1sec85" title="Sqoop" class="link">Sqoop</a></li><li>URL / <a href="#ch11lvl1sec85" title="Sqoop" class="link">Sqoop</a></li></ul></li>
        <li>Sqoop 1<ul><li>about / <a href="#ch11lvl1sec85" title="Sqoop" class="link">Sqoop</a></li></ul></li>
        <li>Sqoop 2<ul><li>about / <a href="#ch11lvl1sec85" title="Sqoop" class="link">Sqoop</a></li></ul></li>
        <li>standalone applications, Apache Spark<ul><li>writing / <a href="#ch05lvl1sec38" title="Writing and running standalone applications" class="link">Writing and running standalone applications</a></li><li>running / <a href="#ch05lvl1sec38" title="Writing and running standalone applications" class="link">Writing and running standalone applications</a></li></ul></li>
        <li>statements<ul><li>about / <a href="#ch06lvl1sec46" title="Fundamentals of Apache Pig" class="link">Fundamentals of Apache Pig</a></li></ul></li>
        <li>Stinger initiative<ul><li>about / <a href="#ch07lvl1sec57" title="Stinger initiative" class="link">Stinger initiative</a></li></ul></li>
        <li>storage<ul><li>about / <a href="#ch01lvl1sec10" title="Storage" class="link">Storage</a></li></ul></li>
        <li>storage, Hadoop 2<ul><li>about / <a href="#ch01lvl1sec11" title="Storage in Hadoop 2" class="link">Storage in Hadoop 2</a></li></ul></li>
        <li>storage, Hive<ul><li>about / <a href="#ch07lvl1sec53" title="File formats and storage" class="link">File formats and storage</a></li><li>columnar stores / <a href="#ch07lvl1sec53" title="Columnar stores" class="link">Columnar stores</a></li></ul></li>
        <li>Storm<ul><li>URL / <a href="#ch04lvl1sec36" title="How Samza works" class="link">How Samza works</a></li><li>about / <a href="#ch04lvl1sec36" title="How Samza works" class="link">How Samza works</a></li></ul></li>
        <li>stream.py<ul><li>reference link / <a href="#ch01lvl1sec17" title="Programmatic access with Python" class="link">Programmatic access with Python</a></li></ul></li>
        <li>stream processing<ul><li>with Samza / <a href="#ch04lvl1sec36" title="Stream processing with Samza" class="link">Stream processing with Samza</a></li></ul></li>
        <li>streams<ul><li>data, processing on / <a href="#ch05lvl1sec40" title="Data processing on streams" class="link">Data processing on streams</a></li></ul></li>
        <li>systems management tools<ul><li>Cloudera Manager, integrating with / <a href="#ch10lvl1sec72" title="Cloudera Manager and other management tools" class="link">Cloudera Manager and other management tools</a></li></ul></li>
      </ul>
      <h2>T</h2>
      <ul>
        <li>table partitioning<ul><li>about / <a href="#ch07lvl1sec53" title="Partitioning a table" class="link">Partitioning a table</a></li><li>data, overwriting / <a href="#ch07lvl1sec53" title="Overwriting and updating data" class="link">Overwriting and updating data</a></li><li>data, updating / <a href="#ch07lvl1sec53" title="Overwriting and updating data" class="link">Overwriting and updating data</a></li><li>bucketing / <a href="#ch07lvl1sec53" title="Bucketing and sorting" class="link">Bucketing and sorting</a></li><li>sorting / <a href="#ch07lvl1sec53" title="Bucketing and sorting" class="link">Bucketing and sorting</a></li><li>data, sampling / <a href="#ch07lvl1sec53" title="Sampling data" class="link">Sampling data</a></li></ul></li>
        <li>Tajo<ul><li>URL / <a href="#ch07lvl1sec58" title="Drill, Tajo, and beyond" class="link">Drill, Tajo, and beyond</a></li><li>about / <a href="#ch07lvl1sec58" title="Drill, Tajo, and beyond" class="link">Drill, Tajo, and beyond</a></li></ul></li>
        <li>tasks processing, Samza<ul><li>about / <a href="#ch04lvl1sec36" title="Stateful tasks" class="link">Stateful tasks</a></li></ul></li>
        <li>term frequency<ul><li>about / <a href="#ch09lvl1sec67" title="Calculate term frequency" class="link">Calculate term frequency</a></li><li>calculating, with TF-IDF / <a href="#ch09lvl1sec67" title="Calculate term frequency" class="link">Calculate term frequency</a></li></ul></li>
        <li>text attribute, entity<ul><li>about / <a href="#ch06lvl1sec49" title="Tweet metadata" class="link">Tweet metadata</a></li></ul></li>
        <li>Text files<ul><li>about / <a href="#ch02lvl1sec27" title="General-purpose file formats" class="link">General-purpose file formats</a></li></ul></li>
        <li>Tez<ul><li>about / <a href="#ch03lvl1sec34" title="Tez" class="link">Tez</a></li><li>URL / <a href="#ch03lvl1sec34" title="Tez" class="link">Tez</a>, <a href="#ch07lvl1sec57" title="Stinger initiative" class="link">Stinger initiative</a></li><li>reference link, for canonical WordCount example / <a href="#ch03lvl1sec34" title="Tez" class="link">Tez</a></li><li>Hive-on-tez / <a href="#ch03lvl1sec34" title="Hive-on-tez" class="link">Hive-on-tez</a></li></ul> / <a href="#ch06lvl1sec43" title="An overview of Pig" class="link">An overview of Pig</a></li>
        <li>TF-IDF<ul><li>about / <a href="#ch09lvl1sec67" title="Finding important words in text" class="link">Finding important words in text</a></li><li>definition / <a href="#ch09lvl1sec67" title="Finding important words in text" class="link">Finding important words in text</a></li><li>term frequency, calculating / <a href="#ch09lvl1sec67" title="Calculate term frequency" class="link">Calculate term frequency</a></li><li>document frequency, calculating / <a href="#ch09lvl1sec67" title="Calculate document frequency" class="link">Calculate document frequency</a></li><li>implementing / <a href="#ch09lvl1sec67" title="Putting it all together â€“ TF-IDF" class="link">Putting it all together â€“ TF-IDF</a></li></ul></li>
        <li>Thrift<ul><li>about / <a href="#ch07lvl1sec56" title="Thrift" class="link">Thrift</a></li></ul></li>
        <li>TOBAG(expression) function / <a href="#ch06lvl1sec47" title="The tuple, bag, and map functions" class="link">The tuple, bag, and map functions</a></li>
        <li>TOMAP(expression) function / <a href="#ch06lvl1sec47" title="The tuple, bag, and map functions" class="link">The tuple, bag, and map functions</a></li>
        <li>tools, data life cycle management<ul><li>orchestration services / <a href="#ch08lvl1sec60" title="Tools to help" class="link">Tools to help</a></li><li>connectors / <a href="#ch08lvl1sec60" title="Tools to help" class="link">Tools to help</a></li><li>file formats / <a href="#ch08lvl1sec60" title="Tools to help" class="link">Tools to help</a></li></ul></li>
        <li>TOP(n, column, relation) function / <a href="#ch06lvl1sec47" title="The tuple, bag, and map functions" class="link">The tuple, bag, and map functions</a></li>
        <li>TOTUPLE(expression) function / <a href="#ch06lvl1sec47" title="The tuple, bag, and map functions" class="link">The tuple, bag, and map functions</a></li>
        <li>troubleshooting<ul><li>about / <a href="#ch10lvl1sec81" title="Troubleshooting" class="link">Troubleshooting</a></li></ul></li>
        <li>tuples<ul><li>about / <a href="#ch06lvl1sec46" title="Fundamentals of Apache Pig" class="link">Fundamentals of Apache Pig</a></li></ul></li>
        <li>Tweet, structure<ul><li>reference link / <a href="#ch01lvl1sec17" title="Anatomy of a Tweet" class="link">Anatomy of a Tweet</a></li></ul></li>
        <li>tweet analysis capability<ul><li>building / <a href="#ch08lvl1sec61" title="Building a tweet analysis capability" class="link">Building a tweet analysis capability</a></li><li>tweet data, obtaining / <a href="#ch08lvl1sec61" title="Getting the tweet data" class="link">Getting the tweet data</a></li><li>Oozie / <a href="#ch08lvl1sec61" title="Introducing Oozie" class="link">Introducing Oozie</a></li><li>derived data, producing / <a href="#ch08lvl1sec61" title="Producing derived data" class="link">Producing derived data</a></li></ul></li>
        <li>tweet sentiment analysis<ul><li>performing / <a href="#ch04lvl1sec36" title="Tweet sentiment analysis" class="link">Tweet sentiment analysis</a></li><li>bootstrap streams / <a href="#ch04lvl1sec36" title="Bootstrap streams" class="link">Bootstrap streams</a></li></ul></li>
        <li>Twitter<ul><li>used, for generating dataset / <a href="#ch01lvl1sec17" title="Data processing with Hadoop" class="link">Data processing with Hadoop</a></li><li>URL / <a href="#ch01lvl1sec17" title="Data processing with Hadoop" class="link">Data processing with Hadoop</a></li><li>about / <a href="#ch01lvl1sec17" title="Why Twitter?" class="link">Why Twitter?</a></li><li>signup page / <a href="#ch01lvl1sec17" title="Twitter credentials" class="link">Twitter credentials</a></li><li>web form / <a href="#ch01lvl1sec17" title="Twitter credentials" class="link">Twitter credentials</a></li></ul></li>
        <li>Twitter data, properties<ul><li>unstructured / <a href="#ch01lvl1sec17" title="Why Twitter?" class="link">Why Twitter?</a></li><li>structured / <a href="#ch01lvl1sec17" title="Why Twitter?" class="link">Why Twitter?</a></li><li>graph / <a href="#ch01lvl1sec17" title="Why Twitter?" class="link">Why Twitter?</a></li><li>geolocated / <a href="#ch01lvl1sec17" title="Why Twitter?" class="link">Why Twitter?</a></li><li>real time / <a href="#ch01lvl1sec17" title="Why Twitter?" class="link">Why Twitter?</a></li></ul></li>
        <li>Twitter Search<ul><li>URL / <a href="#ch03lvl1sec31" title="Trending topics" class="link">Trending topics</a></li></ul></li>
        <li>Twitter stream<ul><li>analyzing / <a href="#ch06lvl1sec49" title="Analyzing the Twitter stream" class="link">Analyzing the Twitter stream</a></li><li>prerequisites / <a href="#ch06lvl1sec49" title="Prerequisites" class="link">Prerequisites</a></li><li>dataset exploration / <a href="#ch06lvl1sec49" title="Dataset exploration" class="link">Dataset exploration</a></li><li>tweet metadata / <a href="#ch06lvl1sec49" title="Tweet metadata" class="link">Tweet metadata</a></li><li>data preparation / <a href="#ch06lvl1sec49" title="Data preparation" class="link">Data preparation</a></li><li>top n statistics / <a href="#ch06lvl1sec49" title="Top n statistics" class="link">Top n statistics</a></li><li>datetime manipulation / <a href="#ch06lvl1sec49" title="Datetime manipulation" class="link">Datetime manipulation</a></li><li>sessions / <a href="#ch06lvl1sec49" title="Sessions" class="link">Sessions</a></li><li>users' interaction, capturing / <a href="#ch06lvl1sec49" title="Capturing user interactions" class="link">Capturing user interactions</a></li><li>link analysis / <a href="#ch06lvl1sec49" title="Link analysis" class="link">Link analysis</a></li><li>influential users, identifying / <a href="#ch06lvl1sec49" title="Influential users" class="link">Influential users</a></li></ul></li>
      </ul>
      <h2>U</h2>
      <ul>
        <li>union operation<ul><li>about / <a href="#ch09lvl1sec69" title="Concepts" class="link">Concepts</a></li></ul></li>
        <li>updateFunc function / <a href="#ch05lvl1sec40" title="State management" class="link">State management</a></li>
        <li>User Defined Aggregate Functions (UDAFs / <a href="#ch07lvl1sec55" title="Extending HiveQL" class="link">Extending HiveQL</a></li>
        <li>User Defined Functions (UDFs) / <a href="#ch06lvl1sec43" title="An overview of Pig" class="link">An overview of Pig</a>, <a href="#ch07lvl1sec55" title="Extending HiveQL" class="link">Extending HiveQL</a><ul><li>about / <a href="#ch06lvl1sec46" title="Fundamentals of Apache Pig" class="link">Fundamentals of Apache Pig</a></li></ul></li>
        <li>User Defined Table Functions (UDTF) / <a href="#ch07lvl1sec55" title="Extending HiveQL" class="link">Extending HiveQL</a></li>
      </ul>
      <h2>V</h2>
      <ul>
        <li>versioning, Hadoop<ul><li>about / <a href="#ch01lvl1sec08" title="A note on versioning" class="link">A note on versioning</a></li></ul></li>
        <li>VirtualBox<ul><li>reference link / <a href="#ch01lvl1sec15" title="Cloudera QuickStart VM" class="link">Cloudera QuickStart VM</a></li></ul></li>
        <li>VMware<ul><li>reference link / <a href="#ch01lvl1sec15" title="Cloudera QuickStart VM" class="link">Cloudera QuickStart VM</a></li></ul></li>
      </ul>
      <h2>W</h2>
      <ul>
        <li>Whir<ul><li>about / <a href="#ch11lvl1sec85" title="Whir" class="link">Whir</a></li><li>URL / <a href="#ch11lvl1sec85" title="Whir" class="link">Whir</a></li></ul></li>
        <li>Who to Follow service<ul><li>reference link / <a href="#ch06lvl1sec49" title="Influential users" class="link">Influential users</a></li></ul></li>
        <li>window function<ul><li>adding / <a href="#ch04lvl1sec36" title="Windowing functions" class="link">Windowing functions</a></li></ul></li>
        <li>WordCount<ul><li>in Java / <a href="#ch05lvl1sec38" title="WordCount in Java" class="link">WordCount in Java</a></li></ul></li>
        <li>WordCount example, MapReduce programs<ul><li>about / <a href="#ch03lvl1sec31" title="WordCount, the Hello World of MapReduce" class="link">WordCount, the Hello World of MapReduce</a></li><li>reference link, for source code / <a href="#ch03lvl1sec31" title="Word co-occurrences" class="link">Word co-occurrences</a></li></ul></li>
        <li>workflow-app<ul><li>about / <a href="#ch08lvl1sec61" title="Introducing Oozie" class="link">Introducing Oozie</a></li></ul></li>
        <li>workflow.xml file<ul><li>reference link / <a href="#ch08lvl1sec61" title="Extracting data and ingesting into Hive" class="link">Extracting data and ingesting into Hive</a></li></ul></li>
        <li>workflows<ul><li>building, Oozie used / <a href="#ch08lvl1sec64" title="Pulling it all together" class="link">Pulling it all together</a></li></ul></li>
        <li>workloads<ul><li>Hive tables, structuring from / <a href="#ch07lvl1sec53" title="Structuring Hive tables for given workloads" class="link">Structuring Hive tables for given workloads</a></li></ul></li>
        <li>wrapper classes<ul><li>about / <a href="#ch02lvl1sec26" title="Introducing the wrapper classes " class="link">Introducing the wrapper classes </a></li></ul></li>
        <li>WritableComparable interface<ul><li>about / <a href="#ch02lvl1sec26" title="The Comparable and WritableComparable interfaces" class="link">The Comparable and WritableComparable interfaces</a></li></ul></li>
        <li>Writable interface<ul><li>about / <a href="#ch02lvl1sec26" title="The Writable interface" class="link">The Writable interface</a></li></ul></li>
      </ul>
      <h2>Y</h2>
      <ul>
        <li>YARN<ul><li>about / <a href="#ch01lvl1sec11" title="Computation in Hadoop 2" class="link">Computation in Hadoop 2</a>, <a href="#ch03lvl1sec33" title="YARN" class="link">YARN</a>, <a href="#ch03lvl1sec34" title="YARN in the real world â€“ Computation beyond MapReduce" class="link">YARN in the real world â€“ Computation beyond MapReduce</a></li><li>architecture / <a href="#ch03lvl1sec33" title="YARN architecture" class="link">YARN architecture</a></li><li>components / <a href="#ch03lvl1sec33" title="The components of YARN" class="link">The components of YARN</a></li><li>processing frameworks / <a href="#ch03lvl1sec33" title="Thinking in layers" class="link">Thinking in layers</a></li><li>processing models / <a href="#ch03lvl1sec33" title="Thinking in layers" class="link">Thinking in layers</a></li><li>issues, with MapReduce / <a href="#ch03lvl1sec34" title="The problem with MapReduce" class="link">The problem with MapReduce</a></li><li>Tez / <a href="#ch03lvl1sec34" title="Tez" class="link">Tez</a></li><li>Apache Spark / <a href="#ch03lvl1sec34" title="Apache Spark" class="link">Apache Spark</a></li><li>Apache Samza / <a href="#ch03lvl1sec34" title="Apache Samza" class="link">Apache Samza</a></li><li>future / <a href="#ch03lvl1sec34" title="YARN today and beyond" class="link">YARN today and beyond</a></li><li>present situation / <a href="#ch03lvl1sec34" title="YARN today and beyond" class="link">YARN today and beyond</a></li><li>Samza, integrating / <a href="#ch04lvl1sec36" title="YARN integration" class="link">YARN integration</a></li><li>Apache Spark on / <a href="#ch05lvl1sec38" title="Spark on YARN" class="link">Spark on YARN</a></li><li>examples, running on / <a href="#ch05lvl1sec40" title="Running the examples on YARN" class="link">Running the examples on YARN</a></li><li>URL / <a href="#ch05lvl1sec40" title="Running the examples on YARN" class="link">Running the examples on YARN</a></li></ul></li>
        <li>YARN API<ul><li>about / <a href="#ch03lvl1sec33" title="Thinking in layers" class="link">Thinking in layers</a></li></ul></li>
        <li>YARN application<ul><li>anatomy / <a href="#ch03lvl1sec33" title="Anatomy of a YARN application" class="link">Anatomy of a YARN application</a></li><li>ApplicationMaster (AM) / <a href="#ch03lvl1sec33" title="Anatomy of a YARN application" class="link">Anatomy of a YARN application</a></li><li>life cycle / <a href="#ch03lvl1sec33" title="Life cycle of a YARN application" class="link">Life cycle of a YARN application</a></li><li>fault-tolerance / <a href="#ch03lvl1sec33" title="Fault tolerance and monitoring" class="link">Fault tolerance and monitoring</a></li><li>monitoring / <a href="#ch03lvl1sec33" title="Fault tolerance and monitoring" class="link">Fault tolerance and monitoring</a></li><li>execution models / <a href="#ch03lvl1sec33" title="Execution models" class="link">Execution models</a></li></ul></li>
      </ul>
      <h2>Z</h2>
      <ul>
        <li>ZooKeeper Failover Controller (ZKFC) / <a href="#ch02lvl1sec23" title="Automatic NameNode failover" class="link">Automatic NameNode failover</a></li>
        <li>ZooKeeper quorum / <a href="#ch02lvl1sec23" title="Automatic NameNode failover" class="link">Automatic NameNode failover</a></li>
      </ul>
    </div>
  </div></div></div>
</div></div></div></body></html>
