<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="">

    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0"> <!--320-->
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no, user-scalable=no">

    <link rel="icon" href="../../mapt/images/favicon.ico">

    <link rel="stylesheet" href="../../mapt/css/font-awesome.css">
    <link rel="stylesheet" href="../../mapt/css/google-fonts.css">
    <link rel="stylesheet" href="../../mapt/css/devicon.css">

    <link rel="stylesheet" href="../../mapt/css/bootstrap.css">
    <link rel="stylesheet" href="../../mapt/css/bootstrap-xl.css">
    <link rel="stylesheet" href="../../mapt/css/magnific-popup.css">
    <link rel="stylesheet" href="../../mapt/css/prism.css">
    <link rel="stylesheet" href="../../mapt/css/hljs-github.css">

    <link rel="stylesheet" href="../../mapt/css/mapt.css">
    <link rel="stylesheet" href="../../mapt/css/custom.css">

    <script src="../../mapt/js/jquery.js"></script>
    <script src="../../mapt/js/bootstrap.js"></script>
    <script src="../../mapt/js/jquery.magnific-popup.js"></script>
    <script src="../../mapt/js/highlight.min.js"></script>

    <script src="../../mapt/js/custom.js"></script>
    
    <title>Fast Data Processing with Spark</title>
</head>

<body class="home-body">
    <div id="wrapper">
        <div id="sidebar-wrapper">    
            <ul class="sidebar-nav">
                <div class="list-group" id="sidebar-nav" role="tablist">
                    <li>
                        <a href="../../index.html" class="sidenav-menu-holder back-btn" id="back-link">
                            <span class="sidenav-menu">Book List</span>
                            <span class="pull-left mr5"><i class="fa fa-chevron-left"></i></span>
                        </a>
                    </li>
                    
                    <li class="book-info copyright">
                        <span class="info text-nowrap"><span class="copyleft">&copy;</span><span><strong>RuTracker</strong>.org</span></span>
                    </li>          
                    <li class="book-info copyright">
                        <span class="info text-nowrap">Pub date: <strong>23 Oct 2013</strong></span>
                    </li>         
                    <li class="book-info">
                        <span class="info text-nowrap">Price: €<strong>17.99</strong></span>
                        <span class="info text-nowrap">ISBN: <strong>9781782167068</strong></span>
                    </li>     
            
                    <li>
                        <a href="graphics/cover.jpg" class="sidenav-menu-holder cover-img">
                            <img src="graphics/cover.jpg" class="cover-image">
                        </a>
                    </li>        
            
                    <div class="book_navigation">
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse1">
                                <div class="section-name">1: Installing Spark and Setting Up Your Cluster</div>
                            </a>
                        </li>
                        <div id="collapse1" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="1" class="sub-nav">
                                <a href="#ch01">
                                    <div class="section-name">Chapter 1: Installing Spark and Setting Up Your Cluster</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec08" class="sub-nav">
                                <a href="#ch01lvl1sec08">                    
                                    <div class="section-name">Running Spark on a single machine</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec09" class="sub-nav">
                                <a href="#ch01lvl1sec09">                    
                                    <div class="section-name">Running Spark on EC2</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec10" class="sub-nav">
                                <a href="#ch01lvl1sec10">                    
                                    <div class="section-name">Deploying Spark on Elastic MapReduce</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec11" class="sub-nav">
                                <a href="#ch01lvl1sec11">                    
                                    <div class="section-name">Deploying Spark with Chef (opscode)</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec12" class="sub-nav">
                                <a href="#ch01lvl1sec12">                    
                                    <div class="section-name">Deploying Spark on Mesos</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec13" class="sub-nav">
                                <a href="#ch01lvl1sec13">                    
                                    <div class="section-name">Deploying Spark on YARN</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec14" class="sub-nav">
                                <a href="#ch01lvl1sec14">                    
                                    <div class="section-name">Deploying set of machines over SSH</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec15" class="sub-nav">
                                <a href="#ch01lvl1sec15">                    
                                    <div class="section-name">Links and references</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec16" class="sub-nav">
                                <a href="#ch01lvl1sec16">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse2">
                                <div class="section-name">2: Using the Spark Shell</div>
                            </a>
                        </li>
                        <div id="collapse2" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="2" class="sub-nav">
                                <a href="#ch02">
                                    <div class="section-name">Chapter 2: Using the Spark Shell</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec17" class="sub-nav">
                                <a href="#ch02lvl1sec17">                    
                                    <div class="section-name">Loading a simple text file</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec18" class="sub-nav">
                                <a href="#ch02lvl1sec18">                    
                                    <div class="section-name">Using the Spark shell to run logistic regression</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec19" class="sub-nav">
                                <a href="#ch02lvl1sec19">                    
                                    <div class="section-name">Interactively loading data from S3</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec20" class="sub-nav">
                                <a href="#ch02lvl1sec20">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse3">
                                <div class="section-name">3: Building and Running a Spark Application</div>
                            </a>
                        </li>
                        <div id="collapse3" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="3" class="sub-nav">
                                <a href="#ch03">
                                    <div class="section-name">Chapter 3: Building and Running a Spark Application</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec21" class="sub-nav">
                                <a href="#ch03lvl1sec21">                    
                                    <div class="section-name">Building your Spark project with sbt</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec22" class="sub-nav">
                                <a href="#ch03lvl1sec22">                    
                                    <div class="section-name">Building your Spark job with Maven</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec23" class="sub-nav">
                                <a href="#ch03lvl1sec23">                    
                                    <div class="section-name">Building your Spark job with something else</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec24" class="sub-nav">
                                <a href="#ch03lvl1sec24">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse4">
                                <div class="section-name">4: Creating a SparkContext</div>
                            </a>
                        </li>
                        <div id="collapse4" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="4" class="sub-nav">
                                <a href="#ch04">
                                    <div class="section-name">Chapter 4: Creating a SparkContext</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec25" class="sub-nav">
                                <a href="#ch04lvl1sec25">                    
                                    <div class="section-name">Scala</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec26" class="sub-nav">
                                <a href="#ch04lvl1sec26">                    
                                    <div class="section-name">Java</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec27" class="sub-nav">
                                <a href="#ch04lvl1sec27">                    
                                    <div class="section-name">Shared Java and Scala APIs</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec28" class="sub-nav">
                                <a href="#ch04lvl1sec28">                    
                                    <div class="section-name">Python</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec29" class="sub-nav">
                                <a href="#ch04lvl1sec29">                    
                                    <div class="section-name">Links and references</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec30" class="sub-nav">
                                <a href="#ch04lvl1sec30">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse5">
                                <div class="section-name">5: Loading and Saving Data in Spark</div>
                            </a>
                        </li>
                        <div id="collapse5" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="5" class="sub-nav">
                                <a href="#ch05">
                                    <div class="section-name">Chapter 5: Loading and Saving Data in Spark</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec31" class="sub-nav">
                                <a href="#ch05lvl1sec31">                    
                                    <div class="section-name">RDDs</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec32" class="sub-nav">
                                <a href="#ch05lvl1sec32">                    
                                    <div class="section-name">Loading data into an RDD</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec33" class="sub-nav">
                                <a href="#ch05lvl1sec33">                    
                                    <div class="section-name">Saving your data</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec34" class="sub-nav">
                                <a href="#ch05lvl1sec34">                    
                                    <div class="section-name">Links and references</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec35" class="sub-nav">
                                <a href="#ch05lvl1sec35">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse6">
                                <div class="section-name">6: Manipulating Your RDD</div>
                            </a>
                        </li>
                        <div id="collapse6" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="6" class="sub-nav">
                                <a href="#ch06">
                                    <div class="section-name">Chapter 6: Manipulating Your RDD</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec36" class="sub-nav">
                                <a href="#ch06lvl1sec36">                    
                                    <div class="section-name">Manipulating your RDD in Scala and Java</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec37" class="sub-nav">
                                <a href="#ch06lvl1sec37">                    
                                    <div class="section-name">Manipulating your RDD in Python</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec38" class="sub-nav">
                                <a href="#ch06lvl1sec38">                    
                                    <div class="section-name">Links and references</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec39" class="sub-nav">
                                <a href="#ch06lvl1sec39">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse7">
                                <div class="section-name">7: Shark – Using Spark with Hive</div>
                            </a>
                        </li>
                        <div id="collapse7" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="7" class="sub-nav">
                                <a href="#ch07">
                                    <div class="section-name">Chapter 7: Shark – Using Spark with Hive</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec40" class="sub-nav">
                                <a href="#ch07lvl1sec40">                    
                                    <div class="section-name">Why Hive/Shark?</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec41" class="sub-nav">
                                <a href="#ch07lvl1sec41">                    
                                    <div class="section-name">Installing Shark</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec42" class="sub-nav">
                                <a href="#ch07lvl1sec42">                    
                                    <div class="section-name">Running Shark</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec43" class="sub-nav">
                                <a href="#ch07lvl1sec43">                    
                                    <div class="section-name">Loading data</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec44" class="sub-nav">
                                <a href="#ch07lvl1sec44">                    
                                    <div class="section-name">Using Hive queries in a Spark program</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec45" class="sub-nav">
                                <a href="#ch07lvl1sec45">                    
                                    <div class="section-name">Links and references</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec46" class="sub-nav">
                                <a href="#ch07lvl1sec46">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse8">
                                <div class="section-name">8: Testing</div>
                            </a>
                        </li>
                        <div id="collapse8" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="8" class="sub-nav">
                                <a href="#ch08">
                                    <div class="section-name">Chapter 8: Testing</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec47" class="sub-nav">
                                <a href="#ch08lvl1sec47">                    
                                    <div class="section-name">Testing in Java and Scala</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec48" class="sub-nav">
                                <a href="#ch08lvl1sec48">                    
                                    <div class="section-name">Testing in Python</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec49" class="sub-nav">
                                <a href="#ch08lvl1sec49">                    
                                    <div class="section-name">Links and references</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec50" class="sub-nav">
                                <a href="#ch08lvl1sec50">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse9">
                                <div class="section-name">9: Tips and Tricks</div>
                            </a>
                        </li>
                        <div id="collapse9" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="9" class="sub-nav">
                                <a href="#ch09">
                                    <div class="section-name">Chapter 9: Tips and Tricks</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec51" class="sub-nav">
                                <a href="#ch09lvl1sec51">                    
                                    <div class="section-name">Where to find logs?</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec52" class="sub-nav">
                                <a href="#ch09lvl1sec52">                    
                                    <div class="section-name">Concurrency limitations</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec53" class="sub-nav">
                                <a href="#ch09lvl1sec53">                    
                                    <div class="section-name">Memory usage and garbage collection</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec54" class="sub-nav">
                                <a href="#ch09lvl1sec54">                    
                                    <div class="section-name">Serialization</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec55" class="sub-nav">
                                <a href="#ch09lvl1sec55">                    
                                    <div class="section-name">IDE integration</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec56" class="sub-nav">
                                <a href="#ch09lvl1sec56">                    
                                    <div class="section-name">Using Spark with other languages</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec57" class="sub-nav">
                                <a href="#ch09lvl1sec57">                    
                                    <div class="section-name">A quick note on security</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec58" class="sub-nav">
                                <a href="#ch09lvl1sec58">                    
                                    <div class="section-name">Mailing lists</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec59" class="sub-nav">
                                <a href="#ch09lvl1sec59">                    
                                    <div class="section-name">Links and references</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec60" class="sub-nav">
                                <a href="#ch09lvl1sec60">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapsebackindex">
                                <div class="section-name">Appendix A: Index</div>
                            </a>
                        </li>
                        <div id="collapsebackindex" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="backindex" class="sub-nav">
                                <a href="#backindex">
                                    <div class="section-name">Chapter Appendix A: Index</div>
                                </a>
                            </li>
                        </div>
                    </div>
                </div>
            </ul>
        </div>
        
        <div id="page-content-wrapper" class="book-page">
            <a href="#" id="menu-toggle" class="toggle-nav"><i class="fa fa-bars fa-2x mr5"></i></a>
            
            <a href="#" id="back_to_top" class="back-to-top"><img src="../../mapt/images/kopimi.svg"></a>
            
            <div class="col-sm-12 col-xl-offset-2 col-xl-8 col-lg-offset-1 col-lg-10">
                <div class="btn-group pull-right mt15 mb30" role="group">
                    <a href="#home" class="btn btn-default">
                        <i class="fa fa-share fa-lg no-text-padding"></i>
                        <span class="hidden-xs ml5">Book Home</span>
                    </a>
                    <button class="btn btn-default disabled" id="code-download">
                        <i class="fa fa-file fa-lg"></i>
                        <span class="hidden-xs ml5">Download Code Files</span>
                    </button>
                </div>
            </div>
            <div class="clearfix"></div>
            
            <div id="book-wrapper" class="container-fluid">
                <div class="col-sm-12 col-xl-offset-2 col-xl-8 col-lg-offset-1 col-lg-10" id="home">
                    <h2 class="product-title">Fast Data Processing with Spark</h2>
                    <hr>
                    <div class="row">
                        <div class="col-sm-12">
                            <h5 class="mt10">By Holden Karau</h5>
                            <div>
                                <p class="mb20"><b>Spark offers a streamlined way to write distributed programs and this tutorial gives you the know-how as a software developer to make the most of Spark’s many great features, providing an extra string to your bow.</b></p>
                                <a href="#ch01" class="btn btn-info btn-lg pull-right hidden-xs">
                                    Start Reading <i class="fa fa-chevron-right ml5"></i>
                                </a>
                                <a href="#ch01" class="btn btn-info btn-lg btn-block mt20 mb20 visible-xs">
                                    Start Reading <i class="fa fa-chevron-right ml5"></i>
                                </a>
                                <div class="clearfix"></div>
                                <div class="col-sm-12">
                                    <ul id="myTabs" class="nav nav-tabs nav-justified hidden-xs mt20" role="tablist">
                                        <li class="active">
                                            <a href="#info" role="tab" id="info-tab" data-toggle="tab">
                                                <h5>Info</h5>
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#content" role="tab" id="content-tab" data-toggle="tab">
                                                <h5>Contents</h5>
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#author" role="tab" id="author-tab" data-toggle="tab">
                                                <h5>Author</h5>
                                            </a>
                                        </li>
                                    </ul>
                
                                    <ul id="myTabsMobile" class="nav nav-pills text-center nav-stacked visible-xs mb60" role="tablist">
                                        <li class="active">
                                            <a href="#info" role="tab" id="info-tab-responsive" data-toggle="tab">
                                                Info
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#content" role="tab" id="content-tab-responsive" data-toggle="tab">
                                                Contents
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#author" role="tab" id="author-tab-responsive" data-toggle="tab">
                                                Author
                                            </a>
                                        </li>
                                    </ul>
                
                                    <div id="myTabContent" class="tab-content pt30">
                                    
                                        <div role="tabpanel" class="tab-pane active fade in" id="info">
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">Features</h5>
                                            </div>
                                            <div class="hidden-xs">
                                                <h5>Features</h5>
                                            </div>
                                            <hr>
                                            <div>
                                                <ul>
                <li>Implement Spark's interactive shell to prototype distributed applications</li>
                <li>Deploy Spark jobs to various clusters such as Mesos, EC2, Chef, YARN, EMR, and so on</li>
                <li>Use Shark's SQL query-like syntax with Spark</li>
                </ul>
                                            </div>
                                            <br>
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">Learning</h5>
                                            </div>
                                            <div class="hidden-xs">
                                                <h5>Learning</h5>
                                            </div>
                                            <hr>
                                            <div>
                                                <ul>
                <li>Prototype distributed applications with Spark's interactive shell</li>
                <li>Learn different ways to interact with Spark's distributed representation of data (RDDs)</li>
                <li>Load data from the various data sources</li>
                <li>Query Spark with a SQL-like query syntax</li>
                <li>Integrate Shark queries with Spark programs</li>
                <li>Effectively test your distributed software</li>
                <li>Tune a Spark installation</li>
                <li>Install and set up Spark on your cluster</li>
                <li>Work effectively with large data sets</li>
                </ul>
                                            </div>
                                            <br>
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">About</h5>
                                            </div>
                                            <div class="hidden-xs">
                                                <h5>About</h5>
                                            </div>
                                            <hr>
                                            <div>
                                                <p>Spark is a framework for writing fast, distributed programs. Spark solves similar problems as Hadoop MapReduce does but with a fast in-memory approach and a clean functional style API. With its ability to integrate with Hadoop and inbuilt tools for interactive query analysis (Shark), large-scale graph processing and analysis (Bagel), and real-time analysis (Spark Streaming), it can be interactively used to quickly process and query big data sets.</p>
                <p>Fast Data Processing with Spark covers how to write distributed map reduce style programs with Spark. The book will guide you through every step required to write effective distributed programs from setting up your cluster and interactively exploring the API, to deploying your job to the cluster, and tuning it for your purposes.</p>
                <p>Fast Data Processing with Spark covers everything from setting up your Spark cluster in a variety of situations (stand-alone, EC2, and so on), to how to use the interactive shell to write distributed code interactively. From there, we move on to cover how to write and deploy distributed jobs in Java, Scala, and Python.</p>
                <p>We then examine how to use the interactive shell to quickly prototype distributed programs and explore the Spark API. We also look at how to use Hive with Spark to use a SQL-like query syntax with Shark, as well as manipulating resilient distributed datasets (RDDs).</p>
                                            </div>
                                        </div>
                                        
                                        <div role="tabpanel" class="tab-pane fade in" id="content">
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">Contents</h5>
                                                <hr>
                                            </div>
                                            <ul>
                                                <div>
                                                    <li data-chapter="1">
                                                        <div class="section-name">1: Installing Spark and Setting Up Your Cluster</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="1" class="chapter-section">
                                                                    <a href="#ch01">        
                                                                        <div class="section-name">Chapter 1: Installing Spark and Setting Up Your Cluster</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec08" class="chapter-section">
                                                                    <a href="#ch01lvl1sec08">                    
                                                                        <div class="section-name">Running Spark on a single machine</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec09" class="chapter-section">
                                                                    <a href="#ch01lvl1sec09">                    
                                                                        <div class="section-name">Running Spark on EC2</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec10" class="chapter-section">
                                                                    <a href="#ch01lvl1sec10">                    
                                                                        <div class="section-name">Deploying Spark on Elastic MapReduce</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec11" class="chapter-section">
                                                                    <a href="#ch01lvl1sec11">                    
                                                                        <div class="section-name">Deploying Spark with Chef (opscode)</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec12" class="chapter-section">
                                                                    <a href="#ch01lvl1sec12">                    
                                                                        <div class="section-name">Deploying Spark on Mesos</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec13" class="chapter-section">
                                                                    <a href="#ch01lvl1sec13">                    
                                                                        <div class="section-name">Deploying Spark on YARN</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec14" class="chapter-section">
                                                                    <a href="#ch01lvl1sec14">                    
                                                                        <div class="section-name">Deploying set of machines over SSH</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec15" class="chapter-section">
                                                                    <a href="#ch01lvl1sec15">                    
                                                                        <div class="section-name">Links and references</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec16" class="chapter-section">
                                                                    <a href="#ch01lvl1sec16">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="2">
                                                        <div class="section-name">2: Using the Spark Shell</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="2" class="chapter-section">
                                                                    <a href="#ch02">        
                                                                        <div class="section-name">Chapter 2: Using the Spark Shell</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec17" class="chapter-section">
                                                                    <a href="#ch02lvl1sec17">                    
                                                                        <div class="section-name">Loading a simple text file</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec18" class="chapter-section">
                                                                    <a href="#ch02lvl1sec18">                    
                                                                        <div class="section-name">Using the Spark shell to run logistic regression</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec19" class="chapter-section">
                                                                    <a href="#ch02lvl1sec19">                    
                                                                        <div class="section-name">Interactively loading data from S3</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec20" class="chapter-section">
                                                                    <a href="#ch02lvl1sec20">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="3">
                                                        <div class="section-name">3: Building and Running a Spark Application</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="3" class="chapter-section">
                                                                    <a href="#ch03">        
                                                                        <div class="section-name">Chapter 3: Building and Running a Spark Application</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec21" class="chapter-section">
                                                                    <a href="#ch03lvl1sec21">                    
                                                                        <div class="section-name">Building your Spark project with sbt</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec22" class="chapter-section">
                                                                    <a href="#ch03lvl1sec22">                    
                                                                        <div class="section-name">Building your Spark job with Maven</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec23" class="chapter-section">
                                                                    <a href="#ch03lvl1sec23">                    
                                                                        <div class="section-name">Building your Spark job with something else</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec24" class="chapter-section">
                                                                    <a href="#ch03lvl1sec24">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="4">
                                                        <div class="section-name">4: Creating a SparkContext</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="4" class="chapter-section">
                                                                    <a href="#ch04">        
                                                                        <div class="section-name">Chapter 4: Creating a SparkContext</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec25" class="chapter-section">
                                                                    <a href="#ch04lvl1sec25">                    
                                                                        <div class="section-name">Scala</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec26" class="chapter-section">
                                                                    <a href="#ch04lvl1sec26">                    
                                                                        <div class="section-name">Java</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec27" class="chapter-section">
                                                                    <a href="#ch04lvl1sec27">                    
                                                                        <div class="section-name">Shared Java and Scala APIs</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec28" class="chapter-section">
                                                                    <a href="#ch04lvl1sec28">                    
                                                                        <div class="section-name">Python</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec29" class="chapter-section">
                                                                    <a href="#ch04lvl1sec29">                    
                                                                        <div class="section-name">Links and references</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec30" class="chapter-section">
                                                                    <a href="#ch04lvl1sec30">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="5">
                                                        <div class="section-name">5: Loading and Saving Data in Spark</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="5" class="chapter-section">
                                                                    <a href="#ch05">        
                                                                        <div class="section-name">Chapter 5: Loading and Saving Data in Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec31" class="chapter-section">
                                                                    <a href="#ch05lvl1sec31">                    
                                                                        <div class="section-name">RDDs</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec32" class="chapter-section">
                                                                    <a href="#ch05lvl1sec32">                    
                                                                        <div class="section-name">Loading data into an RDD</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec33" class="chapter-section">
                                                                    <a href="#ch05lvl1sec33">                    
                                                                        <div class="section-name">Saving your data</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec34" class="chapter-section">
                                                                    <a href="#ch05lvl1sec34">                    
                                                                        <div class="section-name">Links and references</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec35" class="chapter-section">
                                                                    <a href="#ch05lvl1sec35">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="6">
                                                        <div class="section-name">6: Manipulating Your RDD</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="6" class="chapter-section">
                                                                    <a href="#ch06">        
                                                                        <div class="section-name">Chapter 6: Manipulating Your RDD</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec36" class="chapter-section">
                                                                    <a href="#ch06lvl1sec36">                    
                                                                        <div class="section-name">Manipulating your RDD in Scala and Java</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec37" class="chapter-section">
                                                                    <a href="#ch06lvl1sec37">                    
                                                                        <div class="section-name">Manipulating your RDD in Python</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec38" class="chapter-section">
                                                                    <a href="#ch06lvl1sec38">                    
                                                                        <div class="section-name">Links and references</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec39" class="chapter-section">
                                                                    <a href="#ch06lvl1sec39">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="7">
                                                        <div class="section-name">7: Shark – Using Spark with Hive</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="7" class="chapter-section">
                                                                    <a href="#ch07">        
                                                                        <div class="section-name">Chapter 7: Shark – Using Spark with Hive</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec40" class="chapter-section">
                                                                    <a href="#ch07lvl1sec40">                    
                                                                        <div class="section-name">Why Hive/Shark?</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec41" class="chapter-section">
                                                                    <a href="#ch07lvl1sec41">                    
                                                                        <div class="section-name">Installing Shark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec42" class="chapter-section">
                                                                    <a href="#ch07lvl1sec42">                    
                                                                        <div class="section-name">Running Shark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec43" class="chapter-section">
                                                                    <a href="#ch07lvl1sec43">                    
                                                                        <div class="section-name">Loading data</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec44" class="chapter-section">
                                                                    <a href="#ch07lvl1sec44">                    
                                                                        <div class="section-name">Using Hive queries in a Spark program</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec45" class="chapter-section">
                                                                    <a href="#ch07lvl1sec45">                    
                                                                        <div class="section-name">Links and references</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec46" class="chapter-section">
                                                                    <a href="#ch07lvl1sec46">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="8">
                                                        <div class="section-name">8: Testing</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="8" class="chapter-section">
                                                                    <a href="#ch08">        
                                                                        <div class="section-name">Chapter 8: Testing</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec47" class="chapter-section">
                                                                    <a href="#ch08lvl1sec47">                    
                                                                        <div class="section-name">Testing in Java and Scala</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec48" class="chapter-section">
                                                                    <a href="#ch08lvl1sec48">                    
                                                                        <div class="section-name">Testing in Python</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec49" class="chapter-section">
                                                                    <a href="#ch08lvl1sec49">                    
                                                                        <div class="section-name">Links and references</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec50" class="chapter-section">
                                                                    <a href="#ch08lvl1sec50">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="9">
                                                        <div class="section-name">9: Tips and Tricks</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="9" class="chapter-section">
                                                                    <a href="#ch09">        
                                                                        <div class="section-name">Chapter 9: Tips and Tricks</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec51" class="chapter-section">
                                                                    <a href="#ch09lvl1sec51">                    
                                                                        <div class="section-name">Where to find logs?</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec52" class="chapter-section">
                                                                    <a href="#ch09lvl1sec52">                    
                                                                        <div class="section-name">Concurrency limitations</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec53" class="chapter-section">
                                                                    <a href="#ch09lvl1sec53">                    
                                                                        <div class="section-name">Memory usage and garbage collection</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec54" class="chapter-section">
                                                                    <a href="#ch09lvl1sec54">                    
                                                                        <div class="section-name">Serialization</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec55" class="chapter-section">
                                                                    <a href="#ch09lvl1sec55">                    
                                                                        <div class="section-name">IDE integration</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec56" class="chapter-section">
                                                                    <a href="#ch09lvl1sec56">                    
                                                                        <div class="section-name">Using Spark with other languages</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec57" class="chapter-section">
                                                                    <a href="#ch09lvl1sec57">                    
                                                                        <div class="section-name">A quick note on security</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec58" class="chapter-section">
                                                                    <a href="#ch09lvl1sec58">                    
                                                                        <div class="section-name">Mailing lists</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec59" class="chapter-section">
                                                                    <a href="#ch09lvl1sec59">                    
                                                                        <div class="section-name">Links and references</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec60" class="chapter-section">
                                                                    <a href="#ch09lvl1sec60">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="backindex">
                                                        <div class="section-name">Appendix A: Index</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="backindex" class="chapter-section">
                                                                    <a href="#backindex">
                                                                        <div class="section-name">Chapter Appendix A: Index</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                            </ul>
                                        </div>
                                        
                                        <div role="tabpanel" class="tab-pane fade" id="author">
                                            <div class="visible-xs">
                                                <h4 class="mobile-title">About the Author</h4>
                                                <hr>
                                            </div>
                                            <p><strong>Holden Karau</strong></p>
                                            <div>
                                                <p>Holden Karau is a software development engineer and is active in the open source. She has worked on a variety of search, classification, and distributed systems problems at IBM, Alpine, Databricks, Google, Foursquare, and Amazon. She graduated from the University of Waterloo with a bachelor's of mathematics degree in computer science. Other than software, she enjoys playing with fire and hula hoops, and welding.</p>
                                            </div>
                                        </div>
                                        
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    
                    <div class="next-wrapper">
                        <div class="row ns">
                            <hr />
                            <span class="hidden-xs">
                                <h4 class="pull-left">
                                    <strong>Up Next: </strong><span class="section-title"></span>
                                </h4>
                                <a href="#" class="btn btn-primary pull-right btn-lg">
                                    Next Section
                                </a>
                            </span>
                            <span class="visible-xs">
                                <a href="#" class="btn btn-primary btn-block btn-lg">
                                    Next Section
                                </a>
                            </span>
                        </div>
                        <div class="row ns">
                            <hr>
                        </div>
                    </div>
                </div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch01"></a>Chapter 1. Installing Spark and Setting Up Your Cluster</h2></div></div></div><p>This chapter will detail some common methods for setting up Spark. Spark on a single machine is excellent for testing, but you will also learn to use Spark's built-in deployment scripts to a dedicated cluster via SSH (Secure Shell). This chapter will also cover using Mesos, Yarn, Puppet, or Chef to deploy Spark. For cloud deployments of Spark, this chapter will look at EC2 (both traditional and EC2MR). Feel free to skip this chapter if you already have your local Spark instance installed and want to get straight to programming.</p><p>Regardless of how you are going to deploy Spark, you will want to get the latest version of Spark from <a class="ulink" href="http://spark-project.org/download" target="_blank">http://spark-project.org/download</a> (Version 0.7 as of this writing). For coders who live dangerously, try cloning the code directly from the repository <a class="ulink" href="http://git://github.com/mesos/spark.git" target="_blank">git://github.com/mesos/spark.git</a>. Both the source code and pre-built binaries are available. To interact with <span class="strong"><strong>Hadoop Distributed File System</strong></span> (<span class="strong"><strong>HDFS</strong></span>), <a id="id0" class="indexterm"></a>you need to use a Spark version that is built against the same version of Hadoop as your cluster. For Version 0.7 of Spark, the pre-built package is built against Hadoop 1.0.4. If you are up for the challenge, it's recommended that you build against the source since it gives you the flexibility of choosing which HDFS version you want to support as well as apply patches. You will need the appropriate version of Scala installed and the matching JDK. For Version 0.7.1 of Spark, you require Scala 2.9.2 or a later 2.9 series release (2.9.3 works well). At the time of this writing, Ubuntu's LTS release (Precise) has Scala Version 2.9.1. Additionally, the current stable version has 2.9.2 and Fedora 18 has 2.9.2. Up-to-date package information can be found at <a class="ulink" href="http://packages.ubuntu.com/search?keywords=scala" target="_blank">http://packages.ubuntu.com/search?keywords=scala</a>. The latest version of Scala is available from <a class="ulink" href="http://scala-lang.org/download" target="_blank">http://scala-lang.org/download</a>. It is important to choose the version of Scala that matches the version requested by Spark, as Scala is a fast-evolving language.</p><p>The tarball file contains a bin directory that needs to be added to your path, and <code class="literal">SCALA_HOME</code> should be set to the path where the tarball file is extracted. Scala can be installed from source by running:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>wget http://www.scala-lang.org/files/archive/scala-2.9.3.tgz &amp;&amp; tar -xvf scala-2.9.3.tgz &amp;&amp; cd scala-2.9.3 &amp;&amp; export PATH=`pwd`/bin:$PATH &amp;&amp; export SCALA_HOME=`pwd`</strong></span>
</pre></div><p>You will probably want to add these to your <code class="literal">.bashrc</code> file or equivalent:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>export PATH=`pwd`/bin:\$PATH</strong></span>
<span class="strong"><strong>export SCALA_HOME=`pwd`</strong></span>
</pre></div><p>Spark is built with <span class="strong"><strong>sbt</strong></span> (<span class="strong"><strong>simple build tool</strong></span>, which is no longer very simple), and build times can be quite long when compiling Scala's source code. Don't worry if you don't have sbt installed; the build script will download the correct version for you.</p><p>On an admittedly under-powered core 2 laptop with an SSD, installing a fresh copy of Spark took about seven minutes. If you decide to build Version 0.7 from source, you would run:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>wget http://www.spark-project.org/download-spark-0.7.0-sources-tgz &amp;&amp; tar -xvf download-spark-0.7.0-sources-tgz &amp;&amp; cd spark-0.7.0 &amp;&amp; sbt/sbt package</strong></span>
</pre></div><p>If you are going to use a version of HDFS that doesn't match the default version for your Spark instance, you will need to edit <code class="literal">project/SparkBuild.scala</code> and set <code class="literal">HADOOP_VERSION</code> to the corresponding version and recompile it with:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sbt/sbt clean compile</strong></span>
</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note02"></a>Note</h3><p>The sbt tool has made great progress with dependency resolution, but it's still strongly recommended for developers to do a clean build rather than an incremental build. This still doesn't get it quite right all the time.</p></div><p>Once you have started the build it's probably a good time for a break, such as getting a cup of coffee. If you find it stuck on a single line that says "Resolving [XYZ]...." for a long time (say five minutes), stop it and restart the <code class="literal">sbt/sbt</code> package.</p><p>If you can live with the restrictions (such as the fixed HDFS version), using the pre-built binary will get you up and running far quicker. To run the pre-built version, use the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>wget http://www.spark-project.org/download-spark-0.7.0-prebuilt-tgz &amp;&amp; tar -xvf download-spark-0.7.0-prebuilt-tgz &amp;&amp; cd spark-0.7.0</strong></span>
</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note03"></a>Note</h3><p>Spark<a id="id1" class="indexterm"></a> has recently become a part of the Apache Incubator. As an application developer who uses Spark, the most visible changes will likely be the eventual renaming of the package to under the <code class="literal">org.apache</code> namespace.</p><p>Some of the useful links for references are as follows:</p><p>
<a class="ulink" href="http://spark-project.org/docs/latest" target="_blank">http://spark-project.org/docs/latest</a>
</p><p>
<a class="ulink" href="http://spark-project.org/download/" target="_blank">http://spark-project.org/download/</a>
</p><p>
<a class="ulink" href="http://www.scala-lang.org" target="_blank">http://www.scala-lang.org</a>
</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec08"></a>Running Spark on a single machine</h2></div></div><hr /></div><p>A single machine is the simplest use case for Spark. It is also a great way to sanity check your build. In the Spark directory, there is a shell script called run that can be used to launch a Spark job. Run takes the name of a Spark class and some arguments. There is a collection of <a id="id2" class="indexterm"></a>sample Spark jobs in <code class="literal">./examples/src/main/scala/spark/examples/</code>.</p><p>All the sample programs take the parameter <code class="literal">master</code>, which can be the URL of a distributed cluster or <code class="literal">local[N]</code>, where <code class="literal">N</code> is the number of threads. To run <code class="literal">GroupByTest</code> locally with four threads, try the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>./run spark.examples.GroupByTest local[4]</strong></span>
</pre></div><p>If you get an error, as <code class="literal">SCALA_HOME</code> is not set, make sure your <code class="literal">SCALA_HOME</code> is set correctly. In bash, you can do this using the export <code class="literal">SCALA_HOME=[pathyouextractedscalato]</code>.</p><p>If you get the following error, it is likely you are using Scala 2.10, which is not supported by Spark 0.7:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[literal]"Exception in thread "main" java.lang.NoClassDefFoundError: scala/reflect/ClassManifest"[/literal]</strong></span>
</pre></div><p>The Scala developers decided to rearrange some classes between 2.9 and 2.10 versions. You can either downgrade your version of Scala or see if the development build of Spark is ready to be built along with Scala 2.10.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec09"></a>Running Spark on EC2</h2></div></div><hr /></div><p>There are many handy scripts to <a id="id3" class="indexterm"></a>run Spark on EC2 in the <code class="literal">ec2</code> directory. These scripts can be used to run multiple Spark clusters, and even run on-the-spot instances. Spark <a id="id4" class="indexterm"></a>can also be run on Elastic MapReduce (EMR). This is Amazon's solution for MapReduce cluster management, which gives you more flexibility around scaling instances.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec07"></a>Running Spark on EC2 with the scripts</h3></div></div></div><p>To get started, you should make sure that you have EC2 enabled on your account by signing up for it at <a class="ulink" href="https://portal.aws.amazon.com/gp/aws/manageYourAccount" target="_blank">https://portal.aws.amazon.com/gp/aws/manageYourAccount</a>. It is a good idea to generate a separate access key pair for your Spark cluster, which you can do at <a class="ulink" href="https://portal.aws.amazon.com/gp/aws/securityCredentialsR" target="_blank">https://portal.aws.amazon.com/gp/aws/securityCredentialsR</a>. You will also need to create an EC2 key pair, so that the Spark script can SSH to the launched machines; this can be done at <a class="ulink" href="https://console.aws.amazon.com/ec2/home" target="_blank">https://console.aws.amazon.com/ec2/home</a> by selecting <span class="strong"><strong>Key Pairs</strong></span> under <span class="strong"><strong>Network &amp; Security</strong></span>. Remember that key pairs are created "per region", so you need to <a id="id5" class="indexterm"></a>make sure you create your key pair in the same region as you intend to run your spark instances. Make sure to give it a name that you can remember (we will use <code class="literal">spark-keypair</code> in this chapter as its example key pair name) as you will need it for the scripts. You can also choose to upload your public SSH key instead of generating a new key. These are sensitive, so make sure that you keep them private. You also need to set your <code class="literal">AWS_ACCESS_KEY</code> and <code class="literal">AWS_SECRET_KEY</code> key pairs as environment variables for the Amazon EC2 scripts:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>chmod 400 spark-keypair.pem</strong></span>
<span class="strong"><strong>export AWS_ACCESS_KEY="..."</strong></span>
<span class="strong"><strong>export AWS_SECRET_KEY="..."</strong></span>
</pre></div><p>You will find it useful to download the EC2 scripts provided by Amazon from <a class="ulink" href="http://aws.amazon.com/developertools/Amazon-EC2/351" target="_blank">http://aws.amazon.com/developertools/Amazon-EC2/351</a>. Once you unzip the resulting ZIP file, you can add the <code class="literal">bin</code> folder to your <code class="literal">PATH</code> variable in a similar manner to what you did with the Spark <code class="literal">bin</code> folder:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>wget http://s3.amazonaws.com/ec2-downloads/ec2-api-tools.zip</strong></span>
<span class="strong"><strong>unzip ec2-api-tools.zip</strong></span>
<span class="strong"><strong>cd ec2-api-tools-*</strong></span>
<span class="strong"><strong>export EC2_HOME=`pwd`</strong></span>
<span class="strong"><strong>export PATH=$PATH:`pwd`:/bin</strong></span>
</pre></div><p>The Spark EC2 script automatically creates a separate security group and firewall rules for the running Spark cluster. By default your Spark cluster will be universally accessible on port 8080, which is somewhat a poor form. Sadly, the <code class="literal">spark_ec2.py</code> script does not currently provide an easy way to restrict access to just your host. If you have a static IP address, I strongly recommend limiting the access in <code class="literal">spark_ec2.py</code>; simply replace all instances <code class="literal">0.0.0.0/0</code> with <code class="literal">[yourip]/32</code>. This will not affect intra-cluster communication, as all machines within a security group can talk to one another by default.</p><p>Next, try to launch a cluster on EC2:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>./ec2/spark-ec2 -k spark-keypair -i pk-[....].pem -s 1 launch myfirstcluster</strong></span>
</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note04"></a>Note</h3><p>If you get an error message such as "The requested Availability Zone is currently constrained and....", you can specify a different zone by passing in the <code class="literal">--zone</code> flag.</p></div><p>If you get an error <a id="id6" class="indexterm"></a>about not being able to SSH to the master, make sure that only you have permission to read the private key, otherwise SSH will refuse to use it.</p><p>You may also encounter this error due to a race condition when the hosts report themselves as alive, but the Spark<code class="literal">-ec2</code> script cannot yet SSH to them. There is a fix for this issue pending in <a class="ulink" href="https://github.com/mesos/spark/pull/555" target="_blank">https://github.com/mesos/spark/pull/555</a>. For now a temporary workaround, until the fix is available in the version of Spark you are using, is to simply let the cluster sleep an extra 120 seconds at the start of <code class="literal">setup_cluster</code>.</p><p>If you do get a transient error when launching a cluster, you can finish the launch process using the resume feature by running:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>./ec2/spark-ec2 -i ~/spark-keypair.pem launch myfirstsparkcluster --resume</strong></span>
</pre></div><p>If everything goes ok, you should see something like the following screenshot:</p><div class="mediaobject"><img src="graphics/7068OS_01_01.jpg" /></div><p>This will give you a bare-bones cluster with one master and one worker, with all the defaults on the default machine instance size. Next, verify that it has started up, and if your firewall rules were applied by going to the master on port 8080. You can see in the preceding <a id="id7" class="indexterm"></a>screenshot that the name of the master is output at the end of the script.</p><p>Try running one of the example's jobs on your new cluster to make sure everything is ok:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sparkuser@h-d-n:~/repos/spark$ ssh -i ~/spark-keypair.pem root@ec2-107-22-48-231.compute-1.amazonaws.com</strong></span>
<span class="strong"><strong>Last login: Sun Apr  7 03:00:20 2013 from 50-197-136-90-static.hfc.comcastbusiness.net</strong></span>
<span class="strong"><strong>       __|  __|_  )</strong></span>
<span class="strong"><strong>       _|  (     /   Amazon Linux AMI</strong></span>
<span class="strong"><strong>      ___|\___|___|</strong></span>

<span class="strong"><strong>https://aws.amazon.com/amazon-linux-ami/2012.03-release-notes/</strong></span>
<span class="strong"><strong>There are 32 security update(s) out of 272 total update(s) available</strong></span>
<span class="strong"><strong>Run "sudo yum update" to apply all updates.</strong></span>
<span class="strong"><strong>Amazon Linux version 2013.03 is available.</strong></span>
<span class="strong"><strong>[root@domU-12-31-39-16-B6-08 ~]# ls</strong></span>
<span class="strong"><strong>ephemeral-hdfs  hive-0.9.0-bin  mesos  mesos-ec2  persistent-hdfs  scala-2.9.2  shark-0.2  spark  spark-ec2</strong></span>
<span class="strong"><strong>[root@domU-12-31-39-16-B6-08 ~]# cd spark</strong></span>
<span class="strong"><strong>[root@domU-12-31-39-16-B6-08 spark]# ./run spark.examples.GroupByTest spark://`hostname`:7077</strong></span>
<span class="strong"><strong>13/04/07 03:11:38 INFO slf4j.Slf4jEventHandler: Slf4jEventHandler started</strong></span>
<span class="strong"><strong>13/04/07 03:11:39 INFO storage.BlockManagerMaster: Registered BlockManagerMaster Actor</strong></span>
<span class="strong"><strong>....</strong></span>
<span class="strong"><strong>13/04/07 03:11:50 INFO spark.SparkContext: Job finished: count at GroupByTest.scala:35, took 1.100294766 s</strong></span>
<span class="strong"><strong>2000</strong></span>
</pre></div><p>Now that you've run a simple job on our EC2 cluster, it's time to configure your EC2 cluster for our Spark jobs. There are a number of options you can use to configure with the Spark-<code class="literal">ec2</code> script.</p><p>First, consider what instance types you may need. EC2 offers an ever-growing collection of instance types, and you can choose a different instance type for the master and the workers. The instance type has the most obvious impact on the performance of your spark cluster. If your work needs a lot of RAM, you should choose an instance with more RAM. You can specify the instance type with <code class="literal">--instance-type=(name of instance type)</code>. By default, the same instance type will be used for both the master and the workers. This can be wasteful if your computations are particularly intensive and the master isn't being heavily utilized. You can specify a different master instance type with <code class="literal">--master-instance-type=(name of instance)</code>.</p><p>EC2 also has GPU<a id="id8" class="indexterm"></a> instance types that can be useful for workers, but would be completely wasted on the master. This text will cover working with Spark and GPUs later on; however, it is important to note that EC2 GPU performance may be lower than what you get while testing locally, due to the higher I/O overhead imposed by the hypervisor.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip02"></a>Tip</h3><p>
<span class="strong"><strong>Downloading the example code</strong></span>
</p><p>All of the example code from this book is hosted in three separate github repos:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<a class="ulink" href="https://github.com/holdenk/fastdataprocessingwithspark-sharkexamples" target="_blank">https://github.com/holdenk/fastdataprocessingwithspark-sharkexamples</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="https://github.com/holdenk/fastdataprocessingwithsparkexamples" target="_blank">https://github.com/holdenk/fastdataprocessingwithsparkexamples</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="https://github.com/holdenk/chef-cookbook-spark" target="_blank">https://github.com/holdenk/chef-cookbook-spark</a>
</p></li></ul></div></div><p>Spark's EC2 scripts uses <a id="id9" class="indexterm"></a>
<span class="strong"><strong>AMI</strong></span> (<span class="strong"><strong>Amazon Machine Images</strong></span>) provided by the Spark team. These AMIs may not always be up-to-date with the latest version of Spark, and if you have custom patches (such as using a different version of HDFS) for Spark, they will not be included in the machine image. At present, the AMIs are also only available in the U.S. East region, so if you want to run it in a different region you will need to copy the AMIs or make your own AMIs in a different region.</p><p>To use Spark's EC2 scripts, you need to have an AMI available in your region. To copy the default Spark EC2 AMI to a new region, figure out what the latest Spark AMI is by looking at the <code class="literal">spark_ec2.py</code> script and seeing what URL the <code class="literal">LATEST_AMI_URL</code> points to and fetch it. For Spark 0.7, run the following command to get the latest AMI:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>curl https://s3.amazonaws.com/mesos-images/ids/latest-spark-0.7</strong></span>
</pre></div><p>There is an ec2-copy-image script that you would hope provides the ability to copy the image, but sadly it doesn't work on images that you don't own. So you will need to launch an instance of the preceding AMI and snapshot it. You can describe the current image by running:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>ec2-describe-images ami-a60193cf</strong></span>
</pre></div><p>This should show you that it is an <span class="strong"><strong>EBS</strong></span>-based (<span class="strong"><strong>Elastic Block Store)</strong></span> image, so you will need to follow EC2's instructions for creating EBS-based instances. Since you already have a script to launch the instance, you can just start an instance on an EC2 cluster and then snapshot it. You can find the instances you are running with:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>ec2-describe-instances -H</strong></span>
</pre></div><p>You can copy the <code class="literal">i-[string]</code> instance name and save it for later use.</p><p>If you wanted to use a custom version of Spark or install any other tools or dependencies and have them available as part of our AMI, you should do that (or at least update the instance) before snapshotting.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>ssh -i ~/spark-keypair.pem root@[hostname] "yum update"</strong></span>
</pre></div><p>Once you have your updates installed and any other customizations you want, you can go ahead and snapshot your instance with:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>ec2-create-image -n "My customized Spark Instance" i-[instancename]</strong></span>
</pre></div><p>With the AMI<a id="id10" class="indexterm"></a> name from the preceding code, you can launch your customized version of Spark by specifying the <code class="literal">[cmd]--ami[/cmd]</code> command-line argument. You can also copy this image to another region for use there:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>ec2-copy-image -r [source-region] -s [ami] --region [target region]</strong></span>
</pre></div><p>This will give you a new AMI name, which you can use for launching your EC2 tasks. If you want to use a different AMI name, simply specify <code class="literal">--ami [aminame]</code>.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note05"></a>Note</h3><p>As of this writing, there was an issue with the default AMI and HDFS. You may need to update the version of Hadoop on the AMI, as it does not match the version that Spark was compiled for. You can refer to <a class="ulink" href="https://spark-project.atlassian.net/browse/SPARK-683" target="_blank">https://spark-project.atlassian.net/browse/SPARK-683</a> for details.</p></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec10"></a>Deploying Spark on Elastic MapReduce</h2></div></div><hr /></div><p>In addition to <a id="id11" class="indexterm"></a>Amazon's basic EC2 machine offering, Amazon offers a hosted MapReduce solution called Elastic MapReduce. Amazon provides a bootstrap script that simplifies the process of <a id="id12" class="indexterm"></a>getting started using Spark on EMR. You can install the EMR tools from Amazon using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>mkdir emr &amp;&amp; cd emr &amp;&amp; wget http://elasticmapreduce.s3.amazonaws.com/elastic-mapreduce-ruby.zip &amp;&amp; unzip *.zip</strong></span>
</pre></div><p>So that the EMR scripts can access your AWS account, you will want to create a <code class="literal">credentials.json</code> file:</p><div class="informalexample"><pre class="programlisting">{
    "access-id": "&lt;Your AWS access id here&gt;",
    "private-key": "&lt;Your AWS secret access key here&gt;",
    "key-pair": "&lt;The name of your ec2 key-pair here&gt;",
    "key-pair-file": "&lt;path to the .pem file for your ec2 key pair here&gt;",
    "region": "&lt;The region where you wish to launch your job flows (e.g us-east-1)&gt;"
}</pre></div><p>Once you have the EMR tools installed, you can launch a Spark cluster by running:</p><div class="informalexample"><pre class="programlisting">elastic-mapreduce --create --alive --name "Spark/Shark Cluster" \--bootstrap-action s3://elasticmapreduce/samples/spark/install-spark-shark.sh \--bootstrap-name "install Mesos/Spark/Shark" \--ami-version 2.0  \--instance-type m1.large --instance-count 2</pre></div><p>This will give you a running <a id="id13" class="indexterm"></a>EC2MR instance after about five to ten minutes. You can list the status<a id="id14" class="indexterm"></a> of the cluster by running <code class="literal">elastic-mapreduce --list</code>. Once it outputs <code class="literal">j-[jobid]</code>, it is ready.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note06"></a>Note</h3><p>Some of the useful links that you can refer to are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<a class="ulink" href="http://aws.amazon.com/articles/4926593393724923" target="_blank">http://aws.amazon.com/articles/4926593393724923</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-cli-install.html" target="_blank">http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-cli-install.html</a>
</p></li></ul></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec11"></a>Deploying Spark with Chef (opscode)</h2></div></div><hr /></div><p>Chef is an <a id="id15" class="indexterm"></a>open source automation platform that has become increasingly popular for deploying and managing both small and large clusters of machines. Chef can be used to control a traditional static fleet of machines, but can also be used with EC2 and other cloud providers. Chef <a id="id16" class="indexterm"></a>uses cookbooks as the basic building blocks of configuration and can either be generic or site specific. If you have not used Chef before, a good tutorial for getting started with Chef can be found at <a class="ulink" href="https://learnchef.opscode.com/" target="_blank">https://learnchef.opscode.com/</a>. You can use a generic Spark cookbook as the basis for setting up your cluster.</p><p>To get Spark working, you need to create a role for both the master and the workers, as well as configure the workers to connect to the master. Start by getting the cookbook from <a class="ulink" href="https://github.com/holdenk/chef-cookbook-spark" target="_blank">https://github.com/holdenk/chef-cookbook-spark</a>. The bare minimum is setting the master hostname as master (so the worker nodes can connect) and the username so that Chef can install in the correct place. You will also need to either accept Sun's Java license or switch to an alternative JDK. Most of the settings that are available in <code class="literal">spark-env.sh</code> are also exposed through the cookbook's settings. You can see an explanation of the settings on configuring multiple hosts over SSH in the <span class="emphasis"><em>Set of machines over SSH</em></span> section. The settings can be set per-role or you can modify the global defaults:</p><p>To create a role for the master with knife role, create <code class="literal">spark_master_role -e [editor]</code>. This will bring up a template role file that you can edit. For a simple master, set it to:</p><div class="informalexample"><pre class="programlisting">{
  "name": "spark_master_role",

  "description": "",
  "json_class": "Chef::Role",

  "default_attributes": {
    },
  "override_attributes": {
   "username":"spark",
   "group":"spark",
   "home":"/home/spark/sparkhome",
   "master_ip":"10.0.2.15",
  },
  "chef_type": "role",
  "run_list": [
    "recipe[spark::server]",
    "recipe[chef-client]",
  ],
  "env_run_lists": {
    },
}</pre></div><p>Then create a <a id="id17" class="indexterm"></a>role for the client in the same manner except instead of <code class="literal">spark::server</code>, use the <code class="literal">spark::client</code> recipe. Deploy the roles to the different hosts:</p><div class="informalexample"><pre class="programlisting">knife node run_list add master role[spark_master_role]
knife node run_list add worker role[spark_worker_role]</pre></div><p>Then run <code class="literal">chef-client</code> <a id="id18" class="indexterm"></a>on your nodes to update. Congrats, you now have a Spark cluster running!</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec12"></a>Deploying Spark on Mesos</h2></div></div><hr /></div><p>Mesos is a cluster management platform for running multiple distributed applications or frameworks on a cluster. <a id="id19" class="indexterm"></a>Mesos can intelligently schedule and run Spark, Hadoop, and other frameworks concurrently on the same cluster. Spark can be run on Mesos either by scheduling individual jobs as separate Mesos tasks or running all of Spark as a single <a id="id20" class="indexterm"></a>Mesos task. Mesos can quickly scale up to handle large clusters, beyond the size of which you would want to manage, with plain old SSH scripts. It was originally created at UC Berkley as a research project; it is currently undergoing Apache incubation and is actively used by Twitter.</p><p>To get started with Mesos, you can download the latest version from <a class="ulink" href="http://mesos.apache.org/downloads/" target="_blank">http://mesos.apache.org/downloads/</a> and unpack the ZIP files. Mesos has a number of different configuration scripts you can use; for an Ubuntu installation use <code class="literal">configure.ubuntu-lucid-64</code>, and for other cases the Mesos <code class="literal">README</code> file will point you at which configuration file to use. In addition to the requirements of Spark, you will need to ensure that you have the Python C header files installed (<code class="literal">python-dev</code> on Debian systems) or pass <code class="literal">--disable-python</code> to the configured script. Since Mesos needs to be installed on all the <a id="id21" class="indexterm"></a>machines, you may find it easier to configure Mesos to install somewhere other than the root, most easily alongside your Spark installation as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>./configure --prefix=/home/sparkuser/mesos &amp;&amp; make &amp;&amp; make check &amp;&amp; make install</strong></span>
</pre></div><p>Much like with the configuration of Spark in standalone mode with Mesos, you need to make sure the different Mesos nodes can find one another. Start with adding <code class="literal">mesossprefix/var/mesos/deploy/masters</code> to the hostname of the master, and then adding each worker <a id="id22" class="indexterm"></a>hostname to <code class="literal">mesossprefix/var/mesos/deploy/slaves</code>. Then you will want to point the workers at the master (and possibly set some other values) in <code class="literal">mesossprefix/var/mesos/conf/mesos.conf</code>.</p><p>Once you have Mesos built, it's time to configure Spark to work with Mesos. This is as simple as copying the <code class="literal">conf/spark-env.sh.template</code> to <code class="literal">conf/spark-env.sh</code>, and updating <code class="literal">MESOS_NATIVE_LIBRARY</code> to point to the path where Mesos is installed. You can find more information about the different settings in <code class="literal">spark-env.sh</code> in the table shown in the next section.</p><p>You will need to install both Mesos on Spark on all the machines in your cluster. Once both Mesos and Spark are configured, you can copy the build to all the machines using <code class="literal">pscp</code> as shown in the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>pscp -v -r -h  -l sparkuser ./mesos /home/sparkuser/mesos</strong></span>
</pre></div><p>You can then start your Mesos clusters by using <code class="literal">mesosprefix/sbin/mesos-start-cluster.sh</code>, and schedule your Spark on Mesos by using <code class="literal">mesos://[host]:5050</code> as the master.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec13"></a>Deploying Spark on YARN</h2></div></div><hr /></div><p>YARN is Apache <a id="id23" class="indexterm"></a>Hadoop's NextGen MapReduce. The Spark project provides an easy way to schedule jobs on YARN once you have a Spark assembly built. It is important that the Spark job you create uses a standalone master URL. The example Spark <a id="id24" class="indexterm"></a>applications all read the master URL from the command-line arguments, so specify <code class="literal">--args</code> standalone.</p><p>To run the same example as in the SSH section, do the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sbt/sbt assembly #Build the assembly</strong></span>
<span class="strong"><strong>SPARK_JAR=./core/target/spark-core-assembly-0.7.0.jar ./run spark.deploy.yarn.Client --jar examples/target/scala-2.9.2/spark-examples_2.9.2-0.7.0.jar --class spark.examples.GroupByTest --args standalone --num-workers 2 --worker-memory 1g --worker-cores 1</strong></span>
</pre></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec14"></a>Deploying set of machines over SSH</h2></div></div><hr /></div><p>If you have a set of machines without any existing cluster management software, you can deploy Spark over SSH with some handy scripts. This method is known as "standalone mode"<a id="id25" class="indexterm"></a> in the Spark documentation. An individual master and worker can be started by <code class="literal">./run spark.deploy.master.Master</code> and <code class="literal">./run spark.deploy.worker.Worker</code> <code class="literal">spark://MASTERIP:PORT</code> respectively. The default port for the master is 8080. It's likely that you don't want to go to each of your machines and run these commands by hand; there are a number of helper scripts in <code class="literal">bin/</code> to help you run your servers.</p><p>A prerequisite for using <a id="id26" class="indexterm"></a>any of the scripts is having a password-less SSH access setup from the master to all the worker machines. You probably want to create a new user for running Spark on the machines and lock it down. This book uses the username <code class="literal">sparkuser</code>. On your master machine, you can run <code class="literal">ssh-keygen</code> to generate the SSH key and make sure that you do not set a password. Once you have generated the key, add the public one (if you generated an RSA key it would be stored in <code class="literal">~/.ssh/id_rsa.pub</code> by default) to <code class="literal">~/.ssh/authorized_keys2</code> on each of the hosts.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note07"></a>Note</h3><p>The Spark administration scripts require that your username matches. If this isn't the case, you can configure an alternative username in your <code class="literal">~/.ssh/config</code>.</p></div><p>Now that you have SSH access to the machines set up, it is time to configure Spark. There is a simple template in <code class="literal">[filepath]conf/spark-env.sh.template[/filepath]</code> that you should copy to <code class="literal">[filepath]conf/spark-env.sh[/filepath]</code>. You will need to set the <code class="literal">SCALA_HOME</code> variable to the path where you extracted Scala to. You may also find it useful to set some (or all) of the following environment variables:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Name</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Purpose</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Default</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>
<code class="literal">MESOS_NATIVE_LIBRARY</code>
<a id="id27" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>Point to match where Mesos is located</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">None</code>
</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">SCALA_HOME</code>
<a id="id28" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>Point to where you extracted Scala</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">None</code>, must be set</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">SPARK_MASTER_IP</code>
<a id="id29" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>The IP address for the master to listen on and the IP address for the workers to connect to port #</p>
</td><td style="" align="left" valign="top">
<p>The result of running hostname</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">SPARK_MASTER_PORT</code>
<a id="id30" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>The port # for the Spark master to listen on</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">7077</code>
</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">SPARK_MASTER_WEBUI_PORT</code>
<a id="id31" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>The port # of the web UI on the master</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">8080</code>
</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">SPARK_WORKER_CORES</code>
<a id="id32" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>The number of cores to use</p>
</td><td style="" align="left" valign="top">
<p>All of them</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">SPARK_WORKER_MEMORY</code>
<a id="id33" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>The amount of memory to use</p>
</td><td style="" align="left" valign="top">
<p>Max of system memory - (minus) 1 GB (or 512 MB)</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">SPARK_WORKER_PORT</code>
<a id="id34" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>The port # on which the worker runs on</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">random</code>
</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">SPARK_WEBUI_PORT</code>
<a id="id35" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>The port # on which the worker web UI runs on</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">8081</code>
</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">SPARK_WORKER_DIR</code>
<a id="id36" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>The location where to store files from the worker</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">SPARK_HOME/work_dir</code>
</p>
</td></tr></tbody></table></div><p>Once you have your configuration all done, it's time to get your cluster up and running. You will want to copy the version of Spark and the configurations you have built to all of your machines. You may find it useful to install PSSH, a set of parallel SSH tools including PCSP. The PSCP application makes it easy to SCP (securely copy files) to a number of target hosts, although it will take a while, such as:</p><div class="informalexample"><pre class="programlisting">pscp -v -r -h conf/slaves -l sparkuser ../spark-0.7.0 ~/</pre></div><p>If you end up <a id="id37" class="indexterm"></a>changing the configuration, you need to distribute the configuration to all the workers, such as:</p><div class="informalexample"><pre class="programlisting">pscp -v -r -h conf/slaves -l sparkuser conf/spark-env.sh ~/spark-0.7.0/conf/spark-env.sh</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note08"></a>Note</h3><p>If you use a shared NFS on your cluster—although by default Spark names logfiles and similar with the shared names—you should configure a separate worker directory otherwise they will be configured to write to the same place. If you want to have your worker directories on the shared NFS, consider adding `<code class="literal">hostname</code>`, for example, <code class="literal">SPARK_WORKER_DIR=~/work-`hostname`</code>.</p></div><p>You should also consider having your logfiles go to a scratch directory for better performance.</p><p>If you don't have Scala installed on the remote machines yet, you can also use <code class="literal">pssh</code> to set it up:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>pssh -P -i -h conf/slaves -l sparkuser "wget http://www.scala-lang.org/downloads/distrib/files/scala-2.9.3.tgz &amp;&amp; tar -xvf scala-2.9.3.tgz &amp;&amp; cd scala-2.9.3 &amp;&amp; export PATH=$PATH:`pwd`/bin &amp;&amp; export SCALA_HOME=`pwd` &amp;&amp; echo \"export PATH=`pwd`/bin:\\\\$PATH &amp;&amp; export SCALA_HOME=`pwd`\" &gt;&gt; ~/.bashrc" </strong></span>
</pre></div><p>Now you are ready to start the cluster. It is important to note that <code class="literal">start-all</code> and <code class="literal">start-master</code> both assume they are being run on the node, which is the master for the cluster. The start scripts all daemonize, so you don't have to worry about running them in a screen.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>ssh master bin/start-all.sh</strong></span>
</pre></div><p>If you get <a id="id38" class="indexterm"></a>a class not found error, such as <code class="literal">java.lang.NoClassDefFoundError: scala/ScalaObject</code>, check to make sure that you have Scala installed on that worker host and that the <code class="literal">SCALA_HOME</code> is set correctly.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note09"></a>Note</h3><p>The Spark scripts assume that your master has Spark installed as the same directory as your workers. If this is not the case, you should edit <code class="literal">bin/spark-config.sh</code> and set it to the appropriate directories.</p></div><p>The commands provided by Spark to help you administer your cluster are in the following table:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Command</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Use</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>
<code class="literal">bin/slaves.sh &lt;command&gt;</code>
<a id="id39" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>Runs the provided command on all the worker hosts. For example, <code class="literal">bin/slave.sh</code> uptime will show how long each of the worker hosts have been up.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">bin/start-all.sh</code>
<a id="id40" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>Starts the master and all the worker hosts. It must be run on the master.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">bin/start-master.sh</code>
<a id="id41" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>Starts the master host. Must be run on the master.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">bin/start-slaves.sh</code>
<a id="id42" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>Starts the worker hosts.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">bin/start-slave.sh</code>
<a id="id43" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>Start a specific worker.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">bin/stop-all.sh</code>
<a id="id44" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>Stops master and workers.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">bin/stop-master.sh</code>
<a id="id45" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>Stops the master.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">bin/stop-slaves.sh</code>
<a id="id46" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>Stops all the workers.</p>
</td></tr></tbody></table></div><p>You now have a running Spark cluster, as shown in the following screenshot. There is a handy web UI on the<a id="id47" class="indexterm"></a> master running on port 8080; you should visit and switch on all the workers on port 8081. The web UI contains such helpful information as the current workers, and current and past jobs.</p><div class="mediaobject"><img src="graphics/7068OS_01_02.jpg" /></div><p>Now that you have a cluster up and running let's actually do something with it. As with the single host example, you can use the provided run script to run Spark commands. All the examples listed in <code class="literal">examples/src/main/scala/spark/examples/</code> take a parameter, <code class="literal">master</code>, which points them to the master machine. Assuming you are on the master host you could run them like this:</p><div class="informalexample"><pre class="programlisting">./run spark.examples.GroupByTest spark://`hostname`:7077</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note10"></a>Note</h3><p>If you run into an issue with <code class="literal">java.lang.UnsupportedClassVersionError</code>, you may need to update your JDK or recompile Spark if you grabbed the binary version. Version 0.7 was compiled with JDK 1.7 as the target. You can check the version of the JRE targeted by Spark with:</p><div class="informalexample"><pre class="programlisting">java -verbose -classpath ./core/target/scala-2.9.2/classes/
spark.SparkFiles | head -n 20</pre></div><p>Version 49 is JDK1.5, Version 50 is JDK1.6, and Version 60 is JDK1.7.</p></div><p>If you can't connect to the localhost, make sure that you've configured your master to listen to all the IP addresses (or if you don't want to replace the localhost with the IP address configured to listen too).</p><p>If everything has worked correctly, you will see a lot of log messages output to <code class="literal">stdout</code> something along the lines of:</p><div class="informalexample"><pre class="programlisting">13/03/28 06:35:31 INFO spark.SparkContext: Job finished: count at GroupByTest.scala:35, took 2.482816756 s
2000</pre></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec15"></a>Links and references</h2></div></div><hr /></div><p>Some of the <a id="id48" class="indexterm"></a>useful links are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<a class="ulink" href="http://archive09.linux.com/feature/151340" target="_blank">http://archive09.linux.com/feature/151340</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://spark-project.org/docs/latest/spark-standalone.html" target="_blank">http://spark-project.org/docs/latest/spark-standalone.html</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="https://github.com/mesos/spark/blob/master/core/src/main/scala/spark/deploy/worker/WorkerArguments.scala" target="_blank">https://github.com/mesos/spark/blob/master/core/src/main/scala/spark/deploy/worker/WorkerArguments.scala</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://bickson.blogspot.com/2012/10/deploying-graphlabsparkmesos-cluster-on.html" target="_blank">http://bickson.blogspot.com/2012/10/deploying-graphlabsparkmesos-cluster-on.html</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://www.ibm.com/developerworks/library/os-spark/" target="_blank">http://www.ibm.com/developerworks/library/os-spark/</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://mesos.apache.org/" target="_blank">http://mesos.apache.org/</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://aws.amazon.com/articles/Elastic-MapReduce/4926593393724923" target="_blank">http://aws.amazon.com/articles/Elastic-MapReduce/4926593393724923</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://spark-project.org/docs/latest/ec2-scripts.html" target="_blank">http://spark-project.org/docs/latest/ec2-scripts.html</a>
</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec16"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we have installed Spark on our machine for local development and also set up on our cluster, so we are ready to run the applications that we write. In the next chapter, we will learn to use the Spark shell.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch02"></a>Chapter 2. Using the Spark Shell</h2></div></div></div><p>The Spark shell<a id="id49" class="indexterm"></a> is a wonderful tool for rapid prototyping with Spark. It helps to be familiar with Scala, but it isn't necessary when using this tool. The Spark shell allows you to query and interact with the Spark cluster. This can be great for debugging or for just trying things out. The previous chapter should have gotten you to the point of having a Spark instance running, so now all you need to do is start your Spark shell, and point it at your running index with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>MASTER=spark://`hostname`:7077 ./spark-shell</strong></span>
</pre></div><p>If you are running Spark in local mode and don't have a Spark instance already running, you can just run the preceding command without the <code class="literal">MASTER=</code> part. This will run with only one thread, hence to run multiple threads you can specify <code class="literal">local[n]</code>.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec17"></a>Loading a simple text file</h2></div></div><hr /></div><p>When running a <a id="id50" class="indexterm"></a>Spark shell and connecting to an existing cluster, you should see something specifying the <code class="literal">app ID</code> like <code class="literal">Connected to Spark cluster with app ID app-20130330015119-0001</code>. The <code class="literal">app ID</code> will match the application entry as shown in the web UI under running applications (by default, it would be viewable on port 8080). You can start by downloading a dataset to use for some experimentation. There are a number of datasets that are put together for <span class="emphasis"><em>The Elements of Statistical Learning</em></span>, which are in a very convenient form for use. Grab the spam dataset using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>wget http://www-stat.stanford.edu/~tibs/ElemStatLearn/datasets/spam.data</strong></span>
</pre></div><p>Now load it as a text file into Spark with the following command inside your Spark shell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val inFile = sc.textFile("./spam.data")</strong></span>
</pre></div><p>This loads the <code class="literal">spam.data</code> file into Spark with each line being a separate entry in the <span class="strong"><strong>RDD</strong></span> (<span class="strong"><strong>Resilient Distributed Datasets</strong></span>).</p><p>Note that if you've connected to a Spark master, it's possible that it will attempt to load the file on one of the different machines in the cluster, so make sure it's available on all the cluster machines. In general, in future you will want to put your data in HDFS, S3, or similar file systems <a id="id51" class="indexterm"></a>to avoid this problem. In a local mode, you can just load the file directly, for example, <code class="literal">sc.textFile([filepah])</code>. To make a file available across all the machines, you can also use the <code class="literal">addFile</code> function on the SparkContext by writing the following code:</p><div class="informalexample"><pre class="programlisting">scala&gt; import spark.SparkFiles;
scala&gt; val file = sc.addFile("spam.data")
scala&gt; val inFile = sc.textFile(SparkFiles.get("spam.data"))</pre></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip03"></a>Tip</h3><p>Just like most shells, the Spark shell has a command history. You can press the up arrow key to get to the previous commands. Getting tired of typing or not sure what method you want to call on an object? Press <span class="emphasis"><em>Tab</em></span>, and the Spark shell will autocomplete the line of code as best as it can.</p></div><p>For this example, the RDD with each line as an individual string isn't very useful, as our data input is actually represented as space-separated numerical information. Map over the RDD, and quickly convert it to a usable format (note that <code class="literal">_.toDouble</code> is the same as <code class="literal">x =&gt; x.toDouble</code>):</p><div class="informalexample"><pre class="programlisting">scala&gt; val nums = inFile.map(x =&gt; x.split(' ').map(_.toDouble))</pre></div><p>Verify that this is what we want by inspecting some elements in the <code class="literal">nums</code> RDD and comparing them against the original string RDD. Take a look at the first element of each RDD by calling <code class="literal">.first()</code> on the RDDs:</p><div class="informalexample"><pre class="programlisting">scala&gt; inFile.first()
[...]
res2: String = 0 0.64 0.64 0 0.32 0 0 0 0 0 0 0.64 0 0 0 0.32 0 1.29 1.93 0 0.96 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.778 0 0 3.756 61 278 1

scala&gt; nums.first()
[...]
res3: Array[Double] = Array(0.0, 0.64, 0.64, 0.0, 0.32, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.64, 0.0, 0.0, 0.0, 0.32, 0.0, 1.29, 1.93, 0.0, 0.96, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.778, 0.0, 0.0, 3.756, 61.0, 278.0, 1.0)</pre></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec18"></a>Using the Spark shell to run logistic regression</h2></div></div><hr /></div><p>When you run a command and <a id="id52" class="indexterm"></a>have not specified a left-hand side (that is, leaving out the <code class="literal">val x</code> of <code class="literal">val x = y</code>), the Spark shell will print the value along with <code class="literal">res[number]</code>. The <code class="literal">res[number]</code> function can be used as if we had written <code class="literal">val res[number] = y</code>. Now that you have the data in a more usable format, try to do something cool with it! Use Spark to run logistic regression over the dataset as follows:</p><div class="informalexample"><pre class="programlisting">scala&gt; import spark.util.Vector
import spark.util.Vector

scala&gt; case class DataPoint(x: Vector, y: Double)
defined class DataPoint

scala&gt; def parsePoint(x: Array[Double]): DataPoint = {
      DataPoint(new Vector(x.slice(0,x.size-2)) , x(x.size-1))
      }
parsePoint: (x: Array[Double])this.DataPoint

scala&gt; val points = nums.map(parsePoint(_))
points: spark.RDD[this.DataPoint] = MappedRDD[3] at map at &lt;console&gt;:24

scala&gt; import java.util.Random
import java.util.Random

scala&gt; val rand = new Random(53)
rand: java.util.Random = java.util.Random@3f4c24
scala&gt; var w = Vector(nums.first.size-2, _ =&gt; rand.nextDouble)
13/03/31 00:57:30 INFO spark.SparkContext: Starting job: first at &lt;console&gt;:20
...
13/03/31 00:57:30 INFO spark.SparkContext: Job finished: first at &lt;console&gt;:20, took 0.01272858 s
w: spark.util.Vector = (0.7290865701603526, 0.8009687428076777, 0.6136632797111822, 0.9783178194773176, 0.3719683631485643, 0.46409291255379836, 0.5340172959927323, 0.04034252433669905, 0.3074428389716637, 0.8537414030626244, 0.8415816118493813, 0.719935849109521, 0.2431646830671812, 0.17139348575456848, 0.5005137792223062, 0.8915164469396641, 0.7679331873447098, 0.7887571495335223, 0.7263187438977023, 0.40877063468941244, 0.7794519914671199, 0.1651264689613885, 0.1807006937030201, 
0.3227972103818231, 0.2777324549716147, 0.20466985600105037, 0.5823059390134582, 0.4489508737465665, 0.44030858771499415, 0.6419366305419459, 0.5191533842209496, 0.43170678028084863, 0.9237523536173182, 0.5175019655845213, 0.47999523211827544, 0.25862648071479444, 0.020548000101787922, 0.18555332739714137, 0....

scala&gt; val iterations = 100
iterations: Int = 100

scala&gt; import scala.math._

scala&gt; for (i &lt;- 1 to iterations) {
        val gradient = points.map(p =&gt;
          (1 / (1 + exp(-p.y*(w dot p.x))) - 1) * p.y * p.x
        ).reduce(_ + _)
        w -= gradient
      }
[....]

scala&gt; w
res27: spark.util.Vector = (0.2912515190246098, 1.05257972144256, 1.1620192443948825, 0.764385365541841, 1.3340446477767611, 0.6142105091995632, 0.8561985593740342, 0.7221556020229336, 0.40692442223198366, 0.8025693176035453, 0.7013618380649754, 0.943828424041885, 0.4009868306348856, 0.6287356973527756, 0.3675755379524898, 1.2488466496117185, 0.8557220216380228, 0.7633511642942988, 6.389181646047163, 1.43344096405385, 1.729216408954399, 0.4079709812689015, 0.3706358251228279, 0.8683036382227542, 0.36992902312625897, 0.3918455398419239, 0.2840295056632881, 0.7757126171768894, 0.4564171647415838, 0.6960856181900357, 0.6556402580635656, 0.060307680034745986, 0.31278587054264356, 0.9273189009376189, 0.0538302050535121, 0.545536066902774, 0.9298009485403773, 0.922750704590723, 0.072339496591</pre></div><p>If things went well, you just used Spark to run logistic regression. Awsome! We have just done a number of things: we have defined a class, we have created an RDD, and we have also created <a id="id53" class="indexterm"></a>a function. As you can see the Spark shell is quite powerful. Much of the power comes from it being based on the Scala REPL (the Scala interactive shell), so it inherits all the power of the Scala REPL (Read-Evaluate-Print Loop).<a id="id54" class="indexterm"></a> That being said, most of the time you will probably want to work with a more traditionally compiled code rather than working in the REPL environment.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec19"></a>Interactively loading data from S3</h2></div></div><hr /></div><p>Now, let's try a second exercise with the Spark shell. As part of Amazon's EMR Spark support, it has <a id="id55" class="indexterm"></a>provided some handy sample data of Wikipedia's traffic statistics in S3 in the format that Spark can use. To access the data, you first need to set your AWS access credentials as shell's parameters. For instructions on signing up for EC2 and setting up the shell parameters, see the <span class="emphasis"><em>Running Spark on EC2</em></span> with the scripts section in <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Installing Spark and Setting Up Your Cluster</em></span> (S3 access requires additional keys <code class="literal">fs.s3n.awsAccessKeyId/awsSecretAccessKey</code> or using the <code class="literal">s3n://user:pw@</code> syntax). Once that's done, load the S3 data and take a look at the first line:</p><div class="informalexample"><pre class="programlisting">scala&gt; val file = sc.textFile("s3n://bigdatademo/sample/wiki/")
13/04/21 21:26:14 INFO storage.MemoryStore: ensureFreeSpace(37539) called with curMem=37531, maxMem=339585269
13/04/21 21:26:14 INFO storage.MemoryStore: Block broadcast_1 stored as values to memory (estimated size 36.7 KB, free 323.8 MB)
file: spark.RDD[String] = MappedRDD[3] at textFile at &lt;console&gt;:12

scala&gt; file.take(1)
13/04/21 21:26:17 INFO mapred.FileInputFormat: Total input paths to process : 1
...
13/04/21 21:26:17 INFO spark.SparkContext: Job finished: take at &lt;console&gt;:15, took 0.533611079 s
res1: Array[String] = Array(aa.b Pecial:Listusers/sysop 1 4695)</pre></div><p>You don't need to set your AWS credentials as shell's parameters; the general form of the S3<a id="id56" class="indexterm"></a> path is <code class="literal">s3n://&lt;AWS ACCESS ID&gt;:&lt;AWS SECRET&gt;@bucket/path</code>. It's important to take a look at the first line of data because unless we force Spark to materialize something with the data, it won't actually bother to load it. It is useful to note that Amazon provided a small sample dataset to get started with. The data is pulled from a much larger set at <a class="ulink" href="http://aws.amazon.com/datasets/4182" target="_blank">http://aws.amazon.com/datasets/4182</a>. This practice can be quite useful, when developing in interactive mode, since you want the fast feedback of your jobs completing quickly. If your sample data was too big and your executions were taking too long, you could quickly slim down the RDD by using the <code class="literal">sample</code> functionality built into the Spark shell:</p><div class="informalexample"><pre class="programlisting">scala&gt; val seed  = (100*math.random).toInt
seed: Int = 8
scala&gt; file.sample(false,1/10.,seed)
res10: spark.RDD[String] = SampledRDD[4] at sample at &lt;console&gt;:17
//If you wanted to rerun on the sampled data later, you could write it back to S3
scala&gt; res10.saveAsTextFile("s3n://mysparkbucket/test")
13/04/21 22:46:18 INFO spark.PairRDDFunctions: Saving as hadoop file of type (NullWritable, Text)
....
13/04/21 22:47:46 INFO spark.SparkContext: Job finished: saveAsTextFile at &lt;console&gt;:19, took 87.462236222 s</pre></div><p>Now that you have the data loaded, find the most popular articles in a sample. First, parse the data separating it into name and count. Second, as there can be multiple entries with the same name, <a id="id57" class="indexterm"></a>reduce the data by the key summing the counts. Finally, we swap the key/value so that when we sort by key, we get back the highest count item as follows:</p><div class="informalexample"><pre class="programlisting">scala&gt; val parsed = file.sample(false,1/10.,seed).map(x =&gt; x.split(" ")).map(x =&gt; (x(1), x(2).toInt))
parsed: spark.RDD[(java.lang.String, Int)] = MappedRDD[5] at map at &lt;console&gt;:16

scala&gt; val reduced = parsed.reduceByKey(_+_)
13/04/21 23:21:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/04/21 23:21:49 WARN snappy.LoadSnappy: Snappy native library not loaded
13/04/21 23:21:50 INFO mapred.FileInputFormat: Total input paths to process : 1
reduced: spark.RDD[(java.lang.String, Int)] = MapPartitionsRDD[8] at reduceByKey at &lt;console&gt;:18

scala&gt; val countThenTitle = reduced.map(x =&gt; (x._2, x._1))
countThenTitle: spark.RDD[(Int, java.lang.String)] = MappedRDD[9] at map at &lt;console&gt;:20

scala&gt; countThenTitle.sortByKey(false).take(10)
13/04/21 23:22:08 INFO spark.SparkContext: Starting job: take at &lt;console&gt;:23
....
13/04/21 23:23:15 INFO spark.SparkContext: Job finished: take at &lt;console&gt;:23, took 66.815676564 s
res1: Array[(Int, java.lang.String)] = Array((213652,Main_Page), (14851,Special:Search), (9528,Special:Export/Can_You_Hear_Me), (6454,Wikipedia:Hauptseite), (4189,Special:Watchlist), (3520,%E7%89%B9%E5%88%A5:%E3%81%8A%E3%81%BE%E3%81%8B%E3%81%9B%E8%A1%A8%E7%A4%BA), (2857,Special:AutoLogin), (2416,P%C3%A1gina_principal), (1990,Survivor_(TV_series)), (1953,Asperger_syndrome))</pre></div><p>You can also work with Spark interactively in Python by running <code class="literal">./pyspark</code>.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec20"></a>Summary</h2></div></div><hr /></div><p>In this chapter, you have learned how to start the Spark shell, load our data, and we did a few simple things through a hands-on machine-learning approach. Now that you've seen how Spark's interactive console works, it's time to see how to build Spark jobs in a more traditional and persistent environment in the subsequent chapter.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch03"></a>Chapter 3. Building and Running a Spark Application</h2></div></div></div><p>Using Spark in an interactive mode with the Spark shell has limited permanence and does not work in Java. Building Spark jobs is a bit trickier than building a normal application as all the dependencies have to be available on all the machines that are in your cluster. This chapter will cover building a Java and Scala Spark job with Maven or sbt and Spark jobs with a non-maven-aware build system.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec21"></a>Building your Spark project with sbt</h2></div></div><hr /></div><p>The sbt tool is a popular <a id="id58" class="indexterm"></a>build tool for Scala that supports building both Scala and Java code. Building Spark projects with sbt is one of the easiest options because Spark itself is built with sbt. It makes it easy to bring in dependencies (which is especially useful for Spark) as well as package everything into a single deployable/JAR file. The current normal method of building packages that use sbt is to use a shell script that bootstraps the specific version of sbt that your project uses, making installation simpler.</p><p>As a first step, take a Spark job tha<a id="id59" class="indexterm"></a>t already works and go through the process of creating a build file for it. In the <code class="literal">spark</code> directory, begin by copying the <code class="literal">GroupByTest</code> example into a new directory as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>mkdir -p example-scala-build/src/main/scala/spark/examples/</strong></span>
<span class="strong"><strong>cp -af sbt example-scala-build/</strong></span>
<span class="strong"><strong>cp examples/src/main/scala/spark/examples/GroupByTest.scala example-scala-build/src/main/scala/spark/examples/</strong></span>
</pre></div><p>Since you are going to ship your JAR file to the other machines, you will want to ensure that all the dependencies are included in them. You can either add a bunch of JAR files or use a handy sbt plugin called <code class="literal">sbt-assembly</code> to group everything into a single JAR file. If you don't have a bunch of transitive dependencies, you may decide that using the assembly extension isn't useful for your project. Instead of using <code class="literal">sbt-assembly</code>, you probably want to run <code class="literal">sbt/sbt assembly</code> in the Spark project and add the resulting JAR file <code class="literal">core/target/spark-core-assembly-0.7.0.jar</code> to your classpath. The <code class="literal">sbt-assembly</code> package is a <a id="id60" class="indexterm"></a>great tool to avoid having to manually manage a large number of JAR files. To add the assembly extension to your build, add the following code to <code class="literal">project</code>/<code class="literal">plugins.sbt</code>:</p><div class="informalexample"><pre class="programlisting">resolvers += Resolver.url("artifactory", url("http://scalasbt.artifactoryonline.com/scalasbt/sbt-plugin-releases"))(Resolver.ivyStylePatterns)

resolvers += "Typesafe Repository" at "http://repo.typesafe.com/typesafe/releases/"

resolvers += "Spray Repository" at "http://repo.spray.cc/"
addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.8.7")</pre></div><p>Resolvers are used <a id="id61" class="indexterm"></a>by sbt so that it can find where a package is; you can think of this as similar to specifying an additional APT <span class="strong"><strong>PPA</strong></span> (<span class="strong"><strong>Personal Package Archive</strong></span>)<a id="id62" class="indexterm"></a> source, except it only applies to the one package that you are trying to build. If you load up the resolver URLs in your browser, most of them have directory listing turned on, so you can see what packages are provided by the resolver. These resolvers point to web URLs, but there are also resolvers for local paths, which can be useful during development. The <code class="literal">addSbtPlugin</code> directive is deceptively simple; it says to include the <code class="literal">sbt-assembly</code> package from <code class="literal">com.eed3si9n</code> at Version <code class="literal">0.8.7</code> and implicitly adds the Scala version and sbt version. Make sure to run <code class="literal">sbt reload clean update</code> to install new plugins.</p><p>The following is the build file for one of the <code class="literal">GroupByTest.scala</code> examples as if it were being built on its own; insert the following code in <code class="literal">./build.sbt</code>:</p><div class="informalexample"><pre class="programlisting">//Next two lines only needed if you decide to use the assembly plugin
import AssemblyKeys._
assemblySettings

scalaVersion := "2.9.2"

name := "groupbytest"

libraryDependencies ++= Seq(
  "org.spark-project" % "spark-core_2.9.2" % "0.7.0"
)

resolvers ++= Seq(
  "JBoss Repository" at "http://repository.jboss.org/nexus/content/repositories/releases/","Spray Repository" at "http://repo.spray.cc/","Cloudera Repository" at"https://repository.cloudera.com/artifactory/cloudera-repos/","Akka Repository" at "http://repo.akka.io/releases/","Twitter4J Repository" at "http://twitter4j.org/maven2/")
//Only include if using assembly
mergeStrategy in assembly &lt;&lt;= (mergeStrategy in assembly) {
  (old) =&gt;
  {
    case PathList("javax", "servlet", xs @ _*) =&gt; MergeStrategy.first
    case PathList("org", "apache", xs @ _*) =&gt; MergeStrategy.first
    case "about.html"  =&gt; MergeStrategy.rename
    case x =&gt; old(x)
  }
}</pre></div><p>As you can see, the <a id="id63" class="indexterm"></a>build file is similar in format to <code class="literal">plugins.sbt</code>. There are a few unique things about this build file that are worth mentioning. Just as we<a id="id64" class="indexterm"></a> did with the plugin file, you also need to add a number of resolvers so that sbt can find all the dependencies. Note that we are including it as <code class="literal">"org.spark-project" % "spark-core_2.9.2" % "0.7.0"</code> rather than using <code class="literal">"org.spark-project" %% "spark-core" % "0.7.0"</code>. If possible, you should try to use the <code class="literal">%%</code> format, which automatically adds the Scala version. Another unique part of this build file is the use of <code class="literal">mergeStrategy</code>. Since multiple dependencies can define the same files, when you merge everything into a single JAR file, you need to tell the plugin how to handle it. It is a fairly simple build file other than the merge strategy and manually specifying the Scala version of Spark that you are using.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note12"></a>Note</h3><p>If you have a different JDK on the master than JRE on the workers, you may want to switch the target JDK by adding the following code to your build file:</p><div class="informalexample"><pre class="programlisting">javacOptions ++= Seq("-target", "1.6")</pre></div></div><p>Now that your build file is defined, build your <code class="literal">GroupByTest</code> Spark job:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sbt/sbt clean compile package</strong></span>
</pre></div><p>This will produce <code class="literal">target/scala-2.9.2/groupbytest_2.9.2-0.1-SNAPSHOT.jar</code>.</p><p>Run <code class="literal">sbt/sbt assembly</code> in the <code class="literal">spark</code> directory to make sure you have the Spark assembly available to your classpaths. The example requires a pointer to where Spark is using <code class="literal">SPARK_HOME</code> and where the <code class="literal">jar</code> example is using <code class="literal">SPARK_EXAMPLES_JAR</code>. We also need to specify the <a id="id65" class="indexterm"></a>classpath that we built to Scala locally with <code class="literal">-cp</code>. We can then run the following example:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>SPARK_HOME="../"  SPARK_EXAMPLES_JAR="./target/scala-2.9.2/groupbytest-assembly-0.1-SNAPSHOT.jar"  scala -cp/users/sparkuser/spark-0.7.0/example-scala-build/target/scala-2.9.2/groupbytest_2.9.2-0.1-SNAPSHOT.jar:/users/sparkuser/spark-0.7.0/core/target/spark-core-assembly-0.7.0.jar spark.examples.GroupByTest local[1]</strong></span>
</pre></div><p>If you have decided to build all of your dependencies into a single JAR file with the assembly plugin, we need to call it like this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sbt/sbt assembly</strong></span>
</pre></div><p>This will produce an assembly<a id="id66" class="indexterm"></a> snapshot at <code class="literal">target/scala-2.9.2/groupbytest-assembly-0.1-SNAPSHOT.jar</code>, which you can then run in a very similar manner, simply without <code class="literal">spark-core-assembly</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>SPARK_HOME="../" \ SPARK_EXAMPLES_JAR="./target/scala-2.9.2/groupbytest-assembly-0.1-SNAPSHOT.jar" \</strong></span>
<span class="strong"><strong> scala -cp /users/sparkuser/spark-0.7.0/example-scala-build/target/scala-2.9.2/groupbytest-assembly-0.1-SNAPSHOT.jar spark.examples.GroupByTest local[1]</strong></span>
</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note13"></a>Note</h3><p>You may run into merge issues with sbt assembly if things have changed; a quick search over the web will probably provide better current guidance than anything that could be written taking guesses about future merge problems. In general, <code class="literal">MergeStategy.first</code> should work.</p><p>Your success for the preceding code may have given you a false sense of security. Since <code class="literal">sbt</code> will resolve security from the local cache, the <code class="literal">deps</code> package that was brought in by another project could mean that the code builds on one machine and not others. Delete your local ivy cache and run <code class="literal">sbt clean</code> to make sure. If some files fail to download, try looking at Spark's list of resolvers and adding any missing ones to your <code class="literal">build.sbt</code>.</p></div><p>Some of the following links useful for referencing are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<a class="ulink" href="http://www.scala-sbt.org/" target="_blank">http://www.scala-sbt.org/</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="https://github.com/sbt/sbt-assembly" target="_blank">https://github.com/sbt/sbt-assembly</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://spark-project.org/docs/latest/scala-programming-guide.html" target="_blank">http://spark-project.org/docs/latest/scala-programming-guide.html</a>
</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec22"></a>Building your Spark job with Maven</h2></div></div><hr /></div><p>Maven is an <a id="id67" class="indexterm"></a>open source Apache project that builds Spark jobs in Java or Scala. As with sbt, you can include the Spark dependency through Maven, simplifying our build process. As with sbt, Maven has the ability to bundle Spark<a id="id68" class="indexterm"></a> and all of our dependencies, in a single JAR file using a plugin or build Spark as a monolithic JAR using <code class="literal">sbt/sbt assembly</code> for inclusion.</p><p>To illustrate the build process for Spark jobs with Maven, this section will use Java as an example since Maven is more commonly used to build Java tasks. As a first step, let's take a Spark job that already works and go through the process of creating a build file for it. We can start by copying the <code class="literal">GroupByTest</code> example into a new directory and generating the Maven template as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>mkdir example-java-build/; cd example-java-build</strong></span>
<span class="strong"><strong>mvn archetype:generate \</strong></span>
<span class="strong"><strong>   -DarchetypeGroupId=org.apache.maven.archetypes \</strong></span>
<span class="strong"><strong>   -DgroupId=spark.examples \</strong></span>
<span class="strong"><strong>   -DartifactId=JavaWordCount \</strong></span>
<span class="strong"><strong>   -Dfilter=org.apache.maven.archetypes:maven-archetype-quickstart</strong></span>
<span class="strong"><strong>cp ../examples/src/main/java/spark/examples/JavaWordCount.java JavaWordCount/src/main/java/spark/examples/JavaWordCount.java</strong></span>
</pre></div><p>Next, update your Maven <code class="literal">pom.xml</code> to include information about the version of Spark we are using. Also, since the example file we are working with requires JDK 1.5, we will need to update the Java version that Maven is configured to use; at the time of writing, it defaults to 1.3. In between the <code class="literal">&lt;project&gt;</code> tags, we will need to add the following code:</p><div class="informalexample"><pre class="programlisting">  &lt;dependencies&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;junit&lt;/groupId&gt;
      &lt;artifactId&gt;junit&lt;/artifactId&gt;
      &lt;version&gt;3.8.1&lt;/version&gt;
      &lt;scope&gt;test&lt;/scope&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;org.spark-project&lt;/groupId&gt;
      &lt;artifactId&gt;spark-core_2.9.2&lt;/artifactId&gt;
      &lt;version&gt;0.7.0&lt;/version&gt;
    &lt;/dependency&gt;
  &lt;/dependencies&gt;
  &lt;build&gt;
    &lt;plugins&gt;
      &lt;plugin&gt;
        &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
        &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
        &lt;configuration&gt;
          &lt;source&gt;1.5&lt;/source&gt;
          &lt;target&gt;1.5&lt;/target&gt;
        &lt;/configuration&gt;
      &lt;/plugin&gt;
    &lt;/plugins&gt;
  &lt;/build&gt;</pre></div><p>We can now build our <code class="literal">jar</code> with the <code class="literal">maven</code> package, which can be run using the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>SPARK_HOME="../"  SPARK_EXAMPLES_JAR="./target/JavaWordCount-1.0-SNAPSHOT.jar"  java -cp ./target/JavaWordCount-1.0-SNAPSHOT.jar:../../core/target/spark-core-assembly-0.7.0.jar spark.examples.JavaWordCount local[1] ../../README</strong></span>
</pre></div><p>As with sbt, <a id="id69" class="indexterm"></a>we can use a plugin to include all the dependencies<a id="id70" class="indexterm"></a> in our JAR file. In between the <code class="literal">&lt;plugins&gt;</code> tags, add the following code:</p><div class="informalexample"><pre class="programlisting">&lt;plugin&gt;
  &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
  &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;
  &lt;version&gt;1.7&lt;/version&gt;
  &lt;configuration&gt;
    &lt;!-- This transform is used so that merging of akka configuration files works --&gt;
    &lt;transformers&gt;
      &lt;transformer implementation="org.apache.maven.plugins.shade.resource.ApacheLicenseResourceTransformer"&gt;
      &lt;/transformer&gt;
      &lt;transformer implementation="org.apache.maven.plugins.shade.resource.AppendingTransformer"&gt;
        &lt;resource&gt;reference.conf&lt;/resource&gt;
      &lt;/transformer&gt;
    &lt;/transformers&gt;
  &lt;/configuration&gt;
  &lt;executions&gt;
    &lt;execution&gt;
      &lt;phase&gt;package&lt;/phase&gt;
      &lt;goals&gt;
        &lt;goal&gt;shade&lt;/goal&gt;
      &lt;/goals&gt;
    &lt;/execution&gt;
  &lt;/executions&gt;
&lt;/plugin&gt;</pre></div><p>Then run <code class="literal">mvn assembly</code> and the resulting <code class="literal">jar</code> file can be run as the preceding code, but leaving out the <a id="id71" class="indexterm"></a>Spark assembly <code class="literal">jar</code> file from the <a id="id72" class="indexterm"></a>classpath.</p><p>Some of the following links useful for referencing are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<a class="ulink" href="http://maven.apache.org/guides/getting-started/" target="_blank">http://maven.apache.org/guides/getting-started/</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://maven.apache.org/plugins/maven-compiler-plugin/examples/set-compiler-source-and-target.html" target="_blank">http://maven.apache.org/plugins/maven-compiler-plugin/examples/set-compiler-source-and-target.html</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://maven.apache.org/plugins/maven-dependency-plugin/" target="_blank">http://maven.apache.org/plugins/maven-dependency-plugin/</a>
</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec23"></a>Building your Spark job with something else</h2></div></div><hr /></div><p>If neither sbt nor Maven suits <a id="id73" class="indexterm"></a>your needs, you may decide to use another build system. Thankfully, Spark supports building a fat JAR file with all the dependencies of Spark, which makes it easy to include in the build system of your choice. Simply run <code class="literal">sbt/sbt assembly</code> in the Spark directory and copy the resulting assembly JAR file from <code class="literal">core/target/spark-core-assembly-0.7.0.jar</code> to your build dependencies, and you are good to go.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip04"></a>Tip</h3><p>No matter what your build system is, you may find yourself wanting to use a patched version of the Spark libraries. In that case, you can deploy your Spark library locally. I recommend giving it a different version number to ensure that <code class="literal">sbt/maven</code> picks up the modified version. You can change the version by editing <code class="literal">project/SparkBuild.scala</code> and changing the <code class="literal">version :=</code> part of the code. If you are using sbt, you should run an <code class="literal">sbt/sbt</code> update in the project that is importing the custom version. For other build systems, you just need to ensure that you<a id="id74" class="indexterm"></a> use the new assembly <code class="literal">jar</code> file as part of your build.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec24"></a>Summary</h2></div></div><hr /></div><p>So, now you can build your Spark jobs with Maven, sbt, or a build system of your choice. It's time to jump in and start learning how to do more fun and exciting things, such as how to create a Spark context, in the subsequent chapter.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch04"></a>Chapter 4. Creating a SparkContext</h2></div></div></div><p>This chapter will cover how to create a <code class="literal">SparkContext</code> context for your cluster. A <code class="literal">SparkContext</code> class represents the connection to a Spark cluster and provides the entry point for interacting with Spark. <a id="id75" class="indexterm"></a>We need to create a <code class="literal">SparkContext</code> instance so that we can interact with Spark and distribute our jobs. In <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Using the Spark Shell</em></span>, we interacted with Spark through the Spark shell, which created a <code class="literal">SparkContext</code>. Now you can create RDDs, broadcast variables, counters, and so on, and actually do fun things with your data. The Spark shell serves as an example of interaction with the Spark cluster through <code class="literal">SparkContext</code> in <code class="literal">./repl/src/main/scala/spark/repl/SparkILoop.scala</code>.</p><p>The following code snippet creates a <code class="literal">SparkContext</code> instance using the <code class="literal">MASTER</code> environment variable (or <code class="literal">local</code>, if none are set) called <code class="literal">Spark shell</code> and doesn't specify any dependencies. This is because the Spark shell is built into Spark and, as such, doesn't have any JAR files that it needs to be distributed.</p><div class="informalexample"><pre class="programlisting">def createSparkContext(): SparkContext = {
  val master = this.master match {
    case Some(m) =&gt; m
    case None =&gt; {
     val prop = System.getenv("MASTER")
     if (prop != null) prop else "local"
    }
  }
  sparkContext = new SparkContext(master, "Spark shell")
  sparkContext
  }</pre></div><p>For a client to establish a connection to the Spark cluster, the <code class="literal">SparkContext</code> object needs some basic information as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">master</code>: The<code class="literal"> master</code> URL can be in one of the following formats:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">local[n]</code>: for a local mode</p></li><li style="list-style-type: disc"><p>
<code class="literal">spark://[sparkip]</code>: to <a id="id76" class="indexterm"></a>point to a Spark cluster</p></li><li style="list-style-type: disc"><p>
<code class="literal">mesos://</code>: for a mesos path if you are running a mesos cluster</p></li></ul></div></li><li style="list-style-type: disc"><p>
<code class="literal">application name</code>: This is<a id="id77" class="indexterm"></a> the human-readable application name</p></li><li style="list-style-type: disc"><p>
<code class="literal">sparkHome</code>: This is the path to Spark <a id="id78" class="indexterm"></a>on the master/workers machines</p></li><li style="list-style-type: disc"><p>
<code class="literal">jars</code>: This gives the path to<a id="id79" class="indexterm"></a> the list of JAR files required for your job</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec25"></a>Scala</h2></div></div><hr /></div><p>In a Scala program, <a id="id80" class="indexterm"></a>you can create a <code class="literal">SparkContext</code> instance using the following code:</p><div class="informalexample"><pre class="programlisting">val spar kContext = new SparkContext(master_path, "application name", ["optional spark home path"],["optional list of jars"])</pre></div><p>While you can <a id="id81" class="indexterm"></a>hardcode all of these values, it's better to read them from the environment with reasonable defaults. This approach provides maximum flexibility to run the code in a <a id="id82" class="indexterm"></a>changing environment without having to recompile the code. Using <code class="literal">local</code> as the default value for the  master machine makes it easy to launch your application locally in a test environment. By carefully selecting the defaults, you can avoid having to over-specify them. An example would be as follows:</p><div class="informalexample"><pre class="programlisting">import spark.sparkContext
import spark.sparkContext._
import scala.util.Properties

val master = Properties.envOrElse("MASTER","local")
val sparkHome = Properties.get("SPARK_HOME")
val myJars = Seq(System.get("JARS")
val sparkContext = new SparkContext(master, "my app", sparkHome, myJars)</pre></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec26"></a>Java</h2></div></div><hr /></div><p>To create a<a id="id83" class="indexterm"></a> <code class="literal">SparkContext</code> instance<a id="id84" class="indexterm"></a> in Java, try the following code:</p><div class="informalexample"><pre class="programlisting">import spark.api.java.JavaSparkContext;

JavaSparkContext ctx = new JavaSparkContext("master_url", "application name", ["path_to_spark_home", "path_to_jars"]);</pre></div><p>While the preceding code works (once you have replaced the parameters with the correct values for your setup), it requires a code change if you've changed any of the parameters. Instead, use reasonable defaults and allow them to be overridden similar to the example Scala code. The following illustrates how to do this with the environment variables:</p><div class="informalexample"><pre class="programlisting">String master = System.getEnv("MASTER");
if (master == null) {
  master = "local";
}
String sparkHome = System.getEnv("SPARK_HOME");
if (sparkHome == null) {
  sparkHome = "./";
}
String jars = System.getEnv("JARS");
JavaSparkContext ctx = new JavaSparkContext(System.getenv("MASTER"), "my Java app",System.getenv("SPARK_HOME"), System.getenv("JARS"));</pre></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec27"></a>Shared Java and Scala APIs</h2></div></div><hr /></div><p>Once you have a <code class="literal">SparkContext</code> created, it will serve as your main entry point. In the next chapter, <a id="id85" class="indexterm"></a>you will learn how to use our <code class="literal">SparkContext</code> instance to load and save data. You can also use the <code class="literal">SparkContext</code> instance to launch more Spark jobs and add or remove dependencies. <a id="id86" class="indexterm"></a>Some of the non-data-driven methods you can use on the <code class="literal">SparkContext</code> instance are as follows:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Method</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Use</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>
<code class="literal">addJar(path)</code>
<a id="id87" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>Adds the JAR file for all future jobs run through the <code class="literal">SparkContext </code>instance</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">addFile(path)</code>
<a id="id88" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>Downloads the file to all nodes on the cluster</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">stop()</code>
<a id="id89" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>Shuts down the <code class="literal">SparkContext</code> connection</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">clearFiles()</code>
<a id="id90" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>Removes the files so that new nodes will not download them</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">clearJars()</code>
<a id="id91" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>Removes the JAR files from being required for future jobs</p>
</td></tr></tbody></table></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec28"></a>Python</h2></div></div><hr /></div><p>The Python <code class="literal">SparkContext</code> is a bit different from the Scala and Java contexts since Python doesn't use JAR <a id="id92" class="indexterm"></a>files to distribute dependencies. Since you are still likely to have dependencies, set <code class="literal">pyFiles</code> with the ZIP and PY files as desired on SparkContext (or leave it empty if <a id="id93" class="indexterm"></a>you don't have any files to distribute).</p><p>You can create a Python <code class="literal">SparkContext</code> using the following code:</p><div class="informalexample"><pre class="programlisting">from pyspark import SparkContext

sc = SparkContext("master","my python app", sparkHome="sparkhome", pyFiles="placeholderdeps.zip")</pre></div><p>Now you are able to create a connection to your Spark cluster, so it's time to get started on loading our data into Spark.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec29"></a>Links and references</h2></div></div><hr /></div><p>Some useful links for <a id="id94" class="indexterm"></a>referencing are listed as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<a class="ulink" href="http://spark-project.org/docs/latest/quick-start.html" target="_blank">http://spark-project.org/docs/latest/quick-start.html</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://www-stat.stanford.edu/~tibs/ElemStatLearn/data.html" target="_blank">http://www-stat.stanford.edu/~tibs/ElemStatLearn/data.html</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="https://github.com/mesos/spark/blob/master/repl/src/main/scala/spark/repl/SparkILoop.scala" target="_blank">https://github.com/mesos/spark/blob/master/repl/src/main/scala/spark/repl/SparkILoop.scala</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://spark-project.org/docs/0.7.0/api/pyspark/pyspark.context.SparkContext-class.html" target="_blank">http://spark-project.org/docs/0.7.0/api/pyspark/pyspark.context.SparkContext-class.html</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://spark-project.org/docs/0.7.0/api/core/spark/SparkContext.html" target="_blank">http://spark-project.org/docs/0.7.0/api/core/spark/SparkContext.html</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://spark-project.org/docs/0.7.0/api/core/spark/api/java/JavaSparkContext.html" target="_blank">http://spark-project.org/docs/0.7.0/api/core/spark/api/java/JavaSparkContext.html</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://www.scala-lang.org/api/current/index.html#scala.util.Properties$" target="_blank">http://www.scala-lang.org/api/current/index.html#scala.util.Properties$</a>
</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec30"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we've covered how to connect to our Spark cluster using <code class="literal">SparkContext</code>. Using <code class="literal">SparkContext</code>, we will start to look at the different data sources that we can use to load data into Spark in the next chapter.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch05"></a>Chapter 5. Loading and Saving Data in Spark</h2></div></div></div><p>By this point in the book you have experimented with the Spark shell, figured out how to create a connection to the Spark cluster, and built jobs for deployment. Now to make those jobs useful, you will learn how to load and save data in Spark. Spark's primary unit for data representation is an RDD, which allows for easy parallel operations on the data. Other forms of data, such as counters, have their own representation. Spark can load and save RDDs from a variety of sources.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec31"></a>RDDs</h2></div></div><hr /></div><p>Spark RDDs can be created from any supported Hadoop source. Native collections in Scala, Java, and Python can also serve as the basis for an RDD. Creating RDDs from a native collection is especially useful for testing.</p><p>Before jumping into the <a id="id95" class="indexterm"></a>details on the supported data sources/sinks, take some time to learn about what RDDs are and what they are not. It is crucial to understand that even though an RDD is defined, it does not actually contain data. This means that when you go to access the data in an RDD it could fail. The computation to create the data in an RDD is only done when the data is referenced; for example, it is created by caching or writing out the RDD. This means that you can chain a large number of operations together, and not have to worry about excessive blocking. It's important to note that during the application development, you can write code, compile it, and even run your job, and unless you materialize the RDD, your code may not have even tried to load the original data.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note14"></a>Note</h3><p>Each time you materialize an RDD it is re-computed. If we are going to be using something frequently, a performance improvement can be achieved by caching the RDD.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec32"></a>Loading data into an RDD</h2></div></div><hr /></div><p>Now the chapter will examine the different sources you can use for your RDD. If you decide to run through<a id="id96" class="indexterm"></a> the examples in the Spark shell, you can call .<code class="literal">cache</code>() or .<code class="literal">first</code>() on the RDDs you generate to verify that it can be loaded. In <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Using the Spark Shell</em></span>, you learned how to load data text from a file and from the S3 storage system, where you can look at different formats of data and the different sources that are supported.</p><p>One of the easiest ways of creating an RDD is taking an existing Scala collection and converting it into an RDD. The Spark context provides a function called <code class="literal">parallelize</code>; this takes a Scala collection and turns it into an RDD that is of the same type as the data input.</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Scala</strong></span>:</p><div class="informalexample"><pre class="programlisting">val dataRDD = sc.parallelize(List(1,2,4))</pre></div></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Java</strong></span>:</p><div class="informalexample"><pre class="programlisting">JavaRDD&lt;Integer&gt; dataRDD = sc.parallelize(Arrays.asList(1,2,4));</pre></div></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Python</strong></span>:</p><div class="informalexample"><pre class="programlisting">rdd = sc.parallelize([1,2,3])</pre></div></li></ul></div><p>The simplest method for loading external data is loading text from a file. This requires the file to be available on all the nodes in the cluster, which isn't much of a problem for a local mode. When in a distributed mode, you will want to use Spark's <code class="literal">addFile</code> functionality to copy the file to all the machines in your cluster. Assuming your <code class="literal">SparkContext</code> is called <code class="literal">sc</code>, we could load text data from a file (you need to create the file):</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Scala</strong></span>:</p><div class="informalexample"><pre class="programlisting">import spark.SparkFiles;
...
sc.addFile("spam.data")
val inFile = sc.textFile(SparkFiles.get("spam.data"))</pre></div></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Java</strong></span>:</p><div class="informalexample"><pre class="programlisting">import spark.Sparkfiles;
   
sc.addFile("spam.data");
JavaRDD&lt;String&gt; lines = sc.textFile(SparkFiles.get("spam.data"));</pre></div></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Python</strong></span>:</p><div class="informalexample"><pre class="programlisting">from pyspark.files import SparkFiles
   
sc.addFile("spam.data")
sc.textFile(SparkFiles.get("spam.data"))</pre></div></li></ul></div><p>The resulting RDD is an overridden string with each line being a unique element in the RDD. Frequently<a id="id97" class="indexterm"></a>, your input files will be CSV or TSV files, which you will want to parse using one of the standard CSV libraries. In <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Using the Spark Shell</em></span>, you parsed them with a split and <code class="literal">toDouble</code>, but that doesn't always work out so well for more complex CSV files. Looking back to <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Building and Running a Spark Application</em></span> where you learned how to build jobs, you can change the <code class="literal">libraryDependencies</code> in <code class="literal">build.sbt</code> to be:</p><div class="informalexample"><pre class="programlisting">libraryDependencies ++= Seq(
    "org.spark-project" % "spark-core_2.9.2" % "0.7.0",
    "net.sf.opencsv" % "opencsv" % "2.0"
)</pre></div><p>This brings in a CSV parser to use. This chapter uses <code class="literal">opencsv</code> for this example for brevity's sake, but you may find another CSV parser better suited to your needs depending on what you are parsing. Let's look at a sample that parses the input CSV and sums all the rows:</p><div class="informalexample"><pre class="programlisting">package pandaspark.examples

import spark.SparkContext
import spark.SparkContext._
import spark.SparkFiles;
import au.com.bytecode.opencsv.CSVReader
import java.io.StringReader

object LoadCsvExample {
  def main(args: Array[String]) {
    if (args.length != 2) {
      System.err.println("Usage: LoadCsvExample &lt;master&gt; &lt;inputfile&gt;")
      System.exit(1)
    }
    val master = args(0)
    val inputFile = args(1)
    val sc = new SparkContext(master, "Load CSV Example",System.getenv("SPARK_HOME"),
                 Seq(System.getenv("JARS")))
    sc.addFile(inputFile)
    val inFile = sc.textFile(inputFile)
    val splitLines = inFile.map(line =&gt; {
      val reader = new CSVReader(new StringReader(line))
      reader.readNext()
    })
    val numericData = splitLines.map(line =&gt; line.map(_.toDouble))
    val summedData = numericData.map(row =&gt; row.sum)
    println(summedData.collect().mkString(","))
  }

}</pre></div><p>The previous code also illustrates one of the ways of getting the data out of Spark: you can transform it <a id="id98" class="indexterm"></a>to a standard Scala array using the <code class="literal">collect()</code> function. The <a id="id99" class="indexterm"></a>
<code class="literal">collect()</code> function is especially useful for testing, in much the same way as the <a id="id100" class="indexterm"></a>
<code class="literal">parallelize()</code> function is. The <code class="literal">collect()</code> function only works if your data fits in memory on a single host; in that case it adds the bottleneck of everything having to come back to a single machine.</p><p>While loading text files into Spark is certainly easy, text files on a local disk are often not the most convenient format for storing large chunks of data. Spark supports loading from all the different Hadoop formats (sequence files, regular text files, and so on) and from all the support Hadoop storage sources (HDFS, S3, HBase, and so on). If you want you can also load your CSV into HBase using some of its bulk loading tools (such as ImportTsv) and get at your CSV data that way. As of Version 0.7, PySpark does not support any of the advanced methods of loading data we will be discussing in the rest of this chapter.</p><p>Sequence files are binary-flat files consisting of key-value pairs, and they are one of the common ways of storing data for use with Hadoop. Loading a sequence file into Spark is similar to loading a text file, but you also need to let it know about the types of the keys and values. The types must either be subclasses of Hadoop's <code class="literal">Writable</code> class or be implicitly convertible to such a type. For Scala users, some natives are convertible through implicit conversions in <code class="literal">WritableConverter</code>. As of Version 0.7, the standard <code class="literal">WritableConverter</code> types are <code class="literal">Int</code>, <code class="literal">Long</code>, <code class="literal">Double</code>, <code class="literal">Float</code>, <code class="literal">Boolean</code>, <code class="literal">Array</code>, and <code class="literal">String</code>.</p><p>Let's illustrate this by looking at how to load a sequence file of <code class="literal">String</code> to <code class="literal">Integer</code>.</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Scala</strong></span>:</p><div class="informalexample"><pre class="programlisting">val data = sc.sequenceFile[String, Int](inputFile)</pre></div></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Java</strong></span>:</p><div class="informalexample"><pre class="programlisting">JavaPairRDD&lt;Text, IntWritable&gt; dataRDD = sc.sequenceFile(file, Text.class, IntWritable.class);
JavaPairRDD&lt;String, Integer&gt; cleanData = dataRDD.map(new PairFunction&lt;Tuple2&lt;Text, IntWritable&gt;, String, Integer&gt;() {
 @Override
public Tuple2&lt;String, Integer&gt; call(Tuple2&lt;Text, IntWritable&gt; pair) {
return new Tuple2&lt;String, Integer&gt;(pair._1().toString(), pair._2().get());
}
});</pre></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip05"></a>Tip</h3><p>Note that in the preceding cases, like with the text input, the file need not be a traditional file; it can reside on S3, HDFS, and so on. Also note that for Java, you can't rely on implicit conversions between types.</p></div></li></ul></div><p>HBase is a Hadoop-based<a id="id101" class="indexterm"></a> database designed to support random read/write access to entries. Loading data from HBase is a bit different from text files and sequence files. With HBase, we have to specify the type information of our data to Spark in a different way. Since HBase isn't included by default as a dependency of Spark, you will need to add it to your build system like you did with <code class="literal">opencsv</code> previously by adding <code class="literal">org.apache.hbase" % "hbase" % "0.94.6</code>.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip06"></a>Tip</h3><p>If you run into difficulty with unresolved dependencies, make sure to add the Apache HBase release Maven repository at <a class="ulink" href="https://repository.apache.org/content/repositories/releases" target="_blank">https://repository.apache.org/content/repositories/releases</a> to your resolvers.</p></div><p>Let's illustrate the use of<a id="id102" class="indexterm"></a> HBase database:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Scala</strong></span>:</p><div class="informalexample"><pre class="programlisting">import spark._
import org.apache.hadoop.hbase.{HBaseConfiguration, HTableDescriptor}
import org.apache.hadoop.hbase.client.HBaseAdmin
import org.apache.hadoop.hbase.mapreduce.TableInputFormat …
val conf = HBaseConfiguration.create()
conf.set(TableInputFormat.INPUT_TABLE, input_table)
 // Initialize hBase table if necessary
val admin = new HBaseAdmin(conf)
if(!admin.isTableAvailable(input_table)) {
  val tableDesc = new HTableDescriptor(input_table)
  admin.createTable(tableDesc)
}
val hBaseRDD = sc.newAPIHadoopRDD(conf,
                  classOf[TableInputFormat],
                  classOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable],
                  classOf[org.apache.hadoop.hbase.client.Result])</pre></div></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Java</strong></span>:</p><div class="informalexample"><pre class="programlisting">import spark.api.java.JavaPairRDD;
import spark.api.java.JavaSparkContext;
import spark.api.java.function.FlatMapFunction;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.HTableDescriptor;
import org.apache.hadoop.hbase.client.HBaseAdmin;
import org.apache.hadoop.hbase.mapreduce.TableInputFormat;
import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
import org.apache.hadoop.hbase.client.Result;
...
JavaSparkContext sc = new JavaSparkContext(args[0], "sequence load", System.getenv("SPARK_HOME"), System.getenv("JARS"));
Configuration conf = HBaseConfiguration.create();
conf.set(TableInputFormat.INPUT_TABLE, args[1]);
//Initialize HBase table if necessary
HBaseAdmin admin = new HBaseAdmin(conf);
if(!admin.isTableAvailable(args[1])) {
    HTableDescriptor tableDesc = new HTableDescriptor(args[1]);
    admin.createTable(tableDesc);
}
JavaPairRDD&lt;ImmutableBytesWritable, Result&gt; hBaseRDD = sc.newAPIHadoopRDD( conf, TableInputFormat.class, ImmutableBytesWritable.class, Result.class);</pre></div></li></ul></div><p>The method that you used to load the HBase data can be generalized for loading all other sorts of Hadoop data. If a<a id="id103" class="indexterm"></a> helper method in Spark context does not already <a id="id104" class="indexterm"></a>exist for loading the data, simply create a configuration specifying how to load the data and pass it into the <code class="literal">newAPIHadoopRDD</code> method<a id="id105" class="indexterm"></a>. Helper methods exist for plain text files and sequence files. A helper method also exists for Hadoop files similar to the Sequence File API.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec33"></a>Saving your data</h2></div></div><hr /></div><p>While distributed computational jobs are a lot of fun, they are much more applicable when the results get stored somewhere useful. While the methods for loading an RDD are largely found in the <code class="literal">SparkContext</code> class<a id="id106" class="indexterm"></a>, the methods for saving an RDD are defined on the RDD classes. In Scala, implicit conversion exists so that an RDD that can be saved as a sequence file is converted to the appropriate type, and in Java explicit conversion must be used.</p><p>Here are the different ways to save an RDD:<a id="id107" class="indexterm"></a>
</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Scala</strong></span>:</p><div class="informalexample"><pre class="programlisting">rddOfStrings.saveAsTextFile("out.txt")
keyValueRdd.saveAsSequenceFile("sequenceOut")</pre></div></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Java</strong></span>:</p><div class="informalexample"><pre class="programlisting">rddOfStrings.saveAsTextFile("out.txt")
keyValueRdd.saveAsSequenceFile("sequenceOut")</pre></div></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Python</strong></span>:</p><div class="informalexample"><pre class="programlisting">rddOfStrings.saveAsTextFile("out.txt")</pre></div></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec34"></a>Links and references</h2></div></div><hr /></div><p>Some of the useful links that <a id="id108" class="indexterm"></a>you can use for references are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<a class="ulink" href="http://spark-project.org/docs/latest/scala-programming-guide.html#hadoop-datasets" target="_blank">http://spark-project.org/docs/latest/scala-programming-guide.html#hadoop-datasets</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://opencsv.sourceforge.net/" target="_blank">http://opencsv.sourceforge.net/</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://commons.apache.org/proper/commons-csv/" target="_blank">http://commons.apache.org/proper/commons-csv/</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/SequenceFileInputFormat.html" target="_blank">http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/SequenceFileInputFormat.html</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/InputFormat.html" target="_blank">http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/InputFormat.html</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/" target="_blank">http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://spark-project.org/docs/latest/api/pyspark/index.html" target="_blank">http://spark-project.org/docs/latest/api/pyspark/index.html</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://wiki.apache.org/hadoop/SequenceFile" target="_blank">http://wiki.apache.org/hadoop/SequenceFile</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://hbase.apache.org/book/quickstart.html" target="_blank">http://hbase.apache.org/book/quickstart.html</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/TableInputFormat.html" target="_blank">http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/TableInputFormat.html</a>
<a id="id109" class="indexterm"></a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://spark-project.org/docs/latest/api/core/index.html#spark.api.java.JavaPairRDD" target="_blank">http://spark-project.org/docs/latest/api/core/index.html#spark.api.java.JavaPairRDD</a>
</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec35"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we have seen how to load data from a variety of different sources. We have also looked at basic parsing of the data from text input files. Now that we can get our data loaded into a Spark RDD, it is time to explore the different operations we can perform on our data in the next chapter.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch06"></a>Chapter 6. Manipulating Your RDD</h2></div></div></div><p>The last few chapters have been the necessary groundwork for getting Spark working. Now that you know how to load and save your data in different ways, it's time for the big payoff: manipulating the data. The API for manipulating your RDD is similar between the languages, but not identical. Unlike the previous chapters, each language is covered in its own section; you probably only need to read the one pertaining to the language you are interested in using. Particularly, the Python implementation is currently not on feature parity with the Scala/Java API, but it supports most of the basic functionalities as of 0.7 with plans for future versions to improve feature parity.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec36"></a>Manipulating your RDD in Scala and Java</h2></div></div><hr /></div><p>Manipulating your RDD in Scala is quite simple, especially if you are familiar with Scala's collection library. Many of the standard functional list functions are available directly on Spark's RDDs with the primary catch being that one can't rely on them being executed on the <a id="id110" class="indexterm"></a>same machine. This makes porting of the existing Scala code to be distributed in a much simpler way than porting of the, say, Java or Python code.</p><p>Manipulating your <a id="id111" class="indexterm"></a>RDD in Java is fairly simple, but a little more awkward at times than in Scala. As Java doesn't have implicit conversions, we have to be more explicit with our types. While the return types are Java friendly, Spark requires the use of Scala's <code class="literal">Tuple2</code> class for key-value pairs.</p><p>The hallmark of a MapReduce system are the two commands: <code class="literal">map</code> and <code class="literal">reduce</code>. We've seen the <code class="literal">map</code> function used in the previous chapters. The <code class="literal">map</code> function works by taking in a function that works on each individual element in the input RDD and produces a new output element. For example, to produce a new RDD where you add one to every number, use <code class="literal">rdd.map(x =&gt; x+1)</code> or in Java, you can use the following code:</p><div class="informalexample"><pre class="programlisting">rdd.map(new Function&lt;Integer, Integer&gt;() {
  public Integer call(Integer x) { return x+1;}
});</pre></div><p>It is important to understand that the <code class="literal">map</code> function and the other Spark functions do not transform the existing elements, rather they return a new RDD with the new elements. The <code class="literal">reduce</code> function takes a function that operates on pairs to combine all the data. The function you provide needs to be commutative and associative (that is, f(a,b) == f(b,a) and f(a,f(b,c)) == f(f(a,b),c). For example, to sum all the elements, use <code class="literal">rdd.reduce(x,y =&gt; x+y) or rdd.reduce(new Function2&lt;Integer, Integer, Integer&gt;(){ public Integer call(Integer x, Integer y) { return x+y;} }.</code>
</p><p>The <code class="literal">flatMap</code> function is a useful utility, which lets you write a function that returns an <code class="literal">Iterable</code> object of the type you want and then flattens the results. A simple example of this is a case where you want to parse all the data, but may fail to parse some of it. The <code class="literal">flatMap</code> function can be used to output an empty list if it failed, or a list with the success if it worked. In addition to the <code class="literal">reduce</code> function, there is a corresponding <code class="literal">reduceByKey</code> function that works on RDDs, which are key-value pairs to produce another RDD. Unlike when using map on a list in Scala, your function will run on a number of different machines, so you can't depend on a shared state with this.</p><p>Before continuing with the other wonderful functions for manipulating your RDD, first you need to read a bit about shared states. In the preceding example where we added one to every integer, we didn't really share states. However, for even simple tasks, like the distributed parsing of data that we did when loading the CSV file, it can be quite handy to have shared counters for things like keeping track of the number of rejected records. Spark supports both shared immutable data, which it calls <code class="literal">broadcast</code> and <code class="literal">accumulator</code> variables. You can create a new broadcast by calling <code class="literal">sc.broadcast(value)</code>. While you don't have to explicitly broadcast values as Spark does its magic in the background, broadcasting ensures that the value is sent to each node only once. The <code class="literal">broadcast</code> variables are often used for operations such as side inputs (for example, a hashmap), which need to look up as part of the <code class="literal">map</code> function. This returns an object that can be used to reference the <code class="literal">broadcast</code> value.</p><p>Another method of sharing state is with an <code class="literal">accumulator</code> variable. To create an <code class="literal">accumulator</code> variable, use <code class="literal">sc.accumulator(initialvalue)</code>. This returns an object that you can add to in a distributed context and then get back the value by calling <code class="literal">.value()</code>. The <code class="literal">accumulableCollection</code> function can be used to create a collection that is appended in a distributed fashion; however, if you find yourself using this, ask yourself if you could use the results of a map output better. If the predefined accumulators don't work for your use case, you can use <code class="literal">accumulable</code> to define your own accumulation type. A <code class="literal">broadcast</code> value can be read by all the workers and an <code class="literal">accumulator</code> value can be written to by all the workers and only read by the driver.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note15"></a>Note</h3><p>If you are writing Scala code that interacts with a Java Spark process (say for testing), you may find it useful to use <code class="literal">intAccumulator</code> and similar methods on the Java Spark context, otherwise your <code class="literal">accumulator</code> types might not quite match up.</p><p>If you find your <code class="literal">accumulator</code> variable isn't increasing in value like you expect, remember that Spark is lazy. This means that Spark won't actually perform the maps, reduces, or other computation on RDDs until the data outputs the computations.</p></div><p>Look at your previous example that parsed CSV files and made it a bit more robust. In your previous work, you assumed the input was well formatted and if any errors occurred, our entire pipeline would fail. While this can be the correct behavior for some work, when dealing with data from third-party parties, we may want to accept some number of malformed records. On the other hand, we don't want to just throw out all the records and declare it a success; we might miss an important format change and produce meaningless results. Let's add counters for errors to our code:</p><div class="informalexample"><pre class="programlisting">package spark.examples

import spark.SparkContext
import spark.SparkContext._
import spark.SparkFiles;

import au.com.bytecode.opencsv.CSVReader

import java.io.StringReader

object LoadCsvWithCountersExample {
  def main(args: Array[String]) {
    if (args.length != 2) {
      System.err.println("Usage: LoadCsvExample &lt;master&gt;&lt;inputfile&gt;")
      System.exit(1)
    }
    val master = args(0)
    val inputFile = args(1)
    val sc = new SparkContext(master, "Load CSV With CountersExample",
  System.getenv("SPARK_HOME"),
  Seq(System.getenv("JARS")))
    val invalidLineCounter = sc.accumulator(0)
    val invalidNumericLineCounter = sc.accumulator(0)
    sc.addFile(inputFile)
    val inFile = sc.textFile(inputFile)
    val splitLines = inFile.flatMap(line =&gt; {
      try {
        val reader = new CSVReader(new StringReader(line))
        Some(reader.readNext())
      }
      catch {
        case _ =&gt; {
          invalidLineCounter += 1
          None
        }
      }
    }
    )
    val numericData = splitLines.flatMap(line =&gt; {
      try {
        Some(line.map(_.toDouble))
      }
      catch {
        case _ =&gt; {
          invalidNumericLineCounter += 1
          None
        }
      }
    }
    )
    val summedData = numericData.map(row =&gt; row.sum)
    println(summedData.collect().mkString(","))
    println("Errors: "+invalidLineCounter+","+invalidNumericLineCounter)
  }

}</pre></div><p>Or in Java:</p><div class="informalexample"><pre class="programlisting">import spark.Accumulator;
import spark.api.java.JavaRDD;
import spark.api.java.JavaPairRDD;
import spark.api.java.JavaSparkContext;
import spark.api.java.function.FlatMapFunction;

import au.com.bytecode.opencsv.CSVReader;
import java.io.StringReader;
import java.util.Arrays;
import java.util.List;
import java.util.ArrayList;

public class JavaLoadCsvCounters {
  public static void main(String[] args) throws Exception {
    if (args.length != 2) {
      System.err.println("Usage: JavaLoadCsvCounters &lt;master&gt; &lt;inputfile&gt;");
      System.exit(1);
    }
    String master = args[0];
    String inputFile = args[1];
    JavaSparkContext sc = new JavaSparkContext(master,"java load csv with counters",
                          System.getenv("SPARK_HOME"), System.getenv("JARS"));
    final Accumulator&lt;Integer&gt; errors = sc.accumulator(0);
    JavaRDD&lt;String&gt; inFile = sc.textFile(inputFile);
    JavaRDD&lt;Integer[] &gt; splitLines = inFile.flatMap(new FlatMapFunction&lt;String,Integer[]&gt; (){
      public Iterable&lt;Integer[]&gt; call(String line) {
        ArrayList&lt;Integer[]&gt; result = new ArrayList&lt;Integer[]&gt;();
        try {
               CSVReader reader = new CSVReader(new StringReader(line));
               String[] parsedLine = reader.readNext();
               Integer[] intLine = new Integer[parsedLine.length];
               for (int i = 0; i &lt; parsedLine.length; i++) {
                 intLine[i] = Integer.parseInt(parsedLine[i]);
               }
               result.add(intLine);
        }
        catch (Exception e) {
          errors.add(1);
        }
        return result;
        }
  }
  );
    System.out.println("Loaded data "+splitLines.collect());
    System.out.println("Error count "+errors.value());
  }
}</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note16"></a>Note</h3><p>The preceding code example illustrates the use of the <code class="literal">flatMap</code> method<a id="id112" class="indexterm"></a>. In general, <code class="literal">flatMap</code> can be used when your map function returns a sequence of the type you are returning and flattens it. Since options in Scala can be used as sequences through an implicit conversion, you can avoid having to explicitly filter out the <code class="literal">None</code> result and just use <code class="literal">flatMap</code>.</p></div><p>Summary statistics can be quite useful when examining large data sets. In the preceding example, you loaded the data as <code class="literal">Double</code>, to use Spark's provided summary statistics capabilities on the RDD. In Java, this requires explicit use of the <code class="literal">JavaDoubleRDD</code> type. It is also important to use <code class="literal">DoubleFunction&lt;Integer[]&gt;</code> rather than <code class="literal">Function&lt;Integer[], Double&gt;</code> in the following example, since the first won't result in the type <code class="literal">JavaDoubleRDD</code>. No such consideration is required for Scala as implicit conversions deal with the details. Compute the mean and the variance, or compute them together with the statistics. You can extend this by adding on the end of the preceding function to print out the summary statistics as <code class="literal">println(summedData.stats())</code>:</p><p>To do this in Java, you can add the following code:</p><div class="informalexample"><pre class="programlisting">JavaDoubleRDD summedData = splitLines.map(new DoubleFunction&lt;Integer[]&gt;()
{
  public Double call(Integer[] in) {
    Double ret = 0.;
    for (int i = 0; i &lt; in.length; i++) {
      ret += in[i];
    }
    return ret;
  }
});
System.out.println(summedData.stats());</pre></div><p>When working with key-value pair data, it can be quite useful to group data with the same key together (for example, if the key represents the user or sample). The <code class="literal">groupByKey</code> function provides an easy way to group data together by keys. The <code class="literal">groupByKey</code> key-value pair is a special case of <code class="literal">combineByKey</code>. There are several functions in the <code class="literal">PairRDD</code> class, which are all implemented very closely on top of <code class="literal">combineByKey</code>. If you find yourself using <code class="literal">groupByKey</code> or one of the other functions derived from <code class="literal">combineByKey</code> and immediately transforming the result, you should check if the function is better suited to the task. A common thing to do while starting out is to use <code class="literal">groupByKey</code> and then sum the results with <code class="literal">groupByKey().map((x,y) =&gt; (x,y.sum))</code>, or you can also use the following code in Java:</p><div class="informalexample"><pre class="programlisting">pairData.groupByKey().mapValues(new Function&lt;List&lt;Integer&gt;,Integer &gt;(){
  public Integer call(List&lt;Integer&gt; x){
    Integer sum = 0;
    for (Integer i : x) {
      sum += i;
    }
    return sum;
  }
});</pre></div><p>By using <code class="literal">reduceByKey</code>, it could be simplified to <code class="literal">reduceByKey((x,y) =&gt; x+y)</code>, or to do this in Java you can use the following code:</p><div class="informalexample"><pre class="programlisting">pairData.reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() {
  public Integer call (Integer a, Integer b){
    return a+b;
  }
});</pre></div><p>The <code class="literal">foldByKey(zeroValue) (function)</code> method is similar to a traditional <code class="literal">fold</code> operation that works on per key. On a list in a traditional <code class="literal">fold</code> function, the provided values would be called with the initial value and the first element of the list, and then the resulting value and the next element of the list would be the input to the next call of <code class="literal">fold</code>. Doing this requires sequentially processing the entire list, so <code class="literal">foldByKey</code> behaves slightly differently. There is a handy table of functions of <code class="literal">PairRDD</code> at the end of this section. Some of the <code class="literal">PairRDD</code> functionality was only added to the Java API in 0.7.2.</p><p>Sometimes, you will only want to update the values of a key-value pair data structure, such as a <code class="literal">PairRDD</code>. You've learned about <code class="literal">foldByKey</code> and how it doesn't quite work as a traditional <code class="literal">fold</code>. For Scala developers, if you require the "traditional" <code class="literal">fold</code> behavior, you can do a <code class="literal">groupByKey</code> and then map a fold over the resulting RDD by value. This is an example of a case where you only want to change the value and don't care about the key of the RDD, so examine the following code:</p><div class="informalexample"><pre class="programlisting">rdd.groupByKey().mapValues(x =&gt; {x.fold(0)((a,b) =&gt; a+b)})</pre></div><p>Often your data won't be a clean value from a single source and you will want to join the data together for processing, which can be done with <code class="literal">coGroup</code>. This can be done when you are joining web access logs with transaction data or even just joining two different computations on the same data. Provided that the RDDs have the same key, we can join two RDDs together with <code class="literal">rdd.coGroup(otherRdd)</code>. There are a number of different join functions for different purposes, illustrated in the table at the end of this section.</p><p>The next task you will learn is distributing files among the clusters. We illustrate this by adding GeoIP support and mixing it together with the gradient descent example from the earlier chapter. Sometimes the libraries you use need files distributed along with them. While it is possible to add them to JAR and access them as class objects, Spark provides a simple way to distribute the required files by calling <code class="literal">addFile()</code> as shown in the following code:</p><div class="informalexample"><pre class="programlisting">package pandaspark.examples
import scala.math

import spark.SparkContext
import spark.SparkContext._
import spark.SparkFiles;
import spark.util.Vector

import au.com.bytecode.opencsv.CSVReader

import java.util.Random
import java.io.StringReader
import java.io.File

import com.snowplowanalytics.maxmind.geoip.IpGeo

case class DataPoint(x: Vector, y: Double)

object GeoIpExample {

    def main(args: Array[String]) {
      if (args.length != 2) {
        System.err.println("Usage: GeoIpExample &lt;master&gt; &lt;inputfile&gt;")System.exit(1)
    }
    val master = args(0)
    val inputFile = args(1)
    val iterations = 100
    val maxMindPath = "GeoLiteCity.dat"
    val sc = new SparkContext(master, "GeoIpExample",
                 System.getenv("SPARK_HOME"),Seq(System.getenv("JARS")))
    val invalidLineCounter = sc.accumulator(0)
    val inFile = sc.textFile(inputFile)
    val parsedInput = inFile.flatMap(line =&gt; {
      try {
            val row = (new CSVReader(new StringReader(line))).readNext()Some((row(0),row.drop(1).map(_.toDouble)))}
      catch {
        case _ =&gt; {
          invalidLineCounter += 1
          None
        }
      }
    })val geoFile = sc.addFile(maxMindPath)
    //getLocation gives back an option so we use flatMap to only output if it's a some type
    val ipCountries = parsedInput.flatMapWith(_ =&gt; IpGeo(dbFile = SparkFiles.get(maxMindPath)))((pair, ipGeo) =&gt; {ipGeo.getLocation(pair._1).map(c =&gt; (pair._1,c.countryCode)).toSeq
     })
    ipCountries.cache()
    val countries = ipCountries.values.distinct().collect()
    val countriesBc = sc.broadcast(countries)
    val countriesSignal = ipCountries.mapValues(country =&gt; countriesBc.value.map(s =&gt; if (country == s) 1 
           else 0
    ))
    val dataPoints = parsedInput.join(countriesSignal).map(input =&gt; {
          input._2 match {
            case (countryData, originalData) =&gt; DataPoint(new Vector(countryData++originalData.slice(1,originalData.size-2)),originalData(originalData.size-1))
           }
       })
    countriesSignal.cache()
    dataPoints.cache()
    val rand = new Random(53)
    var w = Vector(dataPoints.first.x.length, _ =&gt; rand.nextDouble)
    for (i &lt;- 1 to iterations) {
      val gradient = dataPoints.map(p =&gt;(1 / (1 + math.exp(-p.y*(w dot p.x))) - 1) * p.y * p.x).reduce(_ + _)
        w -= gradient
    }
    println("Final w: "+w)
  }
}</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note17"></a>Note</h3><p>There have been issues with <code class="literal">addFile</code> in the local mode in the past. If you run into this, you can try the workaround that the Shark (a tool we cover later) developers used (essentially using the result from <code class="literal">addFile</code> if it exists and falling back on the original file if it doesn't).</p><p>To know more about this, you can refer to:</p><p>
<a class="ulink" href="https://github.com/amplab/shark/commit/47c21f55621acd5afb412f54a45c68e141240030" target="_blank">https://github.com/amplab/shark/commit/47c21f55621acd5afb412f54a45c68e141240030</a>.</p></div><p>In the preceding code, you see multiple Spark computations. The first is to determine all the countries that our data covers, so we can map each country to a binary feature. The code then uses a public list of proxies and the reported latency to try to estimate the latency I measured. This also illustrates the use of <code class="literal">mapWith</code>. If you have a mapping job that needs to create a per partition resource, <code class="literal">mapWith</code> can be used to do this. This can be useful for connections to backends or the creation of something like a <span class="strong"><strong>PRNG</strong></span> (<span class="strong"><strong>pseudorandom number generator</strong></span>).<a id="id113" class="indexterm"></a> Some elements also can't be serialized over the wire (such as the IpCountry, in the example), so you have to create them per share. You can also see that we cache a number of our RDDs to keep them from having to be re-computed.</p><p>There are several options when working with multiple RDDs.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec08"></a>Scala RDD functions</h3></div></div></div><p>
<code class="literal">PairRDD</code> functions are based on <code class="literal">combineByKey</code>. All operate on RDDs of type <code class="literal">[K,V]</code> as shown in the following table:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Function</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Parameter options</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Explanation</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Return type</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>
<code class="literal">foldByKey</code>
<a id="id114" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(zeroValue)(func(V,V)=&gt;V)</code>
</p>
<p>
<code class="literal">(zeroValue, partitioner)(func(V,V=&gt;V)</code>
</p>
<p>
<code class="literal">(zeroValue, partitions)(func(V,V=&gt;V)</code>
</p>
</td><td style="" align="left" valign="top">
<p>Merges <a id="id115" class="indexterm"></a>the values using the provided function. Unlike a traditional <code class="literal">fold</code> function over a list, the <code class="literal">zeroValue</code> can be added an arbitrary number of times.</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">RDD[K,V]</code>
</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">reduceByKey</code>
<a id="id116" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(func(V,V)=&gt;V)</code>
</p>
<p>
<code class="literal">(func(V,V)=&gt;V,numTasks)</code>
</p>
</td><td style="" align="left" valign="top">
<p>Parallel version of <code class="literal">reduce</code> that merges the values for <a id="id117" class="indexterm"></a>each key using the provided function and returns an RDD.</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">RDD[K,V]</code>
</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">groupByKey</code>
<a id="id118" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">()</code>
</p>
<p>
<code class="literal">(numPartitions)</code>
</p>
</td><td style="" align="left" valign="top">
<p>Groups<a id="id119" class="indexterm"></a> elements together by key.</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">RDD[K,Seq[V]]</code>
</p>
</td></tr></tbody></table></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec09"></a>Functions for joining PairRDD functions</h3></div></div></div><p>Often when working with two or more key-value RDDs, it is useful to join them together. There are a few different methods to do this depending on what your desired behavior is, as shown in the following table:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Function</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Parameter options</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Explanation</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Return type</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>
<code class="literal">cogroup</code>
<a id="id120" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(otherRDD[K,W]...)</code>
</p>
</td><td style="" align="left" valign="top">
<p>Joins two (or more) RDDs by the shared key. Note that if an element is missing in one RDD but present in the other, one of the Seq will simply be <a id="id121" class="indexterm"></a>empty.</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">RDD[(K,(Seq[V],Seq[W]...))]</code>
</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">join</code>
<a id="id122" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(otherRDD[K,W])</code>
</p>
<p>
<code class="literal">(otherRDD[K,W],partitioner)</code>
</p>
<p>
<code class="literal">(otherRDD[K,W], numPartitions)</code>
</p>
</td><td style="" align="left" valign="top">
<p>Joins<a id="id123" class="indexterm"></a> an RDD with another RDD. The result is only present for elements where the key is present in both RDDs.</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">RDD[(K,(V,W))]</code>
</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">subtractKey</code>
<a id="id124" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(otherRDD[K,W])</code>
</p>
<p>
<code class="literal">(otherRDD[K,W],partitioner)</code>
</p>
<p>
<code class="literal">(otherRDD[K,W], numPartitions)</code>
</p>
</td><td style="" align="left" valign="top">
<p>Returns an <a id="id125" class="indexterm"></a>RDD with only keys not present in the other RDD.</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">RDD[(K,V)]</code>
</p>
</td></tr></tbody></table></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec10"></a>Other PairRDD functions</h3></div></div></div><p>Some functions only make sense when working on key-value pairs.</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Function</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Parameter options</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Explanation</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Return type</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>
<code class="literal">lookup</code>
<a id="id126" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(key: K)</code>
</p>
</td><td style="" align="left" valign="top">
<p>Looks up a <a id="id127" class="indexterm"></a>specific element in the RDD. Uses the RDD's partitioner to figure out which partition(s) to look at.</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">Seq[V]</code>
</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">mapValues</code>
<a id="id128" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(f: V =&gt; U)</code>
</p>
</td><td style="" align="left" valign="top">
<p>A specialized<a id="id129" class="indexterm"></a> version of map for PairRDD when you only want to change the value of the key-value pair. This takes the provided <code class="literal">Map</code> function and applies it to the value. If you need to make your change based on both key and value, you must use one of the normal RDD <code class="literal">Map</code> functions.</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">RDD[(K,U)]</code>
</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">collectAsMap</code>
<a id="id130" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">()</code>
</p>
</td><td style="" align="left" valign="top">
<p>Takes <a id="id131" class="indexterm"></a>an RDD and returns a concrete map. Your RDD must be able to fit into the memory.</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">Map[K, V]</code>
</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">countByKey</code>
<a id="id132" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">()</code>
</p>
</td><td style="" align="left" valign="top">
<p>Counts<a id="id133" class="indexterm"></a> the number of elements for each key.</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">Map[K, Long]</code>
</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">partitionBy</code>
<a id="id134" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(partitioner: Partitioner, mapSideCombine: Boolean)</code>
</p>
</td><td style="" align="left" valign="top">
<p>Returns a new RDD with the same data but partitioned by the new <code class="literal">Partitioner</code>, and <code class="literal">mapSideCombine</code> controls <a id="id135" class="indexterm"></a>Spark group values with the same key together before repartitioning. Defaults to false; set to true if you have a large percent of duplicate keys.</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">RDD[(K,V)]</code>
</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">flatMapValues</code>
<a id="id136" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(f: V =&gt; TraversableOnce[U])</code>
</p>
</td><td style="" align="left" valign="top">
<p>Similar to <code class="literal">mapValues</code>. A specialized version of <code class="literal">flatMap</code> for PairRDDs<a id="id137" class="indexterm"></a> when you only want to change the value of the key-value pair. Takes the provided Map function and applies it to the value. The resulting sequence is then "flattened"; that is, instead of getting <code class="literal">Seq[Seq[V]]</code>, you get <code class="literal">Seq[V]</code>. If you need to make your change based on both key and value,<a id="id138" class="indexterm"></a> you must use one of the normal RDD map functions.</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">RDD[(K,U)]</code>
</p>
</td></tr></tbody></table></div><p>For information on saving <code class="literal">PairRDD</code>, refer to the previous chapter.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec11"></a>DoubleRDD functions</h3></div></div></div><p>Spark defines a number of convenience functions, which work when your RDD is comprised of <code class="literal">double</code> data types.</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Function</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Arguments</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Returns</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>
<code class="literal">mean</code>
<a id="id139" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">()</code>
<a id="id140" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>Average of RDDs elements</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">sampleStdev</code>
<a id="id141" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">()</code>
<a id="id142" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>Standard deviation for a sample rather than a population (divides by N-1 rather than N)</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">Stats</code>
<a id="id143" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">()</code>
</p>
</td><td style="" align="left" valign="top">
<p>Mean, <a id="id144" class="indexterm"></a>variance, and count as a StatCounter</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">Stdev</code>
<a id="id145" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">()</code>
</p>
</td><td style="" align="left" valign="top">
<p>Standard <a id="id146" class="indexterm"></a>deviation (for population)</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">Sum</code>
<a id="id147" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">()</code>
</p>
</td><td style="" align="left" valign="top">
<p>Sum <a id="id148" class="indexterm"></a>of the elements</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">variance</code>
<a id="id149" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">()</code>
</p>
</td><td style="" align="left" valign="top">
<p>Variance<a id="id150" class="indexterm"></a> of RDDs elements</p>
</td></tr></tbody></table></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec12"></a>General RDD functions</h3></div></div></div><p>The remaining RDD functions are defined on all RDDs.</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Function</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Arguments</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Returns</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>
<code class="literal">aggregate</code>
<a id="id151" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(zeroValue: U)(seqOp: (U,T) =&gt; T, combOp (U, U) =&gt; U)</code>
</p>
</td><td style="" align="left" valign="top">
<p>Aggregates<a id="id152" class="indexterm"></a> all the elements of each partition of an RDD, and then combines them using <code class="literal">combOp</code>. The argument <code class="literal">zeroValue</code> should be neutral (that is, <code class="literal">0</code> for <code class="literal">+</code> and <code class="literal">1</code> for <code class="literal">*</code>).</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">cache</code>
<a id="id153" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">()</code>
</p>
</td><td style="" align="left" valign="top">
<p>Caches <a id="id154" class="indexterm"></a>an RDD reused without recomputing. Same as <code class="literal">persist</code>(<code class="literal">StorageLevel.MEMORY_ONLY</code>).</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">collect</code>
<a id="id155" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">()</code>
</p>
</td><td style="" align="left" valign="top">
<p>An array<a id="id156" class="indexterm"></a> of all the <a id="id157" class="indexterm"></a>elements in the RDD.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">count</code>
<a id="id158" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">()</code>
</p>
</td><td style="" align="left" valign="top">
<p>The number of elements in an RDD.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">countByValue</code>
<a id="id159" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">()</code>
</p>
</td><td style="" align="left" valign="top">
<p>A map<a id="id160" class="indexterm"></a> of value to the number of times that value occurs.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">distinct</code>
<a id="id161" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">()</code>
</p>
<p>
<code class="literal">(partitions: Int)</code>
</p>
</td><td style="" align="left" valign="top">
<p>RDD <a id="id162" class="indexterm"></a>containing only distinct elements.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">filter</code>
<a id="id163" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(f: T =&gt; Boolean)</code>
</p>
</td><td style="" align="left" valign="top">
<p>RDD containing only elements matching <code class="literal">f</code>.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">filterWith</code>
<a id="id164" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(construct A: Int =&gt; A )(f: (T, A) =&gt; Boolean)</code>
</p>
</td><td style="" align="left" valign="top">
<p>Similar to <code class="literal">filter</code>, but <code class="literal">f</code> takes an additional <a id="id165" class="indexterm"></a>parameter generated by <code class="literal">constructA</code>, which is called per partition. The original motivation for this came from providing PRNG generation per partition.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">first</code>
<a id="id166" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">()</code>
</p>
</td><td style="" align="left" valign="top">
<p>The "first" element <a id="id167" class="indexterm"></a>of the RDD.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">flatMap</code>
<a id="id168" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(f: T =&gt; TraversableOnce[U])</code>
</p>
</td><td style="" align="left" valign="top">
<p>An RDD<a id="id169" class="indexterm"></a> of type <code class="literal">U</code>.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">fold</code>
<a id="id170" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(zeroValue: T)(op: (T,T) =&gt; T)</code>
</p>
</td><td style="" align="left" valign="top">
<p>Merges<a id="id171" class="indexterm"></a> values using the provided operation, first on each partition, and then merges the merged result.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">foreach</code>
<a id="id172" class="indexterm"></a>
<a id="id173" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(f: T =&gt; Unit)</code>
</p>
</td><td style="" align="left" valign="top">
<p>Applies the function <code class="literal">f</code> to each element.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">groupBy</code>
<a id="id174" class="indexterm"></a>
<a id="id175" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(f: T =&gt; K)</code>
</p>
<p>
<code class="literal">(f: T =&gt; K, p: Partitioner)</code>
</p>
<p>
<code class="literal">(f: T =&gt; K, numPartitions:Int)</code>
</p>
</td><td style="" align="left" valign="top">
<p>Takes in an RDD and produces an RDD pair of type <code class="literal">(K,Seq[T])</code> using the result of <code class="literal">f</code> for the key of each element.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">keyBy</code>
<a id="id176" class="indexterm"></a>
<a id="id177" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(f: T =&gt; K)</code>
</p>
<p>
<code class="literal">(f: T =&gt; K, p: Partitioner)</code>
</p>
<p>
<code class="literal">(f: T =&gt; K, numPartitions:Int)</code>
</p>
</td><td style="" align="left" valign="top">
<p>Same as <code class="literal">groupBy</code>, but does not group results together with duplicate keys. Returns an RDD of <code class="literal">(K,T)</code>.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">map</code>
<a id="id178" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(f: T =&gt; U)</code>
</p>
</td><td style="" align="left" valign="top">
<p>An RDD <a id="id179" class="indexterm"></a>of the result of applying <code class="literal">f</code> to every element in the input RDD.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">mapPartitions</code>
<a id="id180" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(f: Iterator[T] =&gt; Iterator[U])</code>
</p>
</td><td style="" align="left" valign="top">
<p>Similar<a id="id181" class="indexterm"></a> to <code class="literal">map</code>, except the provided function takes and returns an <code class="literal">Iterator</code> and is applied to each partition.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">mapPartitionsWithIndex</code>
<a id="id182" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(f: (Int, Iterator[T]) =&gt; Iterator[U], preservePartitions)</code>
</p>
</td><td style="" align="left" valign="top">
<p>Same<a id="id183" class="indexterm"></a> as <code class="literal">mapPartitions</code>, but also provides the index of the original partition.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">mapWith</code>
<a id="id184" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(constructA: Int =&gt; A)(f: (T, A) =&gt; U)</code>
</p>
</td><td style="" align="left" valign="top">
<p>Similar to <a id="id185" class="indexterm"></a>
<code class="literal">map</code>, but <code class="literal">f</code> takes an additional parameter generated by <code class="literal">constructorA</code>, which is called per partition. The original motivation for this came from providing PRNG generation per partition.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">persist</code>
<a id="id186" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">()</code>
</p>
<p>
<code class="literal">(newLevel: StorageLevel)</code>
</p>
</td><td style="" align="left" valign="top">
<p>Sets the<a id="id187" class="indexterm"></a> RDD storage level, which can cause the RDD to be stored after it is computed. Different <code class="literal">StorageLevels</code> can be seen in <code class="literal">StorageLevel.scala</code> (<code class="literal">NONE</code>, <code class="literal">DISK_ONLY</code>, <code class="literal">MEMORY_ONLY</code>, and <code class="literal">MEMORY_AND_DISK</code> are the common ones).</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">pipe</code>
<a id="id188" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(command: Seq[String])</code>
</p>
<p>
<code class="literal">(command: Seq[String], env: Map[String, String])</code>
</p>
</td><td style="" align="left" valign="top">
<p>Takes an<a id="id189" class="indexterm"></a> RDD and calls the specified command with the optional environment and pipes each element through the command. Results in an RDD of <code class="literal">String</code> type.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">sample</code>
<a id="id190" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(withReplacement: Boolean, fraction: Double, seed: Int)</code>
</p>
</td><td style="" align="left" valign="top">
<p>RDD of that <a id="id191" class="indexterm"></a>fraction.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">takeSample</code>
<a id="id192" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(withReplacement: Boolean, num: Int, seed: Int)</code>
</p>
</td><td style="" align="left" valign="top">
<p>An array of<a id="id193" class="indexterm"></a> the requested number of elements. This works by oversampling the RDD and then grabbing a subset.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">toDebugString</code>
<a id="id194" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">()</code>
</p>
</td><td style="" align="left" valign="top">
<p>A handy function<a id="id195" class="indexterm"></a> that outputs the recursive <code class="literal">deps</code> of the RDD.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">union</code>
<a id="id196" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(other: RDD[T])</code>
</p>
</td><td style="" align="left" valign="top">
<p>An RDD <a id="id197" class="indexterm"></a>containing elements of both RDDs. Duplicates are not removed.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">unpersist</code>
<a id="id198" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">()</code>
</p>
</td><td style="" align="left" valign="top">
<p>Remove<a id="id199" class="indexterm"></a> all the persistent blocks of the RDD from the memory/disk.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">zip</code>
<a id="id200" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(other: RDD[U])</code>
</p>
</td><td style="" align="left" valign="top">
<p>Requires<a id="id201" class="indexterm"></a> that the RDDs have the same number of partitions of the same size. Returns an RDD of key-value pairs <code class="literal">RDD[T,U]</code>.</p>
</td></tr></tbody></table></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec13"></a>Java RDD functions</h3></div></div></div><p>Many of the Java RDD functions are quite similar to the Scala RDD functions, but the type signatures are somewhat different.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec14"></a>Spark Java function classes</h3></div></div></div><p>For the<a id="id202" class="indexterm"></a> Java RDD API, we need to extend one of the provided function classes when implementing our function. The following table shows some of the Spark Java functions:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Name</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Parameters</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Purpose</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>
<code class="literal">Function&lt;T,R&gt;</code>
<a id="id203" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">R apply(T t)</code>
</p>
</td><td style="" align="left" valign="top">
<p>This function takes something of type<a id="id204" class="indexterm"></a> <code class="literal">T</code> and returns something of type <code class="literal">R</code>. Commonly used for maps.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">DoubleFunction&lt;T&gt;</code>
<a id="id205" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">Double apply(T t)</code>
</p>
</td><td style="" align="left" valign="top">
<p>Same as <a id="id206" class="indexterm"></a>
<code class="literal">Function&lt;T, Double&gt;</code>, but the result of the map-like call returns a <code class="literal">JavaDoubleRDD</code> (for summary statistics).</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">PairFunction&lt;T, K, V&gt;</code>
<a id="id207" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">Tuple2&lt;K, V&gt; apply(T t)</code>
</p>
</td><td style="" align="left" valign="top">
<p>This <a id="id208" class="indexterm"></a>function results in a <code class="literal">JavaPairRDD</code>. If working on a <code class="literal">JavaPairRDD&lt;A,B&gt;</code>, let <code class="literal">T</code> be of type <code class="literal">Tuple2&lt;A,B&gt;</code>.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">FlatMapFunction&lt;T, R&gt;</code>
<a id="id209" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">Iterable&lt;R&gt; apply(T t)</code>
</p>
</td><td style="" align="left" valign="top">
<p>This function is for producing an <a id="id210" class="indexterm"></a>RDD through a <code class="literal">flatMap </code>function.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">PairFlatMapFunction&lt;T, K, V&gt;</code>
<a id="id211" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">Iterable&lt;Tuple2&lt;K, V&gt;&gt; apply(T t)</code>
</p>
</td><td style="" align="left" valign="top">
<p>This function results in a <code class="literal">JavaPairRDD</code>. If working on a <code class="literal">JavaPairRDD&lt;A,B&gt;</code>, let <code class="literal">T</code> be of <a id="id212" class="indexterm"></a>type <code class="literal">Tuple2&lt;A,B&gt;</code>.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">DoubleFlatMapFunction&lt;T&gt;</code>
<a id="id213" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">Iterable&lt;Double&gt; apply(T t)</code>
</p>
</td><td style="" align="left" valign="top">
<p>Same <a id="id214" class="indexterm"></a>as <code class="literal">FlatMapFunction&lt;T, Double&gt;</code>, but the result of the map-like call returns a <code class="literal">JavaDoubleRDD</code> (for summary statistics).</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">Function2&lt;T1, T2, R&gt;</code>
<a id="id215" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">R apply(T1 t1, T2 t2)</code>
</p>
</td><td style="" align="left" valign="top">
<p>This<a id="id216" class="indexterm"></a> function is for taking two inputs and returning an output. Used by <code class="literal">fold</code> and similar functions.</p>
</td></tr></tbody></table></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec02"></a>Common Java RDD functions</h4></div></div></div><p>The following table explains RDD functions that are available regardless of the type of RDD.</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Name</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Parameters</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Purpose</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>
<code class="literal">cache</code>
<a id="id217" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">()</code>
</p>
</td><td style="" align="left" valign="top">
<p>Persists <a id="id218" class="indexterm"></a>an RDD in memory.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">coalesce</code>
<a id="id219" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">numPartitions: Int</code>
</p>
</td><td style="" align="left" valign="top">
<p>Returns<a id="id220" class="indexterm"></a> a new RDD with <code class="literal">numPartitions</code> partitions.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">collect</code>
<a id="id221" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">()</code>
</p>
</td><td style="" align="left" valign="top">
<p>Returns<a id="id222" class="indexterm"></a> the List representation of the entire RDD.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">count</code>
<a id="id223" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">()</code>
</p>
</td><td style="" align="left" valign="top">
<p>Number<a id="id224" class="indexterm"></a> of elements.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">countByValue</code>
<a id="id225" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">()</code>
</p>
</td><td style="" align="left" valign="top">
<p>A map <a id="id226" class="indexterm"></a>of each unique value to the number of times that a value shows up.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">distinct</code>
<a id="id227" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">()</code>
</p>
<p>
<code class="literal">(Int numPartitions)</code>
</p>
</td><td style="" align="left" valign="top">
<p>An RDD<a id="id228" class="indexterm"></a> consisting of all the distinct elements of the RDD optionally in the provided number of partitions.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">filter</code>
<a id="id229" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(Function&lt;T, Boolean&gt; f)</code>
</p>
</td><td style="" align="left" valign="top">
<p>An<a id="id230" class="indexterm"></a> RDD containing only elements for which <code class="literal">f</code> returns <code class="literal">true</code>.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">first</code>
<a id="id231" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">()</code>
</p>
</td><td style="" align="left" valign="top">
<p>The <a id="id232" class="indexterm"></a>first element of the RDD.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">flatMap</code>
<a id="id233" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(FlatMapFunction&lt;T, U&gt; f)</code>
</p>
<p>
<code class="literal">(DoubleFlatMapFunction&lt;T&gt; f)</code>
</p>
<p>
<code class="literal">(PairFlatMapFunction&lt;T, K, V&gt; f)</code>
</p>
</td><td style="" align="left" valign="top">
<p>An <a id="id234" class="indexterm"></a>RDD of the specified type (<code class="literal">U</code>, <code class="literal">Double</code>, and <code class="literal">Pair&lt;K,V&gt;</code> respectively).</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">fold</code>
<a id="id235" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(T zeroValue, Function2&lt;T, T, T&gt; f)</code>
</p>
</td><td style="" align="left" valign="top">
<p>The<a id="id236" class="indexterm"></a> result <code class="literal">T</code> and each partition is folded individually with the <code class="literal">zeroValue</code> and then the results are folded.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">foreach</code>
<a id="id237" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(VoidFunction&lt;T&gt; f)</code>
</p>
</td><td style="" align="left" valign="top">
<p>Applies <a id="id238" class="indexterm"></a>the function to each element in the RDD.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">groupBy</code>
<a id="id239" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(Function&lt;T, K&gt; f)</code>
</p>
<p>
<code class="literal">(Function&lt;T, K&gt; f, Int numPartitions)</code>
</p>
</td><td style="" align="left" valign="top">
<p>A <a id="id240" class="indexterm"></a>
<code class="literal">JavaPairRDD</code> of grouped elements.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">map</code>
<a id="id241" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(DoubleFunction&lt;T&gt; f)</code>
</p>
<p>
<code class="literal">(PairFunction&lt;T, K2, V2&gt; f)</code>
</p>
<p>
<code class="literal">(Function&lt;T, U&gt; f)</code>
</p>
</td><td style="" align="left" valign="top">
<p>An RDD <a id="id242" class="indexterm"></a>of the appropriate type for the input function (see the previous table) by calling the provided function on each element in the input RDD.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">mapPartitions</code>
<a id="id243" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(DoubleFunction&lt;Iterator&lt;T&gt;&gt; f)</code>
</p>
<p>
<code class="literal">(PairFunction&lt;Iterator&lt;T&gt;, K2, V2&gt; f)</code>
</p>
<p>
<code class="literal">(Function&lt;Iterator&lt;T&gt;, U&gt; f)</code>
</p>
</td><td style="" align="left" valign="top">
<p>Similar<a id="id244" class="indexterm"></a> to <code class="literal">map</code>, but the provided function is called per partition. This can be useful if you have some setup work that you need to do for each partition.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">reduce</code>
<a id="id245" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(Function2&lt;T, T, T&gt; f)</code>
</p>
</td><td style="" align="left" valign="top">
<p>Uses<a id="id246" class="indexterm"></a> the provided function to reduce all the elements.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">sample</code>
<a id="id247" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(Boolean withReplacement, Double fraction, Int seed)</code>
</p>
</td><td style="" align="left" valign="top">
<p>A smaller<a id="id248" class="indexterm"></a> RDD consisting of only the requested fraction of the data.</p>
</td></tr></tbody></table></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec15"></a>Methods for combining JavaPairRDD functions</h3></div></div></div><p>There are a number of different functions we can use to combine RDDs as shown in the following table:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Name</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Parameters</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Purpose</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>
<code class="literal">subtract</code>
<a id="id249" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(JavaRDD&lt;T&gt; other)</code>
</p>
<p>
<code class="literal">(JavaRDD&lt;T&gt; other, Partitioner p)</code>
</p>
<p>
<code class="literal">(JavaRDD&lt;T&gt; other, Int numPartitions)</code>
</p>
</td><td style="" align="left" valign="top">
<p>Returns <a id="id250" class="indexterm"></a>an RDD with only the elements initially present in the first RDD and not present in the other RDD.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">union</code>
<a id="id251" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(JavaRDD&lt;T&gt; other)</code>
</p>
</td><td style="" align="left" valign="top">
<p>The<a id="id252" class="indexterm"></a> union of the two RDDs.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">zip</code>
<a id="id253" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(JavaRDD&lt;U&gt; other)</code>
</p>
</td><td style="" align="left" valign="top">
<p>Returns an <a id="id254" class="indexterm"></a>RDD of key-value pairs <code class="literal">RDD[T,U]</code>.</p>
<p>
<span class="strong"><strong>Important</strong></span>: This function requires that the RDDs have the same number of partitions and size.</p>
</td></tr></tbody></table></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec03"></a>JavaPairRDD functions</h4></div></div></div><p>The following table explains some functions that are only defined on key-value pair RDDs:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Name</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Parameters</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Purpose</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>
<code class="literal">cogroup</code>
<a id="id255" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(JavaPairRDD&lt;K, W&gt;other)</code>
</p>
<p>
<code class="literal">(JavaPairRDD&lt;K, W&gt;other, Int numPartitions)</code>
</p>
<p>
<code class="literal">(JavaPairRDD&lt;K, W&gt;other1,JavaPairRDD&lt;K, W&gt;other2)</code>
</p>
<p>
<code class="literal">(JavaPairRDD&lt;K, W&gt;other1, JavaPairRDD&lt;K, W&gt;other2, IntnumPartitions)</code>
</p>
</td><td style="" align="left" valign="top">
<p>Joins two<a id="id256" class="indexterm"></a> (or more) RDDs by the shared key. Note that if an element is missing in one RDD but present in the other, one of the list will simply be empty.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">combineByKey</code>
<a id="id257" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(Function&lt;V, C&gt;createCombiner,</code>
<code class="literal">Function2&lt;C, V, C&gt;mergeValue,</code>
<code class="literal">Function2&lt;C,C,C&gt;mergeCombiners)</code>
</p>
</td><td style="" align="left" valign="top">
<p>Generic <a id="id258" class="indexterm"></a>function to combine elements by keys. The argument <code class="literal">createCombiner </code>turns something of type <code class="literal">V</code> into something of type <code class="literal">C</code>, <code class="literal">mergeValue</code> adds a type <code class="literal">V</code> to a type <code class="literal">C</code>, and <code class="literal">mergeCombiners</code> is used to combine two <code class="literal">C</code> types into a single <code class="literal">C</code>.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">collectAsMap</code>
<a id="id259" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">()</code>
</p>
</td><td style="" align="left" valign="top">
<p>Returns a <a id="id260" class="indexterm"></a>
<code class="literal">map</code> of the key-value pairs.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">countByKey</code>
<a id="id261" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">()</code>
</p>
</td><td style="" align="left" valign="top">
<p>Returns <a id="id262" class="indexterm"></a>a <code class="literal">map</code> of the key to the number of elements with that key.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">flatMapValues</code>
<a id="id263" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(Function[T] f, Iterable[V] v)</code>
</p>
</td><td style="" align="left" valign="top">
<p>Returns an<a id="id264" class="indexterm"></a> RDD of type <code class="literal">V</code>.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">join</code>
<a id="id265" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(JavaPairRDD&lt;K, W&gt; other)</code>
</p>
<p>
<code class="literal">(JavaPairRDD&lt;K, W&gt; other, Int integers)</code>
</p>
</td><td style="" align="left" valign="top">
<p>Joins<a id="id266" class="indexterm"></a> an RDD with another RDD. The result is only present for elements where the key is present in both RDDs.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">keys</code>
<a id="id267" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">()</code>
</p>
</td><td style="" align="left" valign="top">
<p>Returns<a id="id268" class="indexterm"></a> an RDD of only the keys.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">lookup</code>
<a id="id269" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(Key k)</code>
</p>
</td><td style="" align="left" valign="top">
<p>Looks up <a id="id270" class="indexterm"></a>a specific element in the RDD. Uses the RDDs' partitioner to figure out which partition(s) to look at.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">reduceByKey</code>
<a id="id271" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(Function2[V,V,V] f)</code>
</p>
</td><td style="" align="left" valign="top">
<p>The <a id="id272" class="indexterm"></a>
<code class="literal">reduceByKey</code> is the parallel version of <code class="literal">reduce</code>, which merges the values for each key using the provided function and returns an RDD.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">sortByKey</code>
<a id="id273" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(Comparator[K] comp, Boolean ascending)</code>
</p>
<p>
<code class="literal">(Comparator[K] comp)</code>
</p>
<p>
<code class="literal">(Boolean ascending)</code>
</p>
</td><td style="" align="left" valign="top">
<p>Sorts<a id="id274" class="indexterm"></a> the RDDs by keys, so each partition contains a fixed range.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">values</code>
<a id="id275" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">()</code>
</p>
</td><td style="" align="left" valign="top">
<p>Returns <a id="id276" class="indexterm"></a>an RDD of only the values.</p>
</td></tr></tbody></table></div></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec37"></a>Manipulating your RDD in Python</h2></div></div><hr /></div><p>Spark has a more limited API than Java and Scala, but supports most of the core functionalities.</p><p>The hallmarks <a id="id277" class="indexterm"></a>of a MapReduce system are the two commands: <code class="literal">map</code> and <code class="literal">reduce</code>. You've seen the <code class="literal">map</code> function used in the past chapters. The <code class="literal">map</code> function works by taking in a function that works on each individual element in the input RDD and produces a new output element. For example, to produce a new RDD where you <a id="id278" class="indexterm"></a>have added one to every number, you would use <code class="literal">rdd.map(lambda x: x+1)</code>. It's important to understand that the <code class="literal">map</code> function and the other Spark functions do not transform the<a id="id279" class="indexterm"></a> existing elements, rather they return a new RDD with the new elements. The <code class="literal">reduce</code> function takes a function that operates on pairs to combine all the data. This is returned to the calling program. If you were to sum all the elements, you would use <code class="literal">rdd.reduce(lambda x, y: x+y)</code>.</p><p>The <code class="literal">flatMap</code> function is a useful utility that allows you to write a function which returns an <code class="literal">Iterable</code> object of the type you want and then flattens the results. A simple example of this is a case where you want to parse all the data, but some of the data may not be parsed. The <code class="literal">flatMap</code> function can output an empty list if it failed or a list with the success if it worked. In addition to <code class="literal">reduce</code>, there is a corresponding <code class="literal">reduceByKey</code> function that works on RDDs which are key-value pairs and produces another RDD.</p><p>Many of the mapping operations are also defined with a partition variant. In this case, the function you need to provide takes and returns an <code class="literal">Iterator</code> object that represents all the data on that <a id="id280" class="indexterm"></a>partition. This can be quite useful if the operation you need to perform has to do extensive work on each partition, for example, establishing a connection to a backend server.</p><p>Often, your data can be <a id="id281" class="indexterm"></a>expressed with key-value mappings. As such, many of the functions defined on the Python RDD class only work if your data is in a key-value mapping. The <code class="literal">mapValues</code> function is used when you only want to update the key-value pair you are working with.</p><p>Another variant on the traditional map function is <code class="literal">mapPartitions</code>, which works on a per-partition level. The primary reason for using <code class="literal">mapPartitions</code> is to create the setup for your map function, which can't be serialized across the network. A good example of this is creating an expensive connection to a backend service or parsing some expensive side input.</p><div class="informalexample"><pre class="programlisting">def f(iterator):
      //Expensive work goes here
     for i in iterator:
       yield per_element_function(i)</pre></div><p>In addition to simple operations on the data, Spark provides support for <code class="literal">broadcast</code> and <code class="literal">accumulator</code> values. The <code class="literal">broadcast</code> values can be used to broadcast a read-only value to all the partitions that can save having to reserialize a given value multiple times. Accumulators allow all the partitions to add to the <code class="literal">accumulator</code> and the result can then be read on the master. You can create an accumulator by doing <code class="literal">counter = sc.accumulator(initialValue)</code>. If you want to add custom behavior, you can also provide an <code class="literal">AccumulatorParam</code> as an argument to the <code class="literal">accumulator</code> function. The return can then be incremented as <code class="literal">counter += x</code> on any of the workers. The resulting value can then be read with <code class="literal">counter.value()</code>. The <code class="literal">broadcast</code> value is created with <code class="literal">bc = sc.broadcast(value)</code> and then accessed by <code class="literal">bc.value()</code> by any worker. The <code class="literal">accumulator</code> value can only be read on the master and the <code class="literal">broadcast</code> value can be read on all the partitions.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec16"></a>Standard RDD functions</h3></div></div></div><p>The following table explains some of the functions that are available on all RDDs in Python:</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Name</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Parameters</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Purpose</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>
<code class="literal">flatMap</code>
<a id="id282" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(f, preservesPartitioning=False)</code>
</p>
</td><td style="" align="left" valign="top">
<p>Takes a<a id="id283" class="indexterm"></a> function that returns an <code class="literal">Iterator </code>object of type <code class="literal">U</code> for each input of type <code class="literal">T</code> and returns a flattened RDD of type <code class="literal">U</code>.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">mapPartitions</code>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(f, preservesPartitioning=False)</code>
</p>
</td><td style="" align="left" valign="top">
<p>Takes a function, <a id="id284" class="indexterm"></a>and the function takes in an <code class="literal">Iterator</code> of type <code class="literal">T</code> and returns an Iterator of type <code class="literal">U</code> and results in an RDD of type <code class="literal">U</code>. So, for example, if we provided a function that took in an iterator of integers and returned an iterator of strings and called it on an RDD of integers we would get back an RDD of strings. Useful for map operations with expensive per machine setup work.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">filter</code>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(f)</code>
</p>
</td><td style="" align="left" valign="top">
<p>Takes a function <a id="id285" class="indexterm"></a>and returns an RDD with only the elements that the function returns <code class="literal">true</code> for.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">distinct</code>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">()</code>
</p>
</td><td style="" align="left" valign="top">
<p>Returns an <a id="id286" class="indexterm"></a>RDD with distinct elements (for example, entering 1, 1, and 2 will output 1, 2).</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">union</code>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(other)</code>
</p>
</td><td style="" align="left" valign="top">
<p>Returns a <a id="id287" class="indexterm"></a>union of two RDDs.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">cartesian</code>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(other)</code>
</p>
</td><td style="" align="left" valign="top">
<p>Returns the<a id="id288" class="indexterm"></a> cartesian product of the RDD with the other RDD.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">groupBy</code>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(f, numPartitions=None)</code>
</p>
</td><td style="" align="left" valign="top">
<p>Returns an<a id="id289" class="indexterm"></a> RDD with the elements grouped together for the value that f outputs.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">pipe</code>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(command, env={})</code>
</p>
</td><td style="" align="left" valign="top">
<p>Pipes each <a id="id290" class="indexterm"></a>element of the RDD to the provided command and returns an RDD of the result.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">foreach</code>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">f</code>
</p>
</td><td style="" align="left" valign="top">
<p>Applies the<a id="id291" class="indexterm"></a> function <code class="literal">f</code> to each element in the RDD.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">reduce</code>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">f</code>
</p>
</td><td style="" align="left" valign="top">
<p>Reduces<a id="id292" class="indexterm"></a> the elements using the provided function.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">fold</code>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">zeroValue, op</code>
</p>
</td><td style="" align="left" valign="top">
<p>Each partition <a id="id293" class="indexterm"></a>is folded individually with <code class="literal">zeroValue</code> and then the results are folded.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">countByValue</code>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">()</code>
</p>
</td><td style="" align="left" valign="top">
<p>Returns a<a id="id294" class="indexterm"></a> dictionary mapping each distinct value to the number of times it is found in the RDD.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">take</code>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">num</code>
</p>
</td><td style="" align="left" valign="top">
<p>Returns<a id="id295" class="indexterm"></a> a list of <code class="literal">num</code> elements. This can be slow for large values of <code class="literal">num</code>, so use <code class="literal">collect</code> if you want to get back the entire RDD.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">partitionBy</code>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(numPartitions,partitionFunc= hash)</code>
</p>
</td><td style="" align="left" valign="top">
<p>Makes a new RDD partitioned by the provided partitioning<a id="id296" class="indexterm"></a> function. The <code class="literal">partitionFunc </code>argument simply needs to map the input key to the integer space, at which point <code class="literal">partionBy</code> takes it mod <code class="literal">numPartitions</code>.</p>
</td></tr></tbody></table></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec17"></a>PairRDD functions</h3></div></div></div><p>The following table explains some functions that are only available on key-value pair functions:<a id="id297" class="indexterm"></a>
</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Name</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Parameters</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Purpose</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>
<code class="literal">collectAsMap</code>
<a id="id298" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">()</code>
</p>
</td><td style="" align="left" valign="top">
<p>Returns a dictionary<a id="id299" class="indexterm"></a> consisting of all the key-value pairs of the RDD.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">reduceByKey</code>
<a id="id300" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(func, numPartitions=None)</code>
</p>
</td><td style="" align="left" valign="top">
<p>The <code class="literal">reduceByKey</code> function is the parallel version<a id="id301" class="indexterm"></a> of <code class="literal">reduce</code>, which merges the values for each key using the provided function and returns an RDD.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">countByKey</code>
<a id="id302" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">()</code>
</p>
</td><td style="" align="left" valign="top">
<p>Returns a<a id="id303" class="indexterm"></a> dictionary of the number of elements for each key.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">join</code>
<a id="id304" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(other, numPartitions=None)</code>
</p>
</td><td style="" align="left" valign="top">
<p>Joins an RDD with another RDD. The result is<a id="id305" class="indexterm"></a> only present for elements where the key is present in both RDDs. The value that gets stored for each key is a tuple of the values from each RDD.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">rightOuterJoin</code>
<a id="id306" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(other, numPartitions=None)</code>
</p>
</td><td style="" align="left" valign="top">
<p>Joins an RDD with another RDD. This function<a id="id307" class="indexterm"></a> outputs a given key-value pair only if the key is present in the RDD being joined with. If the key is not present in the source RDD, the first value in the tuple will be <code class="literal">None</code>.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">leftOuterJoin</code>
<a id="id308" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(other, numPartitions=None)</code>
</p>
</td><td style="" align="left" valign="top">
<p>Joins <a id="id309" class="indexterm"></a>an RDD with another RDD. This function outputs a given key-value pair only if the key is present in the source RDD. If the key is not present in the other RDD, the second value in the tuple will be <code class="literal">None</code>.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">combineByKey</code>
<a id="id310" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(createCombiner, mergeValues, mergeCombiners)</code>
</p>
</td><td style="" align="left" valign="top">
<p>Combines elements by keys. This function takes an RDD of type <code class="literal">(K,V)</code> and<a id="id311" class="indexterm"></a> returns an RDD of type <code class="literal">(K,C)</code>. The argument <code class="literal">createCombiner</code> turns something of type <code class="literal">V</code> into something of type <code class="literal">C</code>, <code class="literal">mergeValue</code> adds a <code class="literal">V</code> to a <code class="literal">C</code>, and <code class="literal">mergeCombiners</code> is used to combine two <code class="literal">C</code> types into a single <code class="literal">C</code>.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">groupByKey</code>
<a id="id312" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(numPartitions=None)</code>
</p>
</td><td style="" align="left" valign="top">
<p>Groups<a id="id313" class="indexterm"></a> the values in the RDD by the keys they have.</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>
<code class="literal">cogroup</code>
<a id="id314" class="indexterm"></a>
</p>
</td><td style="" align="left" valign="top">
<p>
<code class="literal">(other, numPartitions=None)</code>
</p>
</td><td style="" align="left" valign="top">
<p>Joins<a id="id315" class="indexterm"></a> two (or more) RDDs by the shared key. Note that if an element is missing in one RDD but present in the other one, the list will simply be empty.</p>
</td></tr></tbody></table></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec38"></a>Links and references</h2></div></div><hr /></div><p>Some of the useful links for <a id="id316" class="indexterm"></a>referencing are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<a class="ulink" href="http://www.scala-lang.org/api/current/index.html#scala.collection.immutable.List" target="_blank">http://www.scala-lang.org/api/current/index.html#scala.collection.immutable.List</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://spark.incubator.apache.org/docs/latest/api/core/index.html#spark.api.java.JavaRDD" target="_blank">http://spark.incubator.apache.org/docs/latest/api/core/index.html#spark.api.java.JavaRDD</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://spark.incubator.apache.org/docs/latest/api/core/index.html#spark.api.java.JavaPairRDD" target="_blank">http://spark.incubator.apache.org/docs/latest/api/core/index.html#spark.api.java.JavaPairRDD</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://spark.incubator.apache.org/docs/latest/api/core/index.html#spark.SparkContext" target="_blank">http://spark.incubator.apache.org/docs/latest/api/core/index.html#spark.SparkContext</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://spark.incubator.apache.org/docs/latest/api/core/index.html#packa" target="_blank">http://spark.incubator.apache.org/docs/latest/api/core/index.html#packa</a>
</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec39"></a>Summary</h2></div></div><hr /></div><p>This chapter looked at how to perform computations on our data in a distributed fashion once loaded into an RDD. Combined with our knowledge of how to load and save RDDs, we can now write distributed programs using Spark. In the next chapter, we will look at how to use Spark with Hive.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch07"></a>Chapter 7. Shark – Using Spark with Hive</h2></div></div></div><p>This chapter will cover how to use Spark with Hive, and how to integrate Hive queries with a Spark program. This chapter isn't needed to understand any of the following chapters, so if you don't want to learn about Hive, skip ahead on to the next chapter.</p><p>The following topics are covered in this chapter:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Uses of Hive/Shark</p></li><li style="list-style-type: disc"><p>How to install Shark</p></li><li style="list-style-type: disc"><p>Loading data into Shark</p></li><li style="list-style-type: disc"><p>Running Shark</p></li><li style="list-style-type: disc"><p>Using HiveQL queries inside of a Spark program</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec40"></a>Why Hive/Shark?</h2></div></div><hr /></div><p>Hive <a id="id317" class="indexterm"></a>is a popular Hadoop project that (among other things) allows for adhoc queries of large datasets. The query language for Hive is called HiveQL, and supports much of SQL as well as number of extensions. Shark is designed to be compatible with the Hive query language, serialization formats, and so on. People primarily choose to use <a id="id318" class="indexterm"></a>Shark because it is much faster than traditional Hive and Hadoop for multiple queries. This chapter will not be able to teach you Hive if you don't already know it, but rather it will look at integrating HiveQL into your Spark programs and how to set up Shark. That being said, HiveQL is very similar to SQL, so if you have a strong grasp of SQL you can probably follow along reasonably well.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec41"></a>Installing Shark</h2></div></div><hr /></div><p>As of the writing<a id="id319" class="indexterm"></a> of this chapter, the latest version of Shark is v0.7.0 and it requires Spark 0.7.2 as<a id="id320" class="indexterm"></a> well as a very recent JVM (Open JK7/Oracle HotSpot JDK7). Shark is available pre-built for both Hadoop 1 and Hadoop 2. As of the writing, the respective files are <a class="ulink" href="http://spark-project.org/download/shark-0.7.0-hadoop1-bin.tgz" target="_blank">http://spark-project.org/download/shark-0.7.0-hadoop1-bin.tgz</a> and <a class="ulink" href="http://spark-project.org/download/shark-0.7.0-hadoop2-bin.tgz" target="_blank">http://spark-project.org/download/shark-0.7.0-hadoop2-bin.tgz</a>. Once you have downloaded and extracted Shark, it's time to configure it. In this example, we will assume that you extracted in <code class="literal">/home/spark/</code>. Shark has a separate configuration from Spark, which lives at <code class="literal">shark-0.7.0/conf/shark-env.sh</code>. For<a id="id321" class="indexterm"></a> local mode, you need to set up at least <code class="literal">HIVE_HOME</code> and <code class="literal">SPARK_HOME</code> like so:</p><div class="informalexample"><pre class="programlisting">export HIVE_HOME=/home/spark/hive-0.9.0-bin
export SPARK_HOME=/home/park/spark-0.7.2
source $SPARK_HOME/conf/spark-env.sh</pre></div><p>In local mode, you also need to create a place for Hive to store its files, which by default is <code class="literal">/user/hive/warehouse</code>. Make sure to use the <code class="literal">chown</code> command<a id="id322" class="indexterm"></a> in order<a id="id323" class="indexterm"></a> to make the files accessible to your user like so:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>mkdir -p /user/hive/warehouse &amp;&amp; chown [your-spark-user] /user/hive/warehouse</strong></span>
</pre></div><p>If you are using Shark with a Spark cluster, you also need to set the <code class="literal">MASTER</code> and <code class="literal">HADOOP_HOME</code> variables. If you are using Shark with an existing Hive installation, you must set <code class="literal">HIVE_CONF_DIR</code> to the directory containing the Hive XML configuration files. If you add these after the <code class="literal">source</code>... line, you can reference the variables in the Spark configuration with:</p><div class="informalexample"><pre class="programlisting">export HADOOP_HOME=/path/to/hadoop
export MASTER=spark://$SPARK_MASTER_IP:7077</pre></div><p>Once you have Shark installed and set up, you also need to copy Shark and its custom hive to all the workers nodes; do this with:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>pscp -v -r -h ./spark-0.7.2/conf/slaves -l sparkuser ./shark-0.7.0 ~/</strong></span>
<span class="strong"><strong>pscp -v -r -h ./spark-0.7.2/conf/slaves -l sparkuser ./hive-0.9.0-bin ~/</strong></span>
</pre></div><p>If you are doing an EC2-based setup, just use the latest AMI; it should already be set up for Shark.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec42"></a>Running Shark</h2></div></div><hr /></div><p>Regardless of what setup mechanism you used in the preceding section, you can launch the Shark CLI in the <a id="id324" class="indexterm"></a>same way for all of them. Shark's <code class="literal">bin</code> directory provides three different variations for different levels of logging. The default of Shark, <code class="literal">./bin/shark</code>, is suitable for most cases. If you run into a problem, you may find <code class="literal">./bin/shark-withinfo</code> to be useful, and if you find problems where you need more debugging information, <code class="literal">./bin/shark-withdebug</code> is the final option. If you are connecting Shark to a Spark cluster, you should be able to see the Spark job in the web UI console<a id="id325" class="indexterm"></a> under running jobs (if you don't, it is possible your Shark job is just running against a local Spark, so double check your configurations).</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec43"></a>Loading data</h2></div></div><hr /></div><p>Hive ships <a id="id326" class="indexterm"></a>with a default dataset in <code class="literal">~/hive-0.9.0-bin/example/</code>, which you can load and use to verify if your Shark setup is working. To load the data in Shark, use the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>shark&gt; CREATE TABLE src(key INT, value STRING);</strong></span>
<span class="strong"><strong>shark&gt; LOAD DATA LOCAL INPATH '${env:HIVE_HOME}/examples/files/in1.txt' INTO TABLE src;</strong></span>
<span class="strong"><strong>shark&gt; SELECT src.key, src.value FROM src WHERE src.key &lt; 100;</strong></span>
</pre></div><p>This should output something similar to the following code, if everything was successful:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>OK</strong></span>
<span class="strong"><strong>48</strong></span>
<span class="strong"><strong>Time taken: 3.02 seconds</strong></span>
</pre></div><p>Shark can also load data from S3 in the same way as Hive. To test this, you can load sample data from one of the public datasets mentioned in the HiveAWS guide, such as <a class="ulink" href="http://s3n://data.s3ndemo.hive/kv" target="_blank">s3n://data.s3ndemo.hive/kv</a>, for the key-value pair data. To access this, you can configure Shark with your AWS credentials in <code class="literal">~/hive-0.9.0-bin/conf/hive-site.xml</code> (you may have to create it) like so:</p><div class="informalexample"><pre class="programlisting">&lt;?xml version="1.0"?&gt;
&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;

&lt;configuration&gt;

&lt;property&gt;
    &lt;name&gt;fs.s3n.awsAccessKeyId&lt;/name&gt;
    &lt;value&gt;accesskeygoeshere&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;fs.s3n.awsSecretAccessKey&lt;/name&gt;
    &lt;value&gt;yoursecretkeygoeshere&lt;/value&gt;
&lt;/property&gt;
&lt;/configuration&gt;</pre></div><p>You can also specify your AWS credintals with <code class="literal">s3n://username:password@[...]</code> when doing your request. Assuming we have configured our AWS credentials in <code class="literal">hive-site.xml</code>, we create a table for the data like so: <code class="literal">create external table kv (key int, values string) location 's3n://data.s3ndemo.hive/kv';</code>. The HiveAWS guide explains how to load more complex data.</p><p>Shark <a id="id327" class="indexterm"></a>also provides a separate Shark shell interface, which is similar to Spark's shell. In this interface, you can write a Scala code that interacts with Shark. The confusion is—the Shark context is available as <code class="literal">sc</code>, which is also used for referring <code class="literal">SparkContext</code> in the Spark shell.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec44"></a>Using Hive queries in a Spark program</h2></div></div><hr /></div><p>If your data analyst (or yourself) has come up with a Hive query that you wish to use as a part of a Spark project, <code class="literal">sql2rdd</code> allows for easy integration with Scala Spark project. It is important to <a id="id328" class="indexterm"></a>note that the return type is <code class="literal">RDD[shark.api.Row]</code>, so you still need to do some transformation work to make it usable by normal Spark code. The Row API provides <code class="literal">get[Type](rowName)</code> for all the supported types. So to get an integer out of a row entry, we can write <code class="literal">row.getInt("key")</code>, assuming the column with the integer is called key.</p><p>Shark was originally designed to be a standalone project and was only recently "mavenized", which allows for easy inclusion as a dependency in a similar Spark environment as covered in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Building and Running a Spark Application</em></span>. However, Shark is not currently deployed to any of the Maven repositories, but you can deploy it to your local maven and make it available for building against it by running <code class="literal">sbt/sbt publish-local</code> in the Spark directory.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip07"></a>Tip</h3><p>If you see errors about unsafe and an unexpected number of parameters, it is possible that sbt is using an old JDK. You can specify a specific JDK using <code class="literal">java_home</code>.</p></div><p>You can experiment with this using the Shark shell commands in an interactive mode. Here is a simple example of a Shark/Spark combination:</p><div class="informalexample"><pre class="programlisting">//Basic Shark example in Scala

package com.pandaspark.examples

import shark._
import spark.SparkContext._

object BasicSharkExample {
  def main(args: Array[String]) {
    val sc = SharkEnv.initWithSharkContext("BasicSharkExample")
    println("Starting shark requests");
    sc.sql("drop table if exists src");
    sc.sql("CREATE TABLE src(key INT, value STRING)")
    sc.sql("LOAD DATA LOCAL INPATH'${env:HIVE_HOME}/examples/files/in1.txt'INTO TABLE src")
    val rdd = sc.sql2rdd("SELECT src.key, src.valueFROM src WHERE src.key &lt; 100")
    rdd.cache()
    println("Found "+rdd.count()+" num rows")
    val normalRDD = rdd.map(x =&gt; (x.getInt("src.key"),x.getString("src.value")))
    println("Formatted as "+normalRDD.collect().mkString(","))
  }
}</pre></div><p>You can do<a id="id329" class="indexterm"></a> the same in Java:</p><div class="informalexample"><pre class="programlisting">//Basic Shark example in Java

package com.pandaspark.examples;

import spark.api.java.JavaRDD;
import spark.api.java.JavaPairRDD;
import spark.api.java.function.PairFunction;

import scala.Tuple2;

import shark.SharkEnv;
import shark.api.Row;
import shark.api.JavaSharkContext;
import shark.api.JavaTableRDD;

public class BasicJavaSharkExample {
  public static void main(String[] args) {
    JavaSharkContext sc = SharkEnv.initWithJavaSharkContext("BasicSharkExample");
    sc.sql("drop table if exists src");
    sc.sql("CREATE TABLE src(key INT, value STRING)");
    sc.sql("LOAD DATA LOCAL INPATH '${env:HIVE_HOME}/examples/files/in1.txt'INTO TABLE src");
    JavaTableRDD rdd = sc.sql2rdd("SELECT src.key,src.value FROM src WHERE src.key &lt; 100");
    rdd.cache();
    System.out.println("Found "+rdd.count()+" num rows");
    JavaPairRDD&lt;Integer, String&gt; normalRDD = rdd.map(newPairFunction&lt;Row, Integer, String&gt;() {
      @Override
      public Tuple2&lt;Integer, String&gt; call(Row x) {
        return new Tuple2&lt;Integer,String&gt;(x.getInt("key"),x.getString("value"));
      }
    });
    System.out.println("Collected: "+normalRDD.collect());
  }
}</pre></div><p>Instead of <a id="id330" class="indexterm"></a>depending directly on Spark, we will have our <code class="literal">build.sbt</code> pointing to:</p><div class="informalexample"><pre class="programlisting">libraryDependencies ++= Seq(
    "edu.berkeley.cs.amplab" % "shark_2.9.3" % "0.7.0"
)</pre></div><p>You also need to include the patched version of Hive, which is distributed with Shark (while excluding an old <code class="literal">guava</code> JAR file), by adding this to the build file as well:</p><div class="informalexample"><pre class="programlisting">unmanagedJars in Compile &lt;++= baseDirectory map {
  base =&gt; val hiveFile = file(System.getenv("HIVE_HOME")) / "lib"val baseDirectories = (base / "lib") +++ (hiveFile)val customJars = (baseDirectories ** "*.jar")//Hive uses an old version of guava that doesn't have what we want
  customJars.classpath.filter(!_.toString.contains("guava"))
}</pre></div><p>Running the resulting jobs is slightly different than just running normal Spark jobs. However, in the SharkEnv initialization logic, it looks at a number of environment variables to help it with the setup. As such, the easiest way to ensure you have all the correct environment variables set up is to use the provided run script and just set the <code class="literal">CLASSPATH</code> as the following code:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>CLASSPATH=/home/spark/fastdataprocessingwithspark-sharkexamples/target/scala-2.9.3/fastdataprocessingwithspark-sharkexamples-assembly-0.1-SNAPSHOT.jar ./run com.pandaspark.examples.BasicSharkExample</strong></span>
</pre></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec45"></a>Links and references</h2></div></div><hr /></div><p>Some<a id="id331" class="indexterm"></a> useful links for referencing are listed as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<a class="ulink" href="http://hive.apache.org/" target="_blank">http://hive.apache.org/</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="https://github.com/amplab/shark/wiki/Shark-User-Guide" target="_blank">https://github.com/amplab/shark/wiki/Shark-User-Guide</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="https://github.com/amplab/shark/wiki" target="_blank">https://github.com/amplab/shark/wiki</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="https://github.com/amplab/shark/wiki/Running-Shark-Locally" target="_blank">https://github.com/amplab/shark/wiki/Running-Shark-Locally</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="https://github.com/amplab/shark/wiki/Running-Shark-on-a-Cluster" target="_blank">https://github.com/amplab/shark/wiki/Running-Shark-on-a-Cluster</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="https://cwiki.apache.org/confluence/display/Hive/HiveAws+HivingS3nRemotely" target="_blank">https://cwiki.apache.org/confluence/display/Hive/HiveAws+HivingS3nRemotely</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="https://github.com/amplab/shark/wiki#developer-documentation" target="_blank">https://github.com/amplab/shark/wiki#developer-documentation</a>
</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec46"></a>Summary</h2></div></div><hr /></div><p>In this chapter, you have seen how to set up Shark and how to integrate Shark into your Spark programs. In the next chapter, you will learn how to write simple unit tests.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch08"></a>Chapter 8. Testing</h2></div></div></div><p>Writing effective software without tests is quite challenging. Effective testing, especially in cases with slow end-to-end running times, such as distributed systems, can help improve developer effectiveness greatly. However, this chapter isn't going to try and convince you that you should be testing; if you really want to ride without a seat belt, that's fine too.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec47"></a>Testing in Java and Scala</h2></div></div><hr /></div><p>For the sake of <a id="id332" class="indexterm"></a>simplicity, this chapter will look at using ScalaTest and JUnit as the testing libraries. ScalaTest can <a id="id333" class="indexterm"></a>be used to test both Scala and Java<a id="id334" class="indexterm"></a> code and is the testing library currently used in Spark. JUnit is a popular testing<a id="id335" class="indexterm"></a> framework for Java.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec18"></a>Refactoring your code for testability</h3></div></div></div><p>If you have code that can be isolated from the RDD interaction or SparkContext interaction, this code can <a id="id336" class="indexterm"></a>be tested using standard methodologies. While it can be quite convenient to use anonymous functions when writing Spark code, by giving them names, you can test them more easily without having to deal with the expensive overhead of setting up SparkContext. For example, in your Scala CSV parser, you could had this hard to test code:</p><div class="informalexample"><pre class="programlisting">  val splitLines = inFile.map(line =&gt; {
      val reader = new CSVReader(new StringReader(line))
      reader.readNext().map(_.toDouble)
  }</pre></div><p>Or in Java you had:</p><div class="informalexample"><pre class="programlisting">JavaRDD&lt;Integer[]&gt; splitLines = inFile.flatMap(new FlatMapFunction&lt;String, Integer[]&gt; (){
  public Iterable&lt;Integer[]&gt; call(String line) {
    ArrayList&lt;Integer[]&gt; result = new ArrayList&lt;Integer[]&gt;();
    try {
    CSVReader reader = new CSVReader(new StringReader(line));
    String[] parsedLine = reader.readNext();
    Integer[] intLine = new Integer[parsedLine.length];
    for (int i = 0; i &lt; parsedLine.length; i++) {
      intLine[i] = Integer.parseInt(parsedLine[i]);
    result.add(intLine);
    }
    catch (Exception e) {
      errors.add(1);
    }
    return result;
    }
  }
);</pre></div><p>Instead in Scala, you could write this in the CSV parser as a separate function:</p><div class="informalexample"><pre class="programlisting">def parseLine(line: String): Array[Double] = {
    val reader = new CSVReader(new StringReader(line))
    reader.readNext().map(_.toDouble)}</pre></div><p>While, in Java, <a id="id337" class="indexterm"></a>you could add this:</p><div class="informalexample"><pre class="programlisting">public class JavaLoadCsvTestable {
    public static class ParseLine extends Function&lt;String, Integer[]&gt; {
      public Integer[] call(String line) throws Exception {
       CSVReader reader = new CSVReader(new StringReader(line));
       String[] parsedLine = reader.readNext();
       Integer[] intLine = new Integer[parsedLine.length];
       for (int i = 0; i &lt; parsedLine.length; i++) {
         intLine[i] = Integer.parseInt(parsedLine[i]);
       }
      return intLine;
      }
    }</pre></div><p>You can then test the Java code, without having to worry about any Spark-specific setup or logic:</p><div class="informalexample"><pre class="programlisting">package pandaspark.examples

import org.scalatest.FunSuite
import org.scalatest.matchers.ShouldMatchers

class TestableLoadCsvExampleSuite extends FunSuite with ShouldMatchers {
    test("should parse a csv line with numbers") {
      TestableLoadCsvExample.parseLine("1,2") should equal(Array[Double](1.0,2.0))
      TestableLoadCsvExample.parseLine("100,-1,1,2,2.5")should equal (Array[Double](100,-1,1.0,2.0,2.5))
    }
    test("should error if there is a non-number") {
      evaluating {
        TestableLoadCsvExample.parseLine("pandas")
      } should produce [NumberFormatException]
    }
}</pre></div><p>Or, to test the Java code, <a id="id338" class="indexterm"></a>you could write something like:</p><div class="informalexample"><pre class="programlisting">class JavaLoadCsvExampleSuite extends FunSuite with ShouldMatchers {

    test("should parse a csv line with numbers") {
      val parseLine = new JavaLoadCsvTestable.ParseLine();
      parseLine.call("1,2") should equal (Array[Integer](1,2))
      parseLine.call("100,-1,1,2,2") should equal (Array[Integer](100,-1,1,2,2))
    }
    test("should error if there is a non-integer") {
      val parseLine = new JavaLoadCsvTestable.ParseLine();
      evaluating { parseLine.call("pandas")  } should produce [NumberFormatException]
      evaluating {parseLine.call("100,-1,1,2.2,2") should equal (Array[Integer](100,-1,1,2,2)) } should produce [NumberFormatException]
    }
}</pre></div><p>Note that the test is still written in Scala; don't worry, we will look at JUnit tests later.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec19"></a>Testing interactions with SparkContext</h3></div></div></div><p>However, you may remember that you later extended our CSV parser to increment counters on invalid input so as to gracefully handle failures. To verify that behavior, you could provide mock counters and other<a id="id339" class="indexterm"></a> mock objects for the Spark components you are using. You are restricted to only testing the parts of our code that don't depend on Spark. Instead, you could re-factor our code to have the core be testable without Spark as well as do a more complete test using a provided SparkContext as illustrated in the following example:</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip08"></a>Tip</h3><p>This does have the significant downside of requiring that your tests run serially as, otherwise, sbt (or another build infrastructure) may try and launch multiple SparkContext at the same time, which will cause confusing error messages. We can force tests to execute sequentially in sbt with <code class="literal">parallelExecution in Test := false</code>.</p></div><div class="informalexample"><pre class="programlisting">object MoreTestableLoadCsvExample {
  def parseLine(line: String): Array[Double] = {
    val reader = new CSVReader(new StringReader(line))
    reader.readNext().map(_.toDouble)
  }
  def handleInput(invalidLineCounter: Accumulator[Int],inFile: RDD[String]): RDD[Double] = {
    val numericData = inFile.flatMap(line =&gt; {
      try {
        Some(parseLine(line))
      }
      catch {
        case _ =&gt; {
        invalidLineCounter += 1
        None
        }
      }
    })
    numericData.map(row =&gt; row.sum)
  }

  def main(args: Array[String]) {
    if (args.length != 2) {
      System.err.println("Usage: TestableLoadCsvExample&lt;master&gt; &lt;inputfile&gt;")
      System.exit(1)
    }
    val master = args(0)
    val inputFile = args(1)
    val sc = new SparkContext(master, "Load CSV Example",System.getenv("SPARK_HOME"),Seq(System.getenv("JARS")))
    sc.addFile(inputFile)
    val inFile = sc.textFile(inputFile)
    val invalidLineCounter = sc.accumulator(0)
    val summedData = handleInput(invalidLineCounter, inFile)
    println(summedData.collect().mkString(","))
    println("Errors: "+invalidLineCounter)
    println(summedData.stats())
  }

}</pre></div><p>We test this <a id="id340" class="indexterm"></a>with the following code:</p><div class="informalexample"><pre class="programlisting">import spark._
import spark.SparkContext._
import org.scalatest.FunSuite
import org.scalatest.matchers.ShouldMatchers

class MoreTestableLoadCsvExampleSuite extends FunSuite with ShouldMatchers {
  test("summ data on input") {
    val sc = new SparkContext("local", "Load CSV Example")
    val counter = sc.accumulator(0)
    val input = sc.parallelize(List("1,2","1,3"))
    val result = MoreTestableLoadCsvExample.handleInput(counter,input)
    result.collect() should equal (Array[Int](3,4))
  }
  test("should parse a csv line with numbers") {
    MoreTestableLoadCsvExample.parseLine("1,2") should equal(Array[Double](1.0,2.0))
    MoreTestableLoadCsvExample.parseLine("100,-1,1,2,2.5")should equal (Array[Double](100,-1,1.0,2.0,2.5))
  }
  test("should error if there is a non-number") {
    evaluating { MoreTestableLoadCsvExample.parseLine("pandas") }should produce [NumberFormatException]
  }
}</pre></div><p>And in Java<a id="id341" class="indexterm"></a> we use:</p><div class="informalexample"><pre class="programlisting">public class JavaLoadCsvMoreTestable {
    public static class ParseLineWithAcc extends FlatMapFunction&lt;String, Integer[]&gt; {
    Accumulator&lt;Integer&gt; acc;
    ParseLineWithAcc(Accumulator&lt;Integer&gt; acc) {
      this.acc = acc;
    }
    public Iterable&lt;Integer[]&gt; call(String line) throws Exception{
      ArrayList&lt;Integer[]&gt; result = new ArrayList&lt;Integer[]&gt;();
      try {
        CSVReader reader = new CSVReader(new StringReader(line));
        String[] parsedLine = reader.readNext();
        Integer[] intLine = new Integer[parsedLine.length];
        for (int i = 0; i &lt; parsedLine.length; i++) {
          intLine[i] = Integer.parseInt(parsedLine[i]);
        }
        result.add(intLine);
      }
      catch (Exception e) {
        acc.add(1);
      }
      return result;
    }
  }
    public static JavaDoubleRDD processData(Accumulator&lt;Integer&gt; acc, JavaRDD&lt;String&gt; input) {
      JavaRDD&lt;Integer[]&gt; splitLines = input.flatMap(new ParseLineWithAcc(acc));
      JavaDoubleRDD summedData = splitLines.map(new DoubleFunction&lt;Integer[]&gt;() {
        public Double call(Integer[] in) {
          Double ret = 0.;
          for (int i = 0; i &lt; in.length; i++) {
            ret += in[i];
          }
        return ret;
        }
      }
      );
    return summedData;
}</pre></div><p>You can test this in Scala with:</p><div class="informalexample"><pre class="programlisting">class JavaLoadCsvMoreTestableSuite extends FunSuitewith ShouldMatchers {
  test("sum data on input") {
    val sc = new JavaSparkContext("local", "Load Java CSV test")
    val counter: Accumulator[Integer] = sc.intAccumulator(0)
    val input: JavaRDD[String] = sc.parallelize(List("1,2","1,3","murh"))
    val javaLoadCsvMoreTestable = new JavaLoadCsvMoreTestable();
    val resultRDD = JavaLoadCsvMoreTestable.processData(counter,input)
    resultRDD.cache();
    val resultCount = resultRDD.count()
    val result = resultRDD.collect().toArray()
    resultCount should equal (2)
    result should equal (Array[Double](3.0, 4.0))
    counter.value should equal (1)
    sc.stop()
  }
}</pre></div><p>Note that we add an invalid input for the counter.</p><p>In Java, using JUnit4 you can add the following code for testing:</p><div class="informalexample"><pre class="programlisting">package pandaspark.examples;

import spark.*;
import spark.api.java.JavaSparkContext;
import spark.api.java.JavaRDD;
import spark.api.java.JavaDoubleRDD;
import org.scalatest.FunSuite;
import org.scalatest.matchers.ShouldMatchers;

import static org.junit.Assert.assertEquals;
import org.junit.Test;
import org.junit.Ignore;
import org.junit.runner.RunWith;
import org.junit.runners.JUnit4;

import java.util.Arrays;
import java.util.List;
import java.util.ArrayList;
@RunWith(JUnit4.class)
public class JavaLoadCsvMore<a id="id342" class="indexterm"></a>TestableSuiteJunit {
    @Test
    public void testSumDataOnInput() {
      JavaSparkContext sc = new JavaSparkContext("local","Load Java CSV test");
      Accumulator&lt;Integer&gt; counter = sc.intAccumulator(0);
      String[] inputArray = {"1,2","1,3","murh"};
      JavaRDD&lt;String&gt; input = sc.parallelize(Arrays.asList(inputArray));
      JavaDoubleRDD resultRDD = JavaLoadCsvMoreTestable.processData(counter, input);
      long resultCount = resultRDD.count();
      assertEquals(resultCount, 2);
      int errors = counter.value();
      assertEquals(errors, 1);
      sc.stop();
    }
}</pre></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec48"></a>Testing in Python</h2></div></div><hr /></div><p>Python testing of Spark is very similar in concept, but the testing libraries are a bit different. PySpark uses<a id="id343" class="indexterm"></a> both <code class="literal">doctest</code> and <code class="literal">unittest</code> to test itself. The <code class="literal">doctest</code> library makes it easy to create tests based on the expected output of code run in the Python interpreter. We can <a id="id344" class="indexterm"></a>run the tests by running <code class="literal">pyspark -m doctest [pathtocode]</code>. By taking the <code class="literal">wordcount.py</code> example from Spark and factoring out <code class="literal">countWords</code>, you can test the word count functionality using <code class="literal">doctest</code>:</p><div class="informalexample"><pre class="programlisting">"""
&gt;&gt;&gt; from pyspark.context import SparkContext
&gt;&gt;&gt; sc = SparkContext('local', 'test')
&gt;&gt;&gt; b = sc.parallelize(["pandas are awesome", "and ninjas are also awesome"])
&gt;&gt;&gt; countWords(b)
[('also', 1), ('and', 1), ('are', 2), ('awesome', 2), ('ninjas', 1), ('pandas', 1)]
"""

import sys
from operator import add

from pyspark import SparkContext
def countWords(lines):
    counts = lines.flatMap(lambda x: x.split(' ')) \.map(lambda x: (x, 1)) \.reduceByKey(add)
    return sorted(counts.collect())


if __name__ == "__main__":
    if len(sys.argv) &lt; 3:
      print &gt;&gt; sys.stderr, "Usage: PythonWordCount&lt;master&gt; &lt;file&gt;"
      exit(-1)
    sc = SparkContext(sys.argv[1], "PythonWordCount")
    lines = sc.textFile(sys.argv[2], 1)
    output = countWords(lines)
    for (word, count) in output:
      print "%s : %i" % (word, count)</pre></div><p>We can also test<a id="id345" class="indexterm"></a> something <a id="id346" class="indexterm"></a>similar to our Java and Scala programs like so:</p><div class="informalexample"><pre class="programlisting">"""
&gt;&gt;&gt; from pyspark.context import SparkContext
&gt;&gt;&gt; sc = SparkContext('local', 'test')
&gt;&gt;&gt; b = sc.parallelize(["1,2","1,3"])
&gt;&gt;&gt; handleInput(b)
[3, 4]
"""

import sys
from operator import add

from pyspark import SparkContext
def handleInput(lines):
    data = lines.map(lambda x: sum(map(int, x.split(','))))
    return sorted(data.collect())


if __name__ == "__main__":
    if len(sys.argv) &lt; 3:
        print &gt;&gt; sys.stderr, "Usage: PythonLoadCsv&lt;master&gt; &lt;file&gt;"
        exit(-1)
    sc = SparkContext(sys.argv[1], "PythonLoadCsv")
    lines = sc.textFile(sys.argv[2], 1)
    output = handleInput(lines)
    for sum in output:
      print sum</pre></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec49"></a>Links and references</h2></div></div><hr /></div><p>Here are <a id="id347" class="indexterm"></a>some useful links for reference:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<a class="ulink" href="http://blog.quantifind.com/posts/spark-unit-test/" target="_blank">http://blog.quantifind.com/posts/spark-unit-test/</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://www.scalatest.org/" target="_blank">http://www.scalatest.org/</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://junit.org/" target="_blank">http://junit.org/</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://docs.python.org/2/library/unittest.html" target="_blank">http://docs.python.org/2/library/unittest.html</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://docs.python.org/2/library/doctest.html" target="_blank">http://docs.python.org/2/library/doctest.html</a>
</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec50"></a>Summary</h2></div></div><hr /></div><p>This chapter has looked at how to structure code so that it is testable and at the testing framework that is used within Spark. Effective testing can save large amounts of debugging time, which can be especially painful in large distributed systems. In the next chapter, we will look at some tips and tricks, such as tuning and securing Spark.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch09"></a>Chapter 9. Tips and Tricks</h2></div></div></div><p>Now that you have the tools to build and test Spark jobs as well as set up a Spark cluster to run them on, it's time to figure out how to make the most of your time as a Spark developer.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec51"></a>Where to find logs?</h2></div></div><hr /></div><p>Spark and Shark have very useful logs for figuring out what's going on when things are not behaving as expected. When working with a program that uses <code class="literal">sql2rdd</code> or any other Shark-related tool, a good place to start debugging is by looking at what HiveQL queries are being run. You should find<a id="id348" class="indexterm"></a> this in the console logs where you execute the Spark program: look for a line such as <span class="strong"><strong>Hive history file=/tmp/spark/hive_job_log_spark_201306090132_919529074.txt</strong></span>. Spark also keeps a per machine log on each machine, by default, in the logs subdirectory of the Spark directory. Spark's web UI provides a convenient place to see the <code class="literal">stdout</code> and <code class="literal">stderr</code> files of each job, running and completing separate output per worker.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec52"></a>Concurrency limitations</h2></div></div><hr /></div><p>Spark's concurrency<a id="id349" class="indexterm"></a> for operations is limited by the number of partitions. Conversely, having too many partitions can cause an excess overhead with too many tasks being launched. If you have too many partitions, you can shrink it down using the <code class="literal">coalesce(count)</code> method; <code class="literal">coalesce</code> will only decrease the number of partitions. When creating a new RDD, you can specify the number of splits to be used. Also, the grouping/joining mechanism on the RDDs of pairs can take the number of partitions or, alternatively, a partitioner. The default number of partitions for new RDDs is controlled by <code class="literal">spark.default.parallelism</code>, which also controls the number of tasks used by <code class="literal">groupByKey</code> and other shuffle operations.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec53"></a>Memory usage and garbage collection</h2></div></div><hr /></div><p>To measure the <a id="id350" class="indexterm"></a>impact of garbage collection, you can ask the JVM to print details about the garbage collection. You can do this by adding <code class="literal">-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps</code> to your <code class="literal">SPARK_JAVA_OPTS</code>  environment variable in <code class="literal">conf/spark-env.sh</code>. The details will then be printed to the standard output when you run your job, which will be available as described in the <span class="emphasis"><em>Where to find logs?</em></span> section of this chapter.</p><p>If you find that your Spark cluster is using too much time on garbage collection, you can reduce the amount of space used for RDD caching by changing <code class="literal">spark.storage.memoryFraction</code>, which is set to <code class="literal">0.66</code> by default. If you are planning to run Spark for a long time on a cluster, you may wish to enable <code class="literal">spark.cleaner.ttl</code>. By default, Spark does not clean up any metadata; set <code class="literal">spark.cleaner.ttl</code> to a nonzero value in seconds to clean up metadata after that length of time.</p><p>You can also control the RDD storage level if you find that you are using too much memory. If your RDDs don't fit in the memory and you still wish to cache them, you can try using a different storage level such as:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">MEMORY_ONLY</code>: This stores<a id="id351" class="indexterm"></a> the entire RDD in the memory if it can and is the default storage level</p></li><li style="list-style-type: disc"><p>
<code class="literal">MEMORY_AND_DISK</code>: This<a id="id352" class="indexterm"></a> stores each partition in the memory if it can, or if it doesn't, it stores it on disk</p></li><li style="list-style-type: disc"><p>
<code class="literal">DISK_ONLY</code>: This stores<a id="id353" class="indexterm"></a> each partition on the disk regardless of whether it can fit in the memory</p></li></ul></div><p>These options are set when you call the persist function on your RDD. By default, the RDDs are stored in a deserialized form, which requires less parsing. We can save space by adding <code class="literal">_SER</code> to the storage level; in this case, Spark will serialize the data to be stored, which normally saves some space.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec54"></a>Serialization</h2></div></div><hr /></div><p>Spark supports different <a id="id354" class="indexterm"></a>serialization mechanisms; the choice is a trade-off between speed, space efficiency, and full support of all Java objects. If you are using a serializer to cache your RDDs, you should strongly consider a fast serializer. The default serializer uses Java's default serialization. The KyroSerializer is much faster and generally uses about one-tenth of the memory as the default serializer. You can switch the serializer by changing <code class="literal">spark.serializer</code> to <code class="literal">spark.KryoSerializer</code>. If you want to use the KyroSerializer, you need to make sure that the classes are serializable by the KyroSerializer.</p><p>Spark provides a trait KryoRegistrator, which<a id="id355" class="indexterm"></a> you can extend to register your classes with Kyro as follows:</p><div class="informalexample"><pre class="programlisting">class MyReigstrator extends spark.KyroRegistrator {
   override def registerClasses(kyro: Kyro) {
    kyro.register(classOf[MyClass])
   }
}</pre></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip09"></a>Tip</h3><p>Visit <a class="ulink" href="https://code.google.com/p/kryo/#Quickstart" target="_blank">https://code.google.com/p/kryo/#Quickstart</a> to figure out how to write custom serializers for your classes if you need something customized. You can substantially decrease the amount of space used for your objects by customizing your serializers. For example, rather than writing out the full class name, you can give them an integer ID by calling <code class="literal">kyro.register(classOf[MyClass],100)</code>.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec55"></a>IDE integration</h2></div></div><hr /></div><p>As an Emacs user, the author finds that having an <span class="strong"><strong>ENhanced Scala Interaction Mode</strong></span> (<span class="strong"><strong>ensime</strong></span>)<a id="id356" class="indexterm"></a> setup helps with development. You <a id="id357" class="indexterm"></a>can install the latest ensime from <a class="ulink" href="https://github.com/aemoncannon/ensime/downloads" target="_blank">https://github.com/aemoncannon/ensime/downloads</a> (make sure to choose the one that matches your Scala version).</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>wget https://github.com/downloads/aemoncannon/ensime/ensime_2.9.2-0.9.8.1.tar.gz</strong></span>
<span class="strong"><strong>tar -xvf ensime_2.9.2-0.9.8.1.tar.gz</strong></span>
</pre></div><p>In your <code class="literal">.emacs</code> file, add:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>;; Load the ensime lisp code...(add-to-list 'load-path "ENSIME_ROOT/elisp/")(require 'ensime);;This step causes the ensime-mode to be started whenever;; scala-mode is started for a buffer. You may have to customize this step;; if you're not using the standard scala mode.(add-hook 'scala-mode-hook 'ensime-scala-mode-hook)</strong></span>
</pre></div><p>You can then add the <code class="literal">ensime sbt</code> plugin to your project (in <code class="literal">project/plugins.sbt</code>):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>addSbtPlugin("org.ensime" % "ensime-sbt-cmd" % "0.1.0")</strong></span>
</pre></div><p>You can then run the plugin:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sbt</strong></span>
<span class="strong"><strong>&gt; ensime generate</strong></span>
</pre></div><p>If you are using git, <a id="id358" class="indexterm"></a>you will probably want to add <code class="literal">.ensime</code> to the <code class="literal">.gitignore</code> file if it isn't already present.</p><p>If you are using IntelliJ, a similar plugin exists called <code class="literal">sbt-idea</code> that can be used to generate IntelliJ IDEA files. You can add the <code class="literal">IntelliJ sbt</code> plugin<a id="id359" class="indexterm"></a> to your project (in <code class="literal">project/plugins.sbt</code>):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>addSbtPlugin("com.github.mpeltonen" % "sbt-idea" % "1.5.1")</strong></span>
</pre></div><p>You can then run the plugin:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sbt</strong></span>
<span class="strong"><strong>&gt; gen-idea</strong></span>
</pre></div><p>This will generate the IDEA project file that can be loaded into IntelliJ.</p><p>Eclipse users can also use <code class="literal">sbt</code> to generate Eclipse project files with the <code class="literal">sbteclipse</code> plugin. You can add the Eclipse sbt plugin to your project (in <code class="literal">project/plugins.sbt</code>):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>addSbtPlugin("com.typesafe.sbteclipse" % "sbteclipse-plugin" % "2.3.0")</strong></span>
</pre></div><p>You can then run the plugin:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sbt</strong></span>
<span class="strong"><strong>&gt; eclipse</strong></span>
</pre></div><p>This will generate the Eclipse project files, and you can then import them into your Eclipse project using the <span class="strong"><strong>Import</strong></span> wizard in Eclipse. Eclipse users might also find the <code class="literal">spark-plug</code> project useful; it can be used to launch clusters from within Eclipse.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec56"></a>Using Spark with other languages</h2></div></div><hr /></div><p>If you find yourself wanting to work with your RDD in another language, there are a few options. With Java/Scala, you can try using the JNI, and with Python, you can use the FFI. Sometimes, however, you<a id="id360" class="indexterm"></a> will want to work with a language that isn't C language or with an already compiled program. In that case, the easiest thing to do is use the pipe interface that is available in all of the three APIs. The Stream API works by taking the RDD, serializing it to strings, and piping it to the specified program. If your data happens to be plain strings, this is very convenient; but if not, you will need to serialize your data in such a way it can be understood on either side. JSON or protocol buffers can be good options depending on how structured your data is.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec57"></a>A quick note on security</h2></div></div><hr /></div><p>Another important consideration in your Spark setup is security. If you are using Spark on EC2 with the default scripts, you will notice that access to your Spark cluster is restricted. This is a good idea even if you aren't running Spark inside EC2 since your Spark cluster will most likely have<a id="id361" class="indexterm"></a> access to data you would rather not share with the world. (And even if it doesn't, you probably don't want to allow arbitrary code execution by strangers.) If your Spark cluster is already on a private network, that's great; otherwise, you should talk to your system's administrator about setting up some IPTables rules to restrict access.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec58"></a>Mailing lists</h2></div></div><hr /></div><p>Probably the <a id="id362" class="indexterm"></a>most useful tip to finish with is that the Spark-users' mailing list is an excellent source of up-to-date information about other people's experiences with Spark. You can subscribe to <a class="ulink" href="https://groups.google.com/forum/?fromgroups#!forum/spark-users" target="_blank">https://groups.google.com/forum/?fromgroups#!forum/spark-users</a> (soon to be <a class="ulink" href="http://mail-archives.apache.org/mod_mbox/incubator-spark-user/" target="_blank">http://mail-archives.apache.org/mod_mbox/incubator-spark-user/</a>) as well as search the archives to see if other people have run into similar problems as you have.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec59"></a>Links and references</h2></div></div><hr /></div><p>Some useful links for<a id="id363" class="indexterm"></a> referencing are listed as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<a class="ulink" href="http://blog.quantifind.com/posts/logging-post/" target="_blank">http://blog.quantifind.com/posts/logging-post/</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://jawher.net/2011/01/17/scala-development-environment-emacs-sbt-ensime/" target="_blank">http://jawher.net/2011/01/17/scala-development-environment-emacs-sbt-ensime/</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="https://www.assembla.com/spaces/liftweb/wiki/Emacs-ENSIME" target="_blank">https://www.assembla.com/spaces/liftweb/wiki/Emacs-ENSIME</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://syndeticlogic.net/?p=311" target="_blank">http://syndeticlogic.net/?p=311</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf" target="_blank">http://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="https://github.com/shivaram/spark-ec2/blob/master/ganglia/init.sh" target="_blank">https://github.com/shivaram/spark-ec2/blob/master/ganglia/init.sh</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://spark-project.org/docs/0.7.2/tuning.html" target="_blank">http://spark-project.org/docs/0.7.2/tuning.html</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="https://github.com/mesos/spark/blob/master/docs/configuration.md" target="_blank">https://github.com/mesos/spark/blob/master/docs/configuration.md</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://kryo.googlecode.com/svn/api/v2/index.html" target="_blank">http://kryo.googlecode.com/svn/api/v2/index.html</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="https://code.google.com/p/kryo/" target="_blank">https://code.google.com/p/kryo/</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://scala-ide.org/download/current.html" target="_blank">http://scala-ide.org/download/current.html</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://syndeticlogic.net/?p=311" target="_blank">http://syndeticlogic.net/?p=311</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://mail-archives.apache.org/mod_mbox/incubator-spark-user/" target="_blank">http://mail-archives.apache.org/mod_mbox/incubator-spark-user/</a>
</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="https://groups.google.com/forum/?fromgroups#!forum/spark-users" target="_blank">https://groups.google.com/forum/?fromgroups#!forum/spark-users</a>
<a id="id364" class="indexterm"></a>
</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec60"></a>Summary</h2></div></div><hr /></div><p>That wraps up some common things, which you can use to help improve your Spark development experience. I wish you the best of luck with your Spark projects. Now go solve some fun problems!</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">

    <div id="backindex">
      <h1 class="title">Index</h1>
      <h2>A</h2>
      <ul>
        <li>addFile(path) method / <a href="#ch04lvl1sec27" title="Shared Java and Scala APIs" class="link">Shared Java and Scala APIs</a></li>
        <li>addJar(path) method / <a href="#ch04lvl1sec27" title="Shared Java and Scala APIs" class="link">Shared Java and Scala APIs</a></li>
        <li>aggregate function / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a></li>
        <li>AMI (Amazon Machine Images) / <a href="#ch01lvl1sec09" title="Running Spark on EC2 with the scripts" class="link">Running Spark on EC2 with the scripts</a></li>
      </ul>
      <h2>B</h2>
      <ul>
        <li>bin/slaves.sh &lt;command&gt; command / <a href="#ch01lvl1sec14" title="Deploying set of machines over SSH" class="link">Deploying set of machines over SSH</a></li>
        <li>bin/start-all.sh command / <a href="#ch01lvl1sec14" title="Deploying set of machines over SSH" class="link">Deploying set of machines over SSH</a></li>
        <li>bin/start-master.sh command / <a href="#ch01lvl1sec14" title="Deploying set of machines over SSH" class="link">Deploying set of machines over SSH</a></li>
        <li>bin/start-slave.sh command / <a href="#ch01lvl1sec14" title="Deploying set of machines over SSH" class="link">Deploying set of machines over SSH</a></li>
        <li>bin/start-slaves.sh command / <a href="#ch01lvl1sec14" title="Deploying set of machines over SSH" class="link">Deploying set of machines over SSH</a></li>
        <li>bin/stop-all.sh command / <a href="#ch01lvl1sec14" title="Deploying set of machines over SSH" class="link">Deploying set of machines over SSH</a></li>
        <li>bin/stop-master.sh command / <a href="#ch01lvl1sec14" title="Deploying set of machines over SSH" class="link">Deploying set of machines over SSH</a></li>
        <li>bin/stop-slaves.sh command / <a href="#ch01lvl1sec14" title="Deploying set of machines over SSH" class="link">Deploying set of machines over SSH</a></li>
      </ul>
      <h2>C</h2>
      <ul>
        <li>cache function / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a>, <a href="#ch06lvl1sec36" title="Common Java RDD functions" class="link">Common Java RDD functions</a></li>
        <li>cartesian function / <a href="#ch06lvl1sec37" title="Standard RDD functions" class="link">Standard RDD functions</a></li>
        <li>Chef (opscode)<ul><li>used, for Spark deploying / <a href="#ch01lvl1sec11" title="Deploying Spark with Chef (opscode)" class="link">Deploying Spark with Chef (opscode)</a></li></ul></li>
        <li>chown command / <a href="#ch07lvl1sec41" title="Installing Shark" class="link">Installing Shark</a></li>
        <li>clearFiles() method / <a href="#ch04lvl1sec27" title="Shared Java and Scala APIs" class="link">Shared Java and Scala APIs</a></li>
        <li>clearJars() method / <a href="#ch04lvl1sec27" title="Shared Java and Scala APIs" class="link">Shared Java and Scala APIs</a></li>
        <li>coalesce function / <a href="#ch06lvl1sec36" title="Common Java RDD functions" class="link">Common Java RDD functions</a></li>
        <li>code<ul><li>testing / <a href="#ch08lvl1sec47" title="Refactoring your code for testability" class="link">Refactoring your code for testability</a></li></ul></li>
        <li>cogroup function / <a href="#ch06lvl1sec36" title="Functions for joining PairRDD functions" class="link">Functions for joining PairRDD functions</a>, <a href="#ch06lvl1sec36" title="JavaPairRDD functions" class="link">JavaPairRDD functions</a>, <a href="#ch06lvl1sec37" title="PairRDD functions" class="link">PairRDD functions</a></li>
        <li>collect() function / <a href="#ch05lvl1sec32" title="Loading data into an RDD" class="link">Loading data into an RDD</a></li>
        <li>collectAsMap function / <a href="#ch06lvl1sec36" title="Other PairRDD functions" class="link">Other PairRDD functions</a>, <a href="#ch06lvl1sec36" title="JavaPairRDD functions" class="link">JavaPairRDD functions</a>, <a href="#ch06lvl1sec37" title="PairRDD functions" class="link">PairRDD functions</a></li>
        <li>collect function / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a>, <a href="#ch06lvl1sec36" title="Common Java RDD functions" class="link">Common Java RDD functions</a></li>
        <li>combineByKey function / <a href="#ch06lvl1sec36" title="JavaPairRDD functions" class="link">JavaPairRDD functions</a>, <a href="#ch06lvl1sec37" title="PairRDD functions" class="link">PairRDD functions</a></li>
        <li>concurrency<ul><li>limitations / <a href="#ch09lvl1sec52" title="Concurrency limitations" class="link">Concurrency limitations</a></li></ul></li>
        <li>countByKey function / <a href="#ch06lvl1sec36" title="Other PairRDD functions" class="link">Other PairRDD functions</a>, <a href="#ch06lvl1sec36" title="JavaPairRDD functions" class="link">JavaPairRDD functions</a>, <a href="#ch06lvl1sec37" title="PairRDD functions" class="link">PairRDD functions</a></li>
        <li>countByValue function / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a>, <a href="#ch06lvl1sec36" title="Common Java RDD functions" class="link">Common Java RDD functions</a>, <a href="#ch06lvl1sec37" title="Standard RDD functions" class="link">Standard RDD functions</a></li>
        <li>count function / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a>, <a href="#ch06lvl1sec36" title="Common Java RDD functions" class="link">Common Java RDD functions</a></li>
      </ul>
      <h2>D</h2>
      <ul>
        <li>data<ul><li>loading / <a href="#ch07lvl1sec43" title="Loading data" class="link">Loading data</a></li></ul></li>
        <li>distinct function / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a>, <a href="#ch06lvl1sec36" title="Common Java RDD functions" class="link">Common Java RDD functions</a>, <a href="#ch06lvl1sec37" title="Standard RDD functions" class="link">Standard RDD functions</a></li>
        <li>DoubleFlatMapFunction function / <a href="#ch06lvl1sec36" title="Spark Java function classes" class="link">Spark Java function classes</a></li>
        <li>DoubleFunction&lt;T&gt; function / <a href="#ch06lvl1sec36" title="Spark Java function classes" class="link">Spark Java function classes</a></li>
        <li>DoubleRDD functions<ul><li>mean / <a href="#ch06lvl1sec36" title="DoubleRDD functions" class="link">DoubleRDD functions</a></li><li>sampleStdev / <a href="#ch06lvl1sec36" title="DoubleRDD functions" class="link">DoubleRDD functions</a></li><li>Stats / <a href="#ch06lvl1sec36" title="DoubleRDD functions" class="link">DoubleRDD functions</a></li><li>Stdev / <a href="#ch06lvl1sec36" title="DoubleRDD functions" class="link">DoubleRDD functions</a></li><li>Sum / <a href="#ch06lvl1sec36" title="DoubleRDD functions" class="link">DoubleRDD functions</a></li><li>variance / <a href="#ch06lvl1sec36" title="DoubleRDD functions" class="link">DoubleRDD functions</a></li></ul></li>
      </ul>
      <h2>E</h2>
      <ul>
        <li>EC2<ul><li>Spark, running on / <a href="#ch01lvl1sec09" title="Running Spark on EC2" class="link">Running Spark on EC2</a></li></ul></li>
        <li>Elastic MapReduce<ul><li>Spark, deploying on / <a href="#ch01lvl1sec10" title="Deploying Spark on Elastic MapReduce" class="link">Deploying Spark on Elastic MapReduce</a></li></ul></li>
        <li>ENhanced Scala Interaction Mode (ensime) / <a href="#ch09lvl1sec55" title="IDE integration" class="link">IDE integration</a></li>
      </ul>
      <h2>F</h2>
      <ul>
        <li>filter function / <a href="#ch06lvl1sec36" title="Common Java RDD functions" class="link">Common Java RDD functions</a>, <a href="#ch06lvl1sec37" title="Standard RDD functions" class="link">Standard RDD functions</a></li>
        <li>filterWith function / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a></li>
        <li>first function / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a>, <a href="#ch06lvl1sec36" title="Common Java RDD functions" class="link">Common Java RDD functions</a></li>
        <li>flatMap function / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a>, <a href="#ch06lvl1sec36" title="Common Java RDD functions" class="link">Common Java RDD functions</a>, <a href="#ch06lvl1sec37" title="Standard RDD functions" class="link">Standard RDD functions</a></li>
        <li>FlatMapFunction&lt;T, R&gt; function / <a href="#ch06lvl1sec36" title="Spark Java function classes" class="link">Spark Java function classes</a></li>
        <li>flatMap method / <a href="#ch06lvl1sec36" title="Manipulating your RDD in Scala and Java" class="link">Manipulating your RDD in Scala and Java</a></li>
        <li>flatMapValues function / <a href="#ch06lvl1sec36" title="Other PairRDD functions" class="link">Other PairRDD functions</a>, <a href="#ch06lvl1sec36" title="JavaPairRDD functions" class="link">JavaPairRDD functions</a></li>
        <li>foldByKey function / <a href="#ch06lvl1sec36" title="Scala RDD functions" class="link">Scala RDD functions</a></li>
        <li>fold function / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a>, <a href="#ch06lvl1sec36" title="Common Java RDD functions" class="link">Common Java RDD functions</a>, <a href="#ch06lvl1sec37" title="Standard RDD functions" class="link">Standard RDD functions</a></li>
        <li>foreach function / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a>, <a href="#ch06lvl1sec36" title="Common Java RDD functions" class="link">Common Java RDD functions</a>, <a href="#ch06lvl1sec37" title="Standard RDD functions" class="link">Standard RDD functions</a></li>
        <li>Function2&lt;T1, T2, R&gt; function / <a href="#ch06lvl1sec36" title="Spark Java function classes" class="link">Spark Java function classes</a></li>
        <li>Function&lt;T,R&gt; function / <a href="#ch06lvl1sec36" title="Spark Java function classes" class="link">Spark Java function classes</a></li>
      </ul>
      <h2>G</h2>
      <ul>
        <li>garbage collection<ul><li>impact, measuring / <a href="#ch09lvl1sec53" title="Memory usage and garbage collection" class="link">Memory usage and garbage collection</a></li></ul></li>
        <li>general RDD functions<ul><li>aggregate / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a></li><li>cache / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a></li><li>collect / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a></li><li>count / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a></li><li>countByValue / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a></li><li>distinct / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a></li><li>filter / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a></li><li>filterWith / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a></li><li>first / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a></li><li>flatMap / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a></li><li>fold / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a></li><li>foreach / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a></li><li>groupBy / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a></li><li>keyBy / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a></li><li>map / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a></li><li>mapPartitions / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a></li><li>mapPartitionsWithIndex / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a></li><li>mapWith / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a></li><li>persist / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a></li><li>pipe / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a></li><li>sample / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a></li><li>takeSample / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a></li><li>toDebugString / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a></li><li>union / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a></li><li>unpersist / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a></li><li>zip / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a></li></ul></li>
        <li>groupBy function / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a>, <a href="#ch06lvl1sec36" title="Common Java RDD functions" class="link">Common Java RDD functions</a>, <a href="#ch06lvl1sec37" title="Standard RDD functions" class="link">Standard RDD functions</a></li>
        <li>groupByKey function / <a href="#ch06lvl1sec36" title="Scala RDD functions" class="link">Scala RDD functions</a>, <a href="#ch06lvl1sec37" title="PairRDD functions" class="link">PairRDD functions</a></li>
      </ul>
      <h2>H</h2>
      <ul>
        <li>HBase database<ul><li>use / <a href="#ch05lvl1sec32" title="Loading data into an RDD" class="link">Loading data into an RDD</a></li></ul></li>
        <li>Hive<ul><li>about / <a href="#ch07lvl1sec40" title="Why Hive/Shark?" class="link">Why Hive/Shark?</a></li></ul></li>
      </ul>
      <h2>I</h2>
      <ul>
        <li>IDE integration / <a href="#ch09lvl1sec55" title="IDE integration" class="link">IDE integration</a></li>
        <li>installation<ul><li>Shark / <a href="#ch07lvl1sec41" title="Installing Shark" class="link">Installing Shark</a></li></ul></li>
        <li>IntelliJ sbt plugin / <a href="#ch09lvl1sec55" title="IDE integration" class="link">IDE integration</a></li>
      </ul>
      <h2>J</h2>
      <ul>
        <li>Java<ul><li>SparkContext, creating in / <a href="#ch04lvl1sec26" title="Java" class="link">Java</a></li><li>testing / <a href="#ch08lvl1sec47" title="Testing in Java and Scala" class="link">Testing in Java and Scala</a></li></ul></li>
        <li>JavaPairRDD functions<ul><li>cogroup / <a href="#ch06lvl1sec36" title="JavaPairRDD functions" class="link">JavaPairRDD functions</a></li><li>combineByKey / <a href="#ch06lvl1sec36" title="JavaPairRDD functions" class="link">JavaPairRDD functions</a></li><li>collectAsMap / <a href="#ch06lvl1sec36" title="JavaPairRDD functions" class="link">JavaPairRDD functions</a></li><li>countByKey / <a href="#ch06lvl1sec36" title="JavaPairRDD functions" class="link">JavaPairRDD functions</a></li><li>flatMapValues / <a href="#ch06lvl1sec36" title="JavaPairRDD functions" class="link">JavaPairRDD functions</a></li><li>join / <a href="#ch06lvl1sec36" title="JavaPairRDD functions" class="link">JavaPairRDD functions</a></li><li>keys / <a href="#ch06lvl1sec36" title="JavaPairRDD functions" class="link">JavaPairRDD functions</a></li><li>lookup / <a href="#ch06lvl1sec36" title="JavaPairRDD functions" class="link">JavaPairRDD functions</a></li><li>reduceByKey / <a href="#ch06lvl1sec36" title="JavaPairRDD functions" class="link">JavaPairRDD functions</a></li><li>sortByKey / <a href="#ch06lvl1sec36" title="JavaPairRDD functions" class="link">JavaPairRDD functions</a></li><li>values / <a href="#ch06lvl1sec36" title="JavaPairRDD functions" class="link">JavaPairRDD functions</a></li></ul></li>
        <li>JavaPairRDD functions combination methods<ul><li>subtract / <a href="#ch06lvl1sec36" title="Methods for combining JavaPairRDD functions" class="link">Methods for combining JavaPairRDD functions</a></li><li>union / <a href="#ch06lvl1sec36" title="Methods for combining JavaPairRDD functions" class="link">Methods for combining JavaPairRDD functions</a></li><li>zip / <a href="#ch06lvl1sec36" title="Methods for combining JavaPairRDD functions" class="link">Methods for combining JavaPairRDD functions</a></li></ul></li>
        <li>Java RDD functions<ul><li>cache / <a href="#ch06lvl1sec36" title="Common Java RDD functions" class="link">Common Java RDD functions</a></li><li>coalesce / <a href="#ch06lvl1sec36" title="Common Java RDD functions" class="link">Common Java RDD functions</a></li><li>collect / <a href="#ch06lvl1sec36" title="Common Java RDD functions" class="link">Common Java RDD functions</a></li><li>count / <a href="#ch06lvl1sec36" title="Common Java RDD functions" class="link">Common Java RDD functions</a></li><li>countByValue / <a href="#ch06lvl1sec36" title="Common Java RDD functions" class="link">Common Java RDD functions</a></li><li>distinct / <a href="#ch06lvl1sec36" title="Common Java RDD functions" class="link">Common Java RDD functions</a></li><li>filter / <a href="#ch06lvl1sec36" title="Common Java RDD functions" class="link">Common Java RDD functions</a></li><li>first / <a href="#ch06lvl1sec36" title="Common Java RDD functions" class="link">Common Java RDD functions</a></li><li>flatMap / <a href="#ch06lvl1sec36" title="Common Java RDD functions" class="link">Common Java RDD functions</a></li><li>fold / <a href="#ch06lvl1sec36" title="Common Java RDD functions" class="link">Common Java RDD functions</a></li><li>foreach / <a href="#ch06lvl1sec36" title="Common Java RDD functions" class="link">Common Java RDD functions</a></li><li>groupBy / <a href="#ch06lvl1sec36" title="Common Java RDD functions" class="link">Common Java RDD functions</a></li><li>map / <a href="#ch06lvl1sec36" title="Common Java RDD functions" class="link">Common Java RDD functions</a></li><li>mapPartitions / <a href="#ch06lvl1sec36" title="Common Java RDD functions" class="link">Common Java RDD functions</a></li><li>reduce / <a href="#ch06lvl1sec36" title="Common Java RDD functions" class="link">Common Java RDD functions</a></li><li>sample / <a href="#ch06lvl1sec36" title="Common Java RDD functions" class="link">Common Java RDD functions</a></li></ul></li>
        <li>JavaRDD functions<ul><li>Spark Java function classes / <a href="#ch06lvl1sec36" title="Spark Java function classes" class="link">Spark Java function classes</a></li></ul></li>
        <li>join function / <a href="#ch06lvl1sec36" title="Functions for joining PairRDD functions" class="link">Functions for joining PairRDD functions</a>, <a href="#ch06lvl1sec36" title="JavaPairRDD functions" class="link">JavaPairRDD functions</a>, <a href="#ch06lvl1sec37" title="PairRDD functions" class="link">PairRDD functions</a></li>
        <li>joining functions, for PairRDD functions<ul><li>cogroup function / <a href="#ch06lvl1sec36" title="Functions for joining PairRDD functions" class="link">Functions for joining PairRDD functions</a></li><li>join function / <a href="#ch06lvl1sec36" title="Functions for joining PairRDD functions" class="link">Functions for joining PairRDD functions</a></li><li>subtractKey function / <a href="#ch06lvl1sec36" title="Functions for joining PairRDD functions" class="link">Functions for joining PairRDD functions</a></li></ul></li>
      </ul>
      <h2>K</h2>
      <ul>
        <li>keyBy function / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a></li>
        <li>keys function / <a href="#ch06lvl1sec36" title="JavaPairRDD functions" class="link">JavaPairRDD functions</a></li>
      </ul>
      <h2>L</h2>
      <ul>
        <li>leftOuterJoin function / <a href="#ch06lvl1sec37" title="PairRDD functions" class="link">PairRDD functions</a></li>
        <li>logs<ul><li>finding / <a href="#ch09lvl1sec51" title="Where to find logs?" class="link">Where to find logs?</a></li></ul></li>
        <li>lookup function / <a href="#ch06lvl1sec36" title="Other PairRDD functions" class="link">Other PairRDD functions</a>, <a href="#ch06lvl1sec36" title="JavaPairRDD functions" class="link">JavaPairRDD functions</a></li>
      </ul>
      <h2>M</h2>
      <ul>
        <li>map function / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a>, <a href="#ch06lvl1sec36" title="Common Java RDD functions" class="link">Common Java RDD functions</a></li>
        <li>mapParitions function / <a href="#ch06lvl1sec37" title="Standard RDD functions" class="link">Standard RDD functions</a></li>
        <li>mapPartitions function / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a>, <a href="#ch06lvl1sec36" title="Common Java RDD functions" class="link">Common Java RDD functions</a></li>
        <li>mapPartitionsWithIndex function / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a></li>
        <li>mapValues function / <a href="#ch06lvl1sec36" title="Other PairRDD functions" class="link">Other PairRDD functions</a></li>
        <li>mapWith function / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a></li>
        <li>Maven<ul><li>used, for Spark job building / <a href="#ch03lvl1sec22" title="Building your Spark job with Maven" class="link">Building your Spark job with Maven</a></li></ul></li>
        <li>mean function / <a href="#ch06lvl1sec36" title="DoubleRDD functions" class="link">DoubleRDD functions</a></li>
        <li>Mesos<ul><li>used, for Spark deploying / <a href="#ch01lvl1sec12" title="Deploying Spark on Mesos" class="link">Deploying Spark on Mesos</a></li></ul></li>
        <li>MESOS_NATIVE_LIBRARY variable / <a href="#ch01lvl1sec14" title="Deploying set of machines over SSH" class="link">Deploying set of machines over SSH</a></li>
      </ul>
      <h2>N</h2>
      <ul>
        <li>newAPIHadoopRDD method / <a href="#ch05lvl1sec32" title="Loading data into an RDD" class="link">Loading data into an RDD</a></li>
      </ul>
      <h2>P</h2>
      <ul>
        <li>PairFlatMapFunction&lt;T, K, V&gt; function / <a href="#ch06lvl1sec36" title="Spark Java function classes" class="link">Spark Java function classes</a></li>
        <li>PairFunction&lt;T, K, V&gt; function / <a href="#ch06lvl1sec36" title="Spark Java function classes" class="link">Spark Java function classes</a></li>
        <li>PairRDD functions, for RDD manipulation in Java<ul><li>lookup / <a href="#ch06lvl1sec36" title="Other PairRDD functions" class="link">Other PairRDD functions</a></li><li>mapValues / <a href="#ch06lvl1sec36" title="Other PairRDD functions" class="link">Other PairRDD functions</a></li><li>collectAsMap / <a href="#ch06lvl1sec36" title="Other PairRDD functions" class="link">Other PairRDD functions</a></li><li>countByKey / <a href="#ch06lvl1sec36" title="Other PairRDD functions" class="link">Other PairRDD functions</a></li><li>partitionBy / <a href="#ch06lvl1sec36" title="Other PairRDD functions" class="link">Other PairRDD functions</a></li><li>flatMapValues / <a href="#ch06lvl1sec36" title="Other PairRDD functions" class="link">Other PairRDD functions</a></li></ul></li>
        <li>PairRDD functions, for RDD manipulation in  Python<ul><li>collectAsMap / <a href="#ch06lvl1sec37" title="PairRDD functions" class="link">PairRDD functions</a></li><li>reduceByKey / <a href="#ch06lvl1sec37" title="PairRDD functions" class="link">PairRDD functions</a></li><li>countByKey / <a href="#ch06lvl1sec37" title="PairRDD functions" class="link">PairRDD functions</a></li><li>join / <a href="#ch06lvl1sec37" title="PairRDD functions" class="link">PairRDD functions</a></li><li>rightOuterJoin / <a href="#ch06lvl1sec37" title="PairRDD functions" class="link">PairRDD functions</a></li><li>leftOuterJoin / <a href="#ch06lvl1sec37" title="PairRDD functions" class="link">PairRDD functions</a></li><li>combineByKey / <a href="#ch06lvl1sec37" title="PairRDD functions" class="link">PairRDD functions</a></li><li>groupByKey / <a href="#ch06lvl1sec37" title="PairRDD functions" class="link">PairRDD functions</a></li><li>cogroup / <a href="#ch06lvl1sec37" title="PairRDD functions" class="link">PairRDD functions</a></li></ul></li>
        <li>parallelize() function / <a href="#ch05lvl1sec32" title="Loading data into an RDD" class="link">Loading data into an RDD</a></li>
        <li>partitionBy function / <a href="#ch06lvl1sec36" title="Other PairRDD functions" class="link">Other PairRDD functions</a>, <a href="#ch06lvl1sec37" title="Standard RDD functions" class="link">Standard RDD functions</a></li>
        <li>persist function / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a></li>
        <li>pipe function / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a>, <a href="#ch06lvl1sec37" title="Standard RDD functions" class="link">Standard RDD functions</a></li>
        <li>PPA (Personal Package Archive) / <a href="#ch03lvl1sec21" title="Building your Spark project with sbt" class="link">Building your Spark project with sbt</a></li>
        <li>PRNG (pseudorandom number generator) / <a href="#ch06lvl1sec36" title="Manipulating your RDD in Scala and Java" class="link">Manipulating your RDD in Scala and Java</a></li>
        <li>Python<ul><li>SparkContext, creating in / <a href="#ch04lvl1sec28" title="Python" class="link">Python</a></li><li>RDD, manipulating in / <a href="#ch06lvl1sec37" title="Manipulating your RDD in Python" class="link">Manipulating your RDD in Python</a></li><li>testing / <a href="#ch08lvl1sec48" title="Testing in Python" class="link">Testing in Python</a></li></ul></li>
      </ul>
      <h2>R</h2>
      <ul>
        <li>RDD<ul><li>manipulating, in Scala / <a href="#ch06lvl1sec36" title="Manipulating your RDD in Scala and Java" class="link">Manipulating your RDD in Scala and Java</a></li><li>manipulating, in Java / <a href="#ch06lvl1sec36" title="Manipulating your RDD in Scala and Java" class="link">Manipulating your RDD in Scala and Java</a></li><li>manipulating in Python / <a href="#ch06lvl1sec37" title="Manipulating your RDD in Python" class="link">Manipulating your RDD in Python</a></li></ul></li>
        <li>RDD manipulation, in Python<ul><li>about / <a href="#ch06lvl1sec37" title="Manipulating your RDD in Python" class="link">Manipulating your RDD in Python</a></li><li>standard RDD functions / <a href="#ch06lvl1sec37" title="Standard RDD functions" class="link">Standard RDD functions</a></li><li>PairRDD functions / <a href="#ch06lvl1sec37" title="PairRDD functions" class="link">PairRDD functions</a></li></ul></li>
        <li>RDDs<ul><li>about / <a href="#ch05lvl1sec31" title="RDDs" class="link">RDDs</a></li><li>data, loading into / <a href="#ch05lvl1sec32" title="Loading data into an RDD" class="link">Loading data into an RDD</a></li><li>saving, ways / <a href="#ch05lvl1sec33" title="Saving your data" class="link">Saving your data</a></li></ul></li>
        <li>reduceByKey function / <a href="#ch06lvl1sec36" title="Scala RDD functions" class="link">Scala RDD functions</a>, <a href="#ch06lvl1sec36" title="JavaPairRDD functions" class="link">JavaPairRDD functions</a>, <a href="#ch06lvl1sec37" title="PairRDD functions" class="link">PairRDD functions</a></li>
        <li>reduce function / <a href="#ch06lvl1sec36" title="Common Java RDD functions" class="link">Common Java RDD functions</a>, <a href="#ch06lvl1sec37" title="Standard RDD functions" class="link">Standard RDD functions</a></li>
        <li>reference links / <a href="#ch06lvl1sec38" title="Links and references" class="link">Links and references</a><ul><li>SparkContext, creating in / <a href="#ch04lvl1sec29" title="Links and references" class="link">Links and references</a></li><li>saving / <a href="#ch05lvl1sec34" title="Links and references" class="link">Links and references</a></li><li>Hive queries, using in / <a href="#ch07lvl1sec45" title="Links and references" class="link">Links and references</a></li><li>mailing lists / <a href="#ch09lvl1sec59" title="Links and references" class="link">Links and references</a></li></ul></li>
        <li>referencelinks / <a href="#ch01lvl1sec15" title="Links and references" class="link">Links and references</a></li>
        <li>rightOuterJoin function / <a href="#ch06lvl1sec37" title="PairRDD functions" class="link">PairRDD functions</a></li>
      </ul>
      <h2>S</h2>
      <ul>
        <li>S3<ul><li>data, loading from / <a href="#ch02lvl1sec19" title="Interactively loading data from S3" class="link">Interactively loading data from S3</a></li><li>path / <a href="#ch02lvl1sec19" title="Interactively loading data from S3" class="link">Interactively loading data from S3</a></li></ul></li>
        <li>sample function / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a>, <a href="#ch06lvl1sec36" title="Common Java RDD functions" class="link">Common Java RDD functions</a></li>
        <li>sampleStdev function / <a href="#ch06lvl1sec36" title="DoubleRDD functions" class="link">DoubleRDD functions</a></li>
        <li>sbt<ul><li>used,for Spark project building / <a href="#ch03lvl1sec21" title="Building your Spark project with sbt" class="link">Building your Spark project with sbt</a></li></ul></li>
        <li>Scala<ul><li>about / <a href="#ch04lvl1sec25" title="Scala" class="link">Scala</a></li><li>SparkContext, creating in / <a href="#ch04lvl1sec25" title="Scala" class="link">Scala</a></li><li>testing / <a href="#ch08lvl1sec47" title="Testing in Java and Scala" class="link">Testing in Java and Scala</a></li></ul></li>
        <li>Scala RDD functions<ul><li>foldByKey / <a href="#ch06lvl1sec36" title="Scala RDD functions" class="link">Scala RDD functions</a></li><li>reduceByKey / <a href="#ch06lvl1sec36" title="Scala RDD functions" class="link">Scala RDD functions</a></li><li>groupByKey / <a href="#ch06lvl1sec36" title="Scala RDD functions" class="link">Scala RDD functions</a></li></ul></li>
        <li>Scala REPL (Read-Evaluate-Print Loop) / <a href="#ch02lvl1sec18" title="Using the Spark shell to run logistic regression" class="link">Using the Spark shell to run logistic regression</a></li>
        <li>SCALA_HOME variable / <a href="#ch01lvl1sec14" title="Deploying set of machines over SSH" class="link">Deploying set of machines over SSH</a></li>
        <li>serialization mechanisms / <a href="#ch09lvl1sec54" title="Serialization" class="link">Serialization</a></li>
        <li>shared Java APIs / <a href="#ch04lvl1sec27" title="Shared Java and Scala APIs" class="link">Shared Java and Scala APIs</a></li>
        <li>shared Scala APIs / <a href="#ch04lvl1sec27" title="Shared Java and Scala APIs" class="link">Shared Java and Scala APIs</a></li>
        <li>Shark<ul><li>about / <a href="#ch07lvl1sec40" title="Why Hive/Shark?" class="link">Why Hive/Shark?</a></li><li>installing / <a href="#ch07lvl1sec41" title="Installing Shark" class="link">Installing Shark</a></li><li>running / <a href="#ch07lvl1sec42" title="Running Shark" class="link">Running Shark</a></li></ul></li>
        <li>simple text file<ul><li>loading / <a href="#ch02lvl1sec17" title="Loading a simple text file" class="link">Loading a simple text file</a></li></ul></li>
        <li>sortByKey function / <a href="#ch06lvl1sec36" title="JavaPairRDD functions" class="link">JavaPairRDD functions</a></li>
        <li>Spark<ul><li>running, on single machine / <a href="#ch01lvl1sec08" title="Running Spark on a single machine" class="link">Running Spark on a single machine</a></li><li>running, on EC2 / <a href="#ch01lvl1sec09" title="Running Spark on EC2" class="link">Running Spark on EC2</a></li><li>running, on EC2 with scripts / <a href="#ch01lvl1sec09" title="Running Spark on EC2 with the scripts" class="link">Running Spark on EC2 with the scripts</a></li><li>deploying, on Elastic MapReduce / <a href="#ch01lvl1sec10" title="Deploying Spark on Elastic MapReduce" class="link">Deploying Spark on Elastic MapReduce</a></li><li>deploying, Chef (opscode) / <a href="#ch01lvl1sec11" title="Deploying Spark with Chef (opscode)" class="link">Deploying Spark with Chef (opscode)</a></li><li>deploying, on Mesos / <a href="#ch01lvl1sec12" title="Deploying Spark on Mesos" class="link">Deploying Spark on Mesos</a></li><li>deploying, on YARN / <a href="#ch01lvl1sec13" title="Deploying Spark on YARN" class="link">Deploying Spark on YARN</a></li><li>using, with other languages / <a href="#ch09lvl1sec56" title="Using Spark with other languages" class="link">Using Spark with other languages</a></li><li>security / <a href="#ch09lvl1sec57" title="A quick note on security" class="link">A quick note on security</a></li><li>mailing lists / <a href="#ch09lvl1sec58" title="Mailing lists" class="link">Mailing lists</a></li></ul></li>
        <li>SPARK<ul><li>deploying, over SSH / <a href="#ch01lvl1sec14" title="Deploying set of machines over SSH" class="link">Deploying set of machines over SSH</a></li></ul></li>
        <li>SparkContext<ul><li>creating, in Scala / <a href="#ch04lvl1sec25" title="Scala" class="link">Scala</a></li><li>creating, in Java / <a href="#ch04lvl1sec26" title="Java" class="link">Java</a></li><li>creating, in Python / <a href="#ch04lvl1sec28" title="Python" class="link">Python</a></li><li>interactions, testing / <a href="#ch08lvl1sec47" title="Testing interactions with SparkContext" class="link">Testing interactions with SparkContext</a></li></ul></li>
        <li>SparkContext class / <a href="#ch05lvl1sec33" title="Saving your data" class="link">Saving your data</a></li>
        <li>Spark Java function classes<ul><li>Function&lt;T,R&gt; / <a href="#ch06lvl1sec36" title="Spark Java function classes" class="link">Spark Java function classes</a></li><li>DoubleFunction&lt;T&gt; / <a href="#ch06lvl1sec36" title="Spark Java function classes" class="link">Spark Java function classes</a></li><li>PairFunction&lt;T, K, V&gt; / <a href="#ch06lvl1sec36" title="Spark Java function classes" class="link">Spark Java function classes</a></li><li>FlatMapFunction&lt;T, R&gt; / <a href="#ch06lvl1sec36" title="Spark Java function classes" class="link">Spark Java function classes</a></li><li>PairFlatMapFunction&lt;T, K, V&gt; / <a href="#ch06lvl1sec36" title="Spark Java function classes" class="link">Spark Java function classes</a></li><li>DoubleFlatMapFunction&lt;T&gt; / <a href="#ch06lvl1sec36" title="Spark Java function classes" class="link">Spark Java function classes</a></li><li>Function2&lt;T1, T2, R&gt; / <a href="#ch06lvl1sec36" title="Spark Java function classes" class="link">Spark Java function classes</a></li></ul></li>
        <li>Spark job<ul><li>building, with Maven / <a href="#ch03lvl1sec22" title="Building your Spark job with Maven" class="link">Building your Spark job with Maven</a></li><li>building, with other options / <a href="#ch03lvl1sec23" title="Building your Spark job with something else" class="link">Building your Spark job with something else</a></li></ul></li>
        <li>Spark program<ul><li>Hive queries, using in / <a href="#ch07lvl1sec44" title="Using Hive queries in a Spark program" class="link">Using Hive queries in a Spark program</a></li></ul></li>
        <li>Spark project<ul><li>building, with sbt / <a href="#ch03lvl1sec21" title="Building your Spark project with sbt" class="link">Building your Spark project with sbt</a></li></ul></li>
        <li>Spark shell<ul><li>used, for logistic regression running / <a href="#ch02lvl1sec18" title="Using the Spark shell to run logistic regression" class="link">Using the Spark shell to run logistic regression</a></li></ul></li>
        <li>SPARK_MASTER_IP variable / <a href="#ch01lvl1sec14" title="Deploying set of machines over SSH" class="link">Deploying set of machines over SSH</a></li>
        <li>SPARK_MASTER_PORT variable / <a href="#ch01lvl1sec14" title="Deploying set of machines over SSH" class="link">Deploying set of machines over SSH</a></li>
        <li>SPARK_MASTER_WEBUI_PORT variable / <a href="#ch01lvl1sec14" title="Deploying set of machines over SSH" class="link">Deploying set of machines over SSH</a></li>
        <li>SPARK_WEBUI_PORT variable / <a href="#ch01lvl1sec14" title="Deploying set of machines over SSH" class="link">Deploying set of machines over SSH</a></li>
        <li>SPARK_WORKER_CORES variable / <a href="#ch01lvl1sec14" title="Deploying set of machines over SSH" class="link">Deploying set of machines over SSH</a></li>
        <li>SPARK_WORKER_DIR variable / <a href="#ch01lvl1sec14" title="Deploying set of machines over SSH" class="link">Deploying set of machines over SSH</a></li>
        <li>SPARK_WORKER_MEMORY variable / <a href="#ch01lvl1sec14" title="Deploying set of machines over SSH" class="link">Deploying set of machines over SSH</a></li>
        <li>SPARK_WORKER_PORT variable / <a href="#ch01lvl1sec14" title="Deploying set of machines over SSH" class="link">Deploying set of machines over SSH</a></li>
        <li>standalone mode / <a href="#ch01lvl1sec14" title="Deploying set of machines over SSH" class="link">Deploying set of machines over SSH</a></li>
        <li>Stats function / <a href="#ch06lvl1sec36" title="DoubleRDD functions" class="link">DoubleRDD functions</a></li>
        <li>Stdev function / <a href="#ch06lvl1sec36" title="DoubleRDD functions" class="link">DoubleRDD functions</a></li>
        <li>stop() method / <a href="#ch04lvl1sec27" title="Shared Java and Scala APIs" class="link">Shared Java and Scala APIs</a></li>
        <li>storage level <ul><li>MEMORY_ONLY / <a href="#ch09lvl1sec53" title="Memory usage and garbage collection" class="link">Memory usage and garbage collection</a></li><li>MEMORY_AND_DISK / <a href="#ch09lvl1sec53" title="Memory usage and garbage collection" class="link">Memory usage and garbage collection</a></li><li>DISK_ONLY / <a href="#ch09lvl1sec53" title="Memory usage and garbage collection" class="link">Memory usage and garbage collection</a></li></ul></li>
        <li>subtract function / <a href="#ch06lvl1sec36" title="Methods for combining JavaPairRDD functions" class="link">Methods for combining JavaPairRDD functions</a></li>
        <li>subtractKey function / <a href="#ch06lvl1sec36" title="Functions for joining PairRDD functions" class="link">Functions for joining PairRDD functions</a></li>
        <li>Sum function / <a href="#ch06lvl1sec36" title="DoubleRDD functions" class="link">DoubleRDD functions</a></li>
      </ul>
      <h2>T</h2>
      <ul>
        <li>take function / <a href="#ch06lvl1sec37" title="Standard RDD functions" class="link">Standard RDD functions</a></li>
        <li>takeSample function / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a></li>
        <li>testing<ul><li>in Java / <a href="#ch08lvl1sec47" title="Testing in Java and Scala" class="link">Testing in Java and Scala</a></li><li>in Scala / <a href="#ch08lvl1sec47" title="Testing in Java and Scala" class="link">Testing in Java and Scala</a></li><li>in Python / <a href="#ch08lvl1sec48" title="Testing in Python" class="link">Testing in Python</a></li><li>reference links / <a href="#ch08lvl1sec49" title="Links and references" class="link">Links and references</a></li></ul></li>
        <li>toDebugString function / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a></li>
      </ul>
      <h2>U</h2>
      <ul>
        <li>union function / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a>, <a href="#ch06lvl1sec36" title="Methods for combining JavaPairRDD functions" class="link">Methods for combining JavaPairRDD functions</a>, <a href="#ch06lvl1sec37" title="Standard RDD functions" class="link">Standard RDD functions</a></li>
        <li>unpersist function / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a></li>
      </ul>
      <h2>V</h2>
      <ul>
        <li>values function / <a href="#ch06lvl1sec36" title="JavaPairRDD functions" class="link">JavaPairRDD functions</a></li>
        <li>variance function / <a href="#ch06lvl1sec36" title="DoubleRDD functions" class="link">DoubleRDD functions</a></li>
      </ul>
      <h2>Y</h2>
      <ul>
        <li>YARN<ul><li>used, for Spark deploying / <a href="#ch01lvl1sec13" title="Deploying Spark on YARN" class="link">Deploying Spark on YARN</a></li></ul></li>
      </ul>
      <h2>Z</h2>
      <ul>
        <li>zip function / <a href="#ch06lvl1sec36" title="General RDD functions" class="link">General RDD functions</a>, <a href="#ch06lvl1sec36" title="Methods for combining JavaPairRDD functions" class="link">Methods for combining JavaPairRDD functions</a></li>
      </ul>
    </div>
  </div></div></div>
</div></div></div></body></html>
