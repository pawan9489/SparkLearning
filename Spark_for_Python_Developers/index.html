<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="">

    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0"> <!--320-->
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no, user-scalable=no">

    <link rel="icon" href="../../mapt/images/favicon.ico">

    <link rel="stylesheet" href="../../mapt/css/font-awesome.css">
    <link rel="stylesheet" href="../../mapt/css/google-fonts.css">
    <link rel="stylesheet" href="../../mapt/css/devicon.css">

    <link rel="stylesheet" href="../../mapt/css/bootstrap.css">
    <link rel="stylesheet" href="../../mapt/css/bootstrap-xl.css">
    <link rel="stylesheet" href="../../mapt/css/magnific-popup.css">
    <link rel="stylesheet" href="../../mapt/css/prism.css">
    <link rel="stylesheet" href="../../mapt/css/hljs-github.css">

    <link rel="stylesheet" href="../../mapt/css/mapt.css">
    <link rel="stylesheet" href="../../mapt/css/custom.css">

    <script src="../../mapt/js/jquery.js"></script>
    <script src="../../mapt/js/bootstrap.js"></script>
    <script src="../../mapt/js/jquery.magnific-popup.js"></script>
    <script src="../../mapt/js/highlight.min.js"></script>

    <script src="../../mapt/js/custom.js"></script>
    
    <title>Spark for Python Developers</title>
</head>

<body class="home-body">
    <div id="wrapper">
        <div id="sidebar-wrapper">    
            <ul class="sidebar-nav">
                <div class="list-group" id="sidebar-nav" role="tablist">
                    <li>
                        <a href="../../index.html" class="sidenav-menu-holder back-btn" id="back-link">
                            <span class="sidenav-menu">Book List</span>
                            <span class="pull-left mr5"><i class="fa fa-chevron-left"></i></span>
                        </a>
                    </li>
                    
                    <li class="book-info copyright">
                        <span class="info text-nowrap"><span class="copyleft">&copy;</span><span><strong>RuTracker</strong>.org</span></span>
                    </li>          
                    <li class="book-info copyright">
                        <span class="info text-nowrap">Pub date: <strong>24 Dec 2015</strong></span>
                    </li>         
                    <li class="book-info">
                        <span class="info text-nowrap">Price: €<strong>28.99</strong></span>
                        <span class="info text-nowrap">ISBN: <strong>9781784399696</strong></span>
                    </li>     
            
                    <li>
                        <a href="graphics/cover.jpg" class="sidenav-menu-holder cover-img">
                            <img src="graphics/cover.jpg" class="cover-image">
                        </a>
                    </li>        
            
                    <div class="book_navigation">
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse1">
                                <div class="section-name">1: Setting Up a Spark Virtual Environment</div>
                            </a>
                        </li>
                        <div id="collapse1" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="1" class="sub-nav">
                                <a href="#ch01">
                                    <div class="section-name">Chapter 1: Setting Up a Spark Virtual Environment</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec08" class="sub-nav">
                                <a href="#ch01lvl1sec08">                    
                                    <div class="section-name">Understanding the architecture of data-intensive applications</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec09" class="sub-nav">
                                <a href="#ch01lvl1sec09">                    
                                    <div class="section-name">Understanding Spark</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec10" class="sub-nav">
                                <a href="#ch01lvl1sec10">                    
                                    <div class="section-name">Understanding Anaconda</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec11" class="sub-nav">
                                <a href="#ch01lvl1sec11">                    
                                    <div class="section-name">Setting up the Spark powered environment</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec12" class="sub-nav">
                                <a href="#ch01lvl1sec12">                    
                                    <div class="section-name">Building our first app with PySpark</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec13" class="sub-nav">
                                <a href="#ch01lvl1sec13">                    
                                    <div class="section-name">Virtualizing the environment with Vagrant</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec14" class="sub-nav">
                                <a href="#ch01lvl1sec14">                    
                                    <div class="section-name">Moving to the cloud</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec15" class="sub-nav">
                                <a href="#ch01lvl1sec15">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse2">
                                <div class="section-name">2: Building Batch and Streaming Apps with Spark</div>
                            </a>
                        </li>
                        <div id="collapse2" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="2" class="sub-nav">
                                <a href="#ch02">
                                    <div class="section-name">Chapter 2: Building Batch and Streaming Apps with Spark</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec16" class="sub-nav">
                                <a href="#ch02lvl1sec16">                    
                                    <div class="section-name">Architecting data-intensive apps</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec17" class="sub-nav">
                                <a href="#ch02lvl1sec17">                    
                                    <div class="section-name">Connecting to social networks</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec18" class="sub-nav">
                                <a href="#ch02lvl1sec18">                    
                                    <div class="section-name">Analyzing the data</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec19" class="sub-nav">
                                <a href="#ch02lvl1sec19">                    
                                    <div class="section-name">Exploring the GitHub world</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec20" class="sub-nav">
                                <a href="#ch02lvl1sec20">                    
                                    <div class="section-name">Previewing our app</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec21" class="sub-nav">
                                <a href="#ch02lvl1sec21">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse3">
                                <div class="section-name">3: Juggling Data with Spark</div>
                            </a>
                        </li>
                        <div id="collapse3" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="3" class="sub-nav">
                                <a href="#ch03">
                                    <div class="section-name">Chapter 3: Juggling Data with Spark</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec22" class="sub-nav">
                                <a href="#ch03lvl1sec22">                    
                                    <div class="section-name">Revisiting the data-intensive app architecture</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec23" class="sub-nav">
                                <a href="#ch03lvl1sec23">                    
                                    <div class="section-name">Serializing and deserializing data</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec24" class="sub-nav">
                                <a href="#ch03lvl1sec24">                    
                                    <div class="section-name">Harvesting and storing data</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec25" class="sub-nav">
                                <a href="#ch03lvl1sec25">                    
                                    <div class="section-name">Exploring data using Blaze</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec26" class="sub-nav">
                                <a href="#ch03lvl1sec26">                    
                                    <div class="section-name">Exploring data using Spark SQL</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec27" class="sub-nav">
                                <a href="#ch03lvl1sec27">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse4">
                                <div class="section-name">4: Learning from Data Using Spark</div>
                            </a>
                        </li>
                        <div id="collapse4" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="4" class="sub-nav">
                                <a href="#ch04">
                                    <div class="section-name">Chapter 4: Learning from Data Using Spark</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec28" class="sub-nav">
                                <a href="#ch04lvl1sec28">                    
                                    <div class="section-name">Contextualizing Spark MLlib in the app architecture</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec29" class="sub-nav">
                                <a href="#ch04lvl1sec29">                    
                                    <div class="section-name">Classifying Spark MLlib algorithms</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec30" class="sub-nav">
                                <a href="#ch04lvl1sec30">                    
                                    <div class="section-name">Spark MLlib data types</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec31" class="sub-nav">
                                <a href="#ch04lvl1sec31">                    
                                    <div class="section-name">Machine learning workflows and data flows</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec32" class="sub-nav">
                                <a href="#ch04lvl1sec32">                    
                                    <div class="section-name">Clustering the Twitter dataset</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec33" class="sub-nav">
                                <a href="#ch04lvl1sec33">                    
                                    <div class="section-name">Building machine learning pipelines</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec34" class="sub-nav">
                                <a href="#ch04lvl1sec34">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse5">
                                <div class="section-name">5: Streaming Live Data with Spark</div>
                            </a>
                        </li>
                        <div id="collapse5" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="5" class="sub-nav">
                                <a href="#ch05">
                                    <div class="section-name">Chapter 5: Streaming Live Data with Spark</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec35" class="sub-nav">
                                <a href="#ch05lvl1sec35">                    
                                    <div class="section-name">Laying the foundations of streaming architecture</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec36" class="sub-nav">
                                <a href="#ch05lvl1sec36">                    
                                    <div class="section-name">Processing live data with TCP sockets</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec37" class="sub-nav">
                                <a href="#ch05lvl1sec37">                    
                                    <div class="section-name">Manipulating Twitter data in real time</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec38" class="sub-nav">
                                <a href="#ch05lvl1sec38">                    
                                    <div class="section-name">Building a reliable and scalable streaming app</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec39" class="sub-nav">
                                <a href="#ch05lvl1sec39">                    
                                    <div class="section-name">Closing remarks on the Lambda and Kappa architecture</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec40" class="sub-nav">
                                <a href="#ch05lvl1sec40">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse6">
                                <div class="section-name">6: Visualizing Insights and Trends</div>
                            </a>
                        </li>
                        <div id="collapse6" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="6" class="sub-nav">
                                <a href="#ch06">
                                    <div class="section-name">Chapter 6: Visualizing Insights and Trends</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec41" class="sub-nav">
                                <a href="#ch06lvl1sec41">                    
                                    <div class="section-name">Revisiting the data-intensive apps architecture</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec42" class="sub-nav">
                                <a href="#ch06lvl1sec42">                    
                                    <div class="section-name">Preprocessing the data for visualization</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec43" class="sub-nav">
                                <a href="#ch06lvl1sec43">                    
                                    <div class="section-name">Gauging words, moods, and memes at a glance</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec44" class="sub-nav">
                                <a href="#ch06lvl1sec44">                    
                                    <div class="section-name">Geo-locating tweets and mapping meetups</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec45" class="sub-nav">
                                <a href="#ch06lvl1sec45">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapsebackindex">
                                <div class="section-name">Appendix A: Index</div>
                            </a>
                        </li>
                        <div id="collapsebackindex" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="backindex" class="sub-nav">
                                <a href="#backindex">
                                    <div class="section-name">Chapter Appendix A: Index</div>
                                </a>
                            </li>
                        </div>
                    </div>
                </div>
            </ul>
        </div>
        
        <div id="page-content-wrapper" class="book-page">
            <a href="#" id="menu-toggle" class="toggle-nav"><i class="fa fa-bars fa-2x mr5"></i></a>
            
            <a href="#" id="back_to_top" class="back-to-top"><img src="../../mapt/images/kopimi.svg"></a>
            
            <div class="col-sm-12 col-xl-offset-2 col-xl-8 col-lg-offset-1 col-lg-10">
                <div class="btn-group pull-right mt15 mb30" role="group">
                    <a href="#home" class="btn btn-default">
                        <i class="fa fa-share fa-lg no-text-padding"></i>
                        <span class="hidden-xs ml5">Book Home</span>
                    </a>
                    <button class="btn btn-default" data-nid="23624" id="code-download">
                        <i class="fa fa-file fa-lg"></i>
                        <span class="hidden-xs ml5">Download Code Files</span>
                    </button>
                </div>
            </div>
            <div class="clearfix"></div>
            
            <div id="book-wrapper" class="container-fluid">
                <div class="col-sm-12 col-xl-offset-2 col-xl-8 col-lg-offset-1 col-lg-10" id="home">
                    <h2 class="product-title">Spark for Python Developers</h2>
                    <hr>
                    <div class="row">
                        <div class="col-sm-12">
                            <h5 class="mt10">By Amit Nandi</h5>
                            <div>
                                <p class="mb20"><b>A concise guide to implementing Spark Big Data analytics for Python developers, and building a real-time and insightful trend tracker data intensive app</b></p>
                                <a href="#ch01" class="btn btn-info btn-lg pull-right hidden-xs">
                                    Start Reading <i class="fa fa-chevron-right ml5"></i>
                                </a>
                                <a href="#ch01" class="btn btn-info btn-lg btn-block mt20 mb20 visible-xs">
                                    Start Reading <i class="fa fa-chevron-right ml5"></i>
                                </a>
                                <div class="clearfix"></div>
                                <div class="col-sm-12">
                                    <ul id="myTabs" class="nav nav-tabs nav-justified hidden-xs mt20" role="tablist">
                                        <li class="active">
                                            <a href="#info" role="tab" id="info-tab" data-toggle="tab">
                                                <h5>Info</h5>
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#content" role="tab" id="content-tab" data-toggle="tab">
                                                <h5>Contents</h5>
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#author" role="tab" id="author-tab" data-toggle="tab">
                                                <h5>Author</h5>
                                            </a>
                                        </li>
                                    </ul>
                
                                    <ul id="myTabsMobile" class="nav nav-pills text-center nav-stacked visible-xs mb60" role="tablist">
                                        <li class="active">
                                            <a href="#info" role="tab" id="info-tab-responsive" data-toggle="tab">
                                                Info
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#content" role="tab" id="content-tab-responsive" data-toggle="tab">
                                                Contents
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#author" role="tab" id="author-tab-responsive" data-toggle="tab">
                                                Author
                                            </a>
                                        </li>
                                    </ul>
                
                                    <div id="myTabContent" class="tab-content pt30">
                                    
                                        <div role="tabpanel" class="tab-pane active fade in" id="info">
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">Features</h5>
                                            </div>
                                            <div class="hidden-xs">
                                                <h5>Features</h5>
                                            </div>
                                            <hr>
                                            <div>
                                                <ul>
                <li>Set up real-time streaming and batch data intensive infrastructure using Spark and Python</li>
                <li>Deliver insightful visualizations in a web app using Spark (PySpark)</li>
                <li>Inject live data using Spark Streaming with real-time events</li>
                </ul>
                                            </div>
                                            <br>
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">Learning</h5>
                                            </div>
                                            <div class="hidden-xs">
                                                <h5>Learning</h5>
                                            </div>
                                            <hr>
                                            <div>
                                                <ul>
                <li>Create a Python development environment powered by Spark (PySpark), Blaze, and Bookeh</li>
                <li>Build a real-time trend tracker data intensive app</li>
                <li>Visualize the trends and insights gained from data using Bookeh</li>
                <li>Generate insights from data using machine learning through Spark MLLIB</li>
                <li>Juggle with data using Blaze</li>
                <li>Create training data sets and train the Machine Learning models</li>
                <li>Test the machine learning models on test datasets</li>
                <li>Deploy the machine learning algorithms and models and scale it for real-time events</li>
                </ul>
                                            </div>
                                            <br>
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">About</h5>
                                            </div>
                                            <div class="hidden-xs">
                                                <h5>About</h5>
                                            </div>
                                            <hr>
                                            <div>
                                                <p>Looking for a cluster computing system that provides high-level APIs? Apache Spark is your answer—an open source, fast, and general purpose cluster computing system. Spark's multi-stage memory primitives provide performance up to 100 times faster than Hadoop, and it is also well-suited for machine learning algorithms.</p>
                <p>Are you a Python developer inclined to work with Spark engine? If so, this book will be your companion as you create data-intensive app using Spark as a processing engine, Python visualization libraries, and web frameworks such as Flask.</p>
                <p>To begin with, you will learn the most effective way to install the Python development environment powered by Spark, Blaze, and Bookeh. You will then find out how to connect with data stores such as MySQL, MongoDB, Cassandra, and Hadoop.</p>
                <p>You’ll expand your skills throughout, getting familiarized with the various data sources (Github, Twitter, Meetup, and Blogs), their data structures, and solutions to effectively tackle complexities. You’ll explore datasets using iPython Notebook and will discover how to optimize the data models and pipeline. Finally, you’ll get to know how to create training datasets and train the machine learning models.</p>
                <p>By the end of the book, you will have created a real-time and insightful trend tracker data-intensive app with Spark.</p>
                                            </div>
                                        </div>
                                        
                                        <div role="tabpanel" class="tab-pane fade in" id="content">
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">Contents</h5>
                                                <hr>
                                            </div>
                                            <ul>
                                                <div>
                                                    <li data-chapter="1">
                                                        <div class="section-name">1: Setting Up a Spark Virtual Environment</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="1" class="chapter-section">
                                                                    <a href="#ch01">        
                                                                        <div class="section-name">Chapter 1: Setting Up a Spark Virtual Environment</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec08" class="chapter-section">
                                                                    <a href="#ch01lvl1sec08">                    
                                                                        <div class="section-name">Understanding the architecture of data-intensive applications</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec09" class="chapter-section">
                                                                    <a href="#ch01lvl1sec09">                    
                                                                        <div class="section-name">Understanding Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec10" class="chapter-section">
                                                                    <a href="#ch01lvl1sec10">                    
                                                                        <div class="section-name">Understanding Anaconda</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec11" class="chapter-section">
                                                                    <a href="#ch01lvl1sec11">                    
                                                                        <div class="section-name">Setting up the Spark powered environment</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec12" class="chapter-section">
                                                                    <a href="#ch01lvl1sec12">                    
                                                                        <div class="section-name">Building our first app with PySpark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec13" class="chapter-section">
                                                                    <a href="#ch01lvl1sec13">                    
                                                                        <div class="section-name">Virtualizing the environment with Vagrant</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec14" class="chapter-section">
                                                                    <a href="#ch01lvl1sec14">                    
                                                                        <div class="section-name">Moving to the cloud</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec15" class="chapter-section">
                                                                    <a href="#ch01lvl1sec15">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="2">
                                                        <div class="section-name">2: Building Batch and Streaming Apps with Spark</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="2" class="chapter-section">
                                                                    <a href="#ch02">        
                                                                        <div class="section-name">Chapter 2: Building Batch and Streaming Apps with Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec16" class="chapter-section">
                                                                    <a href="#ch02lvl1sec16">                    
                                                                        <div class="section-name">Architecting data-intensive apps</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec17" class="chapter-section">
                                                                    <a href="#ch02lvl1sec17">                    
                                                                        <div class="section-name">Connecting to social networks</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec18" class="chapter-section">
                                                                    <a href="#ch02lvl1sec18">                    
                                                                        <div class="section-name">Analyzing the data</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec19" class="chapter-section">
                                                                    <a href="#ch02lvl1sec19">                    
                                                                        <div class="section-name">Exploring the GitHub world</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec20" class="chapter-section">
                                                                    <a href="#ch02lvl1sec20">                    
                                                                        <div class="section-name">Previewing our app</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec21" class="chapter-section">
                                                                    <a href="#ch02lvl1sec21">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="3">
                                                        <div class="section-name">3: Juggling Data with Spark</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="3" class="chapter-section">
                                                                    <a href="#ch03">        
                                                                        <div class="section-name">Chapter 3: Juggling Data with Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec22" class="chapter-section">
                                                                    <a href="#ch03lvl1sec22">                    
                                                                        <div class="section-name">Revisiting the data-intensive app architecture</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec23" class="chapter-section">
                                                                    <a href="#ch03lvl1sec23">                    
                                                                        <div class="section-name">Serializing and deserializing data</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec24" class="chapter-section">
                                                                    <a href="#ch03lvl1sec24">                    
                                                                        <div class="section-name">Harvesting and storing data</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec25" class="chapter-section">
                                                                    <a href="#ch03lvl1sec25">                    
                                                                        <div class="section-name">Exploring data using Blaze</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec26" class="chapter-section">
                                                                    <a href="#ch03lvl1sec26">                    
                                                                        <div class="section-name">Exploring data using Spark SQL</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec27" class="chapter-section">
                                                                    <a href="#ch03lvl1sec27">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="4">
                                                        <div class="section-name">4: Learning from Data Using Spark</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="4" class="chapter-section">
                                                                    <a href="#ch04">        
                                                                        <div class="section-name">Chapter 4: Learning from Data Using Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec28" class="chapter-section">
                                                                    <a href="#ch04lvl1sec28">                    
                                                                        <div class="section-name">Contextualizing Spark MLlib in the app architecture</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec29" class="chapter-section">
                                                                    <a href="#ch04lvl1sec29">                    
                                                                        <div class="section-name">Classifying Spark MLlib algorithms</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec30" class="chapter-section">
                                                                    <a href="#ch04lvl1sec30">                    
                                                                        <div class="section-name">Spark MLlib data types</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec31" class="chapter-section">
                                                                    <a href="#ch04lvl1sec31">                    
                                                                        <div class="section-name">Machine learning workflows and data flows</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec32" class="chapter-section">
                                                                    <a href="#ch04lvl1sec32">                    
                                                                        <div class="section-name">Clustering the Twitter dataset</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec33" class="chapter-section">
                                                                    <a href="#ch04lvl1sec33">                    
                                                                        <div class="section-name">Building machine learning pipelines</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec34" class="chapter-section">
                                                                    <a href="#ch04lvl1sec34">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="5">
                                                        <div class="section-name">5: Streaming Live Data with Spark</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="5" class="chapter-section">
                                                                    <a href="#ch05">        
                                                                        <div class="section-name">Chapter 5: Streaming Live Data with Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec35" class="chapter-section">
                                                                    <a href="#ch05lvl1sec35">                    
                                                                        <div class="section-name">Laying the foundations of streaming architecture</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec36" class="chapter-section">
                                                                    <a href="#ch05lvl1sec36">                    
                                                                        <div class="section-name">Processing live data with TCP sockets</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec37" class="chapter-section">
                                                                    <a href="#ch05lvl1sec37">                    
                                                                        <div class="section-name">Manipulating Twitter data in real time</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec38" class="chapter-section">
                                                                    <a href="#ch05lvl1sec38">                    
                                                                        <div class="section-name">Building a reliable and scalable streaming app</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec39" class="chapter-section">
                                                                    <a href="#ch05lvl1sec39">                    
                                                                        <div class="section-name">Closing remarks on the Lambda and Kappa architecture</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec40" class="chapter-section">
                                                                    <a href="#ch05lvl1sec40">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="6">
                                                        <div class="section-name">6: Visualizing Insights and Trends</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="6" class="chapter-section">
                                                                    <a href="#ch06">        
                                                                        <div class="section-name">Chapter 6: Visualizing Insights and Trends</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec41" class="chapter-section">
                                                                    <a href="#ch06lvl1sec41">                    
                                                                        <div class="section-name">Revisiting the data-intensive apps architecture</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec42" class="chapter-section">
                                                                    <a href="#ch06lvl1sec42">                    
                                                                        <div class="section-name">Preprocessing the data for visualization</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec43" class="chapter-section">
                                                                    <a href="#ch06lvl1sec43">                    
                                                                        <div class="section-name">Gauging words, moods, and memes at a glance</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec44" class="chapter-section">
                                                                    <a href="#ch06lvl1sec44">                    
                                                                        <div class="section-name">Geo-locating tweets and mapping meetups</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec45" class="chapter-section">
                                                                    <a href="#ch06lvl1sec45">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="backindex">
                                                        <div class="section-name">Appendix A: Index</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="backindex" class="chapter-section">
                                                                    <a href="#backindex">
                                                                        <div class="section-name">Chapter Appendix A: Index</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                            </ul>
                                        </div>
                                        
                                        <div role="tabpanel" class="tab-pane fade" id="author">
                                            <div class="visible-xs">
                                                <h4 class="mobile-title">About the Author</h4>
                                                <hr>
                                            </div>
                                            <p><strong>Amit Nandi</strong></p>
                                            <div>
                                                <p>Amit Nandi studied physics at the Free University of Brussels in Belgium, where he did his research on computer generated holograms. Computer generated holograms are the key components of an optical computer, which is powered by photons running at the speed of light. He then worked with the university Cray supercomputer, sending batch jobs of programs written in Fortran. This gave him a taste for computing, which kept growing. He has worked extensively on large business reengineering initiatives, using SAP as the main enabler. He focused for the last 15 years on start-ups in the data space, pioneering new areas of the information technology landscape. He is currently focusing on large-scale data-intensive applications as an enterprise architect, data engineer, and software developer. He understands and speaks seven human languages. Although Python is his computer language of choice, he aims to be able to write fluently in seven computer languages too.</p>
                                            </div>
                                        </div>
                                        
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    
                    <div class="next-wrapper">
                        <div class="row ns">
                            <hr />
                            <span class="hidden-xs">
                                <h4 class="pull-left">
                                    <strong>Up Next: </strong><span class="section-title"></span>
                                </h4>
                                <a href="#" class="btn btn-primary pull-right btn-lg">
                                    Next Section
                                </a>
                            </span>
                            <span class="visible-xs">
                                <a href="#" class="btn btn-primary btn-block btn-lg">
                                    Next Section
                                </a>
                            </span>
                        </div>
                        <div class="row ns">
                            <hr>
                        </div>
                    </div>
                </div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch01"></a>Chapter 1. Setting Up a Spark Virtual Environment</h2></div></div></div><p>In this chapter, we will build an isolated virtual environment for development purposes. The environment will be powered by Spark and the PyData libraries provided by the Python Anaconda distribution. These libraries include Pandas, Scikit-Learn, Blaze, Matplotlib, Seaborn, and Bokeh. We will perform the following activities:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Setting up the development environment using the Anaconda Python distribution. This will include enabling the IPython Notebook environment powered by PySpark for our data exploration tasks.</p></li><li style="list-style-type: disc"><p>Installing and enabling Spark, and the PyData libraries such as Pandas, Scikit- Learn, Blaze, Matplotlib, and Bokeh.</p></li><li style="list-style-type: disc"><p>Building a <code class="literal">word count</code> example app to ensure that everything is working fine.</p></li></ul></div><p>The last decade has seen the rise and dominance of data-driven behemoths such as Amazon, Google, Twitter, LinkedIn, and Facebook. These corporations, by seeding, sharing, or disclosing their infrastructure concepts, software practices, and data processing frameworks, have fostered a vibrant open source software community. This has transformed the enterprise technology, systems, and software architecture.</p><p>This includes new infrastructure and DevOps (short for development and operations), concepts leveraging virtualization, cloud technology, and software-defined networks.</p><p>To process petabytes of data, Hadoop was developed and open sourced, taking its inspiration <a id="id0" class="indexterm"></a>from the <span class="strong"><strong>Google File System</strong></span> (<span class="strong"><strong>GFS</strong></span>) and the adjoining distributed computing framework, MapReduce. Overcoming the complexities of scaling while keeping costs under control has also led to a proliferation of new data stores. Examples of recent database technology include Cassandra, a columnar database; MongoDB, a document database; and Neo4J, a graph database.</p><p>Hadoop, thanks to its ability to process huge datasets, has fostered a vast ecosystem to query data more iteratively and interactively with Pig, Hive, Impala, and Tez. Hadoop is cumbersome as it operates only in batch mode using MapReduce. Spark is creating a revolution in the analytics and data processing realm by targeting the shortcomings of disk input-output and bandwidth-intensive MapReduce jobs.</p><p>Spark is written in Scala, and therefore integrates natively with the <span class="strong"><strong>Java Virtual Machine</strong></span> (<span class="strong"><strong>JVM</strong></span>) powered ecosystem. Spark had early on provided Python API and bindings <a id="id1" class="indexterm"></a>by enabling PySpark. The Spark architecture and ecosystem is inherently polyglot, with an obvious strong presence of Java-led systems.</p><p>This book will focus on PySpark and the PyData ecosystem. Python is one of the preferred languages in the academic and scientific community for data-intensive processing. Python has developed a rich ecosystem of libraries and tools in data manipulation with Pandas and Blaze, in Machine Learning with Scikit-Learn, and in data visualization with Matplotlib, Seaborn, and Bokeh. Hence, the aim of this book is to build an end-to-end architecture for data-intensive applications powered by Spark and Python. In order to put these concepts in to practice, we will analyze social networks such as Twitter, GitHub, and Meetup. We will focus on the activities and social interactions of Spark and the Open Source Software community by tapping into GitHub, Twitter, and Meetup.</p><p>Building data-intensive applications requires highly scalable infrastructure, polyglot storage, seamless data integration, multiparadigm analytics processing, and efficient visualization. The following paragraph describes the data-intensive app architecture blueprint that we will adopt throughout the book. It is the backbone of the book. We will discover Spark in the context of the broader PyData ecosystem.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip02"></a>Tip</h3><p>
<span class="strong"><strong>Downloading the example code</strong></span>
</p><p>You can download the example code files for all Packt books you have purchased from your account at <a class="ulink" href="http://www.packtpub.com" target="_blank">http://www.packtpub.com</a>. If you purchased this book elsewhere, you can visit <a class="ulink" href="http://www.packtpub.com/support" target="_blank">http://www.packtpub.com/support</a> and register to have the files e-mailed directly to you.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec08"></a>Understanding the architecture of data-intensive applications</h2></div></div><hr /></div><p>In <a id="id2" class="indexterm"></a>order to understand the architecture of data-intensive applications, the following conceptual framework is used. The is architecture is designed on the following five layers:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Infrastructure layer</p></li><li style="list-style-type: disc"><p>Persistence layer</p></li><li style="list-style-type: disc"><p>Integration layer</p></li><li style="list-style-type: disc"><p>Analytics layer</p></li><li style="list-style-type: disc"><p>Engagement layer</p></li></ul></div><p>The following screenshot depicts the five layers of the <span class="strong"><strong>Data Intensive App Framework</strong></span>:</p><div class="mediaobject"><img src="graphics/B03968_01_01.jpg" /></div><p>From<a id="id3" class="indexterm"></a> the bottom up, let's go through the layers and their main purpose.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec07"></a>Infrastructure layer</h3></div></div></div><p>The <a id="id4" class="indexterm"></a>infrastructure layer is primarily <a id="id5" class="indexterm"></a>concerned with virtualization, scalability, and continuous integration. In practical terms, and in terms of virtualization, we will go through building our own development environment in a VirtualBox and virtual machine powered by Spark and the Anaconda distribution of Python. If we wish to scale from there, we can create a similar environment in the cloud. The practice of creating a segregated development environment and moving into test and production deployment can be<a id="id6" class="indexterm"></a> automated <a id="id7" class="indexterm"></a>and can be part of a continuous integration<a id="id8" class="indexterm"></a> cycle <a id="id9" class="indexterm"></a>powered by DevOps tools such as <span class="strong"><strong>Vagrant</strong></span>, <span class="strong"><strong>Chef</strong></span>, <span class="strong"><strong>Puppet</strong></span>, and <span class="strong"><strong>Docker</strong></span>. Docker is a very popular open source project that eases the installation and deployment of new environments. The book will be limited to building the virtual machine using VirtualBox. From a data-intensive app architecture point of view, we are describing the essential steps of the infrastructure layer by mentioning scalability and continuous integration beyond just virtualization.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec08"></a>Persistence layer</h3></div></div></div><p>The persistence layer manages the various repositories in accordance with data needs and shapes. It ensures the set up and management of the polyglot data stores. It includes relational <a id="id10" class="indexterm"></a>database management <a id="id11" class="indexterm"></a>systems such as <span class="strong"><strong>MySQL</strong></span> and <span class="strong"><strong>PostgreSQL</strong></span>; key-value data<a id="id12" class="indexterm"></a> stores such as <span class="strong"><strong>Hadoop</strong></span>, <span class="strong"><strong>Riak</strong></span>, and <span class="strong"><strong>Redis</strong></span>; columnar databases<a id="id13" class="indexterm"></a> such as <span class="strong"><strong>HBase</strong></span> and <span class="strong"><strong>Cassandra</strong></span>; document databases <a id="id14" class="indexterm"></a>such as <span class="strong"><strong>MongoDB</strong></span> and <span class="strong"><strong>Couchbase</strong></span>; and graph <a id="id15" class="indexterm"></a>databases<a id="id16" class="indexterm"></a> such as <span class="strong"><strong>Neo4j</strong></span>. The persistence<a id="id17" class="indexterm"></a> layer manages various filesystems such as Hadoop's HDFS. It interacts with various storage systems from native hard drives to Amazon S3. It manages various file storage formats such as <code class="literal">csv</code>, <code class="literal">json</code>, and <code class="literal">parquet</code>, which is a column-oriented format.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec09"></a>Integration layer</h3></div></div></div><p>The<a id="id18" class="indexterm"></a> integration layer focuses on<a id="id19" class="indexterm"></a> data acquisition, transformation, quality, persistence, consumption, and governance. It is essentially driven by the following five Cs: <span class="emphasis"><em>connect</em></span>, <span class="emphasis"><em>collect</em></span>, <span class="emphasis"><em>correct</em></span>, <span class="emphasis"><em>compose</em></span>, and <span class="emphasis"><em>consume</em></span>.</p><p>The five steps describe the lifecycle of data. They are focused on how to acquire the dataset of interest, explore it, iteratively refine and enrich the collected information, and get it ready for consumption. So, the steps perform the following operations:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Connect</strong></span>: Targets the best way to acquire data from the various data sources, APIs<a id="id20" class="indexterm"></a> offered by these sources, the input format, input schemas if they exist, the rate of data collection, and limitations from providers</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Correct</strong></span>: Focuses<a id="id21" class="indexterm"></a> on transforming data for further processing and also ensures that the quality and consistency of the data received are maintained</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Collect</strong></span>: Looks <a id="id22" class="indexterm"></a>at which data to store where and in what format, to ease data composition and consumption at later stages</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Compose</strong></span>: Concentrates<a id="id23" class="indexterm"></a> its attention on how to mash up the various data sets collected, and enrich the information in order to build a compelling data-driven product</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Consume</strong></span>: Takes <a id="id24" class="indexterm"></a>care of data provisioning and rendering and how the right data reaches the right individual at the right time</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Control</strong></span>: This sixth <span class="emphasis"><em>additional</em></span> step will sooner or later be required as the data, the<a id="id25" class="indexterm"></a> organization, and the participants grow and it is about ensuring data governance</p></li></ul></div><p>The following diagram depicts the iterative process of data acquisition and refinement for consumption:</p><div class="mediaobject"><img src="graphics/B03968_01_02.jpg" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec10"></a>Analytics layer</h3></div></div></div><p>The analytics<a id="id26" class="indexterm"></a> layer is where <a id="id27" class="indexterm"></a>Spark processes data with the various models, algorithms, and machine learning pipelines in order to derive insights. For our purpose, in this book, the analytics layer is powered by Spark. We will delve deeper in subsequent chapters into the merits of Spark. In a nutshell, what makes it so powerful is that it allows multiple paradigms of analytics processing in a single unified platform. It allows batch, streaming, and interactive analytics. Batch processing on large datasets with longer latency periods allows us to extract patterns and insights that can feed into real-time events in streaming mode. Interactive and iterative analytics are more suited for data exploration. Spark offers bindings and APIs in Python and R. With its <span class="strong"><strong>SparkSQL</strong></span> module and <a id="id28" class="indexterm"></a>the Spark Dataframe, it offers a very familiar analytics interface.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec11"></a>Engagement layer</h3></div></div></div><p>The <a id="id29" class="indexterm"></a>engagement layer interacts with <a id="id30" class="indexterm"></a>the end user and provides dashboards, interactive visualizations, and alerts. We will focus here on the tools provided by the PyData ecosystem such as Matplotlib, Seaborn, and Bokeh.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec09"></a>Understanding Spark</h2></div></div><hr /></div><p>Hadoop <a id="id31" class="indexterm"></a>scales horizontally as the data grows. Hadoop runs on commodity hardware, so it is cost-effective. Intensive data applications are enabled by scalable, distributed processing frameworks that allow organizations to analyze petabytes of data on large commodity clusters. Hadoop is the first open source implementation of map-reduce. Hadoop<a id="id32" class="indexterm"></a> relies on a distributed framework for storage called <span class="strong"><strong>HDFS</strong></span> (<span class="strong"><strong>Hadoop Distributed File System</strong></span>). Hadoop runs map-reduce tasks in batch jobs. Hadoop requires persisting the data to disk at each map, shuffle, and reduce process step. The overhead and the latency of such batch jobs adversely impact the performance.</p><p>Spark is a fast, distributed general analytics computing engine for large-scale data processing. The major breakthrough from Hadoop is that Spark allows data sharing between processing steps through in-memory processing of data pipelines.</p><p>Spark is unique in that it allows four different styles of data analysis and processing. Spark can be used in:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Batch</strong></span>: This<a id="id33" class="indexterm"></a> mode is used for manipulating large datasets, typically performing large map-reduce jobs</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Streaming</strong></span>: This<a id="id34" class="indexterm"></a> mode is used to process incoming information in near real time</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Iterative</strong></span>: This <a id="id35" class="indexterm"></a>mode is for machine learning algorithms such as a gradient descent where the data is accessed repetitively in order to reach convergence</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Interactive</strong></span>: This<a id="id36" class="indexterm"></a> mode is used for data exploration as large chunks of data are in memory and due to the very quick response time of Spark</p></li></ul></div><p>The following figure highlights the preceding four processing styles:</p><div class="mediaobject"><img src="graphics/B03968_01_03.jpg" /></div><p>Spark operates in three modes: one single mode, standalone on a single machine and two distributed modes on a cluster of machines—on Yarn, the Hadoop distributed resource manager, or on Mesos, the open source cluster manager developed at Berkeley concurrently with Spark:</p><div class="mediaobject"><img src="graphics/B03968_01_04.jpg" /></div><p>Spark offers a polyglot interface in Scala, Java, Python, and R.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec12"></a>Spark libraries</h3></div></div></div><p>Spark<a id="id37" class="indexterm"></a> comes with batteries included, with some powerful libraries:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>SparkSQL</strong></span>: This <a id="id38" class="indexterm"></a>provides the SQL-like ability to interrogate structured data and interactively explore large datasets</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>SparkMLLIB</strong></span>: This<a id="id39" class="indexterm"></a> provides major algorithms and a pipeline framework for machine learning</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Spark Streaming</strong></span>: This<a id="id40" class="indexterm"></a> is for near real-time analysis of data using micro batches and sliding widows on incoming streams of data</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Spark GraphX</strong></span>: This<a id="id41" class="indexterm"></a> is for graph processing and computation on complex connected entities and relationships</p></li></ul></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec01"></a>PySpark in action</h4></div></div></div><p>Spark is<a id="id42" class="indexterm"></a> written in Scala. The whole Spark ecosystem naturally leverages the JVM environment and capitalizes on HDFS natively. Hadoop HDFS is one of the many data stores supported by Spark. Spark is agnostic and from the beginning interacted with multiple data sources, types, and formats.</p><p>PySpark is not a transcribed version of Spark on a Java-enabled dialect of Python such as Jython. PySpark provides integrated API bindings around Spark and enables full usage of the Python ecosystem within all the nodes of the cluster with the pickle Python serialization and, more importantly, supplies access to the rich ecosystem of Python's machine learning libraries such as Scikit-Learn or data processing such as Pandas.</p><p>When we initialize a Spark program, the first thing a Spark program must do is to create a <code class="literal">SparkContext</code> object. It tells Spark how to access the cluster. The Python program creates a <code class="literal">PySparkContext</code>. Py4J is the gateway that binds the Python program to the Spark JVM <code class="literal">SparkContext</code>. The JVM <code class="literal">SparkContextserializes</code> the application codes and the closures and sends them to the cluster for execution. The cluster manager allocates resources and schedules, and ships the closures to the Spark workers in the cluster who activate Python virtual machines as required. In each machine, the Spark Worker is managed by an executor that controls computation, storage, and cache.</p><p>Here's an<a id="id43" class="indexterm"></a> example of how the Spark driver manages both the PySpark context and the Spark context with its local filesystems and its interactions with the Spark worker through the cluster manager:</p><div class="mediaobject"><img src="graphics/B03968_01_05.jpg" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec02"></a>The Resilient Distributed Dataset</h4></div></div></div><p>Spark <a id="id44" class="indexterm"></a>applications consist of a driver <a id="id45" class="indexterm"></a>program that runs the user's main function, creates distributed datasets on the cluster, and executes various parallel operations (transformations and actions) on those datasets.</p><p>Spark applications are run as an independent set of processes, coordinated by a <code class="literal">SparkContext</code> in a driver program.</p><p>The <code class="literal">SparkContext</code> will <a id="id46" class="indexterm"></a>be allocated system resources (machines, memory, CPU) from the <span class="strong"><strong>Cluster manager</strong></span>.</p><p>The <code class="literal">SparkContext</code> manages executors who manage workers in the cluster. The driver program has Spark jobs that need to run. The jobs are split into tasks submitted to the executor for completion. The executor takes care of computation, storage, and caching in each machine.</p><p>The key building block in Spark is the <span class="strong"><strong>RDD</strong></span> (<span class="strong"><strong>Resilient Distributed Dataset</strong></span>). A dataset is a collection of elements. Distributed means the dataset can be on any node in the cluster. Resilient means that the dataset could get lost or partially lost without major harm to the computation in progress as Spark will re-compute from the data lineage in memory, also known as the <span class="strong"><strong>DAG</strong></span> (short for <span class="strong"><strong>Directed Acyclic Graph</strong></span>) of operations. Basically, Spark will snapshot in memory a state of the RDD in the cache. If one of the computing machines crashes during operation, Spark rebuilds the RDDs from the cached RDD <a id="id47" class="indexterm"></a>and<a id="id48" class="indexterm"></a> the DAG of operations. RDDs<a id="id49" class="indexterm"></a> recover from node failure.</p><p>There are two types of operation on RDDs:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Transformations</strong></span>: A transformation takes an existing RDD and leads to a pointer <a id="id50" class="indexterm"></a>of a new transformed RDD. An RDD is immutable. Once created, it cannot be changed. Each transformation creates a new RDD. Transformations are lazily evaluated. Transformations are executed only when an action occurs. In the case of failure, the data lineage of transformations rebuilds the RDD.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Actions</strong></span>: An action on an RDD triggers a Spark job and yields a value. An action <a id="id51" class="indexterm"></a>operation causes Spark to execute the (lazy) transformation operations that are required to compute the RDD returned by the action. The action results in a DAG of operations. The DAG is compiled into stages where each stage is executed as a series of tasks. A task is a fundamental unit of work.</p></li></ul></div><p>Here's some useful information on RDDs:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>RDDs are created from a data source such as an HDFS file or a DB query. There are three ways to create an RDD:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Reading from a datastore</p></li><li style="list-style-type: disc"><p>Transforming an existing RDD</p></li><li style="list-style-type: disc"><p>Using an in-memory collection</p></li></ul></div></li><li style="list-style-type: disc"><p>RDDs are transformed with functions such as <code class="literal">map</code> or <code class="literal">filter</code>, which yield new RDDs.</p></li><li style="list-style-type: disc"><p>An action such as first, take, collect, or count on an RDD will deliver the results into the Spark driver. The Spark driver is the client through which the user interacts with the Spark cluster.</p></li></ul></div><p>The following diagram illustrates the RDD transformation and action:</p><p> </p><div class="mediaobject"><img src="graphics/B03968_01_06.jpg" /></div><p>
</p></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec10"></a>Understanding Anaconda</h2></div></div><hr /></div><p>Anaconda is <a id="id52" class="indexterm"></a>a widely used free Python distribution maintained<a id="id53" class="indexterm"></a> by <span class="strong"><strong>Continuum </strong></span>(<a class="ulink" href="https://www.continuum.io/" target="_blank">https://www.continuum.io/</a>). We will use the prevailing software stack provided by Anaconda to generate our apps. In this book, we will use PySpark and the PyData ecosystem. The PyData ecosystem is promoted, supported, and maintained by <span class="strong"><strong>Continuum</strong></span> and powered by the <span class="strong"><strong>Anaconda</strong></span> Python distribution. The Anaconda Python distribution essentially saves time and aggravation in the installation of the Python environment; we will use it in conjunction with Spark. Anaconda has its own package management that supplements the traditional <code class="literal">pip</code> <code class="literal">install</code> and <code class="literal">easy-install</code>. Anaconda comes with batteries included, namely some of the most important packages such as Pandas, Scikit-Learn, Blaze, Matplotlib, and Bokeh. An upgrade to any of the installed library is a simple command at the console:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ conda update</strong></span>
</pre></div><p>A list of installed libraries in our environment can be obtained with command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ conda list</strong></span>
</pre></div><p>The key components of the stack are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Anaconda</strong></span>: This<a id="id54" class="indexterm"></a> is a free Python distribution with almost 200 Python packages for science, math, engineering, and data analysis.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Conda</strong></span>: This is a package manager that takes care of all the dependencies of installing a<a id="id55" class="indexterm"></a> complex software stack. This is not restricted to <a id="id56" class="indexterm"></a>Python and manages the install process for R and other languages.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Numba</strong></span>: This <a id="id57" class="indexterm"></a>provides the power to speed up code in Python with high-performance functions and just-in-time compilation.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Blaze</strong></span>: This<a id="id58" class="indexterm"></a> enables large scale data analytics by offering a uniform and adaptable interface to access a variety of data providers, which include streaming Python, Pandas, SQLAlchemy, and Spark.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Bokeh</strong></span>: This <a id="id59" class="indexterm"></a>provides interactive data visualizations for large and streaming datasets.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Wakari</strong></span>: This<a id="id60" class="indexterm"></a> allows us to share and deploy IPython Notebooks and other apps on a hosted environment.</p></li></ul></div><p>The following figure shows the components of the Anaconda stack:</p><div class="mediaobject"><img src="graphics/B03968_01_07.jpg" /></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec11"></a>Setting up the Spark powered environment</h2></div></div><hr /></div><p>In this<a id="id61" class="indexterm"></a> section, we will learn to set up Spark:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Create a segregated development environment in a virtual machine running on Ubuntu 14.04, so it does not interfere with any existing system.</p></li><li style="list-style-type: disc"><p>Install Spark 1.3.0 with its dependencies, namely.</p></li><li style="list-style-type: disc"><p>Install the Anaconda Python 2.7 environment with all the required libraries such as Pandas, Scikit-Learn, Blaze, and Bokeh, and enable PySpark, so it can be accessed through IPython Notebooks.</p></li><li style="list-style-type: disc"><p>Set up the backend or data stores of our environment. We will use MySQL as the relational database, MongoDB as the document store, and Cassandra as the columnar database.</p></li></ul></div><p>Each storage backend serves a specific purpose depending on the nature of the data to be handled. The MySQL RDBMs is used for standard tabular processed information that can be easily queried using SQL. As we will be processing a lot of JSON-type data from various APIs, the easiest way to store them is in a document. For real-time and time-series-related information, Cassandra is best suited as a columnar database.</p><p>The following diagram gives a view of the environment we will build and use throughout the book:</p><div class="mediaobject"><img src="graphics/B03968_01_08.jpg" /></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec13"></a>Setting up an Oracle VirtualBox with Ubuntu</h3></div></div></div><p>Setting up a clean new VirtualBox <a id="id62" class="indexterm"></a>environment on Ubuntu 14.04 is the safest way to create a development environment that does not conflict with existing libraries and can be later replicated in the cloud using a similar list of commands.</p><p>In order to set up an environment with Anaconda and Spark, we will create a VirtualBox virtual machine running Ubuntu 14.04.</p><p>Let's go through the steps of using VirtualBox with Ubuntu:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Oracle<a id="id63" class="indexterm"></a> VirtualBox VM is free and can be downloaded from <a class="ulink" href="https://www.virtualbox.org/wiki/Downloads" target="_blank">https://www.virtualbox.org/wiki/Downloads</a>. The installation is pretty straightforward.</p></li><li><p>After installing VirtualBox, let's open the Oracle VM VirtualBox Manager and click the <span class="strong"><strong>New</strong></span> button.</p></li><li><p>We'll give the new VM a name, and select Type <span class="strong"><strong>Linux</strong></span> and Version <span class="strong"><strong>Ubuntu (64 bit)</strong></span>.</p></li><li><p>You need to download the ISO from the Ubuntu website and allocate sufficient RAM (4 GB recommended) and disk space (20 GB recommended). We will <a id="id64" class="indexterm"></a>use the Ubuntu 14.04.1 LTS release, which is found here: <a class="ulink" href="http://www.ubuntu.com/download/desktop" target="_blank">http://www.ubuntu.com/download/desktop</a>.</p></li><li><p>Once the installation <a id="id65" class="indexterm"></a>completed, it is advisable to install the VirtualBox Guest Additions by going to (from the VirtualBox menu, with the new VM running) <span class="strong"><strong>Devices</strong></span> | <span class="strong"><strong>Insert Guest Additions CD image</strong></span>. Failing to provide the guest additions in a Windows host gives a very limited user interface with reduced window sizes.</p></li><li><p>Once the additional installation completes, reboot the VM, and it will be ready to use. It is helpful to enable the shared clipboard by selecting the VM and clicking <span class="strong"><strong>Settings</strong></span>, then go to <span class="strong"><strong>General</strong></span> | <span class="strong"><strong>Advanced</strong></span> | <span class="strong"><strong>Shared Clipboard</strong></span> and click on <span class="strong"><strong>Bidirectional</strong></span>.</p></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec14"></a>Installing Anaconda with Python 2.7</h3></div></div></div><p>PySpark currently runs only <a id="id66" class="indexterm"></a>on Python 2.7. (There are requests from the community to upgrade to Python 3.3.) To install Anaconda, follow these steps:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Download the Anaconda Installer for Linux 64-bit Python 2.7 from <a class="ulink" href="http://continuum.io/downloads#all" target="_blank">http://continuum.io/downloads#all</a>.</p></li><li><p>After <a id="id67" class="indexterm"></a>downloading the Anaconda installer, open a terminal and navigate to the directory or folder where the installer has been saved. From here, run the following command, replacing the <code class="literal">2.x.x</code> in the command with the version number of the downloaded installer file:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># install anaconda 2.x.x</strong></span>
<span class="strong"><strong>bash Anaconda-2.x.x-Linux-x86[_64].sh</strong></span>
</pre></div></li><li><p>After accepting the license terms, you will be asked to specify the install location (which <code class="literal">defaults to ~/anaconda</code>).</p></li><li><p>After the self-extraction is finished, you should add the anaconda binary directory to your PATH environment variable:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># add anaconda to PATH</strong></span>
<span class="strong"><strong>bash Anaconda-2.x.x-Linux-x86[_64].sh</strong></span>
</pre></div></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec15"></a>Installing Java 8</h3></div></div></div><p>Spark<a id="id68" class="indexterm"></a> runs on the JVM and requires<a id="id69" class="indexterm"></a> the Java <span class="strong"><strong>SDK</strong></span> (short for <span class="strong"><strong>Software Development Kit</strong></span>) and not <a id="id70" class="indexterm"></a>the <span class="strong"><strong>JRE</strong></span> (short for <span class="strong"><strong>Java Runtime Environment</strong></span>), as we will build apps with Spark. The recommended version is Java Version <a id="id71" class="indexterm"></a>7 or higher. Java 8 is the most suitable, as it includes many of the functional programming techniques available with Scala and Python.</p><p>To install Java 8, follow these steps:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Install Oracle Java 8 using the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># install oracle java 8</strong></span>
<span class="strong"><strong>$ sudo apt-get install software-properties-common</strong></span>
<span class="strong"><strong>$ sudo add-apt-repository ppa:webupd8team/java</strong></span>
<span class="strong"><strong>$ sudo apt-get update</strong></span>
<span class="strong"><strong>$ sudo apt-get install oracle-java8-installer</strong></span>
</pre></div></li><li><p>Set the <code class="literal">JAVA_HOME</code> environment variable and ensure that the Java program is on your PATH.</p></li><li><p>Check that <code class="literal">JAVA_HOME</code> is properly installed:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># </strong></span>
<span class="strong"><strong>$ echo JAVA_HOME</strong></span>
</pre></div></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec16"></a>Installing Spark</h3></div></div></div><p>Head <a id="id72" class="indexterm"></a>over to the Spark download page at <a class="ulink" href="http://spark.apache.org/downloads.html" target="_blank">http://spark.apache.org/downloads.html</a>.</p><p>The <a id="id73" class="indexterm"></a>Spark download page offers the possibility to download earlier versions of Spark and different package and download types. We will select the latest release, pre-built for Hadoop 2.6 and later. The easiest way to install Spark is to use a Spark package prebuilt for Hadoop 2.6 and later, rather than build it from source. Move the file to the directory <code class="literal">~/spark</code> under the root directory.</p><p>Download the latest release of Spark—Spark 1.5.2, released on November 9, 2015:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Select Spark release <span class="strong"><strong>1.5.2 (Nov 09 2015),</strong></span>
</p></li><li><p>Chose the package type <span class="strong"><strong>Prebuilt for Hadoop 2.6 and later</strong></span>,</p></li><li><p>Chose the download type <span class="strong"><strong>Direct Download</strong></span>,</p></li><li><p>Download Spark: <span class="strong"><strong>spark-1.5.2-bin-hadoop2.6.tgz</strong></span>,</p></li><li><p>Verify this release using the 1.3.0 signatures and checksums,</p></li></ol></div><p>This can also be accomplished by running:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># download spark</strong></span>
<span class="strong"><strong>$ wget http://d3kbcqa49mib13.cloudfront.net/spark-1.5.2-bin-hadoop2.6.tgz</strong></span>
</pre></div><p>Next, we'll extract the files and clean up:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># extract, clean up, move the unzipped files under the spark directory</strong></span>
<span class="strong"><strong>$ tar -xf spark-1.5.2-bin-hadoop2.6.tgz</strong></span>
<span class="strong"><strong>$ rm spark-1.5.2-bin-hadoop2.6.tgz</strong></span>
<span class="strong"><strong>$ sudo mv spark-* spark</strong></span>
</pre></div><p>Now, we<a id="id74" class="indexterm"></a> can run the Spark Python interpreter with:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># run spark</strong></span>
<span class="strong"><strong>$ cd ~/spark</strong></span>
<span class="strong"><strong>./bin/pyspark</strong></span>
</pre></div><p>You should see something like this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Welcome to</strong></span>
<span class="strong"><strong>      ____              __</strong></span>
<span class="strong"><strong>     / __/__  ___ _____/ /__</strong></span>
<span class="strong"><strong>    _\ \/ _ \/ _ `/ __/  '_/</strong></span>
<span class="strong"><strong>   /__ / .__/\_,_/_/ /_/\_\   version 1.5.2</strong></span>
<span class="strong"><strong>      /_/</strong></span>
<span class="strong"><strong>Using Python version 2.7.6 (default, Mar 22 2014 22:59:56)</strong></span>
<span class="strong"><strong>SparkContext available as sc.</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; </strong></span>
</pre></div><p>The interpreter will have already provided us with a Spark context object, <code class="literal">sc</code>, which we can see by running:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; print(sc)</strong></span>
<span class="strong"><strong>&lt;pyspark.context.SparkContext object at 0x7f34b61c4e50&gt;</strong></span>
</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec17"></a>Enabling IPython Notebook</h3></div></div></div><p>We will <a id="id75" class="indexterm"></a>work with IPython Notebook for a friendlier user experience than the console.</p><p>You can launch IPython Notebook by using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ IPYTHON_OPTS="notebook --pylab inline"  ./bin/pyspark</strong></span>
</pre></div><p>Launch PySpark with <code class="literal">IPYNB</code> in the directory <code class="literal">examples/AN_Spark</code> where Jupyter or IPython Notebooks are stored:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># cd to  /home/an/spark/spark-1.5.0-bin-hadoop2.6/examples/AN_Spark</strong></span>
<span class="strong"><strong># launch command using python 2.7 and the spark-csv package:</strong></span>
<span class="strong"><strong>$ IPYTHON_OPTS='notebook' /home/an/spark/spark-1.5.0-bin-hadoop2.6/bin/pyspark --packages com.databricks:spark-csv_2.11:1.2.0</strong></span>

<span class="strong"><strong># launch command using python 3.4 and the spark-csv package:</strong></span>
<span class="strong"><strong>$ IPYTHON_OPTS='notebook' PYSPARK_PYTHON=python3</strong></span>
<span class="strong"><strong> /home/an/spark/spark-1.5.0-bin-hadoop2.6/bin/pyspark --packages com.databricks:spark-csv_2.11:1.2.0</strong></span>
</pre></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec12"></a>Building our first app with PySpark</h2></div></div><hr /></div><p>We are <a id="id76" class="indexterm"></a>ready to check now that everything is working<a id="id77" class="indexterm"></a> fine. The obligatory word count will be put to the test in processing a word count on the first chapter of this book.</p><p>The code we will be running is listed here:</p><div class="informalexample"><pre class="programlisting"># Word count on 1st Chapter of the Book using PySpark

# import regex module
import re
# import add from operator module
from operator import add


# read input file
file_in = sc.textFile('/home/an/Documents/A00_Documents/Spark4Py 20150315')

# count lines
print('number of lines in file: %s' % file_in.count())

# add up lengths of each line
chars = file_in.map(lambda s: len(s)).reduce(add)
print('number of characters in file: %s' % chars)

# Get words from the input file
words =file_in.flatMap(lambda line: re.split('\W+', line.lower().strip()))
# words of more than 3 characters
words = words.filter(lambda x: len(x) &gt; 3)
# set count 1 per word
words = words.map(lambda w: (w,1))
# reduce phase - sum count all the words
words = words.reduceByKey(add)</pre></div><p>In this program, we are first reading the file from the directory <code class="literal">/home/an/Documents/A00_Documents/Spark4Py 20150315</code> into <code class="literal">file_in</code>.</p><p>We are then introspecting the file by counting the number of lines and the number of characters per line.</p><p>We are splitting the input file in to words and getting them in lower case. For our word count purpose, we are choosing words longer than three characters in order to avoid shorter and much more frequent words such as <span class="emphasis"><em>the</em></span>, <span class="emphasis"><em>and</em></span>, <span class="emphasis"><em>for</em></span> to skew the count in their favor. Generally, they <a id="id78" class="indexterm"></a>are considered stop words <a id="id79" class="indexterm"></a>and should be filtered out in any language processing task.</p><p>At this stage, we are getting ready for the MapReduce steps. To each word, we map a value of <code class="literal">1</code> and reduce it by summing all the unique words.</p><p>Here are illustrations of the code in the IPython Notebook. The first 10 cells are preprocessing the word count on the dataset, which is retrieved from the local file directory.</p><div class="mediaobject"><img src="graphics/B03968_01_09.jpg" /></div><p>Swap<a id="id80" class="indexterm"></a> the word count tuples in the format <code class="literal">(count, word)</code> in<a id="id81" class="indexterm"></a> order to sort by <code class="literal">count</code>, which is now the primary key of the tuple:</p><div class="informalexample"><pre class="programlisting"># create tuple (count, word) and sort in descending
words = words.map(lambda x: (x[1], x[0])).sortByKey(False)

# take top 20 words by frequency
words.take(20)</pre></div><p>In order<a id="id82" class="indexterm"></a> to display our result, we are creating the tuple <code class="literal">(count, word)</code> and <a id="id83" class="indexterm"></a>displaying the top 20 most frequently used words in descending order:</p><div class="mediaobject"><img src="graphics/B03968_01_10.jpg" /></div><p>Let's<a id="id84" class="indexterm"></a> create <a id="id85" class="indexterm"></a>a histogram function:</p><div class="informalexample"><pre class="programlisting"># create function for histogram of most frequent words

% matplotlib inline
import matplotlib.pyplot as plt
#

def histogram(words):
    count = map(lambda x: x[1], words)
    word = map(lambda x: x[0], words)
    plt.barh(range(len(count)), count,color = 'grey')
    plt.yticks(range(len(count)), word)

# Change order of tuple (word, count) from (count, word) 
words = words.map(lambda x:(x[1], x[0]))
words.take(25)

# display histogram
histogram(words.take(25))</pre></div><p>Here, we visualize the most frequent words by plotting them in a bar chart. We have to first swap the tuple from the original <code class="literal">(count, word)</code> to <code class="literal">(word, count)</code>:</p><div class="mediaobject"><img src="graphics/B03968_01_11.jpg" /></div><p>So here <a id="id86" class="indexterm"></a>you have it: the most frequent words used<a id="id87" class="indexterm"></a> in the first chapter are <span class="strong"><strong>Spark</strong></span>, followed by <span class="strong"><strong>Data</strong></span> and <span class="strong"><strong>Anaconda</strong></span>.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec13"></a>Virtualizing the environment with Vagrant</h2></div></div><hr /></div><p>In order <a id="id88" class="indexterm"></a>to create a portable Python and Spark <a id="id89" class="indexterm"></a>environment that can be easily shared and <a id="id90" class="indexterm"></a>cloned, the development environment can be built with a <code class="literal">vagrantfile</code>.</p><p>We will point to the <span class="strong"><strong>Massive Open Online Courses</strong></span> (<span class="strong"><strong>MOOCs</strong></span>) delivered by <span class="emphasis"><em>Berkeley University and Databricks</em></span>:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="emphasis"><em>Introduction to Big Data with Apache Spark, Professor Anthony D. Joseph</em></span> can be <a id="id91" class="indexterm"></a>found at <a class="ulink" href="https://www.edx.org/course/introduction-big-data-apache-spark-uc-berkeleyx-cs100-1x" target="_blank">https://www.edx.org/course/introduction-big-data-apache-spark-uc-berkeleyx-cs100-1x</a>
</p></li><li style="list-style-type: disc"><p>
<span class="emphasis"><em>Scalable Machine Learning, Professor</em></span> <span class="emphasis"><em>Ameet Talwalkar</em></span> can be found at <a class="ulink" href="https://www.edx.org/course/scalable-machine-learning-uc-berkeleyx-cs190-1x" target="_blank">https://www.edx.org/course/scalable-machine-learning-uc-berkeleyx-cs190-1x</a>
</p></li></ul></div><p>The course labs were executed on IPython Notebooks powered by PySpark. They can be found in the following GitHub repository: <a class="ulink" href="https://github.com/spark-mooc/mooc-setup/" target="_blank">https://github.com/spark-mooc/mooc-setup/</a>.</p><p>Once you<a id="id92" class="indexterm"></a> have set up Vagrant on your machine, follow these instructions to get started: <a class="ulink" href="https://docs.vagrantup.com/v2/getting-started/index.html" target="_blank">https://docs.vagrantup.com/v2/getting-started/index.html</a>.</p><p>Clone the <code class="literal">spark-mooc/mooc-setup/ github</code> repository in your work directory and launch the command <code class="literal">$ vagrant up</code>, within the cloned directory:</p><p>Be aware that the version of Spark may be outdated as the <code class="literal">vagrantfile</code> may not be up-to-date.</p><p>You will see an output similar to this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>C:\Programs\spark\edx1001\mooc-setup-master&gt;vagrant up</strong></span>
<span class="strong"><strong>Bringing machine 'sparkvm' up with 'virtualbox' provider...</strong></span>
<span class="strong"><strong>==&gt; sparkvm: Checking if box 'sparkmooc/base' is up to date...</strong></span>
<span class="strong"><strong>==&gt; sparkvm: Clearing any previously set forwarded ports...</strong></span>
<span class="strong"><strong>==&gt; sparkvm: Clearing any previously set network interfaces...</strong></span>
<span class="strong"><strong>==&gt; sparkvm: Preparing network interfaces based on configuration...</strong></span>
<span class="strong"><strong>    sparkvm: Adapter 1: nat</strong></span>
<span class="strong"><strong>==&gt; sparkvm: Forwarding ports...</strong></span>
<span class="strong"><strong>    sparkvm: 8001 =&gt; 8001 (adapter 1)</strong></span>
<span class="strong"><strong>    sparkvm: 4040 =&gt; 4040 (adapter 1)</strong></span>
<span class="strong"><strong>    sparkvm: 22 =&gt; 2222 (adapter 1)</strong></span>
<span class="strong"><strong>==&gt; sparkvm: Booting VM...</strong></span>
<span class="strong"><strong>==&gt; sparkvm: Waiting for machine to boot. This may take a few minutes...</strong></span>
<span class="strong"><strong>    sparkvm: SSH address: 127.0.0.1:2222</strong></span>
<span class="strong"><strong>    sparkvm: SSH username: vagrant</strong></span>
<span class="strong"><strong>    sparkvm: SSH auth method: private key</strong></span>
<span class="strong"><strong>    sparkvm: Warning: Connection timeout. Retrying...</strong></span>
<span class="strong"><strong>    sparkvm: Warning: Remote connection disconnect. Retrying...</strong></span>
<span class="strong"><strong>==&gt; sparkvm: Machine booted and ready!</strong></span>
<span class="strong"><strong>==&gt; sparkvm: Checking for guest additions in VM...</strong></span>
<span class="strong"><strong>==&gt; sparkvm: Setting hostname...</strong></span>
<span class="strong"><strong>==&gt; sparkvm: Mounting shared folders...</strong></span>
<span class="strong"><strong>    sparkvm: /vagrant =&gt; C:/Programs/spark/edx1001/mooc-setup-master</strong></span>
<span class="strong"><strong>==&gt; sparkvm: Machine already provisioned. Run `vagrant provision` or use the `--provision`</strong></span>
<span class="strong"><strong>==&gt; sparkvm: to force provisioning. Provisioners marked to run always will still run.</strong></span>

<span class="strong"><strong>C:\Programs\spark\edx1001\mooc-setup-master&gt;</strong></span>
</pre></div><p>This <a id="id93" class="indexterm"></a>will <a id="id94" class="indexterm"></a>launch the IPython Notebooks powered by PySpark on <code class="literal">localhost:8001</code>:</p><div class="mediaobject"><img src="graphics/B03968_01_12.jpg" /></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec14"></a>Moving to the cloud</h2></div></div><hr /></div><p>As we are dealing with distributed systems, an environment on a virtual machine running on a single laptop is limited for exploration and learning. We can move to the cloud in order to experience the power and scalability of the Spark distributed framework.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec18"></a>Deploying apps in Amazon Web Services</h3></div></div></div><p>Once<a id="id95" class="indexterm"></a> we <a id="id96" class="indexterm"></a>are ready to scale our apps, we can migrate our development environment to <span class="strong"><strong>Amazon</strong></span>
<a id="id97" class="indexterm"></a>
<span class="strong"><strong> Web Services</strong></span> (<span class="strong"><strong>AWS</strong></span>).</p><p>How to run<a id="id98" class="indexterm"></a> Spark on EC2 is clearly described in the following page: <a class="ulink" href="https://spark.apache.org/docs/latest/ec2-scripts.html" target="_blank">https://spark.apache.org/docs/latest/ec2-scripts.html</a>.</p><p>We<a id="id99" class="indexterm"></a> emphasize five key steps<a id="id100" class="indexterm"></a> in setting up the AWS Spark environment:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Create<a id="id101" class="indexterm"></a> an AWS EC2 key pair via the AWS console <code class="literal">http://aws.amazon.com/console/</code>.</p></li><li><p>Export your key pair to your environment:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>export AWS_ACCESS_KEY_ID=accesskeyid</strong></span>
<span class="strong"><strong>export AWS_SECRET_ACCESS_KEY=secretaccesskey</strong></span>
</pre></div></li><li><p>Launch your cluster:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>~$ cd $SPARK_HOME/ec2</strong></span>
<span class="strong"><strong>ec2$ ./spark-ec2 -k &lt;keypair&gt; -i &lt;key-file&gt; -s &lt;num-slaves&gt; launch &lt;cluster-name&gt;</strong></span>
</pre></div></li><li><p>SSH into a cluster to run Spark jobs:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>ec2$ ./spark-ec2 -k &lt;keypair&gt; -i &lt;key-file&gt; login &lt;cluster-name&gt;</strong></span>
</pre></div></li><li><p>Destroy your cluster after usage:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>ec2$ ./spark-ec2 destroy &lt;cluster-name&gt;</strong></span>
</pre></div></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec19"></a>Virtualizing the environment with Docker</h3></div></div></div><p>In order<a id="id102" class="indexterm"></a> to create a portable Python and Spark<a id="id103" class="indexterm"></a> environment that can be easily shared and cloned, the development environment can be built in Docker containers.</p><p>We wish capitalize on Docker's two main functions:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Creating isolated containers that can be easily deployed on different operating systems or in the cloud.</p></li><li style="list-style-type: disc"><p>Allowing easy sharing of the development environment image with all its dependencies using The DockerHub. The DockerHub is similar to GitHub. It allows easy cloning and version control. The snapshot image of the configured environment can be the baseline for further enhancements.</p></li></ul></div><p>The following diagram illustrates a Docker-enabled environment with Spark, Anaconda, and the database server and their respective data volumes.</p><div class="mediaobject"><img src="graphics/B03968_01_13.jpg" /></div><p>Docker <a id="id104" class="indexterm"></a>offers the ability to clone and deploy<a id="id105" class="indexterm"></a> an environment from the Dockerfile.</p><p>You can find<a id="id106" class="indexterm"></a> an example Dockerfile with a PySpark and Anaconda setup at the following address: <a class="ulink" href="https://hub.docker.com/r/thisgokeboysef/pyspark-docker/~/dockerfile/" target="_blank">https://hub.docker.com/r/thisgokeboysef/pyspark-docker/~/dockerfile/</a>.</p><p>Install Docker as per the instructions provided at the following links:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<a class="ulink" href="http://docs.docker.com/mac/started/" target="_blank">http://docs.docker.com/mac/started/</a> if you are on Mac OS X</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://docs.docker.com/linux/started/" target="_blank">http://docs.docker.com/linux/started/</a> if you are on Linux</p></li><li style="list-style-type: disc"><p>
<a class="ulink" href="http://docs.docker.com/windows/started/" target="_blank">http://docs.docker.com/windows/started/</a> if you are on Windows</p></li></ul></div><p>Install the docker container with the Dockerfile provided earlier with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ docker pull thisgokeboysef/pyspark-docker</strong></span>
</pre></div><p>Other great sources of information on how to <span class="emphasis"><em>dockerize</em></span> your environment can be seen at Lab41. The GitHub repository contains the necessary code:</p><p>
<a class="ulink" href="https://github.com/Lab41/ipython-spark-docker" target="_blank">https://github.com/Lab41/ipython-spark-docker</a>
</p><p>The supporting blog post is rich in information on thought <a id="id107" class="indexterm"></a>processes <a id="id108" class="indexterm"></a>involved in building the docker environment: <a class="ulink" href="http://lab41.github.io/blog/2015/04/13/ipython-on-spark-on-docker/" target="_blank">http://lab41.github.io/blog/2015/04/13/ipython-on-spark-on-docker/</a>.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec15"></a>Summary</h2></div></div><hr /></div><p>We set the context of building data-intensive apps by describing the overall architecture structured around the infrastructure, persistence, integration, analytics, and engagement layers. We also discussed Spark and Anaconda with their respective building blocks. We set up an environment in a VirtualBox with Anaconda and Spark and demonstrated a word count app using the text content of the first chapter as input.</p><p>In the next chapter, we will delve more deeply into the architecture blueprint for data-intensive apps and tap into the Twitter, GitHub, and Meetup APIs to get a feel of the data we will be mining with Spark.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch02"></a>Chapter 2. Building Batch and Streaming Apps with Spark</h2></div></div></div><p>The objective of the book is to teach you about PySpark and the PyData libraries by building an app that analyzes the Spark community's interactions on social networks. We will gather information on Apache Spark from GitHub, check the relevant tweets on Twitter, and get a feel for the buzz around Spark in the broader open source software communities<a id="id109" class="indexterm"></a> using <span class="strong"><strong>Meetup</strong></span>.</p><p>In this chapter, we will outline the various sources of data and information. We will get an understanding of their structure. We will outline the data processing pipeline, from collection to batch and streaming processing.</p><p>In this section, we will cover the following points:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Outline data processing pipelines from collection to batch and stream processing, effectively depicting the architecture of the app we are planning to build.</p></li><li style="list-style-type: disc"><p>Check out the various data sources (GitHub, Twitter, and Meetup), their data structure (JSON, structured information, unstructured text, geo-location, time series data, and so on), and their complexities. We also discuss the tools to connect to three different APIs, so you can build your own data mashups. The book will focus on Twitter in the following chapters.</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec16"></a>Architecting data-intensive apps</h2></div></div><hr /></div><p>We <a id="id110" class="indexterm"></a>defined the data-intensive app framework architecture blueprint in the previous chapter. Let's put back in context the various software components we are going to use throughout the book in our original framework. Here's an illustration of the various components of software mapped in the data-intensive architecture framework:</p><div class="mediaobject"><img src="graphics/B03986_02_01.jpg" /></div><p>Spark is an<a id="id111" class="indexterm"></a> extremely efficient, distributed computing framework. In order to exploit its full power, we need to architect our solution accordingly. For performance reasons, the overall solution needs to also be aware of its usage in terms of CPU, storage, and network.</p><p>These imperatives drive the architecture of our solution:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Latency</strong></span>: This architecture combines slow and fast processing. Slow processing<a id="id112" class="indexterm"></a> is done on historical data in batch mode. This is also called data at rest. This phase builds precomputed models and data patterns that will be used by the fast processing arm once live continuous data is fed into the system. Fast processing of data or real-time analysis of streaming data refers to data in motion. Data at rest is essentially processing data in batch mode with a longer latency. Data in motion refers to the streaming computation of data ingested in real time.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Scalability</strong></span>: Spark<a id="id113" class="indexterm"></a> is natively linearly scalable through its distributed in-memory computing framework. Databases and data stores interacting with Spark need to be also able to scale linearly as data volume grows.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Fault tolerance</strong></span>: When a failure occurs due to hardware, software, or network <a id="id114" class="indexterm"></a>reasons, the architecture should be resilient enough and provide availability at all times.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Flexibility</strong></span>: The data pipelines put in place in this architecture can be adapted and<a id="id115" class="indexterm"></a> retrofitted very quickly depending on the use case.</p></li></ul></div><p>Spark is <a id="id116" class="indexterm"></a>unique as it allows batch processing and streaming analytics on the same unified platform.</p><p>We will consider two data processing pipelines:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The first one handles data at rest and is focused on putting together the pipeline for batch analysis of the data</p></li><li style="list-style-type: disc"><p>The second one, data in motion, targets real-time data ingestion and delivering insights based on precomputed models and data patterns</p></li></ul></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec20"></a>Processing data at rest</h3></div></div></div><p>Let's get an<a id="id117" class="indexterm"></a> understanding of the data at rest or batch processing pipeline. The objective in this pipeline is to ingest the various datasets from Twitter, GitHub, and Meetup; prepare the data for Spark MLlib, the machine learning engine; and derive the base models that will be applied for insight generation in batch mode or in real time.</p><p>The following diagram illustrates the data pipeline in order to enable processing data at rest:</p><div class="mediaobject"><img src="graphics/B03986_02_02.jpg" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec21"></a>Processing data in motion</h3></div></div></div><p>Processing<a id="id118" class="indexterm"></a> data in motion introduces a new level of complexity, as we are introducing a new possibility of failure. If we want to scale, we need to consider bringing in distributed message queue systems such as Kafka. We will dedicate a subsequent chapter to understanding streaming analytics.</p><p>The following diagram depicts a data pipeline for processing data in motion:</p><div class="mediaobject"><img src="graphics/B03986_02_03.jpg" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec22"></a>Exploring data interactively</h3></div></div></div><p>Building a<a id="id119" class="indexterm"></a> data-intensive app is not as straightforward as exposing a database to a web interface. During the setup of both the data at rest and data in motion processing, we will capitalize on Spark's ability to analyse data interactively and refine the data richness and quality required for the machine learning and streaming activities. Here, we will go through an iterative cycle of data collection, refinement, and investigation in order to get to the dataset of interest for our apps.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec17"></a>Connecting to social networks</h2></div></div><hr /></div><p>Let's <a id="id120" class="indexterm"></a>delve into the first steps of the data-intensive app architecture's integration layer. We are going to focus on harvesting the data, ensuring its integrity and preparing for batch and streaming data processing by Spark at the next stage. This phase is described in the five process steps: <span class="emphasis"><em>connect</em></span>, <span class="emphasis"><em>correct</em></span>, <span class="emphasis"><em>collect</em></span>, <span class="emphasis"><em>compose</em></span>, and <span class="emphasis"><em>consume</em></span>. These are iterative steps of data exploration that will get us acquainted with the data and help us refine the data structure for further processing.</p><p>The following diagram depicts the iterative process of data acquisition and refinement for consumption:</p><div class="mediaobject"><img src="graphics/B03986_02_04.jpg" /></div><p>We connect<a id="id121" class="indexterm"></a> to the social networks of interest: Twitter, GitHub, and Meetup. We will discuss the mode of access to the <span class="strong"><strong>APIs</strong></span> (short for <span class="strong"><strong>Application Programming Interface</strong></span>) and how to create a RESTful connection with those <a id="id122" class="indexterm"></a>services while respecting the rate <a id="id123" class="indexterm"></a>limitation imposed by the social networks. <span class="strong"><strong>REST</strong></span> (short for <span class="strong"><strong>Representation State Transfer</strong></span>) is the most widely adopted architectural style on the Internet in order to enable scalable web services. It relies <a id="id124" class="indexterm"></a>on exchanging messages predominantly in <span class="strong"><strong>JSON</strong></span> (short for <span class="strong"><strong>JavaScript Object Notation</strong></span>). RESTful APIs and web services implement the four most prevalent verbs <code class="literal">GET</code>, <code class="literal">PUT</code>, <code class="literal">POST</code>, and <code class="literal">DELETE</code>. <code class="literal">GET</code> is used to retrieve an element or a collection from a given <code class="literal">URI</code>. <code class="literal">PUT</code> updates a collection with a new one. <code class="literal">POST</code> allows the creation of a new entry, while <code class="literal">DELETE</code> eliminates a collection.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec23"></a>Getting Twitter data</h3></div></div></div><p>Twitter<a id="id125" class="indexterm"></a> allows access to registered users to its search and streaming tweet services under an authorization protocol called OAuth that allows API applications to securely act on a user's behalf. In order to create the connection, the <a id="id126" class="indexterm"></a>first step is to create an application with Twitter at <a class="ulink" href="https://apps.twitter.com/app/new" target="_blank">https://apps.twitter.com/app/new</a>.</p><div class="mediaobject"><img src="graphics/B03986_02_05.jpg" /></div><p>Once the <a id="id127" class="indexterm"></a>application has been created, Twitter will issue the four codes that will allow it to tap into the Twitter hose:</p><div class="informalexample"><pre class="programlisting">CONSUMER_KEY = 'GetYourKey@Twitter'
CONSUMER_SECRET = ' GetYourKey@Twitter'
OAUTH_TOKEN = ' GetYourToken@Twitter'
OAUTH_TOKEN_SECRET = ' GetYourToken@Twitter'</pre></div><p>If you wish to get a feel for the various RESTful queries offered, you can explore the Twitter <a id="id128" class="indexterm"></a>API on the dev console at <a class="ulink" href="https://dev.twitter.com/rest/tools/console" target="_blank">https://dev.twitter.com/rest/tools/console</a>:</p><div class="mediaobject"><img src="graphics/B03986_02_06.jpg" /></div><p>We will <a id="id129" class="indexterm"></a>make a programmatic connection on Twitter using the following code, which will activate our OAuth access and allows us to tap into the Twitter API under the rate limitation. In the streaming mode, the limitation is for a GET request.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec24"></a>Getting GitHub data</h3></div></div></div><p>GitHub <a id="id130" class="indexterm"></a>uses a similar authentication process to Twitter. Head to the developer site and retrieve your credentials after duly registering <a id="id131" class="indexterm"></a>with GitHub at <a class="ulink" href="https://developer.github.com/v3/" target="_blank">https://developer.github.com/v3/</a>:</p><div class="mediaobject"><img src="graphics/B03986_02_07.jpg" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec25"></a>Getting Meetup data</h3></div></div></div><p>Meetup can<a id="id132" class="indexterm"></a> be accessed using the token issued in the developer resources to members of Meetup.com. The necessary token or OAuth credential <a id="id133" class="indexterm"></a>for Meetup API access can be obtained on their developer's website at <a class="ulink" href="https://secure.meetup.com/meetup_api" target="_blank">https://secure.meetup.com/meetup_api</a>:</p><div class="mediaobject"><img src="graphics/B03986_02_08.jpg" /></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec18"></a>Analyzing the data</h2></div></div><hr /></div><p>Let's get <a id="id134" class="indexterm"></a>a first feel for the data extracted from each of the social networks and get an understanding of the data structure from each these sources.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec26"></a>Discovering the anatomy of tweets</h3></div></div></div><p>In <a id="id135" class="indexterm"></a>this section, we are going to establish connection with the Twitter API. Twitter offers two connection modes: the REST API, which allows us to search historical tweets for a given search term or hashtag, and the streaming API, which delivers real-time tweets under the rate limit in place.</p><p>In order to get a better understanding of how to operate with the Twitter API, we will go through the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Install the Twitter Python library.</p></li><li><p>Establish a connection programmatically via OAuth, the authentication required for Twitter.</p></li><li><p>Search<a id="id136" class="indexterm"></a> for recent tweets for the query <span class="emphasis"><em>Apache Spark</em></span> and explore the results obtained.</p></li><li><p>Decide on the key attributes of interest and retrieve the information from the JSON output.</p></li></ol></div><p>Let's go through it step-by-step:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Install the Python Twitter library. In order to install it, you need to write <code class="literal">pip install twitter</code> from the command line:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ pip install twitter</strong></span>
</pre></div></li><li><p>Create the Python Twitter API class and its base methods for authentication, searching, and parsing the results. <code class="literal">self.auth</code> gets the credentials from Twitter. It then creates a registered API as <code class="literal">self.api</code>. We have implemented two methods: the first one to search Twitter with a given query and the second one to parse the output to retrieve relevant information such as the tweet ID, the tweet text, and the tweet author. The code is as follows:</p><div class="informalexample"><pre class="programlisting">import twitter
import urlparse
from pprint import pprint as pp

class TwitterAPI(object):
    """
    TwitterAPI class allows the Connection to Twitter via OAuth
    once you have registered with Twitter and receive the 
    necessary credentiials 
    """

# initialize and get the twitter credentials
     def __init__(self): 
        consumer_key = 'Provide your credentials'
        consumer_secret = 'Provide your credentials'
        access_token = 'Provide your credentials'
        access_secret = 'Provide your credentials'
     
        self.consumer_key = consumer_key
        self.consumer_secret = consumer_secret
        self.access_token = access_token
        self.access_secret = access_secret

#
# authenticate credentials with Twitter using OAuth
        self.auth = twitter.oauth.OAuth(access_token, access_secret, consumer_key, consumer_secret)
    # creates registered Twitter API
        self.api = twitter.Twitter(auth=self.auth)
#
# search Twitter with query q (i.e. "ApacheSpark") and max. result
    def searchTwitter(self, q, max_res=10,**kwargs):
        search_results = self.api.search.tweets(q=q, count=10, **kwargs)
        statuses = search_results['statuses']
        max_results = min(1000, max_res)

        for _ in range(10): 
            try:
                next_results = search_results['search_metadata']['next_results']
            except KeyError as e: 
                break

            next_results = urlparse.parse_qsl(next_results[1:])
            kwargs = dict(next_results)
            search_results = self.api.search.tweets(**kwargs)
            statuses += search_results['statuses']

            if len(statuses) &gt; max_results: 
                break
        return statuses
#
# parse tweets as it is collected to extract id, creation 
# date, user id, tweet text
    def parseTweets(self, statuses):
        return [ (status['id'], 
                  status['created_at'], 
                  status['user']['id'],
                  status['user']['name'], 
                  status['text'], url['expanded_url']) 
                        for status in statuses 
                            for url in status['entities']['urls'] ]</pre></div></li><li><p>Instantiate<a id="id137" class="indexterm"></a> the class with the required authentication:</p><div class="informalexample"><pre class="programlisting">t= TwitterAPI()</pre></div></li><li><p>Run a search on the query term <span class="emphasis"><em>Apache Spark</em></span>:</p><div class="informalexample"><pre class="programlisting">q="ApacheSpark"
tsearch = t.searchTwitter(q)</pre></div></li><li><p>Analyze<a id="id138" class="indexterm"></a> the JSON output:</p><div class="informalexample"><pre class="programlisting">pp(tsearch[1])

{u'contributors': None,
 u'coordinates': None,
 u'created_at': u'Sat Apr 25 14:50:57 +0000 2015',
 u'entities': {u'hashtags': [{u'indices': [74, 86], u'text': u'sparksummit'}],
               u'media': [{u'display_url': u'pic.twitter.com/WKUMRXxIWZ',
                           u'expanded_url': u'http://twitter.com/bigdata/status/591976255831969792/photo/1',
                           u'id': 591976255156715520,
                           u'id_str': u'591976255156715520',
                           u'indices': [143, 144],
                           u'media_url': 
...(snip)... 
 u'text': u'RT @bigdata: Enjoyed catching up with @ApacheSpark users &amp;amp; leaders at #sparksummit NYC: video clips are out http://t.co/qrqpP6cG9s http://t\u2026',
 u'truncated': False,
 u'user': {u'contributors_enabled': False,
           u'created_at': u'Sat Apr 04 14:44:31 +0000 2015',
           u'default_profile': True,
           u'default_profile_image': True,
           u'description': u'',
           u'entities': {u'description': {u'urls': []}},
           u'favourites_count': 0,
           u'follow_request_sent': False,
           u'followers_count': 586,
           u'following': False,
           u'friends_count': 2,
           u'geo_enabled': False,
           u'id': 3139047660,
           u'id_str': u'3139047660',
           u'is_translation_enabled': False,
           u'is_translator': False,
           u'lang': u'zh-cn',
           u'listed_count': 749,
           u'location': u'',
           u'name': u'Mega Data Mama',
           u'notifications': False,
           u'profile_background_color': u'C0DEED',
           u'profile_background_image_url': u'http://abs.twimg.com/images/themes/theme1/bg.png',
           u'profile_background_image_url_https': u'https://abs.twimg.com/images/themes/theme1/bg.png',
           ...(snip)... 
           u'screen_name': u'MegaDataMama',
           u'statuses_count': 26673,
           u'time_zone': None,
           u'url': None,
           u'utc_offset': None,
           u'verified': False}}</pre></div></li><li><p>Parse<a id="id139" class="indexterm"></a> the Twitter output to retrieve key information of interest:</p><div class="informalexample"><pre class="programlisting">tparsed = t.parseTweets(tsearch)
pp(tparsed)

[(591980327784046592,
  u'Sat Apr 25 15:01:23 +0000 2015',
  63407360,
  u'Jos\xe9 Carlos Baquero',
  u'Big Data systems are making a difference in the fight against cancer. #BigData #ApacheSpark http://t.co/pnOLmsKdL9',
  u'http://tmblr.co/ZqTggs1jHytN0'),
 (591977704464875520,
  u'Sat Apr 25 14:50:57 +0000 2015',
  3139047660,
  u'Mega Data Mama',
  u'RT @bigdata: Enjoyed catching up with @ApacheSpark users &amp;amp; leaders at #sparksummit NYC: video clips are out http://t.co/qrqpP6cG9s http://t\u2026',
  u'http://goo.gl/eF5xwK'),
 (591977172589539328,
  u'Sat Apr 25 14:48:51 +0000 2015',
  2997608763,
  u'Emma Clark',
  u'RT @bigdata: Enjoyed catching up with @ApacheSpark users &amp;amp; leaders at #sparksummit NYC: video clips are out http://t.co/qrqpP6cG9s http://t\u2026',
  u'http://goo.gl/eF5xwK'),
 ... (snip)...  
 (591879098349268992,
  u'Sat Apr 25 08:19:08 +0000 2015',
  331263208,
  u'Mario Molina',
  u'#ApacheSpark speeds up big data decision-making http://t.co/8hdEXreNfN',
  u'http://www.computerweekly.com/feature/Apache-Spark-speeds-up-big-data-decision-making')]</pre></div></li></ol></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec19"></a>Exploring the GitHub world</h2></div></div><hr /></div><p>In order<a id="id140" class="indexterm"></a> to get a better understanding on how to operate with the GitHub API, we will go through the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Install the GitHub Python library.</p></li><li><p>Access the API by using the token provided when we registered in the developer website.</p></li><li><p>Retrieve some key facts on the Apache foundation that is hosting the spark repository.</p></li></ol></div><p>Let's go through the process step-by-step:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Install the Python PyGithub library. In order to install it, you need to <code class="literal">pip install PyGithub</code> from the command line:</p><div class="informalexample"><pre class="programlisting">pip install PyGithub</pre></div></li><li><p>Programmatically create a client to instantiate the GitHub API:</p><div class="informalexample"><pre class="programlisting">from github import Github

# Get your own access token

ACCESS_TOKEN = 'Get_Your_Own_Access_Token'

# We are focusing our attention to User = apache and Repo = spark

USER = 'apache'
REPO = 'spark'

g = Github(ACCESS_TOKEN, per_page=100)
user = g.get_user(USER)
repo = user.get_repo(REPO)</pre></div></li><li><p>Retrieve key facts from the Apache User. There are 640 active Apache repositories in GitHub:</p><div class="informalexample"><pre class="programlisting">repos_apache = [repo.name for repo in g.get_user('apache').get_repos()]
len(repos_apache)
640</pre></div></li><li><p>Retrieve<a id="id141" class="indexterm"></a> key facts from the Spark repository, The programing languages used in the Spark repo are given here under:</p><div class="informalexample"><pre class="programlisting">pp(repo.get_languages())

{u'C': 1493,
 u'CSS': 4472,
 u'Groff': 5379,
 u'Java': 1054894,
 u'JavaScript': 21569,
 u'Makefile': 7771,
 u'Python': 1091048,
 u'R': 339201,
 u'Scala': 10249122,
 u'Shell': 172244}</pre></div></li><li><p>Retrieve a few key participants of the wide Spark GitHub repository network. There are 3,738 stargazers in the Apache Spark repository at the time of writing. The network is immense. The first stargazer is <span class="emphasis"><em>Matei Zaharia</em></span>, the cofounder of the Spark project when he was doing his PhD in Berkeley.</p><div class="informalexample"><pre class="programlisting">stargazers = [ s for s in repo.get_stargazers() ]
print "Number of stargazers", len(stargazers)
Number of stargazers 3738

[stargazers[i].login for i in range (0,20)]
[u'mateiz',
 u'beyang',
 u'abo',
 u'CodingCat',
 u'andy327',
 u'CrazyJvm',
 u'jyotiska',
 u'BaiGang',
 u'sundstei',
 u'dianacarroll',
 u'ybotco',
 u'xelax',
 u'prabeesh',
 u'invkrh',
 u'bedla',
 u'nadesai',
 u'pcpratts',
 u'narkisr',
 u'Honghe',
 u'Jacke']</pre></div></li></ol></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec27"></a>Understanding the community through Meetup</h3></div></div></div><p>In order <a id="id142" class="indexterm"></a>to get a better understanding of how to operate with the Meetup API, we will go through the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Create a Python program to call the Meetup API using an authentication token.</p></li><li><p>Retrieve information of past events for meetup groups such as <span class="emphasis"><em>London Data Science</em></span>.</p></li><li><p>Retrieve the profile of the meetup members in order to analyze their participation in similar meetup groups.</p></li></ol></div><p>Let's go through the process step-by-step:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>As there is no reliable Meetup API Python library, we will programmatically create a client to instantiate the Meetup API:</p><div class="informalexample"><pre class="programlisting">import json
import mimeparse
import requests
import urllib
from pprint import pprint as pp

MEETUP_API_HOST = 'https://api.meetup.com'
EVENTS_URL = MEETUP_API_HOST + '/2/events.json'
MEMBERS_URL = MEETUP_API_HOST + '/2/members.json'
GROUPS_URL = MEETUP_API_HOST + '/2/groups.json'
RSVPS_URL = MEETUP_API_HOST + '/2/rsvps.json'
PHOTOS_URL = MEETUP_API_HOST + '/2/photos.json'
GROUP_URLNAME = 'London-Machine-Learning-Meetup'
# GROUP_URLNAME = 'London-Machine-Learning-Meetup' # 'Data-Science-London'

class Mee
tupAPI(object):
    """
    Retrieves information about meetup.com
    """
    def __init__(self, api_key, num_past_events=10, http_timeout=1,
                 http_retries=2):
        """
        Create a new instance of MeetupAPI
        """
        self._api_key = api_key
        self._http_timeout = http_timeout
        self._http_retries = http_retries
        self._num_past_events = num_past_events

    def get_past_events(self):
        """
        Get past meetup events for a given meetup group
        """
        params = {'key': self._api_key,
                  'group_urlname': GROUP_URLNAME,
                  'status': 'past',
                  'desc': 'true'}
        if self._num_past_events:
            params['page'] = str(self._num_past_events)

        query = urllib.urlencode(params)
        url = '{0}?{1}'.format(EVENTS_URL, query)
        response = requests.get(url, timeout=self._http_timeout)
        data = response.json()['results']
        return data

    def get_members(self):
        """
        Get meetup members for a given meetup group
        """
        params = {'key': self._api_key,
                  'group_urlname': GROUP_URLNAME,
                  'offset': '0',
                  'format': 'json',
                  'page': '100',
                  'order': 'name'}
        query = urllib.urlencode(params)
        url = '{0}?{1}'.format(MEMBERS_URL, query)
        response = requests.get(url, timeout=self._http_timeout)
        data = response.json()['results']
        return data

    def get_groups_by_member(self, member_id='38680722'):
        """
        Get meetup groups for a given meetup member
        """
        params = {'key': self._api_key,
                  'member_id': member_id,
                  'offset': '0',
                  'format': 'json',
                  'page': '100',
                  'order': 'id'}
        query = urllib.urlencode(params)
        url = '{0}?{1}'.format(GROUPS_URL, query)
        response = requests.get(url, timeout=self._http_timeout)
        data = response.json()['results']
        return data</pre></div></li><li><p>Then, we <a id="id143" class="indexterm"></a>will retrieve past events from a given Meetup group:</p><div class="informalexample"><pre class="programlisting">m = MeetupAPI(api_key='Get_Your_Own_Key')
last_meetups = m.get_past_events()
pp(last_meetups[5])

{u'created': 1401809093000,
 u'description': u"&lt;p&gt;We are hosting a joint meetup between Spark London and Machine Learning London. Given the excitement in the machine learning community around Spark at the moment a joint meetup is in order!&lt;/p&gt; &lt;p&gt;Michael Armbrust from the Apache Spark core team will be flying over from the States to give us a talk in person.\xa0Thanks to our sponsors, Cloudera, MapR and Databricks for helping make this happen.&lt;/p&gt; &lt;p&gt;The first part of the talk will be about MLlib, the machine learning library for Spark,\xa0and the second part, on\xa0Spark SQL.&lt;/p&gt; &lt;p&gt;Don't sign up if you have already signed up on the Spark London page though!&lt;/p&gt; &lt;p&gt;\n\n\nAbstract for part one:&lt;/p&gt; &lt;p&gt;In this talk, we\u2019ll introduce Spark and show how to use it to build fast, end-to-end machine learning workflows. Using Spark\u2019s high-level API, we can process raw data with familiar libraries in Java, Scala or Python (e.g. NumPy) to extract the features for machine learning. Then, using MLlib, its built-in machine learning library, we can run scalable versions of popular algorithms. We\u2019ll also cover upcoming development work including new built-in algorithms and R bindings.&lt;/p&gt; &lt;p&gt;\n\n\n\nAbstract for part two:\xa0&lt;/p&gt; &lt;p&gt;In this talk, we'll examine Spark SQL, a new Alpha component that is part of the Apache Spark 1.0 release. Spark SQL lets developers natively query data stored in both existing RDDs and external sources such as Apache Hive. A key feature of Spark SQL is the ability to blur the lines between relational tables and RDDs, making it easy for developers to intermix SQL commands that query external data with complex analytics. In addition to Spark SQL, we'll explore the Catalyst optimizer framework, which allows Spark SQL to automatically rewrite query plans to execute more efficiently.&lt;/p&gt;",
 u'event_url': u'http://www.meetup.com/London-Machine-Learning-Meetup/events/186883262/',
 u'group': {u'created': 1322826414000,
            u'group_lat': 51.52000045776367,
            u'group_lon': -0.18000000715255737,
            u'id': 2894492,
            u'join_mode': u'open',
            u'name': u'London Machine Learning Meetup',
            u'urlname': u'London-Machine-Learning-Meetup',
            u'who': u'Machine Learning Enthusiasts'},
 u'headcount': 0,
 u'id': u'186883262',
 u'maybe_rsvp_count': 0,
 u'name': u'Joint Spark London and Machine Learning Meetup',
 u'rating': {u'average': 4.800000190734863, u'count': 5},
 u'rsvp_limit': 70,
 u'status': u'past',
 u'time': 1403200800000,
 u'updated': 1403450844000,
 u'utc_offset': 3600000,
 u'venue': {u'address_1': u'12 Errol St, London',
            u'city': u'EC1Y 8LX',
            u'country': u'gb',
            u'id': 19504802,
            u'lat': 51.522533,
            u'lon': -0.090934,
            u'name': u'Royal Statistical Society',
            u'repinned': False},
 u'visibility': u'public',
 u'waitlist_count': 84,
 u'yes_rsvp_count': 70}</pre></div></li><li><p>Get information about the Meetup members:</p><div class="informalexample"><pre class="programlisting">members = m.get_members()

{u'city': u'London',
  u'country': u'gb',
  u'hometown': u'London',
  u'id': 11337881,
  u'joined': 1421418896000,
  u'lat': 51.53,
  u'link': u'http://www.meetup.com/members/11337881',
  u'lon': -0.09,
  u'name': u'Abhishek Shivkumar',
  u'other_services': {u'twitter': {u'identifier': u'@abhisemweb'}},
  u'photo': {u'highres_link': u'http://photos3.meetupstatic.com/photos/member/9/6/f/3/highres_10898643.jpeg',
             u'photo_id': 10898643,
             u'photo_link': u'http://photos3.meetupstatic.com/photos/member/9/6/f/3/member_10898643.jpeg',
             u'thumb_link': u'http://photos3.meetupstatic.com/photos/member/9/6/f/3/thumb_10898643.jpeg'},
  u'self': {u'common': {}},
  u'state': u'17',
  u'status': u'active',
  u'topics': [{u'id': 1372, u'name': u'Semantic Web', u'urlkey': u'semweb'},
              {u'id': 1512, u'name': u'XML', u'urlkey': u'xml'},
              {u'id': 49585,
               u'name': u'Semantic Social Networks',
               u'urlkey': u'semantic-social-networks'},
              {u'id': 24553,
               u'name': u'Natural Language Processing',
...(snip)...
               u'name': u'Android Development',
               u'urlkey': u'android-developers'}],
  u'visited': 1429281599000}</pre></div></li></ol></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec20"></a>Previewing our app</h2></div></div><hr /></div><p>Our<a id="id144" class="indexterm"></a> challenge is to make sense of the data retrieved from these social networks, finding the key relationships and deriving insights. Some of the elements of interest are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Visualizing the top influencers: Discover the top influencers in the community:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Heavy Twitter users on <span class="emphasis"><em>Apache Spark</em></span>
</p></li><li style="list-style-type: disc"><p>Committers in GitHub</p></li><li style="list-style-type: disc"><p>Leading Meetup presentations</p></li></ul></div></li><li style="list-style-type: disc"><p>Understanding the Network: Network graph of GitHub committers, watchers, and stargazers</p></li><li style="list-style-type: disc"><p>Identifying the Hot Locations: Locating the most active location for Spark</p></li></ul></div><p>The following <a id="id145" class="indexterm"></a>screenshot provides a preview of our app:</p><div class="mediaobject"><img src="graphics/B03986_02_09.jpg" /></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec21"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we laid out the overall architecture of our app. We explained the two main paradigms of processing data: batch processing, also called data at rest, and streaming analytics, referred to as data in motion. We proceeded to establish connections to three social networks of interest: Twitter, GitHub, and Meetup. We sampled the data and provided a preview of what we are aiming to build. The remainder of the book will focus on the Twitter dataset. We provided here the tools and API to access three social networks, so you can at a later stage create your own data mashups. We are now ready to investigate the data collected, which will be the topic of the next chapter.</p><p>In the next chapter, we will delve deeper into data analysis, extracting the key attributes of interest for our purposes and managing the storage of the information for batch and stream processing.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch03"></a>Chapter 3. Juggling Data with Spark</h2></div></div></div><p>As per the batch and streaming architecture laid out in the previous chapter, we need data to fuel our applications. We will harvest data focused on Apache Spark from Twitter. The objective of this chapter is to prepare data to be further used by the machine learning and streaming applications. This chapter focuses on how to exchange code and data across the distributed network. We will get practical insights into serialization, persistence, marshaling, and caching. We will get to grips with on Spark SQL, the key Spark module to interactively explore structured and semi-structured data. The fundamental data structure powering Spark SQL is the Spark dataframe. The Spark dataframe is inspired by the Python Pandas dataframe and the R dataframe. It is a powerful data structure, well understood and appreciated by data scientists with a background in R or Python.</p><p>In this chapter, we will cover the following points:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Connect to Twitter, collect the relevant data, and then persist it in various formats such as JSON and CSV and data stores such as MongoDB</p></li><li style="list-style-type: disc"><p>Analyze the data using Blaze and Odo, a spin-off library from Blaze, in order to connect and transfer data from various sources and destinations</p></li><li style="list-style-type: disc"><p>Introduce Spark dataframes as the foundation for data interchange between the various Spark modules and explore data interactively using Spark SQL</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec22"></a>Revisiting the data-intensive app architecture</h2></div></div><hr /></div><p>Let's <a id="id146" class="indexterm"></a>first put in context the focus of this chapter with respect to the data-intensive app architecture. We will concentrate our attention on the integration layer and essentially run through iterative cycles of the acquisition, refinement, and persistence of the data. This cycle was termed the five Cs. The five Cs stand for <span class="emphasis"><em>connect</em></span>, <span class="emphasis"><em>collect</em></span>, <span class="emphasis"><em>correct</em></span>, <span class="emphasis"><em>compose</em></span>, and <span class="emphasis"><em>consume</em></span>. They are the essential processes we run through in the integration layer in order to get to the right quality and quantity of data retrieved from Twitter. We will also delve deeper in the persistence layer and set up a data store such as MongoDB to collect our data for processing later.</p><p>We will <a id="id147" class="indexterm"></a>explore the data with Blaze, a Python library for data manipulation, and Spark SQL, the interactive module of Spark for data discovery powered by the Spark dataframe. The dataframe paradigm is shared by Python Pandas, Python Blaze, and Spark SQL. We will get a feel for the nuances of the three dataframe flavors.</p><p>The following diagram sets the context of the chapter's focus, highlighting the integration layer and the persistence layer:</p><div class="mediaobject"><img src="graphics/B03986_03_01.jpg" /></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec23"></a>Serializing and deserializing data</h2></div></div><hr /></div><p>As we<a id="id148" class="indexterm"></a> are harvesting data from web APIs under rate limit constraints, we<a id="id149" class="indexterm"></a> need to store them. As the data is processed on a distributed cluster, we need consistent ways to save state and retrieve it for later usage.</p><p>Let's now define serialization, persistence, marshaling, and caching or memorization.</p><p>Serializing a Python object converts it into a stream of bytes. The Python object needs to be retrieved beyond the scope of its existence, when the program is shut. The serialized Python object can be transferred over a network or stored in a persistent storage. Deserialization is the opposite and converts the stream of bytes into the original Python object so the program can carry on from the saved state. The most popular serialization library in Python is Pickle. As a matter of fact, the PySpark commands are transferred over the wire to the worker nodes via pickled data.</p><p>Persistence saves a program's state data to disk or memory so that it can carry on where it left off upon restart. It saves a Python object from memory to a file or a database and loads it later with the same state.</p><p>Marshalling sends Python code or data over a network TCP connection in a multicore or distributed system.</p><p>Caching converts a Python object to a string in memory so that it can be used as a dictionary key later on. Spark supports pulling a dataset into a cluster-wide, in-memory cache. This is very useful when data is accessed repeatedly such as when querying a small reference dataset or running an iterative algorithm such as Google PageRank.</p><p>Caching is a crucial concept for Spark as it allows us to save RDDs in memory or with a spillage to disk. The caching strategy can be selected based on the lineage of the data or the <span class="strong"><strong>DAG</strong></span> (short for <span class="strong"><strong>Directed Acyclic Graph</strong></span>) of transformations applied to the RDDs in order to<a id="id150" class="indexterm"></a> minimize shuffle or cross network heavy data exchange. In order to achieve good performance with Spark, beware of data shuffling. A good partitioning policy and use of RDD caching, coupled with avoiding unnecessary action operations, leads to better performance with Spark.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec24"></a>Harvesting and storing data</h2></div></div><hr /></div><p>Before <a id="id151" class="indexterm"></a>delving into database persistent storage such as MongoDB, we <a id="id152" class="indexterm"></a>will look at some useful file storages that are widely used: <span class="strong"><strong>CSV</strong></span> (short <a id="id153" class="indexterm"></a>for <span class="strong"><strong>comma-separated values</strong></span>) and <a id="id154" class="indexterm"></a>
<span class="strong"><strong>JSON</strong></span> (short for <span class="strong"><strong>JavaScript Object Notation</strong></span>) file storage. The enduring popularity of these two file formats lies in a few key reasons: they are human readable, simple, relatively lightweight, and easy to use.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec28"></a>Persisting data in CSV</h3></div></div></div><p>The CSV<a id="id155" class="indexterm"></a> format is lightweight, human readable, and easy to use. It has delimited text columns with an inherent tabular schema.</p><p>Python offers a robust <code class="literal">csv</code> library that can serialize a <code class="literal">csv</code> file into a Python dictionary. For the purpose of our program, we have written a <code class="literal">python</code> class that manages to persist data in CSV format and read from a given CSV.</p><p>Let's run through the code of the class <code class="literal">IO_csv</code> object. The <code class="literal">__init__</code> section of the class basically instantiates the file path, the filename, and the file suffix (in this case, <code class="literal">.csv</code>):</p><div class="informalexample"><pre class="programlisting">class IO_csv(object):

    def __init__(self, filepath, filename, filesuffix='csv'):
        self.filepath = filepath       # /path/to/file without the /' at the end
        self.filename = filename       # FILE_NAME
        self.filesuffix = filesuffix</pre></div><p>The <code class="literal">save</code> method of the class uses a Python named tuple and the header fields of the <code class="literal">csv</code> file in order to impart a schema while persisting the rows of the CSV. If the <code class="literal">csv</code> file already exists, it will be appended and not overwritten otherwise; it will be created:</p><div class="informalexample"><pre class="programlisting">    def save(self, data, NTname, fields):
        # NTname = Name of the NamedTuple
        # fields = header of CSV - list of the fields name
        NTuple = namedtuple(NTname, fields)
        
        if os.path.isfile('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix)):
            # Append existing file
            with open('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix), 'ab') as f:
                writer = csv.writer(f)
                # writer.writerow(fields) # fields = header of CSV
                writer.writerows([row for row in map(NTuple._make, data)])
                # list comprehension using map on the NamedTuple._make() iterable and the data file to be saved
                # Notice writer.writerows and not writer.writerow (i.e. list of multiple rows sent to csv file
        else:
            # Create new file
            with open('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix), 'wb') as f:
                writer = csv.writer(f)
                writer.writerow(fields) # fields = header of CSV - list of the fields name
                writer.writerows([row for row in map(NTuple._make, data)])
                #  list comprehension using map on the NamedTuple._make() iterable and the data file to be saved
                # Notice writer.writerows and not writer.writerow (i.e. list of multiple rows sent to csv file</pre></div><p>The <code class="literal">load</code> method <a id="id156" class="indexterm"></a>of the class also uses a Python named tuple and the header fields of the <code class="literal">csv</code> file in order to retrieve the data using a consistent schema. The <code class="literal">load</code> method is a memory-efficient generator to avoid loading a huge file in memory: hence we use <code class="literal">yield</code> in place of <code class="literal">return</code>:</p><div class="informalexample"><pre class="programlisting">    def load(self, NTname, fields):
        # NTname = Name of the NamedTuple
        # fields = header of CSV - list of the fields name
        NTuple = namedtuple(NTname, fields)
        with open('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix),'rU') as f:
            reader = csv.reader(f)
            for row in map(NTuple._make, reader):
                # Using map on the NamedTuple._make() iterable and the reader file to be loaded
                yield row </pre></div><p>Here's the named tuple. We are using it to parse the tweet in order to save or retrieve them to and from the <code class="literal">csv</code> file:</p><div class="informalexample"><pre class="programlisting">fields01 = ['id', 'created_at', 'user_id', 'user_name', 'tweet_text', 'url']
Tweet01 = namedtuple('Tweet01',fields01)

def parse_tweet(data):
    """
    Parse a ``tweet`` from the given response data.
    """
    return Tweet01(
        id=data.get('id', None),
        created_at=data.get('created_at', None),
        user_id=data.get('user_id', None),
        user_name=data.get('user_name', None),
        tweet_text=data.get('tweet_text', None),
        url=data.get('url')
    )</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec29"></a>Persisting data in JSON</h3></div></div></div><p>JSON is <a id="id157" class="indexterm"></a>one of the most popular data formats for Internet-based applications. All the APIs we are dealing with, Twitter, GitHub, and Meetup, deliver their data in JSON format. The JSON format is relatively lightweight compared to XML and human readable, and the schema is embedded in JSON. As opposed to the CSV format, where all records follow exactly the same tabular structure, JSON records can vary in their structure. JSON is semi-structured. A JSON record can be mapped into a Python dictionary of dictionaries.</p><p>Let's run<a id="id158" class="indexterm"></a> through the code of the class <code class="literal">IO_json</code> object. The <code class="literal">__init__</code> section of the class basically instantiates the file path, the filename, and the file suffix (in this case, <code class="literal">.json</code>):</p><div class="informalexample"><pre class="programlisting">class IO_json(object):
    def __init__(self, filepath, filename, filesuffix='json'):
        self.filepath = filepath        # /path/to/file without the /' at the end
        self.filename = filename        # FILE_NAME
        self.filesuffix = filesuffix
        # self.file_io = os.path.join(dir_name, .'.join((base_filename, filename_suffix)))</pre></div><p>The <code class="literal">save</code> method of the class uses <code class="literal">utf-8</code> encoding in order to ensure read and write compatibility of the data. If the JSON file already exists, it will be appended and not overwritten; otherwise it will be created:</p><div class="informalexample"><pre class="programlisting">    def save(self, data):
        if os.path.isfile('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix)):
            # Append existing file
            with io.open('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix), 'a', encoding='utf-8') as f:
                f.write(unicode(json.dumps(data, ensure_ascii= False))) # In python 3, there is no "unicode" function 
                # f.write(json.dumps(data, ensure_ascii= False)) # create a \" escape char for " in the saved file        
        else:
            # Create new file
            with io.open('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix), 'w', encoding='utf-8') as f:
                f.write(unicode(json.dumps(data, ensure_ascii= False)))
                # f.write(json.dumps(data, ensure_ascii= False))</pre></div><p>The <code class="literal">load</code> method of the class just returns the file that has been read. A further <code class="literal">json.loads</code> function needs to be applied in order to retrieve the <code class="literal">json</code> out of the file read:</p><div class="informalexample"><pre class="programlisting">    def load(self):
        with io.open('{0}/{1}.{2}'.format(self.filepath, self.filename, self.filesuffix), encoding='utf-8') as f:
            return f.read()</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec30"></a>Setting up MongoDB</h3></div></div></div><p>It is <a id="id159" class="indexterm"></a>crucial to store the information harvested. Thus, we set <a id="id160" class="indexterm"></a>up MongoDB as our main document data store. As all the<a id="id161" class="indexterm"></a> information collected is in JSON format and MongoDB stores information in <span class="strong"><strong>BSON</strong></span> (short for <span class="strong"><strong>Binary JSON</strong></span>), it is therefore a natural choice.</p><p>We will run through the following steps now:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Installing the MongoDB server and client</p></li><li style="list-style-type: disc"><p>Running the MongoDB server</p></li><li style="list-style-type: disc"><p>Running the Mongo client</p></li><li style="list-style-type: disc"><p>Installing the PyMongo driver</p></li><li style="list-style-type: disc"><p>Creating the Python Mongo client</p></li></ul></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec03"></a>Installing the MongoDB server and client</h4></div></div></div><p>In order<a id="id162" class="indexterm"></a> to install the MongoDB package, perform through the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Import the public key used by the package management system (in our case, Ubuntu's <code class="literal">apt</code>). To import the MongoDB public key, we issue the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 7F0CEB10</strong></span>
</pre></div></li><li><p>Create a list file for MongoDB. To create the list file, we use the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>echo "deb http://repo.mongodb.org/apt/ubuntu "$("lsb_release -sc)"/ mongodb-org/3.0 multiverse" | sudo tee /etc/apt/sources.list.d/mongodb-org-3.0.list</strong></span>
</pre></div></li><li><p>Update the local package database as <code class="literal">sudo</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo apt-get update</strong></span>
</pre></div></li><li><p>Install the MongoDB packages. We install the latest stable version of MongoDB with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo apt-get install -y mongodb-org</strong></span>
</pre></div></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec04"></a>Running the MongoDB server</h4></div></div></div><p>Let's <a id="id163" class="indexterm"></a>start the MongoDB server:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>To start MongoDB server, we issue the following command to start <code class="literal">mongod</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo service mongodb start</strong></span>
</pre></div></li><li><p>To check whether <code class="literal">mongod</code> has started properly, we issue the command: </p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>an@an-VB:/usr/bin$ ps -ef | grep mongo</strong></span>
<span class="strong"><strong>mongodb    967     1  4 07:03 ?        00:02:02 /usr/bin/mongod --config /etc/mongod.conf</strong></span>
<span class="strong"><strong>an        3143  3085  0 07:45 pts/3    00:00:00 grep --color=auto mongo</strong></span>
</pre></div><p>In this case, we see that <code class="literal">mongodb</code> is running in process <code class="literal">967</code>.</p></li><li><p>The <code class="literal">mongod</code> server sends a message to the effect that it is waiting for connection on <code class="literal">port 27017</code>. This <a id="id164" class="indexterm"></a>is the default port for MongoDB. It can be changed in the configuration file.</p></li><li><p>We can check the contents of the log file at <code class="literal">/var/log/mongod/mongod.log</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>an@an-VB:/var/lib/mongodb$ ls -lru</strong></span>
<span class="strong"><strong>total 81936</strong></span>
<span class="strong"><strong>drwxr-xr-x 2 mongodb nogroup     4096 Apr 25 11:19 _tmp</strong></span>
<span class="strong"><strong>-rw-r--r-- 1 mongodb nogroup       69 Apr 25 11:19 storage.bson</strong></span>
<span class="strong"><strong>-rwxr-xr-x 1 mongodb nogroup        5 Apr 25 11:19 mongod.lock</strong></span>
<span class="strong"><strong>-rw------- 1 mongodb nogroup 16777216 Apr 25 11:19 local.ns</strong></span>
<span class="strong"><strong>-rw------- 1 mongodb nogroup 67108864 Apr 25 11:19 local.0</strong></span>
<span class="strong"><strong>drwxr-xr-x 2 mongodb nogroup     4096 Apr 25 11:19 journal</strong></span>
</pre></div></li><li><p>In order to stop the <code class="literal">mongodb</code> server, just issue the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo service mongodb stop</strong></span>
</pre></div></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec05"></a>Running the Mongo client</h4></div></div></div><p>Running the Mongo client in the console is as easy as calling <code class="literal">mongo</code>, as highlighted in the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>an@an-VB:/usr/bin$ mongo</strong></span>
<span class="strong"><strong>MongoDB shell version: 3.0.2</strong></span>
<span class="strong"><strong>connecting to: test</strong></span>
<span class="strong"><strong>Server has startup warnings: </strong></span>
<span class="strong"><strong>2015-05-30T07:03:49.387+0200 I CONTROL  [initandlisten] </strong></span>
<span class="strong"><strong>2015-05-30T07:03:49.388+0200 I CONTROL  [initandlisten] </strong></span>
</pre></div><p>At the mongo client console prompt, we can see the databases with the following commands:</p><div class="informalexample"><pre class="programlisting">&gt; show dbs
local  0.078GB
test   0.078GB</pre></div><p>We select the test database using <code class="literal">use test</code>:</p><div class="informalexample"><pre class="programlisting">&gt; use test
switched to db test</pre></div><p>We display <a id="id165" class="indexterm"></a>the collections within the test database:</p><div class="informalexample"><pre class="programlisting">&gt; show collections
restaurants
system.indexes</pre></div><p>We check a sample record in the restaurant collection listed previously:</p><div class="informalexample"><pre class="programlisting">&gt; db.restaurants.find()
{ "_id" : ObjectId("553b70055e82e7b824ae0e6f"), "address : { "building : "1007", "coord" : [ -73.856077, 40.848447 ], "street : "Morris Park Ave", "zipcode : "10462 }, "borough : "Bronx", "cuisine : "Bakery", "grades : [ { "grade : "A", "score" : 2, "date" : ISODate("2014-03-03T00:00:00Z") }, { "date" : ISODate("2013-09-11T00:00:00Z"), "grade : "A", "score" : 6 }, { "score" : 10, "date" : ISODate("2013-01-24T00:00:00Z"), "grade : "A }, { "date" : ISODate("2011-11-23T00:00:00Z"), "grade : "A", "score" : 9 }, { "date" : ISODate("2011-03-10T00:00:00Z"), "grade : "B", "score" : 14 } ], "name : "Morris Park Bake Shop", "restaurant_id : "30075445" }</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec06"></a>Installing the PyMongo driver</h4></div></div></div><p>Installing<a id="id166" class="indexterm"></a> the Python driver with anaconda is easy. Just run the following command at the terminal:</p><div class="informalexample"><pre class="programlisting">conda install pymongo</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec07"></a>Creating the Python client for MongoDB</h4></div></div></div><p>We are<a id="id167" class="indexterm"></a> creating a <code class="literal">IO_mongo</code> class that will be used in our harvesting and processing programs to store the data collected and retrieved saved information. In order to create the <code class="literal">mongo</code> client, we will import the <code class="literal">MongoClient</code> module from <code class="literal">pymongo</code>. We connect to the <code class="literal">mongodb</code> server on localhost at port 27017. The command is as follows:</p><div class="informalexample"><pre class="programlisting">from pymongo import MongoClient as MCli

class IO_mongo(object):
    conn={'host':'localhost', 'ip':'27017'}</pre></div><p>We initialize our class with the client connection, the database (in this case, <code class="literal">twtr_db</code>), and the collection (in this case, <code class="literal">twtr_coll</code>) to be accessed:</p><div class="informalexample"><pre class="programlisting">    def __init__(self, db='twtr_db', coll='twtr_coll', **conn ):
        # Connects to the MongoDB server 
        self.client = MCli(**conn)
        self.db = self.client[db]
        self.coll = self.db[coll]</pre></div><p>The <code class="literal">save</code> method inserts new records in the preinitialized collection and database:</p><div class="informalexample"><pre class="programlisting">    def save(self, data):
        # Insert to collection in db  
        return self.coll.insert(data)</pre></div><p>The <code class="literal">load</code> method <a id="id168" class="indexterm"></a>allows the retrieval of specific records according to criteria and projection. In the case of large amount of data, it returns a cursor:</p><div class="informalexample"><pre class="programlisting">    def load(self, return_cursor=False, criteria=None, projection=None):

            if criteria is None:
                criteria = {}

            if projection is None:
                cursor = self.coll.find(criteria)
            else:
                cursor = self.coll.find(criteria, projection)

            # Return a cursor for large amounts of data
            if return_cursor:
                return cursor
            else:
                return [ item for item in cursor ]</pre></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec31"></a>Harvesting data from Twitter</h3></div></div></div><p>Each <a id="id169" class="indexterm"></a>social network poses its limitations and challenges. One of the main obstacles for harvesting data is an imposed rate limit. While running repeated or long-running connections between rates limit pauses, we have to be careful to avoid collecting duplicate data.</p><p>We have redesigned our connection programs outlined in the previous chapter to take care of the rate limits.</p><p>In this <code class="literal">TwitterAPI</code> class that connects and collects the tweets according to the search query we specify, we have added the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Logging capability using the Python logging library with the aim of collecting any errors or warning in the case of program failure</p></li><li style="list-style-type: disc"><p>Persistence capability using MongoDB, with the <code class="literal">IO_mongo</code> class exposed previously as well as JSON file using the <code class="literal">IO_json</code> class</p></li><li style="list-style-type: disc"><p>API rate limit and error management capability, so we can ensure more resilient calls to Twitter without getting barred for tapping into the firehose</p></li></ul></div><p>Let's go through the steps:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>We initialize by instantiating the Twitter API with our credentials:</p><div class="informalexample"><pre class="programlisting">class TwitterAPI(object):
    """
    TwitterAPI class allows the Connection to Twitter via OAuth
    once you have registered with Twitter and receive the 
    necessary credentials 
    """

    def __init__(self): 
        consumer_key = 'get_your_credentials'
        consumer_secret = get your_credentials'
        access_token = 'get_your_credentials'
        access_secret = 'get your_credentials'
        self.consumer_key = consumer_key
        self.consumer_secret = consumer_secret
        self.access_token = access_token
        self.access_secret = access_secret
        self.retries = 3
        self.auth = twitter.oauth.OAuth(access_token, access_secret, consumer_key, consumer_secret)
        self.api = twitter.Twitter(auth=self.auth)</pre></div></li><li><p>We<a id="id170" class="indexterm"></a> initialize the logger by providing the log level:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">logger.debug</code>(debug message)</p></li><li style="list-style-type: disc"><p>
<code class="literal">logger.info</code>(info message)</p></li><li style="list-style-type: disc"><p>
<code class="literal">logger.warn</code>(warn message)</p></li><li style="list-style-type: disc"><p>
<code class="literal">logger.error</code>(error message)</p></li><li style="list-style-type: disc"><p>
<code class="literal">logger.critical</code>(critical message)</p></li></ul></div></li><li><p>We set the log path and the message format:</p><div class="informalexample"><pre class="programlisting">        # logger initialisation
        appName = 'twt150530'
        self.logger = logging.getLogger(appName)
        #self.logger.setLevel(logging.DEBUG)
        # create console handler and set level to debug
        logPath = '/home/an/spark/spark-1.3.0-bin-hadoop2.4/examples/AN_Spark/data'
        fileName = appName
        fileHandler = logging.FileHandler("{0}/{1}.log".format(logPath, fileName))
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        fileHandler.setFormatter(formatter)
        self.logger.addHandler(fileHandler) 
        self.logger.setLevel(logging.DEBUG)</pre></div></li><li><p>We<a id="id171" class="indexterm"></a> initialize the JSON file persistence instruction:</p><div class="informalexample"><pre class="programlisting">        # Save to JSON file initialisation
        jsonFpath = '/home/an/spark/spark-1.3.0-bin-hadoop2.4/examples/AN_Spark/data'
        jsonFname = 'twtr15053001'
        self.jsonSaver = IO_json(jsonFpath, jsonFname)</pre></div></li><li><p>We initialize the MongoDB database and collection for persistence:</p><div class="informalexample"><pre class="programlisting">        # Save to MongoDB Intitialisation
        self.mongoSaver = IO_mongo(db='twtr01_db', coll='twtr01_coll')</pre></div></li><li><p>The method <code class="literal">searchTwitter</code> launches the search according to the query specified:</p><div class="informalexample"><pre class="programlisting">    def searchTwitter(self, q, max_res=10,**kwargs):
        search_results = self.api.search.tweets(q=q, count=10, **kwargs)
        statuses = search_results['statuses']
        max_results = min(1000, max_res)
        
        for _ in range(10):
            try:
                next_results = search_results['search_metadata']['next_results']
                # self.logger.info('info' in searchTwitter - next_results:%s'% next_results[1:])
            except KeyError as e:
                self.logger.error('error' in searchTwitter: %s', %(e))
                break
            
            # next_results = urlparse.parse_qsl(next_results[1:]) # python 2.7
            next_results = urllib.parse.parse_qsl(next_results[1:])
            # self.logger.info('info' in searchTwitter - next_results[max_id]:', next_results[0:])
            kwargs = dict(next_results)
            # self.logger.info('info' in searchTwitter - next_results[max_id]:%s'% kwargs['max_id'])
            search_results = self.api.search.tweets(**kwargs)
            statuses += search_results['statuses']
            self.saveTweets(search_results['statuses'])
            
            if len(statuses) &gt; max_results:
                self.logger.info('info' in searchTwitter - got %i tweets - max: %i' %(len(statuses), max_results))
                break
        return statuses</pre></div></li><li><p>The <code class="literal">saveTweets</code> method <a id="id172" class="indexterm"></a>actually saves the collected tweets in JSON and in MongoDB:</p><div class="informalexample"><pre class="programlisting">    def saveTweets(self, statuses):
        # Saving to JSON File
        self.jsonSaver.save(statuses)
        
        # Saving to MongoDB
        for s in statuses:
            self.mongoSaver.save(s)</pre></div></li><li><p>The <code class="literal">parseTweets</code> method allows us to extract the key tweet information from the vast amount of information provided by the Twitter API:</p><div class="informalexample"><pre class="programlisting">    def parseTweets(self, statuses):
        return [ (status['id'], 
                  status['created_at'], 
                  status['user']['id'],
                  status['user']['name'] 
                  status['text''text'], 
                  url['expanded_url']) 
                        for status in statuses 
                            for url in status['entities']['urls'] ]</pre></div></li><li><p>The <code class="literal">getTweets</code> method calls the <code class="literal">searchTwitter</code> method described previously. The <code class="literal">getTweets</code> method ensures that API calls are made reliably whilst respecting the imposed rate limit. The code is as follows:</p><div class="informalexample"><pre class="programlisting">    def getTweets(self, q,  max_res=10):
        """
        Make a Twitter API call whilst managing rate limit and errors.
        """
        def handleError(e, wait_period=2, sleep_when_rate_limited=True):
            if wait_period &gt; 3600: # Seconds
                self.logger.error('Too many retries in getTweets: %s', %(e))
                raise e
            if e.e.code == 401:
                self.logger.error('error 401 * Not Authorised * in getTweets: %s', %(e))
                return None
            elif e.e.code == 404:
                self.logger.error('error 404 * Not Found * in getTweets: %s', %(e))
                return None
            elif e.e.code == 429: 
                self.logger.error('error 429 * API Rate Limit Exceeded * in getTweets: %s', %(e))
                if sleep_when_rate_limited:
                    self.logger.error('error 429 * Retrying in 15 minutes * in getTweets: %s', %(e))
                    sys.stderr.flush()
                    time.sleep(60*15 + 5)
                    self.logger.info('error 429 * Retrying now * in getTweets: %s', %(e))
                    return 2
                else:
                    raise e # Caller must handle the rate limiting issue
            elif e.e.code in (500, 502, 503, 504):
                self.logger.info('Encountered %i Error. Retrying in %i seconds' % (e.e.code, wait_period))
                time.sleep(wait_period)
                wait_period *= 1.5
                return wait_period
            else:
                self.logger.error('Exit - aborting - %s', %(e))
                raise e</pre></div></li><li><p>Here, we<a id="id173" class="indexterm"></a> are calling the <code class="literal">searchTwitter</code> API with the relevant query based on the parameters specified. If we encounter any error such as rate limitation from the provider, this will be processed by the <code class="literal">handleError</code> method:</p><div class="informalexample"><pre class="programlisting">        while True:
            try:
                self.searchTwitter( q, max_res=10)
            except twitter.api.TwitterHTTPError as e:
                error_count = 0 
                wait_period = handleError(e, wait_period)
                if wait_period is None:
                    return</pre></div></li></ol></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec25"></a>Exploring data using Blaze</h2></div></div><hr /></div><p>Blaze is <a id="id174" class="indexterm"></a>an open source Python library, primarily developed by<a id="id175" class="indexterm"></a> Continuum.io, leveraging Python Numpy arrays and Pandas dataframe. Blaze extends to out-of-core computing, while Pandas and Numpy are single-core.</p><p>Blaze offers an adaptable, unified, and consistent user interface across various backends. Blaze orchestrates the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Data</strong></span>: Seamless exchange of data across storages such as CSV, JSON, HDF5, HDFS, and Bcolz files.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Computation</strong></span>: Using the same query processing against computational backends such as Spark, MongoDB, Pandas, or SQL Alchemy.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Symbolic expressions</strong></span>: Abstract expressions such as join, group-by, filter, selection, and projection with a syntax similar to Pandas but limited in scope. Implements the split-apply-combine methods pioneered by the R language.</p></li></ul></div><p>Blaze expressions are lazily evaluated and in that respect share a similar processing paradigm with Spark RDDs transformations.</p><p>Let's dive into Blaze by first importing the necessary libraries: <code class="literal">numpy</code>, <code class="literal">pandas</code>, <code class="literal">blaze</code> and <code class="literal">odo</code>. Odo is a spin-off of Blaze and ensures data migration from various backends. The commands are as follows:</p><div class="informalexample"><pre class="programlisting">import numpy as np
import pandas as pd
from blaze import Data, by, join, merge
from odo import odo
BokehJS successfully loaded.</pre></div><p>We create a Pandas <code class="literal">Dataframe</code> by reading the parsed tweets saved in a CSV file, <code class="literal">twts_csv</code>:</p><div class="informalexample"><pre class="programlisting">twts_pd_df = pd.DataFrame(twts_csv_read, columns=Tweet01._fields)
twts_pd_df.head()

Out[65]:
id    created_at    user_id    user_name    tweet_text    url
1   598831111406510082   2015-05-14 12:43:57   14755521 raulsaeztapia    RT @pacoid: Great recap of @StrataConf EU in L...   http://www.mango-solutions.com/wp/2015/05/the-...
2   598831111406510082   2015-05-14 12:43:57   14755521 raulsaeztapia    RT @pacoid: Great recap of @StrataConf EU in L...   http://www.mango-solutions.com/wp/2015/05/the-...
3   98808944719593472   2015-05-14 11:15:52   14755521 raulsaeztapia   RT @alvaroagea: Simply @ApacheSpark http://t.c...    http://www.webex.com/ciscospark/
4   598808944719593472   2015-05-14 11:15:52   14755521 raulsaeztapia   RT @alvaroagea: Simply @ApacheSpark http://t.c...   http://sparkjava.com/</pre></div><p>We run<a id="id176" class="indexterm"></a> the Tweets Panda <code class="literal">Dataframe</code> to the <code class="literal">describe()</code> function<a id="id177" class="indexterm"></a> to get some overall information on the dataset:</p><div class="informalexample"><pre class="programlisting">twts_pd_df.describe()
Out[66]:
id    created_at    user_id    user_name    tweet_text    url
count  19  19  19  19  19  19
unique    7  7   6   6     6   7
top    598808944719593472    2015-05-14 11:15:52    14755521 raulsaeztapia    RT @alvaroagea: Simply @ApacheSpark http://t.c...    http://bit.ly/1Hfd0Xm
freq    6    6    9    9    6    6</pre></div><p>We convert the Pandas <code class="literal">dataframe</code> into a Blaze <code class="literal">dataframe</code> by simply passing it through the <code class="literal">Data()</code> function:</p><div class="informalexample"><pre class="programlisting">#
# Blaze dataframe
#
twts_bz_df = Data(twts_pd_df)</pre></div><p>We can retrieve the schema representation of the Blaze <code class="literal">dataframe</code> by passing the <code class="literal">schema</code> function:</p><div class="informalexample"><pre class="programlisting">twts_bz_df.schema
Out[73]:
dshape("""{
  id: ?string,
  created_at: ?string,
  user_id: ?string,
  user_name: ?string,
  tweet_text: ?string,
  url: ?string
  }""")</pre></div><p>The <code class="literal">.dshape</code> function gives a record count and the schema:</p><div class="informalexample"><pre class="programlisting">twts_bz_df.dshape
Out[74]: 
dshape("""19 * {
  id: ?string,
  created_at: ?string,
  user_id: ?string,
  user_name: ?string,
  tweet_text: ?string,
  url: ?string
  }""")</pre></div><p>We can <a id="id178" class="indexterm"></a>print the Blaze <code class="literal">dataframe</code> content:</p><div class="informalexample"><pre class="programlisting">twts_bz_df.data
Out[75]:
id    created_at    user_id    user_name    tweet_text    url
1    598831111406510082    2015-05-14 12:43:57   14755521 raulsaeztapia    RT @pacoid: Great recap of @StrataConf EU in L...    http://www.mango-solutions.com/wp/2015/05/the-...
2    598831111406510082    2015-05-14 12:43:57    14755521 raulsaeztapia    RT @pacoid: Great recap of @StrataConf EU in L...    http://www.mango-solutions.com/wp/2015/05/the-...
... 
18   598782970082807808    2015-05-14 09:32:39    1377652806 embeddedcomputer.nl    RT @BigDataTechCon: Moving Rating Prediction w...    http://buff.ly/1QBpk8J
19   598777933730160640     2015-05-14 09:12:38   294862170    Ellen Friedman   I'm still on Euro time. If you are too check o...http://bit.ly/1Hfd0Xm</pre></div><p>We<a id="id179" class="indexterm"></a> extract the column <code class="literal">tweet_text</code> and take the unique values:</p><div class="informalexample"><pre class="programlisting">twts_bz_df.tweet_text.distinct()
Out[76]:
    tweet_text
0   RT @pacoid: Great recap of @StrataConf EU in L...
1   RT @alvaroagea: Simply @ApacheSpark http://t.c...
2   RT @PrabhaGana: What exactly is @ApacheSpark a...
3   RT @Ellen_Friedman: I'm still on Euro time. If...
4   RT @BigDataTechCon: Moving Rating Prediction w...
5   I'm still on Euro time. If you are too check o...</pre></div><p>We extract multiple columns <code class="literal">['id', 'user_name','tweet_text']</code> from the <code class="literal">dataframe</code> and take the unique records:</p><div class="informalexample"><pre class="programlisting">twts_bz_df[['id', 'user_name','tweet_text']].distinct()
Out[78]:
  id   user_name   tweet_text
0   598831111406510082   raulsaeztapia   RT @pacoid: Great recap of @StrataConf EU in L...
1   598808944719593472   raulsaeztapia   RT @alvaroagea: Simply @ApacheSpark http://t.c...
2   598796205091500032   John Humphreys   RT @PrabhaGana: What exactly is @ApacheSpark a...
3   598788561127735296   Leonardo D'Ambrosi   RT @Ellen_Friedman: I'm still on Euro time. If...
4   598785545557438464   Alexey Kosenkov   RT @Ellen_Friedman: I'm still on Euro time. If...
5   598782970082807808   embeddedcomputer.nl   RT @BigDataTechCon: Moving Rating Prediction w...
6   598777933730160640   Ellen Friedman   I'm still on Euro time. If you are too check o...</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec32"></a>Transferring data using Odo</h3></div></div></div><p>Odo is <a id="id180" class="indexterm"></a>a spin-off project of Blaze. Odo allows the interchange of data. Odo ensures the migration<a id="id181" class="indexterm"></a> of data across different<a id="id182" class="indexterm"></a> formats (CSV, JSON, HDFS, and more) and across different databases (SQL databases, MongoDB, and so on) using a very simple predicate:</p><div class="informalexample"><pre class="programlisting">Odo(source, target)</pre></div><p>To transfer to a database, the address is specified using a URL. For example, for a MongoDB database, it would look like this: </p><div class="informalexample"><pre class="programlisting">mongodb://username:password@hostname:port/database_name::collection_name</pre></div><p>Let's run some examples of using Odo. Here, we illustrate <code class="literal">odo</code> by reading a CSV file and creating a Blaze <code class="literal">dataframe</code>:</p><div class="informalexample"><pre class="programlisting">filepath   = csvFpath
filename   = csvFname
filesuffix = csvSuffix
twts_odo_df = Data('{0}/{1}.{2}'.format(filepath, filename, filesuffix))</pre></div><p>Count the number of records in the <code class="literal">dataframe</code>:</p><div class="informalexample"><pre class="programlisting">twts_odo_df.count()
Out[81]:
19</pre></div><p>Display the five initial records of the <code class="literal">dataframe</code>:</p><div class="informalexample"><pre class="programlisting">twts_odo_df.head(5)
Out[82]:
  id   created_at   user_id   user_name   tweet_text   url
0   598831111406510082   2015-05-14 12:43:57   14755521   raulsaeztapia   RT @pacoid: Great recap of @StrataConf EU in L...   http://www.mango-solutions.com/wp/2015/05/the-...
1   598831111406510082   2015-05-14 12:43:57   14755521   raulsaeztapia   RT @pacoid: Great recap of @StrataConf EU in L...   http://www.mango-solutions.com/wp/2015/05/the-...
2   598808944719593472   2015-05-14 11:15:52   14755521   raulsaeztapia   RT @alvaroagea: Simply @ApacheSpark http://t.c...   http://www.webex.com/ciscospark/
3   598808944719593472   2015-05-14 11:15:52   14755521   raulsaeztapia   RT @alvaroagea: Simply @ApacheSpark http://t.c...   http://sparkjava.com/
4   598808944719593472   2015-05-14 11:15:52   14755521   raulsaeztapia   RT @alvaroagea: Simply @ApacheSpark http://t.c...   https://www.sparkfun.com/</pre></div><p>Get<a id="id183" class="indexterm"></a> <code class="literal">dshape</code> information from the <code class="literal">dataframe</code>, which gives <a id="id184" class="indexterm"></a>us the number of records and the schema:</p><div class="informalexample"><pre class="programlisting">twts_odo_df.dshape
Out[83]:
dshape("var * {
  id: int64,
  created_at: ?datetime,
  user_id: int64,
  user_name: ?string,
  tweet_text: ?string,
  url: ?string
  }""")</pre></div><p>Save a processed Blaze <code class="literal">dataframe</code> into JSON:</p><div class="informalexample"><pre class="programlisting">odo(twts_odo_distinct_df, '{0}/{1}.{2}'.format(jsonFpath, jsonFname, jsonSuffix))
Out[92]:
&lt;odo.backends.json.JSONLines at 0x7f77f0abfc50&gt;</pre></div><p>Convert a JSON file to a CSV file:</p><div class="informalexample"><pre class="programlisting">odo('{0}/{1}.{2}'.format(jsonFpath, jsonFname, jsonSuffix), '{0}/{1}.{2}'.format(csvFpath, csvFname, csvSuffix))
Out[94]:
&lt;odo.backends.csv.CSV at 0x7f77f0abfe10&gt;</pre></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec26"></a>Exploring data using Spark SQL</h2></div></div><hr /></div><p>Spark SQL is a <a id="id185" class="indexterm"></a>relational query engine built on top<a id="id186" class="indexterm"></a> of <a id="id187" class="indexterm"></a>Spark Core. Spark SQL uses a <a id="id188" class="indexterm"></a>query optimizer called <span class="strong"><strong>Catalyst</strong></span>.</p><p>Relational queries can be expressed using SQL or HiveQL and executed against JSON, CSV, and various databases. Spark SQL gives us the full expressiveness of declarative programing <a id="id189" class="indexterm"></a>with Spark dataframes on top of <a id="id190" class="indexterm"></a>functional programming with RDDs.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec33"></a>Understanding Spark dataframes</h3></div></div></div><p>Here's a<a id="id191" class="indexterm"></a> tweet from <code class="literal">@bigdata</code> announcing Spark 1.3.0, the advent of Spark SQL and dataframes. It also highlights the various data sources in the lower part of the diagram. On the top part, we can notice R as the new language that will be gradually supported on top of Scala, Java, and Python. Ultimately, the Data Frame philosophy is pervasive between R, Python, and Spark.</p><div class="mediaobject"><img src="graphics/B03986_03_02.jpg" /></div><p>Spark <a id="id192" class="indexterm"></a>dataframes originate from SchemaRDDs. It combines RDD with a schema that can be inferred by Spark, if requested, when registering the dataframe. It allows us to query complex nested JSON data with plain SQL. Lazy evaluation, lineage, partitioning, and persistence apply to dataframes.</p><p>Let's query the data with Spark SQL, by first importing <code class="literal">SparkContext</code> and <code class="literal">SQLContext</code>:</p><div class="informalexample"><pre class="programlisting">from pyspark import SparkConf, SparkContext
from pyspark.sql import SQLContext, Row
In [95]:
sc
Out[95]:
&lt;pyspark.context.SparkContext at 0x7f7829581890&gt;
In [96]:
sc.master
Out[96]:
u'local[*]'
''In [98]:
# Instantiate Spark  SQL context
sqlc =  SQLContext(sc)</pre></div><p>We read in<a id="id193" class="indexterm"></a> the JSON file we saved with Odo:</p><div class="informalexample"><pre class="programlisting">twts_sql_df_01 = sqlc.jsonFile ("/home/an/spark/spark-1.3.0-bin-hadoop2.4/examples/AN_Spark/data/twtr15051401_distinct.json")
In [101]:
twts_sql_df_01.show()
created_at           id                 tweet_text           user_id    user_name          
2015-05-14T12:43:57Z 598831111406510082 RT @pacoid: Great... 14755521   raulsaeztapia      
2015-05-14T11:15:52Z 598808944719593472 RT @alvaroagea: S... 14755521   raulsaeztapia      
2015-05-14T10:25:15Z 598796205091500032 RT @PrabhaGana: W... 48695135   John Humphreys     
2015-05-14T09:54:52Z 598788561127735296 RT @Ellen_Friedma... 2385931712 Leonardo D'Ambrosi
2015-05-14T09:42:53Z 598785545557438464 RT @Ellen_Friedma... 461020977  Alexey Kosenkov    
2015-05-14T09:32:39Z 598782970082807808 RT @BigDataTechCo... 1377652806 embeddedcomputer.nl
2015-05-14T09:12:38Z 598777933730160640 I'm still on Euro... 294862170  Ellen Friedman     </pre></div><p>We print the schema of the Spark dataframe:</p><div class="informalexample"><pre class="programlisting">twts_sql_df_01.printSchema()
root
 |-- created_at: string (nullable = true)
 |-- id: long (nullable = true)
 |-- tweet_text: string (nullable = true)
 |-- user_id: long (nullable = true)
 |-- user_name: string (nullable = true)</pre></div><p>We select the <code class="literal">user_name</code> column from the dataframe:</p><div class="informalexample"><pre class="programlisting">twts_sql_df_01.select('user_name').show()
user_name          
raulsaeztapia      
raulsaeztapia      
John Humphreys     
Leonardo D'Ambrosi
Alexey Kosenkov    
embeddedcomputer.nl
Ellen Friedman     </pre></div><p>We register the dataframe as a table, so we can execute a SQL query on it:</p><div class="informalexample"><pre class="programlisting">twts_sql_df_01.registerAsTable('tweets_01')</pre></div><p>We<a id="id194" class="indexterm"></a> execute a SQL statement against the dataframe:</p><div class="informalexample"><pre class="programlisting">twts_sql_df_01_selection = sqlc.sql("SELECT * FROM tweets_01 WHERE user_name = 'raulsaeztapia'")
In [109]:
twts_sql_df_01_selection.show()
created_at           id                 tweet_text           user_id  user_name    
2015-05-14T12:43:57Z 598831111406510082 RT @pacoid: Great... 14755521 raulsaeztapia
2015-05-14T11:15:52Z 598808944719593472 RT @alvaroagea: S... 14755521 raulsaeztapia</pre></div><p>Let's process some more complex JSON; we read the original Twitter JSON file:</p><div class="informalexample"><pre class="programlisting">tweets_sqlc_inf = sqlc.jsonFile(infile)</pre></div><p>Spark SQL is able to infer the schema of a complex nested JSON file:</p><div class="informalexample"><pre class="programlisting">tweets_sqlc_inf.printSchema()
root
 |-- contributors: string (nullable = true)
 |-- coordinates: string (nullable = true)
 |-- created_at: string (nullable = true)
 |-- entities: struct (nullable = true)
 |    |-- hashtags: array (nullable = true)
 |    |    |-- element: struct (containsNull = true)
 |    |    |    |-- indices: array (nullable = true)
 |    |    |    |    |-- element: long (containsNull = true)
 |    |    |    |-- text: string (nullable = true)
 |    |-- media: array (nullable = true)
 |    |    |-- element: struct (containsNull = true)
 |    |    |    |-- display_url: string (nullable = true)
 |    |    |    |-- expanded_url: string (nullable = true)
 |    |    |    |-- id: long (nullable = true)
 |    |    |    |-- id_str: string (nullable = true)
 |    |    |    |-- indices: array (nullable = true)
... (snip) ...
|    |-- statuses_count: long (nullable = true)
 |    |-- time_zone: string (nullable = true)
 |    |-- url: string (nullable = true)
 |    |-- utc_offset: long (nullable = true)
 |    |-- verified: boolean (nullable = true)</pre></div><p>We extract<a id="id195" class="indexterm"></a> the key information of interest from the wall of data by selecting specific columns in the dataframe (in this case, <code class="literal">['created_at', 'id', 'text', 'user.id', 'user.name', 'entities.urls.expanded_url']</code>):</p><div class="informalexample"><pre class="programlisting">tweets_extract_sqlc = tweets_sqlc_inf[['created_at', 'id', 'text', 'user.id', 'user.name', 'entities.urls.expanded_url']].distinct()
In [145]:
tweets_extract_sqlc.show()
created_at           id                 text                 id         name                expanded_url        
Thu May 14 09:32:... 598782970082807808 RT @BigDataTechCo... 1377652806 embeddedcomputer.nl ArrayBuffer(http:...
Thu May 14 12:43:... 598831111406510082 RT @pacoid: Great... 14755521   raulsaeztapia       ArrayBuffer(http:...
Thu May 14 12:18:... 598824733086523393 @rabbitonweb spea... 

...   
Thu May 14 12:28:... 598827171168264192 RT @baandrzejczak... 20909005   Paweł Szulc         ArrayBuffer()       </pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec34"></a>Understanding the Spark SQL query optimizer</h3></div></div></div><p>We<a id="id196" class="indexterm"></a> execute a SQL statement against the dataframe:</p><div class="informalexample"><pre class="programlisting">tweets_extract_sqlc_sel = sqlc.sql("SELECT * from Tweets_xtr_001 WHERE name='raulsaeztapia'")</pre></div><p>We get a detailed view of the query plans executed by Spark SQL:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Parsed logical plan</p></li><li style="list-style-type: disc"><p>Analyzed logical plan</p></li><li style="list-style-type: disc"><p>Optimized logical plan</p></li><li style="list-style-type: disc"><p>Physical plan</p></li></ul></div><p>The query plan uses Spark SQL's Catalyst optimizer. In order to generate the compiled bytecode from the query parts, the Catalyst optimizer runs through logical plan parsing and optimization followed by physical plan evaluation and optimization based on cost.</p><p>This is illustrated in the following tweet:</p><div class="mediaobject"><img src="graphics/B03986_03_03.jpg" /></div><p>Looking<a id="id197" class="indexterm"></a> back at our code, we call the <code class="literal">.explain</code> function on the Spark SQL query we just executed, and it delivers the full details of the steps taken by the Catalyst optimizer in order to assess and optimize the logical plan and the physical plan and get to the result RDD:</p><div class="informalexample"><pre class="programlisting">tweets_extract_sqlc_sel.explain(extended = True)
== Parsed Logical Plan ==
'Project [*]
 'Filter ('name = raulsaeztapia)'name'  'UnresolvedRelation' [Tweets_xtr_001], None
== Analyzed Logical Plan ==
Project [created_at#7,id#12L,text#27,id#80L,name#81,expanded_url#82]
 Filter (name#81 = raulsaeztapia)
  Distinct 
   Project [created_at#7,id#12L,text#27,user#29.id AS id#80L,user#29.name AS name#81,entities#8.urls.expanded_url AS expanded_url#82]
    Relation[contributors#5,coordinates#6,created_at#7,entities#8,favorite_count#9L,favorited#10,geo#11,id#12L,id_str#13,in_reply_to_screen_name#14,in_reply_to_status_id#15,in_reply_to_status_id_str#16,in_reply_to_user_id#17L,in_reply_to_user_id_str#18,lang#19,metadata#20,place#21,possibly_sensitive#22,retweet_count#23L,retweeted#24,retweeted_status#25,source#26,text#27,truncated#28,user#29] JSONRelation(/home/an/spark/spark-1.3.0-bin-hadoop2.4/examples/AN_Spark/data/twtr15051401.json,1.0,None)
== Optimized Logical Plan ==
Filter (name#81 = raulsaeztapia)
 Distinct 
  Project [created_at#7,id#12L,text#27,user#29.id AS id#80L,user#29.name AS name#81,entities#8.urls.expanded_url AS expanded_url#82]
   Relation[contributors#5,coordinates#6,created_at#7,entities#8,favorite_count#9L,favorited#10,geo#11,id#12L,id_str#13,in_reply_to_screen_name#14,in_reply_to_status_id#15,in_reply_to_status_id_str#16,in_reply_to_user_id#17L,in_reply_to_user_id_str#18,lang#19,metadata#20,place#21,possibly_sensitive#22,retweet_count#23L,retweeted#24,retweeted_status#25,source#26,text#27,truncated#28,user#29] JSONRelation(/home/an/spark/spark-1.3.0-bin-hadoop2.4/examples/AN_Spark/data/twtr15051401.json,1.0,None)
== Physical Plan ==
Filter (name#81 = raulsaeztapia)
 Distinct false
  Exchange (HashPartitioning [created_at#7,id#12L,text#27,id#80L,name#81,expanded_url#82], 200)
   Distinct true
    Project [created_at#7,id#12L,text#27,user#29.id AS id#80L,user#29.name AS name#81,entities#8.urls.expanded_url AS expanded_url#82]
     PhysicalRDD [contributors#5,coordinates#6,created_at#7,entities#8,favorite_count#9L,favorited#10,geo#11,id#12L,id_str#13,in_reply_to_screen_name#14,in_reply_to_status_id#15,in_reply_to_status_id_str#16,in_reply_to_user_id#17L,in_reply_to_user_id_str#18,lang#19,metadata#20,place#21,possibly_sensitive#22,retweet_count#23L,retweeted#24,retweeted_status#25,source#26,text#27,truncated#28,user#29], MapPartitionsRDD[165] at map at JsonRDD.scala:41
Code Generation: false
== RDD ==</pre></div><p>Finally, here's the <a id="id198" class="indexterm"></a>result of the query:</p><div class="informalexample"><pre class="programlisting">tweets_extract_sqlc_sel.show()
created_at           id                 text                 id       name          expanded_url        
Thu May 14 12:43:... 598831111406510082 RT @pacoid: Great... 14755521 raulsaeztapia ArrayBuffer(http:...
Thu May 14 11:15:... 598808944719593472 RT @alvaroagea: S... 14755521 raulsaeztapia ArrayBuffer(http:...
In [148]:</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec35"></a>Loading and processing CSV files with Spark SQL</h3></div></div></div><p>We will <a id="id199" class="indexterm"></a>use the Spark package <code class="literal">spark-csv_2.11:1.2.0</code>. The command to be used to launch PySpark with the IPython Notebook <a id="id200" class="indexterm"></a>and the <code class="literal">spark-csv</code> package should explicitly state the <code class="literal">–packages</code> argument:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ IPYTHON_OPTS='notebook' /home/an/spark/spark-1.5.0-bin-hadoop2.6/bin/pyspark --packages com.databricks:spark-csv_2.11:1.2.0</strong></span>
</pre></div><p>This will trigger the following output; we can see that the <code class="literal">spark-csv</code> package is installed with all its dependencies:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>an@an-VB:~/spark/spark-1.5.0-bin-hadoop2.6/examples/AN_Spark$ IPYTHON_OPTS='notebook' /home/an/spark/spark-1.5.0-bin-hadoop2.6/bin/pyspark --packages com.databricks:spark-csv_2.11:1.2.0</strong></span>
</pre></div><div class="informalexample"><pre class="programlisting">... (snip) ...
Ivy Default Cache set to: /home/an/.ivy2/cache
The jars for the packages stored in: /home/an/.ivy2/jars
:: loading settings :: url = jar:file:/home/an/spark/spark-1.5.0-bin-hadoop2.6/lib/spark-assembly-1.5.0-hadoop2.6.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
com.databricks#spark-csv_2.11 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0
  confs: [default]
  found com.databricks#spark-csv_2.11;1.2.0 in central
  found org.apache.commons#commons-csv;1.1 in central
  found com.univocity#univocity-parsers;1.5.1 in central
:: resolution report :: resolve 835ms :: artifacts dl 48ms
  :: modules in use:
  com.databricks#spark-csv_2.11;1.2.0 from central in [default]
  com.univocity#univocity-parsers;1.5.1 from central in [default]
  org.apache.commons#commons-csv;1.1 from central in [default]
  ----------------------------------------------------------------
  |               |          modules            ||   artifacts   |
  |    conf     | number| search|dwnlded|evicted|| number|dwnlded|
  ----------------------------------------------------------------
  |    default     |   3   |   0   |   0   |   0   ||   3   |   0   
  ----------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent
  confs: [default]
  0 artifacts copied, 3 already retrieved (0kB/45ms)</pre></div><p>We are now ready to load our <code class="literal">csv</code> file and process it. Let's first import the <code class="literal">SQLContext</code>:</p><div class="informalexample"><pre class="programlisting">#
# Read csv in a Spark DF
#
sqlContext = SQLContext(sc)
spdf_in = sqlContext.read.format('com.databricks.spark.csv')\
                                    .options(delimiter=";").options(header="true")\
                                    .options(header='true').load(csv_in)</pre></div><p>We access<a id="id201" class="indexterm"></a> the schema of the dataframe created <a id="id202" class="indexterm"></a>from the loaded <code class="literal">csv</code>:</p><div class="informalexample"><pre class="programlisting">In [10]:
spdf_in.printSchema()
root
 |-- : string (nullable = true)
 |-- id: string (nullable = true)
 |-- created_at: string (nullable = true)
 |-- user_id: string (nullable = true)
 |-- user_name: string (nullable = true)
 |-- tweet_text: string (nullable = true)</pre></div><p>We check the columns of the dataframe:</p><div class="informalexample"><pre class="programlisting">In [12]:
spdf_in.columns
Out[12]:
['', 'id', 'created_at', 'user_id', 'user_name', 'tweet_text']</pre></div><p>We introspect the dataframe content:</p><div class="informalexample"><pre class="programlisting">In [13]:
spdf_in.show()
+---+------------------+--------------------+----------+------------------+--------------------+
|   |                id|          created_at|   user_id|         user_name|          tweet_text|
+---+------------------+--------------------+----------+------------------+--------------------+
|  0|638830426971181057|Tue Sep 01 21:46:...|3276255125|     True Equality|ernestsgantt: Bey...|
|  1|638830426727911424|Tue Sep 01 21:46:...|3276255125|     True Equality|ernestsgantt: Bey...|
|  2|638830425402556417|Tue Sep 01 21:46:...|3276255125|     True Equality|ernestsgantt: Bey...|
... (snip) ...
| 41|638830280988426250|Tue Sep 01 21:46:...| 951081582|      Jack Baldwin|RT @cloudaus: We ...|
| 42|638830276626399232|Tue Sep 01 21:46:...|   6525302|Masayoshi Nakamura|PynamoDB使いやすいです  |
+---+------------------+--------------------+----------+------------------+--------------------+
only showing top 20 rows</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec36"></a>Querying MongoDB from Spark SQL</h3></div></div></div><p>There<a id="id203" class="indexterm"></a> are two major ways to interact with MongoDB from Spark: the first is through the Hadoop MongoDB connector, and the second one is directly from Spark to MongoDB.</p><p>The first approach to interact with MongoDB from Spark is to set up a Hadoop environment <a id="id204" class="indexterm"></a>and query through the Hadoop MongoDB connector. The connector details are hosted on GitHub at <a class="ulink" href="https://github.com/mongodb/mongo-hadoop/wiki/Spark-Usage" target="_blank">https://github.com/mongodb/mongo-hadoop/wiki/Spark-Usage</a>. An actual use case is<a id="id205" class="indexterm"></a> described in the series of blog posts from MongoDB:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="emphasis"><em>Using MongoDB with Hadoop &amp; Spark: Part 1 - Introduction &amp; Setup</em></span> (<a class="ulink" href="https://www.mongodb.com/blog/post/using-mongodb-hadoop-spark-part-1-introduction-setup" target="_blank">https://www.mongodb.com/blog/post/using-mongodb-hadoop-spark-part-1-introduction-setup</a>)</p></li><li style="list-style-type: disc"><p>
<span class="emphasis"><em>Using MongoDB with Hadoop and Spark: Part 2 - Hive Example</em></span> (<a class="ulink" href="https://www.mongodb.com/blog/post/using-mongodb-hadoop-spark-part-2-hive-example" target="_blank">https://www.mongodb.com/blog/post/using-mongodb-hadoop-spark-part-2-hive-example</a>)</p></li><li style="list-style-type: disc"><p>
<span class="emphasis"><em>Using MongoDB with Hadoop &amp; Spark: Part 3 - Spark Example &amp; Key Takeaways</em></span> (<a class="ulink" href="https://www.mongodb.com/blog/post/using-mongodb-hadoop-spark-part-3-spark-example-key-takeaways" target="_blank">https://www.mongodb.com/blog/post/using-mongodb-hadoop-spark-part-3-spark-example-key-takeaways</a>)</p></li></ul></div><p>Setting up a full Hadoop environment is bit elaborate. We will favor the second approach. We will use the <code class="literal">spark-mongodb</code> connector developed and maintained by Stratio. We are using the <code class="literal">Stratio spark-mongodb</code> package hosted at <code class="literal">spark.packages.org</code>. The packages information and version can be found in <code class="literal">spark.packages.org</code>:</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note02"></a>Note</h3><p>
<span class="strong"><strong>Releases</strong></span>
</p><p>Version: 0.10.1 ( 8263c8 | zip | jar ) / Date: 2015-11-18 / License: Apache-2.0 / Scala <a id="id206" class="indexterm"></a>version: 2.10</p><p>(<a class="ulink" href="http://spark-packages.org/package/Stratio/spark-mongodb" target="_blank">http://spark-packages.org/package/Stratio/spark-mongodb</a>)</p></div><p>The command to launch PySpark with the IPython Notebook and the <code class="literal">spark-mongodb</code> package should explicitly state the packages argument:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ IPYTHON_OPTS='notebook' /home/an/spark/spark-1.5.0-bin-hadoop2.6/bin/pyspark --packages com.stratio.datasource:spark-mongodb_2.10:0.10.1</strong></span>
</pre></div><p>This will trigger the following output; we can see that the <code class="literal">spark-mongodb</code> package is installed with all its dependencies:</p><div class="informalexample"><pre class="programlisting">an@an-VB:~/spark/spark-1.5.0-bin-hadoop2.6/examples/AN_Spark$ IPYTHON_OPTS='notebook' /home/an/spark/spark-1.5.0-bin-hadoop2.6/bin/pyspark --packages com.stratio.datasource:spark-mongodb_2.10:0.10.1
... (snip) ... 
Ivy Default Cache set to: /home/an/.ivy2/cache
The jars for the packages stored in: /home/an/.ivy2/jars
:: loading settings :: url = jar:file:/home/an/spark/spark-1.5.0-bin-hadoop2.6/lib/spark-assembly-1.5.0-hadoop2.6.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
com.stratio.datasource#spark-mongodb_2.10 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0
  confs: [default]
  found com.stratio.datasource#spark-mongodb_2.10;0.10.1 in central
[W 22:10:50.910 NotebookApp] Timeout waiting for kernel_info reply from 764081d3-baf9-4978-ad89-7735e6323cb6
  found org.mongodb#casbah-commons_2.10;2.8.0 in central
  found com.github.nscala-time#nscala-time_2.10;1.0.0 in central
  found joda-time#joda-time;2.3 in central
  found org.joda#joda-convert;1.2 in central
  found org.slf4j#slf4j-api;1.6.0 in central
  found org.mongodb#mongo-java-driver;2.13.0 in central
  found org.mongodb#casbah-query_2.10;2.8.0 in central
  found org.mongodb#casbah-core_2.10;2.8.0 in central
downloading https://repo1.maven.org/maven2/com/stratio/datasource/spark-mongodb_2.10/0.10.1/spark-mongodb_2.10-0.10.1.jar ...
  [SUCCESSFUL ] com.stratio.datasource#spark-mongodb_2.10;0.10.1!spark-mongodb_2.10.jar (3130ms)
downloading https://repo1.maven.org/maven2/org/mongodb/casbah-commons_2.10/2.8.0/casbah-commons_2.10-2.8.0.jar ...
  [SUCCESSFUL ] org.mongodb#casbah-commons_2.10;2.8.0!casbah-commons_2.10.jar (2812ms)
downloading https://repo1.maven.org/maven2/org/mongodb/casbah-query_2.10/2.8.0/casbah-query_2.10-2.8.0.jar ...
  [SUCCESSFUL ] org.mongodb#casbah-query_2.10;2.8.0!casbah-query_2.10.jar (1432ms)
downloading https://repo1.maven.org/maven2/org/mongodb/casbah-core_2.10/2.8.0/casbah-core_2.10-2.8.0.jar ...
  [SUCCESSFUL ] org.mongodb#casbah-core_2.10;2.8.0!casbah-core_2.10.jar (2785ms)
downloading https://repo1.maven.org/maven2/com/github/nscala-time/nscala-time_2.10/1.0.0/nscala-time_2.10-1.0.0.jar ...
  [SUCCESSFUL ] com.github.nscala-time#nscala-time_2.10;1.0.0!nscala-time_2.10.jar (2725ms)
downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.6.0/slf4j-api-1.6.0.jar ...
  [SUCCESSFUL ] org.slf4j#slf4j-api;1.6.0!slf4j-api.jar (371ms)
downloading https://repo1.maven.org/maven2/org/mongodb/mongo-java-driver/2.13.0/mongo-java-driver-2.13.0.jar ...
  [SUCCESSFUL ] org.mongodb#mongo-java-driver;2.13.0!mongo-java-driver.jar (5259ms)
downloading https://repo1.maven.org/maven2/joda-time/joda-time/2.3/joda-time-2.3.jar ...
  [SUCCESSFUL ] joda-time#joda-time;2.3!joda-time.jar (6949ms)
downloading https://repo1.maven.org/maven2/org/joda/joda-convert/1.2/joda-convert-1.2.jar ...
  [SUCCESSFUL ] org.joda#joda-convert;1.2!joda-convert.jar (548ms)
:: resolution report :: resolve 11850ms :: artifacts dl 26075ms
  :: modules in use:
  com.github.nscala-time#nscala-time_2.10;1.0.0 from central in [default]
  com.stratio.datasource#spark-mongodb_2.10;0.10.1 from central in [default]
  joda-time#joda-time;2.3 from central in [default]
  org.joda#joda-convert;1.2 from central in [default]
  org.mongodb#casbah-commons_2.10;2.8.0 from central in [default]
  org.mongodb#casbah-core_2.10;2.8.0 from central in [default]
  org.mongodb#casbah-query_2.10;2.8.0 from central in [default]
  org.mongodb#mongo-java-driver;2.13.0 from central in [default]
  org.slf4j#slf4j-api;1.6.0 from central in [default]
  ---------------------------------------------------------------------
  |                  |            modules            ||   artifacts   |
  |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
  ---------------------------------------------------------------------
  |      default     |   9   |   9   |   9   |   0   ||   9   |   9   |
  ---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent
  confs: [default]
  9 artifacts copied, 0 already retrieved (2335kB/51ms)
... (snip) ... </pre></div><p>We<a id="id207" class="indexterm"></a> are now ready to query MongoDB on <code class="literal">localhost:27017</code> from the collection <code class="literal">twtr01_coll</code> in the database <code class="literal">twtr01_db</code>.</p><p>We first import the <code class="literal">SQLContext</code>:</p><div class="informalexample"><pre class="programlisting">In [5]:
from pyspark.sql import SQLContext
sqlContext.sql("CREATE TEMPORARY TABLE tweet_table USING com.stratio.datasource.mongodb OPTIONS (host 'localhost:27017', database 'twtr01_db', collection 'twtr01_coll')")
sqlContext.sql("SELECT * FROM tweet_table where id=598830778269769728 ").collect()</pre></div><p>Here's the<a id="id208" class="indexterm"></a> output of our query:</p><div class="informalexample"><pre class="programlisting">Out[5]:
[Row(text=u'@spark_io is now @particle - awesome news - now I can enjoy my Particle Cores/Photons + @sparkfun sensors + @ApacheSpark analytics :-)', _id=u'55aa640fd770871cba74cb88', contributors=None, retweeted=False, user=Row(contributors_enabled=False, created_at=u'Mon Aug 25 14:01:26 +0000 2008', default_profile=True, default_profile_image=False, description=u'Building open source tools for and teaching enterprise software developers', entities=Row(description=Row(urls=[]), url=Row(urls=[Row(url=u'http://t.co/TSHp13EWeu', indices=[0, 22], 

... (snip) ...

 9], name=u'Spark is Particle', screen_name=u'spark_io'), Row(id=487010011, id_str=u'487010011', indices=[17, 26], name=u'Particle', screen_name=u'particle'), Row(id=17877351, id_str=u'17877351', indices=[88, 97], name=u'SparkFun Electronics', screen_name=u'sparkfun'), Row(id=1551361069, id_str=u'1551361069', indices=[108, 120], name=u'Apache Spark', screen_name=u'ApacheSpark')]), is_quote_status=None, lang=u'en', quoted_status_id_str=None, quoted_status_id=None, created_at=u'Thu May 14 12:42:37 +0000 2015', retweeted_status=None, truncated=False, place=None, id=598830778269769728, in_reply_to_user_id=3187046084, retweet_count=0, in_reply_to_status_id=None, in_reply_to_screen_name=u'spark_io', in_reply_to_user_id_str=u'3187046084', source=u'&lt;a href="http://twitter.com" rel="nofollow"&gt;Twitter Web Client&lt;/a&gt;', id_str=u'598830778269769728', coordinates=None, metadata=Row(iso_language_code=u'en', result_type=u'recent'), quoted_status=None)]
#</pre></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec27"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we harvested data from Twitter. Once the data was acquired, we explored the information using <code class="literal">Continuum.io's</code> Blaze and Odo libraries. Spark SQL is an important module for interactive data exploration, analysis, and transformation, leveraging the Spark dataframe datastructure. The dataframe concept originates from R and then was adopted by Python Pandas with great success. The dataframe is the workhorse of the data scientist. The combination of Spark SQL and dataframe creates a powerful engine for data processing.</p><p>We are now gearing up for extracting the insights from the datasets using machine learning from Spark MLlib.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch04"></a>Chapter 4. Learning from Data Using Spark</h2></div></div></div><p>As we have laid the foundation for data to be harvested in the previous chapter, we are now ready to learn from the data. Machine learning is about drawing insights from data. Our <a id="id209" class="indexterm"></a>objective is to give an overview of the Spark <span class="strong"><strong>MLlib</strong></span> (short for <span class="strong"><strong>Machine Learning library</strong></span>) and apply the appropriate algorithms to our dataset in order to derive insights. From the Twitter dataset, we will be applying an unsupervised clustering algorithm in order to distinguish between Apache Spark-relevant tweets versus the rest. We have as initial input a mixed bag of tweets. We first need to preprocess the data in order to extract the relevant features, then apply the machine learning algorithm to our dataset, and finally evaluate the results and the performance of our model.</p><p>In this chapter, we will cover the following points:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Providing an overview of the Spark MLlib module with its algorithms and the typical machine learning workflow.</p></li><li style="list-style-type: disc"><p>Preprocessing the Twitter harvested dataset to extract the relevant features, applying an unsupervised clustering algorithm to identify <span class="emphasis"><em>Apache Spark</em></span>-relevant tweets. Then, evaluating the model and the results obtained.</p></li><li style="list-style-type: disc"><p>Describing the Spark machine learning pipeline.</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec28"></a>Contextualizing Spark MLlib in the app architecture</h2></div></div><hr /></div><p>Let's first <a id="id210" class="indexterm"></a>contextualize the focus of this chapter on data-intensive app architecture. We will concentrate our attention on the analytics layer and more precisely machine learning. This will serve as a foundation for streaming apps as we want to apply the learning from the batch processing of data as inference rules for the streaming analysis.</p><p>The following diagram sets the context of the chapter's focus, highlighting the machine learning module within the analytics layer while using tools for exploratory data analysis, Spark SQL, and Pandas.</p><div class="mediaobject"><img src="graphics/B03986_04_01.jpg" /></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec29"></a>Classifying Spark MLlib algorithms</h2></div></div><hr /></div><p>Spark MLlib is a<a id="id211" class="indexterm"></a> rapidly evolving module of Spark with new algorithms added with each release of Spark.</p><p>The following diagram provides a high-level overview of Spark MLlib algorithms grouped in the traditional broad machine learning techniques and following the categorical or continuous nature of the data:</p><div class="mediaobject"><img src="graphics/B03986_04_02.jpg" /></div><p>We categorize the Spark MLlib algorithms in two columns, categorical or continuous, depending on the type of data. We distinguish between data that is categorical or more qualitative in nature versus continuous data, which is quantitative in nature. An example of qualitative data is predicting the weather; given the atmospheric pressure, the temperature, and the presence and type of clouds, the weather will be sunny, dry, rainy, or overcast. These are discrete values. On the other hand, let's say we want to predict house prices, given the location, square meterage, and the number of beds; the real estate value can be predicted using linear regression. In this case, we are talking about continuous or quantitative values.</p><p>The horizontal grouping reflects the types of machine learning method used. Unsupervised versus supervised machine learning techniques are dependent on whether the training data is labeled. In an unsupervised learning challenge, no labels are given to the learning algorithm. The goal is to find the hidden structure in its input. In the case of supervised learning, the data is labeled. The focus is on making predictions using regression if the data is continuous or classification if the data is categorical.</p><p>An important category of machine learning is recommender systems, which leverage collaborative filtering techniques. The Amazon web store and Netflix have very powerful recommender systems powering their recommendations.</p><p>
<span class="strong"><strong>Stochastic Gradient Descent</strong></span> is<a id="id212" class="indexterm"></a> one of the machine learning optimization techniques that is well suited for Spark distributed computation.</p><p>For<a id="id213" class="indexterm"></a> processing large amounts of text, Spark offers crucial <a id="id214" class="indexterm"></a>libraries for feature extraction and transformation such as <span class="strong"><strong>TF-IDF</strong></span> (short for <span class="strong"><strong>Term Frequency – Inverse Document Frequency</strong></span>), Word2Vec, standard scaler, and normalizer.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec37"></a>Supervised and unsupervised learning</h3></div></div></div><p>We delve<a id="id215" class="indexterm"></a> more deeply here in to the traditional<a id="id216" class="indexterm"></a> machine learning algorithms offered by Spark MLlib. We distinguish between supervised and unsupervised learning depending on whether the data is labeled. We distinguish between categorical or continuous depending on whether the data is discrete or continuous.</p><p>The following diagram explains the Spark MLlib supervised and unsupervised machine learning algorithms and preprocessing techniques:</p><div class="mediaobject"><img src="graphics/B03986_04_03.jpg" /></div><p>The following supervised and unsupervised MLlib algorithms and preprocessing techniques are currently available in Spark:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Clustering</strong></span>: This is an unsupervised machine learning technique where the data is<a id="id217" class="indexterm"></a> not labeled. The aim is to extract structure from the data:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>K-Means</strong></span>: This<a id="id218" class="indexterm"></a> partitions the data in K distinct clusters</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Gaussian Mixture</strong></span>: Clusters are assigned based on the maximum<a id="id219" class="indexterm"></a> posterior probability of the component</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Power Iteration Clustering (PIC)</strong></span>: This <a id="id220" class="indexterm"></a>groups vertices of a graph based on pairwise edge similarities</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Latent Dirichlet Allocation</strong></span> (<span class="strong"><strong>LDA</strong></span>): This<a id="id221" class="indexterm"></a> is used to group collections of text documents into topics</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Streaming K-Means</strong></span>: This<a id="id222" class="indexterm"></a> means clusters dynamically streaming data using a windowing function on the incoming data</p></li></ul></div></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Dimensionality Reduction</strong></span>: This aims to reduce the number of features under <a id="id223" class="indexterm"></a>consideration. Essentially, this<a id="id224" class="indexterm"></a> reduces noise in the data and focuses on the key features:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Singular Value Decomposition</strong></span> (<span class="strong"><strong>SVD</strong></span>): This<a id="id225" class="indexterm"></a> breaks the matrix that contains the data into simpler meaningful pieces. It factorizes the initial matrix into three matrices.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Principal Component Analysis</strong></span> (<span class="strong"><strong>PCA</strong></span>): This <a id="id226" class="indexterm"></a>approximates a high dimensional dataset with a low dimensional sub space.</p></li></ul></div></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Regression and Classification</strong></span>: Regression predicts output values using labeled training data, while<a id="id227" class="indexterm"></a> Classification groups the results into classes. Classification has dependent variables that are categorical or unordered whilst Regression has dependent variables that are continuous and ordered:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Linear Regression Models</strong></span> (linear regression, logistic regression, and support vector machines): Linear regression algorithms<a id="id228" class="indexterm"></a> can be expressed as convex optimization problems that aim to minimize an objective function based on a vector of weight variables. The objective function controls the complexity of the model through the regularized part of the function and the error of the model through the loss part of the function.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Naive Bayes</strong></span>: This makes predictions based on the conditional probability <a id="id229" class="indexterm"></a>distribution of a label given an observation. It assumes that features are mutually independent of each other.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Decision Trees</strong></span>: This performs recursive binary partitioning of the feature <a id="id230" class="indexterm"></a>space. The<a id="id231" class="indexterm"></a> information gain at the tree node level is maximized in order to determine the best split for the partition.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Ensembles of trees</strong></span> (Random Forests and Gradient-Boosted Trees): Tree ensemble algorithms combine base decision tree models in order to build a performant model. They are intuitive and <a id="id232" class="indexterm"></a>very successful <a id="id233" class="indexterm"></a>for classification and regression tasks.</p></li></ul></div></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Isotonic Regression</strong></span>: This<a id="id234" class="indexterm"></a> minimizes the mean squared error between given data<a id="id235" class="indexterm"></a> and observed responses.</p></li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec38"></a>Additional learning algorithms</h3></div></div></div><p>Spark MLlib<a id="id236" class="indexterm"></a> offers more algorithms than the supervised and unsupervised learning ones. We have broadly three more additional types of machine learning methods: recommender systems, optimization algorithms, and feature extraction.</p><div class="mediaobject"><img src="graphics/B03986_04_04.jpg" /></div><p>The following <a id="id237" class="indexterm"></a>additional MLlib algorithms are currently available in Spark:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Collaborative filtering</strong></span>: This is the basis for recommender systems. It creates <a id="id238" class="indexterm"></a>a user-item association matrix and aims to fill the gaps. Based on other users and items along with their ratings, it recommends an item that the target user has no ratings for. In distributed computing, one of the most successful algorithms is <span class="strong"><strong>ALS</strong></span> (short for <span class="strong"><strong>Alternating Least Square</strong></span>):</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Alternating Least Squares</strong></span>: This matrix factorization technique incorporates implicit feedback, temporal effects, and confidence levels. It decomposes the large user item matrix into a lower dimensional user and item factors. It minimizes a quadratic loss function by fixing alternatively its factors.</p></li></ul></div></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Feature extraction and transformation</strong></span>: These are essential techniques for large text document processing. It<a id="id239" class="indexterm"></a> includes<a id="id240" class="indexterm"></a> the following techniques:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Term Frequency</strong></span>: Search engines use TF-IDF to score and rank document relevance in a vast corpus. It is also used in machine learning to determine the importance of a word in a document or corpus. Term frequency statistically determines the weight of a term relative to its frequency in the corpus. Term frequency on its own can be misleading as it overemphasizes words such as <span class="emphasis"><em>the</em></span>, <span class="emphasis"><em>of</em></span>, or <span class="emphasis"><em>and</em></span> that give little information. Inverse Document Frequency provides the specificity or the measure of the amount of information, whether the term is rare or common across all documents in the corpus.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Word2Vec</strong></span>: This includes two models, <span class="strong"><strong>Skip-Gram</strong></span> and <span class="strong"><strong>Continuous Bag of Word</strong></span>. The Skip-Gram predicts neighboring words given a word, based on sliding windows of words, while Continuous Bag of Words predicts the current word given the neighboring words.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Standard Scaler</strong></span>: As part of preprocessing, the dataset must often be standardized by mean removal and variance scaling. We compute the mean and standard deviation on the training data and apply the same transformation to the test data.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Normalizer</strong></span>: We scale the samples to have unit norm. It is useful for quadratic forms such as the dot product or kernel methods.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Feature selection</strong></span>: This reduces the dimensionality of the vector space by selecting the most relevant features for the model.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Chi-Square Selector</strong></span>: This is a statistical method to measure the independence of two events.</p></li></ul></div></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Optimization</strong></span>: These specific Spark MLlib optimization algorithms focus on various<a id="id241" class="indexterm"></a> techniques of gradient descent. Spark provides very efficient implementation of gradient descent on a distributed cluster of machines. It looks for the local minima by iteratively going down the steepest descent. It is compute-intensive as it iterates through all the data available:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Stochastic Gradient Descent</strong></span>: We minimize an objective function that is the sum of differentiable functions. Stochastic Gradient Descent<a id="id242" class="indexterm"></a> uses only a sample of the training data in order to update a parameter in a particular iteration. It is used for large-scale and sparse machine learning problems such as text classification.</p></li></ul></div></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Limited-memory BFGS</strong></span> (<span class="strong"><strong>L-BFGS</strong></span>): As<a id="id243" class="indexterm"></a> the name says, L-BFGS uses limited memory and suits the distributed optimization algorithm implementation of Spark MLlib.</p></li></ul></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec30"></a>Spark MLlib data types</h2></div></div><hr /></div><p>MLlib<a id="id244" class="indexterm"></a> supports four essential data types: <span class="strong"><strong>local vector</strong></span>, <span class="strong"><strong>labeled point</strong></span>, <span class="strong"><strong>local matrix</strong></span>, and <span class="strong"><strong>distributed matrix</strong></span>. These data types are widely used in Spark MLlib algorithms:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Local vector</strong></span>: This <a id="id245" class="indexterm"></a>resides in a single machine. It can be dense or sparse:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Dense vector is a traditional array of doubles. An example of dense vector is <code class="literal">[5.0, 0.0, 1.0, 7.0]</code>.</p></li><li style="list-style-type: disc"><p>Sparse vector uses integer indices and double values. So the sparse representation of the vector <code class="literal">[5.0, 0.0, 1.0, 7.0]</code> would be <code class="literal">(4, [0, 2, 3], [5.0, 1.0, 7.0])</code>, where represent the dimension of the vector.</p><p>Here's an example of local vector in PySpark:</p><div class="informalexample"><pre class="programlisting">import numpy as np
import scipy.sparse as sps
from pyspark.mllib.linalg import Vectors

# NumPy array for dense vector.
dvect1 = np.array([5.0, 0.0, 1.0, 7.0])
# Python list for dense vector.
dvect2 = [5.0, 0.0, 1.0, 7.0]
# SparseVector creation
svect1 = Vectors.sparse(4, [0, 2, 3], [5.0, 1.0, 7.0])
# Sparse vector using a single-column SciPy csc_matrix
svect2 = sps.csc_matrix((np.array([5.0, 1.0, 7.0]), np.array([0, 2, 3])), shape = (4, 1))</pre></div></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Labeled point</strong></span>. A labeled point is a dense or sparse vector with a label used in supervised <a id="id246" class="indexterm"></a>learning. In the case of binary labels, 0.0 represents the negative label whilst 1.0 represents the positive value.</p><p>Here's an example of a labeled point in PySpark:</p><div class="informalexample"><pre class="programlisting">from pyspark.mllib.linalg import SparseVector
from pyspark.mllib.regression import LabeledPoint

# Labeled point with a positive label and a dense feature vector.
lp_pos = LabeledPoint(1.0, [5.0, 0.0, 1.0, 7.0])

# Labeled point with a negative label and a sparse feature vector.
lp_neg = LabeledPoint(0.0, SparseVector(4, [0, 2, 3], [5.0, 1.0, 7.0]))</pre></div></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Local Matrix</strong></span>: This local matrix resides in a single machine with integer-type indices<a id="id248" class="indexterm"></a> and values of type double.</p><p>Here's an example of a local matrix in PySpark:</p><div class="informalexample"><pre class="programlisting">from pyspark.mllib.linalg import Matrix, Matrices

# Dense matrix ((1.0, 2.0, 3.0), (4.0, 5.0, 6.0))
dMatrix = Matrices.dense(2, 3, [1, 2, 3, 4, 5, 6])

# Sparse matrix ((9.0, 0.0), (0.0, 8.0), (0.0, 6.0))
sMatrix = Matrices.sparse(3, 2, [0, 1, 3], [0, 2, 1], [9, 6, 8])</pre></div></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Distributed Matrix</strong></span>: Leveraging the distributed mature of the RDD, distributed <a id="id249" class="indexterm"></a>matrices can be shared in a cluster of machines. We distinguish four distributed matrix types: <code class="literal">RowMatrix</code>, <code class="literal">IndexedRowMatrix</code>, <code class="literal">CoordinateMatrix</code>, and <code class="literal">BlockMatrix</code>:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<code class="literal">RowMatrix</code>: This takes an RDD of vectors and creates a distributed matrix of rows with meaningless indices, called <code class="literal">RowMatrix</code>, from the RDD of vectors.</p></li><li style="list-style-type: disc"><p>
<code class="literal">IndexedRowMatrix</code>: In this case, row indices are meaningful. First, we create an RDD of indexed rows using the class <code class="literal">IndexedRow</code> and then create an <code class="literal">IndexedRowMatrix</code>.</p></li><li style="list-style-type: disc"><p>
<code class="literal">CoordinateMatrix</code>: This is useful to represent very large and very sparse matrices. <code class="literal">CoordinateMatrix</code> is created from RDDs of <a id="id250" class="indexterm"></a>the <code class="literal">MatrixEntry</code> points, represented by a tuple of type (long, long, or float)</p></li><li style="list-style-type: disc"><p>
<code class="literal">BlockMatrix</code>: These are created from RDDs of sub-matrix blocks, where a sub-matrix block is <code class="literal">((blockRowIndex, blockColIndex), sub-matrix)</code>.</p></li></ul></div></li></ul></div></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec31"></a>Machine learning workflows and data flows</h2></div></div><hr /></div><p>Beyond<a id="id251" class="indexterm"></a> algorithms, machine learning is also about <a id="id252" class="indexterm"></a>processes. We will discuss the typical workflows and data flows of supervised and unsupervised machine learning.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec39"></a>Supervised machine learning workflows</h3></div></div></div><p>In <a id="id253" class="indexterm"></a>supervised machine learning, the input training dataset is labeled. One of the key data practices is to split input data into training and test sets, and validate the mode accordingly.</p><p>We typically go through a six-step process flow in supervised learning:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Collect the data</strong></span>: This step essentially ties in with the previous chapter and ensures we collect the right data with the right volume and granularity in order to enable the machine learning algorithm to provide reliable answers.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Preprocess the data</strong></span>: This step is about checking the data quality by sampling, filling in the missing values if any, scaling and normalizing the data. We also define the feature extraction process. Typically, in the case of large text-based datasets, we apply tokenization, stop words removal, stemming, and TF-IDF.</p><p>In the case of supervised learning, we separate the input data into a training and test set. We can also implement various strategies of sampling and splitting the dataset for cross-validation purposes.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Ready the data</strong></span>: In this step, we get the data in the format or data type expected by the algorithms. In the case of Spark MLlib, this includes local vector, dense or sparse vectors, labeled points, local matrix, distributed matrix with row matrix, indexed row matrix, coordinate matrix, and block matrix.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Model</strong></span>: In this step, we apply the algorithms that are suitable for the problem at hand and get the results for evaluation of the most suitable algorithm in the evaluate step. We might have multiple algorithms suitable for the problem; their respective performance will be scored in the evaluate step to select the best preforming ones. We can implement an ensemble or combination of models in order to reach the best results.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Optimize</strong></span>: We may need to run a grid search for the optimal parameters of certain algorithms. These parameters are determined during training, and fine-tuned during the testing and production phase.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Evaluate</strong></span>: We ultimately score the models and select the best one in terms of accuracy, performance, reliability, and scalability. We move the best performing model to test with the held out test data in order to ascertain the prediction accuracy of our model. Once satisfied with the fine-tuned model, we move it to production to process live data.</p></li></ul></div><p>The supervised machine learning workflow and dataflow are represented in the following diagram:</p><div class="mediaobject"><img src="graphics/B03986_04_05.jpg" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec40"></a>Unsupervised machine learning workflows</h3></div></div></div><p>As <a id="id254" class="indexterm"></a>opposed to supervised learning, our initial data is not labeled in the case of unsupervised learning, which is most often the case in real life. We will extract the structure from the data by using clustering or dimensionality reduction algorithms. In the unsupervised learning case, we do not split the data into training and test, as we cannot make any prediction because the data is not labeled. We will train the data along six steps similar to those in supervised learning. Once the model is trained, we will evaluate the results and fine-tune the model and then release it for production.</p><p>Unsupervised learning can be a preliminary step to supervised learning. Namely, we look at reducing the dimensionality of the data prior to attacking the learning phase.</p><p>The unsupervised machine learning workflows and dataflow are represented as follows:</p><div class="mediaobject"><img src="graphics/B03986_04_06.jpg" /></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec32"></a>Clustering the Twitter dataset</h2></div></div><hr /></div><p>Let's first get <a id="id255" class="indexterm"></a>a feel for the data extracted from Twitter and get an understanding of the data structure in order to prepare and run it through the K-Means clustering algorithms. Our plan of attack uses the process and dataflow depicted earlier for unsupervised learning. The steps are as follows:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Combine all tweet files into a single dataframe.</p></li><li><p>Parse the tweets, remove stop words, extract emoticons, extract URL, and finally normalize the words (for example, mapping them to lowercase and removing punctuation and numbers).</p></li><li><p>Feature extraction includes the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Tokenization</strong></span>: This breaks down the parsed tweet text into individual words or tokens</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>TF-IDF</strong></span>: This applies the TF-IDF algorithm to create feature vectors from the tokenized tweet texts</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Hash TF-IDF</strong></span>: This applies a hashing function to the token vectors</p></li></ul></div></li><li><p>Run the K-Means clustering algorithm.</p></li><li><p>Evaluate<a id="id256" class="indexterm"></a> the results of the K-Means clustering:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Identify tweet membership to clusters</p></li><li style="list-style-type: disc"><p>Perform dimensionality reduction to two dimensions with the Multi-Dimensional Scaling or the Principal Component Analysis algorithm</p></li><li style="list-style-type: disc"><p>Plot the clusters</p></li></ul></div></li><li><p>Pipeline:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Fine-tune the number of relevant clusters K</p></li><li style="list-style-type: disc"><p>Measure the model cost</p></li><li style="list-style-type: disc"><p>Select the optimal model</p></li></ul></div></li></ol></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec41"></a>Applying Scikit-Learn on the Twitter dataset</h3></div></div></div><p>Python's <a id="id257" class="indexterm"></a>own Scikit-Learn machine learning library is one of the most reliable, intuitive, and robust tools around. Let's run through a preprocessing and unsupervised learning using Pandas and Scikit-Learn. It is often beneficial to explore a sample of the data using Scikit-Learn before spinning off clusters with Spark MLlib.</p><p>We have a mixed bag of 7,540 tweets. It contains tweets related to Apache Spark, Python, the upcoming presidential election with Hillary Clinton and Donald Trump as protagonists, and some tweets related to fashion and music with Lady Gaga and Justin Bieber. We are running the K-Means clustering algorithm using Python Scikit-Learn on the Twitter dataset harvested. We first load the sample data into a Pandas dataframe:</p><div class="informalexample"><pre class="programlisting">import pandas as pd

csv_in = 'C:\\Users\\Amit\\Documents\\IPython Notebooks\\AN00_Data\\unq_tweetstxt.csv'
twts_df01 = pd.read_csv(csv_in, sep =';', encoding='utf-8')

In [24]:

twts_df01.count()
Out[24]:
Unnamed: 0    7540
id            7540
created_at    7540
user_id       7540
user_name     7538
tweet_text    7540
dtype: int64

#
# Introspecting the tweets text
#
In [82]:

twtstxt_ls01[6910:6920]
Out[82]:
['RT @deroach_Ismoke: I am NOT voting for #hilaryclinton http://t.co/jaZZpcHkkJ',
 'RT @AnimalRightsJen: #HilaryClinton What do Bernie Sanders and Donald Trump Have in Common?: He has so far been th... http://t.co/t2YRcGCh6…',
 'I understand why Bill was out banging other chicks........I mean look at what he is married to.....\n@HilaryClinton',
 '#HilaryClinton What do Bernie Sanders and Donald Trump Have in Common?: He has so far been th... http://t.co/t2YRcGCh67 #Tcot #UniteBlue']</pre></div><p>We<a id="id258" class="indexterm"></a> first perform a feature extraction from the tweets' text. We apply a sparse vectorizer to the dataset using a TF-IDF vectorizer with 10,000 features and English stop words:</p><div class="informalexample"><pre class="programlisting">In [37]:

print("Extracting features from the training dataset using a sparse vectorizer")
t0 = time()
Extracting features from the training dataset using a sparse vectorizer
In [38]:

vectorizer = TfidfVectorizer(max_df=0.5, max_features=10000,
                                 min_df=2, stop_words='english',
                                 use_idf=True)
X = vectorizer.fit_transform(twtstxt_ls01)
#
# Output of the TFIDF Feature vectorizer
#
print("done in %fs" % (time() - t0))
print("n_samples: %d, n_features: %d" % X.shape)
print()
done in 5.232165s
n_samples: 7540, n_features: 6638</pre></div><p>As the <a id="id259" class="indexterm"></a>dataset is now broken into a 7540 sample with vectors of 6,638 features, we are ready to feed this sparse matrix to the K-Means clustering algorithm. We will choose seven clusters and 100 maximum iterations initially:</p><div class="informalexample"><pre class="programlisting">In [47]:

km = KMeans(n_clusters=7, init='k-means++', max_iter=100, n_init=1,
            verbose=1)

print("Clustering sparse data with %s" % km)
t0 = time()
km.fit(X)
print("done in %0.3fs" % (time() - t0))

Clustering sparse data with KMeans(copy_x=True, init='k-means++', max_iter=100, n_clusters=7, n_init=1,
    n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001,
    verbose=1)
Initialization complete
Iteration  0, inertia 13635.141
Iteration  1, inertia 6943.485
Iteration  2, inertia 6924.093
Iteration  3, inertia 6915.004
Iteration  4, inertia 6909.212
Iteration  5, inertia 6903.848
Iteration  6, inertia 6888.606
Iteration  7, inertia 6863.226
Iteration  8, inertia 6860.026
Iteration  9, inertia 6859.338
Iteration 10, inertia 6859.213
Iteration 11, inertia 6859.102
Iteration 12, inertia 6859.080
Iteration 13, inertia 6859.060
Iteration 14, inertia 6859.047
Iteration 15, inertia 6859.039
Iteration 16, inertia 6859.032
Iteration 17, inertia 6859.031
Iteration 18, inertia 6859.029
Converged at iteration 18
done in 1.701s</pre></div><p>The K-Means clustering algorithm converged after 18 iterations. We see in the following results the seven clusters with their respective key words. Clusters <code class="literal">0</code> and <code class="literal">6</code> are about music and <a id="id260" class="indexterm"></a>fashion with Justin Bieber and Lady Gaga-related tweets. Clusters <code class="literal">1</code> and <code class="literal">5</code> are related to the U.S.A. presidential elections with Donald Trump-and Hilary Clinton-related tweets. Clusters <code class="literal">2</code> and <code class="literal">3</code> are the ones of interest to us as they are about Apache Spark and Python. Cluster <code class="literal">4</code> contains Thailand-related tweets:</p><div class="informalexample"><pre class="programlisting">#
# Introspect top terms per cluster
#

In [49]:

print("Top terms per cluster:")
order_centroids = km.cluster_centers_.argsort()[:, ::-1]
terms = vectorizer.get_feature_names()
for i in range(7):
    print("Cluster %d:" % i, end='')
    for ind in order_centroids[i, :20]:
        print(' %s' % terms[ind], end='')
    print()
Top terms per cluster:
Cluster 0: justinbieber love mean rt follow thank hi https whatdoyoumean video wanna hear whatdoyoumeanviral rorykramer happy lol making person dream justin
Cluster 1: donaldtrump hilaryclinton rt https trump2016 realdonaldtrump trump gop amp justinbieber president clinton emails oy8ltkstze tcot like berniesanders hilary people email
Cluster 2: bigdata apachespark hadoop analytics rt spark training chennai ibm datascience apache processing cloudera mapreduce data sap https vora transforming development
Cluster 3: apachespark python https rt spark data amp databricks using new learn hadoop ibm big apache continuumio bluemix learning join open
Cluster 4: ernestsgantt simbata3 jdhm2015 elsahel12 phuketdailynews dreamintentions beyhiveinfrance almtorta18 civipartnership 9_a_6 25whu72ep0 k7erhvu7wn fdmxxxcm3h osxuh2fxnt 5o5rmb0xhp jnbgkqn0dj ovap57ujdh dtzsz3lb6x sunnysai12345 sdcvulih6g
Cluster 5: trump donald donaldtrump starbucks trumpquote trumpforpresident oy8ltkstze https zfns7pxysx silly goy stump trump2016 news jeremy coffee corbyn ok7vc8aetz rt tonight
Cluster 6: ladygaga gaga lady rt https love follow horror cd story ahshotel american japan hotel human trafficking music fashion diet queen ahs</pre></div><p>We will <a id="id261" class="indexterm"></a>visualize the results by plotting the cluster. We have 7,540 samples with 6,638 features. It will be impossible to visualize that many dimensions. We will use the <span class="strong"><strong>Multi-Dimensional Scaling</strong></span> (<span class="strong"><strong>MDS</strong></span>) algorithm to bring down the multidimensional features of the clusters into two tractable dimensions<a id="id262" class="indexterm"></a> to be able to picture them:</p><div class="informalexample"><pre class="programlisting">import matplotlib.pyplot as plt
import matplotlib as mpl
from sklearn.manifold import MDS

MDS()

#
# Bring down the MDS to two dimensions (components) as we will plot 
# the clusters
#
mds = MDS(n_components=2, dissimilarity="precomputed", random_state=1)

pos = mds.fit_transform(dist)  # shape (n_components, n_samples)

xs, ys = pos[:, 0], pos[:, 1]

In [67]:

#
# Set up colors per clusters using a dict
#
cluster_colors = {0: '#1b9e77', 1: '#d95f02', 2: '#7570b3', 3: '#e7298a', 4: '#66a61e', 5: '#9990b3', 6: '#e8888a'}

#
#set up cluster names using a dict
#
cluster_names = {0: 'Music, Pop', 
                 1: 'USA Politics, Election', 
                 2: 'BigData, Spark', 
                 3: 'Spark, Python',
                 4: 'Thailand', 
                 5: 'USA Politics, Election', 
                 6: 'Music, Pop'}
In [115]:
#
# ipython magic to show the matplotlib plots inline
#
%matplotlib inline 

#
# Create data frame which includes MDS results, cluster numbers and tweet texts to be displayed
#
df = pd.DataFrame(dict(x=xs, y=ys, label=clusters, txt=twtstxt_ls02_utf8))
ix_start = 2000
ix_stop  = 2050
df01 = df[ix_start:ix_stop]

print(df01[['label','txt']])
print(len(df01))
print()

# Group by cluster

groups = df.groupby('label')
groups01 = df01.groupby('label')

# Set up the plot

fig, ax = plt.subplots(figsize=(17, 10)) 
ax.margins(0.05) 

#
# Build the plot object
#
for name, group in groups01:
    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, 
            label=cluster_names[name], color=cluster_colors[name], 
            mec='none')
    ax.set_aspect('auto')
    ax.tick_params(\
        axis= 'x',         # settings for x-axis
        which='both',      # 
        bottom='off',      # 
        top='off',         # 
        labelbottom='off')
    ax.tick_params(\
        axis= 'y',         # settings for y-axis
        which='both',      # 
        left='off',        # 
        top='off',         # 
        labelleft='off')
    
ax.legend(numpoints=1)     #
#
# Add label in x,y position with tweet text
#
for i in range(ix_start, ix_stop):
    ax.text(df01.ix[i]['x'], df01.ix[i]['y'], df01.ix[i]['txt'], size=10)  
    
plt.show()                 # Display the plot


      label       text
2000      2       b'RT @BigDataTechCon: '
2001      3       b"@4Quant 's presentat"
2002      2       b'Cassandra Summit 201'</pre></div><p>Here's <a id="id263" class="indexterm"></a>a plot of Cluster <code class="literal">2</code>, <span class="emphasis"><em>Big Data</em></span> and<span class="emphasis"><em> Spark</em></span>, represented by blue dots along with Cluster <code class="literal">3</code>, <span class="emphasis"><em>Spark</em></span> and <span class="emphasis"><em>Python</em></span>, represented by red dots, and some sample tweets related to the respective clusters:</p><div class="mediaobject"><img src="graphics/B03986_04_07.jpg" /></div><p>We have gained some good insights into the data with the exploration and processing done with <a id="id264" class="indexterm"></a>Scikit-Learn. We will now focus our attention on Spark MLlib and take it for a ride on the Twitter dataset.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec42"></a>Preprocessing the dataset</h3></div></div></div><p>Now, we <a id="id265" class="indexterm"></a>will focus on feature extraction and engineering in order to ready the data for the clustering algorithm run. We instantiate the Spark Context and read the Twitter dataset into a Spark dataframe. We will then successively tokenize the tweet text data, apply a hashing Term frequency algorithm to the tokens, and finally apply the Inverse Document Frequency algorithm and rescale the data. The code is as follows:</p><div class="informalexample"><pre class="programlisting">In [3]:
#
# Read csv in a Panda DF
#
#
import pandas as pd
csv_in = '/home/an/spark/spark-1.5.0-bin-hadoop2.6/examples/AN_Spark/data/unq_tweetstxt.csv'
pddf_in = pd.read_csv(csv_in, index_col=None, header=0, sep=';', encoding='utf-8')

In [4]:

sqlContext = SQLContext(sc)

In [5]:

#
# Convert a Panda DF to a Spark DF
#
#

spdf_02 = sqlContext.createDataFrame(pddf_in[['id', 'user_id', 'user_name', 'tweet_text']])

In [8]:

spdf_02.show()

In [7]:

spdf_02.take(3)

Out[7]:

[Row(id=638830426971181057, user_id=3276255125, user_name=u'True Equality', tweet_text=u'ernestsgantt: BeyHiveInFrance: 9_A_6: dreamintentions: elsahel12: simbata3: JDHM2015: almtorta18: dreamintentions:\u2026 http://t.co/VpD7FoqMr0'),
 Row(id=638830426727911424, user_id=3276255125, user_name=u'True Equality', tweet_text=u'ernestsgantt: BeyHiveInFrance: PhuketDailyNews: dreamintentions: elsahel12: simbata3: JDHM2015: almtorta18: CiviPa\u2026 http://t.co/VpD7FoqMr0'),
 Row(id=638830425402556417, user_id=3276255125, user_name=u'True Equality', tweet_text=u'ernestsgantt: BeyHiveInFrance: 9_A_6: ernestsgantt: elsahel12: simbata3: JDHM2015: almtorta18: CiviPartnership: dr\u2026 http://t.co/EMDOn8chPK')]

In [9]:

from pyspark.ml.feature import HashingTF, IDF, Tokenizer

In [10]:

#
# Tokenize the tweet_text 
#
tokenizer = Tokenizer(inputCol="tweet_text", outputCol="tokens")
tokensData = tokenizer.transform(spdf_02)

In [11]:

tokensData.take(1)

Out[11]:

[Row(id=638830426971181057, user_id=3276255125, user_name=u'True Equality', tweet_text=u'ernestsgantt: BeyHiveInFrance: 9_A_6: dreamintentions: elsahel12: simbata3: JDHM2015: almtorta18: dreamintentions:\u2026 http://t.co/VpD7FoqMr0', tokens=[u'ernestsgantt:', u'beyhiveinfrance:', u'9_a_6:', u'dreamintentions:', u'elsahel12:', u'simbata3:', u'jdhm2015:', u'almtorta18:', u'dreamintentions:\u2026', u'http://t.co/vpd7foqmr0'])]

In [14]:

#
# Apply Hashing TF to the tokens
#
hashingTF = HashingTF(inputCol="tokens", outputCol="rawFeatures", numFeatures=2000)
featuresData = hashingTF.transform(tokensData)

In [15]:

featuresData.take(1)

Out[15]:

[Row(id=638830426971181057, user_id=3276255125, user_name=u'True Equality', tweet_text=u'ernestsgantt: BeyHiveInFrance: 9_A_6: dreamintentions: elsahel12: simbata3: JDHM2015: almtorta18: dreamintentions:\u2026 http://t.co/VpD7FoqMr0', tokens=[u'ernestsgantt:', u'beyhiveinfrance:', u'9_a_6:', u'dreamintentions:', u'elsahel12:', u'simbata3:', u'jdhm2015:', u'almtorta18:', u'dreamintentions:\u2026', u'http://t.co/vpd7foqmr0'], rawFeatures=SparseVector(2000, {74: 1.0, 97: 1.0, 100: 1.0, 160: 1.0, 185: 1.0, 742: 1.0, 856: 1.0, 991: 1.0, 1383: 1.0, 1620: 1.0}))]

In [16]:

#
# Apply IDF to the raw features and rescale the data
#
idf = IDF(inputCol="rawFeatures", outputCol="features")
idfModel = idf.fit(featuresData)
rescaledData = idfModel.transform(featuresData)

for features in rescaledData.select("features").take(3):
  print(features)

In [17]:

rescaledData.take(2)

Out[17]:

[Row(id=638830426971181057, user_id=3276255125, user_name=u'True Equality', tweet_text=u'ernestsgantt: BeyHiveInFrance: 9_A_6: dreamintentions: elsahel12: simbata3: JDHM2015: almtorta18: dreamintentions:\u2026 http://t.co/VpD7FoqMr0', tokens=[u'ernestsgantt:', u'beyhiveinfrance:', u'9_a_6:', u'dreamintentions:', u'elsahel12:', u'simbata3:', u'jdhm2015:', u'almtorta18:', u'dreamintentions:\u2026', u'http://t.co/vpd7foqmr0'], rawFeatures=SparseVector(2000, {74: 1.0, 97: 1.0, 100: 1.0, 160: 1.0, 185: 1.0, 742: 1.0, 856: 1.0, 991: 1.0, 1383: 1.0, 1620: 1.0}), features=SparseVector(2000, {74: 2.6762, 97: 1.8625, 100: 2.6384, 160: 2.9985, 185: 2.7481, 742: 5.5269, 856: 4.1406, 991: 2.9518, 1383: 4.694, 1620: 3.073})),
 Row(id=638830426727911424, user_id=3276255125, user_name=u'True Equality', tweet_text=u'ernestsgantt: BeyHiveInFrance: PhuketDailyNews: dreamintentions: elsahel12: simbata3: JDHM2015: almtorta18: CiviPa\u2026 http://t.co/VpD7FoqMr0', tokens=[u'ernestsgantt:', u'beyhiveinfrance:', u'phuketdailynews:', u'dreamintentions:', u'elsahel12:', u'simbata3:', u'jdhm2015:', u'almtorta18:', u'civipa\u2026', u'http://t.co/vpd7foqmr0'], rawFeatures=SparseVector(2000, {74: 1.0, 97: 1.0, 100: 1.0, 160: 1.0, 185: 1.0, 460: 1.0, 987: 1.0, 991: 1.0, 1383: 1.0, 1620: 1.0}), features=SparseVector(2000, {74: 2.6762, 97: 1.8625, 100: 2.6384, 160: 2.9985, 185: 2.7481, 460: 6.4432, 987: 2.9959, 991: 2.9518, 1383: 4.694, 1620: 3.073}))]

In [21]:

rs_pddf = rescaledData.toPandas()

In [22]:

rs_pddf.count()

Out[22]:

id             7540
user_id        7540
user_name      7540
tweet_text     7540
tokens         7540
rawFeatures    7540
features       7540
dtype: int64


In [27]:

feat_lst = rs_pddf.features.tolist()

In [28]:

feat_lst[:2]

Out[28]:

[SparseVector(2000, {74: 2.6762, 97: 1.8625, 100: 2.6384, 160: 2.9985, 185: 2.7481, 742: 5.5269, 856: 4.1406, 991: 2.9518, 1383: 4.694, 1620: 3.073}),
 SparseVector(2000, {74: 2.6762, 97: 1.8625, 100: 2.6384, 160: 2.9985, 185: 2.7481, 460: 6.4432, 987: 2.9959, 991: 2.9518, 1383: 4.694, 1620: 3.073})]</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec43"></a>Running the clustering algorithm</h3></div></div></div><p>We will<a id="id266" class="indexterm"></a> use the K-Means algorithm against the Twitter dataset. As an unlabeled and shuffled bag of tweets, we want to see if the <span class="emphasis"><em>Apache Spark</em></span> tweets are grouped in a single cluster. From the previous steps, the TF-IDF sparse vector of features is converted into an RDD that will be the input to the Spark MLlib program. We initialize the K-Means model with 5 clusters, 10 iterations of 10 runs:</p><div class="informalexample"><pre class="programlisting">In [32]:

from pyspark.mllib.clustering import KMeans, KMeansModel
from numpy import array
from math import sqrt

In [34]:

# Load and parse the data


in_Data = sc.parallelize(feat_lst)

In [35]:

in_Data.take(3)

Out[35]:

[SparseVector(2000, {74: 2.6762, 97: 1.8625, 100: 2.6384, 160: 2.9985, 185: 2.7481, 742: 5.5269, 856: 4.1406, 991: 2.9518, 1383: 4.694, 1620: 3.073}),
 SparseVector(2000, {74: 2.6762, 97: 1.8625, 100: 2.6384, 160: 2.9985, 185: 2.7481, 460: 6.4432, 987: 2.9959, 991: 2.9518, 1383: 4.694, 1620: 3.073}),
 SparseVector(2000, {20: 4.3534, 74: 2.6762, 97: 1.8625, 100: 5.2768, 185: 2.7481, 856: 4.1406, 991: 2.9518, 1039: 3.073, 1620: 3.073, 1864: 4.6377})]

In [37]:

in_Data.count()

Out[37]:

7540

In [38]:

# Build the model (cluster the data)

clusters = KMeans.train(in_Data, 5, maxIterations=10,
        runs=10, initializationMode="random")

In [53]:

# Evaluate clustering by computing Within Set Sum of Squared Errors

def error(point):
    center = clusters.centers[clusters.predict(point)]
    return sqrt(sum([x**2 for x in (point - center)]))

WSSSE = in_Data.map(lambda point: error(point)).reduce(lambda x, y: x + y)
print("Within Set Sum of Squared Error = " + str(WSSSE))</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec44"></a>Evaluating the model and the results</h3></div></div></div><p>One <a id="id267" class="indexterm"></a>way to fine-tune the clustering algorithm is by varying the number of clusters and verifying the output. Let's check the clusters and get a feel for the clustering results so far:</p><div class="informalexample"><pre class="programlisting">In [43]:

cluster_membership = in_Data.map(lambda x: clusters.predict(x))

In [54]:

cluster_idx = cluster_membership.zipWithIndex()

In [55]:

type(cluster_idx)

Out[55]:

pyspark.rdd.PipelinedRDD

In [58]:

cluster_idx.take(20)

Out[58]:

[(3, 0),
 (3, 1),
 (3, 2),
 (3, 3),
 (3, 4),
 (3, 5),
 (1, 6),
 (3, 7),
 (3, 8),
 (3, 9),
 (3, 10),
 (3, 11),
 (3, 12),
 (3, 13),
 (3, 14),
 (1, 15),
 (3, 16),
 (3, 17),
 (1, 18),
 (1, 19)]

In [59]:

cluster_df = cluster_idx.toDF()

In [65]:

pddf_with_cluster = pd.concat([pddf_in, cluster_pddf],axis=1)

In [76]:

pddf_with_cluster._1.unique()

Out[76]:

array([3, 1, 4, 0, 2])

In [79]:

pddf_with_cluster[pddf_with_cluster['_1'] == 0].head(10)

Out[79]:
  Unnamed: 0   id   created_at   user_id   user_name   tweet_text   _1   _2
6227   3   642418116819988480   Fri Sep 11 19:23:09 +0000 2015   49693598   Ajinkya Kale   RT @bigdata: Distributed Matrix Computations i...   0   6227
6257   45   642391207205859328   Fri Sep 11 17:36:13 +0000 2015   937467860   Angela Bassa   [Auto] I'm reading ""Distributed Matrix Comput...   0   6257
6297   119   642348577147064320   Fri Sep 11 14:46:49 +0000 2015   18318677   Ben Lorica   Distributed Matrix Computations in @ApacheSpar...   0   6297
In [80]:

pddf_with_cluster[pddf_with_cluster['_1'] == 1].head(10)

Out[80]:
  Unnamed: 0   id   created_at   user_id   user_name   tweet_text   _1   _2
6   6   638830419090079746   Tue Sep 01 21:46:55 +0000 2015   2241040634   Massimo Carrisi   Python:Python: Removing \xa0 from string? - I ...   1   6
15   17   638830380578045953   Tue Sep 01 21:46:46 +0000 2015   57699376   Rafael Monnerat   RT @ramalhoorg: Noite de autógrafos do Fluent ...   1   15
18   41   638830280988426250   Tue Sep 01 21:46:22 +0000 2015   951081582   Jack Baldwin   RT @cloudaus: We are 3/4 full! 2-day @swcarpen...   1   18
19   42   638830276626399232   Tue Sep 01 21:46:21 +0000 2015   6525302   Masayoshi Nakamura   PynamoDB #AWS #DynamoDB #Python http://...   1   19
20   43   638830213288235008   Tue Sep 01 21:46:06 +0000 2015   3153874869   Baltimore Python   Flexx: Python UI tookit based on web technolog...   1   20
21   44   638830117645516800   Tue Sep 01 21:45:43 +0000 2015   48474625   Radio Free Denali   Hmm, emerge --depclean wants to remove somethi...   1   21
22   46   638829977014636544   Tue Sep 01 21:45:10 +0000 2015   154915461   Luciano Ramalho   Noite de autógrafos do Fluent Python no Garoa ...   1   22
23   47   638829882928070656   Tue Sep 01 21:44:47 +0000 2015   917320920   bsbafflesbrains   @DanSWright Harper channeling Monty Python. "...   1   23
24   48   638829868679954432   Tue Sep 01 21:44:44 +0000 2015   134280898   Lannick Technology   RT @SergeyKalnish: I am #hiring: Senior Back e...   1   24
25   49   638829707484508161   Tue Sep 01 21:44:05 +0000 2015   2839203454   Joshua Jones   RT @LindseyPelas: Surviving Monty Python in Fl...   1   25
In [81]:

pddf_with_cluster[pddf_with_cluster['_1'] == 2].head(10)

Out[81]:
  Unnamed: 0   id   created_at   user_id   user_name   tweet_text   _1   _2
7280   688   639056941592014848   Wed Sep 02 12:47:02 +0000 2015   2735137484   Chris   A true gay icon when will @ladygaga @Madonna @...   2   7280
In [82]:

pddf_with_cluster[pddf_with_cluster['_1'] == 3].head(10)

Out[82]:
  Unnamed: 0   id   created_at   user_id   user_name   tweet_text   _1   _2
0   0   638830426971181057   Tue Sep 01 21:46:57 +0000 2015   3276255125   True Equality   ernestsgantt: BeyHiveInFrance: 9_A_6: dreamint...   3   0
1   1   638830426727911424   Tue Sep 01 21:46:57 +0000 2015   3276255125   True Equality   ernestsgantt: BeyHiveInFrance: PhuketDailyNews...   3   1
2   2   638830425402556417   Tue Sep 01 21:46:56 +0000 2015   3276255125   True Equality   ernestsgantt: BeyHiveInFrance: 9_A_6: ernestsg...   3   2
3   3   638830424563716097   Tue Sep 01 21:46:56 +0000 2015   3276255125   True Equality   ernestsgantt: BeyHiveInFrance: PhuketDailyNews...   3   3
4   4   638830422256816132   Tue Sep 01 21:46:56 +0000 2015   3276255125   True Equality   ernestsgantt: elsahel12: 9_A_6: dreamintention...   3   4
5   5   638830420159655936   Tue Sep 01 21:46:55 +0000 2015   3276255125   True Equality   ernestsgantt: BeyHiveInFrance: PhuketDailyNews...   3   5
7   7   638830418330980352   Tue Sep 01 21:46:55 +0000 2015   3276255125   True Equality   ernestsgantt: elsahel12: 9_A_6: dreamintention...   3   7
8   8   638830397648822272   Tue Sep 01 21:46:50 +0000 2015   3276255125   True Equality   ernestsgantt: BeyHiveInFrance: PhuketDailyNews...   3   8
9   9   638830395375529984   Tue Sep 01 21:46:49 +0000 2015   3276255125   True Equality   ernestsgantt: elsahel12: 9_A_6: dreamintention...   3   9
10   10   638830392389177344   Tue Sep 01 21:46:49 +0000 2015   3276255125   True Equality   ernestsgantt: BeyHiveInFrance: PhuketDailyNews...   3   10
In [83]:

pddf_with_cluster[pddf_with_cluster['_1'] == 4].head(10)

Out[83]:
  Unnamed: 0   id   created_at   user_id   user_name   tweet_text   _1   _2
1361   882   642648214454317056   Sat Sep 12 10:37:28 +0000 2015   27415756   Raymond Enisuoh   LA Chosen For US 2024 Olympic Bid - LA2016 See...   4   1361
1363   885   642647848744583168   Sat Sep 12 10:36:01 +0000 2015   27415756   Raymond Enisuoh   Prison See: https://t.co/x3EKAExeFi … … … … … ...   4   1363
5412   11   640480770369286144   Sun Sep 06 11:04:49 +0000 2015   3242403023   Donald Trump 2016   " igiboooy! @ Starbucks https://t.co/97wdL...   4   5412
5428   27   640477140660518912   Sun Sep 06 10:50:24 +0000 2015   3242403023   Donald Trump 2016   "  @ Starbucks https://t.co/wsEYFIefk7 " - D...   4   5428
5455   61   640469542272110592   Sun Sep 06 10:20:12 +0000 2015   3242403023   Donald Trump 2016   " starbucks @ Starbucks Mam Plaza https://t.co...   4   5455
5456   62   640469541370372096   Sun Sep 06 10:20:12 +0000 2015   3242403023   Donald Trump 2016   " Aaahhh the pumpkin spice latte is back, fall...   4   5456
5457   63   640469539524898817   Sun Sep 06 10:20:12 +0000 2015   3242403023   Donald Trump 2016   " RT kayyleighferry: Oh my goddd Harry Potter ...   4   5457
5458   64   640469537176031232   Sun Sep 06 10:20:11 +0000 2015   3242403023   Donald Trump 2016   " Starbucks https://t.co/3xYYXlwNkf " - Donald...   4   5458
5459   65   640469536119070720   Sun Sep 06 10:20:11 +0000 2015   3242403023   Donald Trump 2016   " A Starbucks is under construction in my neig...   4   5459
5460   66   640469530435813376   Sun Sep 06 10:20:10 +0000 2015   3242403023   Donald Trump 2016   " Babam starbucks'tan fotogtaf atıyor bende du...   4   5460</pre></div><p>We<a id="id268" class="indexterm"></a> map the <code class="literal">5</code> clusters with some sample tweets. Cluster <code class="literal">0</code> is about Spark. Cluster <code class="literal">1</code> is about Python. Cluster <code class="literal">2</code> is about Lady Gaga. Cluster <code class="literal">3</code> is about Thailand's Phuket News. Cluster <code class="literal">4</code> is about Donald Trump.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec33"></a>Building machine learning pipelines</h2></div></div><hr /></div><p>We<a id="id269" class="indexterm"></a> want to compose the feature extraction, preparatory activities, training, testing, and prediction activities while optimizing the best tuning parameter to get the best performing model.</p><p>The following tweet captures perfectly in five lines of code a powerful machine learning Pipeline implemented in Spark MLlib:</p><div class="mediaobject"><img src="graphics/B03986_04_08.jpg" /></div><p>The<a id="id270" class="indexterm"></a> Spark ML pipeline is inspired by Python's Scikit-Learn and creates a succinct, declarative statement of the successive transformations to the data in order to quickly deliver a tunable model.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec34"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we got an overview of Spark MLlib's ever-expanding library of algorithms Spark MLlib. We discussed supervised and unsupervised learning, recommender systems, optimization, and feature extraction algorithms. We then put the harvested data from Twitter into the machine learning process, algorithms, and evaluation to derive insights from the data. We put the Twitter-harvested dataset through a Python Scikit-Learn and Spark MLlib K-means clustering in order to segregate the tweets relevant to <span class="emphasis"><em>Apache Spark</em></span>. We also evaluated the performance of the model.</p><p>This gets us ready for the next chapter, which will cover Streaming Analytics using Spark. Let's jump right in.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch05"></a>Chapter 5. Streaming Live Data with Spark</h2></div></div></div><p>In this chapter, we will focus on live streaming data flowing into Spark and processing it. So far, we have discussed machine learning and data mining with batch processing. We are now looking at processing continuously flowing data and detecting facts and patterns on the fly. We are navigating from a lake to a river.</p><p>We will first investigate the challenges arising from such a dynamic and ever changing environment. After laying the grounds on the prerequisite of a streaming application, we will investigate various implementations using live sources of data such as TCP sockets to the Twitter firehose and put in place a low latency, high throughput, and scalable data pipeline combining Spark, Kafka and Flume.</p><p>In this chapter, we will cover the following points:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Analyzing a streaming application's architectural challenges, constraints, and requirements</p></li><li style="list-style-type: disc"><p>Processing live data from a TCP socket with Spark Streaming</p></li><li style="list-style-type: disc"><p>Connecting to the Twitter firehose directly to parse tweets in quasi real time</p></li><li style="list-style-type: disc"><p>Establishing a reliable, fault tolerant, scalable, high throughput, low latency integrated application using Spark, Kafka, and Flume</p></li><li style="list-style-type: disc"><p>Closing remarks on Lambda and Kappa architecture paradigms</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec35"></a>Laying the foundations of streaming architecture</h2></div></div><hr /></div><p>As customary, let's<a id="id271" class="indexterm"></a> first go back to our original drawing of the data-intensive apps architecture blueprint and highlight the Spark Streaming module that will be the topic of interest.</p><p>The following diagram sets the context by highlighting the Spark Streaming module and interactions with Spark SQL and Spark MLlib within the overall data-intensive apps framework.</p><div class="mediaobject"><img src="graphics/B03968_05_01.jpg" /></div><p>Data flows<a id="id272" class="indexterm"></a> from stock market time series, enterprise transactions, interactions, events, web traffic, click streams, and sensors. All events are time-stamped data and urgent. This is the case for fraud detection and prevention, mobile cross-sell and upsell, or traffic alerts. Those streams of data require immediate processing for monitoring purposes, such as detecting anomalies, outliers, spam, fraud, and intrusion; and also for providing basic statistics, insights, trends, and recommendations. In some cases, the summarized aggregated information is sufficient to be stored for later usage. From an architecture paradigm perspective, we are moving from a service-oriented architecture to an event-driven architecture.</p><p>Two models emerge for processing streams of data:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Processing <a id="id273" class="indexterm"></a>one record at a time as they come in. We do not buffer the incoming records in a container before processing them. This is the case of Twitter's Storm, Yahoo's S4, and Google's MillWheel.</p></li><li style="list-style-type: disc"><p>Micro-batching or batch computations on small intervals as performed by Spark Streaming and Storm Trident. In this case, we buffer the incoming records in a container according to the time window prescribed in the micro-batching settings.</p></li></ul></div><p>Spark Streaming has often been compared against Storm. They are two different models of streaming data. Spark Streaming is based on micro-batching. Storm is based on processing records as they come in. Storm also offers a micro-batching option, with its Storm Trident option.</p><p>The driving<a id="id274" class="indexterm"></a> factor in a streaming application is latency. Latency<a id="id275" class="indexterm"></a> varies from the milliseconds range in the case of <span class="strong"><strong>RPC</strong></span> (short for <span class="strong"><strong>Remote Procedure Call</strong></span>) to several seconds or minutes for micro batching solution such as Spark Streaming.</p><p>RPC allows synchronous operations between the requesting programs waiting for the results from the remote server's procedure. Threads allow concurrency of multiple RPC calls to the server.</p><p>An example of software implementing a distributed RPC model is Apache Storm.</p><p>Storm implements stateless sub millisecond latency processing of unbounded tuples using topologies or directed acyclic graphs combining spouts as source of data streams and bolts for operations such as filter, join, aggregation, and transformation. Storm also implements a <a id="id276" class="indexterm"></a>higher level abstraction called <span class="strong"><strong>Trident</strong></span> which, similarly to Spark, processes data streams in micro batches.</p><p>So, looking at the latency continuum, from sub millisecond to second, Storm is a good candidate. For seconds to minutes scale, Spark Streaming and Storm Trident are excellent fits. For several minutes onward, Spark and a NoSQL database such as Cassandra or HBase are adequate solutions. For ranges beyond the hour and with high volume of data, Hadoop is the ideal contender.</p><p>Although throughput is correlated to latency, it is not a simple inversely linear relationship. If processing a message takes 2 ms, which determines the latency, then one would assume the throughput is limited to 500 messages per sec. Batching messages allows for higher throughput if we allow our messages to be buffered for 8 ms more. With a latency of 10 ms, the system can buffer up to 10,000 messages. For a bearable increase in latency, we have substantially increased throughput. This is the magic of micro-batching that Spark Streaming exploits.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec45"></a>Spark Streaming inner working</h3></div></div></div><p>The Spark <a id="id277" class="indexterm"></a>Streaming architecture leverages the Spark core <a id="id278" class="indexterm"></a>architecture. It overlays on the <span class="strong"><strong>SparkContext</strong></span> a <span class="strong"><strong>StreamingContext</strong></span> as the entry point to the Stream functionality. The Cluster Manager will dedicate at least <a id="id279" class="indexterm"></a>one worker node as Receiver, which will be an executor with a <span class="emphasis"><em>long task</em></span> to process the incoming stream. The Executor creates Discretized Streams or DStreams from input data stream and replicates by default, the DStream to the cache of another worker. One receiver serves one input data stream. Multiple receivers improve parallelism and generate multiple DStreams that Spark can unite or join Resilient Distributed Datasets (RDD).</p><p>The following diagram gives an overview of the inner working of Spark Streaming. The client interacts with the Spark Cluster via the cluster manager, while Spark Streaming has a dedicated <a id="id280" class="indexterm"></a>worker with a long running task ingesting the<a id="id281" class="indexterm"></a> input data stream and transforming it into discretized streams or DStreams. The data is collected, buffered and replicated by a receiver and then pushed to a stream of RDDs.</p><div class="mediaobject"><img src="graphics/B03968_05_02.jpg" /></div><p>Spark receivers can ingest data from many sources. Core input sources range from TCP socket and HDFS/Amazon S3 to Akka Actors. Additional sources include Apache Kafka, Apache Flume, Amazon Kinesis, ZeroMQ, Twitter, and custom or user-defined receivers.</p><p>We distinguish between reliable resources that acknowledges receipt of data to the source and replication for possible resend, versus unreliable receivers who do not acknowledge receipt of the message. Spark scales out in terms of the number of workers, partition and receivers.</p><p>The following diagram gives an overview of Spark Streaming with the possible sources and the persistence options:</p><div class="mediaobject"><img src="graphics/B03968_05_03.jpg" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec46"></a>Going under the hood of Spark Streaming</h3></div></div></div><p>Spark Streaming is <a id="id282" class="indexterm"></a>composed of Receivers and powered by Discretized Streams and Spark Connectors for persistence.</p><p>As for <a id="id283" class="indexterm"></a>Spark Core, the essential data structure is the RDD, the fundamental programming abstraction for Spark Streaming is the Discretized Stream or DStream.</p><p>The following diagram illustrates the Discretized Streams as continuous sequences of RDDs. The batch intervals of DStream are configurable.</p><div class="mediaobject"><img src="graphics/B03968_05_04.jpg" /></div><p>DStreams snapshots the incoming data in batch intervals. Those time steps typically range from 500 ms to several seconds. The underlying structure of a DStream is an RDD.</p><p>A DStream is<a id="id284" class="indexterm"></a> essentially a continuous sequence of RDDs. This is powerful as it allows us to leverage from Spark Streaming all the traditional functions, transformations and actions available in Spark Core and allows us to dialogue with Spark SQL, performing SQL queries on incoming streams of data and Spark MLlib. Transformations similar to those on generic and key-value pair RDDs are applicable. The DStreams benefit from the inner RDDs lineage and fault tolerance. Additional transformation and output operations exist for discretized stream operations. Most generic <a id="id285" class="indexterm"></a>operations on DStream are <span class="strong"><strong>transform</strong></span> and <span class="strong"><strong>foreachRDD</strong></span>.</p><p>The following diagram gives an overview of the lifecycle of DStreams. From creation of the micro-batches of messages materialized to RDDs on which <code class="literal">transformation</code> function and actions that trigger Spark jobs are applied. Breaking down the steps illustrated in the diagram, we read the diagram top down:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>In the Input Stream, the incoming messages are buffered in a container according to the time window allocated for the micro-batching.</p></li><li><p>In the discretized stream step, the buffered micro-batches are transformed as DStream RDDs.</p></li><li><p>The Mapped DStream step is obtained by applying a transformation function to the original DStream. These first three steps constitute the transformation of the original data received in predefined time windows. As the underlying data structure is the RDD, we conserve the data lineage of the transformations.</p></li><li><p>The final step is an action on the RDD. It triggers the Spark job.</p></li></ol></div><div class="mediaobject"><img src="graphics/B03968_05_05.jpg" /></div><p>Transformation<a id="id286" class="indexterm"></a> can be stateless or stateful. <span class="emphasis"><em>Stateless</em></span> means that no state is maintained by the program, while <span class="emphasis"><em>stateful</em></span> means the program keeps a state, in which case previous transactions are remembered and may affect the current transaction. A stateful operation modifies or requires some state of the system, and a stateless operation does not.</p><p>Stateless transformations process each batch in a DStream at a time. Stateful transformations process multiple batches to obtain results. Stateful transformations require the checkpoint directory to be configured. Check pointing is the main mechanism for fault tolerance in Spark Streaming to periodically save data and metadata about an application.</p><p>There are two types of stateful transformations for Spark Streaming: <code class="literal">updateStateByKey</code> and windowed transformations.</p><p>
<code class="literal">updateStateByKey</code> are transformations that maintain state for each key in a stream of Pair RDDs. It returns a new <span class="emphasis"><em>state</em></span> DStream where the state for each key is updated by applying the given function on the previous state of the key and the new values of each key. An example would be a running count of given hashtags in a stream of tweets.</p><p>Windowed transformations are carried over multiple batches in a sliding window. A window has a defined length or duration specified in time units. It must be a multiple of a DStream batch interval. It defines how many batches are included in a windowed transformation.</p><p>A window has<a id="id287" class="indexterm"></a> a sliding interval or sliding duration specified in time units. It must be a multiple of a DStream batch interval. It defines how many batches to slide a window or how frequently to compute a windowed transformation.</p><p>The following schema depicts the windowing operation on DStreams to derive window DStreams with a given length and sliding interval:</p><div class="mediaobject"><img src="graphics/B03968_05_06.jpg" /></div><p>A sample function is <code class="literal">countByWindow</code> (<code class="literal">windowLength</code>, <code class="literal">slideInterval</code>). It returns a new DStream in which each RDD has a single element generated by counting the number of elements in a sliding window over this DStream. An illustration in this case would be a running count of given hashtags in a stream of tweets every 60 seconds. The window time frame is specified.</p><p>Minute scale window length is reasonable. Hour scale window length is not recommended as it is compute and memory intensive. It would be more convenient to aggregate the data in a database such as Cassandra or HBase.</p><p>Windowed transformations compute results based on window length and window slide interval. Spark performance is primarily affected by on window length, window slide interval, and persistence.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec47"></a>Building in fault tolerance</h3></div></div></div><p>Real-time<a id="id288" class="indexterm"></a> stream processing systems must be operational 24/7. They need to be resilient to all sorts of failures in the system. Spark and its RDD abstraction are designed to seamlessly handle failures of any worker nodes in the cluster.</p><p>Main Spark Streaming fault tolerance mechanisms are check pointing, automatic driver restart, and automatic failover. Spark enables recovery from driver failure using check pointing, which preserves the application state.</p><p>Write ahead logs, reliable receivers, and file streams guarantees zero data loss as of Spark Version 1.2. Write ahead logs represent a fault tolerant storage for received data.</p><p>Failures <a id="id289" class="indexterm"></a>require recomputing results. DStream operations have exactly-one semantics. Transformations can be recomputed multiple times but will yield the same result. DStream output operations have at least once semantics. Output operations may be executed multiple times.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec36"></a>Processing live data with TCP sockets</h2></div></div><hr /></div><p>As a<a id="id290" class="indexterm"></a> stepping stone to the overall understanding of streaming operations, we will first experiment with TCP socket. TCP socket establishes two-way communication between client and server, and it can exchange data through the established connection. WebSocket connections are long lived, unlike typical HTTP connections. HTTP is not meant to keep an open connection from the server to push continuously data to the web browsers. Most web applications hence resorted to long polling <a id="id291" class="indexterm"></a>via frequent <span class="strong"><strong>Asynchronous JavaScript</strong></span> (<span class="strong"><strong>AJAX</strong></span>) and XML requests. WebSockets, standardized and implemented in HTML5, are moving beyond web browsers and are becoming a cross-platform standard for real-time communication between client and server.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec48"></a>Setting up TCP sockets</h3></div></div></div><p>We create<a id="id292" class="indexterm"></a> a TCP Socket Server by running <code class="literal">netcat</code>, a small utility found in most Linux systems, as a data server with the command <code class="literal">&gt; nc -lk 9999</code>, where <code class="literal">9999</code> is the port where we are sending data:</p><div class="informalexample"><pre class="programlisting">#
# Socket Server
#
an@an-VB:~$ nc -lk 9999
hello world
how are you
hello  world
cool it works</pre></div><p>Once netcat is running, we will open a second console with our Spark Streaming client to receive the data and process. As soon as the Spark Streaming client console is listening, we start<a id="id293" class="indexterm"></a> typing the words to be processed, that is, <code class="literal">hello world</code>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec49"></a>Processing live data</h3></div></div></div><p>We <a id="id294" class="indexterm"></a>will be using the example program provided in <a id="id295" class="indexterm"></a>the Spark bundle for Spark Streaming called <code class="literal">network_wordcount.py</code>. It can be found on the GitHub repository under <a class="ulink" href="https://github.com/apache/spark/blob/master/examples/src/main/python/streaming/network_wordcount.py" target="_blank">https://github.com/apache/spark/blob/master/examples/src/main/python/streaming/network_wordcount.py</a>. The code is as follows:</p><div class="informalexample"><pre class="programlisting">"""
 Counts words in UTF8 encoded, '\n' delimited text received from the network every second.
 Usage: network_wordcount.py &lt;hostname&gt; &lt;port&gt;
   &lt;hostname&gt; and &lt;port&gt; describe the TCP server that Spark Streaming would connect to receive data.
 To run this on your local machine, you need to first run a Netcat server
    `$ nc -lk 9999`
 and then run the example
    `$ bin/spark-submit examples/src/main/python/streaming/network_wordcount.py localhost 9999`
"""
from __future__ import print_function

import sys

from pyspark import SparkContext
from pyspark.streaming import StreamingContext

if __name__ == "__main__":
    if len(sys.argv) != 3:
        print("Usage: network_wordcount.py &lt;hostname&gt; &lt;port&gt;", file=sys.stderr)
        exit(-1)
    sc = SparkContext(appName="PythonStreamingNetworkWordCount")
    ssc = StreamingContext(sc, 1)

    lines = ssc.socketTextStream(sys.argv[1], int(sys.argv[2]))
    counts = lines.flatMap(lambda line: line.split(" "))\
                  .map(lambda word: (word, 1))\
                  .reduceByKey(lambda a, b: a+b)
    counts.pprint()

    ssc.start()
    ssc.awaitTermination()</pre></div><p>Here, we<a id="id296" class="indexterm"></a> explain the steps of the program:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>The code first initializes a Spark Streaming Context with the command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>ssc = StreamingContext(sc, 1)</strong></span>
</pre></div></li><li><p>Next, the streaming computation is set up.</p></li><li><p>One or more DStream objects that receive data are defined to connect to localhost or <code class="literal">127.0.0.1</code> on <code class="literal">port 9999</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>stream = ssc.socketTextStream("127.0.0.1", 9999)</strong></span>
</pre></div></li><li><p>The DStream computation is defined: transformations and output operations:</p><div class="informalexample"><pre class="programlisting">stream.map(x: lambda (x,1))
.reduce(a+b)
.print()</pre></div></li><li><p>Computation is started:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>ssc.start()</strong></span>
</pre></div></li><li><p>Program termination is pending manual or error processing completion:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>ssc.awaitTermination()</strong></span>
</pre></div></li><li><p>Manual completion is an option when a completion condition is known:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>ssc.stop()</strong></span>
</pre></div></li></ol></div><p>We can monitor the Spark Streaming application by visiting the Spark monitoring home page at <code class="literal">localhost:4040</code>.</p><p>Here's the result of running the program and feeding the words on the <code class="literal">netcat</code> 4server console:</p><div class="informalexample"><pre class="programlisting">#
# Socket Client
# an@an-VB:~/spark/spark-1.5.0-bin-hadoop2.6$ ./bin/spark-submit examples/src/main/python/streaming/network_wordcount.py localhost 9999</pre></div><p>Run the Spark Streaming <code class="literal">network_count</code> program by connecting to the socket localhost on <code class="literal">port 9999</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>an@an-VB:~/spark/spark-1.5.0-bin-hadoop2.6$ ./bin/spark-submit examples/src/main/python/streaming/network_wordcount.py localhost 9999</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>Time: 2015-10-18 20:06:06</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>(u'world', 1)</strong></span>
<span class="strong"><strong>(u'hello', 1)</strong></span>

<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>Time: 2015-10-18 20:06:07</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>. . .</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>Time: 2015-10-18 20:06:17</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>(u'you', 1)</strong></span>
<span class="strong"><strong>(u'how', 1)</strong></span>
<span class="strong"><strong>(u'are', 1)</strong></span>

<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>Time: 2015-10-18 20:06:18</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>

<span class="strong"><strong>. . .</strong></span>

<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>Time: 2015-10-18 20:06:26</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>(u'', 1)</strong></span>
<span class="strong"><strong>(u'world', 1)</strong></span>
<span class="strong"><strong>(u'hello', 1)</strong></span>

<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>Time: 2015-10-18 20:06:27</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>. . .</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>Time: 2015-10-18 20:06:37</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>(u'works', 1)</strong></span>
<span class="strong"><strong>(u'it', 1)</strong></span>
<span class="strong"><strong>(u'cool', 1)</strong></span>

<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>Time: 2015-10-18 20:06:38</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>
</pre></div><p>Thus, we <a id="id297" class="indexterm"></a>have established connection through the socket on <code class="literal">port 9999</code>, streamed the data sent by the <code class="literal">netcat</code> server, and performed a word count on the messages sent.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec37"></a>Manipulating Twitter data in real time</h2></div></div><hr /></div><p>Twitter <a id="id298" class="indexterm"></a>offers two APIs. One search API that essentially allows us to retrieve past tweets based on search terms. This is how we have been collecting our data from Twitter in the previous chapters of the book. Interestingly, for our current purpose, Twitter offers a live streaming API which allows to ingest tweets as they are emitted in the blogosphere.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec50"></a>Processing Tweets in real time from the Twitter firehose</h3></div></div></div><p>The <a id="id299" class="indexterm"></a>following program connects to the Twitter firehose and processes the incoming tweets to exclude deleted or invalid tweets and parses on the fly only the relevant ones to extract <code class="literal">screen name</code>, the actual tweet, or <code class="literal">tweet text</code>, <code class="literal">retweet</code> count, <code class="literal">geo-location</code> information. The processed tweets are gathered into an RDD Queue by Spark Streaming and then displayed on the console at a one-second interval:</p><div class="informalexample"><pre class="programlisting">"""
Twitter Streaming API Spark Streaming into an RDD-Queue to process tweets live
 

 Create a queue of RDDs that will be mapped/reduced one at a time in
 1 second intervals.

 To run this example use
    '$ bin/spark-submit examples/AN_Spark/AN_Spark_Code/s07_twitterstreaming.py'

"""
#
import time
from pyspark import SparkContext
from pyspark.streaming import StreamingContext
import twitter
import dateutil.parser
import json

# Connecting Streaming Twitter with Streaming Spark via Queue
class Tweet(dict):
    def __init__(self, tweet_in):
        super(Tweet, self).__init__(self)
        if tweet_in and 'delete' not in tweet_in:
            self['timestamp'] = dateutil.parser.parse(tweet_in[u'created_at']
                                ).replace(tzinfo=None).isoformat()
            self['text'] = tweet_in['text'].encode('utf-8')
            #self['text'] = tweet_in['text']
            self['hashtags'] = [x['text'].encode('utf-8') for x in tweet_in['entities']['hashtags']]
            #self['hashtags'] = [x['text'] for x in tweet_in['entities']['hashtags']]
            self['geo'] = tweet_in['geo']['coordinates'] if tweet_in['geo'] else None
            self['id'] = tweet_in['id']
            self['screen_name'] = tweet_in['user']['screen_name'].encode('utf-8')
            #self['screen_name'] = tweet_in['user']['screen_name']
            self['user_id'] = tweet_in['user']['id']

def connect_twitter():
    twitter_stream = twitter.TwitterStream(auth=twitter.OAuth(
        token = "get_your_own_credentials",
        token_secret = "get_your_own_credentials",
        consumer_key = "get_your_own_credentials",
        consumer_secret = "get_your_own_credentials"))
    return twitter_stream

def get_next_tweet(twitter_stream):
    stream = twitter_stream.statuses.sample(block=True)
    tweet_in = None
    while not tweet_in or 'delete' in tweet_in:
        tweet_in = stream.next()
        tweet_parsed = Tweet(tweet_in)
    return json.dumps(tweet_parsed)

def process_rdd_queue(twitter_stream):
    # Create the queue through which RDDs can be pushed to
    # a QueueInputDStream
    rddQueue = []
    for i in range(3):
        rddQueue += [ssc.sparkContext.parallelize([get_next_tweet(twitter_stream)], 5)]

    lines = ssc.queueStream(rddQueue)
    lines.pprint()
    
if __name__ == "__main__":
    sc = SparkContext(appName="PythonStreamingQueueStream")
    ssc = StreamingContext(sc, 1)
    
    # Instantiate the twitter_stream
    twitter_stream = connect_twitter()
    # Get RDD queue of the streams json or parsed
    process_rdd_queue(twitter_stream)
    
    ssc.start()
    time.sleep(2)
    ssc.stop(stopSparkContext=True, stopGraceFully=True)</pre></div><p>When<a id="id300" class="indexterm"></a> we run this program, it delivers the following output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>an@an-VB:~/spark/spark-1.5.0-bin-hadoop2.6$ bin/spark-submit examples/AN_Spark/AN_Spark_Code/s07_twitterstreaming.py</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>Time: 2015-11-03 21:53:14</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>{"user_id": 3242732207, "screen_name": "cypuqygoducu", "timestamp": "2015-11-03T20:53:04", "hashtags": [], "text": "RT @VIralBuzzNewss: Our Distinctive Edition Holiday break Challenge Is In this article! Hooray!... -  https://t.co/9d8wumrd5v https://t.co/\u2026", "geo": null, "id": 661647303678259200}</strong></span>

<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>Time: 2015-11-03 21:53:15</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>{"user_id": 352673159, "screen_name": "melly_boo_orig", "timestamp": "2015-11-03T20:53:05", "hashtags": ["eminem"], "text": "#eminem https://t.co/GlEjPJnwxy", "geo": null, "id": 661647307847409668}</strong></span>

<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>Time: 2015-11-03 21:53:16</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>{"user_id": 500620889, "screen_name": "NBAtheist", "timestamp": "2015-11-03T20:53:06", "hashtags": ["tehInterwebbies", "Nutters"], "text": "See? That didn't take long or any actual effort. This is #tehInterwebbies ... #Nutters Abound! https://t.co/QS8gLStYFO", "geo": null, "id": 661647312062709761}</strong></span>
</pre></div><p>So, we got an example of streaming tweets with Spark and processing them on the fly.</p></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec38"></a>Building a reliable and scalable streaming app</h2></div></div><hr /></div><p>Ingesting <a id="id301" class="indexterm"></a>data is the process of acquiring data from various sources and storing it for processing immediately or at a later stage. Data consuming systems are dispersed and can be physically and architecturally far from the sources. Data ingestion is often implemented manually with scripts and rudimentary automation. It actually calls for higher level frameworks like Flume and Kafka.</p><p>The challenges of data ingestion arise from the fact that the sources are physically spread out and are transient which makes the integration brittle. Data production is continuous for weather, traffic, social media, network activity, shop floor sensors, security, and surveillance. Ever increasing data volumes and rates coupled with ever changing data structure and semantics makes data ingestion ad hoc and error prone.</p><p>The aim is<a id="id302" class="indexterm"></a> to become more agile, reliable, and scalable. Agility, reliability, and scalability of the data ingestion determine the overall health of the pipeline. Agility means integrating new sources as they arise and incorporating changes to existing sources as needed. In order to ensure safety and reliability, we need to protect the infrastructure against data loss and downstream applications from silent data corruption at ingress. Scalability avoids ingest bottlenecks while keeping cost tractable.</p><div class="informaltable"><table style="border-collapse: collapse;border-top: 0.5pt solid ; border-bottom: 0.5pt solid ; "><colgroup><col align="left" /><col align="left" /><col align="left" /></colgroup><thead><tr><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Ingest Mode</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Description</p>
</th><th style="border-bottom: 0.5pt solid ; " align="left" valign="bottom">
<p>Example</p>
</th></tr></thead><tbody><tr><td style="" align="left" valign="top">
<p>Manual or Scripted</p>
</td><td style="" align="left" valign="top">
<p>File copy using command line interface or GUI interface</p>
</td><td style="" align="left" valign="top">
<p>HDFS Client, Cloudera Hue</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Batch Data Transport</p>
</td><td style="" align="left" valign="top">
<p>Bulk data transport <a id="id303" class="indexterm"></a>using tools</p>
</td><td style="" align="left" valign="top">
<p>DistCp, Sqoop</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Micro Batch</p>
</td><td style="" align="left" valign="top">
<p>Transport of small <a id="id304" class="indexterm"></a>batches of data</p>
</td><td style="" align="left" valign="top">
<p>Sqoop, Sqoop2</p>
<p>Storm</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Pipelining</p>
</td><td style="" align="left" valign="top">
<p>Flow like transport <a id="id305" class="indexterm"></a>of event streams</p>
</td><td style="" align="left" valign="top">
<p>Flume Scribe</p>
</td></tr><tr><td style="" align="left" valign="top">
<p>Message Queue</p>
</td><td style="" align="left" valign="top">
<p>Publish Subscribe message <a id="id306" class="indexterm"></a>bus of events</p>
</td><td style="" align="left" valign="top">
<p>Kafka, Kinesis</p>
</td></tr></tbody></table></div><p>In order to enable an event-driven business that is able to ingest multiple streams of data, process it in flight, and make sense of it all to get to rapid decisions, the key driver is the Unified Log.</p><p>A Unified Log is a centralized enterprise structured log available for real-time subscription. All the organization's data is put in a central log for subscription. Records are numbered beginning with zero in the order that they are written. It is also known as a commit log or journal. The concept of the <span class="emphasis"><em>Unified Log</em></span> is the central tenet of the Kappa architecture.</p><p>The properties<a id="id307" class="indexterm"></a> of the Unified Log are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Unified</strong></span>: There is a single deployment for the entire organization</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Append only</strong></span>: Events are immutable and are appended</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Ordered</strong></span>: Each <a id="id308" class="indexterm"></a>event has a unique offset within a shard</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Distributed</strong></span>: For fault tolerance purpose, the Unified Log is distributed redundantly on a cluster of computers</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Fast</strong></span>: The <a id="id309" class="indexterm"></a>systems ingests thousands of messages per second</p></li></ul></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec51"></a>Setting up Kafka</h3></div></div></div><p>In order<a id="id310" class="indexterm"></a> to isolate downstream particular consumption<a id="id311" class="indexterm"></a> of data from the vagaries of upstream emission of data, we need to decouple the providers of data from the receivers or consumers of data. As they are living in two different worlds with different cycles and constraints, Kafka decouples the data pipelines.</p><p>Apache Kafka is <a id="id312" class="indexterm"></a>a distributed publish subscribe messaging system rethought as a distributed commit log. The messages are stored by topic.</p><p>Apache Kafka <a id="id313" class="indexterm"></a>has the following properties. It supports:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>High throughput for high volume of events feeds</p></li><li style="list-style-type: disc"><p>Real-time processing of new and derived feeds</p></li><li style="list-style-type: disc"><p>Large data backlogs and persistence for offline consumption</p></li><li style="list-style-type: disc"><p>Low latency as enterprise wide messaging system</p></li><li style="list-style-type: disc"><p>Fault tolerance thanks to its distributed nature</p></li></ul></div><p>Messages are stored in partition with a unique sequential ID called <code class="literal">offset</code>. Consumers track their pointers via tuple of (<code class="literal">offset</code>, <code class="literal">partition</code>, <code class="literal">topic</code>).</p><p>Let's dive deeper in the anatomy of Kafka.</p><p>Kafka has essentially three components: <span class="emphasis"><em>producers</em></span>, <span class="emphasis"><em>consumers</em></span> and <span class="emphasis"><em>brokers</em></span>. Producers push and write data to brokers. Consumers pull and read data from brokers. Brokers do not push messages to consumers. Consumers pull message from brokers. The setup is distributed and coordinated by Apache Zookeeper.</p><p>The brokers manage and store the data in topics. Topics are split in replicated partitions. The data is persisted in the broker, but not removed upon consumption, but until retention period. If a consumer fails, it can always go back to the broker to fetch the data.</p><p>Kafka requires Apache ZooKeeper. ZooKeeper is a high-performance coordination service for distributed applications. It centrally manages configuration, registry or naming service, group membership, lock, and synchronization for coordination between servers. It provides a hierarchical namespace with metadata, monitoring statistics, and state of the cluster. ZooKeeper can introduce brokers and consumers on the fly and then rebalances the cluster.</p><p>Kafka producers do not need ZooKeeper. Kafka brokers use ZooKeeper to provide general state information as well elect leader in case of failure. Kafka consumers use ZooKeeper to track message offset. Newer versions of Kafka will save the consumers to go through ZooKeeper and can retrieve the Kafka special topics information. Kafka provides <a id="id314" class="indexterm"></a>automatic load balancing for producers.</p><p>The following<a id="id315" class="indexterm"></a> diagram gives an overview of the Kafka setup:</p><div class="mediaobject"><img src="graphics/B03968_05_07.jpg" /></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl3sec08"></a>Installing and testing Kafka</h4></div></div></div><p>We<a id="id316" class="indexterm"></a> will <a id="id317" class="indexterm"></a>download the Apache Kafka binaries from <a id="id318" class="indexterm"></a>the dedicated web page at <a class="ulink" href="http://kafka.apache.org/downloads.html" target="_blank">http://kafka.apache.org/downloads.html</a> and install the software in our machine using the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Download the code.</p></li><li><p>Download the 0.8.2.0 release and <code class="literal">un-tar</code> it:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; tar -xzf kafka_2.10-0.8.2.0.tgz</strong></span>
<span class="strong"><strong>&gt; cd kafka_2.10-0.8.2.0</strong></span>
</pre></div></li><li><p>Start <code class="literal">zooeeper</code>. Kafka uses ZooKeeper so we need to first start a ZooKeeper server. We will use the convenience script packaged with Kafka to get a single-node ZooKeeper instance.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; bin/zookeeper-server-start.sh config/zookeeper.properties</strong></span>
<span class="strong"><strong>an@an-VB:~/kafka/kafka_2.10-0.8.2.0$ bin/zookeeper-server-start.sh config/zookeeper.properties</strong></span>

<span class="strong"><strong>[2015-10-31 22:49:14,808] INFO Reading configuration from: config/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)</strong></span>
<span class="strong"><strong>[2015-10-31 22:49:14,816] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)...</strong></span>
</pre></div></li><li><p>Now<a id="id319" class="indexterm"></a> launch<a id="id320" class="indexterm"></a> the Kafka server:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; bin/kafka-server-start.sh config/server.properties</strong></span>

<span class="strong"><strong>an@an-VB:~/kafka/kafka_2.10-0.8.2.0$ bin/kafka-server-start.sh config/server.properties</strong></span>
<span class="strong"><strong>[2015-10-31 22:52:04,643] INFO Verifying properties (kafka.utils.VerifiableProperties)</strong></span>
<span class="strong"><strong>[2015-10-31 22:52:04,714] INFO Property broker.id is overridden to 0 (kafka.utils.VerifiableProperties)</strong></span>
<span class="strong"><strong>[2015-10-31 22:52:04,715] INFO Property log.cleaner.enable is overridden to false (kafka.utils.VerifiableProperties)</strong></span>
<span class="strong"><strong>[2015-10-31 22:52:04,715] INFO Property log.dirs is overridden to /tmp/kafka-logs (kafka.utils.VerifiableProperties) [2013-04-22 15:01:47,051] INFO Property socket.send.buffer.bytes is overridden to 1048576 (kafka.utils.VerifiableProperties)</strong></span>
</pre></div></li><li><p>Create a topic. Let's create a topic named test with a single partition and only one replica:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test</strong></span>
</pre></div></li><li><p>We can now see that topic if we run the <code class="literal">list</code> topic command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; bin/kafka-topics.sh --list --zookeeper localhost:2181</strong></span>
<span class="strong"><strong>Test</strong></span>
<span class="strong"><strong>an@an-VB:~/kafka/kafka_2.10-0.8.2.0$ bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test</strong></span>
<span class="strong"><strong>Created topic "test".</strong></span>
<span class="strong"><strong>an@an-VB:~/kafka/kafka_2.10-0.8.2.0$ bin/kafka-topics.sh --list --zookeeper localhost:2181</strong></span>
<span class="strong"><strong>test</strong></span>
</pre></div></li><li><p>Check the Kafka installation by creating a producer and consumer. We first launch a <code class="literal">producer</code> and type a message in the console:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>an@an-VB:~/kafka/kafka_2.10-0.8.2.0$ bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test</strong></span>
<span class="strong"><strong>[2015-10-31 22:54:43,698] WARN Property topic is not valid (kafka.utils.VerifiableProperties)</strong></span>
<span class="strong"><strong>This is a message</strong></span>
<span class="strong"><strong>This is another message</strong></span>
</pre></div></li><li><p>We<a id="id321" class="indexterm"></a> then <a id="id322" class="indexterm"></a>launch a consumer to check that we receive the message:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>an@an-VB:~$ cd kafka/</strong></span>
<span class="strong"><strong>an@an-VB:~/kafka$ cd kafka_2.10-0.8.2.0/</strong></span>
<span class="strong"><strong>an@an-VB:~/kafka/kafka_2.10-0.8.2.0$ bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --from-beginning</strong></span>
<span class="strong"><strong>This is a message</strong></span>
<span class="strong"><strong>This is another message</strong></span>
</pre></div></li></ol></div><p>The messages were appropriately received by the consumer:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Check Kafka and Spark Streaming consumer. We will be using the Spark Streaming Kafka word count example provided in the Spark bundle. A word of caution: we have to bind the Kafka packages, <code class="literal">--packages org.apache.spark:spark-streaming-kafka_2.10:1.5.0</code>, when we submit the Spark job. The command is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>./bin/spark-submit --packages org.apache.spark:spark-streaming-kafka_2.10:1.5.0 \ examples/src/main/python/streaming/kafka_wordcount.py \</strong></span>

<span class="strong"><strong>localhost:2181 test</strong></span>
</pre></div></li><li><p>When we launch the Spark Streaming word count program with Kafka, we get the following output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>an@an-VB:~/spark/spark-1.5.0-bin-hadoop2.6$ ./bin/spark-submit --packages org.apache.spark:spark-streaming-kafka_2.10:1.5.0 examples/src/main/python/streaming/kafka_wordcount.py </strong></span>
<span class="strong"><strong>localhost:2181 test</strong></span>

<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>Time: 2015-10-31 23:46:33</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>(u'', 1)</strong></span>
<span class="strong"><strong>(u'from', 2)</strong></span>
<span class="strong"><strong>(u'Hello', 2)</strong></span>
<span class="strong"><strong>(u'Kafka', 2)</strong></span>

<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>Time: 2015-10-31 23:46:34</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>

<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>Time: 2015-10-31 23:46:35</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>
</pre></div></li><li><p>Install <a id="id323" class="indexterm"></a>the Kafka Python driver in order to be able to programmatically <a id="id324" class="indexterm"></a>develop Producers and Consumers and interact with Kafka and Spark using Python. We will use the road-tested library<a id="id325" class="indexterm"></a> from David Arthur, aka, Mumrah on GitHub (<a class="ulink" href="https://github.com/mumrah" target="_blank">https://github.com/mumrah</a>). We can pip install it as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; pip install kafka-python</strong></span>
<span class="strong"><strong>an@an-VB:~$ pip install kafka-python</strong></span>
<span class="strong"><strong>Collecting kafka-python</strong></span>
<span class="strong"><strong>  Downloading kafka-python-0.9.4.tar.gz (63kB)</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>Successfully installed kafka-python-0.9.4</strong></span>
</pre></div></li></ol></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl3sec09"></a>Developing producers</h4></div></div></div><p>The<a id="id326" class="indexterm"></a> following program creates a Simple Kafka Producer that will emit the message <span class="emphasis"><em>this is a message sent from the Kafka producer:</em></span> five times, followed by a time stamp every second:</p><div class="informalexample"><pre class="programlisting">#
# kafka producer
#
#
import time
from kafka.common import LeaderNotAvailableError
from kafka.client import KafkaClient
from kafka.producer import SimpleProducer
from datetime import datetime

def print_response(response=None):
    if response:
        print('Error: {0}'.format(response[0].error))
        print('Offset: {0}'.format(response[0].offset))

def main():
    kafka = KafkaClient("localhost:9092")
    producer = SimpleProducer(kafka)
    try:
        time.sleep(5)
        topic = 'test'
        for i in range(5):
            time.sleep(1)
            msg = 'This is a message sent from the kafka producer: ' \
                  + str(datetime.now().time()) + ' -- '\
                  + str(datetime.now().strftime("%A, %d %B %Y %I:%M%p"))
            print_response(producer.send_messages(topic, msg))
    except LeaderNotAvailableError:
        # https://github.com/mumrah/kafka-python/issues/249
        time.sleep(1)
        print_response(producer.send_messages(topic, msg))
 
    kafka.close()
 
if __name__ == "__main__":
    main()</pre></div><p>When<a id="id327" class="indexterm"></a> we run this program, the following output is generated:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>an@an-VB:~/spark/spark-1.5.0-bin-hadoop2.6/examples/AN_Spark/AN_Spark_Code$ python s08_kafka_producer_01.py</strong></span>
<span class="strong"><strong>Error: 0</strong></span>
<span class="strong"><strong>Offset: 13</strong></span>
<span class="strong"><strong>Error: 0</strong></span>
<span class="strong"><strong>Offset: 14</strong></span>
<span class="strong"><strong>Error: 0</strong></span>
<span class="strong"><strong>Offset: 15</strong></span>
<span class="strong"><strong>Error: 0</strong></span>
<span class="strong"><strong>Offset: 16</strong></span>
<span class="strong"><strong>Error: 0</strong></span>
<span class="strong"><strong>Offset: 17</strong></span>
<span class="strong"><strong>an@an-VB:~/spark/spark-1.5.0-bin-hadoop2.6/examples/AN_Spark/AN_Spark_Code$</strong></span>
</pre></div><p>It tells<a id="id328" class="indexterm"></a> us there were no errors and gives the offset of the messages given by the Kafka broker.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl3sec10"></a>Developing consumers</h4></div></div></div><p>To fetch<a id="id329" class="indexterm"></a> the messages from the Kafka brokers, we develop a Kafka consumer:</p><div class="informalexample"><pre class="programlisting"># kafka consumer
# consumes messages from "test" topic and writes them to console.
#
from kafka.client import KafkaClient
from kafka.consumer import SimpleConsumer

def main():
  kafka = KafkaClient("localhost:9092")
  print("Consumer established connection to kafka")
  consumer = SimpleConsumer(kafka, "my-group", "test")
  for message in consumer:
    # This will wait and print messages as they become available
    print(message)

if __name__ == "__main__":
    main()</pre></div><p>When we run this program, we effectively confirm that the consumer received all the messages:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>an@an-VB:~$ cd ~/spark/spark-1.5.0-bin-hadoop2.6/examples/AN_Spark/AN_Spark_Code/</strong></span>
<span class="strong"><strong>an@an-VB:~/spark/spark-1.5.0-bin-hadoop2.6/examples/AN_Spark/AN_Spark_Code$ python s08_kafka_consumer_01.py</strong></span>
<span class="strong"><strong>Consumer established connection to kafka</strong></span>
<span class="strong"><strong>OffsetAndMessage(offset=13, message=Message(magic=0, attributes=0, key=None, value='This is a message sent from the kafka producer: 11:50:17.867309Sunday, 01 November 2015 11:50AM'))</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>OffsetAndMessage(offset=17, message=Message(magic=0, attributes=0, key=None, value='This is a message sent from the kafka producer: 11:50:22.051423Sunday, 01 November 2015 11:50AM'))</strong></span>
</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl3sec11"></a>Developing a Spark Streaming consumer for Kafka</h4></div></div></div><p>Based<a id="id330" class="indexterm"></a> on the example code provided in the Spark Streaming bundle, we will create a Spark Streaming consumer for Kafka and perform a word count on the messages stored with the brokers:</p><div class="informalexample"><pre class="programlisting">#
# Kafka Spark Streaming Consumer    
#
from __future__ import print_function

import sys

from pyspark import SparkContext
from pyspark.streaming import StreamingContext
from pyspark.streaming.kafka import KafkaUtils

if __name__ == "__main__":
    if len(sys.argv) != 3:
        print("Usage: kafka_spark_consumer_01.py &lt;zk&gt; &lt;topic&gt;", file=sys.stderr)
        exit(-1)

    sc = SparkContext(appName="PythonStreamingKafkaWordCount")
    ssc = StreamingContext(sc, 1)

    zkQuorum, topic = sys.argv[1:]
    kvs = KafkaUtils.createStream(ssc, zkQuorum, "spark-streaming-consumer", {topic: 1})
    lines = kvs.map(lambda x: x[1])
    counts = lines.flatMap(lambda line: line.split(" ")) \
        .map(lambda word: (word, 1)) \
        .reduceByKey(lambda a, b: a+b)
    counts.pprint()

    ssc.start()
    ssc.awaitTermination()</pre></div><p>Run this program with the following Spark submit command:</p><div class="informalexample"><pre class="programlisting">./bin/spark-submit --packages org.apache.spark:spark-streaming-kafka_2.10:1.5.0 examples/AN_Spark/AN_Spark_Code/s08_kafka_spark_consumer_01.py localhost:2181 test</pre></div><p>We get the following output:</p><div class="informalexample"><pre class="programlisting">an@an-VB:~$ cd spark/spark-1.5.0-bin-hadoop2.6/
an@an-VB:~/spark/spark-1.5.0-bin-hadoop2.6$ ./bin/spark-submit \
&gt;     --packages org.apache.spark:spark-streaming-kafka_2.10:1.5.0 \
&gt;     examples/AN_Spark/AN_Spark_Code/s08_kafka_spark_consumer_01.py localhost:2181 test
...
:: retrieving :: org.apache.spark#spark-submit-parent
  confs: [default]
  0 artifacts copied, 10 already retrieved (0kB/18ms)
-------------------------------------------
Time: 2015-11-01 12:13:16
-------------------------------------------

-------------------------------------------
Time: 2015-11-01 12:13:17
-------------------------------------------

-------------------------------------------
Time: 2015-11-01 12:13:18
-------------------------------------------

-------------------------------------------
Time: 2015-11-01 12:13:19
-------------------------------------------
(u'a', 5)
(u'the', 5)
(u'11:50AM', 5)
(u'from', 5)
(u'This', 5)
(u'11:50:21.044374Sunday,', 1)
(u'message', 5)
(u'11:50:20.036422Sunday,', 1)
(u'11:50:22.051423Sunday,', 1)
(u'11:50:17.867309Sunday,', 1)
...

-------------------------------------------
Time: 2015-11-01 12:13:20
-------------------------------------------

-------------------------------------------
Time: 2015-11-01 12:13:21
-------------------------------------------</pre></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec52"></a>Exploring flume</h3></div></div></div><p>Flume is<a id="id331" class="indexterm"></a> a continuous ingestion system. It was originally designed to be a log aggregation system, but it evolved to handle any type of streaming event data.</p><p>Flume is a distributed, reliable, scalable, and available pipeline system for efficient collection, aggregation, and transport of large volumes of data. It has built-in support for contextual routing, filtering replication, and multiplexing. It is robust and fault tolerant, with tunable reliability mechanisms and many failover and recovery mechanisms. It uses a simple extensible <a id="id332" class="indexterm"></a>data model that allows for real time analytic application.</p><p>Flume <a id="id333" class="indexterm"></a>offers the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Guaranteed delivery semantics</p></li><li style="list-style-type: disc"><p>Low latency reliable data transfer</p></li><li style="list-style-type: disc"><p>Declarative configuration with no coding required</p></li><li style="list-style-type: disc"><p>Extendable and customizable settings</p></li><li style="list-style-type: disc"><p>Integration with most commonly used end-points</p></li></ul></div><p>The anatomy <a id="id334" class="indexterm"></a>of Flume contains the following elements:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Event</strong></span>: An event is <a id="id335" class="indexterm"></a>the fundamental unit of data that is transported by Flume from source to destination. It is like a message with a byte array payload opaque to Flume and optional headers used for contextual routing.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Client</strong></span>: A <a id="id336" class="indexterm"></a>client produces and transmits events. A client decouples Flume from the data consumers. It is an entity that generates events and sends them to one or more agents. Custom client or Flume log4J append program or embedded application agent can be client.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Agent</strong></span>: An agent is a container hosting sources, channels, sinks, and other elements that enable the transportation of events from one place to the other. It provides configuration, life cycle management and monitoring for hosted components. An agent is a physical Java virtual machine running Flume.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Source</strong></span>: Source<a id="id337" class="indexterm"></a> is the entity through which Flume receives events. Sources require at least one channel to function in order to either actively poll data or passively wait for data to be delivered to them. A variety of sources allow data to be collected, such as log4j logs and syslogs.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Sink</strong></span>: Sink is<a id="id338" class="indexterm"></a> the entity that drains data from the channel and delivers it to the next destination. A variety of sinks allow data to be streamed to a range of destinations. Sinks support serialization to user's format. One example is the HDFS sink that writes events to HDFS.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Channel</strong></span>: Channel is the conduit between the source and the sink that buffers incoming events until drained by sinks. Sources feed events into the channel and the sinks drain the channel. Channels decouple the impedance of upstream and downstream systems. Burst of data upstream is damped by the channels. Failures downstream are transparently absorbed by the channels. Sizing the channel capacity to cope with these events is key to realizing these benefits. Channels offer two levels of persistence: either memory channel, which is volatile if the JVM crashes, or File channel backed by Write Ahead Log that stores the information to disk. Channels are fully transactional.</p></li></ul></div><p>Let's illustrate <a id="id339" class="indexterm"></a>all <a id="id340" class="indexterm"></a>these <a id="id341" class="indexterm"></a>concepts:</p><div class="mediaobject"><img src="graphics/B03968_05_08.jpg" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec53"></a>Developing data pipelines with Flume, Kafka, and Spark</h3></div></div></div><p>Building <a id="id342" class="indexterm"></a>resilient data pipeline <a id="id343" class="indexterm"></a>leverages the learnings from<a id="id344" class="indexterm"></a> the previous sections. We are plumbing together data ingestion and transport with Flume, data brokerage with a reliable and sophisticated publish and subscribe messaging system such as Kafka, and finally process computation on the fly using Spark Streaming.</p><p>The following diagram illustrates the composition of streaming data pipelines as sequence of <span class="emphasis"><em>connect</em></span>, <span class="emphasis"><em>collect</em></span>, <span class="emphasis"><em>conduct</em></span>, <span class="emphasis"><em>compose</em></span>, <span class="emphasis"><em>consume</em></span>, <span class="emphasis"><em>consign</em></span>, and <span class="emphasis"><em>control</em></span> activities. These activities are configurable based on the use case:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Connect establishes the binding with the streaming API.</p></li><li style="list-style-type: disc"><p>Collect creates collection threads.</p></li><li style="list-style-type: disc"><p>Conduct decouples the data producers from the consumers by creating a buffer queue or publish-subscribe mechanism.</p></li><li style="list-style-type: disc"><p>Compose is focused on processing the data.</p></li><li style="list-style-type: disc"><p>Consume provisions the processed data for the consuming systems. Consign takes care of the data persistence.</p></li><li style="list-style-type: disc"><p>Control caters to governance and monitoring of the systems, data, and applications.</p></li></ul></div><div class="mediaobject"><img src="graphics/B03968_05_09.jpg" /></div><p>The <a id="id345" class="indexterm"></a>following diagram illustrates<a id="id346" class="indexterm"></a> the concepts of the streaming<a id="id347" class="indexterm"></a> data pipelines with its key components: Spark Streaming, Kafka, Flume, and low latency databases. In the consuming or controlling applications, we are monitoring our systems in real time (depicted by a monitor) or sending real-time alerts (depicted by red lights) in case certain thresholds are crossed.</p><div class="mediaobject"><img src="graphics/B03968_05_10.jpg" /></div><p>The following diagram illustrates Spark's unique ability to process in a single platform data in motion and data at rest while seamlessly interfacing with multiple persistence data stores as per the use case requirement.</p><p>This <a id="id348" class="indexterm"></a>diagram brings in one unified <a id="id349" class="indexterm"></a>whole all the concepts<a id="id350" class="indexterm"></a> discussed up to now. The top part describes the streaming processing pipeline. The bottom part describes the batch processing pipeline. They both share a common persistence layer in the middle of the diagram depicting the various modes of persistence and serialization.</p><div class="mediaobject"><img src="graphics/B03968_05_11.jpg" /></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec39"></a>Closing remarks on the Lambda and Kappa architecture</h2></div></div><hr /></div><p>Two <a id="id351" class="indexterm"></a>architecture paradigms are currently in vogue: the <a id="id352" class="indexterm"></a>Lambda and Kappa architectures.</p><p>Lambda is the brainchild of the Storm creator and main committer, Nathan Marz. It essentially advocates building a functional architecture on all data. The architecture has two branches. The first is a batch arm envisioned to be powered by Hadoop, where historical, high-latency, high-throughput data are pre-processed and made ready for consumption. The real-time arm is envisioned to be powered by Storm, and it processes incrementally streaming data, derives insights on the fly, and feeds aggregated information back to the batch storage.</p><p>Kappa is the<a id="id353" class="indexterm"></a> brainchild of one the main committer of Kafka, Jay Kreps, and <a id="id354" class="indexterm"></a>his colleagues at Confluent (previously at LinkedIn). It is advocating a full streaming pipeline, effectively implementing, at the enterprise level, the unified log enounced in the previous pages.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec54"></a>Understanding Lambda architecture</h3></div></div></div><p>Lambda<a id="id355" class="indexterm"></a> architecture combines batch and streaming data to provide a unified query mechanism on all available data. Lambda architecture envisions three layers: a batch layer where precomputed information are stored, a speed layer where real-time incremental information is processed as data streams, and finally the serving layer that merges batch and real-time views for ad hoc queries. The following diagram gives an overview of the Lambda architecture:</p><div class="mediaobject"><img src="graphics/B03968_05_12.jpg" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec55"></a>Understanding Kappa architecture</h3></div></div></div><p>The <a id="id356" class="indexterm"></a>Kappa architecture proposes to drive the full enterprise in streaming mode. The Kappa architecture arose from a critique from Jay Kreps and his colleagues at LinkedIn at the time. Since then, they moved and created Confluent with Apache Kafka as the main enabler of the Kappa architecture vision. The basic tenet is to move in all streaming mode with a Unified Log as the main backbone of the enterprise information architecture.</p><p>A Unified Log is a centralized enterprise structured log available for real-time subscription. All the<a id="id357" class="indexterm"></a> organization's data is put in a central log for subscription. Records are numbered beginning with zero so that they are written. It is also known as a commit log or journal. The concept of the Unified Log is the central tenet of the Kappa architecture.</p><p>The<a id="id358" class="indexterm"></a> properties of the unified log are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Unified</strong></span>: There is a single deployment for the entire organization</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Append only</strong></span>: Events are immutable and are appended</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Ordered</strong></span>: Each event has a unique offset within a shard</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Distributed</strong></span>: For fault tolerance purpose, the unified log is distributed redundantly on a cluster of computers</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Fast</strong></span>: The systems ingests thousands of messages per second</p></li></ul></div><p>The following screenshot captures the moment Jay Kreps announced his reservations about the Lambda architecture. His main reservation about the Lambda architecture is implementing the same job in two different systems, Hadoop and Storm, with each of their specific idiosyncrasies, and with all the complexities that come along with it. Kappa architecture processes the real-time data and reprocesses historical data in the same framework powered by Apache Kafka.</p><div class="mediaobject"><img src="graphics/B03968_05_13.jpg" /></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec40"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we laid out the foundations of streaming architecture apps and described their challenges, constraints, and benefits. We went under the hood and examined the inner working of Spark Streaming and how it fits with Spark Core and dialogues with Spark SQL and Spark MLlib. We illustrated the streaming concepts with TCP sockets, followed by live tweet ingestion and processing directly from the Twitter firehose. We discussed the notions of decoupling upstream data publishing from downstream data subscription and consumption using Kafka in order to maximize the resilience of the overall streaming architecture. We also discussed Flume—a reliable, flexible, and scalable data ingestion and transport pipeline system. The combination of Flume, Kafka, and Spark delivers unparalleled robustness, speed, and agility in an ever changing landscape. We closed the chapter with some remarks and observations on two streaming architectural paradigms, the Lambda and Kappa architectures.</p><p>The Lambda architecture combines batch and streaming data in a common query front-end. It was envisioned with Hadoop and Storm in mind initially. Spark has its own batch and streaming paradigms, and it offers a single environment with common code base to effectively bring this architecture paradigm to life.</p><p>The Kappa architecture promulgates the concept of the unified log, which creates an event-oriented architecture where all events in the enterprise are channeled in a centralized commit log that is available to all consuming systems in real time.</p><p>We are now ready for the visualization of the data collected and processed so far.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch06"></a>Chapter 6. Visualizing Insights and Trends</h2></div></div></div><p>So far, we have focused on the collection, analysis, and processing of data from Twitter. We have set the stage to use our data for visual rendering and extracting insights and trends. We will give a quick lay of the land about visualization tools in the Python ecosystem. We will highlight Bokeh as a powerful tool for rendering and viewing large datasets. Bokeh is part of the Python Anaconda Distribution ecosystem.</p><p>In this chapter, we will cover the following points:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Gauging the key words and memes within a social network community using charts and wordcloud</p></li><li style="list-style-type: disc"><p>Mapping the most active location where communities are growing around certain themes or topics</p></li></ul></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec41"></a>Revisiting the data-intensive apps architecture</h2></div></div><hr /></div><p>We have <a id="id359" class="indexterm"></a>reached the final layer of the data-intensive apps architecture: the engagement layer. This layer focuses on how to synthesize, emphasize, and visualize the key context relevant information for the data consumers. A bunch of numbers in a console will not suffice to engage with end-users. It is critical to present the mass of information in a rapid, digestible, and attractive fashion.</p><p>The following diagram sets the context of the chapter's focus highlighting the engagement layer.</p><div class="mediaobject"><img src="graphics/B03968_06_01.jpg" /></div><p>For Python <a id="id360" class="indexterm"></a>plotting and visualizations, we have quite a few tools and libraries. The most interesting and relevant ones for our purpose are the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>Matplotlib</strong></span> is <a id="id361" class="indexterm"></a>the grandfather of the Python plotting libraries. Matplotlib was originally the brainchild of <span class="emphasis"><em>John Hunter</em></span> who was an open source software proponent and established Matplotlib as one of the most prevalent plotting libraries both in the academic and the data scientific communities. Matplotlib allows the generation of plots, histograms, power spectra, bar charts, error charts, scatterplots, and so on. Examples can be found on <a id="id362" class="indexterm"></a>the Matplotlib dedicated website at <a class="ulink" href="http://matplotlib.org/examples/index.html" target="_blank">http://matplotlib.org/examples/index.html</a>.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Seaborn</strong></span>, developed by <span class="emphasis"><em>Michael Waskom</em></span>, is a great library to quickly visualize<a id="id363" class="indexterm"></a> statistical information. It is built on top of Matplotlib and integrates seamlessly with Pandas and the Python data stack, including Numpy. A gallery of graphs from Seaborn at <a class="ulink" href="http://stanford.edu/~mwaskom/software/seaborn/examples/index.html" target="_blank">http://stanford.edu/~mwaskom/software/seaborn/examples/index.html</a> shows<a id="id364" class="indexterm"></a> the potential of the library.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>ggplot</strong></span> is <a id="id365" class="indexterm"></a>relatively new and aims to offer the equivalent of the famous ggplot2 from the R ecosystem for the Python data wranglers. It has the same look and feel of ggplot2 and uses the same grammar of graphics as expounded by Hadley Wickham. The ggplot the Python port is developed by the team<a id="id366" class="indexterm"></a> at <code class="literal">yhat</code>. More information can be found at <a class="ulink" href="http://ggplot.yhathq.com" target="_blank">http://ggplot.yhathq.com</a>.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>D3.js</strong></span> is a <a id="id367" class="indexterm"></a>very popular, JavaScript library developed by <span class="emphasis"><em>Mike Bostock</em></span>. <span class="strong"><strong>D3</strong></span> stands for <span class="strong"><strong>Data Driven Documents</strong></span> and brings data to life on any <a id="id368" class="indexterm"></a>modern browser leveraging HTML, SVG, and CSS. It <a id="id369" class="indexterm"></a>delivers dynamic, powerful, interactive visualizations by manipulating the DOM, the Document Object Model. The Python community could not wait to integrate D3 with Matplotlib. Under the impulse of Jake Vanderplas, mpld3 was created with the aim of bringing <code class="literal">matplotlib</code> to the browser. Examples graphics are hosted at the following address: <a class="ulink" href="http://mpld3.github.io/index.html" target="_blank">http://mpld3.github.io/index.html</a>.</p></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>Bokeh</strong></span> aims<a id="id370" class="indexterm"></a> to deliver high-performance interactivity over very large or streaming datasets whilst leveraging lot of the concepts of <code class="literal">D3.js</code> without the burden of writing some intimidating <code class="literal">javascript</code> and <code class="literal">css</code> code. Bokeh delivers dynamic visualizations on the browser with or without a server. It integrates seamlessly with Matplotlib, Seaborn and ggplot and renders beautifully in IPython notebooks or Jupyter notebooks. Bokeh is actively developed by the team at Continuum.io and is an integral part of the Anaconda Python data stack.</p></li></ul></div><p>Bokeh server provides a full-fledged, dynamic plotting engine that materializes a reactive scene graph from JSON. It uses web sockets to keep state and update the HTML5 canvas using Backbone.js and Coffee-script under the hoods. Bokeh, as it is fueled by data in JSON, creates easy bindings for other languages such as R, Scala, and Julia.</p><p>This gives a high-level overview of the main plotting and visualization library. It is not exhaustive. Let's move to concrete examples of visualizations.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec42"></a>Preprocessing the data for visualization</h2></div></div><hr /></div><p>Before <a id="id371" class="indexterm"></a>jumping into the visualizations, we will do<a id="id372" class="indexterm"></a> some preparatory work on the data harvested:</p><div class="informalexample"><pre class="programlisting">In [16]:
# Read harvested data stored in csv in a Panda DF
import pandas as pd
csv_in = '/home/an/spark/spark-1.5.0-bin-hadoop2.6/examples/AN_Spark/data/unq_tweetstxt.csv'
pddf_in = pd.read_csv(csv_in, index_col=None, header=0, sep=';', encoding='utf-8')
In [20]:
print('tweets pandas dataframe - count:', pddf_in.count())
print('tweets pandas dataframe - shape:', pddf_in.shape)
print('tweets pandas dataframe - colns:', pddf_in.columns)
('tweets pandas dataframe - count:', Unnamed: 0    7540
id            7540
created_at    7540
user_id       7540
user_name     7538
tweet_text    7540
dtype: int64)
('tweets pandas dataframe - shape:', (7540, 6))
('tweets pandas dataframe - colns:', Index([u'Unnamed: 0', u'id', u'created_at', u'user_id', u'user_name', u'tweet_text'], dtype='object'))</pre></div><p>For <a id="id373" class="indexterm"></a>the purpose of our visualization activity, we <a id="id374" class="indexterm"></a>will use a dataset of 7,540 tweets. The key information is stored in the <code class="literal">tweet_text</code> column. We preview the data stored in the dataframe calling the <code class="literal">head()</code> function on the dataframe:</p><div class="informalexample"><pre class="programlisting">In [21]:
pddf_in.head()
Out[21]:
  Unnamed: 0   id   created_at   user_id   user_name   tweet_text
0   0   638830426971181057   Tue Sep 01 21:46:57 +0000 2015   3276255125   True Equality   ernestsgantt: BeyHiveInFrance: 9_A_6: dreamint...
1   1   638830426727911424   Tue Sep 01 21:46:57 +0000 2015   3276255125   True Equality   ernestsgantt: BeyHiveInFrance: PhuketDailyNews...
2   2   638830425402556417   Tue Sep 01 21:46:56 +0000 2015   3276255125   True Equality   ernestsgantt: BeyHiveInFrance: 9_A_6: ernestsg...
3   3   638830424563716097   Tue Sep 01 21:46:56 +0000 2015   3276255125   True Equality   ernestsgantt: BeyHiveInFrance: PhuketDailyNews...
4   4   638830422256816132   Tue Sep 01 21:46:56 +0000 2015   3276255125   True Equality   ernestsgantt: elsahel12: 9_A_6: dreamintention...</pre></div><p>We will now create some utility functions to clean up the tweet text and parse the twitter date. First, we import the Python regular expression regex library <code class="literal">re</code> and the time library to parse dates and time:</p><div class="informalexample"><pre class="programlisting">In [72]:
import re
import time</pre></div><p>We create a dictionary of regex that will be compiled and then passed as function:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>
<span class="strong"><strong>RT</strong></span>: The first regex with key <code class="literal">RT</code> looks for the keyword <code class="literal">RT</code> at the beginning of the tweet text:</p><div class="informalexample"><pre class="programlisting">re.compile(r'^RT'),</pre></div></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>ALNUM</strong></span>: The <a id="id375" class="indexterm"></a>second regex with key <code class="literal">ALNUM</code> looks for words including alphanumeric characters and underscore <a id="id376" class="indexterm"></a>sign preceded by the <code class="literal">@</code> symbol in the tweet text:</p><div class="informalexample"><pre class="programlisting">re.compile(r'(@[a-zA-Z0-9_]+)'),</pre></div></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>HASHTAG</strong></span>: The third regex with key <code class="literal">HASHTAG</code> looks for words including alphanumeric characters preceded by the <code class="literal">#</code> symbol in the tweet text:</p><div class="informalexample"><pre class="programlisting">re.compile(r'(#[\w\d]+)'),</pre></div></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>SPACES</strong></span>: The fourth regex with key <code class="literal">SPACES</code> looks for blank or line space characters in the tweet text:</p><div class="informalexample"><pre class="programlisting">re.compile(r'\s+'), </pre></div></li><li style="list-style-type: disc"><p>
<span class="strong"><strong>URL</strong></span>: The fifth regex with key <code class="literal">URL</code> looks for <code class="literal">url</code> addresses including alphanumeric characters preceded with <code class="literal">https://</code> or <code class="literal">http://</code> markers in the tweet text:</p><div class="informalexample"><pre class="programlisting">re.compile(r'([https://|http://]?[a-zA-Z\d\/]+[\.]+[a-zA-Z\d\/\.]+)')
In [24]:
regexp = {"RT": "^RT", "ALNUM": r"(@[a-zA-Z0-9_]+)",
          "HASHTAG": r"(#[\w\d]+)", "URL": r"([https://|http://]?[a-zA-Z\d\/]+[\.]+[a-zA-Z\d\/\.]+)",
          "SPACES":r"\s+"}
regexp = dict((key, re.compile(value)) for key, value in regexp.items())
In [25]:
regexp
Out[25]:
{'ALNUM': re.compile(r'(@[a-zA-Z0-9_]+)'),
 'HASHTAG': re.compile(r'(#[\w\d]+)'),
 'RT': re.compile(r'^RT'),
 'SPACES': re.compile(r'\s+'),
 'URL': re.compile(r'([https://|http://]?[a-zA-Z\d\/]+[\.]+[a-zA-Z\d\/\.]+)')}</pre></div></li></ul></div><p>We create a utility function to identify whether a tweet is a retweet or an original tweet:</p><div class="informalexample"><pre class="programlisting">In [77]:
def getAttributeRT(tweet):
    """ see if tweet is a RT """
    return re.search(regexp["RT"], tweet.strip()) != None</pre></div><p>Then, we extract all user handles in a tweet:</p><div class="informalexample"><pre class="programlisting">def getUserHandles(tweet):
    """ given a tweet we try and extract all user handles"""
    return re.findall(regexp["ALNUM"], tweet)</pre></div><p>We<a id="id377" class="indexterm"></a> also extract all hashtags in a tweet:</p><div class="informalexample"><pre class="programlisting">def getHashtags(tweet):
    """ return all hashtags"""
    return re.findall(regexp["HASHTAG"], tweet)</pre></div><p>Extract <a id="id378" class="indexterm"></a>all URL links in a tweet as follows:</p><div class="informalexample"><pre class="programlisting">def getURLs(tweet):
    """ URL : [http://]?[\w\.?/]+"""
    return re.findall(regexp["URL"], tweet)</pre></div><p>We strip all URL links and user handles preceded by <code class="literal">@</code> sign in a tweet text. This function will be the basis of the wordcloud we will build soon:</p><div class="informalexample"><pre class="programlisting">def getTextNoURLsUsers(tweet):
    """ return parsed text terms stripped of URLs and User Names in tweet text
        ' '.join(re.sub("(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)"," ",x).split()) """
    return ' '.join(re.sub("(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)|(RT)"," ", tweet).lower().split())</pre></div><p>We label the data so we can create groups of datasets for the wordcloud:</p><div class="informalexample"><pre class="programlisting">def setTag(tweet):
    """ set tags to tweet_text based on search terms from tags_list"""
    tags_list = ['spark', 'python', 'clinton', 'trump', 'gaga', 'bieber']
    lower_text = tweet.lower()
    return filter(lambda x:x.lower() in lower_text,tags_list)</pre></div><p>We parse the twitter date in the <code class="literal">yyyy-mm-dd hh:mm:ss</code> format:</p><div class="informalexample"><pre class="programlisting">def decode_date(s):
    """ parse Twitter date into format yyyy-mm-dd hh:mm:ss"""
    return time.strftime('%Y-%m-%d %H:%M:%S', time.strptime(s,'%a %b %d %H:%M:%S +0000 %Y'))</pre></div><p>We preview the data prior to processing:</p><div class="informalexample"><pre class="programlisting">In [43]:
pddf_in.columns
Out[43]:
Index([u'Unnamed: 0', u'id', u'created_at', u'user_id', u'user_name', u'tweet_text'], dtype='object')
In [45]:
# df.drop([Column Name or list],inplace=True,axis=1)
pddf_in.drop(['Unnamed: 0'], inplace=True, axis=1)
In [46]:
pddf_in.head()
Out[46]:
  id   created_at   user_id   user_name   tweet_text
0   638830426971181057   Tue Sep 01 21:46:57 +0000 2015   3276255125   True Equality   ernestsgantt: BeyHiveInFrance: 9_A_6: dreamint...
1   638830426727911424   Tue Sep 01 21:46:57 +0000 2015   3276255125   True Equality   ernestsgantt: BeyHiveInFrance: PhuketDailyNews...
2   638830425402556417   Tue Sep 01 21:46:56 +0000 2015   3276255125   True Equality   ernestsgantt: BeyHiveInFrance: 9_A_6: ernestsg...
3   638830424563716097   Tue Sep 01 21:46:56 +0000 2015   3276255125   True Equality   ernestsgantt: BeyHiveInFrance: PhuketDailyNews...
4   638830422256816132   Tue Sep 01 21:46:56 +0000 2015   3276255125   True Equality   ernestsgantt: elsahel12: 9_A_6: dreamintention...</pre></div><p>We <a id="id379" class="indexterm"></a>create new dataframe columns by applying the <a id="id380" class="indexterm"></a>utility functions described. We create a new column for <code class="literal">htag</code>, user handles, URLs, the text terms stripped from URLs, and unwanted characters and the labels. We finally parse the date:</p><div class="informalexample"><pre class="programlisting">In [82]:
pddf_in['htag'] = pddf_in.tweet_text.apply(getHashtags)
pddf_in['user_handles'] = pddf_in.tweet_text.apply(getUserHandles)
pddf_in['urls'] = pddf_in.tweet_text.apply(getURLs)
pddf_in['txt_terms'] = pddf_in.tweet_text.apply(getTextNoURLsUsers)
pddf_in['search_grp'] = pddf_in.tweet_text.apply(setTag)
pddf_in['date'] = pddf_in.created_at.apply(decode_date)</pre></div><p>The following code gives a quick snapshot of the newly generated dataframe:</p><div class="informalexample"><pre class="programlisting">In [83]:
pddf_in[2200:2210]
Out[83]:
  id   created_at   user_id   user_name   tweet_text   htag   urls   ptxt   tgrp   date   user_handles   txt_terms   search_grp
2200   638242693374681088   Mon Aug 31 06:51:30 +0000 2015   19525954   CENATIC   El impacto de @ApacheSpark en el procesamiento...   [#sparkSpecial]   [://t.co/4PQmJNuEJB]   el impacto de en el procesamiento de datos y e...   [spark]   2015-08-31 06:51:30   [@ApacheSpark]   el impacto de en el procesamiento de datos y e...   [spark]
2201   638238014695575552   Mon Aug 31 06:32:55 +0000 2015   51115854   Nawfal   Real Time Streaming with Apache Spark\nhttp://...   [#IoT, #SmartMelboune, #BigData, #Apachespark]   [://t.co/GW5PaqwVab]   real time streaming with apache spark iot smar...   [spark]   2015-08-31 06:32:55   []   real time streaming with apache spark iot smar...   [spark]
2202   638236084124516352   Mon Aug 31 06:25:14 +0000 2015   62885987   Mithun Katti   RT @differentsachin: Spark the flame of digita...   [#IBMHackathon, #SparkHackathon, #ISLconnectIN...   []   spark the flame of digital india ibmhackathon ...   [spark]   2015-08-31 06:25:14   [@differentsachin, @ApacheSpark]   spark the flame of digital india ibmhackathon ...   [spark]
2203   638234734649176064   Mon Aug 31 06:19:53 +0000 2015   140462395   solaimurugan v   Installing @ApacheMahout with @ApacheSpark 1.4...   []   [1.4.1, ://t.co/3c5dGbfaZe.]   installing with 1 4 1 got many more issue whil...   [spark]   2015-08-31 06:19:53   [@ApacheMahout, @ApacheSpark]   installing with 1 4 1 got many more issue whil...   [spark]
2204   638233517307072512   Mon Aug 31 06:15:02 +0000 2015   2428473836   Ralf Heineke   RT @RomeoKienzler: Join me @velocityconf on #m...   [#machinelearning, #devOps, #Bl]   [://t.co/U5xL7pYEmF]   join me on machinelearning based devops operat...   [spark]   2015-08-31 06:15:02   [@RomeoKienzler, @velocityconf, @ApacheSpark]   join me on machinelearning based devops operat...   [spark]
2205   638230184848687106   Mon Aug 31 06:01:48 +0000 2015   289355748   Akim Boyko   RT @databricks: Watch live today at 10am PT is...   []   [1.5, ://t.co/16cix6ASti]   watch live today at 10am pt is 1 5 presented b...   [spark]   2015-08-31 06:01:48   [@databricks, @ApacheSpark, @databricks, @pwen...   watch live today at 10am pt is 1 5 presented b...   [spark]
2206   638227830443110400   Mon Aug 31 05:52:27 +0000 2015   145001241   sachin aggarwal   Spark the flame of digital India @ #IBMHackath...   [#IBMHackathon, #SparkHackathon, #ISLconnectIN...   [://t.co/C1AO3uNexe]   spark the flame of digital india ibmhackathon ...   [spark]   2015-08-31 05:52:27   [@ApacheSpark]   spark the flame of digital india ibmhackathon ...   [spark]
2207   638227031268810752   Mon Aug 31 05:49:16 +0000 2015   145001241   sachin aggarwal   RT @pravin_gadakh: Imagine, innovate and Igni...   [#IBMHackathon, #ISLconnectIN2015]   []   gadakh imagine innovate and ignite digital ind...   [spark]   2015-08-31 05:49:16   [@pravin_gadakh, @ApacheSpark]   gadakh imagine innovate and ignite digital ind...   [spark]
2208   638224591920336896   Mon Aug 31 05:39:35 +0000 2015   494725634   IBM Asia Pacific   RT @sachinparmar: Passionate about Spark?? Hav...   [#IBMHackathon, #ISLconnectIN]   [India..]   passionate about spark have dreams of clean sa...   [spark]   2015-08-31 05:39:35   [@sachinparmar]   passionate about spark have dreams of clean sa...   [spark]
2209   638223327467692032   Mon Aug 31 05:34:33 +0000 2015   3158070968   Open Source India   "Game Changer" #ApacheSpark speeds up #bigdata...   [#ApacheSpark, #bigdata]   [://t.co/ieTQ9ocMim]   game changer apachespark speeds up bigdata pro...   [spark]   2015-08-31 05:34:33   []   game changer apachespark speeds up bigdata pro...   [spark]</pre></div><p>We save the processed information in a CSV format. We have 7,540 records and 13 columns. In <a id="id381" class="indexterm"></a>your case, the output will vary according<a id="id382" class="indexterm"></a> to the dataset you chose:</p><div class="informalexample"><pre class="programlisting">In [84]:
f_name = '/home/an/spark/spark-1.5.0-bin-hadoop2.6/examples/AN_Spark/data/unq_tweets_processed.csv'
pddf_in.to_csv(f_name, sep=';', encoding='utf-8', index=False)
In [85]:
pddf_in.shape
Out[85]:
(7540, 13)</pre></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec43"></a>Gauging words, moods, and memes at a glance</h2></div></div><hr /></div><p>We are now ready to proceed with building the wordclouds which will give us a sense of the important <a id="id383" class="indexterm"></a>words carried in those tweets. We will create wordclouds for the datasets harvested. Wordclouds extract the top words in a list of words and create a scatterplot of the words where the size of the word is correlated to its frequency. The more frequent the word in the dataset, the bigger will be the font size in the wordcloud rendering. They include three very different themes and two competing or analogous entities. Our first theme is obviously data processing and analytics, with Apache Spark and Python as our entities. Our second theme is the 2016 presidential election campaign, with the two contenders: Hilary Clinton and Donald Trump. Our last theme is the world of pop music with Justin Bieber and Lady Gaga as the two exponents.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec56"></a>Setting up wordcloud</h3></div></div></div><p>We will<a id="id384" class="indexterm"></a> illustrate the programming steps by analyzing the spark related tweets. We load the data and preview the dataframe:</p><div class="informalexample"><pre class="programlisting">In [21]:
import pandas as pd
csv_in = '/home/an/spark/spark-1.5.0-bin-hadoop2.6/examples/AN_Spark/data/spark_tweets.csv'
tspark_df = pd.read_csv(csv_in, index_col=None, header=0, sep=',', encoding='utf-8')
In [3]:
tspark_df.head(3)
Out[3]:
  id   created_at   user_id   user_name   tweet_text   htag   urls   ptxt   tgrp   date   user_handles   txt_terms   search_grp
0   638818911773856000   Tue Sep 01 21:01:11 +0000 2015   2511247075   Noor Din   RT @kdnuggets: R leads RapidMiner, Python catc...   [#KDN]   [://t.co/3bsaTT7eUs]   r leads rapidminer python catches up big data ...   [spark, python]   2015-09-01 21:01:11   [@kdnuggets]   r leads rapidminer python catches up big data ...   [spark, python]
1   622142176768737000   Fri Jul 17 20:33:48 +0000 2015   24537879   IBM Cloudant   Be one of the first to sign-up for IBM Analyti...   [#ApacheSpark, #SparkInsight]   [://t.co/C5TZpetVA6, ://t.co/R1L29DePaQ]   be one of the first to sign up for ibm analyti...   [spark]   2015-07-17 20:33:48   []   be one of the first to sign up for ibm analyti...   [spark]
2   622140453069169000   Fri Jul 17 20:26:57 +0000 2015   515145898   Arno Candel   Nice article on #apachespark, #hadoop and #dat...   [#apachespark, #hadoop, #datascience]   [://t.co/IyF44pV0f3]   nice article on apachespark hadoop and datasci...   [spark]   2015-07-17 20:26:57   [@h2oai]   nice article on apachespark hadoop and datasci...   [spark]</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note03"></a>Note</h3><p>The <a id="id385" class="indexterm"></a>wordcloud library we will use is the one developed by Andreas Mueller and hosted on his GitHub account at <a class="ulink" href="https://github.com/amueller/word_cloud" target="_blank">https://github.com/amueller/word_cloud</a>.</p></div><p>The <a id="id386" class="indexterm"></a>library requires <span class="strong"><strong>PIL</strong></span> (short for <span class="strong"><strong>Python Imaging Library</strong></span>). PIL is <a id="id387" class="indexterm"></a>easily installable by invoking <code class="literal">conda install pil</code>. PIL is a complex library to install and is not yet ported on Python 3.4, so we need to run a Python 2.7+ environment to be able to see our wordcloud:</p><div class="informalexample"><pre class="programlisting">#
# Install PIL (does not work with Python 3.4)
#
an@an-VB:~$ conda install pil

Fetching package metadata: ....
Solving package specifications: ..................
Package plan for installation in environment /home/an/anaconda:</pre></div><p>The following packages will be downloaded:</p><div class="informalexample"><pre class="programlisting">    package                    |            build
    ---------------------------|-----------------
    libpng-1.6.17              |                0         214 KB
    freetype-2.5.5             |                0         2.2 MB
    conda-env-2.4.4            |           py27_0          24 KB
    pil-1.1.7                  |           py27_2         650 KB
    ------------------------------------------------------------
                                           Total:         3.0 MB</pre></div><p>The following packages will be UPDATED:</p><div class="informalexample"><pre class="programlisting">    conda-env: 2.4.2-py27_0 --&gt; 2.4.4-py27_0
    freetype:  2.5.2-0      --&gt; 2.5.5-0     
    libpng:    1.5.13-1     --&gt; 1.6.17-0    
    pil:       1.1.7-py27_1 --&gt; 1.1.7-py27_2

Proceed ([y]/n)? y</pre></div><p>Next, we<a id="id388" class="indexterm"></a> install the wordcloud library:</p><div class="informalexample"><pre class="programlisting">#
# Install wordcloud
# Andreas Mueller
# https://github.com/amueller/word_cloud/blob/master/wordcloud/wordcloud.py
#

an@an-VB:~$ pip install wordcloud
Collecting wordcloud
  Downloading wordcloud-1.1.3.tar.gz (163kB)
    100% |████████████████████████████████| 163kB 548kB/s 
Building wheels for collected packages: wordcloud
  Running setup.py bdist_wheel for wordcloud
  Stored in directory: /home/an/.cache/pip/wheels/32/a9/74/58e379e5dc614bfd9dd9832d67608faac9b2bc6c194d6f6df5
Successfully built wordcloud
Installing collected packages: wordcloud
Successfully installed wordcloud-1.1.3</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec57"></a>Creating wordclouds</h3></div></div></div><p>At this stage, we<a id="id389" class="indexterm"></a> are ready to invoke the wordcloud program with the generated list of terms from the tweet text.</p><p>Let's get started with the wordcloud program by first calling <code class="literal">%matplotlib</code> inline to display the wordcloud in our notebook:</p><div class="informalexample"><pre class="programlisting">In [4]:
%matplotlib inline
In [11]:</pre></div><p>We convert the dataframe <code class="literal">txt_terms</code> column into a list of words. We make sure it is all converted into the <code class="literal">str</code> type to avoid any bad surprises and check the list's first four records:</p><div class="informalexample"><pre class="programlisting">len(tspark_df['txt_terms'].tolist())
Out[11]:
2024
In [22]:
tspark_ls_str = [str(t) for t in tspark_df['txt_terms'].tolist()]
In [14]:
len(tspark_ls_str)
Out[14]:
2024
In [15]:
tspark_ls_str[:4]
Out[15]:
['r leads rapidminer python catches up big data tools grow spark ignites kdn',
 'be one of the first to sign up for ibm analytics for apachespark today sparkinsight',
 'nice article on apachespark hadoop and datascience',
 'spark 101 running spark and mapreduce together in production hadoopsummit2015 apachespark altiscale']</pre></div><p>We <a id="id390" class="indexterm"></a>first call the Matplotlib and the wordcloud libraries:</p><div class="informalexample"><pre class="programlisting">import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS</pre></div><p>From the input list of terms, we create a unified string of terms separated by a whitespace as the input to the wordcloud program. The wordcloud program removes stopwords:</p><div class="informalexample"><pre class="programlisting"># join tweets to a single string
words = ' '.join(tspark_ls_str)

# create wordcloud 
wordcloud = WordCloud(
                      # remove stopwords
                      stopwords=STOPWORDS,
                      background_color='black',
                      width=1800,
                      height=1400
                     ).generate(words)

# render wordcloud image
plt.imshow(wordcloud)
plt.axis('off')

# save wordcloud image on disk
plt.savefig('./spark_tweets_wordcloud_1.png', dpi=300)

# display image in Jupyter notebook
plt.show()</pre></div><p>Here, we can visualize the wordclouds for Apache Spark and Python. Clearly, in the case of Spark, <span class="emphasis"><em>Hadoop</em></span>, <span class="emphasis"><em>big data</em></span>, and <span class="emphasis"><em>analytics</em></span> are the memes, while Python recalls the root of its name Monty Python with a strong focus on <span class="emphasis"><em>developer</em></span>, <span class="emphasis"><em>apache spark</em></span>, and programming with some hints to java and ruby.</p><div class="mediaobject"><img src="graphics/B03968_06_02.jpg" /></div><p>We can <a id="id391" class="indexterm"></a>also get a glimpse in the following wordclouds of the words preoccupying the North American 2016 presidential election candidates: Hilary Clinton and Donald Trump. Seemingly Hilary Clinton is overshadowed by the presence of her opponents Donald Trump and Bernie Sanders, while Trump is heavily centered only on himself:</p><div class="mediaobject"><img src="graphics/B03968_06_03.jpg" /></div><p>Interestingly, in the case of Justin Bieber and Lady Gaga, the word <span class="emphasis"><em>love</em></span> appears. In the case of Bieber, <span class="emphasis"><em>follow</em></span> and <span class="emphasis"><em>belieber</em></span> are key words, while <span class="emphasis"><em>diet</em></span>, <span class="emphasis"><em>weight loss</em></span>, and <span class="emphasis"><em>fashion</em></span> are the preoccupations for the Lady Gaga crowd.</p><div class="mediaobject"><img src="graphics/B03968_06_04.jpg" /></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec44"></a>Geo-locating tweets and mapping meetups</h2></div></div><hr /></div><p>Now, we <a id="id392" class="indexterm"></a>will dive into the creation of interactive maps with Bokeh. First, we <a id="id393" class="indexterm"></a>create a world map where we geo-locate sample tweets and, on moving our mouse over these locations, we can see the users and their respective tweets in a hover box.</p><p>The second map is focused on mapping upcoming meetups in London. It could be an interactive map that would act as a reminder of date, time, and location for upcoming meetups in a specific city.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec58"></a>Geo-locating tweets</h3></div></div></div><p>The objective is to create a world map scatter plot of the locations of important tweets on the map, and the tweets and authors are revealed on hovering over these points. We will go through three steps to build this interactive visualization:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Create <a id="id394" class="indexterm"></a>the background world map by first loading a dictionary of all the world country boundaries defined by their respective longitude and latitudes.</p></li><li><p>Load the important tweets we wish to geo-locate with their respective coordinates and authors.</p></li><li><p>Finally, scatter plot on the world map the tweets coordinates and activate the hover tool to visualize interactively the tweets and author on the highlighted dots on the map.</p></li></ol></div><p>In step<a id="id395" class="indexterm"></a> one, we create a Python list called data that will contain all the world countries boundaries with their respective latitude and longitude:</p><div class="informalexample"><pre class="programlisting">In [4]:
#
# This module exposes geometry data for World Country Boundaries.
#
import csv
import codecs
import gzip
import xml.etree.cElementTree as et
import os
from os.path import dirname, join

nan = float('NaN')
__file__ = os.getcwd()

data = {}
with gzip.open(join(dirname(__file__), 'AN_Spark/data/World_Country_Boundaries.csv.gz')) as f:
    decoded = codecs.iterdecode(f, "utf-8")
    next(decoded)
    reader = csv.reader(decoded, delimiter=',', quotechar='"')
    for row in reader:
        geometry, code, name = row
        xml = et.fromstring(geometry)
        lats = []
        lons = []
        for i, poly in enumerate(xml.findall('.//outerBoundaryIs/LinearRing/coordinates')):
            if i &gt; 0:
                lats.append(nan)
                lons.append(nan)
            coords = (c.split(',')[:2] for c in poly.text.split())
            lat, lon = list(zip(*[(float(lat), float(lon)) for lon, lat in
                coords]))
            lats.extend(lat)
            lons.extend(lon)
        data[code] = {
            'name'   : name,
            'lats'   : lats,
            'lons'   : lons,
        }
In [5]:
len(data)
Out[5]:
235</pre></div><p>In step two, we load a sample set of important tweets that we wish to visualize with their respective<a id="id396" class="indexterm"></a> geo-location information:</p><div class="informalexample"><pre class="programlisting">In [69]:
# data
#
#
In [8]:
import pandas as pd
csv_in = '/home/an/spark/spark-1.5.0-bin-hadoop2.6/examples/AN_Spark/data/spark_tweets_20.csv'
t20_df = pd.read_csv(csv_in, index_col=None, header=0, sep=',', encoding='utf-8')
In [9]:
t20_df.head(3)
Out[9]:
    id  created_at  user_id     user_name   tweet_text  htag    urls    ptxt    tgrp    date    user_handles    txt_terms   search_grp  lat     lon
0   638818911773856000  Tue Sep 01 21:01:11 +0000 2015  2511247075  Noor Din    RT @kdnuggets: R leads RapidMiner, Python catc...   [#KDN]  [://t.co/3bsaTT7eUs]    r leads rapidminer python catches up big data ...   [spark, python]     2015-09-01 21:01:11     [@kdnuggets]    r leads rapidminer python catches up big data ...   [spark, python]     37.279518   -121.867905
1   622142176768737000  Fri Jul 17 20:33:48 +0000 2015  24537879    IBM Cloudant    Be one of the first to sign-up for IBM Analyti...   [#ApacheSpark, #SparkInsight]   [://t.co/C5TZpetVA6, ://t.co/R1L29DePaQ]    be one of the first to sign up for ibm analyti...   [spark]     2015-07-17 20:33:48     []  be one of the first to sign up for ibm analyti...   [spark]     37.774930   -122.419420
2   622140453069169000  Fri Jul 17 20:26:57 +0000 2015  515145898   Arno Candel     Nice article on #apachespark, #hadoop and #dat...   [#apachespark, #hadoop, #datascience]   [://t.co/IyF44pV0f3]    nice article on apachespark hadoop and datasci...   [spark]     2015-07-17 20:26:57     [@h2oai]    nice article on apachespark hadoop and datasci...   [spark]     51.500130   -0.126305
In [98]:
len(t20_df.user_id.unique())
Out[98]:
19
In [17]:
t20_geo = t20_df[['date', 'lat', 'lon', 'user_name', 'tweet_text']]
In [24]:
# 
t20_geo.rename(columns={'user_name':'user', 'tweet_text':'text' }, inplace=True)
In [25]:
t20_geo.head(4)
Out[25]:
    date    lat     lon     user    text
0   2015-09-01 21:01:11     37.279518   -121.867905     Noor Din    RT @kdnuggets: R leads RapidMiner, Python catc...
1   2015-07-17 20:33:48     37.774930   -122.419420     IBM Cloudant    Be one of the first to sign-up for IBM Analyti...
2   2015-07-17 20:26:57     51.500130   -0.126305   Arno Candel     Nice article on #apachespark, #hadoop and #dat...
3   2015-07-17 19:35:31     51.500130   -0.126305   Ira Michael Blonder     Spark 101: Running Spark and #MapReduce togeth...
In [22]:
df = t20_geo
#</pre></div><p>In step <a id="id397" class="indexterm"></a>three, we first imported all the necessary Bokeh libraries. We will instantiate the output in the Jupyter Notebook. We get the world countries boundary information loaded. We get the geo-located tweet data. We instantiate the Bokeh interactive tools such as wheel and box zoom as well as the hover tool.</p><div class="informalexample"><pre class="programlisting">In [29]:
#
# Bokeh Visualization of tweets on world map
#
from bokeh.plotting import *
from bokeh.models import HoverTool, ColumnDataSource
from collections import OrderedDict

# Output in Jupiter Notebook
output_notebook()

# Get the world map
world_countries = data.copy()

# Get the tweet data
tweets_source = ColumnDataSource(df)

# Create world map 
countries_source = ColumnDataSource(data= dict(
    countries_xs=[world_countries[code]['lons'] for code in world_countries],
    countries_ys=[world_countries[code]['lats'] for code in world_countries],
    country = [world_countries[code]['name'] for code in world_countries],
))

# Instantiate the bokeh interactive tools 
TOOLS="pan,wheel_zoom,box_zoom,reset,resize,hover,save"</pre></div><p>We are <a id="id398" class="indexterm"></a>now ready to layer the various elements gathered into an object figure called <span class="strong"><strong>p</strong></span>. Define the title, width, and height of <span class="strong"><strong>p</strong></span>. Attach the tools. Create the world map background by patches with a light background color and borders. Scatter plot the tweets according to their respective geo-coordinates. Then, activate the hover tool with the users and their respective tweet. Finally, render the picture on the browser. The code is as follows:</p><div class="informalexample"><pre class="programlisting"># Instantiante the figure object
p = figure(
    title="%s tweets " %(str(len(df.index))),
    title_text_font_size="20pt",
    plot_width=1000,
    plot_height=600,
    tools=TOOLS)

# Create world patches background
p.patches(xs="countries_xs", ys="countries_ys", source = countries_source, fill_color="#F1EEF6", fill_alpha=0.3,
        line_color="#999999", line_width=0.5)

# Scatter plots by longitude and latitude
p.scatter(x="lon", y="lat", source=tweets_source, fill_color="#FF0000", line_color="#FF0000")
# 

# Activate hover tool with user and corresponding tweet information
hover = p.select(dict(type=HoverTool))
hover.point_policy = "follow_mouse"
hover.tooltips = OrderedDict([
    ("user", "@user"),
   ("tweet", "@text"),
])

# Render the figure on the browser
show(p)
BokehJS successfully loaded.
    
inspect
    
#
#</pre></div><p>The following <a id="id399" class="indexterm"></a>code gives an overview of the world map with the red dots representing the locations of the tweets' origins:</p><div class="mediaobject"><img src="graphics/B03968_06_05.jpg" /></div><p>We can hover on a specific dot to reveal the tweets in that location:</p><div class="mediaobject"><img src="graphics/B03968_06_06.jpg" /></div><p>We <a id="id400" class="indexterm"></a>can zoom into a specific location:</p><div class="mediaobject"><img src="graphics/B03968_06_07.jpg" /></div><p>Finally, we <a id="id401" class="indexterm"></a>can reveal the tweets in the given zoomed-in location:</p><div class="mediaobject"><img src="graphics/B03968_06_08.jpg" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec59"></a>Displaying upcoming meetups on Google Maps</h3></div></div></div><p>Now, our<a id="id402" class="indexterm"></a> objective is to focus on <a id="id403" class="indexterm"></a>upcoming meetups in London. We are mapping three <a id="id404" class="indexterm"></a>meetups <span class="strong"><strong>Data Science London</strong></span>, <span class="strong"><strong>Apache Spark</strong></span>, and <span class="strong"><strong>Machine Learning</strong></span>. We <a id="id405" class="indexterm"></a>embed a Google Map within a Bokeh visualization and geo-locate the three meetups according to their coordinates and get information such as the name of the upcoming event for each meetup with a hover tool.</p><p>First, import all the necessary Bokeh libraries:</p><div class="informalexample"><pre class="programlisting">In [ ]:
#
# Bokeh Google Map Visualization of London with hover on specific points
#
#
from __future__ import print_function

from bokeh.browserlib import view
from bokeh.document import Document
from bokeh.embed import file_html
from bokeh.models.glyphs import Circle
from bokeh.models import (
    GMapPlot, Range1d, ColumnDataSource,
    PanTool, WheelZoomTool, BoxSelectTool,
    HoverTool, ResetTool,
    BoxSelectionOverlay, GMapOptions)
from bokeh.resources import INLINE

x_range = Range1d()
y_range = Range1d()</pre></div><p>We <a id="id406" class="indexterm"></a>will instantiate the Google Map that will act as the substrate upon which our Bokeh visualization will be layered:</p><div class="informalexample"><pre class="programlisting"># JSON style string taken from: https://snazzymaps.com/style/1/pale-dawn
map_options = GMapOptions(lat=51.50013, lng=-0.126305, map_type="roadmap", zoom=13, styles="""
[{"featureType":"administrative","elementType":"all","stylers":[{"visibility":"on"},{"lightness":33}]},
 {"featureType":"landscape","elementType":"all","stylers":[{"color":"#f2e5d4"}]},
 {"featureType":"poi.park","elementType":"geometry","stylers":[{"color":"#c5dac6"}]},
 {"featureType":"poi.park","elementType":"labels","stylers":[{"visibility":"on"},{"lightness":20}]},
 {"featureType":"road","elementType":"all","stylers":[{"lightness":20}]},
 {"featureType":"road.highway","elementType":"geometry","stylers":[{"color":"#c5c6c6"}]},
 {"featureType":"road.arterial","elementType":"geometry","stylers":[{"color":"#e4d7c6"}]},
 {"featureType":"road.local","elementType":"geometry","stylers":[{"color":"#fbfaf7"}]},
 {"featureType":"water","elementType":"all","stylers":[{"visibility":"on"},{"color":"#acbcc9"}]}]
""")</pre></div><p>Instantiate the Bokeh object plot from the class <code class="literal">GMapPlot</code> with the dimensions and map options from the previous step:</p><div class="informalexample"><pre class="programlisting"># Instantiate Google Map Plot
plot = GMapPlot(
    x_range=x_range, y_range=y_range,
    map_options=map_options,
    title="London Meetups"
)</pre></div><p>Bring in<a id="id407" class="indexterm"></a> the information from our three meetups we wish to plot and get the information by hovering above the respective coordinates:</p><div class="informalexample"><pre class="programlisting">source = ColumnDataSource(
    data=dict(
        lat=[51.49013, 51.50013, 51.51013],
        lon=[-0.130305, -0.126305, -0.120305],
        fill=['orange', 'blue', 'green'],
        name=['LondonDataScience', 'Spark', 'MachineLearning'],
        text=['Graph Data &amp; Algorithms','Spark Internals','Deep Learning on Spark']
    )
)</pre></div><p>Define the dots to be drawn on the Google Map:</p><div class="informalexample"><pre class="programlisting">circle = Circle(x="lon", y="lat", size=15, fill_color="fill", line_color=None)
plot.add_glyph(source, circle)</pre></div><p>Define the stings for the Bokeh tools to be used in this visualization:</p><div class="informalexample"><pre class="programlisting"># TOOLS="pan,wheel_zoom,box_zoom,reset,hover,save"
pan = PanTool()
wheel_zoom = WheelZoomTool()
box_select = BoxSelectTool()
reset = ResetTool()
hover = HoverTool()
# save = SaveTool()

plot.add_tools(pan, wheel_zoom, box_select, reset, hover)
overlay = BoxSelectionOverlay(tool=box_select)
plot.add_layout(overlay)</pre></div><p>Activate the <code class="literal">hover</code> tool with the information that will be carried:</p><div class="informalexample"><pre class="programlisting">hover = plot.select(dict(type=HoverTool))
hover.point_policy = "follow_mouse"
hover.tooltips = OrderedDict([
    ("Name", "@name"),
    ("Text", "@text"),
    ("(Long, Lat)", "(@lon, @lat)"),
])

show(plot)</pre></div><p>Render<a id="id408" class="indexterm"></a> the plot that gives a pretty good view of London:</p><div class="mediaobject"><img src="graphics/B03968_06_09.jpg" /></div><p>Once we<a id="id409" class="indexterm"></a> hover on a highlighted dot, we can get the information of the given meetup:</p><div class="mediaobject"><img src="graphics/B03968_06_10.jpg" /></div><p>Full smooth zooming capability is preserved, as the following screenshot shows:</p><div class="mediaobject"><img src="graphics/B03968_06_11.jpg" /></div></div></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec45"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we focused on few visualization techniques. We saw how to build wordclouds and their intuitive power to reveal, at a glance, lots of the key words, moods, and memes carried through thousands of tweets.</p><p>We then discussed interactive mapping visualizations using Bokeh. We built a world map from the ground up and created a scatter plot of critical tweets. Once the map was rendered on the browser, we could interactively hover from dot to dot and reveal the tweets originating from different parts of the world.</p><p>Our final visualization was focused on mapping upcoming meetups in London on Spark, data science, and machine learning and their respective topics, making a beautiful interactive visualization with an actual Google Map.</p></div></div></div></div>
﻿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">

    <div id="backindex">
      <h1 class="title">Index</h1>
      <h2>A</h2>
      <ul>
        <li>Amazon Web Services (AWS)<ul><li>apps, deploying with / <a href="#ch01lvl1sec14" title="Deploying apps in Amazon Web Services" class="link">Deploying apps in Amazon Web Services</a></li><li>about / <a href="#ch01lvl1sec14" title="Deploying apps in Amazon Web Services" class="link">Deploying apps in Amazon Web Services</a></li></ul></li>
        <li>Anaconda<ul><li>defining / <a href="#ch01lvl1sec10" title="Understanding Anaconda" class="link">Understanding Anaconda</a></li></ul></li>
        <li>Anaconda Installer<ul><li>URL / <a href="#ch01lvl1sec11" title="Installing Anaconda with Python 2.7" class="link">Installing Anaconda with Python 2.7</a></li></ul></li>
        <li>Anaconda stack<ul><li>Anaconda / <a href="#ch01lvl1sec10" title="Understanding Anaconda" class="link">Understanding Anaconda</a></li><li>Conda / <a href="#ch01lvl1sec10" title="Understanding Anaconda" class="link">Understanding Anaconda</a></li><li>Numba / <a href="#ch01lvl1sec10" title="Understanding Anaconda" class="link">Understanding Anaconda</a></li><li>Blaze / <a href="#ch01lvl1sec10" title="Understanding Anaconda" class="link">Understanding Anaconda</a></li><li>Bokeh / <a href="#ch01lvl1sec10" title="Understanding Anaconda" class="link">Understanding Anaconda</a></li><li>Wakari / <a href="#ch01lvl1sec10" title="Understanding Anaconda" class="link">Understanding Anaconda</a></li></ul></li>
        <li>analytics layer / <a href="#ch01lvl1sec08" title="Analytics layer" class="link">Analytics layer</a></li>
        <li>Apache Kafka<ul><li>about / <a href="#ch05lvl1sec38" title="Setting up Kafka" class="link">Setting up Kafka</a></li><li>properties / <a href="#ch05lvl1sec38" title="Setting up Kafka" class="link">Setting up Kafka</a></li></ul></li>
        <li>Apache Spark<ul><li>about / <a href="#ch06lvl1sec44" title="Displaying upcoming meetups on Google Maps" class="link">Displaying upcoming meetups on Google Maps</a></li></ul></li>
        <li>APIs (Application Programming Interface)<ul><li>about / <a href="#ch02lvl1sec17" title="Connecting to social networks" class="link">Connecting to social networks</a></li></ul></li>
        <li>app<ul><li>previewing / <a href="#ch02lvl1sec20" title="Previewing our app" class="link">Previewing our app</a></li></ul></li>
        <li>apps<ul><li>deploying, with Amazon Web Services (AWS) / <a href="#ch01lvl1sec14" title="Deploying apps in Amazon Web Services" class="link">Deploying apps in Amazon Web Services</a></li></ul></li>
        <li>architecture, data-intensive applications<ul><li>about / <a href="#ch01lvl1sec08" title="Understanding the architecture of data-intensive applications" class="link">Understanding the architecture of data-intensive applications</a></li><li>infrastructure layer / <a href="#ch01lvl1sec08" title="Infrastructure layer" class="link">Infrastructure layer</a></li><li>persistence layer / <a href="#ch01lvl1sec08" title="Persistence layer" class="link">Persistence layer</a></li><li>integration layer / <a href="#ch01lvl1sec08" title="Integration layer" class="link">Integration layer</a></li><li>analytics layer / <a href="#ch01lvl1sec08" title="Analytics layer" class="link">Analytics layer</a></li><li>engagement layer / <a href="#ch01lvl1sec08" title="Engagement layer" class="link">Engagement layer</a></li></ul></li>
        <li>Asynchronous JavaScript (AJAX)<ul><li>about / <a href="#ch05lvl1sec36" title="Processing live data with TCP sockets" class="link">Processing live data with TCP sockets</a></li></ul></li>
        <li>AWS console<ul><li>URL / <a href="#ch01lvl1sec14" title="Deploying apps in Amazon Web Services" class="link">Deploying apps in Amazon Web Services</a></li></ul></li>
      </ul>
      <h2>B</h2>
      <ul>
        <li>Big Data, with Apache Spark<ul><li>references / <a href="#ch01lvl1sec13" title="Virtualizing the environment with Vagrant" class="link">Virtualizing the environment with Vagrant</a></li></ul></li>
        <li>Blaze<ul><li>used, for exploring data / <a href="#ch03lvl1sec25" title="Exploring data using Blaze" class="link">Exploring data using Blaze</a></li></ul></li>
        <li>BSON (Binary JSON)<ul><li>about / <a href="#ch03lvl1sec24" title="Setting up MongoDB" class="link">Setting up MongoDB</a></li></ul></li>
      </ul>
      <h2>C</h2>
      <ul>
        <li>Catalyst<ul><li>about / <a href="#ch03lvl1sec26" title="Exploring data using Spark SQL" class="link">Exploring data using Spark SQL</a></li></ul></li>
        <li>Chef<ul><li>about / <a href="#ch01lvl1sec08" title="Infrastructure layer" class="link">Infrastructure layer</a></li></ul></li>
        <li>Clustering<ul><li>K-Means / <a href="#ch04lvl1sec29" title="Supervised and unsupervised learning" class="link">Supervised and unsupervised learning</a></li><li>Gaussian Mixture / <a href="#ch04lvl1sec29" title="Supervised and unsupervised learning" class="link">Supervised and unsupervised learning</a></li><li>Power Iteration Clustering (PIC) / <a href="#ch04lvl1sec29" title="Supervised and unsupervised learning" class="link">Supervised and unsupervised learning</a></li><li>Latent Dirichlet Allocation (LDA) / <a href="#ch04lvl1sec29" title="Supervised and unsupervised learning" class="link">Supervised and unsupervised learning</a></li></ul></li>
        <li>Cluster manager<ul><li>about / <a href="#ch01lvl1sec09" title="The Resilient Distributed Dataset" class="link">The Resilient Distributed Dataset</a></li></ul></li>
        <li>comma-separated values (CSV)<ul><li>about / <a href="#ch03lvl1sec24" title="Harvesting and storing data" class="link">Harvesting and storing data</a></li></ul></li>
        <li>Continuum<ul><li>URL / <a href="#ch01lvl1sec10" title="Understanding Anaconda" class="link">Understanding Anaconda</a></li></ul></li>
        <li>Couchbase<ul><li>about / <a href="#ch01lvl1sec08" title="Persistence layer" class="link">Persistence layer</a></li></ul></li>
      </ul>
      <h2>D</h2>
      <ul>
        <li>D3.js<ul><li>about / <a href="#ch06lvl1sec41" title="Revisiting the data-intensive apps architecture" class="link">Revisiting the data-intensive apps architecture</a></li><li>URL / <a href="#ch06lvl1sec41" title="Revisiting the data-intensive apps architecture" class="link">Revisiting the data-intensive apps architecture</a></li></ul></li>
        <li>DAG (Directed Acyclic Graph)<ul><li>about / <a href="#ch01lvl1sec09" title="The Resilient Distributed Dataset" class="link">The Resilient Distributed Dataset</a>, <a href="#ch03lvl1sec23" title="Serializing and deserializing data" class="link">Serializing and deserializing data</a></li></ul></li>
        <li>data<ul><li>serializing / <a href="#ch03lvl1sec23" title="Serializing and deserializing data" class="link">Serializing and deserializing data</a></li><li>deserializing / <a href="#ch03lvl1sec23" title="Serializing and deserializing data" class="link">Serializing and deserializing data</a></li><li>harvesting / <a href="#ch03lvl1sec24" title="Harvesting and storing data" class="link">Harvesting and storing data</a></li><li>storing / <a href="#ch03lvl1sec24" title="Harvesting and storing data" class="link">Harvesting and storing data</a></li><li>persisting, in CSV / <a href="#ch03lvl1sec24" title="Persisting data in CSV" class="link">Persisting data in CSV</a></li><li>persisting, in JSON / <a href="#ch03lvl1sec24" title="Persisting data in JSON" class="link">Persisting data in JSON</a></li><li>MongoDB, setting up / <a href="#ch03lvl1sec24" title="Setting up MongoDB" class="link">Setting up MongoDB</a></li><li>, harvesting from Twitter / <a href="#ch03lvl1sec24" title="Harvesting data from Twitter" class="link">Harvesting data from Twitter</a></li><li>exploring, Blaze used / <a href="#ch03lvl1sec25" title="Exploring data using Blaze" class="link">Exploring data using Blaze</a></li><li>transferring, Odo used / <a href="#ch03lvl1sec25" title="Transferring data using Odo" class="link">Transferring data using Odo</a></li><li>exploring, Spark SQL used / <a href="#ch03lvl1sec26" title="Exploring data using Spark SQL" class="link">Exploring data using Spark SQL</a></li><li>pre-processing, for visualization / <a href="#ch06lvl1sec42" title="Preprocessing the data for visualization" class="link">Preprocessing the data for visualization</a></li></ul></li>
        <li>data-intensive apps<ul><li>architecting / <a href="#ch02lvl1sec16" title="Architecting data-intensive apps" class="link">Architecting data-intensive apps</a></li><li>latency / <a href="#ch02lvl1sec16" title="Architecting data-intensive apps" class="link">Architecting data-intensive apps</a></li><li>scalability / <a href="#ch02lvl1sec16" title="Architecting data-intensive apps" class="link">Architecting data-intensive apps</a></li><li>fault tolerance / <a href="#ch02lvl1sec16" title="Architecting data-intensive apps" class="link">Architecting data-intensive apps</a></li><li>flexibility / <a href="#ch02lvl1sec16" title="Architecting data-intensive apps" class="link">Architecting data-intensive apps</a></li><li>data at rest, processing / <a href="#ch02lvl1sec16" title="Processing data at rest" class="link">Processing data at rest</a></li><li>data in motion, processing / <a href="#ch02lvl1sec16" title="Processing data in motion" class="link">Processing data in motion</a></li><li>data, exploring / <a href="#ch02lvl1sec16" title="Exploring data interactively" class="link">Exploring data interactively</a></li></ul></li>
        <li>data-intensive apps architecture<ul><li>about / <a href="#ch06lvl1sec41" title="Revisiting the data-intensive apps architecture" class="link">Revisiting the data-intensive apps architecture</a></li></ul></li>
        <li>data analysis<ul><li>defining / <a href="#ch02lvl1sec18" title="Analyzing the data" class="link">Analyzing the data</a></li><li>Tweets anatomy, discovering / <a href="#ch02lvl1sec18" title="Discovering the anatomy of tweets" class="link">Discovering the anatomy of tweets</a></li></ul></li>
        <li>Data Driven Documents (D3)<ul><li>about / <a href="#ch06lvl1sec41" title="Revisiting the data-intensive apps architecture" class="link">Revisiting the data-intensive apps architecture</a></li></ul></li>
        <li>data flows<ul><li>about / <a href="#ch04lvl1sec31" title="Machine learning workflows and data flows" class="link">Machine learning workflows and data flows</a></li></ul></li>
        <li>data intensive apps architecture<ul><li>defining / <a href="#ch03lvl1sec22" title="Revisiting the data-intensive app architecture" class="link">Revisiting the data-intensive app architecture</a></li></ul></li>
        <li>data lifecycle<ul><li>Connect / <a href="#ch01lvl1sec08" title="Integration layer" class="link">Integration layer</a></li><li>Correct / <a href="#ch01lvl1sec08" title="Integration layer" class="link">Integration layer</a></li><li>Collect / <a href="#ch01lvl1sec08" title="Integration layer" class="link">Integration layer</a></li><li>Compose / <a href="#ch01lvl1sec08" title="Integration layer" class="link">Integration layer</a></li><li>Consume / <a href="#ch01lvl1sec08" title="Integration layer" class="link">Integration layer</a></li><li>Control / <a href="#ch01lvl1sec08" title="Integration layer" class="link">Integration layer</a></li></ul></li>
        <li>Data Science London<ul><li>about / <a href="#ch06lvl1sec44" title="Displaying upcoming meetups on Google Maps" class="link">Displaying upcoming meetups on Google Maps</a></li></ul></li>
        <li>data types, Spark MLlib<ul><li>local vector / <a href="#ch04lvl1sec30" title="Spark MLlib data types" class="link">Spark MLlib data types</a></li><li>labeled point / <a href="#ch04lvl1sec30" title="Spark MLlib data types" class="link">Spark MLlib data types</a></li><li>local matrix / <a href="#ch04lvl1sec30" title="Spark MLlib data types" class="link">Spark MLlib data types</a></li><li>distributed matrix / <a href="#ch04lvl1sec30" title="Spark MLlib data types" class="link">Spark MLlib data types</a></li></ul></li>
        <li>Decision Trees<ul><li>about / <a href="#ch04lvl1sec29" title="Supervised and unsupervised learning" class="link">Supervised and unsupervised learning</a></li></ul></li>
        <li>Dimensionality Reduction<ul><li>Singular Value Decomposition (SVD) / <a href="#ch04lvl1sec29" title="Supervised and unsupervised learning" class="link">Supervised and unsupervised learning</a></li><li>Principal Component Analysis (PCA) / <a href="#ch04lvl1sec29" title="Supervised and unsupervised learning" class="link">Supervised and unsupervised learning</a></li></ul></li>
        <li>Docker<ul><li>about / <a href="#ch01lvl1sec08" title="Infrastructure layer" class="link">Infrastructure layer</a></li><li>environment, virtualizing with / <a href="#ch01lvl1sec14" title="Virtualizing the environment with Docker" class="link">Virtualizing the environment with Docker</a></li><li>references / <a href="#ch01lvl1sec14" title="Virtualizing the environment with Docker" class="link">Virtualizing the environment with Docker</a></li></ul></li>
        <li>DStream (Discretized Stream)<ul><li>defining / <a href="#ch05lvl1sec35" title="Going under the hood of Spark Streaming" class="link">Going under the hood of Spark Streaming</a></li></ul></li>
      </ul>
      <h2>E</h2>
      <ul>
        <li>elements, Flume<ul><li>Event / <a href="#ch05lvl1sec38" title="Exploring flume" class="link">Exploring flume</a></li><li>Client / <a href="#ch05lvl1sec38" title="Exploring flume" class="link">Exploring flume</a></li><li>Source / <a href="#ch05lvl1sec38" title="Exploring flume" class="link">Exploring flume</a></li><li>Sink / <a href="#ch05lvl1sec38" title="Exploring flume" class="link">Exploring flume</a></li><li>Channel / <a href="#ch05lvl1sec38" title="Exploring flume" class="link">Exploring flume</a></li></ul></li>
        <li>engagement layer / <a href="#ch01lvl1sec08" title="Engagement layer" class="link">Engagement layer</a></li>
        <li>Ensembles of trees<ul><li>about / <a href="#ch04lvl1sec29" title="Supervised and unsupervised learning" class="link">Supervised and unsupervised learning</a></li></ul></li>
        <li>environment<ul><li>virtualizing, with Vagrant / <a href="#ch01lvl1sec13" title="Virtualizing the environment with Vagrant" class="link">Virtualizing the environment with Vagrant</a></li><li>virtualizing, with Docker / <a href="#ch01lvl1sec14" title="Virtualizing the environment with Docker" class="link">Virtualizing the environment with Docker</a></li></ul></li>
      </ul>
      <h2>F</h2>
      <ul>
        <li>First App<ul><li>building, with PySpark / <a href="#ch01lvl1sec12" title="Building our first app with PySpark" class="link">Building our first app with PySpark</a></li></ul></li>
        <li>Flume<ul><li>about / <a href="#ch05lvl1sec38" title="Exploring flume" class="link">Exploring flume</a></li><li>advantages / <a href="#ch05lvl1sec38" title="Exploring flume" class="link">Exploring flume</a></li><li>elements / <a href="#ch05lvl1sec38" title="Exploring flume" class="link">Exploring flume</a></li></ul></li>
      </ul>
      <h2>G</h2>
      <ul>
        <li>ggplot<ul><li>about / <a href="#ch06lvl1sec41" title="Revisiting the data-intensive apps architecture" class="link">Revisiting the data-intensive apps architecture</a></li><li>URL / <a href="#ch06lvl1sec41" title="Revisiting the data-intensive apps architecture" class="link">Revisiting the data-intensive apps architecture</a></li></ul></li>
        <li>GitHub<ul><li>URL / <a href="#ch02lvl1sec17" title="Getting GitHub data" class="link">Getting GitHub data</a></li><li>about / <a href="#ch02lvl1sec19" title="Exploring the GitHub world" class="link">Exploring the GitHub world</a></li><li>operating, with Meetup API / <a href="#ch02lvl1sec19" title="Understanding the community through Meetup" class="link">Understanding the community through Meetup</a></li></ul></li>
        <li>Google Maps<ul><li>upcoming meetups, displaying on / <a href="#ch06lvl1sec44" title="Displaying upcoming meetups on Google Maps" class="link">Displaying upcoming meetups on Google Maps</a></li></ul></li>
      </ul>
      <h2>H</h2>
      <ul>
        <li>Hadoop MongoDB connector<ul><li>URL / <a href="#ch03lvl1sec26" title="Querying MongoDB from Spark SQL" class="link">Querying MongoDB from Spark SQL</a></li></ul></li>
        <li>Hbase and Cassandra<ul><li>about / <a href="#ch01lvl1sec08" title="Persistence layer" class="link">Persistence layer</a></li></ul></li>
        <li>HDFS (Hadoop Distributed File System)<ul><li>about / <a href="#ch01lvl1sec09" title="Understanding Spark" class="link">Understanding Spark</a></li></ul></li>
      </ul>
      <h2>I</h2>
      <ul>
        <li>infrastructure layer / <a href="#ch01lvl1sec08" title="Infrastructure layer" class="link">Infrastructure layer</a></li>
        <li>Ingest Mode<ul><li>Batch Data Transport / <a href="#ch05lvl1sec38" title="Building a reliable and scalable streaming app" class="link">Building a reliable and scalable streaming app</a></li><li>Micro Batch / <a href="#ch05lvl1sec38" title="Building a reliable and scalable streaming app" class="link">Building a reliable and scalable streaming app</a></li><li>Pipelining / <a href="#ch05lvl1sec38" title="Building a reliable and scalable streaming app" class="link">Building a reliable and scalable streaming app</a></li><li>Message Queue / <a href="#ch05lvl1sec38" title="Building a reliable and scalable streaming app" class="link">Building a reliable and scalable streaming app</a></li></ul></li>
        <li>integration layer / <a href="#ch01lvl1sec08" title="Integration layer" class="link">Integration layer</a></li>
      </ul>
      <h2>J</h2>
      <ul>
        <li>Java 8<ul><li>installing / <a href="#ch01lvl1sec11" title="Installing Java 8" class="link">Installing Java 8</a></li></ul></li>
        <li>JRE (Java Runtime Environment)<ul><li>about / <a href="#ch01lvl1sec11" title="Installing Java 8" class="link">Installing Java 8</a></li></ul></li>
        <li>JSON (JavaScript Object Notation)<ul><li>about / <a href="#ch02lvl1sec17" title="Connecting to social networks" class="link">Connecting to social networks</a>, <a href="#ch03lvl1sec24" title="Harvesting and storing data" class="link">Harvesting and storing data</a></li></ul></li>
      </ul>
      <h2>K</h2>
      <ul>
        <li>Kafka<ul><li>setting up / <a href="#ch05lvl1sec38" title="Setting up Kafka" class="link">Setting up Kafka</a></li><li>installing / <a href="#ch05lvl1sec38" title="Installing and testing Kafka" class="link">Installing and testing Kafka</a></li><li>testing / <a href="#ch05lvl1sec38" title="Installing and testing Kafka" class="link">Installing and testing Kafka</a></li><li>URL / <a href="#ch05lvl1sec38" title="Installing and testing Kafka" class="link">Installing and testing Kafka</a></li><li>producers, developing / <a href="#ch05lvl1sec38" title="Developing producers" class="link">Developing producers</a></li><li>consumers, developing / <a href="#ch05lvl1sec38" title="Developing consumers" class="link">Developing consumers</a></li><li>Spark Streaming consumer, developing for / <a href="#ch05lvl1sec38" title="Developing a Spark Streaming consumer for Kafka" class="link">Developing a Spark Streaming consumer for Kafka</a></li></ul></li>
        <li>Kappa architecture<ul><li>defining / <a href="#ch05lvl1sec39" title="Closing remarks on the Lambda and Kappa architecture" class="link">Closing remarks on the Lambda and Kappa architecture</a>, <a href="#ch05lvl1sec39" title="Understanding Kappa architecture" class="link">Understanding Kappa architecture</a></li></ul></li>
      </ul>
      <h2>L</h2>
      <ul>
        <li>Lambda architecture<ul><li>defining / <a href="#ch05lvl1sec39" title="Closing remarks on the Lambda and Kappa architecture" class="link">Closing remarks on the Lambda and Kappa architecture</a>, <a href="#ch05lvl1sec39" title="Understanding Lambda architecture" class="link">Understanding Lambda architecture</a></li></ul></li>
        <li>Linear Regression Models<ul><li>about / <a href="#ch04lvl1sec29" title="Supervised and unsupervised learning" class="link">Supervised and unsupervised learning</a></li></ul></li>
      </ul>
      <h2>M</h2>
      <ul>
        <li>Machine Learning<ul><li>about / <a href="#ch06lvl1sec44" title="Displaying upcoming meetups on Google Maps" class="link">Displaying upcoming meetups on Google Maps</a></li></ul></li>
        <li>machine learning pipelines<ul><li>building / <a href="#ch04lvl1sec33" title="Building machine learning pipelines" class="link">Building machine learning pipelines</a></li></ul></li>
        <li>machine learning workflows<ul><li>about / <a href="#ch04lvl1sec31" title="Machine learning workflows and data flows" class="link">Machine learning workflows and data flows</a></li></ul></li>
        <li>Massive Open Online Courses (MOOCs)<ul><li>about / <a href="#ch01lvl1sec13" title="Virtualizing the environment with Vagrant" class="link">Virtualizing the environment with Vagrant</a></li></ul></li>
        <li>Matplotlib<ul><li>about / <a href="#ch06lvl1sec41" title="Revisiting the data-intensive apps architecture" class="link">Revisiting the data-intensive apps architecture</a></li><li>URL / <a href="#ch06lvl1sec41" title="Revisiting the data-intensive apps architecture" class="link">Revisiting the data-intensive apps architecture</a></li></ul></li>
        <li>Meetup API<ul><li>URL / <a href="#ch02lvl1sec17" title="Getting Meetup data" class="link">Getting Meetup data</a></li></ul></li>
        <li>meetups<ul><li>mapping / <a href="#ch06lvl1sec44" title="Geo-locating tweets and mapping meetups" class="link">Geo-locating tweets and mapping meetups</a></li></ul></li>
        <li>MLlib algorithms<ul><li>Collaborative filtering / <a href="#ch04lvl1sec29" title="Additional learning algorithms" class="link">Additional learning algorithms</a></li><li>feature extraction and transformation / <a href="#ch04lvl1sec29" title="Additional learning algorithms" class="link">Additional learning algorithms</a></li><li>optimization / <a href="#ch04lvl1sec29" title="Additional learning algorithms" class="link">Additional learning algorithms</a></li><li>Limited-memory BFGS (L-BFGS) / <a href="#ch04lvl1sec29" title="Additional learning algorithms" class="link">Additional learning algorithms</a></li></ul></li>
        <li>models<ul><li>defining, for processing streams of data / <a href="#ch05lvl1sec35" title="Laying the foundations of streaming architecture" class="link">Laying the foundations of streaming architecture</a></li></ul></li>
        <li>MongoDB<ul><li>about / <a href="#ch01lvl1sec08" title="Persistence layer" class="link">Persistence layer</a></li><li>setting up / <a href="#ch03lvl1sec24" title="Setting up MongoDB" class="link">Setting up MongoDB</a></li><li>server and client, installing / <a href="#ch03lvl1sec24" title="Installing the MongoDB server and client" class="link">Installing the MongoDB server and client</a></li><li>server, running / <a href="#ch03lvl1sec24" title="Running the MongoDB server" class="link">Running the MongoDB server</a></li><li>Mongo client, running / <a href="#ch03lvl1sec24" title="Running the Mongo client" class="link">Running the Mongo client</a></li><li>PyMongo driver, installing / <a href="#ch03lvl1sec24" title="Installing the PyMongo driver" class="link">Installing the PyMongo driver</a></li><li>Python client, creating for / <a href="#ch03lvl1sec24" title="Creating the Python client for MongoDB" class="link">Creating the Python client for MongoDB</a></li><li>references / <a href="#ch03lvl1sec26" title="Querying MongoDB from Spark SQL" class="link">Querying MongoDB from Spark SQL</a></li></ul></li>
        <li>MongoDB, from Spark SQL<ul><li>URL / <a href="#ch03lvl1sec26" title="Querying MongoDB from Spark SQL" class="link">Querying MongoDB from Spark SQL</a></li></ul></li>
        <li>Multi-Dimensional Scaling (MDS) algorithm<ul><li>about / <a href="#ch04lvl1sec32" title="Applying Scikit-Learn on the Twitter dataset" class="link">Applying Scikit-Learn on the Twitter dataset</a></li></ul></li>
        <li>Mumrah, on GitHub<ul><li>URL / <a href="#ch05lvl1sec38" title="Installing and testing Kafka" class="link">Installing and testing Kafka</a></li></ul></li>
        <li>MySQL<ul><li>about / <a href="#ch01lvl1sec08" title="Persistence layer" class="link">Persistence layer</a></li></ul></li>
      </ul>
      <h2>N</h2>
      <ul>
        <li>Naive Bayes<ul><li>about / <a href="#ch04lvl1sec29" title="Supervised and unsupervised learning" class="link">Supervised and unsupervised learning</a></li></ul></li>
        <li>Neo4j<ul><li>about / <a href="#ch01lvl1sec08" title="Persistence layer" class="link">Persistence layer</a></li></ul></li>
        <li>network_wordcount.py<ul><li>URL / <a href="#ch05lvl1sec36" title="Processing live data" class="link">Processing live data</a></li></ul></li>
      </ul>
      <h2>O</h2>
      <ul>
        <li>Odo<ul><li>about / <a href="#ch03lvl1sec25" title="Transferring data using Odo" class="link">Transferring data using Odo</a></li><li>used, for transferring data / <a href="#ch03lvl1sec25" title="Transferring data using Odo" class="link">Transferring data using Odo</a></li></ul></li>
        <li>operations, on RDDs<ul><li>transformations / <a href="#ch01lvl1sec09" title="The Resilient Distributed Dataset" class="link">The Resilient Distributed Dataset</a></li><li>action / <a href="#ch01lvl1sec09" title="The Resilient Distributed Dataset" class="link">The Resilient Distributed Dataset</a></li></ul></li>
      </ul>
      <h2>P</h2>
      <ul>
        <li>persistence layer / <a href="#ch01lvl1sec08" title="Persistence layer" class="link">Persistence layer</a></li>
        <li>PIL (Python Imaging Library)<ul><li>about / <a href="#ch06lvl1sec43" title="Setting up wordcloud" class="link">Setting up wordcloud</a></li></ul></li>
        <li>PostgreSQL<ul><li>about / <a href="#ch01lvl1sec08" title="Persistence layer" class="link">Persistence layer</a></li></ul></li>
        <li>Puppet<ul><li>about / <a href="#ch01lvl1sec08" title="Infrastructure layer" class="link">Infrastructure layer</a></li></ul></li>
        <li>PySpark<ul><li>First App, building with / <a href="#ch01lvl1sec12" title="Building our first app with PySpark" class="link">Building our first app with PySpark</a></li></ul></li>
      </ul>
      <h2>R</h2>
      <ul>
        <li>RDD (Resilient Distributed Dataset)<ul><li>about / <a href="#ch01lvl1sec09" title="The Resilient Distributed Dataset" class="link">The Resilient Distributed Dataset</a></li></ul></li>
        <li>Resilient Distributed Datasets (RDD)<ul><li>about / <a href="#ch05lvl1sec35" title="Spark Streaming inner working" class="link">Spark Streaming inner working</a></li></ul></li>
        <li>REST (Representation State Transfer)<ul><li>about / <a href="#ch02lvl1sec17" title="Connecting to social networks" class="link">Connecting to social networks</a></li></ul></li>
        <li>RPC (Remote Procedure Call)<ul><li>about / <a href="#ch05lvl1sec35" title="Laying the foundations of streaming architecture" class="link">Laying the foundations of streaming architecture</a></li></ul></li>
      </ul>
      <h2>S</h2>
      <ul>
        <li>SDK (Software Development Kit)<ul><li>about / <a href="#ch01lvl1sec11" title="Installing Java 8" class="link">Installing Java 8</a></li></ul></li>
        <li>Seaborn<ul><li>about / <a href="#ch06lvl1sec41" title="Revisiting the data-intensive apps architecture" class="link">Revisiting the data-intensive apps architecture</a></li><li>URL / <a href="#ch06lvl1sec41" title="Revisiting the data-intensive apps architecture" class="link">Revisiting the data-intensive apps architecture</a></li></ul></li>
        <li>social networks<ul><li>connecting to / <a href="#ch02lvl1sec17" title="Connecting to social networks" class="link">Connecting to social networks</a></li><li>Twitter data, obtaining / <a href="#ch02lvl1sec17" title="Getting Twitter data" class="link">Getting Twitter data</a></li><li>GitHub data, obtaining / <a href="#ch02lvl1sec17" title="Getting GitHub data" class="link">Getting GitHub data</a></li><li>Meetup data, obtaining / <a href="#ch02lvl1sec17" title="Getting Meetup data" class="link">Getting Meetup data</a></li></ul></li>
        <li>Spark<ul><li>defining / <a href="#ch01lvl1sec09" title="Understanding Spark" class="link">Understanding Spark</a></li><li>Batch / <a href="#ch01lvl1sec09" title="Understanding Spark" class="link">Understanding Spark</a></li><li>Streaming / <a href="#ch01lvl1sec09" title="Understanding Spark" class="link">Understanding Spark</a></li><li>Iterative / <a href="#ch01lvl1sec09" title="Understanding Spark" class="link">Understanding Spark</a></li><li>Interactive / <a href="#ch01lvl1sec09" title="Understanding Spark" class="link">Understanding Spark</a></li><li>libraries / <a href="#ch01lvl1sec09" title="Spark libraries" class="link">Spark libraries</a></li><li>URL / <a href="#ch01lvl1sec11" title="Installing Spark" class="link">Installing Spark</a></li><li>Clustering / <a href="#ch04lvl1sec29" title="Supervised and unsupervised learning" class="link">Supervised and unsupervised learning</a></li><li>Dimensionality Reduction / <a href="#ch04lvl1sec29" title="Supervised and unsupervised learning" class="link">Supervised and unsupervised learning</a></li><li>Regression and Classification / <a href="#ch04lvl1sec29" title="Supervised and unsupervised learning" class="link">Supervised and unsupervised learning</a></li><li>Isotonic Regression / <a href="#ch04lvl1sec29" title="Supervised and unsupervised learning" class="link">Supervised and unsupervised learning</a></li><li>MLlib algorithms / <a href="#ch04lvl1sec29" title="Additional learning algorithms" class="link">Additional learning algorithms</a></li></ul></li>
        <li>Spark, on EC2<ul><li>URL / <a href="#ch01lvl1sec14" title="Deploying apps in Amazon Web Services" class="link">Deploying apps in Amazon Web Services</a></li></ul></li>
        <li>SparkContext<ul><li>about / <a href="#ch05lvl1sec35" title="Spark Streaming inner working" class="link">Spark Streaming inner working</a></li></ul></li>
        <li>Spark dataframes<ul><li>defining / <a href="#ch03lvl1sec26" title="Understanding Spark dataframes" class="link">Understanding Spark dataframes</a></li></ul></li>
        <li>Spark libraries<ul><li>SparkSQL / <a href="#ch01lvl1sec09" title="Spark libraries" class="link">Spark libraries</a></li><li>SparkMLLIB / <a href="#ch01lvl1sec09" title="Spark libraries" class="link">Spark libraries</a></li><li>Spark Streaming / <a href="#ch01lvl1sec09" title="Spark libraries" class="link">Spark libraries</a></li><li>Spark GraphX / <a href="#ch01lvl1sec09" title="Spark libraries" class="link">Spark libraries</a></li><li>PySpark, defining / <a href="#ch01lvl1sec09" title="PySpark in action" class="link">PySpark in action</a></li><li>RDD (Resilient Distributed Dataset) / <a href="#ch01lvl1sec09" title="The Resilient Distributed Dataset" class="link">The Resilient Distributed Dataset</a></li></ul></li>
        <li>Spark MLlib<ul><li>contextualizing, in app architecture / <a href="#ch04lvl1sec28" title="Contextualizing Spark MLlib in the app architecture" class="link">Contextualizing Spark MLlib in the app architecture</a></li><li>data types / <a href="#ch04lvl1sec30" title="Spark MLlib data types" class="link">Spark MLlib data types</a></li></ul></li>
        <li>Spark MLlib algorithms<ul><li>classifying / <a href="#ch04lvl1sec29" title="Classifying Spark MLlib algorithms" class="link">Classifying Spark MLlib algorithms</a></li><li>supervised learning / <a href="#ch04lvl1sec29" title="Supervised and unsupervised learning" class="link">Supervised and unsupervised learning</a></li><li>unsupervised learning / <a href="#ch04lvl1sec29" title="Supervised and unsupervised learning" class="link">Supervised and unsupervised learning</a></li><li>additional learning algorithms / <a href="#ch04lvl1sec29" title="Additional learning algorithms" class="link">Additional learning algorithms</a></li></ul></li>
        <li>Spark Powered Environment<ul><li>setting up / <a href="#ch01lvl1sec11" title="Setting up the Spark powered environment" class="link">Setting up the Spark powered environment</a></li><li>Oracle VirtualBox, setting up with Ubuntu / <a href="#ch01lvl1sec11" title="Setting up an Oracle VirtualBox with Ubuntu" class="link">Setting up an Oracle VirtualBox with Ubuntu</a></li><li>Anaconda, installing with Python 2.7 / <a href="#ch01lvl1sec11" title="Installing Anaconda with Python 2.7" class="link">Installing Anaconda with Python 2.7</a></li><li>Java 8, installing / <a href="#ch01lvl1sec11" title="Installing Java 8" class="link">Installing Java 8</a></li><li>Spark, installing / <a href="#ch01lvl1sec11" title="Installing Spark" class="link">Installing Spark</a></li><li>IPython Notebook, enabling / <a href="#ch01lvl1sec11" title="Enabling IPython Notebook" class="link">Enabling IPython Notebook</a></li></ul></li>
        <li>Spark SQL<ul><li>used, for exploring data / <a href="#ch03lvl1sec26" title="Exploring data using Spark SQL" class="link">Exploring data using Spark SQL</a></li><li>about / <a href="#ch03lvl1sec26" title="Exploring data using Spark SQL" class="link">Exploring data using Spark SQL</a></li><li>CSV files, loading with / <a href="#ch03lvl1sec26" title="Loading and processing CSV files with Spark SQL" class="link">Loading and processing CSV files with Spark SQL</a></li><li>CSV files, processing with / <a href="#ch03lvl1sec26" title="Loading and processing CSV files with Spark SQL" class="link">Loading and processing CSV files with Spark SQL</a></li><li>MongoDB, querying from / <a href="#ch03lvl1sec26" title="Querying MongoDB from Spark SQL" class="link">Querying MongoDB from Spark SQL</a></li></ul></li>
        <li>SparkSQL module<ul><li>about / <a href="#ch01lvl1sec08" title="Analytics layer" class="link">Analytics layer</a></li></ul></li>
        <li>Spark SQL query optimizer<ul><li>defining / <a href="#ch03lvl1sec26" title="Understanding the Spark SQL query optimizer" class="link">Understanding the Spark SQL query optimizer</a></li></ul></li>
        <li>Spark streaming<ul><li>defining / <a href="#ch05lvl1sec35" title="Spark Streaming inner working" class="link">Spark Streaming inner working</a>, <a href="#ch05lvl1sec35" title="Going under the hood of Spark Streaming" class="link">Going under the hood of Spark Streaming</a></li><li>building, in fault tolerance / <a href="#ch05lvl1sec35" title="Building in fault tolerance" class="link">Building in fault tolerance</a></li></ul></li>
        <li>Stochastic Gradient Descent<ul><li>about / <a href="#ch04lvl1sec29" title="Classifying Spark MLlib algorithms" class="link">Classifying Spark MLlib algorithms</a></li></ul></li>
        <li>streaming app<ul><li>building / <a href="#ch05lvl1sec38" title="Building a reliable and scalable streaming app" class="link">Building a reliable and scalable streaming app</a></li><li>Kafka, setting up / <a href="#ch05lvl1sec38" title="Setting up Kafka" class="link">Setting up Kafka</a></li><li>flume, exploring / <a href="#ch05lvl1sec38" title="Exploring flume" class="link">Exploring flume</a></li><li>data pipelines, developing with Flume / <a href="#ch05lvl1sec38" title="Developing data pipelines with Flume, Kafka, and Spark" class="link">Developing data pipelines with Flume, Kafka, and Spark</a></li><li>data pipelines, developing with Kafka / <a href="#ch05lvl1sec38" title="Developing data pipelines with Flume, Kafka, and Spark" class="link">Developing data pipelines with Flume, Kafka, and Spark</a></li><li>data pipelines, developing with Spark / <a href="#ch05lvl1sec38" title="Developing data pipelines with Flume, Kafka, and Spark" class="link">Developing data pipelines with Flume, Kafka, and Spark</a></li></ul></li>
        <li>streaming architecture<ul><li>about / <a href="#ch05lvl1sec35" title="Laying the foundations of streaming architecture" class="link">Laying the foundations of streaming architecture</a></li></ul></li>
        <li>StreamingContext<ul><li>about / <a href="#ch05lvl1sec35" title="Spark Streaming inner working" class="link">Spark Streaming inner working</a></li></ul></li>
        <li>supervised machine learning workflow<ul><li>about / <a href="#ch04lvl1sec31" title="Supervised machine learning workflows" class="link">Supervised machine learning workflows</a></li></ul></li>
      </ul>
      <h2>T</h2>
      <ul>
        <li>TCP Sockets<ul><li>live data, processing with / <a href="#ch05lvl1sec36" title="Processing live data with TCP sockets" class="link">Processing live data with TCP sockets</a>, <a href="#ch05lvl1sec36" title="Processing live data" class="link">Processing live data</a></li><li>setting up / <a href="#ch05lvl1sec36" title="Setting up TCP sockets" class="link">Setting up TCP sockets</a></li></ul></li>
        <li>TF-IDF (Term Frequency - Inverse Document Frequency)<ul><li>about / <a href="#ch04lvl1sec29" title="Classifying Spark MLlib algorithms" class="link">Classifying Spark MLlib algorithms</a></li></ul></li>
        <li>Trident<ul><li>about / <a href="#ch05lvl1sec35" title="Laying the foundations of streaming architecture" class="link">Laying the foundations of streaming architecture</a></li></ul></li>
        <li>tweets<ul><li>geo-locating / <a href="#ch06lvl1sec44" title="Geo-locating tweets and mapping meetups" class="link">Geo-locating tweets and mapping meetups</a>, <a href="#ch06lvl1sec44" title="Geo-locating tweets" class="link">Geo-locating tweets</a></li></ul></li>
        <li>Twitter<ul><li>URL / <a href="#ch02lvl1sec17" title="Getting Twitter data" class="link">Getting Twitter data</a></li></ul></li>
        <li>Twitter API, on dev console<ul><li>URL / <a href="#ch02lvl1sec17" title="Getting Twitter data" class="link">Getting Twitter data</a></li></ul></li>
        <li>Twitter data<ul><li>manipulating / <a href="#ch05lvl1sec37" title="Manipulating Twitter data in real time" class="link">Manipulating Twitter data in real time</a></li><li>tweets, processing from Twitter firehose / <a href="#ch05lvl1sec37" title="Processing Tweets in real time from the Twitter firehose" class="link">Processing Tweets in real time from the Twitter firehose</a></li></ul></li>
        <li>Twitter dataset<ul><li>clustering / <a href="#ch04lvl1sec32" title="Clustering the Twitter dataset" class="link">Clustering the Twitter dataset</a></li><li>SciKit-Learn, applying on / <a href="#ch04lvl1sec32" title="Applying Scikit-Learn on the Twitter dataset" class="link">Applying Scikit-Learn on the Twitter dataset</a></li><li>dataset, preprocessing / <a href="#ch04lvl1sec32" title="Preprocessing the dataset" class="link">Preprocessing the dataset</a></li><li>clustering algorithm, running / <a href="#ch04lvl1sec32" title="Running the clustering algorithm" class="link">Running the clustering algorithm</a></li><li>model and results, evaluating / <a href="#ch04lvl1sec32" title="Evaluating the model and the results" class="link">Evaluating the model and the results</a></li></ul></li>
      </ul>
      <h2>U</h2>
      <ul>
        <li>Ubuntu 14.04.1 LTS release<ul><li>URL / <a href="#ch01lvl1sec11" title="Setting up an Oracle VirtualBox with Ubuntu" class="link">Setting up an Oracle VirtualBox with Ubuntu</a></li></ul></li>
        <li>unified log<ul><li>properties / <a href="#ch05lvl1sec39" title="Understanding Kappa architecture" class="link">Understanding Kappa architecture</a></li></ul></li>
        <li>Unified Log<ul><li>properties / <a href="#ch05lvl1sec38" title="Building a reliable and scalable streaming app" class="link">Building a reliable and scalable streaming app</a></li></ul></li>
        <li>unsupervised machine learning workflow<ul><li>about / <a href="#ch04lvl1sec31" title="Unsupervised machine learning workflows" class="link">Unsupervised machine learning workflows</a></li></ul></li>
      </ul>
      <h2>V</h2>
      <ul>
        <li>Vagrant<ul><li>about / <a href="#ch01lvl1sec08" title="Infrastructure layer" class="link">Infrastructure layer</a></li><li>environment, virtualizing with / <a href="#ch01lvl1sec13" title="Virtualizing the environment with Vagrant" class="link">Virtualizing the environment with Vagrant</a></li><li>reference / <a href="#ch01lvl1sec13" title="Virtualizing the environment with Vagrant" class="link">Virtualizing the environment with Vagrant</a></li></ul></li>
        <li>VirtualBox VM<ul><li>URL / <a href="#ch01lvl1sec11" title="Setting up an Oracle VirtualBox with Ubuntu" class="link">Setting up an Oracle VirtualBox with Ubuntu</a></li></ul></li>
        <li>visualization<ul><li>data, pre-processing for / <a href="#ch06lvl1sec42" title="Preprocessing the data for visualization" class="link">Preprocessing the data for visualization</a></li></ul></li>
      </ul>
      <h2>W</h2>
      <ul>
        <li>wordclouds<ul><li>creating / <a href="#ch06lvl1sec43" title="Gauging words, moods, and memes at a glance" class="link">Gauging words, moods, and memes at a glance</a>, <a href="#ch06lvl1sec43" title="Creating wordclouds" class="link">Creating wordclouds</a></li><li>setting up / <a href="#ch06lvl1sec43" title="Setting up wordcloud" class="link">Setting up wordcloud</a></li><li>URL / <a href="#ch06lvl1sec43" title="Setting up wordcloud" class="link">Setting up wordcloud</a></li></ul></li>
      </ul>
    </div>
  </div></div></div>
</div></div></div></body></html>
