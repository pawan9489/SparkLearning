<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="">

    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0"> <!--320-->
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no, user-scalable=no">

    <link rel="icon" href="../../mapt/images/favicon.ico">

    <link rel="stylesheet" href="../../mapt/css/font-awesome.css">
    <link rel="stylesheet" href="../../mapt/css/google-fonts.css">
    <link rel="stylesheet" href="../../mapt/css/devicon.css">

    <link rel="stylesheet" href="../../mapt/css/bootstrap.css">
    <link rel="stylesheet" href="../../mapt/css/bootstrap-xl.css">
    <link rel="stylesheet" href="../../mapt/css/magnific-popup.css">
    <link rel="stylesheet" href="../../mapt/css/prism.css">
    <link rel="stylesheet" href="../../mapt/css/hljs-github.css">

    <link rel="stylesheet" href="../../mapt/css/mapt.css">
    <link rel="stylesheet" href="../../mapt/css/custom.css">

    <script src="../../mapt/js/jquery.js"></script>
    <script src="../../mapt/js/bootstrap.js"></script>
    <script src="../../mapt/js/jquery.magnific-popup.js"></script>
    <script src="../../mapt/js/highlight.min.js"></script>

    <script src="../../mapt/js/custom.js"></script>
    
    <title>Machine Learning with Spark</title>
</head>

<body class="home-body">
    <div id="wrapper">
        <div id="sidebar-wrapper">    
            <ul class="sidebar-nav">
                <div class="list-group" id="sidebar-nav" role="tablist">
                    <li>
                        <a href="../../index.html" class="sidenav-menu-holder back-btn" id="back-link">
                            <span class="sidenav-menu">Book List</span>
                            <span class="pull-left mr5"><i class="fa fa-chevron-left"></i></span>
                        </a>
                    </li>
                    
                    <li class="book-info copyright">
                        <span class="info text-nowrap"><span class="copyleft">&copy;</span><span><strong>RuTracker</strong>.org</span></span>
                    </li>          
                    <li class="book-info copyright">
                        <span class="info text-nowrap">Pub date: <strong>20 Feb 2015</strong></span>
                    </li>         
                    <li class="book-info">
                        <span class="info text-nowrap">Price: â‚¬<strong>23.99</strong></span>
                        <span class="info text-nowrap">ISBN: <strong>9781783288519</strong></span>
                    </li>     
            
                    <li>
                        <a href="graphics/cover.jpg" class="sidenav-menu-holder cover-img">
                            <img src="graphics/cover.jpg" class="cover-image">
                        </a>
                    </li>        
            
                    <div class="book_navigation">
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse1">
                                <div class="section-name">1: Getting Up and Running with Spark</div>
                            </a>
                        </li>
                        <div id="collapse1" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="1" class="sub-nav">
                                <a href="#ch01">
                                    <div class="section-name">Chapter 1: Getting Up and Running with Spark</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec08" class="sub-nav">
                                <a href="#ch01lvl1sec08">                    
                                    <div class="section-name">Installing and setting up Spark locally</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec09" class="sub-nav">
                                <a href="#ch01lvl1sec09">                    
                                    <div class="section-name">Spark clusters</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec10" class="sub-nav">
                                <a href="#ch01lvl1sec10">                    
                                    <div class="section-name">The Spark programming model</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec11" class="sub-nav">
                                <a href="#ch01lvl1sec11">                    
                                    <div class="section-name">The first step to a Spark program in Scala</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec12" class="sub-nav">
                                <a href="#ch01lvl1sec12">                    
                                    <div class="section-name">The first step to a Spark program in Java</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec13" class="sub-nav">
                                <a href="#ch01lvl1sec13">                    
                                    <div class="section-name">The first step to a Spark program in Python</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec14" class="sub-nav">
                                <a href="#ch01lvl1sec14">                    
                                    <div class="section-name">Getting Spark running on Amazon EC2</div>
                                </a>
                            </li>
                            <li data-chapter="1" data-section-id="ch01lvl1sec15" class="sub-nav">
                                <a href="#ch01lvl1sec15">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse2">
                                <div class="section-name">2: Designing a Machine Learning System</div>
                            </a>
                        </li>
                        <div id="collapse2" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="2" class="sub-nav">
                                <a href="#ch02">
                                    <div class="section-name">Chapter 2: Designing a Machine Learning System</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec16" class="sub-nav">
                                <a href="#ch02lvl1sec16">                    
                                    <div class="section-name">Introducing MovieStream</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec17" class="sub-nav">
                                <a href="#ch02lvl1sec17">                    
                                    <div class="section-name">Business use cases for a machine learning system</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec18" class="sub-nav">
                                <a href="#ch02lvl1sec18">                    
                                    <div class="section-name">Types of machine learning models</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec19" class="sub-nav">
                                <a href="#ch02lvl1sec19">                    
                                    <div class="section-name">The components of a data-driven machine learning system</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec20" class="sub-nav">
                                <a href="#ch02lvl1sec20">                    
                                    <div class="section-name">An architecture for a machine learning system</div>
                                </a>
                            </li>
                            <li data-chapter="2" data-section-id="ch02lvl1sec21" class="sub-nav">
                                <a href="#ch02lvl1sec21">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse3">
                                <div class="section-name">3: Obtaining, Processing, and Preparing Data with Spark</div>
                            </a>
                        </li>
                        <div id="collapse3" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="3" class="sub-nav">
                                <a href="#ch03">
                                    <div class="section-name">Chapter 3: Obtaining, Processing, and Preparing Data with Spark</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec22" class="sub-nav">
                                <a href="#ch03lvl1sec22">                    
                                    <div class="section-name">Accessing publicly available datasets</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec23" class="sub-nav">
                                <a href="#ch03lvl1sec23">                    
                                    <div class="section-name">Exploring and visualizing your data</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec24" class="sub-nav">
                                <a href="#ch03lvl1sec24">                    
                                    <div class="section-name">Processing and transforming your data</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec25" class="sub-nav">
                                <a href="#ch03lvl1sec25">                    
                                    <div class="section-name">Extracting useful features from your data</div>
                                </a>
                            </li>
                            <li data-chapter="3" data-section-id="ch03lvl1sec26" class="sub-nav">
                                <a href="#ch03lvl1sec26">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse4">
                                <div class="section-name">4: Building a Recommendation Engine with Spark</div>
                            </a>
                        </li>
                        <div id="collapse4" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="4" class="sub-nav">
                                <a href="#ch04">
                                    <div class="section-name">Chapter 4: Building a Recommendation Engine with Spark</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec27" class="sub-nav">
                                <a href="#ch04lvl1sec27">                    
                                    <div class="section-name">Types of recommendation models</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec28" class="sub-nav">
                                <a href="#ch04lvl1sec28">                    
                                    <div class="section-name">Extracting the right features from your data</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec29" class="sub-nav">
                                <a href="#ch04lvl1sec29">                    
                                    <div class="section-name">Training the recommendation model</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec30" class="sub-nav">
                                <a href="#ch04lvl1sec30">                    
                                    <div class="section-name">Using the recommendation model</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec31" class="sub-nav">
                                <a href="#ch04lvl1sec31">                    
                                    <div class="section-name">Evaluating the performance of recommendation models</div>
                                </a>
                            </li>
                            <li data-chapter="4" data-section-id="ch04lvl1sec32" class="sub-nav">
                                <a href="#ch04lvl1sec32">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse5">
                                <div class="section-name">5: Building a Classification Model with Spark</div>
                            </a>
                        </li>
                        <div id="collapse5" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="5" class="sub-nav">
                                <a href="#ch05">
                                    <div class="section-name">Chapter 5: Building a Classification Model with Spark</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec33" class="sub-nav">
                                <a href="#ch05lvl1sec33">                    
                                    <div class="section-name">Types of classification models</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec34" class="sub-nav">
                                <a href="#ch05lvl1sec34">                    
                                    <div class="section-name">Extracting the right features from your data</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec35" class="sub-nav">
                                <a href="#ch05lvl1sec35">                    
                                    <div class="section-name">Training classification models</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec36" class="sub-nav">
                                <a href="#ch05lvl1sec36">                    
                                    <div class="section-name">Using classification models</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec37" class="sub-nav">
                                <a href="#ch05lvl1sec37">                    
                                    <div class="section-name">Evaluating the performance of classification models</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec38" class="sub-nav">
                                <a href="#ch05lvl1sec38">                    
                                    <div class="section-name">Improving model performance and tuning parameters</div>
                                </a>
                            </li>
                            <li data-chapter="5" data-section-id="ch05lvl1sec39" class="sub-nav">
                                <a href="#ch05lvl1sec39">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse6">
                                <div class="section-name">6: Building a Regression Model with Spark</div>
                            </a>
                        </li>
                        <div id="collapse6" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="6" class="sub-nav">
                                <a href="#ch06">
                                    <div class="section-name">Chapter 6: Building a Regression Model with Spark</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec40" class="sub-nav">
                                <a href="#ch06lvl1sec40">                    
                                    <div class="section-name">Types of regression models</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec41" class="sub-nav">
                                <a href="#ch06lvl1sec41">                    
                                    <div class="section-name">Extracting the right features from your data</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec42" class="sub-nav">
                                <a href="#ch06lvl1sec42">                    
                                    <div class="section-name">Training and using regression models</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec43" class="sub-nav">
                                <a href="#ch06lvl1sec43">                    
                                    <div class="section-name">Evaluating the performance of regression models</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec44" class="sub-nav">
                                <a href="#ch06lvl1sec44">                    
                                    <div class="section-name">Improving model performance and tuning parameters</div>
                                </a>
                            </li>
                            <li data-chapter="6" data-section-id="ch06lvl1sec45" class="sub-nav">
                                <a href="#ch06lvl1sec45">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse7">
                                <div class="section-name">7: Building a Clustering Model with Spark</div>
                            </a>
                        </li>
                        <div id="collapse7" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="7" class="sub-nav">
                                <a href="#ch07">
                                    <div class="section-name">Chapter 7: Building a Clustering Model with Spark</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec47" class="sub-nav">
                                <a href="#ch07lvl1sec47">                    
                                    <div class="section-name">Types of clustering models</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec48" class="sub-nav">
                                <a href="#ch07lvl1sec48">                    
                                    <div class="section-name">Extracting the right features from your data</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec49" class="sub-nav">
                                <a href="#ch07lvl1sec49">                    
                                    <div class="section-name">Training a clustering model</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec50" class="sub-nav">
                                <a href="#ch07lvl1sec50">                    
                                    <div class="section-name">Making predictions using a clustering model</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec51" class="sub-nav">
                                <a href="#ch07lvl1sec51">                    
                                    <div class="section-name">Evaluating the performance of clustering models</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec52" class="sub-nav">
                                <a href="#ch07lvl1sec52">                    
                                    <div class="section-name">Tuning parameters for clustering models</div>
                                </a>
                            </li>
                            <li data-chapter="7" data-section-id="ch07lvl1sec53" class="sub-nav">
                                <a href="#ch07lvl1sec53">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse8">
                                <div class="section-name">8: Dimensionality Reduction with Spark</div>
                            </a>
                        </li>
                        <div id="collapse8" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="8" class="sub-nav">
                                <a href="#ch08">
                                    <div class="section-name">Chapter 8: Dimensionality Reduction with Spark</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec54" class="sub-nav">
                                <a href="#ch08lvl1sec54">                    
                                    <div class="section-name">Types of dimensionality reduction</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec55" class="sub-nav">
                                <a href="#ch08lvl1sec55">                    
                                    <div class="section-name">Extracting the right features from your data</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec56" class="sub-nav">
                                <a href="#ch08lvl1sec56">                    
                                    <div class="section-name">Training a dimensionality reduction model</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec57" class="sub-nav">
                                <a href="#ch08lvl1sec57">                    
                                    <div class="section-name">Using a dimensionality reduction model</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec58" class="sub-nav">
                                <a href="#ch08lvl1sec58">                    
                                    <div class="section-name">Evaluating dimensionality reduction models</div>
                                </a>
                            </li>
                            <li data-chapter="8" data-section-id="ch08lvl1sec59" class="sub-nav">
                                <a href="#ch08lvl1sec59">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse9">
                                <div class="section-name">9: Advanced Text Processing with Spark</div>
                            </a>
                        </li>
                        <div id="collapse9" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="9" class="sub-nav">
                                <a href="#ch09">
                                    <div class="section-name">Chapter 9: Advanced Text Processing with Spark</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec60" class="sub-nav">
                                <a href="#ch09lvl1sec60">                    
                                    <div class="section-name">What&#x27;s so special about text data?</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec61" class="sub-nav">
                                <a href="#ch09lvl1sec61">                    
                                    <div class="section-name">Extracting the right features from your data</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec62" class="sub-nav">
                                <a href="#ch09lvl1sec62">                    
                                    <div class="section-name">Using a TF-IDF model</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec63" class="sub-nav">
                                <a href="#ch09lvl1sec63">                    
                                    <div class="section-name">Evaluating the impact of text processing</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec64" class="sub-nav">
                                <a href="#ch09lvl1sec64">                    
                                    <div class="section-name">Word2Vec models</div>
                                </a>
                            </li>
                            <li data-chapter="9" data-section-id="ch09lvl1sec65" class="sub-nav">
                                <a href="#ch09lvl1sec65">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapse10">
                                <div class="section-name">10: Real-time Machine Learning with Spark Streaming</div>
                            </a>
                        </li>
                        <div id="collapse10" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="10" class="sub-nav">
                                <a href="#ch10">
                                    <div class="section-name">Chapter 10: Real-time Machine Learning with Spark Streaming</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec66" class="sub-nav">
                                <a href="#ch10lvl1sec66">                    
                                    <div class="section-name">Online learning</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec67" class="sub-nav">
                                <a href="#ch10lvl1sec67">                    
                                    <div class="section-name">Stream processing</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec68" class="sub-nav">
                                <a href="#ch10lvl1sec68">                    
                                    <div class="section-name">Creating a Spark Streaming application</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec69" class="sub-nav">
                                <a href="#ch10lvl1sec69">                    
                                    <div class="section-name">Online learning with Spark Streaming</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec70" class="sub-nav">
                                <a href="#ch10lvl1sec70">                    
                                    <div class="section-name">Online model evaluation</div>
                                </a>
                            </li>
                            <li data-chapter="10" data-section-id="ch10lvl1sec71" class="sub-nav">
                                <a href="#ch10lvl1sec71">                    
                                    <div class="section-name">Summary</div>
                                </a>
                            </li>
                        </div>
                        <li>
                            <a data-toggle="collapse" data-parent="#sidebar-nav" class="sidenav-menu-holder collapsed" href="#collapsebackindex">
                                <div class="section-name">Appendix A: Index</div>
                            </a>
                        </li>
                        <div id="collapsebackindex" class="panel-collapse collapse" role="tabpanel">
                            <li data-chapter="backindex" class="sub-nav">
                                <a href="#backindex">
                                    <div class="section-name">Chapter Appendix A: Index</div>
                                </a>
                            </li>
                        </div>
                    </div>
                </div>
            </ul>
        </div>
        
        <div id="page-content-wrapper" class="book-page">
            <a href="#" id="menu-toggle" class="toggle-nav"><i class="fa fa-bars fa-2x mr5"></i></a>
            
            <a href="#" id="back_to_top" class="back-to-top"><img src="../../mapt/images/kopimi.svg"></a>
            
            <div class="col-sm-12 col-xl-offset-2 col-xl-8 col-lg-offset-1 col-lg-10">
                <div class="btn-group pull-right mt15 mb30" role="group">
                    <a href="#home" class="btn btn-default">
                        <i class="fa fa-share fa-lg no-text-padding"></i>
                        <span class="hidden-xs ml5">Book Home</span>
                    </a>
                    <button class="btn btn-default" data-nid="17400" id="code-download">
                        <i class="fa fa-file fa-lg"></i>
                        <span class="hidden-xs ml5">Download Code Files</span>
                    </button>
                </div>
            </div>
            <div class="clearfix"></div>
            
            <div id="book-wrapper" class="container-fluid">
                <div class="col-sm-12 col-xl-offset-2 col-xl-8 col-lg-offset-1 col-lg-10" id="home">
                    <h2 class="product-title">Machine Learning with Spark</h2>
                    <hr>
                    <div class="row">
                        <div class="col-sm-12">
                            <h5 class="mt10">By Nick Pentreath</h5>
                            <div>
                                <p class="mb20"><b>Create scalable machine learning applications to power a modern data-driven business using Spark</b></p>
                                <a href="#ch01" class="btn btn-info btn-lg pull-right hidden-xs">
                                    Start Reading <i class="fa fa-chevron-right ml5"></i>
                                </a>
                                <a href="#ch01" class="btn btn-info btn-lg btn-block mt20 mb20 visible-xs">
                                    Start Reading <i class="fa fa-chevron-right ml5"></i>
                                </a>
                                <div class="clearfix"></div>
                                <div class="col-sm-12">
                                    <ul id="myTabs" class="nav nav-tabs nav-justified hidden-xs mt20" role="tablist">
                                        <li class="active">
                                            <a href="#info" role="tab" id="info-tab" data-toggle="tab">
                                                <h5>Info</h5>
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#content" role="tab" id="content-tab" data-toggle="tab">
                                                <h5>Contents</h5>
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#author" role="tab" id="author-tab" data-toggle="tab">
                                                <h5>Author</h5>
                                            </a>
                                        </li>
                                    </ul>
                
                                    <ul id="myTabsMobile" class="nav nav-pills text-center nav-stacked visible-xs mb60" role="tablist">
                                        <li class="active">
                                            <a href="#info" role="tab" id="info-tab-responsive" data-toggle="tab">
                                                Info
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#content" role="tab" id="content-tab-responsive" data-toggle="tab">
                                                Contents
                                            </a>
                                        </li>
                                        <li>
                                            <a href="#author" role="tab" id="author-tab-responsive" data-toggle="tab">
                                                Author
                                            </a>
                                        </li>
                                    </ul>
                
                                    <div id="myTabContent" class="tab-content pt30">
                                    
                                        <div role="tabpanel" class="tab-pane active fade in" id="info">
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">Features</h5>
                                            </div>
                                            <div class="hidden-xs">
                                                <h5>Features</h5>
                                            </div>
                                            <hr>
                                            <div>
                                                <ul>
                <li>A practical tutorial with real-world use cases allowing you to develop your own machine learning systems with Spark</li>
                <li>Combine various techniques and models into an intelligent machine learning system</li>
                <li>Use Sparkâ€™s powerful tools to load, analyze, clean, and transform your data</li>
                </ul>
                                            </div>
                                            <br>
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">Learning</h5>
                                            </div>
                                            <div class="hidden-xs">
                                                <h5>Learning</h5>
                                            </div>
                                            <hr>
                                            <div>
                                                <ul>
                <li>Create your first Spark program in Scala, Java, and Python</li>
                <li>Set up and configure a development environment for Spark on your own computer, as well as on Amazon EC2</li>
                <li>Access public machine learning datasets and use Spark to load, process, clean, and transform data</li>
                <li>Use Spark's machine learning library to implement programs utilizing well-known machine learning models including collaborative filtering, classification, regression, clustering, and dimensionality reduction</li>
                <li>Write Spark functions to evaluate the performance of your machine learning models</li>
                <li>Deal with large-scale text data, including feature extraction and using text data as input to your machine learning models</li>
                <li>Explore online learning methods and use Spark Streaming for online learning and model evaluation</li>
                </ul>
                                            </div>
                                            <br>
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">About</h5>
                                            </div>
                                            <div class="hidden-xs">
                                                <h5>About</h5>
                                            </div>
                                            <hr>
                                            <div>
                                                <p>Apache Spark is a framework for distributed computing that is designed from the ground up to be optimized for low latency tasks and in-memory data storage. It is one of the few frameworks for parallel computing that combines speed, scalability, in-memory processing, and fault tolerance with ease of programming and a flexible, expressive, and powerful API design.</p>
                <p>This book guides you through the basics of Spark's API used to load and process data and prepare the data to use as input to the various machine learning models. There are detailed examples and real-world use cases for you to explore common machine learning models including recommender systems, classification, regression, clustering, and dimensionality reduction. You will cover advanced topics such as working with large-scale text data, and methods for online machine learning and model evaluation using Spark Streaming.</p>
                                            </div>
                                        </div>
                                        
                                        <div role="tabpanel" class="tab-pane fade in" id="content">
                                            <div class="visible-xs">
                                                <h5 class="mobile-title">Contents</h5>
                                                <hr>
                                            </div>
                                            <ul>
                                                <div>
                                                    <li data-chapter="1">
                                                        <div class="section-name">1: Getting Up and Running with Spark</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="1" class="chapter-section">
                                                                    <a href="#ch01">        
                                                                        <div class="section-name">Chapter 1: Getting Up and Running with Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec08" class="chapter-section">
                                                                    <a href="#ch01lvl1sec08">                    
                                                                        <div class="section-name">Installing and setting up Spark locally</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec09" class="chapter-section">
                                                                    <a href="#ch01lvl1sec09">                    
                                                                        <div class="section-name">Spark clusters</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec10" class="chapter-section">
                                                                    <a href="#ch01lvl1sec10">                    
                                                                        <div class="section-name">The Spark programming model</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec11" class="chapter-section">
                                                                    <a href="#ch01lvl1sec11">                    
                                                                        <div class="section-name">The first step to a Spark program in Scala</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec12" class="chapter-section">
                                                                    <a href="#ch01lvl1sec12">                    
                                                                        <div class="section-name">The first step to a Spark program in Java</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec13" class="chapter-section">
                                                                    <a href="#ch01lvl1sec13">                    
                                                                        <div class="section-name">The first step to a Spark program in Python</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec14" class="chapter-section">
                                                                    <a href="#ch01lvl1sec14">                    
                                                                        <div class="section-name">Getting Spark running on Amazon EC2</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="1" data-section-id="ch01lvl1sec15" class="chapter-section">
                                                                    <a href="#ch01lvl1sec15">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="2">
                                                        <div class="section-name">2: Designing a Machine Learning System</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="2" class="chapter-section">
                                                                    <a href="#ch02">        
                                                                        <div class="section-name">Chapter 2: Designing a Machine Learning System</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec16" class="chapter-section">
                                                                    <a href="#ch02lvl1sec16">                    
                                                                        <div class="section-name">Introducing MovieStream</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec17" class="chapter-section">
                                                                    <a href="#ch02lvl1sec17">                    
                                                                        <div class="section-name">Business use cases for a machine learning system</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec18" class="chapter-section">
                                                                    <a href="#ch02lvl1sec18">                    
                                                                        <div class="section-name">Types of machine learning models</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec19" class="chapter-section">
                                                                    <a href="#ch02lvl1sec19">                    
                                                                        <div class="section-name">The components of a data-driven machine learning system</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec20" class="chapter-section">
                                                                    <a href="#ch02lvl1sec20">                    
                                                                        <div class="section-name">An architecture for a machine learning system</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="2" data-section-id="ch02lvl1sec21" class="chapter-section">
                                                                    <a href="#ch02lvl1sec21">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="3">
                                                        <div class="section-name">3: Obtaining, Processing, and Preparing Data with Spark</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="3" class="chapter-section">
                                                                    <a href="#ch03">        
                                                                        <div class="section-name">Chapter 3: Obtaining, Processing, and Preparing Data with Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec22" class="chapter-section">
                                                                    <a href="#ch03lvl1sec22">                    
                                                                        <div class="section-name">Accessing publicly available datasets</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec23" class="chapter-section">
                                                                    <a href="#ch03lvl1sec23">                    
                                                                        <div class="section-name">Exploring and visualizing your data</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec24" class="chapter-section">
                                                                    <a href="#ch03lvl1sec24">                    
                                                                        <div class="section-name">Processing and transforming your data</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec25" class="chapter-section">
                                                                    <a href="#ch03lvl1sec25">                    
                                                                        <div class="section-name">Extracting useful features from your data</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="3" data-section-id="ch03lvl1sec26" class="chapter-section">
                                                                    <a href="#ch03lvl1sec26">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="4">
                                                        <div class="section-name">4: Building a Recommendation Engine with Spark</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="4" class="chapter-section">
                                                                    <a href="#ch04">        
                                                                        <div class="section-name">Chapter 4: Building a Recommendation Engine with Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec27" class="chapter-section">
                                                                    <a href="#ch04lvl1sec27">                    
                                                                        <div class="section-name">Types of recommendation models</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec28" class="chapter-section">
                                                                    <a href="#ch04lvl1sec28">                    
                                                                        <div class="section-name">Extracting the right features from your data</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec29" class="chapter-section">
                                                                    <a href="#ch04lvl1sec29">                    
                                                                        <div class="section-name">Training the recommendation model</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec30" class="chapter-section">
                                                                    <a href="#ch04lvl1sec30">                    
                                                                        <div class="section-name">Using the recommendation model</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec31" class="chapter-section">
                                                                    <a href="#ch04lvl1sec31">                    
                                                                        <div class="section-name">Evaluating the performance of recommendation models</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="4" data-section-id="ch04lvl1sec32" class="chapter-section">
                                                                    <a href="#ch04lvl1sec32">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="5">
                                                        <div class="section-name">5: Building a Classification Model with Spark</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="5" class="chapter-section">
                                                                    <a href="#ch05">        
                                                                        <div class="section-name">Chapter 5: Building a Classification Model with Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec33" class="chapter-section">
                                                                    <a href="#ch05lvl1sec33">                    
                                                                        <div class="section-name">Types of classification models</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec34" class="chapter-section">
                                                                    <a href="#ch05lvl1sec34">                    
                                                                        <div class="section-name">Extracting the right features from your data</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec35" class="chapter-section">
                                                                    <a href="#ch05lvl1sec35">                    
                                                                        <div class="section-name">Training classification models</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec36" class="chapter-section">
                                                                    <a href="#ch05lvl1sec36">                    
                                                                        <div class="section-name">Using classification models</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec37" class="chapter-section">
                                                                    <a href="#ch05lvl1sec37">                    
                                                                        <div class="section-name">Evaluating the performance of classification models</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec38" class="chapter-section">
                                                                    <a href="#ch05lvl1sec38">                    
                                                                        <div class="section-name">Improving model performance and tuning parameters</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="5" data-section-id="ch05lvl1sec39" class="chapter-section">
                                                                    <a href="#ch05lvl1sec39">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="6">
                                                        <div class="section-name">6: Building a Regression Model with Spark</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="6" class="chapter-section">
                                                                    <a href="#ch06">        
                                                                        <div class="section-name">Chapter 6: Building a Regression Model with Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec40" class="chapter-section">
                                                                    <a href="#ch06lvl1sec40">                    
                                                                        <div class="section-name">Types of regression models</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec41" class="chapter-section">
                                                                    <a href="#ch06lvl1sec41">                    
                                                                        <div class="section-name">Extracting the right features from your data</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec42" class="chapter-section">
                                                                    <a href="#ch06lvl1sec42">                    
                                                                        <div class="section-name">Training and using regression models</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec43" class="chapter-section">
                                                                    <a href="#ch06lvl1sec43">                    
                                                                        <div class="section-name">Evaluating the performance of regression models</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec44" class="chapter-section">
                                                                    <a href="#ch06lvl1sec44">                    
                                                                        <div class="section-name">Improving model performance and tuning parameters</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="6" data-section-id="ch06lvl1sec45" class="chapter-section">
                                                                    <a href="#ch06lvl1sec45">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="7">
                                                        <div class="section-name">7: Building a Clustering Model with Spark</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="7" class="chapter-section">
                                                                    <a href="#ch07">        
                                                                        <div class="section-name">Chapter 7: Building a Clustering Model with Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec47" class="chapter-section">
                                                                    <a href="#ch07lvl1sec47">                    
                                                                        <div class="section-name">Types of clustering models</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec48" class="chapter-section">
                                                                    <a href="#ch07lvl1sec48">                    
                                                                        <div class="section-name">Extracting the right features from your data</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec49" class="chapter-section">
                                                                    <a href="#ch07lvl1sec49">                    
                                                                        <div class="section-name">Training a clustering model</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec50" class="chapter-section">
                                                                    <a href="#ch07lvl1sec50">                    
                                                                        <div class="section-name">Making predictions using a clustering model</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec51" class="chapter-section">
                                                                    <a href="#ch07lvl1sec51">                    
                                                                        <div class="section-name">Evaluating the performance of clustering models</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec52" class="chapter-section">
                                                                    <a href="#ch07lvl1sec52">                    
                                                                        <div class="section-name">Tuning parameters for clustering models</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="7" data-section-id="ch07lvl1sec53" class="chapter-section">
                                                                    <a href="#ch07lvl1sec53">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="8">
                                                        <div class="section-name">8: Dimensionality Reduction with Spark</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="8" class="chapter-section">
                                                                    <a href="#ch08">        
                                                                        <div class="section-name">Chapter 8: Dimensionality Reduction with Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec54" class="chapter-section">
                                                                    <a href="#ch08lvl1sec54">                    
                                                                        <div class="section-name">Types of dimensionality reduction</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec55" class="chapter-section">
                                                                    <a href="#ch08lvl1sec55">                    
                                                                        <div class="section-name">Extracting the right features from your data</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec56" class="chapter-section">
                                                                    <a href="#ch08lvl1sec56">                    
                                                                        <div class="section-name">Training a dimensionality reduction model</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec57" class="chapter-section">
                                                                    <a href="#ch08lvl1sec57">                    
                                                                        <div class="section-name">Using a dimensionality reduction model</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec58" class="chapter-section">
                                                                    <a href="#ch08lvl1sec58">                    
                                                                        <div class="section-name">Evaluating dimensionality reduction models</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="8" data-section-id="ch08lvl1sec59" class="chapter-section">
                                                                    <a href="#ch08lvl1sec59">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="9">
                                                        <div class="section-name">9: Advanced Text Processing with Spark</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="9" class="chapter-section">
                                                                    <a href="#ch09">        
                                                                        <div class="section-name">Chapter 9: Advanced Text Processing with Spark</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec60" class="chapter-section">
                                                                    <a href="#ch09lvl1sec60">                    
                                                                        <div class="section-name">What&#x27;s so special about text data?</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec61" class="chapter-section">
                                                                    <a href="#ch09lvl1sec61">                    
                                                                        <div class="section-name">Extracting the right features from your data</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec62" class="chapter-section">
                                                                    <a href="#ch09lvl1sec62">                    
                                                                        <div class="section-name">Using a TF-IDF model</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec63" class="chapter-section">
                                                                    <a href="#ch09lvl1sec63">                    
                                                                        <div class="section-name">Evaluating the impact of text processing</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec64" class="chapter-section">
                                                                    <a href="#ch09lvl1sec64">                    
                                                                        <div class="section-name">Word2Vec models</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="9" data-section-id="ch09lvl1sec65" class="chapter-section">
                                                                    <a href="#ch09lvl1sec65">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="10">
                                                        <div class="section-name">10: Real-time Machine Learning with Spark Streaming</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="10" class="chapter-section">
                                                                    <a href="#ch10">        
                                                                        <div class="section-name">Chapter 10: Real-time Machine Learning with Spark Streaming</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec66" class="chapter-section">
                                                                    <a href="#ch10lvl1sec66">                    
                                                                        <div class="section-name">Online learning</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec67" class="chapter-section">
                                                                    <a href="#ch10lvl1sec67">                    
                                                                        <div class="section-name">Stream processing</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec68" class="chapter-section">
                                                                    <a href="#ch10lvl1sec68">                    
                                                                        <div class="section-name">Creating a Spark Streaming application</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec69" class="chapter-section">
                                                                    <a href="#ch10lvl1sec69">                    
                                                                        <div class="section-name">Online learning with Spark Streaming</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec70" class="chapter-section">
                                                                    <a href="#ch10lvl1sec70">                    
                                                                        <div class="section-name">Online model evaluation</div>
                                                                    </a>
                                                                </li>
                                                                <li data-chapter="10" data-section-id="ch10lvl1sec71" class="chapter-section">
                                                                    <a href="#ch10lvl1sec71">                    
                                                                        <div class="section-name">Summary</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                                <div>
                                                    <li data-chapter="backindex">
                                                        <div class="section-name">Appendix A: Index</div>
                                                        <div class="panel-collapse" role="tabpanel">
                                                            <ul>
                                                                <li data-chapter="backindex" class="chapter-section">
                                                                    <a href="#backindex">
                                                                        <div class="section-name">Chapter Appendix A: Index</div>
                                                                    </a>
                                                                </li>
                                                            </ul>
                                                        </div>
                                                    </li>
                                                </div>
                                            </ul>
                                        </div>
                                        
                                        <div role="tabpanel" class="tab-pane fade" id="author">
                                            <div class="visible-xs">
                                                <h4 class="mobile-title">About the Author</h4>
                                                <hr>
                                            </div>
                                            <p><strong>Nick Pentreath</strong></p>
                                            <div>
                                                <p>Nick Pentreath has a background in financial markets, machine learning, and software development. He has worked at Goldman Sachs Group, Inc.; as a research scientist at the online ad targeting start-up Cognitive Match Limited, London; and led the Data Science and Analytics team at Mxit, Africa's largest social network.</p>
                <p>He is a cofounder of Graphflow, a big data and machine learning company focused on user-centric recommendations and customer intelligence. He is passionate about combining commercial focus with machine learning and cutting-edge technology to build intelligent systems that learn from data to add value to the bottom line.</p>
                <p>Nick is a member of the Apache Spark Project Management Committee.</p>
                                            </div>
                                        </div>
                                        
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    
                    <div class="next-wrapper">
                        <div class="row ns">
                            <hr />
                            <span class="hidden-xs">
                                <h4 class="pull-left">
                                    <strong>Up Next: </strong><span class="section-title"></span>
                                </h4>
                                <a href="#" class="btn btn-primary pull-right btn-lg">
                                    Next Section
                                </a>
                            </span>
                            <span class="visible-xs">
                                <a href="#" class="btn btn-primary btn-block btn-lg">
                                    Next Section
                                </a>
                            </span>
                        </div>
                        <div class="row ns">
                            <hr>
                        </div>
                    </div>
                </div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch01"></a>ChapterÂ 1.Â Getting Up and Running with Spark</h2></div></div></div><p>Apache Spark is a framework for distributed computing; this framework aims to make it simpler to write programs that run in parallel across many nodes in a cluster of computers. It tries to abstract the tasks of resource scheduling, job submission, execution, tracking, and communication between nodes, as well as the low-level operations that are inherent in parallel data processing. It also provides a higher level API to work with distributed data. In this way, it is similar to other distributed processing frameworks such as Apache Hadoop; however, the underlying architecture is somewhat different.</p><p>Spark began as a research project at the University of California, Berkeley. The university was focused on the use case of distributed machine learning algorithms. Hence, it is designed from the ground up for high performance in applications of an iterative nature, where the same data is accessed multiple times. This performance is achieved primarily through caching datasets in memory, combined with low latency and overhead to launch parallel computation tasks. Together with other features such as fault tolerance, flexible distributed-memory data structures, and a powerful functional API, Spark has proved to be broadly useful for a wide range of large-scale data processing tasks, over and above machine learning and iterative analytics.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note02"></a>Note</h3><p>For more <a id="id0" class="indexterm"></a>background on Spark, including the research papers underlying Spark's development, see the project's history page at <a class="ulink" href="http://spark.apache.org/community.html#history" target="_blank">http://spark.apache.org/community.html#history</a>.</p></div><p>Spark runs in four modes:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The <a id="id1" class="indexterm"></a>standalone local mode, where all Spark<a id="id2" class="indexterm"></a> processes are run within the<a id="id3" class="indexterm"></a> same <span class="strong"><strong>Java Virtual Machine</strong></span> (<span class="strong"><strong>JVM</strong></span>) process</p></li><li style="list-style-type: disc"><p>The <a id="id4" class="indexterm"></a>standalone cluster mode, using <a id="id5" class="indexterm"></a>Spark's own built-in job-scheduling framework</p></li><li style="list-style-type: disc"><p>Using Mesos, a <a id="id6" class="indexterm"></a>popular open source cluster-computing <a id="id7" class="indexterm"></a>framework</p></li><li style="list-style-type: disc"><p>Using <a id="id8" class="indexterm"></a>YARN (commonly referred to as NextGen MapReduce), a<a id="id9" class="indexterm"></a> Hadoop-related cluster-computing and resource-scheduling framework</p></li></ul></div><p>In this chapter, we will:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Download the Spark binaries and set up a development environment that runs in Spark's standalone local mode. This environment will be used throughout the rest of the book to run the example code.</p></li><li style="list-style-type: disc"><p>Explore Spark's programming model and API using Spark's interactive console.</p></li><li style="list-style-type: disc"><p>Write our first Spark program in Scala, Java, and Python.</p></li><li style="list-style-type: disc"><p>Set up a Spark cluster using Amazon's <span class="strong"><strong>Elastic Cloud Compute</strong></span> (<span class="strong"><strong>EC2</strong></span>) platform, which<a id="id10" class="indexterm"></a> can be used for large-sized data and heavier computational requirements, rather than running in the local mode.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip02"></a>Tip</h3><p>Spark can also be run on Amazon's Elastic MapReduce service using custom bootstrap action scripts, but this is beyond the scope of this book. The following article is a good reference guide: <a class="ulink" href="http://aws.amazon.com/articles/Elastic-MapReduce/4926593393724923" target="_blank">http://aws.amazon.com/articles/Elastic-MapReduce/4926593393724923</a>.</p><p>At the time of writing this book, the article covers running Spark Version 1.1.0.</p></div></li></ul></div><p>If you have previous experience in setting up Spark and are familiar with the basics of writing a Spark program, feel free to skip this chapter.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec08"></a>Installing and setting up Spark locally</h2></div></div><hr /></div><p>Spark <a id="id11" class="indexterm"></a>can be run using the built-in standalone cluster scheduler in the local <a id="id12" class="indexterm"></a>mode. This means that all the Spark processes are run within the same JVMâ€”effectively, a single, multithreaded instance of Spark. The local mode is very useful for prototyping, development, debugging, and testing. However, this mode can also be useful in real-world scenarios to perform parallel computation across multiple cores on a single computer.</p><p>As Spark's local mode is fully compatible with the cluster mode, programs written and tested locally can be run on a cluster with just a few additional steps.</p><p>The first step in setting up Spark locally is to download the latest version (at the time of writing this<a id="id13" class="indexterm"></a> book, the version is 1.2.0). The download page of the Spark project website, found at <a class="ulink" href="http://spark.apache.org/downloads.html" target="_blank">http://spark.apache.org/downloads.html</a>, contains links to download various versions as well as to obtain the latest source code via GitHub.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip03"></a>Tip</h3><p>The<a id="id14" class="indexterm"></a> Spark project documentation website at <a class="ulink" href="http://spark.apache.org/docs/latest/" target="_blank">http://spark.apache.org/docs/latest/</a> is a comprehensive resource to learn more about Spark. We highly recommend that you explore it!</p></div><p>Spark<a id="id15" class="indexterm"></a> needs to be built against a specific version of Hadoop in order to <a id="id16" class="indexterm"></a>access <span class="strong"><strong>Hadoop Distributed File System</strong></span> (<span class="strong"><strong>HDFS</strong></span>) as well as<a id="id17" class="indexterm"></a> standard and custom Hadoop input sources. The download page provides prebuilt binary packages for Hadoop 1, CDH4 (Cloudera's Hadoop Distribution), MapR's Hadoop distribution, and Hadoop 2 (YARN). Unless you wish to build Spark against a specific Hadoop version, we recommend that you download the prebuilt Hadoop 2.4 package from an Apache mirror using this link: <a class="ulink" href="http://www.apache.org/dyn/closer.cgi/spark/spark-1.2.0/spark-1.2.0-bin-hadoop2.4.tgz" target="_blank">http://www.apache.org/dyn/closer.cgi/spark/spark-1.2.0/spark-1.2.0-bin-hadoop2.4.tgz</a>.</p><p>Spark requires the Scala programming language (version 2.10.4 at the time of writing this book) in order to run. Fortunately, the prebuilt binary package comes with the Scala runtime packages included, so you don't need to install Scala separately in order to get started. However, you <a id="id18" class="indexterm"></a>will need to have a <span class="strong"><strong>Java Runtime Environment</strong></span> (<span class="strong"><strong>JRE</strong></span>) or <span class="strong"><strong>Java Development Kit</strong></span> (<span class="strong"><strong>JDK</strong></span>) installed (see the software <a id="id19" class="indexterm"></a>and hardware list in this book's code bundle for installation instructions).</p><p>Once you have downloaded the Spark binary package, unpack the contents of the package and change into the newly created directory by running the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;tar xfvz spark-1.2.0-bin-hadoop2.4.tgz</strong></span>
<span class="strong"><strong>&gt;cd spark-1.2.0-bin-hadoop2.4</strong></span>
</pre></div><p>Spark places user scripts to run Spark in the <code class="literal">bin</code> directory. You can test whether everything is working correctly by running one of the example programs included in Spark:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;./bin/run-example org.apache.spark.examples.SparkPi</strong></span>
</pre></div><p>This will run the example in Spark's local standalone mode. In this mode, all the Spark processes are run within the same JVM, and Spark uses multiple threads for parallel processing. By default, the preceding example uses a number of threads equal to the number of cores available on your system. Once the program is finished running, you should see something similar to the following lines near the end of the output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>â€¦</strong></span>
<span class="strong"><strong>14/11/27 20:58:47 INFO SparkContext: Job finished: reduce at SparkPi.scala:35, took 0.723269 s</strong></span>
<span class="strong"><strong>Pi is roughly 3.1465</strong></span>
<span class="strong"><strong>â€¦</strong></span>
</pre></div><p>To<a id="id20" class="indexterm"></a> configure<a id="id21" class="indexterm"></a> the level of parallelism in the local mode, you can pass in a <code class="literal">master</code> parameter of the <code class="literal">local[N]</code> form, where <code class="literal">N</code> is the number of threads to use. For example, to use only two threads, run the following command instead:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;MASTER=local[2] ./bin/run-example org.apache.spark.examples.SparkPi</strong></span>
</pre></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec09"></a>Spark clusters</h2></div></div><hr /></div><p>A Spark<a id="id22" class="indexterm"></a> cluster is made up of two types of processes: a driver program and multiple executors. In the local mode, all these processes are run within the same JVM. In a cluster, these processes are usually run on separate nodes.</p><p>For example, a typical cluster that runs in Spark's standalone mode (that is, using Spark's built-in cluster-management modules) will have:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>A master node that runs the Spark standalone master process as well as the driver program</p></li><li style="list-style-type: disc"><p>A number of worker nodes, each running an executor process</p></li></ul></div><p>While we will be using Spark's local standalone mode throughout this book to illustrate concepts and examples, the same Spark code that we write can be run on a Spark cluster. In the preceding example, if we run the code on a Spark standalone cluster, we could simply pass in the URL for the master node as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;MASTER=spark://IP:PORT ./bin/run-example org.apache.spark.examples.SparkPi</strong></span>
</pre></div><p>Here, <code class="literal">IP</code> is the IP address, and <code class="literal">PORT</code> is the port of the Spark master. This tells Spark to run the program on the cluster where the Spark master process is running.</p><p>A full treatment of Spark's cluster management and deployment is beyond the scope of this book. However, we will briefly teach you how to set up and use an Amazon EC2 cluster later in this chapter.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note03"></a>Note</h3><p>For an overview of the Spark cluster-application deployment, take a look at the following<a id="id23" class="indexterm"></a> links:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><a class="ulink" href="http://spark.apache.org/docs/latest/cluster-overview.html" target="_blank">http://spark.apache.org/docs/latest/cluster-overview.html</a></p></li><li style="list-style-type: disc"><p><a class="ulink" href="http://spark.apache.org/docs/latest/submitting-applications.html" target="_blank">http://spark.apache.org/docs/latest/submitting-applications.html</a></p></li></ul></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec10"></a>The Spark programming model</h2></div></div><hr /></div><p>Before <a id="id24" class="indexterm"></a>we delve into a high-level overview of Spark's design, we will introduce the <code class="literal">SparkContext</code> object as well as the Spark shell, which we will use to interactively explore the basics of the Spark programming model.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip04"></a>Tip</h3><p>While this section provides a brief overview and examples of using Spark, we recommend that you read the following documentation to get a detailed understanding:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Spark <a id="id25" class="indexterm"></a>Quick Start: <a class="ulink" href="http://spark.apache.org/docs/latest/quick-start.html" target="_blank">http://spark.apache.org/docs/latest/quick-start.html</a></p></li><li style="list-style-type: disc"><p><span class="emphasis"><em>Spark Programming guide</em></span>, which covers Scala, Java, and Python: <a class="ulink" href="http://spark.apache.org/docs/latest/programming-guide.html" target="_blank">http://spark.apache.org/docs/latest/programming-guide.html</a></p></li></ul></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec07"></a>SparkContext and SparkConf</h3></div></div></div><p>The starting<a id="id26" class="indexterm"></a> point of writing any Spark program<a id="id27" class="indexterm"></a> is <code class="literal">SparkContext</code> (or <code class="literal">JavaSparkContext</code> in Java). <code class="literal">SparkContext</code> is<a id="id28" class="indexterm"></a> initialized with an instance of <a id="id29" class="indexterm"></a>a <code class="literal">SparkConf</code> object, which contains various Spark cluster-configuration settings (for example, the URL of the master node).</p><p>Once initialized, we will use the various methods found in the <code class="literal">SparkContext</code> object to create and manipulate distributed datasets and shared variables. The Spark shell (in both Scala and Python, which is unfortunately not supported in Java) takes care of this context initialization for us, but the following lines of code show an example of creating a context running in the local mode in Scala:</p><div class="informalexample"><pre class="programlisting">val conf = new SparkConf()
.setAppName("Test Spark App")
.setMaster("local[4]")
val sc = new SparkContext(conf) </pre></div><p>This creates a context running in the local mode with four threads, with the name of the application set to <code class="literal">Test Spark App</code>. If we wish to use default configuration values, we could also<a id="id30" class="indexterm"></a> call the following simple constructor<a id="id31" class="indexterm"></a> for our <code class="literal">SparkContext</code> object, which <a id="id32" class="indexterm"></a>works in exactly the <a id="id33" class="indexterm"></a>same way:</p><div class="informalexample"><pre class="programlisting">val sc = new SparkContext("local[4]", "Test Spark App")</pre></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip05"></a>Tip</h3><p><span class="strong"><strong>Downloading the example code</strong></span></p><p>You can download the example code files for all Packt books you have purchased from your account at <a class="ulink" href="http://www.packtpub.com" target="_blank">http://www.packtpub.com</a>. If you purchased this book elsewhere, you can visit <a class="ulink" href="http://www.packtpub.com/support" target="_blank">http://www.packtpub.com/support</a> and register to have the files e-mailed directly to you.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec08"></a>The Spark shell</h3></div></div></div><p>Spark<a id="id34" class="indexterm"></a> supports writing programs interactively<a id="id35" class="indexterm"></a> using either the Scala or Python REPL (that is, the <span class="strong"><strong>Read-Eval-Print-Loop</strong></span>, or interactive shell). The shell provides instant feedback as we enter<a id="id36" class="indexterm"></a> code, as this code is immediately evaluated. In the Scala shell, the return result and type is also displayed after a piece of code is run.</p><p>To use the Spark shell with Scala, simply run <code class="literal">./bin/spark-shell</code> from the Spark base directory. This will launch the Scala shell and initialize <code class="literal">SparkContext</code>, which is available to us as the Scala value, <code class="literal">sc</code>. Your console output should look similar to the following screenshot:</p><div class="mediaobject"><img src="graphics/8519OS_01_01.jpg" /></div><p>To <a id="id37" class="indexterm"></a>use the Python shell with Spark, simply run<a id="id38" class="indexterm"></a> the <code class="literal">./bin/pyspark</code> command. Like the Scala shell, the Python <code class="literal">SparkContext</code> object should be available as the Python variable <code class="literal">sc</code>. You should see an output similar to the one shown in this screenshot:</p><div class="mediaobject"><img src="graphics/8519OS_01_02.jpg" /></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec09"></a>Resilient Distributed Datasets</h3></div></div></div><p>The core of Spark is a concept called the <span class="strong"><strong>Resilient Distributed Dataset</strong></span> (<span class="strong"><strong>RDD</strong></span>). An RDD is a collection of "records" (strictly<a id="id39" class="indexterm"></a> speaking, objects of some type) that is distributed <a id="id40" class="indexterm"></a>or partitioned across many nodes in a cluster (for the purposes of the Spark local mode, the single multithreaded process can be thought of in the same way). An RDD in Spark is fault-tolerant; this means that if a given node or task fails (for some reason other than erroneous user code, such as hardware failure, loss of communication, and so on), the<a id="id41" class="indexterm"></a> RDD can be<a id="id42" class="indexterm"></a> reconstructed automatically on the remaining nodes and the job will still complete.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec01"></a>Creating RDDs</h4></div></div></div><p>RDDs can be <a id="id43" class="indexterm"></a>created from existing collections, for example, in the Scala Spark shell that you launched earlier:</p><div class="informalexample"><pre class="programlisting">val collection = List("a", "b", "c", "d", "e")
val rddFromCollection = sc.parallelize(collection)</pre></div><p>RDDs can also be created from Hadoop-based input sources, including the local filesystem, HDFS, and Amazon S3. A Hadoop-based RDD can utilize any input format that implements the Hadoop <code class="literal">InputFormat</code> interface, including text files, other standard Hadoop formats, HBase, Cassandra, and many more. The following code is an example of creating an RDD from a text file located on the local filesystem:</p><div class="informalexample"><pre class="programlisting">val rddFromTextFile = sc.textFile("LICENSE")</pre></div><p>The preceding <code class="literal">textFile</code> method returns an RDD where each record is a <code class="literal">String</code> object that represents one line of the text file.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec02"></a>Spark operations</h4></div></div></div><p>Once we have created an RDD, we have a distributed collection of records that we can manipulate. In <a id="id44" class="indexterm"></a>Spark's programming model, operations are split <a id="id45" class="indexterm"></a>into transformations and actions. Generally speaking, a transformation operation applies some function to all the records in the dataset, changing the records in some way. An action typically runs some computation or aggregation operation and returns the result to the driver program where <code class="literal">SparkContext</code> is running.</p><p>Spark operations are functional in style. For programmers familiar with functional programming in Scala or Python, these operations should seem natural. For those without experience in functional programming, don't worry; the Spark API is relatively easy to learn.</p><p>One of the most common transformations that you will use in Spark programs is the <code class="literal">map</code> operator. This applies a function to each record of an RDD, thus <span class="emphasis"><em>mapping</em></span> the input to some new output. For example, the following code fragment takes the RDD we created from a local text file and applies the <code class="literal">size</code> function to each record in the RDD. Remember that we created an RDD of <code class="literal">Strings</code>. Using <code class="literal">map</code>, we can transform each string to an integer, thus returning an RDD of <code class="literal">Ints</code>:</p><div class="informalexample"><pre class="programlisting">val intsFromStringsRDD = rddFromTextFile.map(<span class="strong"><strong>line =&gt; line.size</strong></span>)</pre></div><p>You should see output similar to the following line in your shell; this indicates the type of the RDD:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>intsFromStringsRDD: org.apache.spark.rdd.RDD[Int] = MappedRDD[5] at map at &lt;console&gt;:14</strong></span>
</pre></div><p>In the preceding<a id="id46" class="indexterm"></a> code, we saw the <code class="literal">=&gt;</code> syntax used. This is the Scala<a id="id47" class="indexterm"></a> syntax for an anonymous function, which is a function that is not a named method (that is, one defined using the <code class="literal">def</code> keyword in Scala or Python, for example).</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note04"></a>Note</h3><p>While a detailed treatment of anonymous functions is beyond the scope of this book, they are used extensively in Spark code in Scala and Python, as well as in Java 8 (both in examples and real-world applications), so it is useful to cover a few practicalities.</p><p>The <code class="literal">line =&gt; line.size</code> syntax means that we are applying a function where <a id="id48" class="indexterm"></a>the input variable is to the left of the <code class="literal">=&gt;</code> operator, and the output is the result of the code to the right of the <code class="literal">=&gt;</code> operator. In this case, the input is <code class="literal">line</code>, and the output is the result of calling <code class="literal">line.size</code>. In Scala, this function that maps a string to an integer is expressed as <code class="literal">String =&gt; Int</code>.</p><p>This syntax saves us from having to separately define functions every time we use methods such as <code class="literal">map</code>; this is useful when the function is simple and will only be used once, as in this example.</p></div><p>Now, we can apply a common action operation, <code class="literal">count</code>, to return the number of records in our RDD:</p><div class="informalexample"><pre class="programlisting">intsFromStringsRDD.count</pre></div><p>The result should look something like the following console output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>14/01/29 23:28:28 INFO SparkContext: Starting job: count at &lt;console&gt;:17</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>14/01/29 23:28:28 INFO SparkContext: Job finished: count at &lt;console&gt;:17, took 0.019227 s</strong></span>
<span class="strong"><strong>res4: Long = 398</strong></span>
</pre></div><p>Perhaps we want to find the average length of each line in this text file. We can first use the <code class="literal">sum</code> function to add up all the lengths of all the records and then divide the sum by the number of records:</p><div class="informalexample"><pre class="programlisting">val sumOfRecords = intsFromStringsRDD.sum
val numRecords = intsFromStringsRDD.count
val aveLengthOfRecord = sumOfRecords / numRecords</pre></div><p>The result will be as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>aveLengthOfRecord: Double = 52.06030150753769</strong></span>
</pre></div><p>Spark<a id="id49" class="indexterm"></a> operations, in most cases, return a new RDD, with the exception<a id="id50" class="indexterm"></a> of most actions, which return the result of a computation (such as <code class="literal">Long</code> for <code class="literal">count</code> and <code class="literal">Double</code> for <code class="literal">sum</code> in the preceding example). This means that we can naturally chain together operations to make our program flow more concise and expressive. For example, the same result as the one in the preceding line of code can be achieved using the following code:</p><div class="informalexample"><pre class="programlisting">val aveLengthOfRecordChained = rddFromTextFile.map(line =&gt; line.size).sum / rddFromTextFile.count</pre></div><p>An important point to note is that Spark transformations are lazy. That is, invoking a transformation on an RDD does not immediately trigger a computation. Instead, transformations are chained together and are effectively only computed when an action is called. This allows Spark to be more efficient by only returning results to the driver when necessary so that the majority of operations are performed in parallel on the cluster.</p><p>This means that if your Spark program never uses an action operation, it will never trigger an actual computation, and you will not get any results. For example, the following code will simply return a new RDD that represents the chain of transformations:</p><div class="informalexample"><pre class="programlisting">val transformedRDD = rddFromTextFile.map(line =&gt; line.size).filter(size =&gt; size &gt; 10).map(size =&gt; size * 2)</pre></div><p>This returns the following result in the console:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>transformedRDD: org.apache.spark.rdd.RDD[Int] = MappedRDD[8] at map at &lt;console&gt;:14</strong></span>
</pre></div><p>Notice that no actual computation happens and no result is returned. If we now call an action, such as <code class="literal">sum</code>, on the resulting RDD, the computation will be triggered:</p><div class="informalexample"><pre class="programlisting">val computation = transformedRDD.sum</pre></div><p>You will now see that a Spark job is run, and it results in the following console output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>14/11/27 21:48:21 INFO SparkContext: Job finished: sum at &lt;console&gt;:16, took 0.193513 s</strong></span>
<span class="strong"><strong>computation: Double = 60468.0</strong></span>
</pre></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip06"></a>Tip</h3><p>The<a id="id51" class="indexterm"></a> complete list of transformations and actions <a id="id52" class="indexterm"></a>possible on RDDs as well as a set of more detailed examples are available in the Spark programming guide (located at <a class="ulink" href="http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations" target="_blank">http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations</a>), and the API documentation (the Scala API documentation) is located at <a class="ulink" href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD" target="_blank">http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD</a>).</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl3sec03"></a>Caching RDDs</h4></div></div></div><p>One of the <a id="id53" class="indexterm"></a>most powerful features of Spark is the ability to cache data in memory across a cluster. This is achieved through use of the <code class="literal">cache</code> method on an RDD:</p><div class="informalexample"><pre class="programlisting">rddFromTextFile.cache</pre></div><p>Calling <code class="literal">cache</code> on an RDD tells Spark that the RDD should be kept in memory. The first time an action is called on the RDD that initiates a computation, the data is read from its source and put into memory. Hence, the first time such an operation is called, the time it takes to run the task is partly dependent on the time it takes to read the data from the input source. However, when the data is accessed the next time (for example, in subsequent queries in analytics or iterations in a machine learning model), the data can be read directly from memory, thus avoiding expensive I/O operations and speeding up the computation, in many cases, by a significant factor.</p><p>If we now call the <code class="literal">count</code> or <code class="literal">sum</code> function on our cached RDD, we will see that the RDD is loaded into memory:</p><div class="informalexample"><pre class="programlisting">val aveLengthOfRecordChained = rddFromTextFile.map(line =&gt; line.size).sum / rddFromTextFile.count</pre></div><p>Indeed, in the following output, we see that the dataset was cached in memory on the first call, taking up approximately 62 KB and leaving us with around 270 MB of memory free:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>14/01/30 06:59:27 INFO MemoryStore: ensureFreeSpace(63454) called with curMem=32960, maxMem=311387750</strong></span>
<span class="strong"><strong>14/01/30 06:59:27 INFO MemoryStore: Block rdd_2_0 stored as values to memory (estimated size 62.0 KB, free 296.9 MB)</strong></span>
<span class="strong"><strong>14/01/30 06:59:27 INFO BlockManagerMasterActor$BlockManagerInfo: Added rdd_2_0 in memory on 10.0.0.3:55089 (size: 62.0 KB, free: 296.9 MB)</strong></span>
<span class="strong"><strong>... </strong></span>
</pre></div><p>Now, we will call the same function again:</p><div class="informalexample"><pre class="programlisting">val aveLengthOfRecordChainedFromCached = rddFromTextFile.map(line =&gt; line.size).sum / rddFromTextFile.count</pre></div><p>We will see <a id="id54" class="indexterm"></a>from the console output that the cached data is read directly from memory:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>14/01/30 06:59:34 INFO BlockManager: Found block rdd_2_0 locally</strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip07"></a>Tip</h3><p>Spark also allows more fine-grained control over caching behavior. You can use the <code class="literal">persist</code> method to specify what approach Spark uses to cache data. More information <a id="id55" class="indexterm"></a>on <code class="literal">RDD</code> caching can be found here: <a class="ulink" href="http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence" target="_blank">http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence</a>.</p></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec10"></a>Broadcast variables and accumulators</h3></div></div></div><p>Another core feature of Spark is the ability to create two special types of variables: broadcast variables and accumulators.</p><p>A <span class="strong"><strong>broadcast variable</strong></span> is a <span class="emphasis"><em>read-only</em></span> variable that is made available from the driver program<a id="id56" class="indexterm"></a> that runs the <code class="literal">SparkContext</code> object to the nodes that will execute the computation. This is very useful in applications<a id="id57" class="indexterm"></a> that need to make the same data available to the <a id="id58" class="indexterm"></a>worker nodes in an efficient manner, such <a id="id59" class="indexterm"></a>as machine learning algorithms. Spark makes creating broadcast variables as simple as calling a method on <code class="literal">SparkContext</code> as follows:</p><div class="informalexample"><pre class="programlisting">val broadcastAList = sc.broadcast(List("a", "b", "c", "d", "e"))</pre></div><p>The console output shows that the broadcast variable was stored in memory, taking up approximately 488 bytes, and it also shows that we still have 270 MB available to us:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>14/01/30 07:13:32 INFO MemoryStore: ensureFreeSpace(488) called with curMem=96414, maxMem=311387750</strong></span>
<span class="strong"><strong>14/01/30 07:13:32 INFO MemoryStore: Block broadcast_1 stored as values to memory (estimated size 488.0 B, free 296.9 MB)</strong></span>
<span class="strong"><strong>broadCastAList: org.apache.spark.broadcast.Broadcast[List[String]] = Broadcast(1)</strong></span>
</pre></div><p>A broadcast variable can be accessed from nodes other than the driver program that created it (that is, the worker nodes) by calling <code class="literal">value</code> on the variable:</p><div class="informalexample"><pre class="programlisting">sc.parallelize(List("1", "2", "3")).map(x =&gt; broadcastAList.<span class="strong"><strong>value</strong></span> ++ x).collect</pre></div><p>This code creates a new RDD with three records from a collection (in this case, a Scala <code class="literal">List</code>) of <code class="literal">("1", "2", "3")</code>. In the <code class="literal">map</code> function, it returns a new collection with the relevant <a id="id60" class="indexterm"></a>record from our new RDD appended to the <code class="literal">broadcastAList</code> that is our broadcast variable.</p><p>Notice <a id="id61" class="indexterm"></a>that we used the <code class="literal">collect</code> method in<a id="id62" class="indexterm"></a> the preceding code. This is a Spark <span class="emphasis"><em>action</em></span> that returns <a id="id63" class="indexterm"></a>the entire RDD to the driver as a Scala (or Python or Java) collection.</p><p>We will<a id="id64" class="indexterm"></a> often use <code class="literal">collect</code> when we wish to apply further processing to our results locally within the driver program.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note05"></a>Note</h3><p>Note that <code class="literal">collect</code> should generally only be used in cases where we really want to return the full result set to the driver and perform further processing. If we try to call <code class="literal">collect</code> on a very large dataset, we might run out of memory on the driver and crash our program.</p><p>It is preferable to perform as much heavy-duty processing on our Spark cluster as possible, preventing the driver from becoming a bottleneck. In many cases, however, collecting results to the driver is necessary, such as during iterations in many machine learning models.</p></div><p>On inspecting the result, we will see that for each of the three records in our new RDD, we now have a record that is our original broadcasted <code class="literal">List</code>, with the new element appended to it (that is, there is now either <code class="literal">"1"</code>, <code class="literal">"2"</code>, or <code class="literal">"3"</code> at the end):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>14/01/31 10:15:39 INFO SparkContext: Job finished: collect at &lt;console&gt;:15, took 0.025806 s</strong></span>
<span class="strong"><strong>res6: Array[List[Any]] = Array(List(a, b, c, d, e, 1), List(a, b, c, d, e, 2), List(a, b, c, d, e, 3))</strong></span>
</pre></div><p>An <span class="strong"><strong>accumulator</strong></span> is also a variable that is broadcasted to the worker nodes. The key difference between a broadcast variable and an accumulator is that while the broadcast variable is read-only, the accumulator can be added to. There are limitations to this, that is, in particular, the addition must be an associative operation so that the global accumulated value can be correctly computed in parallel and returned to the driver program. Each worker node can only access and add to its own local accumulator value, and only the driver program can access the global value. Accumulators are also accessed within<a id="id65" class="indexterm"></a> the <a id="id66" class="indexterm"></a>Spark code using the <code class="literal">value</code> method.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip08"></a>Tip</h3><p>For more<a id="id67" class="indexterm"></a> details on broadcast variables and accumulators, see the <span class="emphasis"><em>Shared Variables</em></span> section of the <span class="emphasis"><em>Spark Programming Guide</em></span>: <a class="ulink" href="http://spark.apache.org/docs/latest/programming-guide.html#shared-variables" target="_blank">http://spark.apache.org/docs/latest/programming-guide.html#shared-variables</a>.</p></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec11"></a>The first step to a Spark program in Scala</h2></div></div><hr /></div><p>We will <a id="id68" class="indexterm"></a>now use the ideas we introduced in the previous section to <a id="id69" class="indexterm"></a>write a basic Spark program to manipulate a dataset. We will start with Scala and then write the same program in Java and Python. Our program will be based on exploring some data from an online store, about which users have purchased which products. The data is contained in a <span class="strong"><strong>comma-separated-value</strong></span> (<span class="strong"><strong>CSV</strong></span>) file called <code class="literal">UserPurchaseHistory.csv</code>, and the contents are shown<a id="id70" class="indexterm"></a> in the following snippet. The first column of the CSV is the username, the second column is the product name, and the final column is the price:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>John,iPhone Cover,9.99</strong></span>
<span class="strong"><strong>John,Headphones,5.49</strong></span>
<span class="strong"><strong>Jack,iPhone Cover,9.99</strong></span>
<span class="strong"><strong>Jill,Samsung Galaxy Cover,8.95</strong></span>
<span class="strong"><strong>Bob,iPad Cover,5.49</strong></span>
</pre></div><p>For our Scala program, we need to create two files: our Scala code and our project build configuration file, using the build tool <span class="strong"><strong>Scala Build Tool</strong></span> (<span class="strong"><strong>sbt</strong></span>). For ease of use, we recommend<a id="id71" class="indexterm"></a> that you download the sample project code called <code class="literal">scala-spark-app</code> for this chapter. This code also contains the CSV file under the <code class="literal">data</code> directory. You will need SBT installed on your system in order to run this example program (we use version 0.13.1 at the time of writing this book).</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip09"></a>Tip</h3><p>Setting up SBT is beyond the scope of this book; however, you can find more information at <a class="ulink" href="http://www.scala-sbt.org/release/docs/Getting-Started/Setup.html" target="_blank">http://www.scala-sbt.org/release/docs/Getting-Started/Setup.html</a>.</p></div><p>Our SBT configuration file, <code class="literal">build.sbt</code>, looks like this (note that the empty lines between each line of code are required):</p><div class="informalexample"><pre class="programlisting">name := "scala-spark-app"

version := "1.0"

scalaVersion := "2.10.4"

libraryDependencies += "org.apache.spark" %% "spark-core" % "1.2.0 "</pre></div><p>The last line adds the dependency on Spark to our project.</p><p>Our Scala <a id="id72" class="indexterm"></a>program is contained in the <code class="literal">ScalaApp.scala</code> file. We <a id="id73" class="indexterm"></a>will walk through the program piece by piece. First, we need to import the required Spark classes:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._

/**
 * A simple Spark app in Scala
 */
object ScalaApp {</pre></div><p>In our main method, we need to initialize our <code class="literal">SparkContext</code> object and use this to access our CSV data file with the <code class="literal">textFile</code> method. We will then map the raw text by splitting the string on the delimiter character (a comma in this case) and extracting the relevant records for username, product, and price:</p><div class="informalexample"><pre class="programlisting">  def main(args: Array[String]) {
    val sc = new SparkContext("local[2]", "First Spark App")
    // we take the raw data in CSV format and convert it into a set of records of the form (user, product, price)
    val data = sc.textFile("data/UserPurchaseHistory.csv")
      .map(line =&gt; line.split(","))
      .map(purchaseRecord =&gt; (purchaseRecord(0), purchaseRecord(1), purchaseRecord(2)))</pre></div><p>Now that we have an RDD, where each record is made up of <code class="literal">(user, product, price)</code>, we can compute various interesting metrics for our store, such as the following ones:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The total number of purchases</p></li><li style="list-style-type: disc"><p>The number of unique users who purchased</p></li><li style="list-style-type: disc"><p>Our total revenue</p></li><li style="list-style-type: disc"><p>Our most popular product</p></li></ul></div><p>Let's compute the preceding metrics:</p><div class="informalexample"><pre class="programlisting">    // let's count the number of purchases
    val numPurchases = data.count()
    // let's count how many unique users made purchases
    val uniqueUsers = data.map{ case (user, product, price) =&gt; user }.distinct().count()
    // let's sum up our total revenue
    val totalRevenue = data.map{ case (user, product, price) =&gt; price.toDouble }.sum()
    // let's find our most popular product
    val productsByPopularity = data
      .map{ case (user, product, price) =&gt; (product, 1) }
      .reduceByKey(_ + _)
      .collect()
      .sortBy(-_._2)    
    val mostPopular = productsByPopularity(0)</pre></div><p>This last <a id="id74" class="indexterm"></a>piece of code to compute the most popular product is<a id="id75" class="indexterm"></a> an example of the <span class="emphasis"><em>Map/Reduce</em></span> pattern made popular by Hadoop. First, we mapped our records of <code class="literal">(user, product, price)</code> to the records of <code class="literal">(product, 1)</code>. Then, we performed a <code class="literal">reduceByKey</code> operation, where we summed up the 1s for each unique product.</p><p>Once we have this transformed RDD, which contains the number of purchases for each product, we will call <code class="literal">collect</code>, which returns the results of the computation to the driver program as a local Scala collection. We will then sort these counts locally (note that in practice, if the amount of data is large, we will perform the sorting in parallel, usually with a Spark operation such as <code class="literal">sortByKey</code>).</p><p>Finally, we will print out the results of our computations to the console:</p><div class="informalexample"><pre class="programlisting">    println("Total purchases: " + numPurchases)
    println("Unique users: " + uniqueUsers)
    println("Total revenue: " + totalRevenue)
    println("Most popular product: %s with %d purchases".format(mostPopular._1, mostPopular._2))
  }
}</pre></div><p>We can run this program by running <code class="literal">sbt run</code> in the project's base directory or by running the program in your Scala IDE if you are using one. The output should look similar to the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>[info] Compiling 1 Scala source to ...</strong></span>
<span class="strong"><strong>[info] Running ScalaApp</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>14/01/30 10:54:40 INFO spark.SparkContext: Job finished: collect at ScalaApp.scala:25, took 0.045181 s</strong></span>
<span class="strong"><strong>Total purchases: 5</strong></span>
<span class="strong"><strong>Unique users: 4</strong></span>
<span class="strong"><strong>Total revenue: 39.91</strong></span>
<span class="strong"><strong>Most popular product: iPhone Cover with 2 purchases</strong></span>
</pre></div><p>We<a id="id76" class="indexterm"></a> can see that we have five purchases from four different <a id="id77" class="indexterm"></a>users with a total revenue of 39.91. Our most popular product is an iPhone cover with 2 purchases.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec12"></a>The first step to a Spark program in Java</h2></div></div><hr /></div><p>The Java <a id="id78" class="indexterm"></a>API is very similar in principle to the Scala API. However, while Scala can call the Java code quite easily, in some cases, it is not possible to<a id="id79" class="indexterm"></a> call the Scala code from Java. This is particularly the case when such Scala code makes use of certain Scala features such as implicit conversions, default parameters, and the Scala reflection API.</p><p>Spark makes heavy use of these features in general, so it is necessary to have a separate API specifically for Java that includes Java versions of the common classes. Hence, <code class="literal">SparkContext</code> becomes <code class="literal">JavaSparkContext</code>, and <code class="literal">RDD</code> becomes <code class="literal">JavaRDD</code>.</p><p>Java versions prior to version 8 do not support anonymous functions and do not have succinct syntax for functional-style programming, so functions in the Spark Java API must implement a <code class="literal">WrappedFunction</code> interface with the <code class="literal">call</code> method signature. While it is significantly more verbose, we will often create one-off anonymous classes to pass to our Spark operations, which implement this interface and the <code class="literal">call</code> method, to achieve much the same effect as anonymous functions in Scala.</p><p>Spark provides support for Java 8's anonymous function (or <span class="emphasis"><em>lambda</em></span>) syntax. Using this syntax makes a Spark program written in Java 8 look very close to the equivalent Scala program.</p><p>In Scala, an RDD of key/value pairs provides special operators (such as <code class="literal">reduceByKey</code> and <code class="literal">saveAsSequenceFile</code>, for example) that are accessed automatically via implicit conversions. In Java, special types of <code class="literal">JavaRDD</code> classes are required in order to access similar functions. These include <code class="literal">JavaPairRDD</code> to work with key/value pairs and <code class="literal">JavaDoubleRDD</code> to work with numerical records.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip10"></a>Tip</h3><p>In this section, we covered the standard Java API syntax. For more details and examples related to working RDDs in Java as well as the Java 8 lambda syntax, see the Java sections of the <span class="emphasis"><em>Spark Programming Guide</em></span> found at <a class="ulink" href="http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations" target="_blank">http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations</a>.</p></div><p>We will<a id="id80" class="indexterm"></a> see examples of most of these differences in the following<a id="id81" class="indexterm"></a> Java program, which is included in the example code of this chapter in the directory named <code class="literal">java-spark-app</code>. The code directory also contains the CSV data file under the <code class="literal">data</code> subdirectory.</p><p>We will build and run this project with the Maven build tool, which we assume you have installed on your system.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip11"></a>Tip</h3><p>Installing and setting up Maven is beyond the scope of this book. Usually, Maven can easily be installed using the package manager on your Linux system or HomeBrew or MacPorts on Mac OS X.</p><p>Detailed installation instructions can be found here: <a class="ulink" href="http://maven.apache.org/download.cgi" target="_blank">http://maven.apache.org/download.cgi</a>.</p></div><p>The project contains a Java file called <code class="literal">JavaApp.java</code>, which contains our program code:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.DoubleFunction;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.api.java.function.Function2;
import org.apache.spark.api.java.function.PairFunction;
import scala.Tuple2;

import java.util.Collections;
import java.util.Comparator;
import java.util.List;

/**
 * A simple Spark app in Java
 */
public class JavaApp {

  public static void main(String[] args) {</pre></div><p>As in our Scala example, we first need to initialize our context. Notice that we will use the <code class="literal">JavaSparkContext</code> class here instead of the <code class="literal">SparkContext</code> class that we used earlier. We<a id="id82" class="indexterm"></a> will use the <code class="literal">JavaSparkContext</code> class in the same way <a id="id83" class="indexterm"></a>to access our data using <code class="literal">textFile</code> and then split each row into the required fields. Note how we used an anonymous class to define a split function that performs the string processing, in the highlighted code:</p><div class="informalexample"><pre class="programlisting">    JavaSparkContext sc = new JavaSparkContext("local[2]", "First Spark App");
    // we take the raw data in CSV format and convert it into a set of records of the form (user, product, price)
    JavaRDD&lt;String[]&gt; data = sc.textFile("data/UserPurchaseHistory.csv")
    .map(<span class="strong"><strong>new Function&lt;String, String[]&gt;() {</strong></span>
<span class="strong"><strong>      @Override</strong></span>
<span class="strong"><strong>      public String[] call(String s) throws Exception {</strong></span>
<span class="strong"><strong>        return s.split(",");</strong></span>
<span class="strong"><strong>      }</strong></span>
<span class="strong"><strong>    }</strong></span>);</pre></div><p>Now, we can compute the same metrics as we did in our Scala example. Note how some methods are the same (for example, <code class="literal">distinct</code> and <code class="literal">count</code>) for the Java and Scala APIs. Also note the use of anonymous classes that we pass to the <code class="literal">map</code> function. This code is highlighted here:</p><div class="informalexample"><pre class="programlisting">    // let's count the number of purchases
    long numPurchases = data.count();
    // let's count how many unique users made purchases
    long uniqueUsers = data.map(<span class="strong"><strong>new Function&lt;String[], String&gt;() {</strong></span>
<span class="strong"><strong>      @Override</strong></span>
<span class="strong"><strong>      public String call(String[] strings) throws Exception {</strong></span>
<span class="strong"><strong>        return strings[0];</strong></span>
<span class="strong"><strong>      }</strong></span>
<span class="strong"><strong>    }</strong></span>).distinct().count();
    // let's sum up our total revenue
    double totalRevenue = data.map(<span class="strong"><strong>new DoubleFunction&lt;String[]&gt;() {</strong></span>
<span class="strong"><strong>      @Override</strong></span>
<span class="strong"><strong>      public Double call(String[] strings) throws Exception {</strong></span>
<span class="strong"><strong>        return Double.parseDouble(strings[2]);</strong></span>
<span class="strong"><strong>      }</strong></span>
<span class="strong"><strong>    }</strong></span>).sum();</pre></div><p>In the following lines of code, we can see that the approach to compute the most popular product is the same as that in the Scala example. The extra code might seem complex, but it is mostly <a id="id84" class="indexterm"></a>related to the Java code required to create the anonymous<a id="id85" class="indexterm"></a> functions (which we have highlighted here). The actual functionality is the same:</p><div class="informalexample"><pre class="programlisting">    // let's find our most popular product
    // first we map the data to records of (product, 1)using a PairFunction
    // and the Tuple2 class.
    // then we call a reduceByKey operation with a Function2, which is essentially the sum function
    List&lt;Tuple2&lt;String, Integer&gt;&gt; pairs = data.map(<span class="strong"><strong>new PairFunction&lt;String[], String, Integer&gt;() {</strong></span>
<span class="strong"><strong>      @Override</strong></span>
<span class="strong"><strong>      public Tuple2&lt;String, Integer&gt; call(String[] strings)throws Exception {</strong></span>
<span class="strong"><strong>        return new Tuple2(strings[1], 1);</strong></span>
<span class="strong"><strong>      }</strong></span>
<span class="strong"><strong>    }</strong></span>).reduceByKey(<span class="strong"><strong>new Function2&lt;Integer, Integer, Integer&gt;() {</strong></span>
<span class="strong"><strong>      @Override</strong></span>
<span class="strong"><strong>      public Integer call(Integer integer, Integer integer2)throws Exception {</strong></span>
<span class="strong"><strong>        return integer + integer2;</strong></span>
<span class="strong"><strong>      }</strong></span>
<span class="strong"><strong>    }</strong></span>).collect();
    // finally we sort the result. Note we need to create a Comparator function,
    // that reverses the sort order.
    Collections.sort(pairs, new Comparator&lt;Tuple2&lt;String,Integer&gt;&gt;() {
      @Override
      public int compare(Tuple2&lt;String, Integer&gt; o1,Tuple2&lt;String, Integer&gt; o2) {
        return -(o1._2() - o2._2());
      }
    });
    String mostPopular = pairs.get(0)._1();
    int purchases = pairs.get(0)._2();
    System.out.println("Total purchases: " + numPurchases);
    System.out.println("Unique users: " + uniqueUsers);
    System.out.println("Total revenue: " + totalRevenue);
    System.out.println(String.format("Most popular product:%s with %d purchases", mostPopular, purchases));
  }
}</pre></div><p>As can be seen, the general structure is similar to the Scala version, apart from the extra boilerplate code to declare variables and functions via anonymous inner classes. It is a good exercise to work through both examples and compare the same lines of Scala code to those in <a id="id86" class="indexterm"></a>Java to understand how the same result is achieved <a id="id87" class="indexterm"></a>in each language.</p><p>This program can be run with the following command executed from the project's base directory:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;mvn exec:java -Dexec.mainClass="JavaApp"</strong></span>
</pre></div><p>You will see output that looks very similar to the Scala version, with the results of the computation identical:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>14/01/30 17:02:43 INFO spark.SparkContext: Job finished: collect at JavaApp.java:46, took 0.039167 s</strong></span>
<span class="strong"><strong>Total purchases: 5</strong></span>
<span class="strong"><strong>Unique users: 4</strong></span>
<span class="strong"><strong>Total revenue: 39.91</strong></span>
<span class="strong"><strong>Most popular product: iPhone Cover with 2 purchases</strong></span>
</pre></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec13"></a>The first step to a Spark program in Python</h2></div></div><hr /></div><p>Spark's Python<a id="id88" class="indexterm"></a> API exposes virtually all the functionalities<a id="id89" class="indexterm"></a> of Spark's Scala API in the Python language. There are some features that are not yet supported (for example, graph processing with GraphX and a few API methods here and there). See the Python section of the <span class="emphasis"><em>Spark Programming Guide</em></span> (<a class="ulink" href="http://spark.apache.org/docs/latest/programming-guide.html" target="_blank">http://spark.apache.org/docs/latest/programming-guide.html</a>) for more details.</p><p>Following <a id="id90" class="indexterm"></a>on from the preceding examples, we will now write a Python version. We assume that you have Python version 2.6 and higher installed on your system (for example, most Linux and Mac OS X systems come with Python preinstalled).</p><p>The example program is included in the sample code for this chapter, in the directory named <code class="literal">python-spark-app</code>, which also contains the CSV data file under the <code class="literal">data</code> subdirectory. The project contains a script, <code class="literal">pythonapp.py</code>, provided here:</p><div class="informalexample"><pre class="programlisting">"""A simple Spark app in Python"""
from pyspark import SparkContext

sc = SparkContext("local[2]", "First Spark App")
# we take the raw data in CSV format and convert it into a set of records of the form (user, product, price)
data = sc.textFile("data/UserPurchaseHistory.csv").map(lambda line: line.split(",")).map(lambda record: (record[0], record[1], record[2]))
# let's count the number of purchases
numPurchases = data.count()
# let's count how many unique users made purchases
uniqueUsers = data.map(lambda record: record[0]).distinct().count()
# let's sum up our total revenue
totalRevenue = data.map(lambda record: float(record[2])).sum()
# let's find our most popular product
<span class="strong"><strong>products = data.map(lambda record: (record[1], 1.0)).reduceByKey(lambda a, b: a + b).collect()</strong></span>
mostPopular = sorted(products, key=lambda x: x[1], reverse=True)[0]

print "Total purchases: %d" % numPurchases
print "Unique users: %d" % uniqueUsers
print "Total revenue: %2.2f" % totalRevenue
print "Most popular product: %s with %d purchases" % (mostPopular[0], mostPopular[1])</pre></div><p>If you <a id="id91" class="indexterm"></a>compare the Scala and Python versions of our program, you <a id="id92" class="indexterm"></a>will see that generally, the syntax looks very similar. One key difference is how we express anonymous functions (also called <code class="literal">lambda</code> functions; hence, the use of this keyword for the Python syntax). In Scala, we've seen that an anonymous function mapping an input <code class="literal">x</code> to an output <code class="literal">y</code> is expressed as <code class="literal">x =&gt; y</code>, while in Python, it is <code class="literal">lambda x: y</code>. In the highlighted line in the preceding code, we are applying an anonymous function that maps two inputs, <code class="literal">a</code> and <code class="literal">b</code>, generally of the same type, to an output. In this case, the function that we apply is the <span class="emphasis"><em>plus</em></span> function; hence, <code class="literal">lambda a, b: a + b</code>.</p><p>The best way to run the script is to run the following command from the base directory of the sample project:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;$SPARK_HOME/bin/spark-submit pythonapp.py</strong></span>
</pre></div><p>Here, the <code class="literal">SPARK_HOME</code> variable should be replaced with the path of the directory in which you originally unpacked the Spark prebuilt binary package at the start of this chapter.</p><p>Upon running the script, you should see output similar to that of the Scala and Java examples, with<a id="id93" class="indexterm"></a> the results of our computation being<a id="id94" class="indexterm"></a> the same:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>14/01/30 11:43:47 INFO SparkContext: Job finished: collect at pythonapp.py:14, took 0.050251 s</strong></span>
<span class="strong"><strong>Total purchases: 5</strong></span>
<span class="strong"><strong>Unique users: 4</strong></span>
<span class="strong"><strong>Total revenue: 39.91</strong></span>
<span class="strong"><strong>Most popular product: iPhone Cover with 2 purchases</strong></span>
</pre></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec14"></a>Getting Spark running on Amazon EC2</h2></div></div><hr /></div><p>The <a id="id95" class="indexterm"></a>Spark project provides scripts to run a Spark <a id="id96" class="indexterm"></a>cluster in the cloud on Amazon's EC2 service. These scripts are located in the <code class="literal">ec2</code> directory. You can run the <code class="literal">spark-ec2</code> script contained in this directory with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;./ec2/spark-ec2 </strong></span>
</pre></div><p>Running it in this way without an argument will show the help output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Usage: spark-ec2 [options] &lt;action&gt; &lt;cluster_name&gt;</strong></span>
<span class="strong"><strong>&lt;action&gt; can be: launch, destroy, login, stop, start, get-master</strong></span>

<span class="strong"><strong>Options:</strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><p>Before creating a Spark EC2 cluster, you will need to ensure you have an Amazon account.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip12"></a>Tip</h3><p>If you don't<a id="id97" class="indexterm"></a> have an Amazon Web Services account, you can sign up at <a class="ulink" href="http://aws.amazon.com/" target="_blank">http://aws.amazon.com/</a>.</p><p>The AWS console<a id="id98" class="indexterm"></a> is available at <a class="ulink" href="http://aws.amazon.com/console/" target="_blank">http://aws.amazon.com/console/</a>.</p></div><p>You will also<a id="id99" class="indexterm"></a> need to create an Amazon EC2 key pair and retrieve the relevant security credentials. The Spark documentation for EC2 (available at <a class="ulink" href="http://spark.apache.org/docs/latest/ec2-scripts.html" target="_blank">http://spark.apache.org/docs/latest/ec2-scripts.html</a>) explains the requirements:</p><p><span class="emphasis"><em>Create an Amazon EC2 key pair for yourself. This can be done by logging into your Amazon Web Services account through the AWS console, clicking on</em></span> <span class="strong"><strong>Key Pairs</strong></span> <span class="emphasis"><em>on the left sidebar, and creating and downloading a key. Make sure that you set the permissions for the private key file to 600 (that is, only you can read and write it) so that <code class="literal">ssh</code> will work.</em></span></p><p><span class="emphasis"><em>Whenever you want to use the</em></span> <code class="literal">spark-ec2</code> <span class="emphasis"><em>script, set the environment variables</em></span> <code class="literal">AWS_ACCESS_KEY_ID</code> <span class="emphasis"><em>and</em></span> <code class="literal">AWS_SECRET_ACCESS_KEY</code> <span class="emphasis"><em>to your Amazon EC2 access key ID and secret access key, respectively. These can be obtained from the AWS homepage by clicking</em></span> <span class="strong"><strong>Account</strong></span> | <span class="strong"><strong>Security Credentials</strong></span> | <span class="strong"><strong>Access Credentials</strong></span>.</p><p>When <a id="id100" class="indexterm"></a>creating a key pair, choose a name that is easy to<a id="id101" class="indexterm"></a> remember. We will simply use <code class="literal">spark</code> for the key pair name. The key pair file itself will be called <code class="literal">spark.pem</code>. As mentioned earlier, ensure that the key pair file permissions are set appropriately and that the environment variables for the AWS credentials are exported using the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;chmod 600 spark.pem</strong></span>
<span class="strong"><strong>&gt;export AWS_ACCESS_KEY_ID="..."</strong></span>
<span class="strong"><strong>&gt;export AWS_SECRET_ACCESS_KEY="..."</strong></span>
</pre></div><p>You should also be careful to keep your downloaded key pair file safe and not lose it, as it can only be downloaded once when it is created!</p><p>Note that launching an Amazon EC2 cluster in the following section will <span class="emphasis"><em>incur costs</em></span> to your AWS account.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl2sec11"></a>Launching an EC2 Spark cluster</h3></div></div></div><p>We're<a id="id102" class="indexterm"></a> now ready to launch a small Spark cluster<a id="id103" class="indexterm"></a> by changing into the <code class="literal">ec2</code> directory and then running the cluster launch command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;cd ec2</strong></span>
<span class="strong"><strong>&gt;./spark-ec2 -k spark -i spark.pem -s 1 â€“-instance-type m3.medium --hadoop-major-version 2 launch test-cluster</strong></span>
</pre></div><p>This will launch a new Spark cluster called <code class="literal">test-cluster</code> with one master and one slave node of instance type <code class="literal">m3.medium</code>. This cluster will be launched with a Spark version built for Hadoop 2. The key pair name we used is <code class="literal">spark</code>, and the key pair file is <code class="literal">spark.pem</code> (if you gave the files different names or have an existing AWS key pair, use that name instead).</p><p>It might take quite a while for the cluster to fully launch and initialize. You should see something like this screenshot immediately after running the launch command:</p><div class="mediaobject"><img src="graphics/8519OS_01_03.jpg" /></div><p>If the cluster has launched successfully, you should eventually see the console output similar to the following screenshot:</p><div class="mediaobject"><img src="graphics/8519OS_01_04.jpg" /></div><p>To test whether we can connect to our new cluster, we can run the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;ssh -i spark.pem root@ec2-54-227-127-14.compute-1.amazonaws.com</strong></span>
</pre></div><p>Remember to replace the public domain name of the master node (the address after <code class="literal">root@</code> in the preceding command) with the correct Amazon EC2 public domain name that will be shown in your console output after launching the cluster.</p><p>You can also retrieve your cluster's master public domain name by running this line of code:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;./spark-ec2 â€“i spark.pem get-master test-cluster</strong></span>
</pre></div><p>After <a id="id104" class="indexterm"></a>successfully running the <code class="literal">ssh</code> command, you <a id="id105" class="indexterm"></a>will be connected to your Spark master node in EC2, and your terminal output should match the following screenshot:</p><div class="mediaobject"><img src="graphics/8519OS_01_05.jpg" /></div><p>We can test whether our cluster is correctly set up with Spark by changing into the Spark directory and running an example in the local mode:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;cd spark</strong></span>
<span class="strong"><strong>&gt;MASTER=local[2] ./bin/run-example SparkPi</strong></span>
</pre></div><p>You should see output similar to running the same command on your local computer:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>14/01/30 20:20:21 INFO SparkContext: Job finished: reduce at SparkPi.scala:35, took 0.864044012 s</strong></span>
<span class="strong"><strong>Pi is roughly 3.14032</strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><p>Now that<a id="id106" class="indexterm"></a> we have an actual cluster with multiple <a id="id107" class="indexterm"></a>nodes, we can test Spark in the cluster mode. We can run the same example on the cluster, using our 1 slave node, by passing in the master URL instead of the local version:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;MASTER=spark://ec2-54-227-127-14.compute-1.amazonaws.com:7077 ./bin/run-example SparkPi </strong></span>
</pre></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip13"></a>Tip</h3><p>Note that you will need to substitute the preceding master domain name with the correct domain name for your specific cluster.</p></div><p>Again, the output should be similar to running the example locally; however, the log messages will show that your driver program has connected to the Spark master:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>14/01/30 20:26:17 INFO client.Client$ClientActor: Connecting to master spark://ec2-54-220-189-136.eu-west-1.compute.amazonaws.com:7077</strong></span>
<span class="strong"><strong>14/01/30 20:26:17 INFO cluster.SparkDeploySchedulerBackend: Connected to Spark cluster with app ID app-20140130202617-0001</strong></span>
<span class="strong"><strong>14/01/30 20:26:17 INFO client.Client$ClientActor: Executor added: app-20140130202617-0001/0 on worker-20140130201049-ip-10-34-137-45.eu-west-1.compute.internal-57119 (ip-10-34-137-45.eu-west-1.compute.internal:57119) with 1 cores</strong></span>
<span class="strong"><strong>14/01/30 20:26:17 INFO cluster.SparkDeploySchedulerBackend: Granted executor ID app-20140130202617-0001/0 on hostPort ip-10-34-137-45.eu-west-1.compute.internal:57119 with 1 cores, 2.4 GB RAM</strong></span>
<span class="strong"><strong>14/01/30 20:26:17 INFO client.Client$ClientActor: Executor updated: app-20140130202617-0001/0 is now RUNNING</strong></span>
<span class="strong"><strong>14/01/30 20:26:18 INFO spark.SparkContext: Starting job: reduce at SparkPi.scala:39</strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><p>Feel <a id="id108" class="indexterm"></a>free to experiment with your cluster. Try <a id="id109" class="indexterm"></a>out the interactive console in Scala, for example:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; ./bin/spark-shell --master spark://ec2-54-227-127-14.compute-1.amazonaws.com:7077</strong></span>
</pre></div><p>Once you've finished, type <code class="literal">exit</code> to leave the console. You can also try the PySpark console by running the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; ./bin/pyspark --master spark://ec2-54-227-127-14.compute-1.amazonaws.com:7077</strong></span>
</pre></div><p>You can use the Spark Master web interface to see the applications registered with the master. To load the Master Web UI, navigate to <code class="literal">ec2-54-227-127-14.compute-1.amazonaws.com:8080</code> (again, remember to replace this domain name with your own master domain name). You should see something similar to the following screenshot showing the example you ran as well as the two console applications you launched:</p><div class="mediaobject"><img src="graphics/8519OS_01_06.jpg" /></div><p>Remember that <span class="emphasis"><em>you will be charged by Amazon</em></span> for usage of the cluster. Don't forget to stop <a id="id110" class="indexterm"></a>or terminate this test cluster once you're<a id="id111" class="indexterm"></a> done with it. To do this, you can first exit the <code class="literal">ssh</code> session by typing <code class="literal">exit</code> to return to your own local system and then, run the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;./ec2/spark-ec2 -k spark -i spark.pem destroy test-cluster</strong></span>
</pre></div><p>You should see the following output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Are you sure you want to destroy the cluster test-cluster?</strong></span>
<span class="strong"><strong>The following instances will be terminated:</strong></span>
<span class="strong"><strong>Searching for existing cluster test-cluster...</strong></span>
<span class="strong"><strong>Found 1 master(s), 1 slaves</strong></span>
<span class="strong"><strong>&gt; ec2-54-227-127-14.compute-1.amazonaws.com</strong></span>
<span class="strong"><strong>&gt; ec2-54-91-61-225.compute-1.amazonaws.com</strong></span>
<span class="strong"><strong>ALL DATA ON ALL NODES WILL BE LOST!!</strong></span>
<span class="strong"><strong>Destroy cluster test-cluster (y/N): y</strong></span>
<span class="strong"><strong>Terminating master...</strong></span>
<span class="strong"><strong>Terminating slaves...</strong></span>
</pre></div><p>Hit <span class="emphasis"><em>Y</em></span> and then <span class="emphasis"><em>Enter</em></span> to destroy the cluster.</p><p>Congratulations! You've just set up a Spark cluster in the cloud, run a fully parallel example program on this cluster, and terminated it. If you would like to try out any of the example code in the subsequent chapters (or your own Spark programs) on a cluster, feel free to experiment with the Spark EC2 scripts and launch a cluster of your chosen size and instance profile (just be mindful of the costs and remember to shut it down when you're done!).</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch01lvl1sec15"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we covered how to set up Spark locally on our own computer as well as in the cloud as a cluster running on Amazon EC2. You learned the basics of Spark's programming model and API using the interactive Scala console, and we wrote the same basic Spark program in Scala, Java, and Python.</p><p>In the next chapter, we will consider how to go about using Spark to create a machine learning system.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch02"></a>ChapterÂ 2.Â Designing a Machine Learning System</h2></div></div></div><p>In this chapter, we will design a high-level architecture for an intelligent, distributed machine learning system that uses Spark as its core computation engine. The problem we will focus on will be taking the existing architecture for a web-based business and redesigning it to use automated machine learning systems to power key areas of the business. In this chapter, we will:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Introduce our hypothetical business scenario</p></li><li style="list-style-type: disc"><p>Provide an overview of the current architecture</p></li><li style="list-style-type: disc"><p>Explore various ways in which machine learning systems can enhance or replace certain business functions</p></li><li style="list-style-type: disc"><p>Provide a new architecture based on these ideas</p></li></ul></div><p>A modern large-scale data environment<a id="id112" class="indexterm"></a> includes the following requirements:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>It must integrate with other components of the system, especially with data collection and storage systems, analytics and reporting, and frontend applications.</p></li><li style="list-style-type: disc"><p>It should be easily scalable and independent of the rest of the architecture. Ideally, this should be in the form of horizontal as well as vertical scalability.</p></li><li style="list-style-type: disc"><p>It should allow efficient computation in respect of the type of workload in mind, that is machine learning and iterative analytics applications.</p></li><li style="list-style-type: disc"><p>If possible, it should support both batch and real-time workloads.</p></li></ul></div><p>As a framework, Spark<a id="id113" class="indexterm"></a> meets these criteria. However, we must ensure that the machine learning systems designed on Spark also meet these criteria. There is no good in implementing an algorithm that ends up having bottlenecks that cause our system to fail in terms of one or more of these requirements.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec16"></a>Introducing MovieStream</h2></div></div><hr /></div><p>To better illustrate the design of our architecture, we will introduce a practical scenario. Let's assume that we have just been appointed to head the data science team of MovieStream, a fictitious Internet business that streams movies and television shows to its users.</p><p>MovieStream <a id="id114" class="indexterm"></a>is growing rapidly, adding both users and titles to its catalogue. The current MovieStream system is outlined in the following diagram:</p><div class="mediaobject"><img src="graphics/8519OS_02_01.jpg" /><div class="caption"><p>MovieStream's current architecture</p></div></div><p>As we can see in the preceding diagram, currently, MovieStream's content editorial team is responsible for deciding which movies and shows are promoted and shown on the various parts of the site. They are also responsible for creating the content for MovieStream's bulk marketing campaigns, which include e-mail and other direct marketing channels. Currently, MovieStream collects basic data on what titles are viewed by users on an aggregate basis and has access to some demographic data collected from users when they sign up to the service. In addition, they have access to some basic metadata about the titles in their catalogue.</p><p>The MovieStream<a id="id115" class="indexterm"></a> team is stretched thin due to their rapid growth, and they can't keep up with the number of new releases and the growing activity of their users. The CEO of MovieStream has heard a lot about big data, machine learning, and artificial intelligence, and would like us to build a machine learning system for MovieStream that can handle many of the functions currently handled by the content team in an automated manner.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec17"></a>Business use cases for a machine learning system</h2></div></div><hr /></div><p>Perhaps the first question we <a id="id116" class="indexterm"></a>should answer is, "Why use machine<a id="id117" class="indexterm"></a> learning at all?" Why doesn't MovieStream simply continue with human-driven decisions? There are many reasons to use machine learning (and certainly some reasons not to), but the most important ones are mentioned here:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The scale of data involved means that full human involvement quickly becomes infeasible as MovieStream grows</p></li><li style="list-style-type: disc"><p>Model-driven approaches such as machine learning and statistics can often benefit from uncovering patterns that cannot be seen by humans (due to the size and complexity of the datasets)</p></li><li style="list-style-type: disc"><p>Model-driven approaches can avoid human and emotional biases (as long as the correct processes are carefully applied)</p></li></ul></div><p>However, there is no reason why both model-driven and human-driven processes and decision making cannot coexist. For example, many machine learning systems rely on receiving labeled data in order to train models. Often, labeling such data is costly, time consuming, and requires human input. A good example of this is classifying textual data into categories or assigning a sentiment indicator to the text. Many real-world systems use some form of human-driven system to generate labels for such data (or at least part of it) to provide training data to models. These models are then used to make predictions in the live system at a larger scale.</p><p>In the context of MovieStream, we need not fear that our machine learning system will make the content team redundant. Indeed, we will see that our aim is to lift the burden of time-consuming tasks where machine learning might be able to perform better while providing tools to allow the team to better understand the users and content. This might, for example, help them in selecting which new content to acquire for the catalogue (which involves a significant amount of cost and is therefore a critical aspect of the business).</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec12"></a>Personalization</h3></div></div></div><p>Perhaps one of the most important <a id="id118" class="indexterm"></a>potential<a id="id119" class="indexterm"></a> applications of machine learning in MovieStream's business is personalization. Generally speaking, personalization refers to adapting the experience of a user and the content presented to them based on various factors, which might include user behavior data as well as external factors.</p><p><span class="strong"><strong>Recommendations</strong></span><a id="id120" class="indexterm"></a> are essentially<a id="id121" class="indexterm"></a> a subset of personalization. Recommendation generally refers to presenting a user with a list of items that we hope the user will be interested in. Recommendations might be used in web pages (for example, recommending related products), via e-mails or other direct marketing channels, via mobile apps, and so on.</p><p>Personalization is very similar to recommendations, but while recommendations are usually focused on an <span class="emphasis"><em>explicit</em></span> presentation of products or content to the user, personalization is more generic and, often, more <span class="emphasis"><em>implicit</em></span>. For example, applying personalization to search on the MovieStream site might allow us to adapt the search results for a given user, based on the data available about that user. This might include recommendation-based data (in the case of a search for products or content) but might also include various other factors such as geolocation and past search history. It might not be apparent to the user that the search results are adapted to their specific profile; this is why personalization tends to be more implicit.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec13"></a>Targeted marketing and customer segmentation</h3></div></div></div><p>In a manner similar to<a id="id122" class="indexterm"></a> recommendations, targeted marketing<a id="id123" class="indexterm"></a> uses a model to select what to target at users. While generally recommendations and personalization are focused on a one-to-one situation, segmentation <a id="id124" class="indexterm"></a>approaches might try to assign users into groups based on characteristics and, possibly, behavioral data. The approach might be fairly simple or might involve a machine learning model such as clustering. Either way, the result is a set of segment assignments that might allow us to understand the broad characteristics of each group of users, what makes them similar to each other within a group, and what makes them different from others in different groups.</p><p>This could help MovieStream to better understand the drivers of user behavior and might also allow a broader targeting approach where groups are targeted as opposed to (or more likely, in addition to) direct one-to-one targeting with personalization.</p><p>These methods can<a id="id125" class="indexterm"></a> also help when we don't necessarily have<a id="id126" class="indexterm"></a> labeled data available (as is the case with certain user and content profile data) but we still wish to perform more focused targeting than a complete <span class="emphasis"><em>one-size-fits-all</em></span> approach.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec14"></a>Predictive modeling and analytics</h3></div></div></div><p>A third area where machine learning <a id="id127" class="indexterm"></a>can be applied is in predictive analytics. This is a very broad term, and in some ways, it encompasses recommendations, personalization, and targeting too. In this context, since recommendations and segmentation are somewhat distinct, we use the term <span class="strong"><strong>predictive modeling</strong></span><a id="id128" class="indexterm"></a> to refer to other models that seek to make predictions. An example of this can be a model to predict the potential viewing activity and revenue of new titles before any data is available on how popular the title might be. MovieStream can use past activity and revenue data, together with content attributes, to create a <span class="strong"><strong>regression model</strong></span><a id="id129" class="indexterm"></a> that can be used to make predictions for brand new titles.</p><p>As another example, we can use a <span class="strong"><strong>classification model</strong></span><a id="id130" class="indexterm"></a> to automatically assign tags, keywords, or categories to new titles for which we only have partial data.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec18"></a>Types of machine learning models</h2></div></div><hr /></div><p>While we have <a id="id131" class="indexterm"></a>highlighted a few use cases for machine learning in the context of the preceding MovieStream example, there are many other examples, some of which we will touch on in the relevant chapters when we introduce each machine learning task.</p><p>However, we can broadly divide the preceding use cases and methods into two categories of machine learning:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><span class="strong"><strong>Supervised learning</strong></span>: These <a id="id132" class="indexterm"></a>types of models use <span class="emphasis"><em>labeled</em></span> data to learn. Recommendation<a id="id133" class="indexterm"></a> engines, regression, and classification are examples of supervised learning methods. The labels in these models can be user-movie ratings (for recommendation), movie tags (in the case of the preceding classification example), or revenue figures (for regression). We will cover supervised learning models in <a class="link" href="#" linkend="ch04">Chapter 4</a>, <span class="emphasis"><em>Building a Recommendation Engine with Spark</em></span>, <a class="link" href="#" linkend="ch05">Chapter 5</a>, <span class="emphasis"><em>Building a Classification Model with Spark</em></span>, and <a class="link" href="#" linkend="ch06">Chapter 6</a>, <span class="emphasis"><em>Building a Regression Model with Spark</em></span>.</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Unsupervised learning</strong></span>: When a <a id="id134" class="indexterm"></a>model does not require labeled data, we refer to <a id="id135" class="indexterm"></a>unsupervised learning. These types of models try to learn or extract some underlying structure in the data or reduce the data down to its most important features. Clustering, dimensionality reduction, and some forms of feature extraction, such as text processing, are all unsupervised techniques and will be dealt with in <a class="link" href="#" linkend="ch07">Chapter 7</a>, <span class="emphasis"><em>Building a Clustering Model with Spark</em></span>, <a class="link" href="#" linkend="ch08">Chapter 8</a>, <span class="emphasis"><em>Dimensionality Reduction with Spark</em></span>, and <a class="link" href="#" linkend="ch09">Chapter 9</a>, <span class="emphasis"><em>Advanced Text Processing with Spark</em></span>.</p></li></ul></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec19"></a>The components of a data-driven machine learning system</h2></div></div><hr /></div><p>The high-level components of<a id="id136" class="indexterm"></a> our machine <a id="id137" class="indexterm"></a>learning system are outlined in the following diagram. This diagram illustrates the machine learning pipeline from which we obtain data and in which we store data. We then transform it into a form that is usable as input to a machine learning model; train, test, and refine our model; and then, deploy the final model to our production system. The process is then repeated as new data is generated.</p><div class="mediaobject"><img src="graphics/8519OS_02_02.jpg" /><div class="caption"><p>A general machine learning pipeline</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec15"></a>Data ingestion and storage</h3></div></div></div><p>The first step in our machine learning pipeline will be taking in the data that we require for training our models. Like many other businesses, MovieStream's data is typically generated by user <a id="id138" class="indexterm"></a>activity, other systems (this is commonly referred to as machine-generated data), and external sources (for example, the time of day and weather during a particular user's visit to the site).</p><p>This<a id="id139" class="indexterm"></a> data can be ingested in <a id="id140" class="indexterm"></a>various ways, for example, gathering user activity data from browser and mobile application event logs or accessing external web APIs to collect data on geolocation or weather.</p><p>Once the collection mechanisms are in place, the data usually needs to be stored. This includes the raw data, data resulting from intermediate processing, and final model results to be used in production.</p><p>Data storage<a id="id141" class="indexterm"></a> can be complex and <a id="id142" class="indexterm"></a>involve a wide variety of systems, including HDFS, Amazon S3, and other filesystems; SQL databases such as MySQL or PostgreSQL; distributed NoSQL data stores such as HBase, Cassandra, and DynamoDB; and search engines such as Solr or Elasticsearch to stream data systems such as Kafka, Flume, or Amazon Kinesis.</p><p>For the purposes of this book, we will assume that the relevant data is available to us, so we will focus on the processing and modeling steps in the following pipeline.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec16"></a>Data cleansing and transformation</h3></div></div></div><p>The majority of machine<a id="id143" class="indexterm"></a> learning models <a id="id144" class="indexterm"></a>operate on features, which are typically numerical representations of the input variables that will be used for the model.</p><p>While we might want to spend the majority of our time exploring machine learning models, data collected <a id="id145" class="indexterm"></a>via various systems and sources in the preceding ingestion step is, in most cases, in a raw form. For example, we <a id="id146" class="indexterm"></a>might log user events such as details of when a user views the<a id="id147" class="indexterm"></a> information page for a movie, when they watch a movie, or when they provide some other feedback. We might also collect external information such as the location of the user (as provided through their IP address, for example). These event logs will typically contain some combination of textual and numeric information about the event (and also, perhaps, other forms of data such as images or audio).</p><p>In order to use this raw data in our models, in almost all cases, we need to perform preprocessing, which might include:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><span class="strong"><strong>Filtering data</strong></span>: Let's assume that we want to create a model from a subset of the raw data, such as only the most recent few months of activity data or only events that match certain criteria.</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Dealing with missing, incomplete, or corrupted data</strong></span>: Many real-world datasets are incomplete in some way. This might include data that is missing (for example, due to a missing user input) or data that is incorrect or flawed (for example, due to an error in data ingestion or storage, technical issues or bugs, or software or hardware failure). We might need to filter out bad data or alternatively decide a method to fill in missing data points (such as using the average value from the dataset for missing points, for example).</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Dealing with potential anomalies, errors, and outliers</strong></span>: Erroneous or outlier data might skew the results of model training, so we might wish to filter these cases out or use techniques that are able to deal with outliers.</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Joining together disparate data sources</strong></span>: For example, we might need to match up the event data for each user with different internal data sources, such as user profiles, as well as external data, such as geolocation, weather, and economic data.</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Aggregating data</strong></span>: Certain models might require input data that is aggregated in some way, such as computing the sum of a number of different event types per user.</p></li></ul></div><p>Once we have performed initial preprocessing on our data, we often need to transform the data into a representation that is suitable for machine learning models. For many model types, this representation will take the form of a vector or matrix structure that contains numerical data. Common challenges during data transformation and feature extraction include:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Taking <a id="id148" class="indexterm"></a>categorical data (such as country for geolocation or <a id="id149" class="indexterm"></a>category for a movie) and encoding it in a numerical representation.</p></li><li style="list-style-type: disc"><p>Extracting useful features from text data.</p></li><li style="list-style-type: disc"><p>Dealing with image or audio data.</p></li><li style="list-style-type: disc"><p>We often convert numerical data into categorical data to reduce the number of values a variable can take on. An example of this is converting a variable for age into buckets (such as 25-35, 45-55, and so on).</p></li><li style="list-style-type: disc"><p>Transforming<a id="id150" class="indexterm"></a> numerical<a id="id151" class="indexterm"></a> features; for example, applying a log transformation to a numerical variable can help deal with variables that take on a very large range of values.</p></li><li style="list-style-type: disc"><p>Normalizing<a id="id152" class="indexterm"></a> and standardizing numerical features ensures that all the different input variables for a model have a consistent scale. Many machine learning models require standardized input to work properly.</p></li><li style="list-style-type: disc"><p>Feature engineering is the process of combining or transforming the existing variables to create new features. For example, we can create a new variable that is the average of some other data, such as the average number of times a user watches a movie.</p></li></ul></div><p>We will cover all of these techniques through the examples in this book.</p><p>These data-cleansing, exploration, aggregation, and transformation steps can be carried out using both Spark's core API functions as well as the SparkSQL engine, not to mention other external Scala, Java, or Python libraries. We can take advantage of Spark's Hadoop compatibility to read data from and write data to the various different storage systems mentioned earlier.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec17"></a>Model training and testing loop</h3></div></div></div><p>Once we have our training data in a form that is suitable for our model, we can proceed with the model's training and testing phase. During this phase, we are primarily concerned with <a id="id153" class="indexterm"></a>
<span class="strong"><strong>model selection</strong></span>. This can refer to choosing the best modeling <a id="id154" class="indexterm"></a>approach for <a id="id155" class="indexterm"></a>our task, or the best parameter settings for a given model. In fact, the term model selection often refers to <a id="id156" class="indexterm"></a>both of these processes, as, in many cases, we might wish to<a id="id157" class="indexterm"></a> try out various models and select the best performing model (with the best performing parameter settings for each model). It is also common to explore the application of combinations of different models (known as<a id="id158" class="indexterm"></a> <span class="strong"><strong>ensemble methods</strong></span>) in this phase.</p><p>This is typically a fairly straightforward process of running our chosen model on our training dataset and testing its performance on a test dataset (that is, a set of data that is held out for the evaluation of the model that the model has not seen in the training phase). This process is referred to as <a id="id159" class="indexterm"></a>
<span class="strong"><strong>cross-validation</strong></span>.</p><p>However, due to the large scale of data we are typically working with, it is often useful to carry out this initial train-test loop on a smaller representative sample of our full dataset or perform model <a id="id160" class="indexterm"></a>selection using parallel methods where possible.</p><p>For this part of the pipeline, Spark's built-in machine learning library, MLlib, is a perfect fit. We will focus most of our attention in this book on the model training, evaluation, and cross-validation steps for various machine learning techniques, using MLlib and Spark's core features.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec18"></a>Model deployment and integration</h3></div></div></div><p>Once we have found the optimal<a id="id161" class="indexterm"></a> model based <a id="id162" class="indexterm"></a>on the<a id="id163" class="indexterm"></a> train-test loop, we might still face the task of deploying the <a id="id164" class="indexterm"></a>model to a production system so that it can be used to make actionable predictions.</p><p>Usually, this process involves exporting the trained model to a central data store from where the production-serving system can obtain the latest version. Thus, the live system <span class="emphasis"><em>refreshes</em></span> the model periodically as a new model is trained.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec19"></a>Model monitoring and feedback</h3></div></div></div><p>It is critically important to monitor<a id="id165" class="indexterm"></a> the performance of our machine learning system in production. Once we deploy our optimal trained model, we wish to understand how it is doing in the "wild". Is it performing as we expect on new, unseen data? Is its accuracy good enough? The reality is regardless of how much model selection and tuning we try to do in the earlier phases; the only way to measure true performance is to observe what happens in our production system.</p><p>Also, bear in mind that model <a id="id166" class="indexterm"></a>accuracy and predictive performance is only one aspect of a real-world system. Usually, we are concerned with other metrics related to business performance (for example, revenue and profitability) or user experience (such as the time spent on our site and how active our users are overall). In most cases, we cannot easily map model-predictive performance to these business metrics. The accuracy of a recommendation or targeting system might be important, but it relates only indirectly to the true metrics we are concerned about, namely whether we are improving user experience, activity, and ultimately, revenue.</p><p>So, in real-world systems, we should monitor both model-accuracy metrics as well as business metrics. If possible, we should be able to experiment with different models running in production to allow us to optimize against these business metrics by making changes to the models. This is often done using live split tests. However, doing this correctly is not an easy task, and live testing and experimentation is expensive, in the sense that mistakes, poor performance, and using baseline models (they provide a control against which we test out production models) can negatively impact user experience and revenue.</p><p>Another important aspect of this phase is <a id="id167" class="indexterm"></a>
<span class="strong"><strong>model feedback</strong></span>. This is the <a id="id168" class="indexterm"></a>process where the predictions of our model feed through into user behavior; this, in turn, feeds through into our model. In a real-world system, our models are essentially influencing their own future training data by impacting decision-making and potential user behavior.</p><p>For example, if we have deployed a recommendation system, then, by making recommendations, we might be influencing user behavior because we are only allowing users a limited selection of choices. We hope that this selection is relevant due to our model; however, this feedback loop, in turn, can influence our model's training data. This, in turn, feeds back into real-world performance. It is possible to get into an ever-narrowing feedback loop; ultimately, this can negatively affect both model accuracy and our important business metrics.</p><p>Fortunately, there are mechanisms by which we can try to limit the potential negative impact of this feedback loop. These include providing some unbiased training data by having a small portion of data coming from users who are not exposed to our models or by being principled in the way we balance exploration, to learn more about our data, and exploitation, to use what we have learned to improve our system's performance.</p><p>We will briefly cover <a id="id169" class="indexterm"></a>some aspects of real-time monitoring and model updates in <a class="link" href="#" linkend="ch10">Chapter 10</a>, <span class="emphasis"><em>Real-time Machine Learning with Spark Streaming</em></span>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec20"></a>Batch versus real time</h3></div></div></div><p>In the previous sections, we outlined the common batch processing approach, where the model is retrained using all data or a subset of all data, periodically. As the preceding pipeline takes some time to complete, it might not be possible to use this approach to update models immediately as new data arrives.</p><p>While we will be mostly<a id="id170" class="indexterm"></a> covering batch machine learning approaches in this book, there is a class of machine learning algorithms known as <a id="id171" class="indexterm"></a>
<span class="strong"><strong>online learning</strong></span>; they update immediately as new data is fed into the model, thus enabling a real-time system. A common example is an online-optimization algorithm for a linear model, such as stochastic gradient descent. We can learn this algorithm using examples. The advantages of these methods are that the system can react very quickly to new information and also that the system can adapt to changes in the underlying behavior (that is, if the characteristics and distribution of the input data are changing over time, which is almost always the case in real-world situations).</p><p>However, online-learning models come with their own unique challenges in a production context. For example, it might be difficult to ingest and transform data in real time. It can also be complex to properly perform model selection in a purely online setting. Latency of the online training and the model selection and deployment phases might be too high for true real-time requirements (for example, in online advertising, latency requirements are measured in single-digit milliseconds). Finally, batch-oriented frameworks might make it awkward to handle real-time processes of a streaming nature.</p><p>Fortunately, Spark's real-time stream processing<a id="id172" class="indexterm"></a> component, <span class="strong"><strong>Spark Streaming</strong></span>, is a good potential fit for real-time machine learning workflows. We will explore Spark Streaming and online learning in <a class="link" href="#" linkend="ch10">Chapter 10</a>, <span class="emphasis"><em>Real-time Machine Learning with Spark Streaming</em></span>.</p><p>Due to the complexities inherent in a true real-time machine learning system, in practice, many systems target near real-time operations. This is essentially a hybrid approach where models are not necessarily updated immediately as new data arrives; instead, the new data is collected into mini-batches of a small set of training data. These mini-batches can be fed to an online-learning algorithm. In many cases, this approach is combined with a periodic batch process that might recompute the model on the entire data set and perform more complex processing and model selection. This can help ensure that the real-time model does not degrade over time.</p><p>Another similar approach involves making approximate updates to a more complex model as new data arrives while recomputing the entire model in a batch process periodically. In this way, the model can learn from new data, with a short delay (usually measured in seconds or, perhaps, a few minutes), but will become more and more inaccurate over time due to the approximation applied. The periodic recomputation takes care of this by<a id="id173" class="indexterm"></a> retraining the model on all available data.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec20"></a>An architecture for a machine learning system</h2></div></div><hr /></div><p>Now that we have explored<a id="id174" class="indexterm"></a> how our machine learning system might <a id="id175" class="indexterm"></a>work in the context of MovieStream, we can outline a possible architecture for our system:</p><div class="mediaobject"><img src="graphics/8519OS_02_03.jpg" /><div class="caption"><p>MovieStream's future architecture</p></div></div><p>As we can see, our<a id="id176" class="indexterm"></a> system incorporates the machine learning pipeline<a id="id177" class="indexterm"></a> outlined in the preceding diagram; this system also includes:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Collecting data about users, their behavior, and our content titles</p></li><li style="list-style-type: disc"><p>Transforming this data into features</p></li><li style="list-style-type: disc"><p>Training our models, including our training-testing and model-selection phases</p></li><li style="list-style-type: disc"><p>Deploying the trained models to both our live model-serving system as well as using these models for offline processes</p></li><li style="list-style-type: disc"><p>Feeding back the model results into the MovieStream website through recommendation and targeting pages</p></li><li style="list-style-type: disc"><p>Feeding back the model results into MovieStream's personalized marketing channels</p></li><li style="list-style-type: disc"><p>Using the <a id="id178" class="indexterm"></a>offline models to provide tools<a id="id179" class="indexterm"></a> to MovieStream's various teams to better understand user behavior, characteristics of the content catalogue, and drivers of revenue for the business</p></li></ul></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl2sec21"></a>Practical exercise</h3></div></div></div><p>Imagine that you now need to provide input to the frontend and infrastructure engineering team about the data that your machine learning system will need. Consider a brief for them on how they should structure the data-collection mechanisms. Write down some examples of what the raw data might look like (for example, web logs, event logs, and so on) and how it should flow through the system. Take into account the following aspects:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>What data sources will be required</p></li><li style="list-style-type: disc"><p>What format should the data be in</p></li><li style="list-style-type: disc"><p>How often should data be collected, processed, potentially aggregated, and stored</p></li><li style="list-style-type: disc"><p>What data storage will you use to ensure scalability</p></li></ul></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch02lvl1sec21"></a>Summary</h2></div></div><hr /></div><p>In this chapter, you learned about the components inherent in a data-driven, automated machine learning system. We also outlined how a possible high-level architecture for such a system might look in a real-world situation.</p><p>In the next chapter, we will discuss how to obtain publicly-available datasets for common machine learning tasks. We will also explore general concepts related to processing, cleaning, and transforming data so that they can be used to train a machine learning model.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch03"></a>ChapterÂ 3.Â Obtaining, Processing, and Preparing Data with Spark</h2></div></div></div><p>Machine learning is an extremely broad field, and these days, applications can be found across <a id="id180" class="indexterm"></a>areas that include web and mobile applications, Internet of Things and sensor networks, financial services, healthcare, and various scientific fields, to name just a few.</p><p>Therefore, the range of data available for potential use in machine learning is enormous. In this book, we will focus mostly on business applications. In this context, the data available often consists of data internal to an organization (such as transactional data for a financial services company) as well as external data sources (such as financial asset price data for the same financial services company).</p><p>For example, recall from <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Designing a Machine Learning System</em></span>, that the main internal source of data for our hypothetical Internet business, MovieStream, consists of data on the movies available on the site, the users of the service, and their behavior. This includes data about movies and other content (for example, title, categories, description, images, actors, and directors), user information (for example, demographics, location, and so on), and user activity data (for example, web page views, title previews and views, ratings, reviews, and social data such as <span class="emphasis"><em>likes</em></span>, <span class="emphasis"><em>shares</em></span>, and social network profiles on services including Facebook and Twitter).</p><p>External data sources in this example might include weather and geolocation services, third-party movie ratings and review sites such as <span class="emphasis"><em>IMDB</em></span> and <span class="emphasis"><em>Rotten Tomatoes</em></span>, and so on.</p><p>Generally speaking, it is quite difficult to obtain data of an internal nature for real-world services and businesses, as it is commercially sensitive (in particular, data on purchasing activity, user or customer behavior, and revenue) and of great potential value to the organization concerned. This is why it is also often the most useful and interesting data on which to apply machine learningâ€”a good machine learning model that can make accurate predictions can be highly valuable (witness the success of machine learning competitions such as the <span class="emphasis"><em>Netflix Prize</em></span> and <span class="emphasis"><em>Kaggle</em></span>).</p><p>In this book, we will make use of datasets that are publicly available to illustrate concepts around data processing and training of machine learning models.</p><p>In this chapter, we will:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Briefly cover the types of data typically used in machine learning.</p></li><li style="list-style-type: disc"><p>Provide examples of where to obtain interesting datasets, often publicly available on the Internet. We will use some of these datasets throughout the book to illustrate the use of the models we introduce.</p></li><li style="list-style-type: disc"><p>Discover how to process, clean, explore, and visualize our data.</p></li><li style="list-style-type: disc"><p>Introduce various techniques to transform our raw data into features that can be used as input to machine learning algorithms.</p></li><li style="list-style-type: disc"><p>Learn how to normalize input features using external libraries as well as Spark's built-in functionality.</p></li></ul></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec22"></a>Accessing publicly available datasets</h2></div></div><hr /></div><p>Fortunately, while commercially-sensitive data can be hard to come by, there are still a number<a id="id181" class="indexterm"></a> of useful datasets available publicly. Many of these are often used as benchmark datasets for specific types of machine learning problems. Examples of common data sources include:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><span class="strong"><strong>UCI Machine Learning Repository</strong></span>: This <a id="id182" class="indexterm"></a>is a collection <a id="id183" class="indexterm"></a>of almost 300 datasets of various types and sizes for tasks including classification, regression, clustering, and recommender systems. The<a id="id184" class="indexterm"></a> list is available at <a class="ulink" href="http://archive.ics.uci.edu/ml/" target="_blank">http://archive.ics.uci.edu/ml/</a>.</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Amazon AWS public datasets</strong></span>: This is a<a id="id185" class="indexterm"></a> set of often very large datasets that can be accessed via Amazon S3. These datasets include the Human Genome Project, the Common <a id="id186" class="indexterm"></a>Crawl web corpus, Wikipedia <a id="id187" class="indexterm"></a>data, and Google Books Ngrams. Information on these datasets can be found at <a class="ulink" href="http://aws.amazon.com/publicdatasets/" target="_blank">http://aws.amazon.com/publicdatasets/</a>.</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Kaggle</strong></span>: This<a id="id188" class="indexterm"></a> is a collection of datasets used in machine<a id="id189" class="indexterm"></a> learning competitions run by Kaggle. Areas include classification, regression, ranking, recommender systems, and image analysis. These datasets<a id="id190" class="indexterm"></a> can be found under the <span class="emphasis"><em>Competitions</em></span> section at <a class="ulink" href="http://www.kaggle.com/competitions" target="_blank">http://www.kaggle.com/competitions</a>.</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>KDnuggets</strong></span>: This has <a id="id191" class="indexterm"></a>a detailed list of public datasets, including <a id="id192" class="indexterm"></a>some of those mentioned earlier. The list<a id="id193" class="indexterm"></a> is available at <a class="ulink" href="http://www.kdnuggets.com/datasets/index.html" target="_blank">http://www.kdnuggets.com/datasets/index.html</a>.</p></li></ul></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip14"></a>Tip</h3><p>There are many other resources to find public datasets depending on the specific domain and machine learning task. Hopefully, you might also have exposure to some interesting academic or commercial data of your own!</p></div><p>To <a id="id194" class="indexterm"></a>illustrate a few key concepts related to data processing, transformation, and feature extraction in Spark, we will download a commonly-used dataset for<a id="id195" class="indexterm"></a> movie recommendations; this dataset is known as the <span class="strong"><strong>MovieLens</strong></span> dataset. As it is applicable to recommender systems as well as potentially other machine learning tasks, it serves as a useful example dataset.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note06"></a>Note</h3><p>Spark's machine learning library, MLlib, has been under heavy development since its inception, and unlike the Spark core, it is still not in a fully stable state with regard to its overall API and design.</p><p>As of Spark Version 1.2.0, a new, experimental API for MLlib has been released under the <code class="literal">ml</code> package (whereas the current library resides under the <code class="literal">mllib</code> package). This new API aims to enhance the APIs and interfaces for models as well as feature extraction and transformation so as to make it easier to build pipelines that chain together steps that include feature extraction, normalization, dataset transformations, model training, and cross-validation.</p><p>In the upcoming chapters, we will only cover the existing, more developed MLlib API, since the new API is still experimental and may be subject to major changes in the next few Spark releases. Over time, the various feature-processing techniques and models that we will cover will simply be ported to the new API; however, the core concepts and most underlying code will remain largely unchanged.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec22"></a>The MovieLens 100k dataset</h3></div></div></div><p>The MovieLens <a id="id196" class="indexterm"></a>100k dataset is a set of 100,000 data <a id="id197" class="indexterm"></a>points related to ratings given by a set of users to a set of movies. It also contains movie metadata and user profiles. While it is a small dataset, you can quickly download it and run Spark code on it. This makes it ideal for illustrative purposes.</p><p>You can <a id="id198" class="indexterm"></a>download the dataset from <a class="ulink" href="http://files.grouplens.org/datasets/movielens/ml-100k.zip" target="_blank">http://files.grouplens.org/datasets/movielens/ml-100k.zip</a>.</p><p>Once you have downloaded the data, unzip it using your terminal:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;unzip ml-100k.zip</strong></span>
<span class="strong"><strong>  inflating: ml-100k/allbut.pl       </strong></span>
<span class="strong"><strong>  inflating: ml-100k/mku.sh          </strong></span>
<span class="strong"><strong>  inflating: ml-100k/README</strong></span>
<span class="strong"><strong>  ...</strong></span>
<span class="strong"><strong>  inflating: ml-100k/ub.base         </strong></span>
<span class="strong"><strong>  inflating: ml-100k/ub.test</strong></span>
</pre></div><p>This <a id="id199" class="indexterm"></a>will create a directory called <code class="literal">ml-100k</code>. Change into<a id="id200" class="indexterm"></a> this directory and examine the contents. The important files are <code class="literal">u.user</code> (user profiles), <code class="literal">u.item</code> (movie metadata), and <code class="literal">u.data</code> (the ratings given by users to movies):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;cd ml-100k</strong></span>
</pre></div><p>The <code class="literal">README</code> file contains more information on the dataset, including the variables present in each data file. We can use the <code class="literal">head</code> command to examine the contents of the various files.</p><p>For example, we can see that the <code class="literal">u.user</code> file contains the <code class="literal">user id</code>, <code class="literal">age</code>, <code class="literal">gender</code>, <code class="literal">occupation</code>, and <code class="literal">ZIP code</code> fields, separated by a pipe (<code class="literal">|</code> character):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;head -5 u.user</strong></span>
<span class="strong"><strong>  1|24|M|technician|85711</strong></span>
<span class="strong"><strong>  2|53|F|other|94043</strong></span>
<span class="strong"><strong>  3|23|M|writer|32067</strong></span>
<span class="strong"><strong>  4|24|M|technician|43537</strong></span>
<span class="strong"><strong>  5|33|F|other|15213</strong></span>
</pre></div><p>The <code class="literal">u.item</code> file contains the <code class="literal">movie id</code>, <code class="literal">title</code>, <code class="literal">release data</code>, and <code class="literal">IMDB link</code> fields and a set of fields related to movie category data. It is also separated by a <code class="literal">|</code> character:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;head -5 u.item</strong></span>
<span class="strong"><strong>  1|Toy Story (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)|0|0|0|1|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0</strong></span>
<span class="strong"><strong>  2|GoldenEye (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?GoldenEye%20(1995)|0|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0|1|0|0</strong></span>
<span class="strong"><strong>  3|Four Rooms (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Four%20Rooms%20(1995)|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|1|0|0</strong></span>
<span class="strong"><strong>  4|Get Shorty (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Get%20Shorty%20(1995)|0|1|0|0|0|1|0|0|1|0|0|0|0|0|0|0|0|0|0</strong></span>
<span class="strong"><strong>  5|Copycat (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Copycat%20(1995)|0|0|0|0|0|0|1|0|1|0|0|0|0|0|0|0|1|0|0</strong></span>
</pre></div><p>Finally, the <code class="literal">u.data</code> file <a id="id201" class="indexterm"></a>contains the <code class="literal">user id</code>, <code class="literal">movie id</code>, <code class="literal">rating (1-5 scale)</code>, and <code class="literal">timestamp</code> fields and is separated<a id="id202" class="indexterm"></a> by a tab (the <code class="literal">\t</code> character):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;head -5 u.data</strong></span>
<span class="strong"><strong>196	242	3	881250949</strong></span>
<span class="strong"><strong>186	302	3	891717742</strong></span>
<span class="strong"><strong>22	377	1	878887116</strong></span>
<span class="strong"><strong>244	51	2	880606923</strong></span>
<span class="strong"><strong>166	346	1	886397596</strong></span>
</pre></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec23"></a>Exploring and visualizing your data</h2></div></div><hr /></div><p>Now<a id="id203" class="indexterm"></a> that we have our data available, let's fire up an interactive Spark <a id="id204" class="indexterm"></a>console and explore it! For this section, we will use Python and the PySpark shell, as we are going to use the IPython interactive console and the matplotlib plotting library to process and visualize our data.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note07"></a>Note</h3><p>IPython is <a id="id205" class="indexterm"></a>an advanced, interactive shell for Python. It includes a useful set of features called pylab, which includes NumPy and SciPy for numerical <a id="id206" class="indexterm"></a>computing and matplotlib for interactive plotting and visualization.</p><p>We recommend that you use the latest version of IPython (2.3.1 at the time of writing this book). To install IPython for your platform, follow the instructions available at <a class="ulink" href="http://ipython.org/install.html" target="_blank">http://ipython.org/install.html</a>. If this is the first time you are using IPython, you can find a tutorial at <a class="ulink" href="http://ipython.org/ipython-doc/stable/interactive/tutorial.html" target="_blank">http://ipython.org/ipython-doc/stable/interactive/tutorial.html</a>.</p></div><p>You will need to install all the packages listed earlier in order to work through the code in this chapter. Instructions to install the packages can be found in the code bundle. If you are starting out with Python or are unfamiliar with the process of installing these packages, we strongly recommend that you use a prebuilt scientific Python installation such as <a id="id207" class="indexterm"></a>Anaconda (available at <a class="ulink" href="http://continuum.io/downloads" target="_blank">http://continuum.io/downloads</a>) or Enthought (available at <a class="ulink" href="https://store.enthought.com/downloads/" target="_blank">https://store.enthought.com/downloads/</a>). These make the installation process much easier and include everything you will need to follow the example code.</p><p>The PySpark console allows the option of setting which Python executable needs to be used to<a id="id208" class="indexterm"></a> run the shell. We can choose to use IPython, as opposed to the<a id="id209" class="indexterm"></a> standard Python shell, when launching our PySpark console. We can also pass in additional options to IPython, including telling it to launch with the pylab functionality enabled.</p><p>We can do this by running the following command from the Spark home directory (that is, the same directory that we used previously to explore the Spark interactive console):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;IPYTHON=1 IPYTHON_OPTS="--pylab" ./bin/pyspark</strong></span>
</pre></div><p>You will see the PySpark console start up, showing output similar to the following screenshot:</p><div class="mediaobject"><img src="graphics/8519OS_03_01.jpg" /><div class="caption"><p>The PySpark console using IPython</p></div></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip15"></a>Tip</h3><p>Notice the <code class="literal">IPython 2.3.1 -- An enhanced Interactive Python</code> and <code class="literal">Using matplotlib backend: MacOSX</code> lines; they indicate that both the IPython and pylab functionalities are being used by the PySpark shell.</p><p>You might see a slightly different output, depending on your operating system and software versions.</p></div><p>Now<a id="id210" class="indexterm"></a> that we have our IPython console open, we can start to explore <a id="id211" class="indexterm"></a>the MovieLens dataset and do some basic analysis.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note08"></a>Note</h3><p>You can follow along with this chapter by entering the code examples into your IPython console. IPython also provides an HTML-enabled Notebook application. It provides some enhanced functionality over the standard IPython console, such as inline graphics for plotting, the HTML markup functionality, as well as the ability to run cells of code independently.</p><p>The images used in this chapter were generated using the IPython Notebook, so don't worry if yours look a little bit different in style, as long as they contain the same content! You can also use the Notebook for the code in this chapter, if you prefer. In addition to the Python code for this chapter, we have provided a version saved in the IPython Notebook format, which you can load into your own IPython Notebook.</p><p>Check<a id="id212" class="indexterm"></a> out the instructions on how to use the IPython Notebook at <a class="ulink" href="http://ipython.org/ipython-doc/stable/interactive/notebook.html" target="_blank">http://ipython.org/ipython-doc/stable/interactive/notebook.html</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec23"></a>Exploring the user dataset</h3></div></div></div><p>First, we <a id="id213" class="indexterm"></a>will analyze the characteristics of MovieLens <a id="id214" class="indexterm"></a>users. Enter the following lines into your console (where <code class="literal">PATH</code> refers to the base directory in which you performed the <code class="literal">unzip</code> command to unzip the preceding MovieLens 100k dataset):</p><div class="informalexample"><pre class="programlisting">user_data = sc.textFile("/<span class="strong"><strong>PATH</strong></span>/ml-100k/u.user")
user_data.first()</pre></div><p>You should see output similar to this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>u'1|24|M|technician|85711'</strong></span>
</pre></div><p>As we can see, this is the first line of our user data file, separated by the <code class="literal">"|"</code> character.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip16"></a>Tip</h3><p>The <code class="literal">first</code> function is similar to <code class="literal">collect</code>, but it only returns the first element of the RDD to the driver. We can also use <code class="literal">take(k)</code> to collect only the first <span class="emphasis"><em>k</em></span> elements of the RDD to the driver.</p></div><p>Let's transform<a id="id215" class="indexterm"></a> the data by splitting each line, around the <code class="literal">"|"</code> character. This will give us an RDD where each record is a Python list <a id="id216" class="indexterm"></a>that contains the user ID, age, gender, occupation, and ZIP code fields.</p><p>We will then count the number of users, genders, occupations, and ZIP codes. We can achieve this by running the following code in the console, line by line. Note that we do not cache the data, as it is unnecessary for this small size:</p><div class="informalexample"><pre class="programlisting">user_fields = user_data.map(lambda line: line.split("|"))
num_users = user_fields.map(lambda fields: fields[0]).count()
num_genders = user_fields.map(lambda fields:fields[2]).distinct().count()
num_occupations = user_fields.map(lambda fields:fields[3]).distinct().count()
num_zipcodes = user_fields.map(lambda fields:fields[4]).distinct().count()
print "Users: %d, genders: %d, occupations: %d, ZIP codes: %d" % (num_users, num_genders, num_occupations, num_zipcodes)</pre></div><p>You will see the following output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Users: 943, genders: 2, occupations: 21, ZIP codes: 795</strong></span>
</pre></div><p>Next, we will create a histogram to analyze the distribution of user ages, using matplotlib's <code class="literal">hist</code> function:</p><div class="informalexample"><pre class="programlisting">ages = user_fields.map(lambda x: int(x[1])).collect()
hist(ages, bins=20, color='lightblue', normed=True)
fig = matplotlib.pyplot.gcf()
fig.set_size_inches(16, 10)</pre></div><p>We passed in the <code class="literal">ages</code> array, together with the number of <code class="literal">bins</code> for our histogram (<code class="literal">20</code> in this case), to the <code class="literal">hist</code> function. Using the <code class="literal">normed=True</code> argument, we also specified that we want the histogram to be normalized so that each bucket represents the percentage of the overall data that falls into that bucket.</p><p>You will see an image containing the histogram chart, which looks something like the one shown here. As we can see, the ages of MovieLens users are somewhat skewed towards younger viewers. A large number of users are between the ages of about 15 and 35.</p><div class="mediaobject"><img src="graphics/8519OS_03_02.jpg" /><div class="caption"><p>Distribution of user ages</p></div></div><p>We<a id="id217" class="indexterm"></a> might also want to explore the relative frequencies <a id="id218" class="indexterm"></a>of the various occupations of our users. We can do this using the following code snippet. First, we will use the MapReduce approach introduced previously to count the occurrences of each occupation in the dataset. Then, we will use <code class="literal">matplotlib</code> to display a bar chart of occupation counts, using the <code class="literal">bar</code> function.</p><p>Since part of our data is the descriptions of textual occupation, we will need to manipulate it a little to get it to work with the <code class="literal">bar</code> function:</p><div class="informalexample"><pre class="programlisting">count_by_occupation = user_fields.map(lambda fields: (fields[3], 1)).reduceByKey(lambda x, y: x + y).collect()
x_axis1 = np.array([c[0] for c in count_by_occupation])
y_axis1 = np.array([c[1] for c in count_by_occupation])</pre></div><p>Once we have collected the <code class="literal">RDD</code> of counts per occupation, we will convert it into two arrays for the <span class="emphasis"><em>x</em></span> axis (the occupations) and the <span class="emphasis"><em>y</em></span> axis (the counts) of our chart. The <code class="literal">collect</code> function returns the count data to us in no particular order. We need to sort the count data so that our bar chart is ordered from the lowest to the highest count.</p><p>We will <a id="id219" class="indexterm"></a>achieve this by first creating two <code class="literal">numpy</code> arrays<a id="id220" class="indexterm"></a> and then using the <code class="literal">argsort</code> method of <code class="literal">numpy</code> to select the elements from each array, ordered by the count data in an ascending fashion. Notice that here, we will sort both the <span class="emphasis"><em>x</em></span> and <span class="emphasis"><em>y</em></span> axis arrays by the <span class="emphasis"><em>y</em></span> axis (that is, by the counts):</p><div class="informalexample"><pre class="programlisting">x_axis = x_axis1[np.argsort(y_axis1)]
y_axis = y_axis1[np.argsort(y_axis1)]</pre></div><p>Once we have the <span class="emphasis"><em>x</em></span> and <span class="emphasis"><em>y</em></span> axis data for our chart, we will create the bar chart with the occupations as labels on the <span class="emphasis"><em>x</em></span> axis and the counts as the values on the <span class="emphasis"><em>y</em></span> axis. We will also add a few lines, such as the <code class="literal">plt.xticks(rotation=30)</code> code, to display a better-looking chart:</p><div class="informalexample"><pre class="programlisting">pos = np.arange(len(x_axis))
width = 1.0

ax = plt.axes()
ax.set_xticks(pos + (width / 2))
ax.set_xticklabels(x_axis)

plt.bar(pos, y_axis, width, color='lightblue')
plt.xticks(rotation=30)
fig = matplotlib.pyplot.gcf()
fig.set_size_inches(16, 10)</pre></div><p>The image you have generated should look like the one here. It appears that the most prevalent occupations are <span class="strong"><strong>student</strong></span>, <span class="strong"><strong>other</strong></span>, <span class="strong"><strong>educator</strong></span>, <span class="strong"><strong>administrator</strong></span>, <span class="strong"><strong>engineer</strong></span>, and <span class="strong"><strong>programmer</strong></span>.</p><div class="mediaobject"><img src="graphics/8519OS_03_03.jpg" /><div class="caption"><p>Distribution of user occupations</p></div></div><p>Spark <a id="id221" class="indexterm"></a>provides a convenience method on RDDs called <code class="literal">countByValue</code>; this<a id="id222" class="indexterm"></a> method counts the occurrences of each unique value in the RDD and returns it to the driver as a Python <code class="literal">dict</code> method (or a Scala or Java <code class="literal">Map</code> method). We can create the <code class="literal">count_by_occupation</code> variable using this method:</p><div class="informalexample"><pre class="programlisting">count_by_occupation2 = user_fields.map(lambda fields: fields[3]).countByValue()
print "Map-reduce approach:"
print dict(count_by_occupation2)
print ""
print "countByValue approach:"
print dict(count_by_occupation)</pre></div><p>You should see that the results are the same for each approach.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec24"></a>Exploring the movie dataset</h3></div></div></div><p>Next, we <a id="id223" class="indexterm"></a>will investigate a few properties of the movie catalogue. We <a id="id224" class="indexterm"></a>can inspect a row of the movie data file, as we did for the user data earlier, and then count the number of movies:</p><div class="informalexample"><pre class="programlisting">movie_data = sc.textFile("/<span class="strong"><strong>PATH</strong></span>/ml-100k/u.item")
print movie_data.first()
num_movies = movie_data.count()
print "Movies: %d" % num_movies</pre></div><p>You will see the following output on your console:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>1|Toy Story (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)|0|0|0|1|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0 </strong></span>
<span class="strong"><strong>Movies: 1682 </strong></span>
</pre></div><p>In the same manner as we did for user ages and occupations earlier, we can plot the distribution of movie age, that is, the year of release relative to the current date (note that for this dataset, the current year is 1998).</p><p>In the following code block, we can see that we need a small function called <code class="literal">convert_year</code> to handle errors in the parsing of the <code class="literal">release date</code> field. This is due to some bad data in one line of the movie data:</p><div class="informalexample"><pre class="programlisting">def convert_year(x):
  try:
    return int(x[-4:])
  except:
    return 1900 # there is a 'bad' data point with a blank year,
    which we set to 1900 and will filter out later</pre></div><p>Once we have our utility function to parse the year of release, we can apply it to the movie data using a <code class="literal">map</code> transformation and collect the results:</p><div class="informalexample"><pre class="programlisting">movie_fields = movie_data.map(lambda lines: lines.split("|"))
years = movie_fields.map(lambda fields: fields[2]).map(lambda x: convert_year(x))</pre></div><p>Since we have assigned the value <code class="literal">1900</code> to any error in parsing, we can filter these bad values out of the resulting data using Spark's <code class="literal">filter</code> transformation:</p><div class="informalexample"><pre class="programlisting">years_filtered = years.filter(lambda x: x != 1900)</pre></div><p>This is a good example of how real-world datasets can often be messy and require a more in-depth approach to parsing data. In fact, this also illustrates why data exploration is so important, as many of these issues in data integrity and quality are picked up during this phase.</p><p>After<a id="id225" class="indexterm"></a> filtering out bad data, we will transform the list of movie <a id="id226" class="indexterm"></a>release years into movie ages by subtracting the current year, use <code class="literal">countByValue</code> to compute the counts for each movie age, and finally, plot our histogram of movie ages (again, using the <code class="literal">hist</code> function, where the <code class="literal">values</code> variable are the values of the result from <code class="literal">countByValue</code>, and the <code class="literal">bins</code> variable are the keys):</p><div class="informalexample"><pre class="programlisting">movie_ages = years_filtered.map(lambda yr: 1998-yr).countByValue()
values = movie_ages.values()
bins = movie_ages.keys()
hist(values, bins=bins, color='lightblue', normed=True)
fig = matplotlib.pyplot.gcf()
fig.set_size_inches(16,10)</pre></div><p>You will see an image similar to the one here; it illustrates that most of the movies were released in the last few years before 1998:</p><div class="mediaobject"><img src="graphics/8519OS_03_04.jpg" /><div class="caption"><p>Distribution of movie ages</p></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec25"></a>Exploring the rating dataset</h3></div></div></div><p>Let's<a id="id227" class="indexterm"></a> now take a look at the ratings data:</p><div class="informalexample"><pre class="programlisting">rating_data = sc.textFile("/<span class="strong"><strong>PATH</strong></span>/ml-100k/u.data")
print rating_data.first()
num_ratings = rating_data.count()
print "Ratings: %d" % num_ratings</pre></div><p>This gives us the following result:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>196	242	3	881250949</strong></span>
<span class="strong"><strong>Ratings: 100000</strong></span>
</pre></div><p>There are<a id="id228" class="indexterm"></a> 100,000 ratings, and unlike the user and movie datasets, these records are split with a tab character (<code class="literal">"\t"</code>). As you might have guessed, we'd probably want to compute some basic summary statistics and frequency histograms for the rating values. Let's do this now:</p><div class="informalexample"><pre class="programlisting">rating_data = rating_data_raw.map(lambda line: line.split("\t"))
ratings = rating_data.map(lambda fields: int(fields[2]))
max_rating = ratings.reduce(lambda x, y: max(x, y))
min_rating = ratings.reduce(lambda x, y: min(x, y))
mean_rating = ratings.reduce(lambda x, y: x + y) / num_ratings
median_rating = np.median(ratings.collect())
ratings_per_user = num_ratings / num_users
ratings_per_movie = num_ratings / num_movies
print "Min rating: %d" % min_rating
print "Max rating: %d" % max_rating
print "Average rating: %2.2f" % mean_rating
print "Median rating: %d" % median_rating
print "Average # of ratings per user: %2.2f" % ratings_per_user
print "Average # of ratings per movie: %2.2f" % ratings_per_movie</pre></div><p>After running these lines on your console, you will see output similar to the following result:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Min rating: 1</strong></span>
<span class="strong"><strong>Max rating: 5</strong></span>
<span class="strong"><strong>Average rating: 3.53</strong></span>
<span class="strong"><strong>Median rating: 4</strong></span>
<span class="strong"><strong>Average # of ratings per user: 106.00</strong></span>
<span class="strong"><strong>Average # of ratings per movie: 59.00</strong></span>
</pre></div><p>We can see that the minimum rating is 1, while the maximum rating is 5. This is in line with <a id="id229" class="indexterm"></a>what we expect, since the ratings are on a scale <a id="id230" class="indexterm"></a>of 1 to 5.</p><p>Spark also provides a <code class="literal">stats</code> function for RDDs; this function contains a numeric variable (such as <code class="literal">ratings</code> in this case) to compute similar summary statistics:</p><div class="informalexample"><pre class="programlisting">ratings.stats()</pre></div><p>Here is the output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>(count: 100000, mean: 3.52986, stdev: 1.12566797076, max: 5.0, min: 1.0)</strong></span>
</pre></div><p>Looking at the results, the average rating given by a user to a movie is around 3.5 and the median rating is 4, so we might expect that the distribution of ratings will be skewed towards slightly higher ratings. Let's see whether this is true by creating a bar chart of rating values using a similar procedure as we did for occupations:</p><div class="informalexample"><pre class="programlisting">count_by_rating = ratings.countByValue()
x_axis = np.array(count_by_rating.keys())
y_axis = np.array([float(c) for c in count_by_rating.values()])
# we normalize the y-axis here to percentages
y_axis_normed = y_axis / y_axis.sum()
pos = np.arange(len(x_axis))
width = 1.0

ax = plt.axes()
ax.set_xticks(pos + (width / 2))
ax.set_xticklabels(x_axis)

plt.bar(pos, y_axis_normed, width, color='lightblue')
plt.xticks(rotation=30)
fig = matplotlib.pyplot.gcf()
fig.set_size_inches(16, 10)</pre></div><p>The preceding code should produce the following chart:</p><div class="mediaobject"><img src="graphics/8519OS_03_05.jpg" /><div class="caption"><p>Distribution of rating values</p></div></div><p>In line <a id="id231" class="indexterm"></a>with what we might have expected after seeing some <a id="id232" class="indexterm"></a>summary statistics, it is clear that the distribution of ratings is skewed towards average to high ratings.</p><p>We can also look at the distribution of the number of ratings made by each user. Recall that we previously computed the <code class="literal">rating_data</code> RDD used in the preceding code by splitting the ratings with the tab character. We will now use the <code class="literal">rating_data</code> variable again in the following code.</p><p>To compute the distribution of ratings per user, we will first extract the user ID as key and rating as value from <code class="literal">rating_data</code> RDD. We will then group the ratings by user ID using Spark's <code class="literal">groupByKey</code> function:</p><div class="informalexample"><pre class="programlisting">user_ratings_grouped = rating_data.map(lambda fields: (int(fields[0]), int(fields[2]))).\
    groupByKey()</pre></div><p>Next, for each key (user ID), we will find the size of the set of ratings; this will give us the number of ratings for that user:</p><div class="informalexample"><pre class="programlisting">user_ratings_byuser = user_ratings_grouped.map(lambda (k, v): (k, len(v)))
user_ratings_byuser.take(5)</pre></div><p>We can<a id="id233" class="indexterm"></a> inspect the resulting RDD by taking a few records <a id="id234" class="indexterm"></a>from it; this should give us an RDD of the (user ID, number of ratings) pairs:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[(1, 272), (2, 62), (3, 54), (4, 24), (5, 175)]</strong></span>
</pre></div><p>Finally, we will plot the histogram of number of ratings per user using our favorite <code class="literal">hist</code> function:</p><div class="informalexample"><pre class="programlisting">user_ratings_byuser_local = user_ratings_byuser.map(lambda (k, v):v).collect()
hist(user_ratings_byuser_local, bins=200, color='lightblue',normed=True)
fig = matplotlib.pyplot.gcf()
fig.set_size_inches(16,10)</pre></div><p>Your chart should look similar to the following screenshot. We can see that most of the users give fewer than 100 ratings. The distribution of the ratings shows, however, that there are fairly large number of users that provide hundreds of ratings.</p><div class="mediaobject"><img src="graphics/8519OS_03_06.jpg" /><div class="caption"><p>Distribution of ratings per user</p></div></div><p>We <a id="id235" class="indexterm"></a>leave it to you to perform a similar analysis to create a <a id="id236" class="indexterm"></a>histogram plot for the number of ratings given to each movie. Perhaps, if you're feeling adventurous, you could also extract a dataset of movie ratings by date (taken from the timestamps in the last column of the rating dataset) and chart a time series of the total number of ratings, number of unique users who gave a rating, and the number of unique movies rated, for each day.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec24"></a>Processing and transforming your data</h2></div></div><hr /></div><p>Now <a id="id237" class="indexterm"></a>that we have done some initial exploratory analysis of our dataset and <a id="id238" class="indexterm"></a>we know a little more about the characteristics of our users and movies, what do we do next?</p><p>In order to make the raw data usable in a machine learning algorithm, we first need to clean it up and possibly transform it in various ways before extracting useful features from the transformed data. The transformation and feature extraction steps are closely linked, and in some cases, certain transformations are themselves a case of feature extraction.</p><p>We have already seen an example of the need to clean data in the movie dataset. Generally, real-world datasets contain bad data, missing data points, and outliers. Ideally, we would<a id="id239" class="indexterm"></a> correct bad data; however, this is often not possible, as many datasets <a id="id240" class="indexterm"></a>derive from some form of collection process that cannot be repeated (this is the case, for example, in web activity data and sensor data). Missing values and outliers are also common and can be dealt with in a manner similar to bad data. Overall, the broad options are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><span class="strong"><strong>Filter out or remove records with bad or missing values</strong></span>: This is sometimes <a id="id241" class="indexterm"></a>unavoidable; however, this means losing the good part of a bad or missing record.</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Fill in bad or missing data</strong></span>: We can try to assign a value to bad or missing data based on the rest of the data we have available. Approaches can include assigning a zero value, assigning the global mean or median, interpolating nearby or similar data points (usually, in a time-series dataset), and so on. Deciding on the correct approach is often a tricky task and depends on the data, situation, and one's own experience.</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Apply robust techniques to outliers</strong></span>: The main issue with outliers is that they might be correct values, even though they are extreme. They might also be errors. It is often very difficult to know which case you are dealing with. Outliers can also be removed or filled in, although fortunately, there are statistical techniques (such as robust regression) to handle outliers and extreme values.</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Apply transformations to potential outliers</strong></span>: Another approach for outliers or extreme values is to apply transformations, such as a logarithmic or Gaussian kernel transformation, to features that have potential outliers, or display large ranges of potential values. These types of transformations have the effect of dampening the impact of large changes in the scale of a variable and turning a nonlinear relationship into one that is linear.</p></li></ul></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec26"></a>Filling in bad or missing data</h3></div></div></div><p>We have <a id="id242" class="indexterm"></a>already seen an example of filtering out bad data. Following<a id="id243" class="indexterm"></a> on from the preceding code, the following code snippet applies the fill-in approach to the bad release date record by assigning a value to the data point that is equal to the median year of release:</p><div class="informalexample"><pre class="programlisting">years_pre_processed = movie_fields.map(lambda fields: fields[2]).map(lambda x: convert_year(x)).collect()
years_pre_processed_array = np.array(years_pre_processed)   </pre></div><p>First, we<a id="id244" class="indexterm"></a> will compute the mean and median year of release after selecting <a id="id245" class="indexterm"></a>all the year of release data, <span class="emphasis"><em>except</em></span> the bad data point. We will then use the <code class="literal">numpy</code> function, <code class="literal">where</code>, to find the index of the bad value in <code class="literal">years_pre_processed_array</code> (recall that we assigned the value <code class="literal">1900</code> to this data point). Finally, we will use this index to assign the median release year to the bad value:</p><div class="informalexample"><pre class="programlisting">mean_year = np.mean(years_pre_processed_array[years_pre_processed_array!=1900])
median_year = np.median(years_pre_processed_array[years_pre_processed_array!=1900])
index_bad_data = np.where(years_pre_processed_array==1900)[0][0]
years_pre_processed_array[index_bad_data] = median_year
print "Mean year of release: %d" % mean_year
print "Median year of release: %d" % median_year
print "Index of '1900' after assigning median: %s" % np.where(years_pre_processed_array == 1900)[0]</pre></div><p>You should expect to see the following output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Mean year of release: 1989</strong></span>
<span class="strong"><strong>Median year of release: 1995</strong></span>
<span class="strong"><strong>Index of '1900' after assigning median: []</strong></span>
</pre></div><p>We computed both the mean and the median year of release here. As can be seen from the output, the median release year is quite higher because of the skewed distribution of the years. While it is not always straightforward to decide on precisely which fill-in value to use for a given situation, in this case, it is certainly feasible to use the median due to this skew.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip17"></a>Tip</h3><p>Note that the preceding code example is, strictly speaking, not very scalable, as it requires collecting all the data to the driver. We can use Spark's <code class="literal">mean</code> function for numeric RDDs to compute the mean, but there is no median function available currently. We can solve this by creating our own or by computing the median on a sample of the dataset created using the <code class="literal">sample</code> function (we will see more of this in the upcoming chapters).</p></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec25"></a>Extracting useful features from your data</h2></div></div><hr /></div><p>Once <a id="id246" class="indexterm"></a>we have completed the initial exploration, processing, and<a id="id247" class="indexterm"></a> cleaning of our data, we are ready to get down to the business of extracting actual features from the data, with which our machine learning model can be trained.</p><p><span class="strong"><strong>Features</strong></span> refer <a id="id248" class="indexterm"></a>to the variables that we use to train our model. Each row of data contains various information that we would like to extract into a training example. Almost all machine learning models ultimately work on numerical representations in <a id="id249" class="indexterm"></a>the form of a <span class="strong"><strong>vector</strong></span>; hence, we need to convert raw data into numbers.</p><p>Features broadly fall into a few categories, which are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><span class="strong"><strong>Numerical features</strong></span>: These features are typically real or integer numbers, for example, the <a id="id250" class="indexterm"></a>user age that we used in an example earlier.</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Categorical features</strong></span>: These features refer to variables that can take one of a set of possible states at any given time. Examples from our dataset might include<a id="id251" class="indexterm"></a> a user's gender or occupation or movie categories.</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Text features</strong></span>: These are features derived from the text content in the data, for<a id="id252" class="indexterm"></a> example, movie titles, descriptions, or reviews.</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Other features</strong></span>: Most other types of features are ultimately represented numerically. For example, images, video, and audio can be represented as sets of numerical data. Geographical locations can be represented as latitude and longitude or geohash data.</p></li></ul></div><p>Here we will cover numerical, categorical, and text features.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec27"></a>Numerical features</h3></div></div></div><p>What is <a id="id253" class="indexterm"></a>the difference between any old number and a numerical feature? Well, in reality, any numerical data can be used as an input variable. However, in a<a id="id254" class="indexterm"></a> machine learning model, we learn about a vector of weights for each feature. The weights play a role in mapping feature values to an outcome or target variable (in the case of supervised learning models).</p><p>Thus, we want to use features that make sense, that is, where the model can learn the relationship between feature values and the target variable. For example, age might be a reasonable feature. Perhaps there is a direct relationship between increasing age and a certain outcome. Similarly, height is a good example of a numerical feature that can be used directly.</p><p>We will often see that numerical features are less useful in their raw form, but can be turned into representations that are more useful. Location is an example of such a case. Using raw <a id="id255" class="indexterm"></a>locations (say, latitude and longitude) might not <a id="id256" class="indexterm"></a>be that useful unless our data is very dense indeed, since our model might not be able to learn about a useful relationship between the raw location and an outcome. However, a relationship might exist between some aggregated or binned representation of the location (for example, a city or country) and the outcome.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec28"></a>Categorical features</h3></div></div></div><p>Categorical features<a id="id257" class="indexterm"></a> cannot be used as input in their <a id="id258" class="indexterm"></a>raw form, as they are not numbers; instead, they are members of a set of possible values that the variable can take. In the example mentioned earlier, user occupation is a categorical variable that can take the value of student, programmer, and so on.</p><p>Such categorical <a id="id259" class="indexterm"></a>variables are also known as <span class="strong"><strong>nominal</strong></span> variables where there is no concept of order between the values of the variable. By contrast, when there is a concept of order between variables (such as the ratings mentioned earlier, where a rating of 5 is conceptually higher or better than a rating of 1), we refer<a id="id260" class="indexterm"></a> to <span class="strong"><strong>ordinal</strong></span> variables.</p><p>To transform categorical variables into a numerical representation, we can use a common approach known as <span class="strong"><strong>1-of-k</strong></span> encoding. An approach such as 1-of-k encoding is required to represent <a id="id261" class="indexterm"></a>nominal variables in a way that makes sense for machine learning tasks. Ordinal variables might be used in their raw form but are often encoded in the same way as nominal variables.</p><p>Assume that there are k possible values that the variable can take. If we assign each possible value an index from the set of 1 to k, then we can represent a given state of the variable using a binary vector of length k; here, all entries are zero, except the entry at the index that corresponds to the given state of the variable. This entry is set to one.</p><p>For example, we can collect all the possible states of the <code class="literal">occupation</code> variable:</p><div class="informalexample"><pre class="programlisting">all_occupations = user_fields.map(lambda fields: fields[3]).distinct().collect()
all_occupations.sort()</pre></div><p>We can then assign index values to each possible occupation in turn (note that we start from zero, since Python, Scala, and Java arrays all use zero-based indices):</p><div class="informalexample"><pre class="programlisting">idx = 0
all_occupations_dict = {}
for o in all_occupations:
    all_occupations_dict[o] = idx
    idx +=1
# try a few examples to see what "1-of-k" encoding is assigned
print "Encoding of 'doctor': %d" % all_occupations_dict['doctor']
print "Encoding of 'programmer': %d" % all_occupations_dict['programmer']</pre></div><p>You will <a id="id262" class="indexterm"></a>see the following output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Encoding of 'doctor': 2 </strong></span>
<span class="strong"><strong>Encoding of 'programmer': 14</strong></span>
</pre></div><p>Finally, we<a id="id263" class="indexterm"></a> can encode the value of <code class="literal">programmer</code>. We will start by creating a <code class="literal">numpy</code> array of a length that is equal to the number of possible occupations (k in this case) and filling it with zeros. We will use the <code class="literal">zeros</code> function of <code class="literal">numpy</code> to create this array.</p><p>We will then extract the index of the word <code class="literal">programmer</code> and assign a value of <code class="literal">1</code> to the array value at this index:</p><div class="informalexample"><pre class="programlisting">K = len(all_occupations_dict)
binary_x = np.zeros(K)
k_programmer = all_occupations_dict['programmer']
binary_x[k_programmer] = 1
print "Binary feature vector: %s" % binary_x
print "Length of binary vector: %d" % K</pre></div><p>This will give us the resulting binary feature vector of length <code class="literal">21</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Binary feature vector: [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0. 0.  0.  0.]</strong></span>
<span class="strong"><strong>Length of binary vector: 21 </strong></span>
</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec29"></a>Derived features</h3></div></div></div><p>As we <a id="id264" class="indexterm"></a>mentioned earlier, it is often useful to compute a derived feature from one or more <a id="id265" class="indexterm"></a>available variables. We hope that the derived feature can add more information than only using the variable in its raw form.</p><p>For instance, we can compute the average rating given by each user to all the movies they rated. This would be a feature that could provide a <span class="emphasis"><em>user-specific</em></span> intercept in our model (in fact, this is a commonly used approach in recommendation models). We have taken the raw rating data and created a new feature that can allow us to learn a better model.</p><p>Examples of features derived from raw data include computing average values, median values, variances, sums, differences, maximums or minimums, and counts. We have already seen a case of this when we created a new <code class="literal">movie age</code> feature from the year of release of the movie and the current year. Often, the idea behind using these transformations is to summarize the numerical data in some way that might make it easier for a model to learn.</p><p>It is also <a id="id266" class="indexterm"></a>common to transform numerical features into categorical <a id="id267" class="indexterm"></a>features, for example, by binning features. Common examples of this include variables such as age, geolocation, and time.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec04"></a>Transforming timestamps into categorical features</h4></div></div></div><p>To <a id="id268" class="indexterm"></a>illustrate how to<a id="id269" class="indexterm"></a> derive categorical features <a id="id270" class="indexterm"></a>from numerical data, we will use the times of the ratings given by users to movies. These are in the form of Unix timestamps. We can use Python's <code class="literal">datetime</code> module to extract the date and time from the timestamp and, in turn, extract the <code class="literal">hour</code> of the day. This will result in an RDD of the hour of the day for each rating.</p><p>We will need a function to extract a <code class="literal">datetime</code> representation of the rating timestamp (in seconds); we will create this function now:</p><div class="informalexample"><pre class="programlisting">def extract_datetime(ts):
    import datetime
    return datetime.datetime.fromtimestamp(ts)</pre></div><p>We will again use the <code class="literal">rating_data</code> RDD that we computed in the earlier examples as our starting point.</p><p>First, we will use a <code class="literal">map</code> transformation to extract the timestamp field, converting it to a Python <code class="literal">int</code> datatype. We will then apply our <code class="literal">extract_datetime</code> function to each timestamp and extract the hour from the resulting <code class="literal">datetime</code> object:</p><div class="informalexample"><pre class="programlisting">timestamps = rating_data.map(lambda fields: int(fields[3]))
hour_of_day = timestamps.map(lambda ts: extract_datetime(ts).<span class="strong"><strong>hour</strong></span>)
hour_of_day.take(5)</pre></div><p>If we take the first five records of the resulting RDD, we will see the following output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[17, 21, 9, 7, 7]</strong></span>
</pre></div><p>We have transformed the raw time data into a categorical feature that represents the hour of the day in which the rating was given.</p><p>Now, say that we decide this is too coarse a representation. Perhaps we want to further refine the transformation. We can assign each hour-of-the-day value into a defined bucket that represents a time of day.</p><p>For<a id="id271" class="indexterm"></a> example, we can <a id="id272" class="indexterm"></a>say that morning is from 7 a.m. to 11 a.m., while lunch is from 11 a.m. to 1 a.m., and so on. Using these buckets, we <a id="id273" class="indexterm"></a>can create a function to assign a time of day, given the hour of the day as input:</p><div class="informalexample"><pre class="programlisting">def assign_tod(hr):
  times_of_day = {
    'morning' : range(7, 12),
    'lunch' : range(12, 14),
    'afternoon' : range(14, 18),
    'evening' : range(18, 23),
    'night' : range(23, 7)
  }
  for k, v in times_of_day.iteritems():
    if hr in v: 
      return k</pre></div><p>Now, we will apply the <code class="literal">assign_tod</code> function to the hour of each rating event contained in the <code class="literal">hour_of_day</code> RDD:</p><div class="informalexample"><pre class="programlisting">time_of_day = hour_of_day.map(lambda hr: assign_tod(hr))
time_of_day.take(5)</pre></div><p>If we again take the first five records of this new RDD, we will see the following transformed values:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>['afternoon', 'evening', 'morning', 'morning', 'morning']</strong></span>
</pre></div><p>We have now transformed the timestamp variable (which can take on thousands of values and is probably not useful to a model in its raw form) into hours (taking on 24 values) and then into a time of day (taking on five possible values). Now that we have a categorical feature, we can use the same 1-of-k encoding method outlined earlier to generate a binary feature vector.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec30"></a>Text features</h3></div></div></div><p>In some <a id="id274" class="indexterm"></a>ways, text features are a form of categorical and derived<a id="id275" class="indexterm"></a> features. Let's take the example of the description for a movie (which we do not have in our dataset). Here, the raw text could not be used directly, even as a categorical feature, since there are virtually unlimited possible combinations of words that could occur if each piece of text was a possible value. Our model would almost never see two occurrences of the same feature and would not be able to learn effectively. Therefore, we would like to turn raw text into a form that is more amenable to machine learning.</p><p>There are numerous ways of dealing with text, and the field of natural language processing is dedicated to processing, representing, and modeling textual content. A full treatment is beyond the scope of this book, but we will introduce a simple and standard approach for text-feature extraction; this approach is known as the <span class="strong"><strong>bag-of-words</strong></span> representation.</p><p>The bag-of-words approach treats a piece of text content as a set of the words, and possibly numbers, in the text (these are often referred to as terms). The process of the bag-of-words approach is as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><span class="strong"><strong>Tokenization</strong></span>: First, some form of tokenization is applied to the text to split it into a set of tokens (generally words, numbers, and so on). An example of this is simple whitespace tokenization, which splits the text on each space and might remove punctuation and other characters that are not alphabetical or numerical.</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Stop word removal</strong></span>: Next, it is usual to remove very common words such as "the", "and", and "but" (these are known as <span class="strong"><strong>stop words</strong></span>).</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Stemming</strong></span>: The next step can include stemming, which refers to taking a term and reducing it to its base form or stem. A common example is plural terms becoming singular (for example, dogs becomes dog and so on). There are many approaches to stemming, and text-processing libraries often contain various stemming algorithms.</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Vectorization</strong></span>: The final step is turning the processed terms into a vector representation. The simplest form is, perhaps, a binary vector representation, where we assign a value of one if a term exists in the text and zero if it does not. This is essentially identical to the categorical 1-of-k encoding we encountered earlier. Like 1-of-k encoding, this requires a dictionary of terms mapping a given term to an index number. As you might gather, there are potentially millions of individual possible terms (even after stop word removal and stemming). Hence, it becomes critical to use a sparse vector representation <a id="id276" class="indexterm"></a>where only the fact that a term is <a id="id277" class="indexterm"></a>present is stored, to save memory and disk space as well as compute time.</p></li></ul></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note09"></a>Note</h3><p>In <a class="link" href="#" linkend="ch09">Chapter 9</a>, <span class="emphasis"><em>Advanced Text Processing with Spark</em></span>, we will cover more complex text processing and feature extraction, including methods to weight terms; these methods go beyond the basic binary encoding we saw earlier.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec05"></a>Simple text feature extraction</h4></div></div></div><p>To show <a id="id278" class="indexterm"></a>an example of extracting textual features in the binary vector representation, we can use the movie titles that we have available.</p><p>First, we will create a function to strip away the year of release for each movie, if the year is present, leaving only the title of the movie.</p><p>We will use Python's regular expression module, <code class="literal">re</code>, to search for the year between parentheses in the movie titles. If we find a match with this regular expression, we will extract only the title up to the index of the first match (that is, the index in the title string of the opening parenthesis). This is done with the following <code class="literal">raw[:grps.start()]</code> code snippet:</p><div class="informalexample"><pre class="programlisting">def extract_title(raw):
  import re
  # this regular expression finds the non-word (numbers) betweenparentheses
  grps = re.search("\((\w+)\)", raw)
  if grps:
    # we take only the title part, and strip the trailing whitespace from the remaining text, below
    return raw[:grps.start()].strip() 
  else:
    return raw</pre></div><p>Next, we will extract the raw movie titles from the <code class="literal">movie_fields</code> RDD:</p><div class="informalexample"><pre class="programlisting">raw_titles = movie_fields.map(lambda fields: fields[1])</pre></div><p>We can test out our <code class="literal">extract_title</code> function on the first five raw titles as follows:</p><div class="informalexample"><pre class="programlisting">for raw_title in raw_titles.take(5):
  print extract_title(raw_title)</pre></div><p>We can verify that our function works by inspecting the results, which should look like this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Toy Story</strong></span>
<span class="strong"><strong>GoldenEye</strong></span>
<span class="strong"><strong>Four Rooms</strong></span>
<span class="strong"><strong>Get Shorty</strong></span>
<span class="strong"><strong>Copycat</strong></span>
</pre></div><p>We would <a id="id279" class="indexterm"></a>then like to apply our function to the raw titles and apply a tokenization scheme to the extracted titles to convert them to terms. We will use the simple whitespace tokenization we covered earlier:</p><div class="informalexample"><pre class="programlisting">movie_titles = raw_titles.map(lambda m: extract_title(m))
# next we tokenize the titles into terms. We'll use simple whitespace tokenization
title_terms = movie_titles.map(lambda t: t.split(" "))
print title_terms.take(5)</pre></div><p>Applying this simple tokenization gives the following result:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[[u'Toy', u'Story'], [u'GoldenEye'], [u'Four', u'Rooms'], [u'Get', u'Shorty'], [u'Copycat']]</strong></span>
</pre></div><p>We can see that we have split each title on spaces so that each word becomes a token.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip18"></a>Tip</h3><p>Here, we do not cover details such as converting text to lowercase, removing non-word or non-numerical characters such as punctuation and special characters, removing stop words, and stemming. These steps will be important in a real-world application. We will cover many of these topics in <a class="link" href="#" linkend="ch09">Chapter 9</a>, <span class="emphasis"><em>Advanced Text Processing with Spark</em></span>.</p><p>This additional processing can be done fairly simply using string functions, regular expressions, and the Spark API (apart from stemming). Perhaps you would like to give it a try!</p></div><p>In order to assign each term to an index in our vector, we need to create the term dictionary, which maps each term to an integer index.</p><p>First, we will use Spark's <code class="literal">flatMap</code> function (highlighted in the following code snippet) to expand the list of strings in each record of the <code class="literal">title_terms</code> RDD into a new RDD of strings where each record is a term called <code class="literal">all_terms</code>.</p><p>We can then collect all the unique terms and assign indexes in exactly the same way that we did for the 1-of-k encoding of user occupations earlier:</p><div class="informalexample"><pre class="programlisting"># next we would like to collect all the possible terms, in order to build out dictionary of term &lt;-&gt; index mappings
all_terms = title_terms.<span class="strong"><strong>flatMap</strong></span>(lambda x: x).distinct().collect()
# create a new dictionary to hold the terms, and assign the "1-of-k" indexes
idx = 0
all_terms_dict = {}
for term in all_terms:
  all_terms_dict[term] = idx
  idx +=1</pre></div><p>We can<a id="id280" class="indexterm"></a> print out the total number of unique terms and test out our term mapping on a few terms:</p><div class="informalexample"><pre class="programlisting">print "Total number of terms: %d" % len(all_terms_dict)
print "Index of term 'Dead': %d" % all_terms_dict['Dead']
print "Index of term 'Rooms': %d" % all_terms_dict['Rooms']</pre></div><p>This will result in the following output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Total number of terms: 2645</strong></span>
<span class="strong"><strong>Index of term 'Dead': 147</strong></span>
<span class="strong"><strong>Index of term 'Rooms': 1963</strong></span>
</pre></div><p>We can also achieve the same result more efficiently using Spark's <code class="literal">zipWithIndex</code> function. This function takes an RDD of values and merges them together with an index to create a new RDD of key-value pairs, where the key will be the term and the value will be the index in the term dictionary. We will use <code class="literal">collectAsMap</code> to collect the key-value RDD to the driver as a Python <code class="literal">dict</code> method:</p><div class="informalexample"><pre class="programlisting">all_terms_dict2 = title_terms.flatMap(lambda x: x).distinct().zipWithIndex().collectAsMap()
print "Index of term 'Dead': %d" % all_terms_dict2['Dead']
print "Index of term 'Rooms': %d" % all_terms_dict2['Rooms']</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Index of term 'Dead': 147</strong></span>
<span class="strong"><strong>Index of term 'Rooms': 1963 </strong></span>
</pre></div><p>The final step is to create a function that converts a set of terms into a sparse vector representation. To do this, we will create an empty sparse matrix with one row and a number of columns equal to the total number of terms in our dictionary. We will then step through each term in the input list of terms and check whether this term is in our term dictionary. If it is, we assign a value of 1 to the vector at the index that corresponds to the term in our dictionary mapping:</p><div class="informalexample"><pre class="programlisting"># this function takes a list of terms and encodes it as a scipy sparse vector using an approach 
# similar to the 1-of-k encoding
def create_vector(terms, term_dict):
  from scipy import sparse as sp
    num_terms = len(term_dict)
    x = sp.csc_matrix((1, num_terms))  
    for t in terms:
      if t in term_dict:
        idx = term_dict[t]
        x[0, idx] = 1
  return x</pre></div><p>Once we <a id="id281" class="indexterm"></a>have our function, we will apply it to each record in our RDD of extracted terms:</p><div class="informalexample"><pre class="programlisting">all_terms_bcast = sc.<span class="strong"><strong>broadcast</strong></span>(all_terms_dict)
term_vectors = title_terms.map(lambda terms: create_vector(terms, all_terms_bcast.value))
term_vectors.take(5)</pre></div><p>We can then inspect the first few records of our new RDD of sparse vectors:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[&lt;1x2645 sparse matrix of type '&lt;type 'numpy.float64'&gt;'</strong></span>
<span class="strong"><strong>  with 2 stored elements in Compressed Sparse Column format&gt;,</strong></span>
<span class="strong"><strong> &lt;1x2645 sparse matrix of type '&lt;type 'numpy.float64'&gt;'</strong></span>
<span class="strong"><strong>  with 1 stored elements in Compressed Sparse Column format&gt;,</strong></span>
<span class="strong"><strong> &lt;1x2645 sparse matrix of type '&lt;type 'numpy.float64'&gt;'</strong></span>
<span class="strong"><strong>  with 2 stored elements in Compressed Sparse Column format&gt;,</strong></span>
<span class="strong"><strong> &lt;1x2645 sparse matrix of type '&lt;type 'numpy.float64'&gt;'</strong></span>
<span class="strong"><strong>  with 2 stored elements in Compressed Sparse Column format&gt;,</strong></span>
<span class="strong"><strong> &lt;1x2645 sparse matrix of type '&lt;type 'numpy.float64'&gt;'</strong></span>
<span class="strong"><strong>  with 1 stored elements in Compressed Sparse Column format&gt;]</strong></span>
</pre></div><p>We can see that each movie title has now been transformed into a sparse vector. We can see that the titles where we extracted two terms have two non-zero entries in the vector, titles where we extracted only one term have one non-zero entry, and so on.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip19"></a>Tip</h3><p>Note the use of Spark's <code class="literal">broadcast</code> method in the preceding example code to create a broadcast variable that contains the term dictionary. In real-world applications, such term dictionaries can be extremely large, so using a broadcast variable is advisable.</p></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec31"></a>Normalizing features</h3></div></div></div><p>Once the <a id="id282" class="indexterm"></a>features have been extracted into the form<a id="id283" class="indexterm"></a> of a vector, a common preprocessing step is to normalize the numerical data. The idea behind this is to transform each numerical feature in a way that scales it to a standard size. We can perform different kinds of normalization, which are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><span class="strong"><strong>Normalize a feature</strong></span>: This is usually a transformation applied to an individual <a id="id284" class="indexterm"></a>feature across the dataset, for example, subtracting the mean (<span class="emphasis"><em>centering</em></span> the feature) or applying the standard normal transformation (such that the feature has a mean of zero and a standard deviation of 1).</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>Normalize a feature vector</strong></span>: This is usually a transformation applied to all features<a id="id285" class="indexterm"></a> in a given row of the dataset such that the resulting feature vector has a normalized length. That is, we will ensure that each feature in the vector is scaled such that the vector has a norm of 1 (typically, on an L1 or L2 norm).</p></li></ul></div><p>We will use the second case as an example. We can use the <code class="literal">norm</code> function of <code class="literal">numpy</code> to achieve the vector normalization by first computing the L2 norm of a random vector and then dividing each element in the vector by this norm to create our normalized vector:</p><div class="informalexample"><pre class="programlisting">np.random.seed(42)
x = np.random.randn(10)
norm_x_2 = np.linalg.norm(x)
normalized_x = x / norm_x_2
print "x:\n%s" % x
print "2-Norm of x: %2.4f" % norm_x_2
print "Normalized x:\n%s" % normalized_x
print "2-Norm of normalized_x: %2.4f" % np.linalg.norm(normalized_x)</pre></div><p>This should give the following result (note that in the preceding code snippet, we set the random seed equal to 42 so that the result will always be the same):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>x: [ 0.49671415 -0.1382643 0.64768854 1.52302986 -0.23415337 -0.23413696 1.57921282 0.76743473 -0.46947439 0.54256004]</strong></span>
<span class="strong"><strong>2-Norm of x: 2.5908 </strong></span>
<span class="strong"><strong>Normalized x: [ 0.19172213 -0.05336737 0.24999534 0.58786029 -0.09037871 -0.09037237 0.60954584 0.29621508 -0.1812081 0.20941776]</strong></span>
<span class="strong"><strong>2-Norm of normalized_x: 1.0000</strong></span>
</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl3sec06"></a>Using MLlib for feature normalization</h4></div></div></div><p>Spark<a id="id286" class="indexterm"></a> provides some built-in functions for feature <a id="id287" class="indexterm"></a>scaling and standardization in its MLlib machine learning library. These include <code class="literal">StandardScaler</code>, which applies the standard normal transformation, and <code class="literal">Normalizer</code>, which applies the same feature vector normalization we showed you in our preceding example code.</p><p>We will explore the use of these methods in the upcoming chapters, but for now, let's simply compare the results of using MLlib's <code class="literal">Normalizer</code> to our own results:</p><div class="informalexample"><pre class="programlisting">from pyspark.mllib.feature import Normalizer
normalizer = Normalizer()
vector = sc.parallelize([x])</pre></div><p>After importing the required class, we will instantiate <code class="literal">Normalizer</code> (by default, it will use the L2 norm as we did earlier). Note that as in most situations in Spark, we need to provide <code class="literal">Normalizer</code> with an RDD as input (it contains <code class="literal">numpy</code> arrays or MLlib vectors); hence, we will create a single-element RDD from our vector <code class="literal">x</code> for illustrative purposes.</p><p>We will then use the <code class="literal">transform</code> function of <code class="literal">Normalizer</code> on our RDD. Since the RDD only has one vector in it, we will return our vector to the driver by calling <code class="literal">first</code> and finally by calling the <code class="literal">toArray</code> function to convert the vector back into a <code class="literal">numpy</code> array:</p><div class="informalexample"><pre class="programlisting">normalized_x_mllib = normalizer.transform(vector).first().toArray()</pre></div><p>Finally, we can print out the same details as we did previously, comparing the results:</p><div class="informalexample"><pre class="programlisting">print "x:\n%s" % x
print "2-Norm of x: %2.4f" % norm_x_2
print "Normalized x MLlib:\n%s" % normalized_x_mllib
print "2-Norm of normalized_x_mllib: %2.4f" % np.linalg.norm(normalized_x_mllib)</pre></div><p>You will end up with exactly the same normalized vector as we did with our own code. However, using MLlib's built-in methods is certainly more convenient and efficient than writing our own functions!</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl2sec32"></a>Using packages for feature extraction</h3></div></div></div><p>While <a id="id288" class="indexterm"></a>we have covered many different approaches<a id="id289" class="indexterm"></a> to feature extraction, it will be rather painful to have to create the code to perform these common tasks each and every time. Certainly, we can create our own reusable code libraries for this purpose; however, fortunately, we can rely on the existing tools and packages.</p><p>Since Spark supports Scala, Java, and Python bindings, we can use packages available in these languages that provide sophisticated tools to process and extract features and represent them as vectors. A few examples of packages for feature extraction include scikit-learn, gensim, scikit-image, matplotlib, and NLTK in Python; OpenNLP in Java; and Breeze and Chalk in Scala. In fact, Breeze has been part of Spark MLlib since version 1.0, and we will see how to use some Breeze functionality for linear algebra in the later chapters.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch03lvl1sec26"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we saw how to find common, publicly-available datasets that can be used to test various machine learning models. You learned how to load, process, and clean data, as well as how to apply common techniques to transform raw data into feature vectors that can be used as training examples for our models.</p><p>In the next chapter, you will learn the basics of recommender systems and explore how to create a recommendation model, use the model to make predictions, and evaluate the model.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch04"></a>ChapterÂ 4.Â Building a Recommendation Engine with Spark</h2></div></div></div><p>Now that you have learned the basics of data processing and feature extraction, we will move on to explore individual machine learning models in detail, starting with recommendation engines.</p><p>Recommendation engines are probably among the best types of machine learning model known to the general public. Even if people do not know exactly what a recommendation engine is, they have most likely experienced one through the use of popular websites such as Amazon, Netflix, YouTube, Twitter, LinkedIn, and Facebook. Recommendations are a core part of all these businesses, and in some cases, they drive significant percentages of their revenue.</p><p>The idea behind recommendation engines is to predict what people might like and to uncover relationships between items to aid in the discovery process (in this way, it is similar and, in fact, often complementary to search engines, which also play a role in discovery). However, unlike search engines, recommendation engines try to present people with relevant content that they did not necessarily search for or that they might not even have heard of.</p><p>Typically, a recommendation engine tries to model the connections between users and some type of item. In our MovieStream scenario from <a class="link" href="#" linkend="ch02">Chapter 2</a>, <span class="emphasis"><em>Designing a Machine Learning System</em></span>, for example, we could use a recommendation engine to show our users movies that they might enjoy. If we can do this well, we could keep our users engaged using our service, which is good for both our users and us. Similarly, if we can do a good job of showing our users movies related to a given movie, we could aid in discovery and navigation on our site, again improving our users' experience, engagement, and the relevance of our content to them.</p><p>However, recommendation engines are not limited to movies, books, or products. The techniques we will explore in this chapter can be applied to just about any user-to-item relationship as well as user-to-user connections, such as those found on social networks, allowing us to make recommendations such as people you may know or who to follow.</p><p>Recommendation engines are most effective in two general scenarios (which are not mutually exclusive). They <a id="id290" class="indexterm"></a>are explained here:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><span class="strong"><strong>Large number of available options for users</strong></span>: When there are a very large number of available items, it becomes increasingly difficult for the user to find something they want. Searching can help when the user knows what they are looking for, but often, the right item might be something previously unknown to them. In this case, being recommended relevant items, that the user may not already know about, can help them discover new items.</p></li><li style="list-style-type: disc"><p><span class="strong"><strong>A significant degree of personal taste involved</strong></span>: When personal taste plays a large role in selection, recommendation models, which often utilize a wisdom of the crowd approach, can be helpful in discovering items based on the behavior of others that have similar taste profiles.</p></li></ul></div><p>In this chapter, we will:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Introduce the various types of recommendation engines</p></li><li style="list-style-type: disc"><p>Build a recommendation model using data about user preferences</p></li><li style="list-style-type: disc"><p>Use the trained model to compute recommendations for a given user as well compute similar items for a given item (that is, related items)</p></li><li style="list-style-type: disc"><p>Apply standard evaluation metrics to the model that we created to measure how well it performs in terms of predictive capability</p></li></ul></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec27"></a>Types of recommendation models</h2></div></div><hr /></div><p>Recommender <a id="id291" class="indexterm"></a>systems are widely studied, and there are <a id="id292" class="indexterm"></a>many approaches used, but there are two that are probably most prevalent: content-based filtering and collaborative filtering. Recently, other approaches such as ranking models have also gained in popularity. In practice, many approaches are hybrids, incorporating elements of many different methods into a model or combination of models.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec33"></a>Content-based filtering</h3></div></div></div><p>Content-based <a id="id293" class="indexterm"></a>methods try to use<a id="id294" class="indexterm"></a> the content or attributes of an item, together with some notion of similarity between two pieces of content, to generate items similar to a given item. These attributes are often textual content (such as titles, names, tags, and other metadata attached to an item), or in the case of media, they could include other features of the item, such as attributes extracted from audio and video content.</p><p>In a <a id="id295" class="indexterm"></a>similar manner, user recommendations <a id="id296" class="indexterm"></a>can be generated based on attributes of users or user profiles, which are then matched to item attributes using the same measure of similarity. For example, a user can be represented by the combined attributes of the items they have interacted with. This becomes their user profile, which is then compared to item attributes to find items that match the user profile.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec34"></a>Collaborative filtering</h3></div></div></div><p>Collaborative <a id="id297" class="indexterm"></a>filtering is a form of wisdom<a id="id298" class="indexterm"></a> of the crowd approach where the set of preferences of many users with respect to items is used to generate estimated preferences of users for items with which they have not yet interacted. The idea behind this is the notion of similarity.</p><p>In a user-based approach, if two users have exhibited similar preferences (that is, patterns of interacting with the same items in broadly the same way), then we would assume that they are similar to each other in terms of taste. To generate recommendations for unknown items for a given user, we can use the known preferences of other users that exhibit similar behavior. We can do this by selecting a set of similar users and computing some form of combined score based on the items they have shown a preference for. The overall logic is that if others have tastes similar to a set of items, these items would tend to be good candidates for recommendation.</p><p>We can also take an item-based approach that computes some measure of similarity between items. This is usually based on the existing user-item preferences or ratings. Items that tend to be rated the same by similar users will be classed as similar under this approach. Once we have these similarities, we can represent a user in terms of the items they have interacted with and find items that are similar to these known items, which we can then recommend to the user. Again, a set of items similar to the known items is used to generate a combined score to estimate for an unknown item.</p><p>The user- and item-based approaches are usually referred to as nearest-neighbor models, since the estimated scores are computed based on the set of most similar users or items (that is, their neighbors).</p><p>Finally, there are many model-based methods that attempt to model the user-item preferences themselves so that new preferences can be estimated directly by applying the model <a id="id299" class="indexterm"></a>to <a id="id300" class="indexterm"></a>unknown user-item combinations.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl3sec07"></a>Matrix factorization</h4></div></div></div><p>Since <a id="id301" class="indexterm"></a>Spark's recommendation models <a id="id302" class="indexterm"></a>currently only include an implementation of matrix factorization, we will focus our attention on this class of models. This focus is with good reason; however, these types of models have consistently been shown to perform extremely well in collaborative filtering and were among the best models in well-known competitions such as the Netflix prize.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note10"></a>Note</h3><p>For more information on and a brief overview of the performance of the best algorithms for the Netflix prize, see <a class="ulink" href="http://techblog.netflix.com/2012/04/netflix-recommendations-beyond-5-stars.html" target="_blank">http://techblog.netflix.com/2012/04/netflix-recommendations-beyond-5-stars.html</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch04lvl4sec01"></a>Explicit matrix factorization</h5></div></div></div><p>When we deal with data that consists of preferences of users that are provided by the users themselves, we refer to explicit preference data. This includes, for example, ratings, thumbs <a id="id303" class="indexterm"></a>up, likes, and so on that are given <a id="id304" class="indexterm"></a>by users to items.</p><p>We can take these ratings and form a two-dimensional matrix with users as rows and items as columns. Each entry represents a rating given by a user to a certain item. Since in most cases, each user has only interacted with a relatively small set of items, this matrix has only a few non-zero entries (that is, it is very sparse).</p><p>As a simple example, let's assume that we have the following user ratings for a set of movies:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Tom, Star Wars, 5</strong></span>
<span class="strong"><strong>Jane, Titanic, 4</strong></span>
<span class="strong"><strong>Bill, Batman, 3</strong></span>
<span class="strong"><strong>Jane, Star Wars, 2</strong></span>
<span class="strong"><strong>Bill, Titanic, 3</strong></span>
</pre></div><p>We will form the following ratings matrix:</p><div class="mediaobject"><img src="graphics/8519OS_04_01.jpg" /><div class="caption"><p>A simple movie-rating matrix</p></div></div><p>Matrix<a id="id305" class="indexterm"></a> factorization (or matrix completion) attempts <a id="id306" class="indexterm"></a>to directly model this user-item matrix by representing it as a product of two smaller matrices of lower dimension. Thus, it is a dimensionality-reduction technique. If we have <span class="strong"><strong>U</strong></span> users and <span class="strong"><strong>I</strong></span> items, then our user-item matrix is of dimension U x I and might look something like the one shown in the following diagram:</p><div class="mediaobject"><img src="graphics/8519OS_04_02.jpg" /><div class="caption"><p>A sparse ratings matrix</p></div></div><p>If we want to find a lower dimension (low-rank) approximation to our user-item matrix with <a id="id307" class="indexterm"></a>the dimension <span class="strong"><strong>k</strong></span>, we would end up<a id="id308" class="indexterm"></a> with two matrices: one for users of size U x k and one for items of size I x k. These are known as factor matrices. If we multiply these two factor matrices, we would reconstruct an approximate version of the original ratings matrix. Note that while the original ratings matrix is typically very sparse, each factor matrix is dense, as shown in the following diagram:</p><div class="mediaobject"><img src="graphics/8519OS_04_03.jpg" /><div class="caption"><p>The user- and item-factor matrices</p></div></div><p>These models are often also called latent feature models, as we are trying to discover some form<a id="id309" class="indexterm"></a> of hidden features (which are represented by the factor matrices) that account for the structure of behavior inherent in the user-item rating matrix. While the latent features or factors are not directly interpretable, they might, perhaps, represent things such as the tendency of a user to like movies from a certain director, genre, style, or group of actors, for example.</p><p>As we are directly modeling the user-item matrix, the prediction in these models is relatively straightforward: to compute a predicted rating for a user and item, we compute the vector dot product between the relevant row of the user-factor matrix (that is, the user's factor vector) and the relevant row of the item-factor matrix (that is, the item's factor vector).</p><p>This is illustrated with the highlighted vectors in the following diagram:</p><div class="mediaobject"><img src="graphics/8519OS_04_04.jpg" /><div class="caption"><p>Computing recommendations from user- and item-factor vectors</p></div></div><p>To <a id="id310" class="indexterm"></a>find out the similarity between<a id="id311" class="indexterm"></a> two items, we can use the same measures of similarity as we would use in the nearest-neighbor models, except that we can use the factor vectors directly by computing the similarity between two item-factor vectors, as illustrated in the following diagram:</p><div class="mediaobject"><img src="graphics/8519OS_04_05.jpg" /><div class="caption"><p>Computing similarity with item-factor vectors</p></div></div><p>The <a id="id312" class="indexterm"></a>benefit of factorization models is<a id="id313" class="indexterm"></a> the relative ease of computing recommendations once the model is created. However, for very large user and itemsets, this can become a challenge as it requires storage and computation across potentially many millions of user- and item-factor vectors. Another advantage, as mentioned earlier, is that they tend to offer very good performance.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note11"></a>Note</h3><p>Projects such<a id="id314" class="indexterm"></a> as Oryx (<a class="ulink" href="https://github.com/OryxProject/oryx" target="_blank">https://github.com/OryxProject/oryx</a>) and Prediction.io (<a class="ulink" href="https://github.com/PredictionIO/PredictionIO" target="_blank">https://github.com/PredictionIO/PredictionIO</a>) focus on model serving <a id="id315" class="indexterm"></a>for large-scale models, including recommenders based on matrix factorization.</p></div><p>On the down side, factorization models are relatively more complex to understand and interpret compared to nearest-neighbor models and are often more computationally intensive during the model's training phase.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch04lvl4sec02"></a>Implicit matrix factorization</h5></div></div></div><p>So far, we have dealt with explicit preferences such as ratings. However, much of the preference data that we might be able to collect is implicit feedback, where the preferences between a user and item are not given to us, but are, instead, implied from the interactions they might have with an item. Examples include binary data (such as whether a user <a id="id316" class="indexterm"></a>viewed a movie, whether they <a id="id317" class="indexterm"></a>purchased a product, and so on) as well as count data (such as the number of times a user watched a movie).</p><p>There are many different approaches to deal with implicit data. MLlib implements a particular approach that treats the input rating matrix as two matrices: a binary preference matrix, <span class="strong"><strong>P</strong></span>, and a matrix of confidence weights, <span class="strong"><strong>C</strong></span>.</p><p>For example, let's assume that the user-movie ratings we saw previously were, in fact, the number of times each user had viewed that movie. The two matrices would look something like ones shown in the following screenshot. Here, the matrix <span class="strong"><strong>P</strong></span> informs us that a movie was viewed by a user, and the matrix <span class="strong"><strong>C</strong></span> represents the confidence weighting, in the form of the view countsâ€”generally, the more a user has watched a movie, the higher the confidence that they actually like it.</p><div class="mediaobject"><img src="graphics/8519OS_04_06.jpg" /><div class="caption"><p>Representation of an implicit preference and confidence matrix</p></div></div><p>The implicit model still creates a user- and item-factor matrix. In this case, however, the matrix that the model is attempting to approximate is not the overall ratings matrix but the preference matrix P. If we compute a recommendation by calculating the dot product of a user- and item-factor vector, the score will not be an estimate of a rating directly. It will rather be an estimate of the preference of a user for an item (though not strictly between 0 and 1, these scores will generally be fairly close to a scale of 0 to 1).</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch04lvl4sec03"></a>Alternating least squares</h5></div></div></div><p><span class="strong"><strong>Alternating Least Squares</strong></span> (<span class="strong"><strong>ALS</strong></span>) is an optimization technique to solve matrix factorization<a id="id318" class="indexterm"></a> problems; this <a id="id319" class="indexterm"></a>technique is powerful, achieves good performance, and has proven to be relatively easy to implement in a parallel fashion. Hence, it is well suited for platforms such as Spark. At the time of writing this book, it is the only recommendation model implemented in MLlib.</p><p>ALS works by iteratively solving a series of least squares regression problems. In each iteration, one of the user- or item-factor matrices is treated as fixed, while the other one is updated using the fixed factor and the rating data. Then, the factor matrix that was solved for is, in turn, treated as fixed, while the other one is updated. This process continues until the model has converged (or for a fixed number of iterations).</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note12"></a>Note</h3><p>Spark's documentation for collaborative filtering contains references to the papers that underlie the ALS algorithms implemented each component of explicit and implicit data. You can view the documentation at <a class="ulink" href="http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html" target="_blank">http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html</a>.</p></div></div></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec28"></a>Extracting the right features from your data</h2></div></div><hr /></div><p>In this <a id="id320" class="indexterm"></a>section, we will use explicit rating data, without additional user or item metadata or other information related to the user-item interactions. Hence, the features that we need as inputs are simply the user IDs, movie IDs, and the ratings assigned to each user and movie pair.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec35"></a>Extracting features from the MovieLens 100k dataset</h3></div></div></div><p>Start <a id="id321" class="indexterm"></a>the Spark shell in the Spark <a id="id322" class="indexterm"></a>base directory, ensuring that you provide enough memory via the <code class="literal">â€“driver-memory</code> option:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;./bin/spark-shell â€“driver-memory 4g</strong></span>
</pre></div><p>In this example, we will use the same MovieLens dataset that we used in the previous chapter. Use the directory in which you placed the MovieLens 100k dataset as the input path in the following code.</p><p>First, let's inspect the raw ratings dataset:</p><div class="informalexample"><pre class="programlisting">val rawData = sc.textFile("/<span class="strong"><strong>PATH</strong></span>/ml-100k/u.data")
rawData.first()</pre></div><p>You will see output similar to these lines of code:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>14/03/30 11:42:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</strong></span>
<span class="strong"><strong>14/03/30 11:42:41 WARN LoadSnappy: Snappy native library not loaded</strong></span>
<span class="strong"><strong>14/03/30 11:42:41 INFO FileInputFormat: Total input paths to process : 1</strong></span>
<span class="strong"><strong>14/03/30 11:42:41 INFO SparkContext: Starting job: first at &lt;console&gt;:15</strong></span>
<span class="strong"><strong>14/03/30 11:42:41 INFO DAGScheduler: Got job 0 (first at &lt;console&gt;:15) with 1 output partitions (allowLocal=true)</strong></span>
<span class="strong"><strong>14/03/30 11:42:41 INFO DAGScheduler: Final stage: Stage 0 (first at &lt;console&gt;:15)</strong></span>
<span class="strong"><strong>14/03/30 11:42:41 INFO DAGScheduler: Parents of final stage: List()</strong></span>
<span class="strong"><strong>14/03/30 11:42:41 INFO DAGScheduler: Missing parents: List()</strong></span>
<span class="strong"><strong>14/03/30 11:42:41 INFO DAGScheduler: Computing the requested partition locally</strong></span>
<span class="strong"><strong>14/03/30 11:42:41 INFO HadoopRDD: Input split: file:/Users/Nick/workspace/datasets/ml-100k/u.data:0+1979173</strong></span>
<span class="strong"><strong>14/03/30 11:42:41 INFO SparkContext: Job finished: first at &lt;console&gt;:15, took 0.030533 s</strong></span>
<span class="strong"><strong>res0: String = 196  242  3  881250949</strong></span>
</pre></div><p>Recall <a id="id323" class="indexterm"></a>that this dataset consisted <a id="id324" class="indexterm"></a>of the <code class="literal">user id</code>, <code class="literal">movie id</code>, <code class="literal">rating</code>, <code class="literal">timestamp</code> fields separated by a tab (<code class="literal">"\t"</code>) character. We don't need the time when the rating was made to train our model, so let's simply extract the first three fields:</p><div class="informalexample"><pre class="programlisting">val rawRatings = rawData.map(_.split("\t").<span class="strong"><strong>take</strong></span>(3))</pre></div><p>We will first split each record on the <code class="literal">"\t"</code> character, which gives us an <code class="literal">Array[String]</code> array. We will then use Scala's <code class="literal">take</code> function to keep only the first <code class="literal">3</code> elements of the array, which correspond to <code class="literal">user id</code>, <code class="literal">movie id</code>, and <code class="literal">rating</code>, respectively.</p><p>We can inspect the first record of our new RDD by calling <code class="literal">rawRatings.first()</code>, which collects just the first record of the RDD back to the driver program. This will result in the following output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>14/03/30 12:24:00 INFO SparkContext: Starting job: first at &lt;console&gt;:21</strong></span>
<span class="strong"><strong>14/03/30 12:24:00 INFO DAGScheduler: Got job 1 (first at &lt;console&gt;:21) with 1 output partitions (allowLocal=true)</strong></span>
<span class="strong"><strong>14/03/30 12:24:00 INFO DAGScheduler: Final stage: Stage 1 (first at &lt;console&gt;:21)</strong></span>
<span class="strong"><strong>14/03/30 12:24:00 INFO DAGScheduler: Parents of final stage: List()</strong></span>
<span class="strong"><strong>14/03/30 12:24:00 INFO DAGScheduler: Missing parents: List()</strong></span>
<span class="strong"><strong>14/03/30 12:24:00 INFO DAGScheduler: Computing the requested partition locally</strong></span>
<span class="strong"><strong>14/03/30 12:24:00 INFO HadoopRDD: Input split: file:/Users/Nick/workspace/datasets/ml-100k/u.data:0+1979173</strong></span>
<span class="strong"><strong>14/03/30 12:24:00 INFO SparkContext: Job finished: first at &lt;console&gt;:21, took 0.00391 s</strong></span>
<span class="strong"><strong>res6: Array[String] = Array(196, 242, 3)</strong></span>
</pre></div><p>We <a id="id325" class="indexterm"></a>will use Spark's MLlib library <a id="id326" class="indexterm"></a>to train our model. Let's take a look at what methods are available for us to use and what input is required. First, import the <code class="literal">ALS</code> model from MLlib:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark.mllib.recommendation.ALS</pre></div><p>On the console, we can inspect the available methods on the ALS object using tab completion. Type in <code class="literal">ALS.</code> (note the dot) and then press the <span class="emphasis"><em>Tab</em></span> key. You should see the autocompletion of the methods:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>ALS.</strong></span>
<span class="strong"><strong>asInstanceOf    isInstanceOf    main            toString        train           trainImplicit</strong></span>
</pre></div><p>The method we want to use is <code class="literal">train</code>. If we type <code class="literal">ALS.train</code> and hit <span class="emphasis"><em>Enter</em></span>, we will get an error. However, this error will tell us what the method signature looks like:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>ALS.train</strong></span>
<span class="strong"><strong>&lt;console&gt;:12: error: ambiguous reference to overloaded definition,</strong></span>
<span class="strong"><strong>both method train in object ALS of type (ratings: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating], rank: Int</strong></span>
<span class="strong"><strong>, iterations: Int)org.apache.spark.mllib.recommendation.MatrixFactorizationModel</strong></span>
<span class="strong"><strong>and  method train in object ALS of type (ratings: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating], rank: Int, iterations: Int, lambda: Double)org.apache.spark.mllib.recommendation.MatrixFactorizationModel</strong></span>
<span class="strong"><strong>match expected type ?</strong></span>
<span class="strong"><strong>              ALS.train</strong></span>
<span class="strong"><strong>                  ^</strong></span>
</pre></div><p>So, we can see that at a minimum, we need to provide the input arguments, <code class="literal">ratings</code>, <code class="literal">rank</code>, and <code class="literal">iterations</code>. The second method also requires an argument called <code class="literal">lambda</code>. We'll cover these three shortly, but let's take a look at the <code class="literal">ratings</code> argument. First, let's import the <code class="literal">Rating</code> class that it references and use a similar approach to find out what an instance of <code class="literal">Rating</code> requires, by typing in <code class="literal">Rating()</code> and hitting <span class="emphasis"><em>Enter</em></span>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>import org.apache.spark.mllib.recommendation.Rating</strong></span>
<span class="strong"><strong>Rating()</strong></span>
<span class="strong"><strong>&lt;console&gt;:13: error: not enough arguments for method apply: (user: Int, product: Int, rating: Double)org.apache.spark.mllib.recommendation.Rating in object Rating.</strong></span>
<span class="strong"><strong>Unspecified value parameters user, product, rating.</strong></span>
<span class="strong"><strong>              Rating()</strong></span>
<span class="strong"><strong>                    ^</strong></span>
</pre></div><p>As we<a id="id327" class="indexterm"></a> can see from the preceding <a id="id328" class="indexterm"></a>output, we need to provide the <code class="literal">ALS</code> model with an RDD that consists of <code class="literal">Rating</code> records. A <code class="literal">Rating</code> class, in turn, is just a wrapper around <code class="literal">user id</code>, <code class="literal">movie id</code> (called <code class="literal">product</code> here), and the actual <code class="literal">rating</code> arguments. We'll create our rating dataset using the <code class="literal">map</code> method and transforming the array of IDs and ratings into a <code class="literal">Rating</code> object:</p><div class="informalexample"><pre class="programlisting">val ratings = rawRatings.map { case Array(user, movie, rating) =&gt; Rating(user.<span class="strong"><strong>toInt</strong></span>, movie.<span class="strong"><strong>toInt</strong></span>, rating.<span class="strong"><strong>toDouble</strong></span>) }</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note13"></a>Note</h3><p>Notice that we need to use <code class="literal">toInt</code> or <code class="literal">toDouble</code> to convert the raw rating data (which was extracted as <code class="literal">Strings</code> from the text file) to <code class="literal">Int</code> or <code class="literal">Double</code> numeric inputs. Also, note the use of a <code class="literal">case</code> statement that allows us to extract the relevant variable names and use them directly (this saves us from having to use something like <code class="literal">val user = ratings(0)</code>).</p><p>For more on Scala case statements and pattern matching as used here, take a look at <a class="ulink" href="http://docs.scala-lang.org/tutorials/tour/pattern-matching.html" target="_blank">http://docs.scala-lang.org/tutorials/tour/pattern-matching.html</a>.</p></div><p>We now have an <code class="literal">RDD[Rating]</code> that we can verify by calling:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>ratings.first()</strong></span>
<span class="strong"><strong>14/03/30 12:32:48 INFO SparkContext: Starting job: first at &lt;console&gt;:24</strong></span>
<span class="strong"><strong>14/03/30 12:32:48 INFO DAGScheduler: Got job 2 (first at &lt;console&gt;:24) with 1 output partitions (allowLocal=true)</strong></span>
<span class="strong"><strong>14/03/30 12:32:48 INFO DAGScheduler: Final stage: Stage 2 (first at &lt;console&gt;:24)</strong></span>
<span class="strong"><strong>14/03/30 12:32:48 INFO DAGScheduler: Parents of final stage: List()</strong></span>
<span class="strong"><strong>14/03/30 12:32:48 INFO DAGScheduler: Missing parents: List()</strong></span>
<span class="strong"><strong>14/03/30 12:32:48 INFO DAGScheduler: Computing the requested partition locally</strong></span>
<span class="strong"><strong>14/03/30 12:32:48 INFO HadoopRDD: Input split: file:/Users/Nick/workspace/datasets/ml-100k/u.data:0+1979173</strong></span>
<span class="strong"><strong>14/03/30 12:32:48 INFO SparkContext: Job finished: first at &lt;console&gt;:24, took 0.003752 s</strong></span>
<span class="strong"><strong>res8: org.apache.spark.mllib.recommendation.Rating = Rating(196,242,3.0)</strong></span>
</pre></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec29"></a>Training the recommendation model</h2></div></div><hr /></div><p>Once <a id="id329" class="indexterm"></a>we have extracted these simple features from our raw data, we are ready to proceed with model training; MLlib takes care of this for us. All we have to do is provide the correctly-parsed input RDD we just created as well as our chosen model parameters.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec36"></a>Training a model on the MovieLens 100k dataset</h3></div></div></div><p>We're <a id="id330" class="indexterm"></a>now ready<a id="id331" class="indexterm"></a> to train our model! The other inputs required for our model are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><code class="literal">rank</code>: This <a id="id332" class="indexterm"></a>refers to the number of factors in our ALS model, that is, the number of hidden features in our low-rank approximation matrices. Generally, the greater the number of factors, the better, but this has a direct impact on memory usage, both for computation and to store models for serving, particularly for large number of users or items. Hence, this is often a trade-off in real-world use cases. A rank in the range of 10 to 200 is usually reasonable.</p></li><li style="list-style-type: disc"><p><code class="literal">iterations</code>: This refers to the number of iterations to run. While each iteration <a id="id333" class="indexterm"></a>in <code class="literal">ALS</code> is guaranteed to decrease the reconstruction error of the ratings matrix, <code class="literal">ALS</code> models will converge to a reasonably good solution after relatively few iterations. So, we don't need to run for too many iterations in most cases (around 10 is often a good default).</p></li><li style="list-style-type: disc"><p><code class="literal">lambda</code>: This<a id="id334" class="indexterm"></a> parameter controls the regularization of our model. Thus, <code class="literal">lambda</code> controls over fitting. The higher the value of <code class="literal">lambda</code>, the more is the regularization applied. What constitutes a sensible value is very dependent on the size, nature, and sparsity of the underlying data, and as with almost all machine learning models, the regularization parameter is something that should be tuned using out-of-sample test data and cross-validation approaches.</p></li></ul></div><p>We'll use <code class="literal">rank</code> of <code class="literal">50</code>, <code class="literal">10</code> iterations, and a lambda parameter of <code class="literal">0.01</code> to illustrate how to train our model:</p><div class="informalexample"><pre class="programlisting">val model = ALS.train(ratings, 50, 10, 0.01)</pre></div><p>This <a id="id335" class="indexterm"></a>returns a <code class="literal">MatrixFactorizationModel</code> object, which contains the user and item factors in the form <a id="id336" class="indexterm"></a>of an RDD of <code class="literal">(id, factor)</code> pairs. These are called <code class="literal">userFeatures</code> and <code class="literal">productFeatures</code>, respectively. For example:</p><div class="informalexample"><pre class="programlisting">model.userFeatures</pre></div><p>You will see the output as:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>res14: org.apache.spark.rdd.RDD[(Int, Array[Double])] = FlatMappedRDD[659] at flatMap at ALS.scala:231</strong></span>
</pre></div><p>We can see that the factors are in the form of an <code class="literal">Array[Double]</code>.</p><p>Note that the operations used in MLlib's <code class="literal">ALS</code> implementation are lazy transformations, so the actual computation will only be performed once we call some sort of action on the resulting <code class="literal">RDDs</code> of the user and item factors. We can force the computation using a Spark action such as <code class="literal">count</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>model.userFeatures.count</strong></span>
</pre></div><p>This will trigger the computation, and we will see a quite a bit of output text similar to the following lines of code:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>14/03/30 13:10:40 INFO SparkContext: Starting job: count at &lt;console&gt;:26</strong></span>
<span class="strong"><strong>14/03/30 13:10:40 INFO DAGScheduler: Registering RDD 665 (map at ALS.scala:147)</strong></span>
<span class="strong"><strong>14/03/30 13:10:40 INFO DAGScheduler: Registering RDD 664 (map at ALS.scala:146)</strong></span>
<span class="strong"><strong>14/03/30 13:10:40 INFO DAGScheduler: Registering RDD 674 (mapPartitionsWithIndex at ALS.scala:164)</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>14/03/30 13:10:45 INFO SparkContext: Job finished: count at &lt;console&gt;:26, took 5.068255 s</strong></span>
<span class="strong"><strong>res16: Long = 943</strong></span>
</pre></div><p>If we call <code class="literal">count</code> for the movie factors, we will see the following output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>model.productFeatures.count</strong></span>
<span class="strong"><strong>14/03/30 13:15:21 INFO SparkContext: Starting job: count at &lt;console&gt;:26</strong></span>
<span class="strong"><strong>14/03/30 13:15:21 INFO DAGScheduler: Got job 10 (count at &lt;console&gt;:26) with 1 output partitions (allowLocal=false)</strong></span>
<span class="strong"><strong>14/03/30 13:15:21 INFO DAGScheduler: Final stage: Stage 165 (count at &lt;console&gt;:26)</strong></span>
<span class="strong"><strong>14/03/30 13:15:21 INFO DAGScheduler: Parents of final stage: List(Stage 169, Stage 166)</strong></span>
<span class="strong"><strong>14/03/30 13:15:21 INFO DAGScheduler: Missing parents: List()</strong></span>
<span class="strong"><strong>14/03/30 13:15:21 INFO DAGScheduler: Submitting Stage 165 (FlatMappedRDD[883] at flatMap at ALS.scala:231), which has no missing parents</strong></span>
<span class="strong"><strong>14/03/30 13:15:21 INFO DAGScheduler: Submitting 1 missing tasks from Stage 165 (FlatMappedRDD[883] at flatMap at ALS.scala:231)</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>14/03/30 13:15:21 INFO SparkContext: Job finished: count at &lt;console&gt;:26, took 0.030044 s</strong></span>
<span class="strong"><strong>res21: Long = 1682</strong></span>
</pre></div><p>As expected, we have a <a id="id337" class="indexterm"></a>factor array for <a id="id338" class="indexterm"></a>each user (<code class="literal">943</code> factors) and movie (<code class="literal">1682</code> factors).</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl3sec08"></a>Training a model using implicit feedback data</h4></div></div></div><p>The <a id="id339" class="indexterm"></a>standard matrix factorization approach <a id="id340" class="indexterm"></a>in MLlib deals with explicit ratings. To work with implicit data, you can use the <code class="literal">trainImplicit</code> method. It is called in a manner similar to the standard <code class="literal">train</code> method. There is an additional parameter, <code class="literal">alpha</code>, that can be set (and in the same way, the regularization parameter, <code class="literal">lambda</code>, should be selected via testing and cross-validation methods).</p><p>The <code class="literal">alpha</code> parameter<a id="id341" class="indexterm"></a> controls the baseline level of confidence weighting applied. A higher level of <code class="literal">alpha</code> tends to make the model more confident about the fact that missing data equates to no preference for the relevant user-item pair.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note14"></a>Note</h3><p>As an exercise, try to take the existing MovieLens dataset and convert it into an implicit dataset. One possible approach is to convert it to binary feedback (0s and 1s) by applying a threshold on the ratings at some level.</p><p>Another approach could be to convert the ratings' values into confidence weights (for example, perhaps, low ratings could imply zero weights, or even negative weights, which are supported by MLlib's implementation).</p><p>Train a model on this dataset and compare the results of the following section with those generated by your implicit model.</p></div></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec30"></a>Using the recommendation model</h2></div></div><hr /></div><p>Now that<a id="id342" class="indexterm"></a> we have our trained model, we're ready to use it to make predictions. These predictions typically take one of two forms: recommendations for a given user and related or similar items for a given item.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec37"></a>User recommendations</h3></div></div></div><p>In<a id="id343" class="indexterm"></a> this case, we would like to generate<a id="id344" class="indexterm"></a> recommended items for a given user. This usually takes the form of a <span class="emphasis"><em>top-K</em></span> list, that is, the <span class="emphasis"><em>K</em></span> items that our model predicts will have the highest probability of the user liking them. This is done by computing the predicted score for each item and ranking the list based on this score.</p><p>The exact method to perform this computation depends on the model involved. For example, in user-based approaches, the ratings of similar users on items are used to compute the recommendations for a user, while in an item-based approach, the computation is based on the similarity of items the user has rated to the candidate items.</p><p>In matrix factorization, because we are modeling the ratings matrix directly, the predicted score can be computed as the vector dot product between a user-factor vector and an item-factor vector.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl3sec09"></a>Generating movie recommendations from the MovieLens 100k dataset</h4></div></div></div><p>As<a id="id345" class="indexterm"></a> MLlib's recommendation<a id="id346" class="indexterm"></a> model is based on<a id="id347" class="indexterm"></a> matrix factorization, we can use the factor matrices computed by our model to compute predicted scores (or ratings) for a user. We will focus on the explicit rating case using MovieLens data; however, the approach is the same when using the implicit model.</p><p>The <code class="literal">MatrixFactorizationModel</code> class has a convenient <code class="literal">predict</code> method that will compute a predicted score for a given user and item combination:</p><div class="informalexample"><pre class="programlisting">val predictedRating = model.predict(789, 123)</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>14/03/30 16:10:10 INFO SparkContext: Starting job: lookup at MatrixFactorizationModel.scala:45</strong></span>
<span class="strong"><strong>14/03/30 16:10:10 INFO DAGScheduler: Got job 30 (lookup at MatrixFactorizationModel.scala:45) with 1 output partitions (allowLocal=false)</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>14/03/30 16:10:10 INFO SparkContext: Job finished: lookup at MatrixFactorizationModel.scala:46, took 0.023077 s</strong></span>
<span class="strong"><strong>predictedRating: Double = 3.128545693368485</strong></span>
</pre></div><p>As <a id="id348" class="indexterm"></a>we <a id="id349" class="indexterm"></a>can see, this model predicts <a id="id350" class="indexterm"></a>a rating of <code class="literal">3.12</code> for user <code class="literal">789</code> and movie <code class="literal">123</code>.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip20"></a>Tip</h3><p>Note that you might see different results than those shown in this section because the <code class="literal">ALS</code> model is initialized randomly. So, different runs of the model will lead to different solutions.</p></div><p>The <code class="literal">predict</code> method can also take an RDD of <code class="literal">(user, item)</code> IDs as the input and will generate predictions for each of these. We can use this method to make predictions for many users and items at the same time.</p><p>To generate the <span class="emphasis"><em>top-K</em></span> recommended items for a user, <code class="literal">MatrixFactorizationModel</code> provides a convenience method called <code class="literal">recommendProducts</code>. This takes two arguments: <code class="literal">user</code> and <code class="literal">num</code>, where <code class="literal">user</code> is the user ID, and <code class="literal">num</code> is the number of items to recommend.</p><p>It returns the top <code class="literal">num</code> items ranked in the order of the predicted score. Here, the scores are computed as the dot product between the user-factor vector and each item-factor vector.</p><p>Let's generate the top <code class="literal">10</code> recommended items for user <code class="literal">789</code>:</p><div class="informalexample"><pre class="programlisting">val userId = 789
val K = 10
val topKRecs = model.recommendProducts(userId, K)</pre></div><p>We now have a set of predicted ratings for each movie for user <code class="literal">789</code>. If we print this out, we could inspect the top 10 recommendations for this user:</p><div class="informalexample"><pre class="programlisting">println(topKRecs.mkString("\n"))</pre></div><p>You should see the following output on your console:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Rating(789,715,5.931851273771102)</strong></span>
<span class="strong"><strong>Rating(789,12,5.582301095666215)</strong></span>
<span class="strong"><strong>Rating(789,959,5.516272981542168)</strong></span>
<span class="strong"><strong>Rating(789,42,5.458065302395629)</strong></span>
<span class="strong"><strong>Rating(789,584,5.449949837103569)</strong></span>
<span class="strong"><strong>Rating(789,750,5.348768847643657)</strong></span>
<span class="strong"><strong>Rating(789,663,5.30832117499004)</strong></span>
<span class="strong"><strong>Rating(789,134,5.278933936827717)</strong></span>
<span class="strong"><strong>Rating(789,156,5.250959077906759)</strong></span>
<span class="strong"><strong>Rating(789,432,5.169863417126231)</strong></span>
</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch04lvl4sec04"></a>Inspecting the recommendations</h5></div></div></div><p>We <a id="id351" class="indexterm"></a>can give these recommendations a sense check by taking a quick look at the titles of the movies a user has rated and the recommended movies. First, we need to load the movie data (which is the one of the datasets we explored in the previous chapter). We'll collect this data as a <code class="literal">Map[Int, String]</code> method mapping the movie ID to the title:</p><div class="informalexample"><pre class="programlisting">val movies = sc.textFile("/PATH/ml-100k/u.item")
val titles = movies.map(line =&gt; line.split("\\|").take(2)).map(array =&gt; (array(0).toInt,array(1))).collectAsMap()
titles(123)</pre></div><p>The preceding code will produce the output as:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>res68: String = Frighteners, The (1996)</strong></span>
</pre></div><p>For our user <code class="literal">789</code>, we can find out what movies they have rated, take the <code class="literal">10</code> movies with the highest rating, and then check the titles. We will do this now by first using the <code class="literal">keyBy</code> Spark function to create an RDD of key-value pairs from our <code class="literal">ratings</code> RDD, where the key will be the user ID. We will then use the <code class="literal">lookup</code> function to return just the ratings for this key (that is, that particular user ID) to the driver:</p><div class="informalexample"><pre class="programlisting">val moviesForUser = ratings.keyBy(_.user).lookup(789)</pre></div><p>Let's see how many movies this user has rated. This will be the <code class="literal">size</code> of the <code class="literal">moviesForUser</code> collection:</p><div class="informalexample"><pre class="programlisting">println(moviesForUser.size)</pre></div><p>We will see that this user has rated <code class="literal">33</code> movies.</p><p>Next, we will take the 10 movies with the highest ratings by sorting the <code class="literal">moviesForUser</code> collection using the <code class="literal">rating</code> field of the <code class="literal">Rating</code> object. We will then extract the movie title for the relevant product ID attached to the <code class="literal">Rating</code> class from our mapping of movie titles and print out the top <code class="literal">10</code> titles with their ratings:</p><div class="informalexample"><pre class="programlisting">moviesForUser.sortBy(-_.rating).take(10).map(rating =&gt; (titles(rating.product), rating.rating)).foreach(println)</pre></div><p>You <a id="id352" class="indexterm"></a>will see the following output displayed:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>(Godfather, The (1972),5.0)</strong></span>
<span class="strong"><strong>(Trainspotting (1996),5.0)</strong></span>
<span class="strong"><strong>(Dead Man Walking (1995),5.0)</strong></span>
<span class="strong"><strong>(Star Wars (1977),5.0)</strong></span>
<span class="strong"><strong>(Swingers (1996),5.0)</strong></span>
<span class="strong"><strong>(Leaving Las Vegas (1995),5.0)</strong></span>
<span class="strong"><strong>(Bound (1996),5.0)</strong></span>
<span class="strong"><strong>(Fargo (1996),5.0)</strong></span>
<span class="strong"><strong>(Last Supper, The (1995),5.0)</strong></span>
<span class="strong"><strong>(Private Parts (1997),4.0) </strong></span>
</pre></div><p>Now, let's take a look at the top 10 recommendations for this user and see what the titles are using the same approach as the one we used earlier (note that the recommendations are already sorted):</p><div class="informalexample"><pre class="programlisting">topKRecs.map(rating =&gt; (titles(rating.product), rating.rating)).foreach(println)</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>(To Die For (1995),5.931851273771102)</strong></span>
<span class="strong"><strong>(Usual Suspects, The (1995),5.582301095666215)</strong></span>
<span class="strong"><strong>(Dazed and Confused (1993),5.516272981542168)</strong></span>
<span class="strong"><strong>(Clerks (1994),5.458065302395629)</strong></span>
<span class="strong"><strong>(Secret Garden, The (1993),5.449949837103569)</strong></span>
<span class="strong"><strong>(Amistad (1997),5.348768847643657)</strong></span>
<span class="strong"><strong>(Being There (1979),5.30832117499004)</strong></span>
<span class="strong"><strong>(Citizen Kane (1941),5.278933936827717)</strong></span>
<span class="strong"><strong>(Reservoir Dogs (1992),5.250959077906759)</strong></span>
<span class="strong"><strong>(Fantasia (1940),5.169863417126231) </strong></span>
</pre></div><p>We leave it to you to decide whether these recommendations make sense.</p></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec38"></a>Item recommendations</h3></div></div></div><p>Item <a id="id353" class="indexterm"></a>recommendations are about answering <a id="id354" class="indexterm"></a>the following question: for a certain item, what are the items most similar to it? Here, the precise definition of similarity is dependent on the model involved. In most cases, similarity is computed by comparing the vector representation of two items using some similarity measure. Common similarity measures include Pearson correlation and cosine similarity for real-valued vectors and Jaccard similarity for binary vectors.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl3sec10"></a>Generating similar movies for the MovieLens 100k dataset</h4></div></div></div><p>The current <code class="literal">MatrixFactorizationModel</code> API <a id="id355" class="indexterm"></a>does not directly support item-to-item similarity<a id="id356" class="indexterm"></a> computations. Therefore, we will need to create our own code to do this.</p><p>We will use the cosine similarity metric, and we will use the jblas linear algebra library (a dependency of MLlib) to compute the required vector dot products. This is similar to how the existing <code class="literal">predict</code> and <code class="literal">recommendProducts</code> methods work, except that we will use cosine similarity as opposed to just the dot product.</p><p>We would like to compare the factor vector of our chosen item with each of the other items, using our similarity metric. In order to perform linear algebra computations, we will first need to create a vector object out of the factor vectors, which are in the form of an <code class="literal">Array[Double]</code>. The <code class="literal">JBLAS</code> class, <code class="literal">DoubleMatrix</code>, takes an <code class="literal">Array[Double]</code> as the constructor argument as follows:</p><div class="informalexample"><pre class="programlisting">import org.jblas.DoubleMatrix
val aMatrix = new DoubleMatrix(Array(1.0, 2.0, 3.0))</pre></div><p>Here is the output of the preceding code:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>aMatrix: org.jblas.DoubleMatrix = [1.000000; 2.000000; 3.000000]</strong></span>
</pre></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip21"></a>Tip</h3><p>Note that using jblas, vectors are represented as a one-dimensional <code class="literal">DoubleMatrix</code> class, while matrices are a two-dimensional <code class="literal">DoubleMatrix</code> class.</p></div><p>We will need a method to compute the cosine similarity between two vectors. Cosine similarity is a measure of the angle between two vectors in an <span class="emphasis"><em>n</em></span>-dimensional space. It is computed by first calculating the dot product between the vectors and then dividing the result by a denominator, which is the norm (or length) of each vector multiplied together (specifically, the L2-norm is used in cosine similarity). In this way, cosine similarity is a normalized dot product.</p><p>The cosine similarity measure takes on values between -1 and 1. A value of 1 implies completely similar, while a value of 0 implies independence (that is, no similarity). This measure is useful because it also captures negative similarity, that is, a value of -1 implies that not only are the vectors not similar, but they are also completely dissimilar.</p><p>Let's create our <code class="literal">cosineSimilarity</code> function here:</p><div class="informalexample"><pre class="programlisting">def cosineSimilarity(vec1: DoubleMatrix, vec2: DoubleMatrix): Double = {
  vec1.dot(vec2) / (vec1.norm2() * vec2.norm2())
}</pre></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip22"></a>Tip</h3><p>Note that we defined a return type for this function of <code class="literal">Double</code>. We are not required to do this, since Scala features type inference. However, it can often be useful to document return types for Scala functions.</p></div><p>Let's try it out on one <a id="id357" class="indexterm"></a>of our item factors for item <code class="literal">567</code>. We will need to collect an item <a id="id358" class="indexterm"></a>factor from our model; we will do this using the <code class="literal">lookup</code> method in a similar way that we did earlier to collect the ratings for a specific user. In the following lines of code, we also use the <code class="literal">head</code> function, since <code class="literal">lookup</code> returns an array of values, and we only need the first value (in fact, there will only be one value, which is the factor vector for this item).</p><p>Since this will be an <code class="literal">Array[Double]</code>, we will then need to create a <code class="literal">DoubleMatrix</code> object from it and compute the cosine similarity with itself:</p><div class="informalexample"><pre class="programlisting">val itemId = 567
val itemFactor = model.productFeatures.lookup(itemId).head
val itemVector = new DoubleMatrix(itemFactor)
cosineSimilarity(itemVector, itemVector)</pre></div><p>A similarity metric should measure how close, in some sense, two vectors are to each other. Here, we can see that our cosine similarity metric tells us that this item vector is identical to itself, which is what we would expect:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>res113: Double = 1.0</strong></span>
</pre></div><p>Now, we are ready to apply our similarity metric to each item:</p><div class="informalexample"><pre class="programlisting">val sims = model.productFeatures.map{ case (id, factor) =&gt; 
  val factorVector = new DoubleMatrix(factor)
  val sim = cosineSimilarity(factorVector, itemVector)
  (id, sim)
}</pre></div><p>Next, we can compute the top 10 most similar items by sorting out the similarity score for each item:</p><div class="informalexample"><pre class="programlisting">// recall we defined K = 10 earlier
val sortedSims = sims.top(K)(Ordering.by[(Int, Double), Double] { case (id, similarity) =&gt; similarity })</pre></div><p>In the preceding code snippet, we used Spark's <code class="literal">top</code> function, which is an efficient way to compute <span class="emphasis"><em>top-K</em></span> results in a distributed fashion, instead of using <code class="literal">collect</code> to return all the data to the driver and sorting it locally (remember that we could be dealing with millions of users and items in <a id="id359" class="indexterm"></a>the case of recommendation models).</p><p>We <a id="id360" class="indexterm"></a>need to tell Spark how to sort the <code class="literal">(item id, similarity score)</code> pairs in the <code class="literal">sims</code> RDD. To do this, we will pass an extra argument to <code class="literal">top</code>, which is a Scala <code class="literal">Ordering</code> object that tells Spark that it should sort by the value in the key-value pair (that is, sort by <code class="literal">similarity</code>).</p><p>Finally, we can print the 10 items with the highest computed similarity metric to our given item:</p><div class="informalexample"><pre class="programlisting">println(sortedSims.take(10).mkString("\n"))</pre></div><p>You will see output like the following one:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>(567,1.0000000000000002)</strong></span>
<span class="strong"><strong>(1471,0.6932331537649621)</strong></span>
<span class="strong"><strong>(670,0.6898690594544726)</strong></span>
<span class="strong"><strong>(201,0.6897964975027041)</strong></span>
<span class="strong"><strong>(343,0.6891221044611473)</strong></span>
<span class="strong"><strong>(563,0.6864214133620066)</strong></span>
<span class="strong"><strong>(294,0.6812075443259535)</strong></span>
<span class="strong"><strong>(413,0.6754663844488256)</strong></span>
<span class="strong"><strong>(184,0.6702643811753909)</strong></span>
<span class="strong"><strong>(109,0.6594872765176396)</strong></span>
</pre></div><p>Not surprisingly, we can see that the top-ranked similar item is our item. The rest are the other items in our set of items, ranked in order of our similarity metric.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch04lvl4sec05"></a>Inspecting the similar items</h5></div></div></div><p>Let's<a id="id361" class="indexterm"></a> see what the title of our chosen movie is:</p><div class="informalexample"><pre class="programlisting">println(titles(itemId))</pre></div><p>The preceding code will print the following output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Wes Craven's New Nightmare (1994)</strong></span>
</pre></div><p>As we did for user recommendations, we can sense check our item-to-item similarity computations and take a look at the titles of the most similar movies. This time, we will take the top 11 so that we can exclude our given movie. So, we will take the numbers 1 to 11 in the list:</p><div class="informalexample"><pre class="programlisting">val sortedSims2 = sims.top(K + 1)(Ordering.by[(Int, Double), Double] { case (id, similarity) =&gt; similarity })
sortedSims2.slice(1, 11).map{ case (id, sim) =&gt; (titles(id), sim) }.mkString("\n")</pre></div><p>You <a id="id362" class="indexterm"></a>will see the movie titles and scores displayed similar to this output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>(Hideaway (1995),0.6932331537649621)</strong></span>
<span class="strong"><strong>(Body Snatchers (1993),0.6898690594544726)</strong></span>
<span class="strong"><strong>(Evil Dead II (1987),0.6897964975027041)</strong></span>
<span class="strong"><strong>(Alien: Resurrection (1997),0.6891221044611473)</strong></span>
<span class="strong"><strong>(Stephen King's The Langoliers (1995),0.6864214133620066)</strong></span>
<span class="strong"><strong>(Liar Liar (1997),0.6812075443259535)</strong></span>
<span class="strong"><strong>(Tales from the Crypt Presents: Bordello of Blood (1996),0.6754663844488256)</strong></span>
<span class="strong"><strong>(Army of Darkness (1993),0.6702643811753909)</strong></span>
<span class="strong"><strong>(Mystery Science Theater 3000: The Movie (1996),0.6594872765176396)</strong></span>
<span class="strong"><strong>(Scream (1996),0.6538249646863378)</strong></span>
</pre></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip23"></a>Tip</h3><p>Once again note that you might see quite different results due to random model initialization.</p></div><p>Now that you have computed similar items using cosine similarity, see if you can do the same with the user-factor vectors to compute similar users for a given user.</p></div></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec31"></a>Evaluating the performance of recommendation models</h2></div></div><hr /></div><p>How<a id="id363" class="indexterm"></a> do we know whether the model we have trained is a good model? We need to be able to evaluate its predictive performance in some way. <span class="strong"><strong>Evaluation metrics</strong></span> are measures of a model's predictive capability <a id="id364" class="indexterm"></a>or accuracy. Some are direct measures of how well a model predicts the model's target variable (such as Mean Squared Error), while others are concerned with how well the model performs at predicting things that might not be directly optimized in the model but are often closer to what we care about in the real world (such as Mean average precision).</p><p>Evaluation metrics provide a standardized way of comparing the performance of the same model with different parameter settings and of comparing performance across different models. Using these metrics, we can perform model selection to choose the best-performing model from the set of models we wish to evaluate.</p><p>Here, we <a id="id365" class="indexterm"></a>will show you how to calculate two common evaluation metrics used in recommender systems and collaborative filtering models: Mean Squared Error and Mean average precision at K.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec39"></a>Mean Squared Error</h3></div></div></div><p>The <span class="strong"><strong>Mean Squared Error</strong></span> (<span class="strong"><strong>MSE</strong></span>) is a direct measure of the reconstruction error of the user-item rating matrix. It is <a id="id366" class="indexterm"></a>also the objective function being minimized in certain models, specifically many<a id="id367" class="indexterm"></a> matrix-factorization techniques, including <code class="literal">ALS</code>. As such, it is commonly used in explicit ratings settings.</p><p>It is defined as the sum of the squared errors divided by the number of observations. The squared error, in turn, is the square of the difference between the predicted rating for a given user-item pair and the actual rating.</p><p>We will use our user <code class="literal">789</code> as an example. Let's take the first rating for this user from the <code class="literal">moviesForUser</code> set of <code class="literal">Ratings</code> that we previously computed:</p><div class="informalexample"><pre class="programlisting">val actualRating = moviesForUser.take(1)(0)</pre></div><p>Here is the output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>actualRating: org.apache.spark.mllib.recommendation.Rating = Rating(789,1012,4.0)</strong></span>
</pre></div><p>We will see that the rating for this user-item combination is 4. Next, we will compute the model's predicted rating:</p><div class="informalexample"><pre class="programlisting">val predictedRating = model.predict(789, actualRating.product)</pre></div><p>The output of the model's predicted rating is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>14/04/13 13:01:15 INFO SparkContext: Job finished: lookup at MatrixFactorizationModel.scala:46, took 0.025404 s</strong></span>
<span class="strong"><strong>predictedRating: Double = 4.001005374200248</strong></span>
</pre></div><p>We will see that the predicted rating is about 4, very close to the actual rating. Finally, we will compute the squared error between the actual rating and the predicted rating:</p><div class="informalexample"><pre class="programlisting">val squaredError = math.pow(predictedRating - actualRating.rating, 2.0)</pre></div><p>The preceding code will output the squared error:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>squaredError: Double = 1.010777282523947E-6</strong></span>
</pre></div><p>So, in order to compute the overall MSE for the dataset, we need to compute this squared error for each <code class="literal">(user, movie, actual rating, predicted rating)</code> entry, sum them up, and divide them by the number of ratings. We will do this in the following code snippet.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip24"></a>Tip</h3><p>Note the following code is adapted from the Apache Spark programming guide for ALS at <a class="ulink" href="http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html" target="_blank">http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html</a>.</p></div><p>First, we will extract <a id="id368" class="indexterm"></a>the <a id="id369" class="indexterm"></a>user and product IDs from the <code class="literal">ratings</code> RDD and make predictions for each user-item pair using <code class="literal">model.predict</code>. We will use the user-item pair as the key and the predicted rating as the value:</p><div class="informalexample"><pre class="programlisting">val usersProducts = ratings.map{ case Rating(user, product, rating)  =&gt; (user, product)}
val predictions = model.predict(usersProducts).map{
    case Rating(user, product, rating) =&gt; ((user, product), rating)
}</pre></div><p>Next, we extract the actual ratings and also map the <code class="literal">ratings</code> RDD so that the user-item pair is the key and the actual rating is the value. Now that we have two RDDs with the same form of key, we can join them together to create a new RDD with the actual and predicted ratings for each user-item combination:</p><div class="informalexample"><pre class="programlisting">val ratingsAndPredictions = ratings.map{
  case Rating(user, product, rating) =&gt; ((user, product), rating)
}.join(predictions)</pre></div><p>Finally, we will compute the MSE by summing up the squared errors using <code class="literal">reduce</code> and dividing by the <code class="literal">count</code> method of the number of records:</p><div class="informalexample"><pre class="programlisting">val MSE = ratingsAndPredictions.map{
    case ((user, product), (actual, predicted)) =&gt;  math.pow((actual - predicted), 2)
}.reduce(_ + _) / ratingsAndPredictions.count
println("Mean Squared Error = " + MSE)</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Mean Squared Error = 0.08231947642632852</strong></span>
</pre></div><p>It is common to use the <span class="strong"><strong>Root Mean Squared Error</strong></span> (<span class="strong"><strong>RMSE</strong></span>), which is just the square root of the MSE metric. This is somewhat <a id="id370" class="indexterm"></a>more interpretable, as it is in the same units as the underlying data (that is, the ratings in this case). It is equivalent to the standard deviation of the differences between the predicted and actual ratings. We can compute it simply as follows:</p><div class="informalexample"><pre class="programlisting">val RMSE = math.sqrt(MSE)
println("Root Mean Squared Error = " + RMSE)</pre></div><p>The preceding code will print the Root Mean Squared Error:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Root Mean Squared Error = 0.2869137090247319</strong></span>
</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec40"></a>Mean average precision at K</h3></div></div></div><p><span class="strong"><strong>Mean average precision at K</strong></span> (<span class="strong"><strong>MAPK</strong></span>) is the mean of the <span class="strong"><strong>average precision at K</strong></span> (<span class="strong"><strong>APK</strong></span>) metric<a id="id371" class="indexterm"></a> across all instances in the dataset. APK is a metric commonly used in information <a id="id372" class="indexterm"></a>retrieval. APK is a measure of the average relevance scores of a set of the <span class="emphasis"><em>top-K</em></span> documents presented in response to a query. For each query instance, we will compare the set of <span class="emphasis"><em>top-K</em></span> results with the set of actual relevant documents (that is, a ground truth set of relevant documents for the query).</p><p>In the APK metric, the order of the result set matters, in that, the APK score would be higher if the result documents are both relevant and the relevant documents are presented higher in the results. It is, thus, a good metric for recommender systems in that typically we would compute the <span class="emphasis"><em>top-K</em></span> recommended items for each user and present these to the user. Of course, we prefer models where the items with the highest predicted scores (which are presented at the top of the list of recommendations) are, in fact, the most relevant items for the user. APK and other ranking-based metrics are also more appropriate evaluation measures for implicit datasets; here, MSE makes less sense.</p><p>In order to evaluate our model, we can use APK, where each user is the equivalent of a query, and the set of <span class="emphasis"><em>top-K</em></span> recommended items is the document result set. The relevant documents (that is, the ground truth) in this case, is the set of items that a user interacted with. Hence, APK attempts to measure how good our model is at predicting items that a user will find relevant and choose to interact with.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note15"></a>Note</h3><p>The code for the following average precision computation is based on <a class="ulink" href="https://github.com/benhamner/Metrics" target="_blank">https://github.com/benhamner/Metrics</a>.</p><p>More information <a id="id373" class="indexterm"></a>on MAPK can be found at <a class="ulink" href="https://www.kaggle.com/wiki/MeanAveragePrecision" target="_blank">https://www.kaggle.com/wiki/MeanAveragePrecision</a>.</p></div><p>Our function to compute the APK is shown here:</p><div class="informalexample"><pre class="programlisting">def avgPrecisionK(actual: Seq[Int], predicted: Seq[Int], k: Int): Double = {
  val predK = predicted.take(k)
  var score = 0.0
  var numHits = 0.0
  for ((p, i) &lt;- predK.zipWithIndex) {
    if (actual.contains(p)) {
      numHits += 1.0
      score += numHits / (i.toDouble + 1.0)
    }
  }
  if (actual.isEmpty) {
    1.0
  } else {
    score / scala.math.min(actual.size, k).toDouble
  }
}</pre></div><p>As you can see, this <a id="id374" class="indexterm"></a>takes as input a list of <code class="literal">actual</code> item IDs that are associated with the user <a id="id375" class="indexterm"></a>and another list of <code class="literal">predicted</code> ids so that our estimate will be relevant for the user.</p><p>We can compute the APK metric for our example user <code class="literal">789</code> as follows. First, we will extract the actual movie IDs for the user:</p><div class="informalexample"><pre class="programlisting">val actualMovies = moviesForUser.map(_.product)</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>actualMovies: Seq[Int] = ArrayBuffer(1012, 127, 475, 93, 1161, 286, 293, 9, 50, 294, 181, 1, 1008, 508, 284, 1017, 137, 111, 742, 248, 249, 1007, 591, 150, 276, 151, 129, 100, 741, 288, 762, 628, 124)</strong></span>
</pre></div><p>We will then use the movie recommendations we made previously to compute the APK score using <code class="literal">K = 10</code>:</p><div class="informalexample"><pre class="programlisting">val predictedMovies = topKRecs.map(_.product)</pre></div><p>Here is the output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>predictedMovies: Array[Int] = Array(27, 497, 633, 827, 602, 849, 401, 584, 1035, 1014)</strong></span>
</pre></div><p>The following code will produce the average precision:</p><div class="informalexample"><pre class="programlisting">val apk10 = avgPrecisionK(actualMovies, predictedMovies, 10)</pre></div><p>The preceding code will print:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>apk10: Double = 0.0</strong></span>
</pre></div><p>In this case, we can see that our model is not doing a very good job of predicting relevant movies for this user as the APK score is 0.</p><p>In order to compute the APK for each user and average them to compute the overall MAPK, we will need to generate the list of recommendations for each user in our dataset. While this can be fairly intensive on a large scale, we can distribute the computation using our Spark functionality. However, one limitation is that each worker must have the full item-factor matrix available so that it can compute the dot product between the relevant user vector and all item vectors. This can be a problem when the number of items is extremely high as the item matrix must fit in the memory of one machine.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip25"></a>Tip</h3><p>There is actually no easy way around this limitation. One possible approach is to only compute recommendations for a subset of items from the total item set, using approximate techniques such as Locality Sensitive Hashing (<a class="ulink" href="http://en.wikipedia.org/wiki/Locality-sensitive_hashing" target="_blank">http://en.wikipedia.org/wiki/Locality-sensitive_hashing</a>).</p></div><p>We will <a id="id376" class="indexterm"></a>now <a id="id377" class="indexterm"></a>see how to go about this. First, we will collect the item factors and form a <code class="literal">DoubleMatrix</code> object from them:</p><div class="informalexample"><pre class="programlisting">val itemFactors = model.productFeatures.map { case (id, factor) =&gt; factor }.collect()
val itemMatrix = new DoubleMatrix(itemFactors)
println(itemMatrix.rows, itemMatrix.columns)</pre></div><p>The output of the preceding code is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>(1682,50)</strong></span>
</pre></div><p>This gives us a matrix with <code class="literal">1682</code> rows and <code class="literal">50</code> columns, as we would expect from <code class="literal">1682</code> movies with a factor dimension of <code class="literal">50</code>. Next, we will distribute the item matrix as a broadcast variable so that it is available on each worker node:</p><div class="informalexample"><pre class="programlisting">val imBroadcast = sc.broadcast(itemMatrix)</pre></div><p>You will see the output as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>14/04/13 21:02:01 INFO MemoryStore: ensureFreeSpace(672960) called with curMem=4006896, maxMem=311387750</strong></span>
<span class="strong"><strong>14/04/13 21:02:01 INFO MemoryStore: Block broadcast_21 stored as values to memory (estimated size 657.2 KB, free 292.5 MB)</strong></span>
<span class="strong"><strong>imBroadcast: org.apache.spark.broadcast.Broadcast[org.jblas.DoubleMatrix] = Broadcast(21)</strong></span>
</pre></div><p>Now we are ready to compute the recommendations for each user. We will do this by applying a <code class="literal">map</code> function to each user factor within which we will perform a matrix multiplication between the user-factor vector and the movie-factor matrix. The result is a vector (of length <code class="literal">1682</code>, that is, the number of movies we have) with the predicted rating for each movie. We will then sort these predictions by the predicted rating:</p><div class="informalexample"><pre class="programlisting">val allRecs = model.userFeatures.map{ case (userId, array) =&gt; 
  val userVector = new DoubleMatrix(array)
  val scores = imBroadcast.value.mmul(userVector)
  val sortedWithId = scores.data.zipWithIndex.sortBy(-_._1)
  val recommendedIds = sortedWithId.map<span class="strong"><strong>(_._2 + 1</strong></span>).toSeq
  (userId, recommendedIds)
}</pre></div><p>You will see the following on the screen:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>allRecs: org.apache.spark.rdd.RDD[(Int, Seq[Int])] = MappedRDD[269] at map at &lt;console&gt;:29</strong></span>
</pre></div><p>As we can see, we now have an RDD that contains a list of movie IDs for each user ID. These movie IDs are sorted in order of the estimated rating.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip26"></a>Tip</h3><p>Note that we needed to add 1 to the returned movie ids (as highlighted in the preceding code snippet), as the item-factor matrix is 0-indexed, while our movie IDs start at <code class="literal">1</code>.</p></div><p>We also need<a id="id378" class="indexterm"></a> the <a id="id379" class="indexterm"></a>list of movie IDs for each user to pass into our APK function as the <code class="literal">actual</code> argument. We already have the <code class="literal">ratings</code> RDD ready, so we can extract just the user and movie IDs from it.</p><p>If we use Spark's <code class="literal">groupBy</code> operator, we will get an RDD that contains a list of <code class="literal">(userid, movieid)</code> pairs for each user ID (as the user ID is the key on which we perform the <code class="literal">groupBy</code> operation):</p><div class="informalexample"><pre class="programlisting">val userMovies = ratings.map{ case Rating(user, product, rating) =&gt; (user, product) }.groupBy(_._1)</pre></div><p>The output of the preceding code is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>userMovies: org.apache.spark.rdd.RDD[(Int, Seq[(Int, Int)])] = MapPartitionsRDD[277] at groupBy at &lt;console&gt;:21</strong></span>
</pre></div><p>Finally, we can use Spark's <code class="literal">join</code> operator to join these two RDDs together on the user ID key. Then, for each user, we have the list of actual and predicted movie IDs that we can pass to our APK function. In a manner similar to how we computed MSE, we will sum each of these APK scores using a <code class="literal">reduce</code> action and divide by the number of users (that is, the count of the <code class="literal">allRecs</code> RDD):</p><div class="informalexample"><pre class="programlisting">val K = 10
val MAPK = allRecs.join(userMovies).map{ case (userId, (predicted, actualWithIds)) =&gt; 
  val actual = actualWithIds.map(_._2).toSeq
  avgPrecisionK(actual, predicted, K)
}.reduce(_ + _) / allRecs.count
println("Mean Average Precision at K = " + MAPK)</pre></div><p>The preceding code will print the mean average precision at K as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Mean Average Precision at K = 0.030486963254725705 </strong></span>
</pre></div><p>Our model achieves a fairly low MAPK. However, note that typical values for recommendation tasks are usually relatively low, especially if the item set is extremely large.</p><p>Try out a few parameter settings for <code class="literal">lambda</code> and <code class="literal">rank </code>(and <code class="literal">alpha</code> if you are using the implicit version of ALS) and <a id="id380" class="indexterm"></a>see whether you can find a model that performs better based on the <a id="id381" class="indexterm"></a>RMSE and MAPK evaluation metrics.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl2sec41"></a>Using MLlib's built-in evaluation functions</h3></div></div></div><p>While we have <a id="id382" class="indexterm"></a>computed MSE, RMSE, and MAPK from scratch, and it a useful learning <a id="id383" class="indexterm"></a>exercise to do so, MLlib provides convenience functions to do this for us in the <code class="literal">RegressionMetrics</code> and <code class="literal">RankingMetrics</code> classes.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl3sec11"></a>RMSE and MSE</h4></div></div></div><p>First, we<a id="id384" class="indexterm"></a> will compute the MSE and RMSE metrics <a id="id385" class="indexterm"></a>using <code class="literal">RegressionMetrics</code>. We will instantiate a <code class="literal">RegressionMetrics</code> instance by passing in an RDD of key-value pairs that represent the <a id="id386" class="indexterm"></a>predicted and true values for each data point, as<a id="id387" class="indexterm"></a> shown in the following code snippet. Here, we will again use the <code class="literal">ratingsAndPredictions</code> RDD we computed in our earlier example:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark.mllib.evaluation.RegressionMetrics
val predictedAndTrue = ratingsAndPredictions.map { case ((user, product), (predicted, actual)) =&gt; (predicted, actual) }
val regressionMetrics = new RegressionMetrics(predictedAndTrue)</pre></div><p>We can then access various metrics, including MSE and RMSE. We will print out these metrics here:</p><div class="informalexample"><pre class="programlisting">println("Mean Squared Error = " + regressionMetrics.meanSquaredError)
println("Root Mean Squared Error = " + regressionMetrics.rootMeanSquaredError)</pre></div><p>You will see that the output for MSE and RMSE is exactly the same as the metrics we computed earlier:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Mean Squared Error = 0.08231947642632852</strong></span>
<span class="strong"><strong>Root Mean Squared Error = 0.2869137090247319</strong></span>
</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl3sec12"></a>MAP</h4></div></div></div><p>As we <a id="id388" class="indexterm"></a>did for MSE and RMSE, we can compute<a id="id389" class="indexterm"></a> ranking-based evaluation metrics using MLlib's <code class="literal">RankingMetrics</code> class. Similarly, to our own average precision function, we need to pass in an RDD of key-value pairs, where the key is an <code class="literal">Array</code> of predicted item IDs for a user, while the value is an array of actual item IDs.</p><p>The implementation of the average precision at the K function in <code class="literal">RankingMetrics</code> is slightly different from ours, so we will get different results. However, the computation of the overall mean average precision (MAP, which does not use a threshold at K) is the same as our function if we select <code class="literal">K</code> to be very high (say, at least as high as the number of items in our item set):</p><p>First, we<a id="id390" class="indexterm"></a> will calculate MAP using <code class="literal">RankingMetrics</code>:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark.mllib.evaluation.RankingMetrics
val predictedAndTrueForRanking = allRecs.join(userMovies).map{ case (userId, (predicted, actualWithIds)) =&gt; 
  val actual = actualWithIds.map(_._2)
  (predicted.toArray, actual.toArray)
}
val rankingMetrics = new RankingMetrics(predictedAndTrueForRanking)
println("Mean Average Precision = " + rankingMetrics.meanAveragePrecision)</pre></div><p>You will see the following output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Mean Average Precision = 0.07171412913757183</strong></span>
</pre></div><p>Next, we will use our function to compute the MAP in exactly the same way as we did previously, except that we set <code class="literal">K</code> to a very high value, say <code class="literal">2000</code>:</p><div class="informalexample"><pre class="programlisting">val MAPK2000 = allRecs.join(userMovies).map{ case (userId, (predicted, actualWithIds)) =&gt; 
  val actual = actualWithIds.map(_._2).toSeq
  avgPrecisionK(actual, predicted, 2000)
}.reduce(_ + _) / allRecs.count
println("Mean Average Precision = " + MAPK2000)</pre></div><p>You will see that the MAP from our own function is the same as the one computed using <code class="literal">RankingMetrics</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Mean Average Precision = 0.07171412913757186</strong></span>
</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note16"></a>Note</h3><p>We will not cover cross-validation in this chapter, as we will provide a detailed treatment in the next few chapters. However, note that the same techniques for cross-validation that are explored in the upcoming chapters can be used to evaluate recommendation models, using the performance metrics such as MSE, RMSE, and MAP, which we covered in this section.</p></div></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch04lvl1sec32"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we used Spark's MLlib library to train a collaborative filtering recommendation model, and you learned how to use this model to make predictions for the items that a given user might have a preference for. We also used our model to find items that are similar or related to a given item. Finally, we explored common metrics to evaluate the predictive capability of our recommendation model.</p><p>In the next chapter, you will learn how to use Spark to train a model to classify your data and to use standard evaluation mechanisms to gauge the performance of your model.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch05"></a>ChapterÂ 5.Â Building a Classification Model with Spark</h2></div></div></div><p>In this chapter, you will learn the basics of classification models and how they can be used in a variety of contexts. Classification generically refers to classifying things into distinct <a id="id391" class="indexterm"></a>categories or classes. In the case of a classification model, we typically wish to assign classes based on a set of features. The features might represent variables related to an item or object, an event or context, or some combination of these.</p><p>The<a id="id392" class="indexterm"></a> simplest form of classification is when we have two classes; this is referred to as binary classification. One of the classes is usually labeled as the positive class (assigned a label of 1), while the other is labeled as the negative class (assigned a label of -1 or, sometimes, 0).</p><p>A simple example with two classes is shown in the following figure. The input features in this case have two dimensions, and the feature values are represented on the <span class="emphasis"><em>x</em></span> and <span class="emphasis"><em>y</em></span> axes in the figure.</p><p>Our task is to train a model that can classify new data points in this two-dimensional space as either one class (red) or the other (blue).</p><div class="mediaobject"><img src="graphics/8519OS_05_01.jpg" /><div class="caption"><p>A simple binary classification problem</p></div></div><p>If we have more than two classes, we would refer to multiclass classification, and classes are typically labeled using integer numbers starting at 0 (for example, five different classes would range from label 0 to 4). An example is shown in the following figure. Again, the input features are assumed to be two-dimensional for ease of illustration.</p><div class="mediaobject"><img src="graphics/8519OS_05_02.jpg" /><div class="caption"><p>A simple multiclass classification problem</p></div></div><p>Classification<a id="id393" class="indexterm"></a> is a form of supervised learning where we train a model with training examples that include known targets or outcomes of interest (that is, the model is supervised with these example outcomes). Classification models can be used in<a id="id394" class="indexterm"></a> many situations, but a few common examples include:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Predicting the probability of Internet users clicking on an online advert; here, the classes are binary in nature (that is, click or no click)</p></li><li style="list-style-type: disc"><p>Detecting fraud; again, in this case, the classes are commonly binary (fraud or no fraud)</p></li><li style="list-style-type: disc"><p>Predicting defaults on loans (binary)</p></li><li style="list-style-type: disc"><p>Classifying images, video, or sounds (most often multiclass, with potentially very many different classes)</p></li><li style="list-style-type: disc"><p>Assigning categories or tags to news articles, web pages, or other content (multiclass)</p></li><li style="list-style-type: disc"><p>Discovering<a id="id395" class="indexterm"></a> e-mail and web spam, network intrusions, and other malicious behavior (binary or multiclass)</p></li><li style="list-style-type: disc"><p>Detecting failure situations, for example in computer systems or networks</p></li><li style="list-style-type: disc"><p>Ranking customers or users in order of probability that they might purchase a product or use a service (this can be framed as classification by predicting probabilities and then ranking in the descending order)</p></li><li style="list-style-type: disc"><p>Predicting customers or users who might stop using a product, service, or provider (called churn)</p></li></ul></div><p>These are just a few possible use cases. In fact, it is probably safe to say that classification is one of the most widely used machine learning and statistical techniques in modern businesses and especially online businesses.</p><p>In this chapter, we will:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Discuss the types of classification models available in MLlib</p></li><li style="list-style-type: disc"><p>Use Spark to extract the appropriate features from raw input data</p></li><li style="list-style-type: disc"><p>Train a number of classification models using MLlib</p></li><li style="list-style-type: disc"><p>Make predictions with our classification models</p></li><li style="list-style-type: disc"><p>Apply a number of standard evaluation techniques to assess the predictive performance of our models</p></li><li style="list-style-type: disc"><p>Illustrate how to improve model performance using some of the feature-extraction approaches from <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Obtaining, Processing, and Preparing Data with Spark</em></span></p></li><li style="list-style-type: disc"><p>Explore the impact of parameter tuning on model performance and learn how to use cross-validation to select the most optimal model parameters</p></li></ul></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec33"></a>Types of classification models</h2></div></div><hr /></div><p>We will <a id="id396" class="indexterm"></a>explore three common classification models available in Spark: linear models, decision trees, and naÃ¯ve Bayes models. Linear models, while less complex, are relatively easier to scale to very large datasets. Decision tree is a powerful nonlinear technique that can be a little more difficult to scale up (fortunately, MLlib takes care of this for us!) and more computationally intensive to train, but delivers leading performance in many situations. NaÃ¯ve Bayes models are more simple but are easy to train efficiently and parallelize (in fact, they require only one pass over the dataset). They can also give reasonable performance in many cases when appropriate feature <a id="id397" class="indexterm"></a>engineering is used. A naÃ¯ve Bayes model also provides a good baseline model against which we can measure the performance of other models.</p><p>Currently, Spark's MLlib library supports binary classification for linear models, decision trees, and naÃ¯ve Bayes models and multiclass classification for decision trees and naÃ¯ve Bayes models. In this book, for simplicity in illustrating the examples, we will focus on the binary case.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec42"></a>Linear models</h3></div></div></div><p>The <a id="id398" class="indexterm"></a>core idea of linear models (or generalized linear models) is that we model the predicted outcome of interest (often called the <a id="id399" class="indexterm"></a>target or dependent variable) as a function of a simple linear predictor applied to the input variables (also referred to as features or independent variables).</p><div class="informalexample"><pre class="programlisting">y = f(w<sup>T</sup>x)</pre></div><p>Here, <span class="emphasis"><em>y </em></span>is the target variable, <span class="emphasis"><em>w</em></span> is the vector of parameters (known as the weight vector), and <span class="emphasis"><em>x</em></span> is the vector of input features.</p><p><span class="emphasis"><em>w<sup>T</sup>x</em></span> is the linear predictor (or vector dot product) of the weight vector <span class="emphasis"><em>w</em></span> and feature vector <span class="emphasis"><em>x</em></span>. To this linear predictor, we applied a function <span class="emphasis"><em>f</em></span> (called the link function).</p><p>Linear models can, in fact, be used for both classification and regression, simply by changing the link function. Standard linear regression (covered in the next chapter) uses an identity link (that is, <span class="emphasis"><em>y = w<sup>T</sup>x</em></span> directly), while binary classification uses alternative link functions as discussed here.</p><p>Let's take a look at the example of online advertising. In this case, the target variable would be 0 (often assigned the class label of -1 in mathematical treatments) if no click was observed for a given advert displayed on a web page (called an impression). The target variable would be 1 if a click occurred. The feature vector for each impression would consist of variables related to the impression event (such as features relating to the user, web page, advert and advertiser, and various other factors relating to the context of the event, such as the type of device used, time of the day, and geolocation).</p><p>Thus, we would like to find a model that maps a given input feature vector (advert impression) to a predicted outcome (click or not). To make a prediction for a new data point, we will take the new feature vector (which is unseen, and hence, we do not know what the target variable is) and compute the dot product with our weight vector. We will then apply the relevant link function, and the result is our predicted outcome (after applying a threshold to the prediction, in the case of some models).</p><p>Given a<a id="id400" class="indexterm"></a> set of input data in the form of feature <a id="id401" class="indexterm"></a>vectors and target variables, we would like to find the weight<a id="id402" class="indexterm"></a> vector that is the best fit for the data, in the sense that we<a id="id403" class="indexterm"></a> minimize some error between what our model predicts and the actual <a id="id404" class="indexterm"></a>outcomes observed. This process is called <span class="strong"><strong>model</strong></span> <span class="strong"><strong>fitting</strong></span>, <span class="strong"><strong>training</strong></span>, or <span class="strong"><strong>optimization</strong></span>.</p><p>More formally, we seek to find the weight vector that minimizes the sum, over all the training examples, of the loss (or error) computed from some loss function. The loss function takes the weight vector, feature vector, and the actual outcome for a given training example as input and outputs the loss. In fact, the loss function itself is effectively specified by the link function; hence, for a given type of classification or regression (that is, a given link function), there is a corresponding loss function.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip27"></a>Tip</h3><p>For further details on linear models and loss functions, see the linear methods section <a id="id405" class="indexterm"></a>related to binary classification in the <span class="emphasis"><em>Spark Programming Guide</em></span> at <a class="ulink" href="http://spark.apache.org/docs/latest/mllib-linear-methods.html#binary-classification" target="_blank">http://spark.apache.org/docs/latest/mllib-linear-methods.html#binary-classification</a>.Also, see<a id="id406" class="indexterm"></a> the Wikipedia entry for generalized linear models at <a class="ulink" href="http://en.wikipedia.org/wiki/Generalized_linear_model" target="_blank">http://en.wikipedia.org/wiki/Generalized_linear_model</a>.</p></div><p>While a detailed treatment of linear models and loss functions is beyond the scope of this book, MLlib provides two loss functions suitable to binary classification (you can learn more about them from the Spark documentation). The first one is logistic loss, which equates to a model<a id="id407" class="indexterm"></a> known as <span class="strong"><strong>logistic regression</strong></span>, while the second one is the hinge loss, which is equivalent to a linear <span class="strong"><strong>Support Vector Machine</strong></span> (<span class="strong"><strong>SVM</strong></span>). Note<a id="id408" class="indexterm"></a> that the SVM does not strictly fall into the statistical framework of generalized linear models but can be used in the same way as it essentially specifies a loss and link function.</p><p>In the following image, we show the logistic loss and hinge loss relative to the actual zero-one loss. The zero-one loss is the true loss for binary classificationâ€”it is either zero if the model predicts correctly or one if the model predicts incorrectly. The reason it is not actually used is that it is not a differentiable loss function, so it is not possible to easily compute a gradient and, thus, very difficult to optimize.</p><p>The <a id="id409" class="indexterm"></a>other loss functions are approximations to the <a id="id410" class="indexterm"></a>zero-one loss that make optimization possible.</p><div class="mediaobject"><img src="graphics/8519OS_05_03.jpg" /><div class="caption"><p>The logistic, hinge and zero-one loss functions</p></div></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note17"></a>Note</h3><p>The preceding loss diagram is adapted from the scikit-learn example at <a class="ulink" href="http://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_loss_functions.html" target="_blank">http://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_loss_functions.html</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl3sec13"></a>Logistic regression</h4></div></div></div><p>Logistic<a id="id411" class="indexterm"></a> regression is a probabilistic modelâ€”that is, its<a id="id412" class="indexterm"></a> predictions are bounded between 0 and 1, and for binary classification equate to the model's estimate of the probability of the data point belonging to the positive class. Logistic regression is one of the most widely used linear classification models.</p><p>As mentioned earlier, the link function used in logistic regression is the logit link:</p><div class="informalexample"><pre class="programlisting">1 / (1 + exp(-w<sup>T</sup>x))</pre></div><p>The related loss function for logistic regression is the logistic loss:</p><div class="informalexample"><pre class="programlisting">log(1 + exp(-yw<sup>T</sup>x))</pre></div><p>Here, <span class="emphasis"><em>y</em></span> is the actual target variable (either <span class="emphasis"><em>1</em></span> for the positive class or <span class="emphasis"><em>-1</em></span> for the negative class).</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl3sec14"></a>Linear support vector machines</h4></div></div></div><p>SVM is<a id="id413" class="indexterm"></a> a powerful and popular technique for<a id="id414" class="indexterm"></a> regression and classification. Unlike logistic regression, it is not a probabilistic model but predicts classes based on whether the model evaluation is positive or negative.</p><p>The SVM link function is the identity link, so the predicted outcome is:</p><div class="informalexample"><pre class="programlisting">y = w<sup>T</sup>x</pre></div><p>Hence, if the evaluation of <span class="emphasis"><em>w<sup>T</sup>x</em></span> is greater than or equal to a threshold of 0, the SVM will assign the data point to class 1; otherwise, the SVM will assign it to class 0 (this threshold is a model parameter of SVM and can be adjusted).</p><p>The loss<a id="id415" class="indexterm"></a> function for SVM is known as the <span class="strong"><strong>hinge loss</strong></span> and is defined as:</p><div class="informalexample"><pre class="programlisting">max(0, 1 â€“ yw<sup>T</sup>x)</pre></div><p>SVM is a maximum margin classifierâ€”it tries to find a weight vector such that the classes are separated as much as possible. It has been shown to perform well on many classification tasks, and the linear variant can scale to very large datasets.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note18"></a>Note</h3><p>SVMs have a large amount of theory behind them, which is beyond the scope of this book, but you can visit <a class="ulink" href="http://en.wikipedia.org/wiki/Support_vector_machine" target="_blank">http://en.wikipedia.org/wiki/Support_vector_machine</a> and <a class="ulink" href="http://www.support-vector-machines.org/" target="_blank">http://www.support-vector-machines.org/</a> for more details.</p></div><p>In the <a id="id416" class="indexterm"></a>following image, we have plotted the<a id="id417" class="indexterm"></a> different decision functions for logistic regression (the blue line) and linear SVM (the red line), based on the simple binary classification example explained earlier.</p><p>You can see that the SVM effectively focuses on the points that lie closest to the decision function (the margin lines are shown with red dashes):</p><div class="mediaobject"><img src="graphics/8519OS_05_04.jpg" /><div class="caption"><p>Decision functions for logistic regression and linear SVM for binary classification</p></div></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec43"></a>The naÃ¯ve Bayes model</h3></div></div></div><p>NaÃ¯ve <a id="id418" class="indexterm"></a>Bayes is a probabilistic model that makes<a id="id419" class="indexterm"></a> predictions by computing the probability of a data point that belongs to a given class. A naÃ¯ve Bayes model assumes that each feature makes an independent contribution to the probability assigned to a class (it assumes conditional independence between features).</p><p>Due to this assumption, the probability of each class becomes a function of the product of the probability of a feature occurring, given the class, as well as the probability of this class. This makes training the model tractable and relatively straightforward. The class prior probabilities and feature conditional probabilities are all estimated from the frequencies present in the dataset. Classification is performed by selecting the most probable class, given the features and class probabilities.</p><p>An assumption is also made about the feature distributions (the parameters of which are estimated from the data). MLlib implements multinomial naÃ¯ve Bayes that assumes that the feature distribution is a multinomial distribution that represents non-negative frequency counts of the features.</p><p>It is suitable for binary features (for example, <span class="emphasis"><em>1-of-k</em></span> encoded categorical features) and is commonly used for text and document classification (where, as we have seen in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Obtaining, Processing, and Preparing Data with Spark</em></span>, the bag-of-words vector is a typical feature representation).</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note19"></a>Note</h3><p>Take a look at the <span class="emphasis"><em>MLlib - Naive Bayes</em></span> section in the Spark documentation at <a class="ulink" href="http://spark.apache.org/docs/latest/mllib-naive-bayes.html" target="_blank">http://spark.apache.org/docs/latest/mllib-naive-bayes.html</a> for more information.</p><p>The Wikipedia page at <a class="ulink" href="http://en.wikipedia.org/wiki/Naive_Bayes_classifier" target="_blank">http://en.wikipedia.org/wiki/Naive_Bayes_classifier</a> has a more detailed explanation of the mathematical formulation.</p></div><p>Here, we have shown the decision function of naÃ¯ve Bayes on our simple binary classification example:</p><div class="mediaobject"><img src="graphics/8519OS_05_05.jpg" /><div class="caption"><p>Decision function of naÃ¯ve Bayes for binary classification</p></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec44"></a>Decision trees</h3></div></div></div><p>Decision <a id="id420" class="indexterm"></a>tree model is a powerful, nonprobabilistic<a id="id421" class="indexterm"></a> technique that can capture more complex nonlinear patterns and feature interactions. They have been shown to perform well on many tasks, are relatively easy to understand and interpret, can handle categorical and numerical features, and do not require input data to be scaled or standardized. They are well suited to be included in ensemble methods (for example, ensembles of decision tree models, which are called decision forests).</p><p>The decision tree model constructs a tree where the leaves represent a class assignment to class 0 or 1, and the branches are a set of features. In the following figure, we show a simple decision tree where the binary outcome is <span class="strong"><strong>Stay at home</strong></span> or <span class="strong"><strong>Go to the beach</strong></span>. The features<a id="id422" class="indexterm"></a> are the weather outside.</p><div class="mediaobject"><img src="graphics/8519OS_05_06.jpg" /><div class="caption"><p>A simple decision tree</p></div></div><p>The decision <a id="id423" class="indexterm"></a>tree algorithm is a top-down approach that begins at a root node (or feature), and then selects a feature at each step that gives the best split of the dataset, as measured by the information gain of this split. The information gain is computed from the node impurity (which is the extent to which the labels at the node are similar, or homogenous) minus the weighted sum of the impurities for the two child nodes that <a id="id424" class="indexterm"></a>would be created by the split. For classification<a id="id425" class="indexterm"></a> tasks, there are two measures that can be used to select the best split. These are Gini impurity and entropy.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note20"></a>Note</h3><p>See the <span class="emphasis"><em>MLlib - Decision Tree</em></span> section in the <span class="emphasis"><em>Spark Programming Guide</em></span> at <a class="ulink" href="http://spark.apache.org/docs/latest/mllib-decision-tree.html" target="_blank">http://spark.apache.org/docs/latest/mllib-decision-tree.html</a> for further details on the decision tree algorithm and impurity measures for classification.</p></div><p>In the following screenshot, we have plotted the decision boundary for the decision tree model, as we did for the other models earlier. We can see that the decision tree is able to fit complex, nonlinear models.</p><div class="mediaobject"><img src="graphics/8519OS_05_07.jpg" /><div class="caption"><p>Decision function for a decision tree for binary classification</p></div></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec34"></a>Extracting the right features from your data</h2></div></div><hr /></div><p>You might recall from <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Obtaining, Processing, and Preparing Data with Spark</em></span> that<a id="id426" class="indexterm"></a> the majority of machine learning models operate <a id="id427" class="indexterm"></a>on numerical data in the form of feature vectors. In addition, for supervised learning methods such as classification and regression, we need to provide the target variable (or variables in the case of multiclass situations) together with the feature vector.</p><p>Classification models in MLlib operate on instances of <code class="literal">LabeledPoint</code>, which is a wrapper <a id="id428" class="indexterm"></a>around the target variable (called the <span class="strong"><strong>label</strong></span>) and the <span class="strong"><strong>feature vector</strong></span>:</p><div class="informalexample"><pre class="programlisting">case class LabeledPoint(label: Double, features: Vector)</pre></div><p>While in<a id="id429" class="indexterm"></a> most examples of using classification, you will come across existing datasets that are already in the vector format, in practice, you will usually start with raw data that needs to be transformed into features. As we have already seen, this can involve preprocessing and transformation, such as binning numerical features, scaling and normalizing features, and using <span class="emphasis"><em>1-of-k</em></span> encodings for categorical features.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec45"></a>Extracting features from the Kaggle/StumbleUpon evergreen classification dataset</h3></div></div></div><p>In<a id="id430" class="indexterm"></a> this chapter, we will use a <a id="id431" class="indexterm"></a>different dataset from the one we used for our recommendation model, as the MovieLens data doesn't have much for us to work with in terms of a classification problem. We will use a dataset from a competition on Kaggle. The dataset was provided by StumbleUpon, and the problem relates to classifying whether a given web page is ephemeral (that is, short lived and will cease being popular soon) or evergreen (that is, persistently popular) on their web content recommendation pages.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note21"></a>Note</h3><p>The <a id="id432" class="indexterm"></a>dataset used here can be downloaded from <a class="ulink" href="http://www.kaggle.com/c/stumbleupon/data" target="_blank">http://www.kaggle.com/c/stumbleupon/data</a>.</p><p>Download the training data (<code class="literal">train.tsv</code>)â€”you will need to accept the terms and conditions before downloading the dataset.</p><p>You can find more information about the competition at <a class="ulink" href="http://www.kaggle.com/c/stumbleupon" target="_blank">http://www.kaggle.com/c/stumbleupon</a>.</p></div><p>Before<a id="id433" class="indexterm"></a> we begin, it will be<a id="id434" class="indexterm"></a> easier for us to work with the data in Spark if we remove the column name header from the first line of the file. Change to the directory in which you downloaded the data (referred to as <code class="literal">PATH</code> here) and run the following command to remove the first line and pipe the result to a new file called <code class="literal">train_noheader.tsv</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;sed 1d train.tsv &gt; train_noheader.tsv</strong></span>
</pre></div><p>Now, we are ready to start up our Spark shell (remember to run this command from your Spark installation directory):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;./bin/spark-shell â€“-driver-memory 4g</strong></span>
</pre></div><p>You can type in the code that follows for the remainder of this chapter directly into your Spark shell.</p><p>In a manner similar to what we did in the earlier chapters, we will load the raw training data into an RDD and inspect it:</p><div class="informalexample"><pre class="programlisting">val rawData = sc.textFile("/<span class="strong"><strong>PATH</strong></span>/train_noheader.tsv")
val records = rawData.map(line =&gt; line.split("\t"))
records.first()</pre></div><p>You will the following on the screen:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Array[String] = Array("http://www.bloomberg.com/news/2010-12-23/ibm-predicts-holographic-calls-air-breathing-batteries-by-2015.html", "4042", ...</strong></span>
</pre></div><p>You can check the fields that are available by reading through the overview on the dataset page above. The first two columns contain the URL and ID of the page. The next column contains some raw textual content. The next column contains the category assigned to the page. The next 22 columns contain numeric or categorical features of various kinds. The final column contains the targetâ€”1 is evergreen, while 0 is non-evergreen.</p><p>We'll start off with a simple approach of using only the available numeric features directly. As each categorical variable is binary, we already have a <span class="emphasis"><em>1-of-k</em></span> encoding for these variables, so we don't need to do any further feature extraction.</p><p>Due to the way the data is formatted, we will have to do a bit of data cleaning during our initial processing by trimming out the extra quotation characters (<code class="literal">"</code>). There are also missing values in the dataset; they are denoted by the <code class="literal">"?"</code> character. In this case, we will simply assign a zero value to these missing values:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.linalg.Vectors
val data = records.map { r =&gt;
  val trimmed = r.map(_.replaceAll("\"", ""))
  val label = trimmed(r.size - 1).toInt
  val features = trimmed.slice(4, r.size - 1).map(d =&gt; if (d == "?") 0.0 else d.toDouble)
  LabeledPoint(label, Vectors.dense(features))
}</pre></div><p>In the<a id="id435" class="indexterm"></a> preceding code, we extracted <a id="id436" class="indexterm"></a>the label variable from the last column and an array of features for columns 5 to 25 after cleaning and dealing with missing values. We converted the label to an <code class="literal">Int</code> value and the features to an <code class="literal">Array[Double]</code>. Finally, we wrapped the label and features in a <code class="literal">LabeledPoint</code> instance, converting the features into an MLlib <code class="literal">Vector</code>.</p><p>We will also cache the data and count the number of data points:</p><div class="informalexample"><pre class="programlisting">data.cache
val numData = data.count</pre></div><p>You will see that the value of <code class="literal">numData</code> is 7395.</p><p>We will explore the dataset in more detail a little later, but we will tell you now that there are some negative feature values in the numeric data. As we saw earlier, the naÃ¯ve Bayes model requires non-negative features and will throw an error if it encounters negative values. So, for now, we will create a version of our input feature vectors for the naÃ¯ve Bayes model by setting any negative feature values to zero:</p><div class="informalexample"><pre class="programlisting">val nbData = records.map { r =&gt;
  val trimmed = r.map(_.replaceAll("\"", ""))
  val label = trimmed(r.size - 1).toInt
  val features = trimmed.slice(4, r.size - 1).map(d =&gt; if (d == "?") 0.0 else d.toDouble).map(d =&gt; if (d &lt; 0) 0.0 else d)
  LabeledPoint(label, Vectors.dense(features))
}</pre></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec35"></a>Training classification models</h2></div></div><hr /></div><p>Now <a id="id437" class="indexterm"></a>that we have extracted some basic features from our dataset and created our input RDD, we are ready to train a number of models. To compare the performance and use of different models, we will train a model using logistic regression, SVM, naÃ¯ve Bayes, and a decision tree. You will notice that training each model looks nearly identical, although each has its own specific model parameters that can be set. MLlib sets sensible defaults in most cases, but in practice, the best parameter setting should be selected using evaluation techniques, which we will cover later in this chapter.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec46"></a>Training a classification model on the Kaggle/StumbleUpon evergreen classification dataset</h3></div></div></div><p>We<a id="id438" class="indexterm"></a> can now apply <a id="id439" class="indexterm"></a>the models from MLlib to our input data. First, we need to import the required classes and set up some minimal input parameters for each model. For logistic regression and SVM, this is the number of iterations, while for the decision tree model, it is the maximum tree depth:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark.mllib.classification.LogisticRegressionWithSGD
import org.apache.spark.mllib.classification.SVMWithSGD
import org.apache.spark.mllib.classification.NaiveBayes
import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.mllib.tree.configuration.Algo
import org.apache.spark.mllib.tree.impurity.Entropy 
val numIterations = 10
val maxTreeDepth = 5</pre></div><p>Now, train each model in turn. First, we will train logistic regression:</p><div class="informalexample"><pre class="programlisting">val lrModel = LogisticRegressionWithSGD.train(data, numIterations)</pre></div><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>14/12/06 13:41:47 INFO DAGScheduler: Job 81 finished: reduce at RDDFunctions.scala:112, took 0.011968 s</strong></span>
<span class="strong"><strong>14/12/06 13:41:47 INFO GradientDescent: GradientDescent.runMiniBatchSGD finished. Last 10 stochastic losses 0.6931471805599474, 1196521.395699124, Infinity, 1861127.002201189, Infinity, 2639638.049627607, Infinity, Infinity, Infinity, Infinity</strong></span>
<span class="strong"><strong>lrModel: org.apache.spark.mllib.classification.LogisticRegressionModel = (weights=[-0.11372778986947886,-0.511619752777837, </strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><p>Next up, we will train an SVM model:</p><div class="informalexample"><pre class="programlisting">val svmModel = SVMWithSGD.train(data, numIterations)</pre></div><p>You will see the following output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>14/12/06 13:43:08 INFO DAGScheduler: Job 94 finished: reduce at RDDFunctions.scala:112, took 0.007192 s</strong></span>
<span class="strong"><strong>14/12/06 13:43:08 INFO GradientDescent: GradientDescent.runMiniBatchSGD finished. Last 10 stochastic losses 1.0, 2398226.619666797, 2196192.9647478117, 3057987.2024311484, 271452.9038284356, 3158131.191895948, 1041799.350498323, 1507522.941537049, 1754560.9909073508, 136866.76745605646</strong></span>
<span class="strong"><strong>svmModel: org.apache.spark.mllib.classification.SVMModel = (weights=[-0.12218838697834929,-0.5275107581589767,</strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><p>Then, we<a id="id440" class="indexterm"></a> will train<a id="id441" class="indexterm"></a> the naÃ¯ve Bayes model; remember to use your special non-negative feature dataset:</p><div class="informalexample"><pre class="programlisting">val nbModel = NaiveBayes.train(nbData)</pre></div><p>The following is the output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>14/12/06 13:44:48 INFO DAGScheduler: Job 95 finished: collect at NaiveBayes.scala:120, took 0.441273 s</strong></span>
<span class="strong"><strong>nbModel: org.apache.spark.mllib.classification.NaiveBayesModel = org.apache.spark.mllib.classification.NaiveBayesModel@666ac612 </strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><p>Finally, we will train our decision tree:</p><div class="informalexample"><pre class="programlisting">val dtModel = DecisionTree.train(data, Algo.Classification, Entropy, maxTreeDepth)</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>14/12/06 13:46:03 INFO DAGScheduler: Job 104 finished: collectAsMap at DecisionTree.scala:653, took 0.031338 s</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>  total: 0.343024</strong></span>
<span class="strong"><strong>  findSplitsBins: 0.119499</strong></span>
<span class="strong"><strong>  findBestSplits: 0.200352</strong></span>
<span class="strong"><strong>  chooseSplits: 0.199705</strong></span>
<span class="strong"><strong>dtModel: org.apache.spark.mllib.tree.model.DecisionTreeModel = DecisionTreeModel classifier of depth 5 with 61 nodes </strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><p>Notice<a id="id442" class="indexterm"></a> that we set<a id="id443" class="indexterm"></a> the mode, or <code class="literal">Algo</code>, of the decision tree to <code class="literal">Classification</code>, and we used the <code class="literal">Entropy</code> impurity measure.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec36"></a>Using classification models</h2></div></div><hr /></div><p>We now<a id="id444" class="indexterm"></a> have four models trained on our input labels and features. We will now see how to use these models to make predictions on our dataset. For now, we will use the same training data to illustrate the <code class="literal">predict</code> method of each model.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec47"></a>Generating predictions for the Kaggle/StumbleUpon evergreen classification dataset</h3></div></div></div><p>We will<a id="id445" class="indexterm"></a> use <a id="id446" class="indexterm"></a>our logistic regression model as an example (the other models<a id="id447" class="indexterm"></a> are used in the same way):</p><div class="informalexample"><pre class="programlisting">val dataPoint = data.first
val prediction = lrModel.predict(dataPoint.features)</pre></div><p>The following is the output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>prediction: Double = 1.0</strong></span>
</pre></div><p>We saw that for the first data point in our training dataset, the model predicted a label of <code class="literal">1</code> (that is, evergreen). Let's examine the true label for this data point:</p><div class="informalexample"><pre class="programlisting">val trueLabel = dataPoint.label</pre></div><p>You can see the following output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>trueLabel: Double = 0.0</strong></span>
</pre></div><p>So, in this case, our model got it wrong!</p><p>We can<a id="id448" class="indexterm"></a> also make predictions in bulk by passing in an <code class="literal">RDD[Vector]</code> as input:</p><div class="informalexample"><pre class="programlisting">val predictions = lrModel.predict(data.map(lp =&gt; lp.features))
predictions.take(5)</pre></div><p>The following <a id="id449" class="indexterm"></a>is<a id="id450" class="indexterm"></a> the output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Array[Double] = Array(1.0, 1.0, 1.0, 1.0, 1.0)</strong></span>
</pre></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec37"></a>Evaluating the performance of classification models</h2></div></div><hr /></div><p>When <a id="id451" class="indexterm"></a>we make predictions using our model, as we did earlier, how do we know whether the predictions are good or not? We need to be able to evaluate how well our model performs. Evaluation metrics commonly used in binary classification include prediction accuracy and error, precision and <a id="id452" class="indexterm"></a>recall, and area under the precision-recall <a id="id453" class="indexterm"></a>curve, the <span class="strong"><strong>receiver operating characteristic</strong></span> (<span class="strong"><strong>ROC</strong></span>) curve, <span class="strong"><strong>area under ROC curve</strong></span> (<span class="strong"><strong>AUC</strong></span>), and F-measure.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec48"></a>Accuracy and prediction error</h3></div></div></div><p>The <a id="id454" class="indexterm"></a>prediction error for binary<a id="id455" class="indexterm"></a> classification is possibly the <a id="id456" class="indexterm"></a>simplest measure available. It is the number of training examples that are misclassified, divided by the total number of examples. Similarly, accuracy is the number of correctly classified examples divided by the total examples.</p><p>We can calculate the accuracy of our models in our training data by making predictions on each input feature and comparing them to the true label. We will sum up the number of correctly classified instances and divide this by the total number of data points to get the average classification accuracy:</p><div class="informalexample"><pre class="programlisting">val lrTotalCorrect = data.map { point =&gt;
  if (lrModel.predict(point.features) == point.label) 1 else 0
}.sum 
val lrAccuracy = lrTotalCorrect / data.count</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>lrAccuracy: Double = 0.5146720757268425</strong></span>
</pre></div><p>This gives us 51.5 percent accuracy, which doesn't look particularly impressive! Our model<a id="id457" class="indexterm"></a> got only half of the<a id="id458" class="indexterm"></a> training examples correct, which <a id="id459" class="indexterm"></a>seems to be about as good as a random chance.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note22"></a>Note</h3><p>Note that the predictions made by the model are not naturally exactly 1 or 0. The output is usually a real number that must be turned into a class prediction. This is done through use of a threshold in the classifier's decision or scoring function.</p><p>For example, binary logistic regression is a probabilistic model that returns the estimated probability of class 1 in its scoring function. Thus, a decision threshold of 0.5 is typical. That is, if the estimated probability of being in class 1 is higher than 50 percent, the model decides to classify the point as class 1; otherwise, it will be classified as class 0.</p><p>Note that the threshold itself is effectively a model parameter that can be tuned in some models. It also plays a role in evaluation measures, as we will see now.</p></div><p>What about the other models? Let's compute the accuracy for the other three:</p><div class="informalexample"><pre class="programlisting">val svmTotalCorrect = data.map { point =&gt;
  if (svmModel.predict(point.features) == point.label) 1 else 0
}.sum
val nbTotalCorrect = nbData.map { point =&gt;
  if (nbModel.predict(point.features) == point.label) 1 else 0
}.sum</pre></div><p>Note that the decision tree prediction threshold needs to be specified explicitly, as highlighted here:</p><div class="informalexample"><pre class="programlisting">val dtTotalCorrect = data.map { point =&gt;
  val score = dtModel.predict(point.features)
  val predicted = if (<span class="strong"><strong>score &gt; 0.5</strong></span>) 1 else 0 
  if (predicted == point.label) 1 else 0
}.sum </pre></div><p>We can now inspect the accuracy for the other three models.</p><p>First, the SVM model:</p><div class="informalexample"><pre class="programlisting">val svmAccuracy = svmTotalCorrect / numData</pre></div><p>Here is the output for the SVM model:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>svmAccuracy: Double = 0.5146720757268425</strong></span>
</pre></div><p>Next, our naÃ¯ve Bayes model:</p><div class="informalexample"><pre class="programlisting">val nbAccuracy = nbTotalCorrect / numData</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>nbAccuracy: Double = 0.5803921568627451</strong></span>
</pre></div><p>Finally, we compute the accuracy for the decision tree:</p><div class="informalexample"><pre class="programlisting">val dtAccuracy = dtTotalCorrect / numData</pre></div><p>And, the output is:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong> dtAccuracy: Double = 0.6482758620689655</strong></span>
</pre></div><p>We <a id="id460" class="indexterm"></a>can see that both SVM<a id="id461" class="indexterm"></a> and naÃ¯ve Bayes also performed<a id="id462" class="indexterm"></a> quite poorly. The decision tree model is better with 65 percent accuracy, but this is still not particularly high.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec49"></a>Precision and recall</h3></div></div></div><p>In <a id="id463" class="indexterm"></a>information retrieval, precision is a <a id="id464" class="indexterm"></a>commonly used measure of the quality of the results, while recall is a <a id="id465" class="indexterm"></a>measure of the completeness of the results.</p><p>In the binary classification context, precision is defined as the number of true positives (that is, the number of examples correctly predicted as class 1) divided by the sum of true positives and false positives (that is, the number of examples that were incorrectly predicted as class 1). Thus, we can see that a precision of 1.0 (or 100 percent) is achieved if every example predicted by the classifier to be class 1 is, in fact, in class 1 (that is, there are no false positives).</p><p>Recall is <a id="id466" class="indexterm"></a>defined as the number of true positives divided by the sum of true positives and false negatives (that is, the number of examples that were in class 1, but were predicted as class 0 by the model). We can see that a recall of 1.0 (or 100 percent) is achieved if the model doesn't miss any examples that were in class 1 (that is, there are no false negatives).</p><p>Generally, precision and recall are inversely related; often, higher precision is related to lower recall and vice versa. To illustrate this, assume that we built a model that always predicted class 1. In this case, the model predictions would have no false negatives because the model always predicts 1; it will not miss any of class 1. Thus, the recall will be 1.0 for this model. On the other hand, the false positive rate could be very high, meaning precision would be low (this depends on the exact distribution of the classes in the dataset).</p><p>Precision and recall are not particularly useful as standalone metrics, but are typically used together to form an aggregate or averaged metric. Precision and recall are also dependent on the threshold selected for the model.</p><p>Intuitively, below some threshold level, a model will always predict class 1. Hence, it will have a recall of 1, but most likely, it will have low precision. At a high enough threshold, the model will always predict class 0. The model will then have a recall of 0, since it <a id="id467" class="indexterm"></a>cannot achieve any true positives<a id="id468" class="indexterm"></a> and will likely have many false negatives. Furthermore, its<a id="id469" class="indexterm"></a> precision score will be undefined, as it will achieve <a id="id470" class="indexterm"></a>zero true positives and zero false positives.</p><p>The <span class="strong"><strong>precision-recall</strong></span> (<span class="strong"><strong>PR</strong></span>) curve<a id="id471" class="indexterm"></a> shown in the following figure plots precision against recall outcomes for a given model, as the decision threshold of the classifier is changed. The area under this PR curve is referred to as the average precision. Intuitively, an area under the PR curve of 1.0 will equate to a perfect classifier that will achieve 100 percent in both precision and recall.</p><div class="mediaobject"><img src="graphics/8519OS_05_10.jpg" /><div class="caption"><p>Precision-recall curve</p></div></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip28"></a>Tip</h3><p>See <a class="ulink" href="http://en.wikipedia.org/wiki/Precision_and_recall" target="_blank">http://en.wikipedia.org/wiki/Precision_and_recall</a> and <a class="ulink" href="http://en.wikipedia.org/wiki/Average_precision#Average_precision" target="_blank">http://en.wikipedia.org/wiki/Average_precision#Average_precision</a> for more details on precision, recall, and area under the PR curve.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec50"></a>ROC curve and AUC</h3></div></div></div><p>The <span class="strong"><strong>ROC</strong></span> curve <a id="id472" class="indexterm"></a>is a concept similar to<a id="id473" class="indexterm"></a> the <a id="id474" class="indexterm"></a>PR curve. It is<a id="id475" class="indexterm"></a> a graphical illustration of the true positive rate against the false positive rate for a classifier.</p><p>The <span class="strong"><strong>true positive rate</strong></span> (<span class="strong"><strong>TPR</strong></span>) is the number of true positives divided by the sum of true positives<a id="id476" class="indexterm"></a> and false negatives. In other words, it is the ratio of true positives to all positive examples. This is the same as the recall we saw earlier and is also commonly referred to as sensitivity.</p><p>The <span class="strong"><strong>false positive rate</strong></span> (<span class="strong"><strong>FPR</strong></span>) is the number of false positives divided by the sum of false positives<a id="id477" class="indexterm"></a> and <span class="strong"><strong>true negatives</strong></span> (that is, the <a id="id478" class="indexterm"></a>number of examples correctly predicted as class 0). In other <a id="id479" class="indexterm"></a>words, it is <a id="id480" class="indexterm"></a>the ratio of false positives to all negative examples.</p><p>In a <a id="id481" class="indexterm"></a>manner similar to precision and recall, the ROC curve (plotted in the following figure) represents the classifier's performance tradeoff of TPR against FPR, for different decision thresholds. Each point on the curve represents a different <a id="id482" class="indexterm"></a>threshold in the decision function for the classifier.</p><div class="mediaobject"><img src="graphics/8519OS_05_11.jpg" /><div class="caption"><p>The ROC curve</p></div></div><p>The area under the ROC curve (commonly referred to as AUC) represents an average value. Again, an AUC of 1.0 will represent a perfect classifier. An area of 0.5 is referred to as<a id="id483" class="indexterm"></a> the random score. Thus, a model <a id="id484" class="indexterm"></a>that achieves an AUC of 0.5 is no better than randomly guessing.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note23"></a>Note</h3><p>As both the area under the PR curve and the area under the ROC curve are effectively normalized (with a minimum of 0 and maximum of 1), we can use these measures to compare models with differing parameter settings and even compare completely different models. Thus, these metrics are popular for model evaluation and selection purposes.</p></div><p>MLlib <a id="id485" class="indexterm"></a>comes with a set of built-in routines to<a id="id486" class="indexterm"></a> compute the area under the PR and ROC curves for binary classification. Here, we will compute these metrics for each of our models:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
val metrics = Seq(lrModel, svmModel).map { model =&gt; 
  val scoreAndLabels = data.map { point =&gt;
    (model.predict(point.features), point.label)
  }
  val metrics = new BinaryClassificationMetrics(scoreAndLabels)
  (model.getClass.getSimpleName, metrics.areaUnderPR, metrics.areaUnderROC)
}</pre></div><p>As we did previously to train the naÃ¯ve Bayes model and computing accuracy, we need to use the special <code class="literal">nbData</code> version of the dataset that we created to compute the classification metrics:</p><div class="informalexample"><pre class="programlisting">val nbMetrics = Seq(nbModel).map{ model =&gt;
  val scoreAndLabels = nbData.map { point =&gt;
    val score = model.predict(point.features)
    (if (score &gt; 0.5) 1.0 else 0.0, point.label)
  }
  val metrics = new BinaryClassificationMetrics(scoreAndLabels)
  (model.getClass.getSimpleName, metrics.areaUnderPR, metrics.areaUnderROC)
}</pre></div><p>Note that because the <code class="literal">DecisionTreeModel</code> model does not implement the <code class="literal">ClassificationModel</code> interface that is implemented by the other three models, we need to compute the results separately for this model in the following code:</p><div class="informalexample"><pre class="programlisting">val dtMetrics = Seq(dtModel).map{ model =&gt;
  val scoreAndLabels = data.map { point =&gt;
    val score = model.predict(point.features)
    (if (score &gt; 0.5) 1.0 else 0.0, point.label)
  }
  val metrics = new BinaryClassificationMetrics(scoreAndLabels)
  (model.getClass.getSimpleName, metrics.areaUnderPR, metrics.areaUnderROC)
}
val allMetrics = metrics ++ nbMetrics ++ dtMetrics
allMetrics.foreach{ case (m, pr, roc) =&gt; 
  println(f"$m, Area under PR: ${pr * 100.0}%2.4f%%, Area under ROC: ${roc * 100.0}%2.4f%%") 
}</pre></div><p>Your<a id="id487" class="indexterm"></a> output will look similar to the<a id="id488" class="indexterm"></a> one here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>LogisticRegressionModel, Area under PR: 75.6759%, Area under ROC: 50.1418%</strong></span>
<span class="strong"><strong>SVMModel, Area under PR: 75.6759%, Area under ROC: 50.1418%</strong></span>
<span class="strong"><strong>NaiveBayesModel, Area under PR: 68.0851%, Area under ROC: 58.3559%</strong></span>
<span class="strong"><strong>DecisionTreeModel, Area under PR: 74.3081%, Area under ROC: 64.8837%</strong></span>
</pre></div><p>We <a id="id489" class="indexterm"></a>can see that all models achieve broadly<a id="id490" class="indexterm"></a> similar results for the average precision metric.</p><p>Logistic regression and SVM achieve results of around 0.5 for AUC. This indicates that they do no better than random chance! Our naÃ¯ve Bayes and decision tree models fare a little better, achieving an AUC of 0.58 and 0.65, respectively. Still, this is not a very good result in terms of binary classification performance.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note24"></a>Note</h3><p>While we don't cover multiclass classification here, MLlib provides a similar evaluation class called <code class="literal">MulticlassMetrics</code>, which provides averaged versions of many common metrics.</p></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec38"></a>Improving model performance and tuning parameters</h2></div></div><hr /></div><p>So, what<a id="id491" class="indexterm"></a> went wrong? Why have our sophisticated<a id="id492" class="indexterm"></a> models achieved nothing better than random chance? Is there a problem with our models?</p><p>Recall that we started out by just throwing the data at our model. In fact, we didn't even throw all our data at the model, just the numeric columns that were easy to use. Furthermore, we didn't do a lot of analysis on these numeric features.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec51"></a>Feature standardization</h3></div></div></div><p>Many<a id="id493" class="indexterm"></a> models that we employ make inherent <a id="id494" class="indexterm"></a>assumptions about the distribution or scale of input data. One of the most common forms of assumption is about normally-distributed features. Let's take a deeper look at the distribution of our features.</p><p>To do this, we can represent the feature vectors as a distributed matrix in MLlib, using the <code class="literal">RowMatrix</code> class. <code class="literal">RowMatrix</code> is an RDD made up of vector, where each vector is a row of our matrix.</p><p>The <code class="literal">RowMatrix</code> class comes with some useful methods to operate on the matrix, one of which is a utility to compute statistics on the columns of the matrix:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark.mllib.linalg.distributed.RowMatrix
val vectors = data.map(lp =&gt; lp.features)
val matrix = new RowMatrix(vectors)
val matrixSummary = matrix.computeColumnSummaryStatistics()</pre></div><p>The following code statement will print the mean of the matrix:</p><div class="informalexample"><pre class="programlisting">println(matrixSummary.mean)</pre></div><p>Here is the output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[0.41225805299526636,2.761823191986623,0.46823047328614004, ...</strong></span>
</pre></div><p>The following code statement will print the minimum value of the matrix:</p><div class="informalexample"><pre class="programlisting">println(matrixSummary.min)</pre></div><p>Here is the output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[0.0,0.0,0.0,0.0,0.0,0.0,0.0,-1.0,0.0,0.0,0.0,0.045564223,-1.0, ...</strong></span>
</pre></div><p>The following code statement will print the maximum value of the matrix:</p><div class="informalexample"><pre class="programlisting">println(matrixSummary.max)</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[0.999426,363.0,1.0,1.0,0.980392157,0.980392157,21.0,0.25,0.0,0.444444444, ...</strong></span>
</pre></div><p>The following code statement will print the variance of the matrix:</p><div class="informalexample"><pre class="programlisting">println(matrixSummary.variance)</pre></div><p>The output of the variance is:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[0.1097424416755897,74.30082476809638,0.04126316989120246, ...</strong></span>
</pre></div><p>The following code statement will print the nonzero number of the matrix:</p><div class="informalexample"><pre class="programlisting">println(matrixSummary.numNonzeros)</pre></div><p>Here is the output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[5053.0,7354.0,7172.0,6821.0,6160.0,5128.0,7350.0,1257.0,0.0, ...</strong></span>
</pre></div><p>The <code class="literal">computeColumnSummaryStatistics</code> method computes a number of statistics over each column of features, including the mean and variance, storing each of these in a <code class="literal">Vector</code> with one entry per column (that is, one entry per feature in our case).</p><p>Looking at the preceding output for mean and variance, we can see quite clearly that the second feature has a much higher mean and variance than some of the other features (you will find a few other features that are similar and a few others that are more extreme). So, our data definitely does not conform to a standard Gaussian distribution in its raw form. To get the data in a more suitable form for our models, we can standardize each feature such that it has zero mean and unit standard deviation. We can do this by subtracting the column mean from each feature value and then scaling it by dividing it by the column standard deviation for the feature:</p><div class="informalexample"><pre class="programlisting">(x â€“ Î¼) / sqrt(variance)</pre></div><p>Practically, for each feature vector in our input dataset, we can simply perform an element-wise subtraction of the preceding mean vector from the feature vector and then perform an element-wise division of the feature vector by the vector of feature standard deviations. The standard deviation vector itself can be obtained by performing an element-wise square root operation on the variance vector.</p><p>As we mentioned in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Obtaining, Processing, and Preparing Data with Spark</em></span>, we fortunately have access to a convenience method from Spark's <code class="literal">StandardScaler</code> to accomplish this.</p><p><code class="literal">StandardScaler</code> works in much the same way as the <code class="literal">Normalizer</code> feature we used in that chapter. We will instantiate it by passing in two arguments that tell it whether to subtract the mean from the data and whether to apply standard deviation scaling. We will then<a id="id495" class="indexterm"></a> fit <code class="literal">StandardScaler</code> on our input <code class="literal">vectors</code>. Finally, we will pass in an input vector to the <code class="literal">transform</code> function, which will then <a id="id496" class="indexterm"></a>return a normalized vector. We will do this within the following <code class="literal">map</code> function to preserve the <code class="literal">label</code> from our dataset:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark.mllib.feature.StandardScaler
val scaler = new StandardScaler(withMean = true, withStd = true).fit(vectors)
val scaledData = data.map(lp =&gt; LabeledPoint(lp.label, scaler.transform(lp.features)))</pre></div><p>Our data should now be standardized. Let's inspect the first row of the original and standardized features:</p><div class="informalexample"><pre class="programlisting">println(data.first.features)</pre></div><p>The output of the preceding line of code is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[0.789131,2.055555556,0.676470588,0.205882353,</strong></span>
</pre></div><p>The following code will the first row of the standardized features:</p><div class="informalexample"><pre class="programlisting">println(scaledData.first.features)</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[1.1376439023494747,-0.08193556218743517,1.025134766284205,-0.0558631837375738,</strong></span>
</pre></div><p>As we can see, the first feature has been transformed by applying the standardization formula. We can check this by subtracting the mean (which we computed earlier) from the first feature and dividing the result by the square root of the variance (which we computed earlier):</p><div class="informalexample"><pre class="programlisting">println((0.789131 - 0.41225805299526636)/ math.sqrt(0.1097424416755897))</pre></div><p>The result should be equal to the first element of our scaled vector:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>1.137647336497682</strong></span>
</pre></div><p>We can<a id="id497" class="indexterm"></a> now retrain our model using the standardized data. We will use only the logistic regression model to illustrate the impact of feature standardization (since the decision tree and naÃ¯ve Bayes are not impacted by this):</p><div class="informalexample"><pre class="programlisting">val lrModelScaled = LogisticRegressionWithSGD.train(scaledData, numIterations)
val lrTotalCorrectScaled = scaledData.map { point =&gt;
  if (lrModelScaled.predict(point.features) == point.label) 1 else 0
}.sum
val lrAccuracyScaled = lrTotalCorrectScaled / numData
val lrPredictionsVsTrue = scaledData.map { point =&gt; 
  (lrModelScaled.predict(point.features), point.label) 
}
val lrMetricsScaled = new BinaryClassificationMetrics(lrPredictionsVsTrue)
val lrPr = lrMetricsScaled.areaUnderPR
val lrRoc = lrMetricsScaled.areaUnderROC
println(f"${lrModelScaled.getClass.getSimpleName}\nAccuracy: ${lrAccuracyScaled * 100}%2.4f%%\nArea under PR: ${lrPr * 100.0}%2.4f%%\nArea under ROC: ${lrRoc * 100.0}%2.4f%%") </pre></div><p>The<a id="id498" class="indexterm"></a> result should look similar to this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>LogisticRegressionModel</strong></span>
<span class="strong"><strong>Accuracy: 62.0419%</strong></span>
<span class="strong"><strong>Area under PR: 72.7254%</strong></span>
<span class="strong"><strong>Area under ROC: 61.9663% </strong></span>
</pre></div><p>Simply through standardizing our features, we have improved the logistic regression performance for accuracy and AUC from 50 percent, no better than random, to 62 percent.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec52"></a>Additional features</h3></div></div></div><p>We <a id="id499" class="indexterm"></a>have seen that we need to be careful about standardizing and potentially normalizing our features, and the impact on model performance can be serious. In this case, we used only a portion of the features available. For example, we completely ignored the category variable and the textual content in the boilerplate variable column.</p><p>This was done for ease of illustration, but let's assess the impact of adding an additional feature such as the category feature.</p><p>First, we will inspect the categories and form a mapping of index to category, which you might recognize as the basis for a <span class="emphasis"><em>1-of-k</em></span> encoding of this categorical feature:</p><div class="informalexample"><pre class="programlisting">val categories = records.map(r =&gt; r(3)).distinct.collect.zipWithIndex.toMap
val numCategories = categories.size
println(categories)</pre></div><p>The output of the different categories is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Map("weather" -&gt; 0, "sports" -&gt; 6, "unknown" -&gt; 4, "computer_internet" -&gt; 12, "?" -&gt; 11, "culture_politics" -&gt; 3, "religion" -&gt; 8, "recreation" -&gt; 2, "arts_entertainment" -&gt; 9, "health" -&gt; 5, "law_crime" -&gt; 10, "gaming" -&gt; 13, "business" -&gt; 1, "science_technology" -&gt; 7)</strong></span>
</pre></div><p>The following code will print the number of categories:</p><div class="informalexample"><pre class="programlisting">println(numCategories)</pre></div><p>Here is the output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>14</strong></span>
</pre></div><p>So, we will need to create a vector of length 14 to represent this feature and assign a value of 1 for the index of the relevant category for each data point. We can then prepend this new feature vector to the vector of other numerical features:</p><div class="informalexample"><pre class="programlisting">val dataCategories = records.map { r =&gt;
  val trimmed = r.map(_.replaceAll("\"", ""))
  val label = trimmed(r.size - 1).toInt
  val categoryIdx = categories(r(3))
  val categoryFeatures = Array.ofDim[Double](numCategories)
  categoryFeatures(categoryIdx) = 1.0
  val otherFeatures = trimmed.slice(4, r.size - 1).map(d =&gt; if (d == "?") 0.0 else d.toDouble)
  val features = categoryFeatures ++ otherFeatures
  LabeledPoint(label, Vectors.dense(features))
}
println(dataCategories.first)</pre></div><p>You should see output similar to what is shown here. You can see that the first part of our feature vector is now a vector of length 14 with one nonzero entry at the relevant category index:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>LabeledPoint(0.0, [0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.789131,2.055555556,0.676470588,0.205882353,0.047058824,0.023529412,0.443783175,0.0,0.0,0.09077381,0.0,0.245831182,0.003883495,1.0,1.0,24.0,0.0,5424.0,170.0,8.0,0.152941176,0.079129575])</strong></span>
</pre></div><p>Again, since our raw features are not standardized, we should perform this transformation using the same <code class="literal">StandardScaler</code> approach that we used earlier before training a new model on this expanded dataset:</p><div class="informalexample"><pre class="programlisting">val scalerCats = new StandardScaler(withMean = true, withStd = true).fit(dataCategories.map(lp =&gt; lp.features))
val scaledDataCats = dataCategories.map(lp =&gt; LabeledPoint(lp.label, scalerCats.transform(lp.features)))</pre></div><p>We can inspect the features before and after scaling as we did earlier:</p><div class="informalexample"><pre class="programlisting">println(dataCategories.first.features)</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.789131,2.055555556 ...</strong></span>
</pre></div><p>The following code will print the features after scaling:</p><div class="informalexample"><pre class="programlisting">println(scaledDataCats.first.features)</pre></div><p>You will see the following on the screen:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[-0.023261105535492967,2.720728254208072,-0.4464200056407091,-0.2205258360869135, ...</strong></span>
</pre></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip29"></a>Tip</h3><p>Note<a id="id500" class="indexterm"></a> that while the original raw features were sparse (that is, there are many entries that are zero), if we subtract the mean from each entry, we would end up with a non-sparse (dense) representation, as can be seen in the preceding example.</p><p>This is not a problem in this case as the data size is small, but often large-scale real-world problems have extremely sparse input data with many features (online advertising and text classification are good examples). In this case, it is not advisable to lose this sparsity, as the memory and processing requirements for the equivalent dense representation can quickly explode with many millions of features. We can use StandardScaler and set <code class="literal">withMean</code> to <code class="literal">false</code> to avoid this.</p></div><p>We're now ready to train a new logistic regression model with our expanded feature set, and then we will evaluate the performance:</p><div class="informalexample"><pre class="programlisting">val lrModelScaledCats = LogisticRegressionWithSGD.train(scaledDataCats, numIterations)
val lrTotalCorrectScaledCats = scaledDataCats.map { point =&gt;
  if (lrModelScaledCats.predict(point.features) == point.label) 1 else 0
}.sum
val lrAccuracyScaledCats = lrTotalCorrectScaledCats / numData
val lrPredictionsVsTrueCats = scaledDataCats.map { point =&gt; 
  (lrModelScaledCats.predict(point.features), point.label) 
}
val lrMetricsScaledCats = new BinaryClassificationMetrics(lrPredictionsVsTrueCats)
val lrPrCats = lrMetricsScaledCats.areaUnderPR
val lrRocCats = lrMetricsScaledCats.areaUnderROC
println(f"${lrModelScaledCats.getClass.getSimpleName}\nAccuracy: ${lrAccuracyScaledCats * 100}%2.4f%%\nArea under PR: ${lrPrCats * 100.0}%2.4f%%\nArea under ROC: ${lrRocCats * 100.0}%2.4f%%") </pre></div><p>You should see output similar to this one:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>LogisticRegressionModel</strong></span>
<span class="strong"><strong>Accuracy: 66.5720%</strong></span>
<span class="strong"><strong>Area under PR: 75.7964%</strong></span>
<span class="strong"><strong>Area under ROC: 66.5483%</strong></span>
</pre></div><p>By <a id="id501" class="indexterm"></a>applying a feature standardization transformation to our data, we improved both the accuracy and AUC measures from 50 percent to 62 percent, and then, we achieved a further boost to 66 percent by adding the category feature into our model (remember to apply the standardization to our new feature set).</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note25"></a>Note</h3><p>Note that the best model performance in the competition was an AUC of 0.88906 (see <a class="ulink" href="http://www.kaggle.com/c/stumbleupon/leaderboard/private" target="_blank">http://www.kaggle.com/c/stumbleupon/leaderboard/private</a>).</p><p>One approach to achieving performance almost as high is outlined at <a class="ulink" href="http://www.kaggle.com/c/stumbleupon/forums/t/5680/beating-the-benchmark-leaderboard-auc-0-878" target="_blank">http://www.kaggle.com/c/stumbleupon/forums/t/5680/beating-the-benchmark-leaderboard-auc-0-878</a>.</p><p>Notice that there are still features that we have not yet used; most notably, the text features in the boilerplate variable. The leading competition submissions predominantly use the boilerplate features and features based on the raw textual content to achieve their performance. As we saw earlier, while adding category-improved performance, it appears that most of the variables are not very useful as predictors, while the textual content turned out to be highly predictive.</p><p>Going through some of the best performing approaches for these competitions can give you a good idea as to how feature extraction and engineering play a critical role in model performance.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec53"></a>Using the correct form of data</h3></div></div></div><p>Another<a id="id502" class="indexterm"></a> critical aspect of model performance <a id="id503" class="indexterm"></a>is using the correct form of data for each model. Previously, we saw that applying a naÃ¯ve Bayes model to our numerical features resulted in very poor performance. Is this because the model itself is deficient?</p><p>In this case, recall that MLlib implements a multinomial model. This model works on input in the form of non-zero count data. This can include a binary representation of categorical features (such as the <span class="emphasis"><em>1-of-k</em></span> encoding covered previously) or frequency data (such as the frequency of occurrences of words in a document). The numerical features we used initially do not conform to this assumed input distribution, so it is probably unsurprising that the model did so poorly.</p><p>To illustrate this, we'll use only the category feature, which, when <span class="emphasis"><em>1-of-k</em></span> encoded, is of the correct form for the model. We will create a new dataset as follows:</p><div class="informalexample"><pre class="programlisting">val dataNB = records.map { r =&gt;
  val trimmed = r.map(_.replaceAll("\"", ""))
  val label = trimmed(r.size - 1).toInt
  val categoryIdx = categories(r(3))
  val categoryFeatures = Array.ofDim[Double](numCategories)
  categoryFeatures(categoryIdx) = 1.0
  LabeledPoint(label, Vectors.dense(categoryFeatures))
}</pre></div><p>Next, we will train a new naÃ¯ve Bayes model and evaluate its performance:</p><div class="informalexample"><pre class="programlisting">val nbModelCats = NaiveBayes.train(dataNB)
val nbTotalCorrectCats = dataNB.map { point =&gt;
  if (nbModelCats.predict(point.features) == point.label) 1 else 0
}.sum
val nbAccuracyCats = nbTotalCorrectCats / numData
val nbPredictionsVsTrueCats = dataNB.map { point =&gt; 
  (nbModelCats.predict(point.features), point.label) 
}
val nbMetricsCats = new BinaryClassificationMetrics(nbPredictionsVsTrueCats)
val nbPrCats = nbMetricsCats.areaUnderPR
val nbRocCats = nbMetricsCats.areaUnderROC
println(f"${nbModelCats.getClass.getSimpleName}\nAccuracy: ${nbAccuracyCats * 100}%2.4f%%\nArea under PR: ${nbPrCats * 100.0}%2.4f%%\nArea under ROC: ${nbRocCats * 100.0}%2.4f%%")</pre></div><p>You<a id="id504" class="indexterm"></a> should see the following output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>NaiveBayesModel</strong></span>
<span class="strong"><strong>Accuracy: 60.9601%</strong></span>
<span class="strong"><strong>Area under PR: 74.0522%</strong></span>
<span class="strong"><strong>Area under ROC: 60.5138%</strong></span>
</pre></div><p>So, by<a id="id505" class="indexterm"></a> ensuring that we use the correct form of input, we have improved the performance of the naÃ¯ve Bayes model slightly from 58 percent to 60 percent.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec54"></a>Tuning model parameters</h3></div></div></div><p>The previous<a id="id506" class="indexterm"></a> section showed the impact on model performance of feature extraction and selection, as well as the form of input data and a model's assumptions around data distributions. So far, we have discussed model parameters only in passing, but they also play a significant role in model performance.</p><p>MLlib's default <code class="literal">train</code> methods use default values for the parameters of each model. Let's take a deeper look at them.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl3sec15"></a>Linear models</h4></div></div></div><p>Both <a id="id507" class="indexterm"></a>logistic regression and SVM share the same parameters, because <a id="id508" class="indexterm"></a>they use the same underlying optimization<a id="id509" class="indexterm"></a> technique of <span class="strong"><strong>stochastic gradient descent</strong></span> (<span class="strong"><strong>SGD</strong></span>). They differ only in the loss function applied. If we take a look at the class definition for logistic regression in MLlib, we will see the following definition:</p><div class="informalexample"><pre class="programlisting">class LogisticRegressionWithSGD private (
  private var stepSize: Double,
  private var numIterations: Int,
  private var regParam: Double,
  private var miniBatchFraction: Double)
  extends GeneralizedLinearAlgorithm[LogisticRegressionModel] ...</pre></div><p>We<a id="id510" class="indexterm"></a> can see that the arguments that can be passed to<a id="id511" class="indexterm"></a> the constructor are <code class="literal">stepSize</code>, <code class="literal">numIterations</code>, <code class="literal">regParam</code>, and <code class="literal">miniBatchFraction</code>. Of these, all except <code class="literal">regParam</code> are related to the underlying optimization technique.</p><p>The instantiation code for logistic regression initializes the <code class="literal">Gradient</code>, <code class="literal">Updater</code>, and <code class="literal">Optimizer</code> and sets the relevant arguments for <code class="literal">Optimizer</code> (<code class="literal">GradientDescent</code> in this case):</p><div class="informalexample"><pre class="programlisting">  private val gradient = new LogisticGradient()
  private val updater = new SimpleUpdater()
  override val optimizer = new GradientDescent(gradient, updater)
    .setStepSize(stepSize)
    .setNumIterations(numIterations)
    .setRegParam(regParam)
    .setMiniBatchFraction(miniBatchFraction)</pre></div><p><code class="literal">LogisticGradient</code> sets up the logistic loss function that defines our logistic regression model.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip30"></a>Tip</h3><p>While a detailed treatment of optimization techniques is beyond the scope of this book, MLlib provides two optimizers for linear models: SGD and L-BFGS. L-BFGS is often more accurate and has fewer parameters to tune.</p><p>SGD is the default, while L-BGFS can currently only be used directly for logistic regression via <code class="literal">LogisticRegressionWithLBFGS</code>. Try it out yourself and compare the results to those found with SGD.</p><p>See <a class="ulink" href="http://spark.apache.org/docs/latest/mllib-optimization.html" target="_blank">http://spark.apache.org/docs/latest/mllib-optimization.html</a> for further details.</p></div><p>To <a id="id512" class="indexterm"></a>investigate the impact of the remaining parameter<a id="id513" class="indexterm"></a> settings, we will create a helper function that will train a logistic regression model, given a set of parameter inputs. First, we will import the required classes:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.optimization.Updater
import org.apache.spark.mllib.optimization.SimpleUpdater
import org.apache.spark.mllib.optimization.L1Updater
import org.apache.spark.mllib.optimization.SquaredL2Updater
import org.apache.spark.mllib.classification.ClassificationModel</pre></div><p>Next, we will define our helper function to train a mode given a set of inputs:</p><div class="informalexample"><pre class="programlisting">def trainWithParams(input: RDD[LabeledPoint], regParam: Double, numIterations: Int, updater: Updater, stepSize: Double) = {
  val lr = new LogisticRegressionWithSGD
  lr.optimizer.setNumIterations(numIterations). setUpdater(updater).setRegParam(regParam).setStepSize(stepSize)
  lr.run(input)
}</pre></div><p>Finally, we will create a second helper function to take the input data and a classification model and generate the relevant AUC metrics:</p><div class="informalexample"><pre class="programlisting">def createMetrics(label: String, data: RDD[LabeledPoint], model: ClassificationModel) = {
  val scoreAndLabels = data.map { point =&gt;
    (model.predict(point.features), point.label)
  }
  val metrics = new BinaryClassificationMetrics(scoreAndLabels)
  (label, metrics.areaUnderROC)
}</pre></div><p>We will also cache our scaled dataset, including categories, to speed up the multiple model training runs that we will be using to explore these different parameter settings:</p><div class="informalexample"><pre class="programlisting">scaledDataCats.cache</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch05lvl4sec06"></a>Iterations</h5></div></div></div><p>Many machine learning methods are iterative in nature, converging to a solution (the optimal weight <a id="id514" class="indexterm"></a>vector that minimizes the chosen loss function) over a number of iteration steps. SGD typically requires relatively few iterations to converge to a reasonable solution but can be run for more iterations to improve the solution. We can see this by trying a few different settings for the <code class="literal">numIterations</code> parameter and comparing the AUC results:</p><div class="informalexample"><pre class="programlisting">val iterResults = Seq(1, 5, 10, 50).map { param =&gt;
  val model = trainWithParams(scaledDataCats, 0.0, param, new SimpleUpdater, 1.0)
  createMetrics(s"$param iterations", scaledDataCats, model)
}
iterResults.foreach { case (param, auc) =&gt; println(f"$param, AUC = ${auc * 100}%2.2f%%") }</pre></div><p>Your <a id="id515" class="indexterm"></a>output should look like this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>1 iterations, AUC = 64.97%</strong></span>
<span class="strong"><strong>5 iterations, AUC = 66.62%</strong></span>
<span class="strong"><strong>10 iterations, AUC = 66.55%</strong></span>
<span class="strong"><strong>50 iterations, AUC = 66.81%</strong></span>
</pre></div><p>So, we can see that the number of iterations has minor impact on the results once a certain number of iterations have been completed.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch05lvl4sec07"></a>Step size</h5></div></div></div><p>In SGD, the<a id="id516" class="indexterm"></a> step size parameter controls how far in the direction of the steepest gradient the algorithm takes a step when updating the model weight vector after each training example. A larger step size might speed up convergence, but a step size that is too large might cause problems with convergence as good solutions are overshot.</p><p>We can see the impact of changing the step size here:</p><div class="informalexample"><pre class="programlisting">val stepResults = Seq(0.001, 0.01, 0.1, 1.0, 10.0).map { param =&gt;
  val model = trainWithParams(scaledDataCats, 0.0, numIterations, new SimpleUpdater, param)
  createMetrics(s"$param step size", scaledDataCats, model)
}
stepResults.foreach { case (param, auc) =&gt; println(f"$param, AUC = ${auc * 100}%2.2f%%") }</pre></div><p>This will give us the following results, which show that increasing the step size too much can begin to negatively impact performance.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>0.001 step size, AUC = 64.95%</strong></span>
<span class="strong"><strong>0.01 step size, AUC = 65.00%</strong></span>
<span class="strong"><strong>0.1 step size, AUC = 65.52%</strong></span>
<span class="strong"><strong>1.0 step size, AUC = 66.55%</strong></span>
<span class="strong"><strong>10.0 step size, AUC = 61.92%</strong></span>
</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch05lvl4sec08"></a>Regularization</h5></div></div></div><p>We briefly<a id="id517" class="indexterm"></a> touched on the <code class="literal">Updater</code> class in the preceding logistic regression code. An <code class="literal">Updater</code> class in MLlib implements regularization. Regularization can help avoid over-fitting of a model to training data by effectively penalizing model complexity. This can be done by adding a term to the loss function that acts to increase the loss as a function of the model weight vector.</p><p>Regularization is almost always required in real use cases, but is of particular importance when the feature dimension is very high (that is, the effective number of variable weights that can be learned is high) relative to the number of training examples.</p><p>When regularization is absent or low, models can tend to over-fit. Without regularization, most models will over-fit on a training dataset. This is a key reason behind the use of cross-validation techniques for model fitting (which we will cover now).</p><p>Conversely, since applying regularization encourages simpler models, model performance can suffer when regularization is high through under-fitting the data.</p><p>The forms of regularization available in MLlib are:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><code class="literal">SimpleUpdater</code>: This equates to no regularization and is the default for logistic<a id="id518" class="indexterm"></a> regression</p></li><li style="list-style-type: disc"><p><code class="literal">SquaredL2Updater</code>: This implements a regularizer based on the squared L2-norm<a id="id519" class="indexterm"></a> of the weight vector; this is the default for SVM models</p></li><li style="list-style-type: disc"><p><code class="literal">L1Updater</code>: This <a id="id520" class="indexterm"></a>applies a regularizer based on the L1-norm of the weight vector; this can lead to sparse solutions in the weight vector (as less important weights are pulled towards zero)</p></li></ul></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note26"></a>Note</h3><p>Regularization and its relation to optimization is a broad and heavily researched area. Some more information is available from the following links:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>General<a id="id521" class="indexterm"></a> regularization overview: <a class="ulink" href="http://en.wikipedia.org/wiki/Regularization_(mathematics)" target="_blank">http://en.wikipedia.org/wiki/Regularization_(mathematics)</a></p></li><li style="list-style-type: disc"><p>L2<a id="id522" class="indexterm"></a> regularization: <a class="ulink" href="http://en.wikipedia.org/wiki/Tikhonov_regularization" target="_blank">http://en.wikipedia.org/wiki/Tikhonov_regularization</a></p></li><li style="list-style-type: disc"><p>Over-fitting <a id="id523" class="indexterm"></a>and under-fitting: <a class="ulink" href="http://en.wikipedia.org/wiki/Overfitting" target="_blank">http://en.wikipedia.org/wiki/Overfitting</a></p></li><li style="list-style-type: disc"><p>Detailed overview of over-fitting and L1 versus L2 regularization: <a class="ulink" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.92.9860&amp;rep=rep1&amp;type=pdf" target="_blank">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.92.9860&amp;rep=rep1&amp;type=pdf</a></p></li></ul></div></div><p>Let's <a id="id524" class="indexterm"></a>explore the impact of a range of regularization parameters using <code class="literal">SquaredL2Updater</code>:</p><div class="informalexample"><pre class="programlisting">val regResults = Seq(0.001, 0.01, 0.1, 1.0, 10.0).map { param =&gt;
  val model = trainWithParams(scaledDataCats, param, numIterations, new SquaredL2Updater, 1.0)
  createMetrics(s"$param L2 regularization parameter", scaledDataCats, model)
}
regResults.foreach { case (param, auc) =&gt; println(f"$param, AUC = ${auc * 100}%2.2f%%") }</pre></div><p>Your output should look like this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>0.001 L2 regularization parameter, AUC = 66.55%</strong></span>
<span class="strong"><strong>0.01 L2 regularization parameter, AUC = 66.55%</strong></span>
<span class="strong"><strong>0.1 L2 regularization parameter, AUC = 66.63%</strong></span>
<span class="strong"><strong>1.0 L2 regularization parameter, AUC = 66.04%</strong></span>
<span class="strong"><strong>10.0 L2 regularization parameter, AUC = 35.33%</strong></span>
</pre></div><p>As we can see, at low levels of regularization, there is not much impact in model performance. However, as we increase regularization, we can see the impact of under-fitting on our model evaluation.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip31"></a>Tip</h3><p>You will find similar results when using the L1 regularization. Give it a try by performing the same evaluation of regularization parameter against the AUC measure for <code class="literal">L1Updater</code>.</p></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl3sec16"></a>Decision trees</h4></div></div></div><p>The decision<a id="id525" class="indexterm"></a> tree model we trained earlier was the best <a id="id526" class="indexterm"></a>performer on the raw data that we first used. We set a parameter called <code class="literal">maxDepth</code>, which controls the maximum depth of the tree and, thus, the complexity of the model. Deeper trees result in more complex models that will be able to fit the data better.</p><p>For classification problems, we can also select between two measures of impurity: <code class="literal">Gini</code> and <code class="literal">Entropy</code>.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch05lvl4sec09"></a>Tuning tree depth and impurity</h5></div></div></div><p>We will <a id="id527" class="indexterm"></a>illustrate the impact of tree depth in a similar <a id="id528" class="indexterm"></a>manner as we did for our logistic regression model.</p><p>First, we will need to create another helper function in the Spark shell:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark.mllib.tree.impurity.Impurity
import org.apache.spark.mllib.tree.impurity.Entropy
import org.apache.spark.mllib.tree.impurity.Gini

def trainDTWithParams(input: RDD[LabeledPoint], maxDepth: Int, impurity: Impurity) = {
  DecisionTree.train(input, Algo.Classification, impurity, maxDepth)
}</pre></div><p>Now, we're ready to compute our AUC metric for different settings of tree depth. We will simply use our original dataset in this example since we do not need the data to be standardized.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip32"></a>Tip</h3><p>Note that decision tree models generally do not require features to be standardized or normalized, nor do they require categorical features to be binary-encoded.</p></div><p>First, train the model using the <code class="literal">Entropy</code> impurity measure and varying tree depths:</p><div class="informalexample"><pre class="programlisting">val dtResultsEntropy = Seq(1, 2, 3, 4, 5, 10, 20).map { param =&gt;
  val model = trainDTWithParams(data, param, Entropy)
  val scoreAndLabels = data.map { point =&gt;
    val score = model.predict(point.features)
    (if (score &gt; 0.5) 1.0 else 0.0, point.label)
  }
  val metrics = new BinaryClassificationMetrics(scoreAndLabels)
  (s"$param tree depth", metrics.areaUnderROC)
}
dtResultsEntropy.foreach { case (param, auc) =&gt; println(f"$param, AUC = ${auc * 100}%2.2f%%") }</pre></div><p>This should output the results shown here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>1 tree depth, AUC = 59.33%</strong></span>
<span class="strong"><strong>2 tree depth, AUC = 61.68%</strong></span>
<span class="strong"><strong>3 tree depth, AUC = 62.61%</strong></span>
<span class="strong"><strong>4 tree depth, AUC = 63.63%</strong></span>
<span class="strong"><strong>5 tree depth, AUC = 64.88%</strong></span>
<span class="strong"><strong>10 tree depth, AUC = 76.26%</strong></span>
<span class="strong"><strong>20 tree depth, AUC = 98.45%</strong></span>
</pre></div><p>Next, we <a id="id529" class="indexterm"></a>will perform the same computation using<a id="id530" class="indexterm"></a> the <code class="literal">Gini</code> impurity measure (we omitted the code as it is very similar, but it can be found in the code bundle). Your results should look something like this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>1 tree depth, AUC = 59.33%</strong></span>
<span class="strong"><strong>2 tree depth, AUC = 61.68%</strong></span>
<span class="strong"><strong>3 tree depth, AUC = 62.61%</strong></span>
<span class="strong"><strong>4 tree depth, AUC = 63.63%</strong></span>
<span class="strong"><strong>5 tree depth, AUC = 64.89%</strong></span>
<span class="strong"><strong>10 tree depth, AUC = 78.37%</strong></span>
<span class="strong"><strong>20 tree depth, AUC = 98.87%</strong></span>
</pre></div><p>As you can see from the preceding results, increasing the tree depth parameter results in a more accurate model (as expected since the model is allowed to get more complex with greater tree depth). It is very likely that at higher tree depths, the model will over-fit the dataset significantly.</p><p>There is very little difference in performance between the two impurity measures.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl3sec17"></a>The naÃ¯ve Bayes model</h4></div></div></div><p>Finally, let's see the impact of changing the <code class="literal">lambda</code> parameter for naÃ¯ve Bayes. This parameter<a id="id531" class="indexterm"></a> controls additive smoothing, which handles <a id="id532" class="indexterm"></a>the case when a class and feature value do not occur together in the dataset.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip33"></a>Tip</h3><p>See <a class="ulink" href="http://en.wikipedia.org/wiki/Additive_smoothing" target="_blank">http://en.wikipedia.org/wiki/Additive_smoothing</a> for more details <a id="id533" class="indexterm"></a>on additive smoothing.</p></div><p>We will take the same approach as we did earlier, first creating a convenience training function and training the model with varying levels of <code class="literal">lambda</code>:</p><div class="informalexample"><pre class="programlisting">def trainNBWithParams(input: RDD[LabeledPoint], lambda: Double) = {
  val nb = new NaiveBayes
  nb.setLambda(lambda)
  nb.run(input)
}
val nbResults = Seq(0.001, 0.01, 0.1, 1.0, 10.0).map { param =&gt;
  val model = trainNBWithParams(dataNB, param)
  val scoreAndLabels = dataNB.map { point =&gt;
    (model.predict(point.features), point.label)
  }
  val metrics = new BinaryClassificationMetrics(scoreAndLabels)
  (s"$param lambda", metrics.areaUnderROC)
}
nbResults.foreach { case (param, auc) =&gt; println(f"$param, AUC = ${auc * 100}%2.2f%%") 
}</pre></div><p>The results of the training are as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>0.001 lambda, AUC = 60.51%</strong></span>
<span class="strong"><strong>0.01 lambda, AUC = 60.51%</strong></span>
<span class="strong"><strong>0.1 lambda, AUC = 60.51%</strong></span>
<span class="strong"><strong>1.0 lambda, AUC = 60.51%</strong></span>
<span class="strong"><strong>10.0 lambda, AUC = 60.51%</strong></span>
</pre></div><p>We <a id="id534" class="indexterm"></a>can see that <code class="literal">lambda</code> has no impact in this case, since<a id="id535" class="indexterm"></a> it will not be a problem if the combination of feature and class label not occurring together in the dataset.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl2sec55"></a>Cross-validation</h3></div></div></div><p>So far in<a id="id536" class="indexterm"></a> this book, we have only briefly mentioned the idea of cross-validation and out-of-sample testing. Cross-validation is a critical part of real-world machine learning and is central to many model selection and parameter tuning pipelines.</p><p>The general idea behind cross-validation is that we want to know how our model will perform on unseen data. Evaluating this on real, live data (for example, in a production system) is risky, because we don't really know whether the trained model is the best in the sense of being able to make accurate predictions on new data. As we saw previously with regard to regularization, our model might have over-fit the training data and be poor at making predictions on data it has not been trained on.</p><p>Cross-validation provides a mechanism where we use part of our available dataset to train our model and another part to evaluate the performance of this model. As the model is tested on data that it has not seen during the training phase, its performance, when evaluated on this part of the dataset, gives us an estimate as to how well our model generalizes for the new data points.</p><p>Here, we will implement a simple cross-validation evaluation approach using a train-test split. We <a id="id537" class="indexterm"></a>will divide our dataset into two non-overlapping parts. The first dataset is used to train our model and is called the training set. The second dataset, called the test set or hold-out set, is used to evaluate the performance of our model using our chosen evaluation measure. Common splits used in practice include 50/50, 60/40, and 80/20 splits, but you can use any split as long as the training set is not too small for the model to learn (generally, at least 50 percent is a practical minimum).</p><p>In many cases, three sets are created: a training set, an evaluation set (which is used like the above test set to tune the model parameters such as lambda and step size), and a test set (which is never used to train a model or tune any parameters, but is only used to generate an estimated true performance on completely unseen data).</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note27"></a>Note</h3><p>Here, we will explore a simple train-test split approach. There are many cross-validation techniques that are more exhaustive and complex.</p><p>One popular example is K-fold cross-validation, where the dataset is split into K non-overlapping folds. The model is trained on K-1 folds of data and tested on the remaining, held-out fold. This is repeated K times, and the results are averaged to give the cross-validation score. The train-test split is effectively like two-fold cross-validation.</p><p>Other<a id="id538" class="indexterm"></a> approaches include leave-one-out cross-validation and random sampling. See the article at <a class="ulink" href="http://en.wikipedia.org/wiki/Cross-validation_(statistics)" target="_blank">http://en.wikipedia.org/wiki/Cross-validation_(statistics)</a> for further details.</p></div><p>First, we will split our dataset into a 60 percent training set and a 40 percent test set (we will use a constant random seed of 123 here to ensure that we get the same results for ease of illustration):</p><div class="informalexample"><pre class="programlisting">val trainTestSplit = scaledDataCats.randomSplit(Array(0.6, 0.4), 123)
val train = trainTestSplit(0)
val test = trainTestSplit(1)</pre></div><p>Next, we will compute the evaluation metric of interest (again, we will use AUC) for a range of regularization parameter settings. Note that here we will use a finer-grained step size between the evaluated regularization parameters to better illustrate the differences in AUC, which are very small in this case:</p><div class="informalexample"><pre class="programlisting">val regResultsTest = Seq(0.0, 0.001, 0.0025, 0.005, 0.01).map { param =&gt;
  val model = trainWithParams(<span class="strong"><strong>train</strong></span>, param, numIterations, new SquaredL2Updater, 1.0)
  createMetrics(s"$param L2 regularization parameter", test, model)
}
regResultsTest.foreach { case (param, auc) =&gt; println(f"$param, AUC = ${auc * 100}%2.6f%%") 
}</pre></div><p>This will compute the results of training on the training set and the results of evaluating on the test set, as shown here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>0.0 L2 regularization parameter, AUC = 66.480874%</strong></span>
<span class="strong"><strong>0.001 L2 regularization parameter, AUC = 66.480874%</strong></span>
<span class="strong"><strong>0.0025 L2 regularization parameter, AUC = 66.515027%</strong></span>
<span class="strong"><strong>0.005 L2 regularization parameter, AUC = 66.515027%</strong></span>
<span class="strong"><strong>0.01 L2 regularization parameter, AUC = 66.549180%</strong></span>
</pre></div><p>Now, let's compare this to the results of training and testing on the training set (this is what we were doing previously by training and testing on all data). Again, we will omit the code as it is very similar (but it is available in the code bundle):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>0.0 L2 regularization parameter, AUC = 66.260311%</strong></span>
<span class="strong"><strong>0.001 L2 regularization parameter, AUC = 66.260311%</strong></span>
<span class="strong"><strong>0.0025 L2 regularization parameter, AUC = 66.260311%</strong></span>
<span class="strong"><strong>0.005 L2 regularization parameter, AUC = 66.238294%</strong></span>
<span class="strong"><strong>0.01 L2 regularization parameter, AUC = 66.238294%</strong></span>
</pre></div><p>So, we <a id="id539" class="indexterm"></a>can see that when we train and evaluate our model on the same dataset, we generally achieve the highest performance when regularization is lower. This is because our model has seen all the data points, and with low levels of regularization, it can over-fit the data set and achieve higher performance.</p><p>In contrast, when we train on one dataset and test on another, we see that generally a slightly higher level of regularization results in better test set performance.</p><p>In cross-validation, we would typically find the parameter settings (including regularization as well as the various other parameters such as step size and so on) that result in the best test set performance. We would then use these parameter settings to retrain the <a id="id540" class="indexterm"></a>model on all of our data in order to use it to make predictions on new data.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip34"></a>Tip</h3><p>Recall from <a class="link" href="#" linkend="ch04">Chapter 4</a>, <span class="emphasis"><em>Building a Recommendation Engine with Spark</em></span>, that we did not cover cross-validation. You can apply the same techniques we used earlier to split the ratings dataset from that chapter into a training and test dataset. You can then try out different parameter settings on the training set while evaluating the MSE and MAP performance metrics on the test set in a manner similar to what we did earlier. Give it a try!</p></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch05lvl1sec39"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we covered the various classification models available in Spark MLlib, and we saw how to train models on input data and how to evaluate their performance using standard metrics and measures. We also explored how to apply some of the techniques previously introduced to transform our features. Finally, we investigated the impact of using the correct input data format or distribution on model performance, and we also saw the impact of adding more data to our model, tuning model parameters, and implementing cross-validation.</p><p>In the next chapter, we will take a similar approach to delve into MLlib's regression models.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch06"></a>ChapterÂ 6.Â Building a Regression Model with Spark</h2></div></div></div><p>In this chapter, we will build on what we covered in <a class="link" href="#" linkend="ch05">Chapter 5</a>, <span class="emphasis"><em>Building a Classification Model with Spark</em></span>. While classification models deal with outcomes that represent discrete classes, regression models are concerned with target variables that can take any real value. The underlying principle is very similarâ€”we wish to find a model that maps input features to predicted target variables. Like classification, regression is also a form of supervised learning.</p><p>Regression models can be used to predict just about any variable of interest. A few examples include the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Predicting stock returns and other economic variables</p></li><li style="list-style-type: disc"><p>Predicting loss amounts for loan defaults (this can be combined with a classification model that predicts the probability of default, while the regression model predicts the amount in the case of a default)</p></li><li style="list-style-type: disc"><p>Recommendations (the Alternating Least Squares factorization model from <a class="link" href="#" linkend="ch04">Chapter 4</a>, <span class="emphasis"><em>Building a Recommendation Engine with Spark</em></span>, uses linear regression in each iteration)</p></li><li style="list-style-type: disc"><p>Predicting <span class="strong"><strong>customer lifetime value</strong></span> (<span class="strong"><strong>CLTV</strong></span>) in a retail, mobile, or other business, based<a id="id541" class="indexterm"></a> on user behavior and spending patterns</p></li></ul></div><p>In the following sections, we will:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Introduce the various types of regression models available in MLlib</p></li><li style="list-style-type: disc"><p>Explore feature extraction and target variable transformation for regression models</p></li><li style="list-style-type: disc"><p>Train a number of regression models using MLlib</p></li><li style="list-style-type: disc"><p>See how to make predictions using the trained models</p></li><li style="list-style-type: disc"><p>Investigate the impact on performance of various parameter settings for regression using cross-validation</p></li></ul></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec40"></a>Types of regression models</h2></div></div><hr /></div><p>Spark's MLlib library offers two broad classes of regression models: linear models and decision <a id="id542" class="indexterm"></a>tree regression models.</p><p>Linear models are essentially the same as their classification counterparts, the only difference is that linear regression models use a different loss function, related link function, and decision function. MLlib provides a standard least squares regression model (although other types of generalized linear models for regression are planned).</p><p>Decision trees can also be used for regression by changing the impurity measure.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec56"></a>Least squares regression</h3></div></div></div><p>You<a id="id543" class="indexterm"></a> might recall from <a class="link" href="#" linkend="ch05">Chapter 5</a>, <span class="emphasis"><em>Building a Classification Model with Spark</em></span>, that there are a variety of loss functions that can be <a id="id544" class="indexterm"></a>applied to generalized linear models. The loss function used for least squares is the squared loss, which is defined as follows:</p><div class="informalexample"><pre class="programlisting">Â½ (w<sup>T</sup>x - y)<sup>2</sup></pre></div><p>Here, as for the classification setting, <span class="emphasis"><em>y</em></span> is the target variable (this time, real valued), <span class="emphasis"><em>w</em></span> is the weight vector, and <span class="emphasis"><em>x</em></span> is the feature vector.</p><p>The related link function is the identity link, and the decision function is also the identity function, as generally, no thresholding is applied in regression. So, the model's prediction is simply <span class="emphasis"><em>y = w<sup>T</sup>x</em></span>.</p><p>The standard least squares regression in MLlib does not use regularization. Looking at the squared loss function, we can see that the loss applied to incorrectly predicted points will be magnified since the loss is squared. This means that least squares regression is susceptible to outliers in the dataset and also to over-fitting. Generally, as for classification, we should apply some level of regularization in practice.</p><p>Linear regression with L2 regularization is commonly referred to as ridge regression, while applying <a id="id545" class="indexterm"></a>L1 regularization is called the <span class="strong"><strong>lasso</strong></span>.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip35"></a>Tip</h3><p>See the section on linear least squares in the Spark MLlib documentation at <a class="ulink" href="http://spark.apache.org/docs/latest/mllib-linear-methods.html#linear-least-squares-lasso-and-ridge-regression" target="_blank">http://spark.apache.org/docs/latest/mllib-linear-methods.html#linear-least-squares-lasso-and-ridge-regression</a> for further information.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec57"></a>Decision trees for regression</h3></div></div></div><p>Just like<a id="id546" class="indexterm"></a> using linear models for regression<a id="id547" class="indexterm"></a> tasks involves changing the loss function used, using decision trees for regression involves changing the measure of the node impurity used. The<a id="id548" class="indexterm"></a> impurity metric is called <span class="strong"><strong>variance</strong></span> and is defined in the same way as the squared loss for least squares linear regression.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note28"></a>Note</h3><p>See<a id="id549" class="indexterm"></a> the <span class="emphasis"><em>MLlib - Decision Tree</em></span> section in the Spark documentation at <a class="ulink" href="http://spark.apache.org/docs/latest/mllib-decision-tree.html" target="_blank">http://spark.apache.org/docs/latest/mllib-decision-tree.html</a> for further details on the decision tree algorithm and impurity measure for regression.</p></div><p>Now, we will plot a simple example of a regression problem with only one input variable shown on the <span class="emphasis"><em>x</em></span> axis and the target variable on the <span class="emphasis"><em>y</em></span> axis. The linear model prediction function is shown by a red dashed line, while the decision tree prediction function is shown by a green dashed line. We can see that the decision tree allows a more complex, nonlinear model to be fitted to the data.</p><div class="mediaobject"><img src="graphics/8519OS_06_00.jpg" /><div class="caption"><p>Linear model and decision tree prediction functions for regression</p></div></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec41"></a>Extracting the right features from your data</h2></div></div><hr /></div><p>As the <a id="id550" class="indexterm"></a>underlying models for regression are the same as<a id="id551" class="indexterm"></a> those for the classification case, we can use the same approach to create input features. The only practical difference is that the target is now a real-valued variable, as opposed to a categorical one. The <code class="literal">LabeledPoint</code> class in MLlib already takes this into account, as the <code class="literal">label</code> field is of the <code class="literal">Double</code> type, so it can handle both cases.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec58"></a>Extracting features from the bike sharing dataset</h3></div></div></div><p>To<a id="id552" class="indexterm"></a> illustrate the concepts in this chapter, we <a id="id553" class="indexterm"></a>will be using the bike sharing dataset. This dataset contains hourly records of the number of bicycle rentals in the capital bike sharing system. It also contains variables related to date and time, weather, and seasonal and holiday information.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note29"></a>Note</h3><p>The dataset is available at <a class="ulink" href="http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset" target="_blank">http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset</a>.</p><p>Click on the <span class="strong"><strong>Data Folder</strong></span> link and then download the <code class="literal">Bike-Sharing-Dataset.zip</code> file.</p><p>The bike sharing data was enriched with weather and seasonal data by Hadi Fanaee-T at the University of Porto and used in the following paper:</p><p>Fanaee-T, Hadi and Gama Joao, Event labeling combining ensemble detectors and background knowledge, <span class="emphasis"><em>Progress in Artificial Intelligence</em></span>, pp. 1-15, Springer Berlin Heidelberg, 2013.</p><p>The paper is available at <a class="ulink" href="http://link.springer.com/article/10.1007%2Fs13748-013-0040-3" target="_blank">http://link.springer.com/article/10.1007%2Fs13748-013-0040-3</a>.</p></div><p>Once you have downloaded the <code class="literal">Bike-Sharing-Dataset.zip</code> file, unzip it. This will create a directory called <code class="literal">Bike-Sharing-Dataset</code>, which contains the <code class="literal">day.csv</code>, <code class="literal">hour.csv</code>, and the <code class="literal">Readme.txt</code> files.</p><p>The <code class="literal">Readme.txt</code> file <a id="id554" class="indexterm"></a>contains information on the dataset, including <a id="id555" class="indexterm"></a>the variable names and descriptions. Take a look at the file, and you will see that we have the following variables available:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><code class="literal">instant</code>: This is the record ID</p></li><li style="list-style-type: disc"><p><code class="literal">dteday</code>: This is the raw date</p></li><li style="list-style-type: disc"><p><code class="literal">season</code>: This is different seasons such as spring, summer, winter, and fall</p></li><li style="list-style-type: disc"><p><code class="literal">yr</code>: This is the year (2011 or 2012)</p></li><li style="list-style-type: disc"><p><code class="literal">mnth</code>: This is the month of the year</p></li><li style="list-style-type: disc"><p><code class="literal">hr</code>: This is the hour of the day</p></li><li style="list-style-type: disc"><p><code class="literal">holiday</code>: This is whether the day was a holiday or not</p></li><li style="list-style-type: disc"><p><code class="literal">weekday</code>: This is the day of the week</p></li><li style="list-style-type: disc"><p><code class="literal">workingday</code>: This is whether the day was a working day or not</p></li><li style="list-style-type: disc"><p><code class="literal">weathersit</code>: This is a categorical variable that describes the weather at a particular time</p></li><li style="list-style-type: disc"><p><code class="literal">temp</code>: This is<a id="id556" class="indexterm"></a> the normalized temperature</p></li><li style="list-style-type: disc"><p><code class="literal">atemp</code>: This is the normalized apparent temperature</p></li><li style="list-style-type: disc"><p><code class="literal">hum</code>: This is the normalized humidity</p></li><li style="list-style-type: disc"><p><code class="literal">windspeed</code>: This is the normalized wind speed</p></li><li style="list-style-type: disc"><p><code class="literal">cnt</code>: This is the target variable, that is, the count of bike rentals for that hour</p></li></ul></div><p>We <a id="id557" class="indexterm"></a>will work with the hourly data contained<a id="id558" class="indexterm"></a> in <code class="literal">hour.csv</code>. If you look at the first line of the dataset, you will see that it contains the column names as a header. You can do this by running the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;head -1 hour.csv</strong></span>
</pre></div><p>This should output the following result:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>instant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt</strong></span>
</pre></div><p>Before we work with the data in Spark, we will again remove the header from the first line of the file using the same <code class="literal">sed</code> command that we used previously to create a new file called <code class="literal">hour_noheader.csv</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;sed 1d hour.csv &gt; hour_noheader.csv</strong></span>
</pre></div><p>Since we will be doing some plotting of our dataset later on, we will use the Python shell for this chapter. This also serves to illustrate how to use MLlib's linear model and decision tree functionality from PySpark.</p><p>Start up your PySpark shell from your Spark installation directory. If you want to use IPython, which we highly recommend, remember to include the <code class="literal">IPYTHON=1</code> environment variable together with the <code class="literal">pylab</code> functionality:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;IPYTHON=1 IPYTHON_OPTS="â€”pylab" ./bin/pyspark</strong></span>
</pre></div><p>If you prefer to use IPython Notebook, you can start it with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;IPYTHON=1 IPYTHON_OPTS=notebook ./bin/pyspark</strong></span>
</pre></div><p>You can type all the code that follows for the remainder of this chapter directly into your PySpark shell (or into IPython Notebook if you wish to use it).</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip36"></a>Tip</h3><p>Recall that we used the IPython shell in <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Obtaining, Processing, and Preparing Data with Spark</em></span>. Take a look at that chapter and the code bundle for instructions to install IPython.</p></div><p>We'll start as usual by loading the dataset and inspecting it:</p><div class="informalexample"><pre class="programlisting">path = "/<span class="strong"><strong>PATH</strong></span>/hour_noheader.csv"
raw_data = sc.textFile(path)
num_data = raw_data.count()
records = raw_data.map(lambda x: x.split(","))
first = records.first()
print first
print num_data</pre></div><p>You<a id="id559" class="indexterm"></a> should see the following output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[u'1', u'2011-01-01', u'1', u'0', u'1', u'0', u'0', u'6', u'0', u'1', u'0.24', u'0.2879', u'0.81', u'0', u'3', u'13', u'16']</strong></span>
<span class="strong"><strong>17379</strong></span>
</pre></div><p>So, we <a id="id560" class="indexterm"></a>have <code class="literal">17,379</code> hourly records in our dataset. We have inspected the column names already. We will ignore the record ID and raw date columns. We will also ignore the <code class="literal">casual</code> and <code class="literal">registered</code> count target variables and focus on the overall count variable, <code class="literal">cnt</code> (which is the sum of the other two counts). We are left with 12 variables. The first eight are categorical, while the last 4 are normalized real-valued variables.</p><p>To deal with the eight categorical variables, we will use the binary encoding approach with which you should be quite familiar by now. The four real-valued variables will be left as is.</p><p>We will first cache our dataset, since we will be reading from it many times:</p><div class="informalexample"><pre class="programlisting">records.cache()</pre></div><p>In order to extract each categorical feature into a binary vector form, we will need to know the feature mapping of each feature value to the index of the nonzero value in our binary vector. Let's define a function that will extract this mapping from our dataset for a given column:</p><div class="informalexample"><pre class="programlisting">def get_mapping(rdd, idx):
    return rdd.map(lambda fields: fields[idx]).distinct().zipWithIndex().collectAsMap()</pre></div><p>Our function first maps the field to its unique values and then uses the <code class="literal">zipWithIndex</code> transformation to zip the value up with a unique index such that a key-value RDD is formed, where the key is the variable and the value is the index. This index will be the index of the nonzero entry in the binary vector representation of the feature. We will finally <a id="id561" class="indexterm"></a>collect this RDD back to the driver<a id="id562" class="indexterm"></a> as a Python dictionary.</p><p>We can test our function on the third variable column (index 2):</p><div class="informalexample"><pre class="programlisting">print "Mapping of first categorical feasture column: %s" % get_mapping(records, 2)</pre></div><p>The preceding line of code will give us the following output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Mapping of first categorical feasture column: {u'1': 0, u'3': 2, u'2': 1, u'4': 3}</strong></span>
</pre></div><p>Now, we can apply this function to each categorical column (that is, for variable indices 2 to 9):</p><div class="informalexample"><pre class="programlisting">mappings = [get_mapping(records, i) for i in range(2,10)]
cat_len = sum(map(len, mappings))
num_len = len(records.first()[11:15])
total_len = num_len + cat_len</pre></div><p>We now have the mappings for each variable, and we can see how many values in total we need for our binary vector representation:</p><div class="informalexample"><pre class="programlisting">print "Feature vector length for categorical features: %d" % cat_len 
print "Feature vector length for numerical features: %d" % num_len
print "Total feature vector length: %d" % total_len</pre></div><p>The output of the preceding code is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Feature vector length for categorical features: 57</strong></span>
<span class="strong"><strong>Feature vector length for numerical features: 4</strong></span>
<span class="strong"><strong>Total feature vector length: 61</strong></span>
</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec18"></a>Creating feature vectors for the linear model</h4></div></div></div><p>The<a id="id563" class="indexterm"></a> next step is to use our <a id="id564" class="indexterm"></a>extracted mappings to convert the categorical features to binary-encoded features. Again, it will be helpful to create a function that we can apply to each record in our dataset for this purpose. We will also create a function to extract the target variable from each record. We will need to import <code class="literal">numpy</code> for linear algebra utilities and MLlib's <code class="literal">LabeledPoint</code> class to wrap our feature vectors and target variables:</p><div class="informalexample"><pre class="programlisting">from pyspark.mllib.regression import LabeledPoint
import numpy as np

def extract_features(record):
  cat_vec = np.zeros(cat_len)
  i = 0
  step = 0
  for field in record[2:9]:
    m = mappings[i]
    idx = m[field]
    cat_vec[idx + step] = 1
    i = i + 1
    step = step + len(m)
  num_vec = np.array([float(field) for field in record[10:14]])
  return np.concatenate((cat_vec, num_vec))

def extract_label(record):
  return float(record[-1])</pre></div><p>In<a id="id565" class="indexterm"></a> the preceding <code class="literal">extract_features</code> function, we ran through each column in the row of data. We extracted<a id="id566" class="indexterm"></a> the binary encoding for each variable in turn from the mappings we created previously. The <code class="literal">step</code> variable ensures that the nonzero feature index in the full feature vector is correct (and is somewhat more efficient than, say, creating many smaller binary vectors and concatenating them). The numeric vector is created directly by first converting the data to floating point numbers and wrapping these in a <code class="literal">numpy</code> array. The resulting two vectors are then concatenated. The <code class="literal">extract_label</code> function simply converts the last column variable (the count) into a float.</p><p>With our utility functions defined, we can proceed with extracting feature vectors and labels from our data records:</p><div class="informalexample"><pre class="programlisting">data = records.map(lambda r: LabeledPoint(extract_label(r), extract_features(r)))</pre></div><p>Let's inspect the first record in the extracted feature RDD:</p><div class="informalexample"><pre class="programlisting">first_point = data.first()
print "Raw data: " + str(first[2:])
print "Label: " + str(first_point.label)
print "Linear Model feature vector:\n" + str(first_point.features)
print "Linear Model feature vector length: " + str(len(first_point.features))</pre></div><p>You should see output similar to the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Raw data: [u'1', u'0', u'1', u'0', u'0', u'6', u'0', u'1', u'0.24', u'0.2879', u'0.81', u'0', u'3', u'13', u'16'] </strong></span>
<span class="strong"><strong>Label: 16.0 </strong></span>
<span class="strong"><strong>Linear Model feature vector: [1.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.24,0.2879,0.81,0.0] </strong></span>
<span class="strong"><strong>Linear Model feature vector length: 61</strong></span>
</pre></div><p>As we <a id="id567" class="indexterm"></a>can see, we converted<a id="id568" class="indexterm"></a> the raw data into a feature vector made up of the binary categorical and real numeric features, and we indeed have a total vector length of <code class="literal">61</code>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec19"></a>Creating feature vectors for the decision tree</h4></div></div></div><p>As<a id="id569" class="indexterm"></a> we have seen, decision<a id="id570" class="indexterm"></a> tree models typically work on raw features (that is, it is not required to convert categorical features into a binary vector encoding; they can, instead, be used directly). Therefore, we will create a separate function to extract the decision tree feature vector, which simply converts all the values to floats and wraps them in a <code class="literal">numpy</code> array:</p><div class="informalexample"><pre class="programlisting">def extract_features_dt(record):
  return np.array(map(float, record[2:14]))
data_dt = records.map(lambda r: LabeledPoint(extract_label(r), extract_features_dt(r)))
first_point_dt = data_dt.first()
print "Decision Tree feature vector: " + str(first_point_dt.features)
print "Decision Tree feature vector length: " + str(len(first_point_dt.features))</pre></div><p>The following output shows the extracted feature vector, and we can see that we have a vector length of <code class="literal">12</code>, which matches the number of raw variables we are using:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Decision Tree feature vector: [1.0,0.0,1.0,0.0,0.0,6.0,0.0,1.0,0.24,0.2879,0.81,0.0]</strong></span>
<span class="strong"><strong>Decision Tree feature vector length: 12</strong></span>
</pre></div></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec42"></a>Training and using regression models</h2></div></div><hr /></div><p>Training<a id="id571" class="indexterm"></a> for regression models using decision trees and linear <a id="id572" class="indexterm"></a>models follows the same procedure as for classification models. We simply pass the training data contained in a <code class="literal">[LabeledPoint]</code> RDD to the relevant <code class="literal">train</code> method. Note that in Scala, if we wanted to customize the various model parameters (such as regularization and step size for the SGD optimizer), we are required to instantiate a new model instance and use the <code class="literal">optimizer</code> field to access these available parameter setters.</p><p>In Python, we are provided with a convenience method that gives us access to all the available model arguments, so we only have to use this one entry point for training. We can see the details of these convenience functions by importing the relevant modules and then calling the <code class="literal">help</code> function on the <code class="literal">train</code> methods:</p><div class="informalexample"><pre class="programlisting">from pyspark.mllib.regression import LinearRegressionWithSGD
from pyspark.mllib.tree import DecisionTree
help(LinearRegressionWithSGD.train)</pre></div><p>Doing this for the linear model outputs the following documentation:</p><div class="mediaobject"><img src="graphics/8519OS_06_01.jpg" /><div class="caption"><p>Linear regression help documentation</p></div></div><p>We can see from the linear regression documentation that we need to pass in the training data at a <a id="id573" class="indexterm"></a>minimum, but we can set any of the other model parameters using this <code class="literal">train</code> method.</p><p>Similarly, for <a id="id574" class="indexterm"></a>the decision tree model, which has a <code class="literal">trainRegressor</code> method (in addition to a <code class="literal">trainClassifier</code> method for classification models):</p><div class="informalexample"><pre class="programlisting">help(DecisionTree.trainRegressor)</pre></div><p>The preceding code would display the following documentation:</p><div class="mediaobject"><img src="graphics/8519OS_06_02.jpg" /><div class="caption"><p>Decision tree regression help documentation</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec59"></a>Training a regression model on the bike sharing dataset</h3></div></div></div><p>We're <a id="id575" class="indexterm"></a>ready <a id="id576" class="indexterm"></a>to use the features we have extracted to train our models on the bike sharing data. First, we'll train the linear regression model and take a look at the first few predictions that the model makes on the data:</p><div class="informalexample"><pre class="programlisting">linear_model = LinearRegressionWithSGD.train(data, iterations=10, step=0.1, intercept=False)
true_vs_predicted = data.map(lambda p: (p.label, linear_model.predict(p.features)))
print "Linear Model predictions: " + str(true_vs_predicted.take(5))</pre></div><p>Note that we have not used the default settings for <code class="literal">iterations</code> and <code class="literal">step</code> here. We've changed the number of iterations so that the model does not take too long to train. As for the step <a id="id577" class="indexterm"></a>size, you will see why<a id="id578" class="indexterm"></a> this has been changed from the default a little later. You will see the following output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Linear Model predictions: [(16.0, 119.30920003093595), (40.0, 116.95463511937379), (32.0, 116.57294610647752), (13.0, 116.43535423855654), (1.0, 116.221247828503)]</strong></span>
</pre></div><p>Next, we will train the decision tree model simply using the default arguments to the <code class="literal">trainRegressor</code> method (which equates to using a tree depth of 5). Note that we need to pass in the other form of the dataset, <code class="literal">data_dt</code>, that we created from the raw feature values (as opposed to the binary encoded features that we used for the preceding linear model).</p><p>We also need to pass in an argument for <code class="literal">categoricalFeaturesInfo</code>. This is a dictionary that maps the categorical feature index to the number of categories for the feature. If a feature is not in this mapping, it will be treated as continuous. For our purposes, we will leave this as is, passing in an empty mapping:</p><div class="informalexample"><pre class="programlisting">dt_model = DecisionTree.trainRegressor(data_dt,{})
preds = dt_model.predict(data_dt.map(lambda p: p.features))
actual = data.map(lambda p: p.label)
true_vs_predicted_dt = actual.zip(preds)
print "Decision Tree predictions: " + str(true_vs_predicted_dt.take(5))
print "Decision Tree depth: " + str(dt_model.depth())
print "Decision Tree number of nodes: " + str(dt_model.numNodes())</pre></div><p>This should output these predictions:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Decision Tree predictions: [(16.0, 54.913223140495866), (40.0, 54.913223140495866), (32.0, 53.171052631578945), (13.0, 14.284023668639053), (1.0, 14.284023668639053)]</strong></span>
<span class="strong"><strong>Decision Tree depth: 5</strong></span>
<span class="strong"><strong>Decision Tree number of nodes: 63</strong></span>
</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note30"></a>Note</h3><p>This is not as bad as it sounds. While we do not cover it here, the Python code included with this chapter's code bundle includes an example of using <code class="literal">categoricalFeaturesInfo</code>. It does not make a large difference to performance in this case.</p></div><p>From <a id="id579" class="indexterm"></a>a quick glance at these predictions, it <a id="id580" class="indexterm"></a>appears that the decision tree might do better, as the linear model is quite a way off in its predictions. However, we will apply more stringent evaluation methods to find out.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec43"></a>Evaluating the performance of regression models</h2></div></div><hr /></div><p>We saw in <a class="link" href="#" linkend="ch05">Chapter 5</a>, <span class="emphasis"><em>Building a Classification Model with Spark</em></span>, that evaluation methods for classification models typically focus on measurements related to predicted class <a id="id581" class="indexterm"></a>memberships relative to the actual class memberships. These are binary outcomes (either the predicted class is correct or incorrect), and it is less important whether the model just barely predicted correctly or not; what we care most about is the number of correct and incorrect predictions.</p><p>When dealing with regression models, it is very unlikely that our model will precisely predict the target variable, because the target variable can take on any real value. However, we would naturally like to understand how far away our predicted values are from the true values, so will we utilize a metric that takes into account the overall deviation.</p><p>Some of the standard evaluation metrics used to measure the performance of regression models include the <span class="strong"><strong>Mean Squared Error</strong></span> (<span class="strong"><strong>MSE</strong></span>) and <span class="strong"><strong>Root Mean Squared Error</strong></span> (<span class="strong"><strong>RMSE</strong></span>), the <span class="strong"><strong>Mean Absolute Error</strong></span> (<span class="strong"><strong>MAE</strong></span>), the R-squared coefficient, and many others.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec60"></a>Mean Squared Error and Root Mean Squared Error</h3></div></div></div><a id="id582" class="indexterm"></a><p>MSE<a id="id583" class="indexterm"></a> is the average of the<a id="id584" class="indexterm"></a> squared<a id="id585" class="indexterm"></a> error that is used as the loss function for least squares regression:</p><div class="mediaobject"><img src="graphics/8519OS_06_14.jpg" /></div><p>It is the sum, over all the data points, of the square of the difference between the predicted and actual target variables, divided by the number of data points.</p><p>RMSE is the square root of MSE. MSE is measured in units that are the square of the target variable, while RMSE is measured in the same units as the target variable. Due to its formulation, MSE, just like the squared loss function that it derives from, effectively penalizes larger errors more severely.</p><p>In<a id="id586" class="indexterm"></a> order to evaluate our predictions based on the<a id="id587" class="indexterm"></a> mean of an error metric, we will first make predictions for each input<a id="id588" class="indexterm"></a> feature vector in an RDD of <code class="literal">LabeledPoint</code> instances by computing the error for each record using a function that takes the <a id="id589" class="indexterm"></a>prediction and true target value as inputs. This will return a <code class="literal">[Double]</code> RDD that contains the error values. We can then find the average using the <code class="literal">mean</code> method of RDDs that contain <code class="literal">Double</code> values.</p><p>Let's define our squared error function as follows:</p><div class="informalexample"><pre class="programlisting">def squared_error(actual, pred):
    return (pred - actual)**2</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec61"></a>Mean Absolute Error</h3></div></div></div><p>MAE<a id="id590" class="indexterm"></a> is the average of the absolute differences<a id="id591" class="indexterm"></a> between the predicted and actual targets:</p><div class="mediaobject"><img src="graphics/8519OS_06_13.jpg" /></div><p>MAE is similar in principle to MSE, but it does not punish large deviations as much.</p><p>Our function to compute MAE is as follows:</p><div class="informalexample"><pre class="programlisting">def abs_error(actual, pred):
    return np.abs(pred - actual)</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec62"></a>Root Mean Squared Log Error</h3></div></div></div><p>This measurement is<a id="id592" class="indexterm"></a> not <a id="id593" class="indexterm"></a>as widely used as MSE and MAE, but it is used as the metric for the Kaggle competition that uses the bike sharing dataset. It is effectively the RMSE of the log-transformed predicted and target values. This measurement is useful when there is a wide range in the target variable, and you do not necessarily want to penalize large errors when the predicted and target values are themselves high. It is also effective when you care about percentage errors rather than the absolute value of errors.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note31"></a>Note</h3><p>The <a id="id594" class="indexterm"></a>Kaggle competition evaluation page can be found at <a class="ulink" href="https://www.kaggle.com/c/bike-sharing-demand/details/evaluation" target="_blank">https://www.kaggle.com/c/bike-sharing-demand/details/evaluation</a>.</p></div><p>The <a id="id595" class="indexterm"></a>function to compute <a id="id596" class="indexterm"></a>RMSLE is shown here:</p><div class="informalexample"><pre class="programlisting">def squared_log_error(pred, actual):
    return (np.log(pred + 1) - np.log(actual + 1))**2</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec63"></a>The R-squared coefficient</h3></div></div></div><p>The <a id="id597" class="indexterm"></a>R-squared coefficient, also<a id="id598" class="indexterm"></a> known as the coefficient of determination, is a measure of how well a model fits a dataset. It is commonly used in statistics. It measures the degree of variation in the target variable; this is explained by the variation in the input features. An R-squared coefficient generally takes a value between 0 and 1, where 1 equates to a perfect fit of the model.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec64"></a>Computing performance metrics on the bike sharing dataset</h3></div></div></div><p>Given<a id="id599" class="indexterm"></a> the functions we <a id="id600" class="indexterm"></a>defined earlier, we <a id="id601" class="indexterm"></a>can now compute the various evaluation metrics on our bike sharing data.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec20"></a>Linear model</h4></div></div></div><p>Our <a id="id602" class="indexterm"></a>approach will be to apply the relevant error function <a id="id603" class="indexterm"></a>to each record in the <code class="literal">RDD</code> we computed earlier, which is <code class="literal">true_vs_predicted</code> for our linear model:</p><div class="informalexample"><pre class="programlisting">mse = true_vs_predicted.map(lambda (t, p): squared_error(t, p)).mean()
mae = true_vs_predicted.map(lambda (t, p): abs_error(t, p)).mean()
rmsle = np.sqrt(true_vs_predicted.map(lambda (t, p): squared_log_error(t, p)).mean())
print "Linear Model - Mean Squared Error: %2.4f" % mse
print "Linear Model - Mean Absolute Error: %2.4f" % mae
print "Linear Model - Root Mean Squared Log Error: %2.4f" % rmsle</pre></div><p>This <a id="id604" class="indexterm"></a>outputs <a id="id605" class="indexterm"></a>the following metrics:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Linear Model - Mean Squared Error: 28166.3824</strong></span>
<span class="strong"><strong>Linear Model - Mean Absolute Error: 129.4506</strong></span>
<span class="strong"><strong>Linear Model - Root Mean Squared Log Error: 1.4974</strong></span>
</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec21"></a>Decision tree</h4></div></div></div><p>We will <a id="id606" class="indexterm"></a>use the same approach for the decision<a id="id607" class="indexterm"></a> tree model, using the <code class="literal">true_vs_predicted_dt</code> RDD:</p><div class="informalexample"><pre class="programlisting">mse_dt = true_vs_predicted_dt.map(lambda (t, p): squared_error(t, p)).mean()
mae_dt = true_vs_predicted_dt.map(lambda (t, p): abs_error(t, p)).mean()
rmsle_dt = np.sqrt(true_vs_predicted_dt.map(lambda (t, p): squared_log_error(t, p)).mean())
print "Decision Tree - Mean Squared Error: %2.4f" % mse_dt
print "Decision Tree - Mean Absolute Error: %2.4f" % mae_dt
print "Decision Tree - Root Mean Squared Log Error: %2.4f" % rmsle_dt</pre></div><p>You should see output similar to this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Decision Tree - Mean Squared Error: 11560.7978</strong></span>
<span class="strong"><strong>Decision Tree - Mean Absolute Error: 71.0969</strong></span>
<span class="strong"><strong>Decision Tree - Root Mean Squared Log Error: 0.6259</strong></span>
</pre></div><p>Looking at the results, we can see that our initial guess about the decision tree model being the better performer is indeed true.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note32"></a>Note</h3><p>The Kaggle competition leaderboard lists the Mean Value Benchmark score on the test set at about 1.58. So, we see that our linear model performance is not much better. However, the decision tree with default settings achieves a performance of 0.63.</p><p>The winning score at the time of writing this book is listed as 0.29504.</p></div></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec44"></a>Improving model performance and tuning parameters</h2></div></div><hr /></div><p>In <a class="link" href="#" linkend="ch05">Chapter 5</a>, <span class="emphasis"><em>Building a Classification Model with Spark</em></span>, we showed how feature transformation and selection can make a large difference to the performance of a model. In<a id="id608" class="indexterm"></a> this chapter, we will focus on another type of transformation<a id="id609" class="indexterm"></a> that can be applied to a dataset: transforming the target variable itself.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec65"></a>Transforming the target variable</h3></div></div></div><p>Recall <a id="id610" class="indexterm"></a>that many machine learning models, including linear models, make assumptions regarding the distribution of the input data as well as target variables. In particular, linear regression assumes a normal distribution.</p><p>In many real-world cases, the distributional assumptions of linear regression do not hold. In this case, for example, we know that the number of bike rentals can never be negative. This alone should indicate that the assumption of normality might be problematic. To get a better idea of the target distribution, it is often a good idea to plot a histogram of the target values.</p><p>In this section, if you are using IPython Notebook, enter the magic function, <code class="literal">%pylab inline</code>, to import <code class="literal">pylab</code> (that is, the <code class="literal">numpy</code> and <code class="literal">matplotlib</code> plotting functions) into the workspace. This will also create any figures and plots inline within the <code class="literal">Notebook</code> cell.</p><p>If you are using the standard IPython console, you can use <code class="literal">%pylab</code> to import the necessary functionality (your plots will appear in a separate window).</p><p>We will now create a plot of the target variable distribution in the following piece of code:</p><div class="informalexample"><pre class="programlisting">targets = records.map(lambda r: float(r[-1])).collect()
hist(targets, bins=40, color='lightblue', normed=True)
fig = matplotlib.pyplot.gcf()
fig.set_size_inches(16, 10) </pre></div><p>Looking at the histogram plot, we can see that the distribution is highly skewed and certainly does not follow a normal distribution:</p><div class="mediaobject"><img src="graphics/8519OS_06_03.jpg" /><div class="caption"><p>Distribution of raw target variable values</p></div></div><p>One<a id="id611" class="indexterm"></a> way in which we might deal with this situation is by applying a transformation to the target variable, such that we take the logarithm of the target value instead of the raw value. This is often referred to as log-transforming the target variable (this transformation can also be applied to feature values).</p><p>We will apply a log transformation to the following target variable and plot a histogram of the log-transformed values:</p><div class="informalexample"><pre class="programlisting">log_targets = records.map(lambda r: np.log(float(r[-1]))).collect()
hist(log_targets, bins=40, color='lightblue', normed=True)
fig = matplotlib.pyplot.gcf()
fig.set_size_inches(16, 10) </pre></div><div class="mediaobject"><img src="graphics/8519OS_06_04.jpg" /><div class="caption"><p>Distribution of log-transformed target variable values</p></div></div><p>A<a id="id612" class="indexterm"></a> second type of transformation that is useful in the case of target values that do not take on negative values and, in addition, might take on a very wide range of values, is to take the square root of the variable.</p><p>We will apply the square root transform in the following code, once more plotting the resulting target variable distribution:</p><div class="informalexample"><pre class="programlisting">sqrt_targets = records.map(lambda r: np.sqrt(float(r[-1]))).collect()
hist(sqrt_targets, bins=40, color='lightblue', normed=True)
fig = matplotlib.pyplot.gcf()
fig.set_size_inches(16, 10)</pre></div><p>From the plots of the log and square root transformations, we can see that both result in a more even distribution relative to the raw values. While they are still not normally distributed, they are a lot closer to a normal distribution when compared to the original target variable.</p><div class="mediaobject"><img src="graphics/8519OS_06_05.jpg" /><div class="caption"><p>Distribution of square-root-transformed target variable values</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec22"></a>Impact of training on log-transformed targets</h4></div></div></div><p>So, does<a id="id613" class="indexterm"></a> applying these <a id="id614" class="indexterm"></a>transformations have any impact on model performance? Let's evaluate the various metrics we used previously on log-transformed data as an example.</p><p>We will do this first for the linear model by applying the <code class="literal">numpy log</code> function to the <code class="literal">label</code> field of each <code class="literal">LabeledPoint</code> RDD. Here, we will only transform the target variable, and we will not apply any transformations to the features:</p><div class="informalexample"><pre class="programlisting">data_log = data.map(lambda lp: LabeledPoint(np.log(lp.label), lp.features))</pre></div><p>We will then train a model on this transformed data and form the RDD of predicted versus true values:</p><div class="informalexample"><pre class="programlisting">model_log = LinearRegressionWithSGD.train(data_log, iterations=10, step=0.1) </pre></div><p>Note that now that we have transformed the target variable, the predictions of the model will be on the log scale, as will the target values of the transformed dataset. Therefore, in order <a id="id615" class="indexterm"></a>to use our model and<a id="id616" class="indexterm"></a> evaluate its performance, we must first transform the log data back into the original scale by taking the exponent of both the predicted and true values using the <code class="literal">numpy exp</code> function. We will show you how to do this in the code here:</p><div class="informalexample"><pre class="programlisting">true_vs_predicted_log = data_log.map(lambda p: (np.exp(p.label), np.exp(model_log.predict(p.features)))) </pre></div><p>Finally, we will compute the MSE, MAE, and RMSLE metrics for the model:</p><div class="informalexample"><pre class="programlisting">mse_log = true_vs_predicted_log.map(lambda (t, p): squared_error(t, p)).mean()
mae_log = true_vs_predicted_log.map(lambda (t, p): abs_error(t, p)).mean()
rmsle_log = np.sqrt(true_vs_predicted_log.map(lambda (t, p): squared_log_error(t, p)).mean())
print "Mean Squared Error: %2.4f" % mse_log
print "Mean Absolue Error: %2.4f" % mae_log
print "Root Mean Squared Log Error: %2.4f" % rmsle_log
print "Non log-transformed predictions:\n" + str(true_vs_predicted.take(3))
print "Log-transformed predictions:\n" + str(true_vs_predicted_log.take(3)) </pre></div><p>You should see output similar to the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Mean Squared Error: 38606.0875</strong></span>
<span class="strong"><strong>Mean Absolue Error: 135.2726</strong></span>
<span class="strong"><strong>Root Mean Squared Log Error: 1.3516</strong></span>
<span class="strong"><strong>Non log-transformed predictions:</strong></span>
<span class="strong"><strong>[(16.0, 119.30920003093594), (40.0, 116.95463511937378), (32.0, 116.57294610647752)]</strong></span>
<span class="strong"><strong>Log-transformed predictions:</strong></span>
<span class="strong"><strong>[(15.999999999999998, 45.860944832110015), (40.0, 43.255903592233274), (32.0, 42.311306147884252)]</strong></span>
</pre></div><p>If we compare these results to the results on the raw target variable, we see that while we did not improve the MSE or MAE, we improved the RMSLE.</p><p>We will perform the same analysis for the decision tree model:</p><div class="informalexample"><pre class="programlisting">data_dt_log = data_dt.map(lambda lp: LabeledPoint(np.log(lp.label), lp.features))
dt_model_log = DecisionTree.trainRegressor(data_dt_log,{})

preds_log = dt_model_log.predict(data_dt_log.map(lambda p: p.features))
actual_log = data_dt_log.map(lambda p: p.label)
true_vs_predicted_dt_log = actual_log.zip(preds_log).map(lambda (t, p): (np.exp(t), np.exp(p)))

mse_log_dt = true_vs_predicted_dt_log.map(lambda (t, p): squared_error(t, p)).mean()
mae_log_dt = true_vs_predicted_dt_log.map(lambda (t, p): abs_error(t, p)).mean()
rmsle_log_dt = np.sqrt(true_vs_predicted_dt_log.map(lambda (t, p): squared_log_error(t, p)).mean())
print "Mean Squared Error: %2.4f" % mse_log_dt
print "Mean Absolue Error: %2.4f" % mae_log_dt
print "Root Mean Squared Log Error: %2.4f" % rmsle_log_dt
print "Non log-transformed predictions:\n" + str(true_vs_predicted_dt.take(3))
print "Log-transformed predictions:\n" + str(true_vs_predicted_dt_log.take(3))</pre></div><p>From<a id="id617" class="indexterm"></a> the results here, we<a id="id618" class="indexterm"></a> can see that we actually made our metrics slightly worse for the decision tree:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Mean Squared Error: 14781.5760</strong></span>
<span class="strong"><strong>Mean Absolue Error: 76.4131</strong></span>
<span class="strong"><strong>Root Mean Squared Log Error: 0.6406</strong></span>
<span class="strong"><strong>Non log-transformed predictions:</strong></span>
<span class="strong"><strong>[(16.0, 54.913223140495866), (40.0, 54.913223140495866), (32.0, 53.171052631578945)]</strong></span>
<span class="strong"><strong>Log-transformed predictions:</strong></span>
<span class="strong"><strong>[(15.999999999999998, 37.530779787154508), (40.0, 37.530779787154508), (32.0, 7.2797070993907287)]</strong></span>
</pre></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip37"></a>Tip</h3><p>It is probably not surprising that the log transformation results in a better RMSLE performance for the linear model. As we are minimizing the squared error, once we have transformed the target variable to log values, we are effectively minimizing a loss function that is very similar to the RMSLE.</p><p>This is good for Kaggle competition purposes, since we can more directly optimize against the competition-scoring metric.</p><p>It <a id="id619" class="indexterm"></a>might or might not be as useful in a real-world situation. This depends on how important larger absolute errors are (recall that RMSLE essentially penalizes relative errors rather than absolute magnitude of errors).</p></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl2sec66"></a>Tuning model parameters</h3></div></div></div><p>So far <a id="id620" class="indexterm"></a>in this chapter, we have illustrated the concepts of model training and evaluation for MLlib's regression models by training and testing on the same dataset. We will now use a similar cross-validation approach that we used previously to evaluate the effect on performance of different parameter settings for our models.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec23"></a>Creating training and testing sets to evaluate parameters</h4></div></div></div><p>The<a id="id621" class="indexterm"></a> first step is to create <a id="id622" class="indexterm"></a>a test and training<a id="id623" class="indexterm"></a> set for cross-validation purposes. Spark's Python API does not yet provide the <code class="literal">randomSplit</code> convenience method<a id="id624" class="indexterm"></a> that is available in Scala. Hence, we will need to create a training and test dataset manually.</p><p>One relatively easy way to do this is by first taking a random sample of, say, 20 percent of our data as our test set. We will then define our training set as the elements of the original RDD that are not in the test set RDD.</p><p>We can achieve this using the <code class="literal">sample</code> method to take a random sample for our test set, followed by using the <code class="literal">subtractByKey</code> method, which takes care of returning the elements in one RDD where the keys do not overlap with the other RDD.</p><p>Note that <code class="literal">subtractByKey</code>, as the name suggests, works on the keys of the RDD elements that consist of key-value pairs. Therefore, here we will use <code class="literal">zipWithIndex</code> on our RDD of extracted training examples. This creates an RDD of <code class="literal">(LabeledPoint, index)</code> pairs.</p><p>We will then reverse the keys and values so that we can operate on the index keys:</p><div class="informalexample"><pre class="programlisting">data_with_idx = data.zipWithIndex().map(lambda (k, v): (v, k)) 
test = data_with_idx.sample(False, 0.2, 42)
train = data_with_idx.subtractByKey(test) </pre></div><p>Once we have the two RDDs, we will recover just the <code class="literal">LabeledPoint</code> instances we need for training and test data, using <code class="literal">map</code> to extract the value from the key-value pairs:</p><div class="informalexample"><pre class="programlisting">train_data = train.map(lambda (idx, p): p)
test_data = test.map(lambda (idx, p) : p)
train_size = train_data.count()
test_size = test_data.count()
print "Training data size: %d" % train_size
print "Test data size: %d" % test_size
print "Total data size: %d " % num_data
print "Train + Test size : %d" % (train_size + test_size)</pre></div><p>We <a id="id625" class="indexterm"></a>can confirm that <a id="id626" class="indexterm"></a>we now have two<a id="id627" class="indexterm"></a> distinct datasets that add up to the original dataset in total:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Training data size: 13934</strong></span>
<span class="strong"><strong>Test data size: 3445</strong></span>
<span class="strong"><strong>Total data size: 17379 </strong></span>
<span class="strong"><strong>Train + Test size : 17379</strong></span>
</pre></div><p>The final step is to apply the same approach to the features extracted for the decision tree model:</p><div class="informalexample"><pre class="programlisting">data_with_idx_dt = data_dt.zipWithIndex().map(lambda (k, v): (v, k))
test_dt = data_with_idx_dt.sample(False, 0.2, 42)
train_dt = data_with_idx_dt.subtractByKey(test_dt)
train_data_dt = train_dt.map(lambda (idx, p): p)
test_data_dt = test_dt.map(lambda (idx, p) : p)</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec24"></a>The impact of parameter settings for linear models</h4></div></div></div><p>Now<a id="id628" class="indexterm"></a> that we have prepared <a id="id629" class="indexterm"></a>our training and test sets, we are ready to investigate the impact of different parameter settings on model performance. We will first carry out this evaluation for the linear model. We will create a convenience function to evaluate the relevant performance metric by training the model on the training set and evaluating it on the test set for different parameter settings.</p><p>We will use the RMSLE evaluation metric, as it is the one used in the Kaggle competition with this dataset, and this allows us to compare our model results against the competition leaderboard to see how we perform.</p><p>The evaluation function is defined here:</p><div class="informalexample"><pre class="programlisting">def evaluate(train, test, iterations, step, regParam, regType, intercept):
    model = LinearRegressionWithSGD.train(train, iterations, step, regParam=regParam, regType=regType, intercept=intercept)
    tp = test.map(lambda p: (p.label, model.predict(p.features)))
    rmsle = np.sqrt(tp.map(lambda (t, p): squared_log_error(t, p)).mean())
    return rmsle</pre></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip38"></a>Tip</h3><p>Note<a id="id630" class="indexterm"></a> that in <a id="id631" class="indexterm"></a>the following sections, you might get slightly different results due to some random initialization for SGD. However, your results will be comparable.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch06lvl4sec10"></a>Iterations</h5></div></div></div><p>As we saw <a id="id632" class="indexterm"></a>when evaluating our classification models, we generally expect that a model trained with SGD will achieve better performance as the number of iterations increases, although the increase in performance will slow down as the number of iterations goes above some minimum number. Note that here, we will set the step size to 0.01 to better illustrate the impact at higher iteration numbers:</p><div class="informalexample"><pre class="programlisting">params = [1, 5, 10, 20, 50, 100]
metrics = [evaluate(train_data, test_data, <span class="strong"><strong>param</strong></span>, 0.01, 0.0, 'l2', False) for param in params]
print params
print metrics</pre></div><p>The output shows that the error metric indeed decreases as the number of iterations increases. It also does so at a decreasing rate, again as expected. What is interesting is that eventually, the SGD optimization tends to overshoot the optimal solution, and the RMSLE eventually starts to increase slightly:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[1, 5, 10, 20, 50, 100]</strong></span>
<span class="strong"><strong>[2.3532904530306888, 1.6438528499254723, 1.4869656275309227, 1.4149741941240344, 1.4159641262731959, 1.4539667094611679]</strong></span>
</pre></div><p>Here, we will use the <code class="literal">matplotlib</code> library to plot a graph of the RMSLE metric against the number of iterations. We will use a log scale for the <span class="emphasis"><em>x </em></span>axis to make the output easier to visualize:</p><div class="informalexample"><pre class="programlisting">plot(params, metrics)
fig = matplotlib.pyplot.gcf()
pyplot.xscale('log')</pre></div><div class="mediaobject"><img src="graphics/8519OS_06_06.jpg" /><div class="caption"><p>Metrics for varying number of iterations</p></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch06lvl4sec11"></a>Step size</h5></div></div></div><p>We will<a id="id633" class="indexterm"></a> perform a similar analysis for step size in the following code:</p><div class="informalexample"><pre class="programlisting">params = [0.01, 0.025, 0.05, 0.1, 1.0]
metrics = [evaluate(train_data, test_data, 10, <span class="strong"><strong>param</strong></span>, 0.0, 'l2', False) for param in params]
print params
print metrics</pre></div><p>The output of the preceding code:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[0.01, 0.025, 0.05, 0.1, 0.5]</strong></span>
<span class="strong"><strong>[1.4869656275309227, 1.4189071944747715, 1.5027293911925559, 1.5384660954019973, nan]</strong></span>
</pre></div><p>Now, we can see why we avoided using the default step size when training the linear model originally. The default is set to <span class="emphasis"><em>1.0</em></span>, which, in this case, results in a <code class="literal">nan</code> output for the RMSLE metric. This typically means that the SGD model has converged to a very poor local minimum in the error function that it is optimizing. This can happen when the step size is relatively large, as it is easier for the optimization algorithm to overshoot good solutions.</p><p>We can<a id="id634" class="indexterm"></a> also see that for low step sizes and a relatively low number of iterations (we used 10 here), the model performance is slightly poorer. However, in the preceding <span class="emphasis"><em>Iterations</em></span> section, we saw that for the lower step-size setting, a higher number of iterations will generally converge to a better solution.</p><p>Generally speaking, setting step size and number of iterations involves a trade-off. A lower step size means that convergence is slower but slightly more assured. However, it requires a higher number of iterations, which is more costly in terms of computation and time, in particular at a very large scale.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip39"></a>Tip</h3><p>Selecting the best parameter settings can be an intensive process that involves training a model on many combinations of parameter settings and selecting the best outcome. Each instance of model training involves a number of iterations, so this process can be very expensive and time consuming when performed on very large datasets.</p></div><p>The output is plotted here, again using a log scale for the step-size axis:</p><div class="mediaobject"><img src="graphics/8519OS_06_07.jpg" /><div class="caption"><p>Metrics for varying values of step size</p></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch06lvl4sec12"></a>L2 regularization</h5></div></div></div><p>In <a class="link" href="#" linkend="ch05">Chapter 5</a>, <span class="emphasis"><em>Building a Classification Model with Spark</em></span>, we saw that regularization has the effect<a id="id635" class="indexterm"></a> of penalizing <a id="id636" class="indexterm"></a>model complexity in the form of an additional loss term that is a function of the model weight vector. L2 regularization penalizes the L2-norm of the weight vector, while L1 regularization penalizes the L1-norm.</p><p>We expect training set performance to deteriorate with increasing regularization, as the model cannot fit the dataset well. However, we would also expect some amount of regularization that will result in optimal generalization performance as evidenced by the best performance on the test set.</p><p>We will evaluate the impact of different levels of L2 regularization in this code:</p><div class="informalexample"><pre class="programlisting">params = [0.0, 0.01, 0.1, 1.0, 5.0, 10.0, 20.0]
metrics = [evaluate(train_data, test_data, 10, 0.1, <span class="strong"><strong>param</strong></span>, <span class="strong"><strong>'l2'</strong></span>, False) for param in params]
print params
print metrics
plot(params, metrics)
fig = matplotlib.pyplot.gcf()
pyplot.xscale('log')</pre></div><p>As <a id="id637" class="indexterm"></a>expected, there is an optimal<a id="id638" class="indexterm"></a> setting of the regularization parameter with respect to the test set RMSLE:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[0.0, 0.01, 0.1, 1.0, 5.0, 10.0, 20.0]</strong></span>
<span class="strong"><strong>[1.5384660954019971, 1.5379108106882864, 1.5329809395123755, 1.4900275345312988, 1.4016676336981468, 1.40998359211149, 1.5381771283158705]</strong></span>
</pre></div><p>This is easiest to see in the following plot (where we once more use the log scale for the regularization parameter axis):</p><div class="mediaobject"><img src="graphics/8519OS_06_08.jpg" /><div class="caption"><p>Metrics for varying levels of L2 regularization</p></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch06lvl4sec13"></a>L1 regularization</h5></div></div></div><p>We can<a id="id639" class="indexterm"></a> apply the same <a id="id640" class="indexterm"></a>approach for differing levels of L1 regularization:</p><div class="informalexample"><pre class="programlisting">params = [0.0, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]
metrics = [evaluate(train_data, test_data, 10, 0.1, <span class="strong"><strong>param</strong></span>, <span class="strong"><strong>'l1'</strong></span>, False) for param in params]
print params
print metrics
plot(params, metrics)
fig = matplotlib.pyplot.gcf()
pyplot.xscale('log')</pre></div><p>Again, the results are more clearly seen when plotted in the following graph. We see that there is a much more subtle decline in RMSLE, and it takes a very high value to cause a jump back up. Here, the level of L1 regularization required is much higher than that for the L2 form; however, the overall performance is poorer:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[0.0, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]</strong></span>
<span class="strong"><strong>[1.5384660954019971, 1.5384518080419873, 1.5383237472930684, 1.5372017600929164, 1.5303809928601677, 1.4352494587433793, 4.7551250073268614]</strong></span>
</pre></div><div class="mediaobject"><img src="graphics/8519OS_06_09.jpg" /><div class="caption"><p>Metrics for varying levels of L1 regularization</p></div></div><p>Using<a id="id641" class="indexterm"></a> L1 regularization<a id="id642" class="indexterm"></a> can encourage sparse weight vectors. Does this hold true in this case? We can find out by examining the number of entries in the weight vector that are zero, with increasing levels of regularization:</p><div class="informalexample"><pre class="programlisting">model_l1 = LinearRegressionWithSGD.train(train_data, 10, 0.1, regParam=1.0, regType='l1', intercept=False)
model_l1_10 = LinearRegressionWithSGD.train(train_data, 10, 0.1, regParam=10.0, regType='l1', intercept=False)
model_l1_100 = LinearRegressionWithSGD.train(train_data, 10, 0.1, regParam=100.0, regType='l1', intercept=False)
print "L1 (1.0) number of zero weights: " + str(sum(model_l1.weights.array == 0))
print "L1 (10.0) number of zeros weights: " + str(sum(model_l1_10.weights.array == 0))
print "L1 (100.0) number of zeros weights: " + str(sum(model_l1_100.weights.array == 0))</pre></div><p>We can see from the results that as we might expect, the number of zero feature weights in the<a id="id643" class="indexterm"></a> model weight <a id="id644" class="indexterm"></a>vector increases as greater levels of L1 regularization are applied:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>L1 (1.0) number of zero weights: 4</strong></span>
<span class="strong"><strong>L1 (10.0) number of zeros weights: 20</strong></span>
<span class="strong"><strong>L1 (100.0) number of zeros weights: 55</strong></span>
</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch06lvl4sec14"></a>Intercept</h5></div></div></div><p>The<a id="id645" class="indexterm"></a> final parameter option for the linear model is whether to use an intercept or not. An intercept is a constant term that is added to the weight vector and effectively accounts for the mean value of the target variable. If the data is already centered or normalized, an intercept is not necessary; however, it often does not hurt to use one in any case.</p><p>We will evaluate the effect of adding an intercept term to the model here:</p><div class="informalexample"><pre class="programlisting">params = [False, True]
metrics = [evaluate(train_data, test_data, 10, 0.1, 1.0, 'l2', <span class="strong"><strong>param</strong></span>) for param in params]
print params
print metrics
bar(params, metrics, color='lightblue')
fig = matplotlib.pyplot.gcf()</pre></div><p>We can see from the result and plot that adding the intercept term results in a very slight increase in RMSLE:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[False, True]</strong></span>
<span class="strong"><strong>[1.4900275345312988, 1.506469812020645]</strong></span>
</pre></div><div class="mediaobject"><img src="graphics/8519OS_06_10.jpg" /><div class="caption"><p>Metrics without and with an intercept</p></div></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl3sec25"></a>The impact of parameter settings for the decision tree</h4></div></div></div><p>Decision<a id="id646" class="indexterm"></a> trees provide <a id="id647" class="indexterm"></a>two main parameters: maximum tree depth and the maximum number of bins. We will now perform the same evaluation of the effect of parameter settings for the decision tree model. Our starting point is to create an evaluation function for the model, similar to the one used for the linear regression earlier. This function is provided here:</p><div class="informalexample"><pre class="programlisting">def evaluate_dt(train, test, maxDepth, maxBins):
    model = DecisionTree.trainRegressor(train, {}, impurity='variance', maxDepth=maxDepth, maxBins=maxBins)
    preds = model.predict(test.map(lambda p: p.features))
    actual = test.map(lambda p: p.label)
    tp = actual.zip(preds)
    rmsle = np.sqrt(tp.map(lambda (t, p): squared_log_error(t, p)).mean())
    return rmsle </pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch06lvl4sec15"></a>Tree depth</h5></div></div></div><p>We would <a id="id648" class="indexterm"></a>generally expect performance to increase with more complex trees (that is, trees of greater depth). Having a lower tree depth acts as a form of regularization, and it might be the case that as with L2 or L1 regularization in linear models, there is a tree depth that is optimal with respect to the test set performance.</p><p>Here, we will try to increase the depths of trees to see what impact they have on test set RMSLE, keeping the number of bins at the default level of <code class="literal">32</code>:</p><div class="informalexample"><pre class="programlisting">params = [1, 2, 3, 4, 5, 10, 20]
metrics = [evaluate_dt(train_data_dt, test_data_dt, param, 32) for param in params] 
print params
print metrics
plot(params, metrics)
fig = matplotlib.pyplot.gcf()</pre></div><p>In this case, it appears that the decision tree starts over-fitting at deeper tree levels. An optimal tree depth appears to be around 10 on this dataset.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note33"></a>Note</h3><p>Notice that our best RMSLE of 0.42 is now quite close to the Kaggle winner of around 0.29!</p></div><p>The output of the tree depth is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[1, 2, 3, 4, 5, 10, 20]</strong></span>
<span class="strong"><strong>[1.0280339660196287, 0.92686672078778276, 0.81807794023407532, 0.74060228537329209, 0.63583503599563096, 0.42851360418692447, 0.45500008049779139] </strong></span>
</pre></div><div class="mediaobject"><img src="graphics/8519OS_06_11.jpg" /><div class="caption"><p>Metrics for different tree depths</p></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch06lvl4sec16"></a>Maximum bins</h5></div></div></div><p>Finally, we <a id="id649" class="indexterm"></a>will perform our evaluation on the impact of setting the number of bins for the decision tree. As with the tree depth, a larger number of bins should allow the model to become more complex and might help performance with larger feature dimensions. After a certain point, it is unlikely that it will help any more and might, in fact, hinder performance on the test set due to over-fitting:</p><div class="informalexample"><pre class="programlisting">params = [2, 4, 8, 16, 32, 64, 100]
metrics = [evaluate_dt(train_data_dt, test_data_dt, 5, param) for param in params]
print params
print metrics
plot(params, metrics)
fig = matplotlib.pyplot.gcf()</pre></div><p>Here, we will show the output and plot to vary the number of bins (while keeping the tree depth <a id="id650" class="indexterm"></a>at the default level of 5). In this case, using a small number of bins hurts performance, while there is no impact when we use around 32 bins (the default setting) or more. There seems to be an optimal setting for test set performance at around 16-20 bins:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[2, 4, 8, 16, 32, 64, 100]</strong></span>
<span class="strong"><strong>[1.3069788763726049, 0.81923394899750324, 0.75745322513058744, 0.62328384445223795, 0.63583503599563096, 0.63583503599563096, 0.63583503599563096]</strong></span>
</pre></div><div class="mediaobject"><img src="graphics/8519OS_06_12.jpg" /><div class="caption"><p>Metrics for different maximum bins</p></div></div></div></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch06lvl1sec45"></a>Summary</h2></div></div><hr /></div><p>In this chapter, you saw how to use MLlib's linear model and decision tree functionality in Python within the context of regression models. We explored categorical feature extraction and the impact of applying transformations to the target variable in a regression problem. Finally, we implemented various performance-evaluation metrics and used them to implement a cross-validation exercise that explores the impact of the various parameter settings available in both linear models and decision trees on test set model performance.</p><p>In the next chapter, we will cover a different approach to machine learning, that is unsupervised learning, specifically in clustering models.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch07"></a>ChapterÂ 7.Â Building a Clustering Model with Spark</h2></div></div></div><p>In the last few chapters, we covered supervised learning methods, where the training data is labeled with the true outcome that we would like to predict (for example, a rating for recommendations and class assignment for classification or real target variable in the case of regression).</p><p>Next, we will consider the case when we do not have labeled data available. This is called unsupervised learning, as the model is not supervised with the true target label. The unsupervised case is very common in practice, since obtaining labeled training data can be very difficult or expensive in many real-world scenarios (for example, having humans label training data with class labels for classification). However, we would still like to learn some underlying structure in the data and use these to make predictions.</p><p>This is where unsupervised learning approaches can be useful. Unsupervised learning models are also often combined with supervised models, for example, applying unsupervised techniques to create new input features for supervised models.</p><p>Clustering models are, in many ways, the unsupervised equivalent of classification models. With<a id="id651" class="indexterm"></a> classification, we tried to learn a model that would predict which class a given training example belonged to. The model was essentially a mapping from a set of features to the class.</p><p>In clustering, we would like to segment the data such that each training example is assigned to a<a id="id652" class="indexterm"></a> segment called a <span class="strong"><strong>cluster</strong></span>. The clusters act much like classes, except that the true class assignments are unknown.</p><p>Clustering<a id="id653" class="indexterm"></a> models have many use cases that are the same as classification; these include the following:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Segmenting users or customers into different groups based on behavior characteristics and metadata</p></li><li style="list-style-type: disc"><p>Grouping content on a website or products in a retail business</p></li><li style="list-style-type: disc"><p>Finding clusters of similar genes</p></li><li style="list-style-type: disc"><p>Segmenting communities in ecology</p></li><li style="list-style-type: disc"><p>Creating<a id="id654" class="indexterm"></a> image segments for use in image analysis applications such as object detection</p></li></ul></div><p>In this chapter, we will:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Briefly explore a few types of clustering models</p></li><li style="list-style-type: disc"><p>Extract features from data specifically using the output of one model as input features for our clustering model</p></li><li style="list-style-type: disc"><p>Train a clustering model and use it to make predictions</p></li><li style="list-style-type: disc"><p>Apply performance-evaluation and parameter-selection techniques to select the optimal number of clusters to use</p></li></ul></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec47"></a>Types of clustering models</h2></div></div><hr /></div><p>There <a id="id655" class="indexterm"></a>are many different forms of clustering models available, ranging from simple to extremely complex ones. The MLlib library currently provides K-means clustering, which is among the simplest approaches available. However, it is often very effective, and its simplicity means it is relatively easy to understand and is scalable.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec67"></a>K-means clustering</h3></div></div></div><p>K-means<a id="id656" class="indexterm"></a> attempts to partition a set of data points<a id="id657" class="indexterm"></a> into K distinct clusters (where K is an input parameter for the model).</p><p>More <a id="id658" class="indexterm"></a>formally, K-means tries to find clusters so as to minimize the sum of squared errors (or distances) within each cluster. This objective function is known as the <span class="strong"><strong>within cluster sum of squared errors</strong></span> (<span class="strong"><strong>WCSS</strong></span>).</p><div class="mediaobject"><img src="graphics/8519OS_07_11.jpg" /></div><p>It is the sum, over each cluster, of the squared errors between each point and the cluster center.</p><p>Starting with a set of K initial cluster centers (which are computed as the mean vector for all data points in the cluster), the standard method for K-means iterates between two steps:</p><div class="orderedlist"><ol class="orderedlist arabic" type="1"><li><p>Assign each data point to the cluster that minimizes the WCSS. The sum of squares is equivalent to the squared Euclidean distance; therefore, this equates to assigning each point to the <span class="strong"><strong>closest</strong></span> cluster center as measured by the Euclidean distance metric.</p></li><li><p>Compute the new cluster centers based on the cluster assignments from the first step.</p></li></ol></div><p>The <a id="id659" class="indexterm"></a>algorithm proceeds until either a maximum <a id="id660" class="indexterm"></a>number of iterations has been reached or <span class="strong"><strong>convergence</strong></span> has <a id="id661" class="indexterm"></a>been achieved. Convergence means that the cluster assignments no longer change during the first step; therefore, the value of the WCSS objective function does not change either.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip40"></a>Tip</h3><p>For more details, refer to Spark's documentation on clustering at <a class="ulink" href="http://spark.apache.org/docs/latest/mllib-clustering.html" target="_blank">http://spark.apache.org/docs/latest/mllib-clustering.html</a> or refer to <a class="ulink" href="http://en.wikipedia.org/wiki/K-means_clustering" target="_blank">http://en.wikipedia.org/wiki/K-means_clustering</a>.</p></div><p>To illustrate the basics of K-means, we will use the simple dataset we showed in our multiclass classification example in <a class="link" href="#" linkend="ch05">Chapter 5</a>, <span class="emphasis"><em>Building a Classification Model with Spark</em></span>. Recall that we have five classes, which are shown in the following figure:</p><div class="mediaobject"><img src="graphics/8519OS_07_01.jpg" /><div class="caption"><p>Multiclass dataset</p></div></div><p>However, assume<a id="id662" class="indexterm"></a> that we don't actually know the<a id="id663" class="indexterm"></a> true classes. If we use K-means with five clusters, then after the first step, the model's cluster assignments might look like this:</p><div class="mediaobject"><img src="graphics/8519OS_07_02.jpg" /><div class="caption"><p>Cluster assignments after the first K-means iteration</p></div></div><p>We <a id="id664" class="indexterm"></a>can see that K-means has already picked out<a id="id665" class="indexterm"></a> the centers of each cluster fairly well. After the next iteration, the assignments might look like those shown in the following figure:</p><div class="mediaobject"><img src="graphics/8519OS_07_03.jpg" /><div class="caption"><p>Cluster assignments after the second K-means iteration</p></div></div><p>Things<a id="id666" class="indexterm"></a> are starting to stabilize, but the overall cluster <a id="id667" class="indexterm"></a>assignments are broadly the same as they were after the first iteration. Once the model has converged, the final assignments could look like this:</p><div class="mediaobject"><img src="graphics/8519OS_07_04.jpg" /><div class="caption"><p>Final cluster assignments for K-means</p></div></div><p>As we<a id="id668" class="indexterm"></a> can see, the model has done a decent job<a id="id669" class="indexterm"></a> of separating the five clusters. The leftmost three are fairly accurate (with a few incorrect points). However, the two clusters in the bottom-right corner are less accurate.</p><p>This illustrates:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>The iterative nature of K-means</p></li><li style="list-style-type: disc"><p>The model's dependency on the method of initially selecting clusters' centers (here, we will use a random approach)</p></li><li style="list-style-type: disc"><p>That the final cluster assignments can be very good for well-separated data but can be poor for data that is more difficult</p></li></ul></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl3sec26"></a>Initialization methods</h4></div></div></div><p>The<a id="id670" class="indexterm"></a> standard initialization method for K-means, usually <a id="id671" class="indexterm"></a>simply referred to as the random method, starts by randomly assigning each data point to a cluster before proceeding with the first update step.</p><p>MLlib provides a parallel variant for this initialization method, called K-means ||, which is the default initialization method used.</p><p>MLlib <a id="id672" class="indexterm"></a>provides a parallel variant called <span class="strong"><strong>K-means</strong></span> <span class="strong"><strong>||</strong></span>, <span class="strong"><strong>||</strong></span>, for this initialization method; this is the default initialization method used.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note34"></a>Note</h3><p>See <a class="ulink" href="http://en.wikipedia.org/wiki/K-means_clustering#Initialization_methods" target="_blank">http://en.wikipedia.org/wiki/K-means_clustering#Initialization_methods</a> and <a class="ulink" href="http://en.wikipedia.org/wiki/K-means%2B%2B" target="_blank">http://en.wikipedia.org/wiki/K-means%2B%2B</a> for more information.</p></div><p>The results of using K-means++ are shown here. Note that this time, the difficult lower-right points have been mostly correctly clustered.</p><div class="mediaobject"><img src="graphics/8519OS_07_05.jpg" /><div class="caption"><p>Final cluster assignments for K-means++</p></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl3sec27"></a>Variants</h4></div></div></div><p>There are<a id="id673" class="indexterm"></a> many other variants of K-means; they focus<a id="id674" class="indexterm"></a> on initialization methods or the core model. One of the more common variants is fuzzy K-means. This model does not assign each point to one cluster as K-means does (a so-called hard assignment). Instead, it is a soft version of K-means, where each point can belong to many clusters, and is represented by the relative membership to each cluster. So, for K clusters, each point is represented as a K-dimensional membership vector, with each entry in this vector indicating the membership proportion in each cluster.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec68"></a>Mixture models</h3></div></div></div><p>A <span class="strong"><strong>mixture model</strong></span> is essentially an extension of the idea behind fuzzy K-means; however, it makes <a id="id675" class="indexterm"></a>an assumption that there is an underlying <a id="id676" class="indexterm"></a>probability distribution that generates the data. For example, we might assume that the data points are drawn from a set of K-independent Gaussian (normal) probability distributions. The cluster assignments are also soft, so each point is represented by K membership weights in each of the K underlying probability distributions.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note35"></a>Note</h3><p>See <a class="ulink" href="http://en.wikipedia.org/wiki/Mixture_model" target="_blank">http://en.wikipedia.org/wiki/Mixture_model</a> for further details and for a mathematical treatment of mixture models.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec69"></a>Hierarchical clustering</h3></div></div></div><p><span class="strong"><strong>Hierarchical clustering</strong></span> is a structured clustering approach that results in a multilevel hierarchy <a id="id677" class="indexterm"></a>of clusters, where each cluster might <a id="id678" class="indexterm"></a>contain many subclusters (or child clusters). Each child cluster is, thus, linked to the parent cluster. This form of clustering is often also called tree clustering.</p><p>Agglomerative<a id="id679" class="indexterm"></a> clustering is a bottom-up approach where:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Each data point begins in its own cluster</p></li><li style="list-style-type: disc"><p>The similarity (or distance) between each pair of clusters is evaluated</p></li><li style="list-style-type: disc"><p>The pair of clusters that are most similar are found; this pair is then merged to form a new cluster</p></li><li style="list-style-type: disc"><p>The process is repeated until only one top-level cluster remains</p></li></ul></div><p><span class="strong"><strong>Divisive</strong></span> clustering<a id="id680" class="indexterm"></a> is a top-down approach that works in reverse, starting with one cluster and at each stage, splitting a cluster into two, until all data points are allocated to their own bottom-level cluster.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note36"></a>Note</h3><p>You can find more information at <a class="ulink" href="http://en.wikipedia.org/wiki/Hierarchical_clustering" target="_blank">http://en.wikipedia.org/wiki/Hierarchical_clustering</a>.</p></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec48"></a>Extracting the right features from your data</h2></div></div><hr /></div><p>Like <a id="id681" class="indexterm"></a>most of the machine learning models we have encountered<a id="id682" class="indexterm"></a> so far, K-means clustering requires numerical vectors as input. The same feature extraction and transformation approaches that we have seen for classification and regression are applicable for clustering.</p><p>As K-means, like least squares regression, uses a squared error function as the optimization objective, it tends to be impacted by outliers and features with large variance.</p><p>As for regression and classification cases, input data can be normalized and standardized to overcome this, which might improve accuracy. In some cases, however, it might be desirable not to standardize data, if, for example, the objective is to find segmentations according to certain specific features.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec70"></a>Extracting features from the MovieLens dataset</h3></div></div></div><p>For<a id="id683" class="indexterm"></a> this example, we will return to the movie<a id="id684" class="indexterm"></a> rating dataset we used in <a class="link" href="#" linkend="ch04">Chapter 4</a>, <span class="emphasis"><em>Building a Recommendation Engine with Spark</em></span>. Recall that we have three main datasets: one that contains the movie ratings (in the <code class="literal">u.data</code> file), a second one with user data (<code class="literal">u.user</code>), and a third one with movie data (<code class="literal">u.item</code>). We will also be using the genre data file to extract the genres for each movie (<code class="literal">u.genre</code>).</p><p>We will start by looking at the movie data:</p><div class="informalexample"><pre class="programlisting">val movies = sc.textFile("/PATH/ml-100k/u.item")
println(movies.first)</pre></div><p>This should output the first line of the dataset:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>1|Toy Story (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)|0|0|0|1|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0</strong></span>
</pre></div><p>So, we have access to the move title, and we already have the movies categorized into genres. Why do we need to apply a clustering model to the movies? Clustering the movies is a useful exercise for two reasons:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>First, because we have access to the true genre labels, we can use these to evaluate the quality of the clusters that the model finds</p></li><li style="list-style-type: disc"><p>Second, we might wish to segment the movies based on some other attributes or features, apart from their genres</p></li></ul></div><p>For example, in this case, it seems that we don't have a lot of data to use for clustering, apart from the genres and title. However, this is not trueâ€”we also have the ratings data. Previously, we created a matrix factorization model from the ratings data. The model is made up of a set of user and movie factor vectors.</p><p>We<a id="id685" class="indexterm"></a> can think of the movie factors as <a id="id686" class="indexterm"></a>representing each movie in a new latent feature space, where each latent feature, in turn, represents some form of structure in the ratings matrix. While it is not possible to directly interpret each latent feature, they might represent some hidden structure that influences the ratings behavior between users and movies. One factor could represent genre preference, another could refer to actors or directors, while yet another could represent the theme of the movie, and so on.</p><p>So, if we use these factor vector representations of each movie as inputs to our clustering model, we will end up with a clustering that is based on the actual rating behavior of users rather than manual genre assignments.</p><p>The same logic applies to the user factorsâ€”they represent users in the latent feature space of rating behavior, so clustering the user vectors should result in a clustering based on user rating behavior.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl3sec28"></a>Extracting movie genre labels</h4></div></div></div><p>Before <a id="id687" class="indexterm"></a>proceeding further, let's<a id="id688" class="indexterm"></a> extract the genre mappings from the <code class="literal">u.genre</code> file. As you can see from the first line of the preceding dataset, we will need to map from the numerical genre assignments to the textual version so that they are readable.</p><p>Take a look at the first few lines of <code class="literal">u.genre</code>:</p><div class="informalexample"><pre class="programlisting">val genres = sc.textFile("/PATH/ml-100k/u.genre")
genres.take(5).foreach(println)</pre></div><p>You should see the following output displayed:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>unknown|0</strong></span>
<span class="strong"><strong>Action|1</strong></span>
<span class="strong"><strong>Adventure|2</strong></span>
<span class="strong"><strong>Animation|3</strong></span>
<span class="strong"><strong>Children's|4</strong></span>
</pre></div><p>Here, <code class="literal">0</code> is the index of the relevant genre, while <code class="literal">unknown</code> is the genre assigned for this index. The indices correspond to the indices of the binary subvector that will represent the genres for each movie (that is, the 0s and 1s in the preceding movie data).</p><p>To extract the genre mappings, we will split each line and extract a key-value pair, where the key is the text genre and the value is the index. Note that we have to filter out an empty line at the end; this will, otherwise, throw an error when we try to split the line (see the code highlighted here):</p><div class="informalexample"><pre class="programlisting">val genreMap = genres.filter(<span class="strong"><strong>!_.isEmpty</strong></span>).map(line =&gt; line.split("\\|")).map(array =&gt; (array(1), array(0))).collectAsMap
println(genreMap)</pre></div><p>The preceding code will provide the following output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Map(2 -&gt; Adventure, 5 -&gt; Comedy, 12 -&gt; Musical, 15 -&gt; Sci-Fi, 8 -&gt; Drama, 18 -&gt; Western, ...</strong></span>
</pre></div><p>Next, we'll create a new RDD from the movie data and our genre mapping; this RDD contains<a id="id689" class="indexterm"></a> the movie ID, title, and genres. We will use this later to create a more readable output when we evaluate <a id="id690" class="indexterm"></a>the clusters assigned to each movie by our clustering model.</p><p>In the following code section, we will map over each movie and extract the genres subvector (which will still contain <code class="literal">Strings</code> rather than <code class="literal">Int</code> indexes). We will then apply the <code class="literal">zipWithIndex</code> method to create a new collection that contains the indices of the genre subvector, and we will filter this collection so that we are left only with the positive assignments (that is, the 1s that denote a genre assignment for the relevant index). We can then use our extracted genre mapping to map these indices to the textual genres. Finally, we will inspect the first record of the new <code class="literal">RDD</code> to see the result of these operations:</p><div class="informalexample"><pre class="programlisting">val titlesAndGenres = movies.map(_.split("\\|")).map { array =&gt;
  val genres = array.toSeq.slice(5, array.size)
  val genresAssigned = genres.zipWithIndex.filter { case (g, idx) =&gt; 
    g == "1" 
  }.map { case (g, idx) =&gt; 
    genreMap(idx.toString) 
  }
  (array(0).toInt, (array(1), genresAssigned))
}
println(titlesAndGenres.first)</pre></div><p>This should output the following result:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>(1,(Toy Story (1995),ArrayBuffer(Animation, Children's, Comedy)))</strong></span>
</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl3sec29"></a>Training the recommendation model</h4></div></div></div><p>To<a id="id691" class="indexterm"></a> get the user and movie<a id="id692" class="indexterm"></a> factor vectors, we first need to train another recommendation model. Fortunately, we have already done this in <a class="link" href="#" linkend="ch04">Chapter 4</a>, <span class="emphasis"><em>Building a Recommendation Engine with Spark</em></span>, so we will follow the same procedure:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark.mllib.recommendation.ALS
import org.apache.spark.mllib.recommendation.Rating
val rawData = sc.textFile("/PATH/ml-100k/u.data")
val rawRatings = rawData.map(_.split("\t").take(3))
val ratings = rawRatings.map{ case Array(user, movie, rating) =&gt; Rating(user.toInt, movie.toInt, rating.toDouble) }
ratings.cache
val alsModel = ALS.train(ratings, 50, 10, 0.1)</pre></div><p>Recall from <a class="link" href="#" linkend="ch04">Chapter 4</a>, <span class="emphasis"><em>Building a Recommendation Engine with Spark</em></span>, that the ALS model returned contains the factors in two RDDs of key-value pairs (called <code class="literal">userFeatures</code> and <code class="literal">productFeatures</code>) with the user or movie ID as the key and the factor as the value. We will need to extract just the factors and transform each one of them into an MLlib <code class="literal">Vector</code> to use as training input for our clustering model.</p><p>We will do this for both users and movies as follows:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark.mllib.linalg.Vectors
val movieFactors = alsModel.productFeatures.map { case (id, factor) =&gt; (id, Vectors.dense(factor)) }
val movieVectors = movieFactors.map(_._2)
val userFactors = alsModel.userFeatures.map { case (id, factor) =&gt; (id, Vectors.dense(factor)) }
val userVectors = userFactors.map(_._2)</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl3sec30"></a>Normalization</h4></div></div></div><p>Before<a id="id693" class="indexterm"></a> we train our clustering model, it might be<a id="id694" class="indexterm"></a> useful to look into the distribution of the input data in the form of the factor vectors. This will tell us whether we need to normalize the training data.</p><p>We will follow the same approach as we did in <a class="link" href="#" linkend="ch05">Chapter 5</a>, <span class="emphasis"><em>Building a Classification Model with Spark</em></span>, using MLlib's summary statistics available in the distributed <code class="literal">RowMatrix</code> class:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark.mllib.linalg.distributed.RowMatrix
val movieMatrix = new RowMatrix(movieVectors)
val movieMatrixSummary = movieMatrix.computeColumnSummaryStatistics()
val userMatrix = new RowMatrix(userVectors)
val userMatrixSummary = 
userMatrix.computeColumnSummaryStatistics()
println("Movie factors mean: " + movieMatrixSummary.mean)
println("Movie factors variance: " + movieMatrixSummary.variance)
println("User factors mean: " + userMatrixSummary.mean)
println("User factors variance: " + userMatrixSummary.variance)</pre></div><p>You <a id="id695" class="indexterm"></a>should see output similar to the one<a id="id696" class="indexterm"></a> here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Movie factors mean: [0.28047737659519767,0.26886479057520024,0.2935579964446398,0.27821738264113755, ... </strong></span>
<span class="strong"><strong>Movie factors variance: [0.038242041794064895,0.03742229118854288,0.044116961097355877,0.057116244055791986, ...</strong></span>
<span class="strong"><strong>User factors mean: [0.2043520841572601,0.22135773814655782,0.2149706318418221,0.23647602029329481, ...</strong></span>
<span class="strong"><strong>User factors variance: [0.037749421148850396,0.02831191551960241,0.032831876953314174,0.036775110657850954, ...</strong></span>
</pre></div><p>If we look at the output, we will see that there do not appear to be any important outliers that might skew the clustering results, so normalization should not be required in this case.</p></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec49"></a>Training a clustering model</h2></div></div><hr /></div><p>Training<a id="id697" class="indexterm"></a> for K-means in MLlib takes an approach similar to the other modelsâ€”we pass an RDD that contains our training data to the <code class="literal">train</code> method of the <code class="literal">KMeans</code> object. Note that here we do not use <code class="literal">LabeledPoint</code> instances, as the labels are not used in clustering; they are used only in the feature vectors. Thus, we use a RDD <code class="literal">[Vector]</code> as input to the <code class="literal">train</code> method.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec71"></a>Training a clustering model on the MovieLens dataset</h3></div></div></div><p>We <a id="id698" class="indexterm"></a>will <a id="id699" class="indexterm"></a>train a model for both the movie and user factors that we generated by running our recommendation model. We need to pass in the number of clusters K and the maximum number of iterations for the algorithm to run. Model training might run for less than the maximum number of iterations if the change in the objective function from one iteration to the next is less than the tolerance level (the default for this tolerance is 0.0001).</p><p>MLlib's K-means provides random and K-means || initialization, with the default being K-means ||. As both of these initialization methods are based on random selection to some extent, each model training run will return a different result.</p><p>K-means does not generally converge <a id="id700" class="indexterm"></a>to a global optimum model, so<a id="id701" class="indexterm"></a> performing multiple training runs and selecting the most optimal model from these runs is a common practice. MLlib's training methods expose an option to complete multiple model training runs. The best training run, as measured by the evaluation of the loss function, is selected as the final model.</p><p>We will first set up the required imports, as well as model parameters: K, maximum iterations, and number of runs:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark.mllib.clustering.KMeans
val numClusters = 5
val numIterations = 10
val numRuns = 3</pre></div><p>We will then run K-means on the movie factor vectors:</p><div class="informalexample"><pre class="programlisting">val movieClusterModel = KMeans.train(movieVectors, numClusters, numIterations, numRuns)</pre></div><p>Once the model has completed training, we should see output that looks something like this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>14/09/02 21:53:58 INFO SparkContext: Job finished: collectAsMap at KMeans.scala:193, took 0.02043 s</strong></span>
<span class="strong"><strong>14/09/02 21:53:58 INFO KMeans: Iterations took 0.331 seconds.</strong></span>
<span class="strong"><strong>14/09/02 21:53:58 INFO KMeans: KMeans reached the max number of iterations: 10.</strong></span>
<span class="strong"><strong>14/09/02 21:53:58 INFO KMeans: The cost for the best run is 2586.298785925147</strong></span>
<span class="strong"><strong>.</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>movieClusterModel: org.apache.spark.mllib.clustering.KMeansModel = org.apache.spark.mllib.clustering.KMeansModel@71c6f512</strong></span>
</pre></div><p>As can be seen from the highlighted text, the model training output tells us that the maximum number of iterations was reached, so the training process did not stop early based on the convergence criterion. It also shows the training set error (that is, the value of the K-means objective function) for the best run.</p><p>We <a id="id702" class="indexterm"></a>can try a much larger setting<a id="id703" class="indexterm"></a> for the maximum iterations and use only one training run to see an example where the K-means model converges:</p><div class="informalexample"><pre class="programlisting">val movieClusterModelConverged = KMeans.train(movieVectors, numClusters, 100)</pre></div><p>You should be able to see the <code class="literal">KMeans converged in ... iterations</code> text in the model output; this text indicates that after so many iterations, the K-means objective function did not decrease more than the tolerance level:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>14/09/02 22:04:38 INFO SparkContext: Job finished: collectAsMap at KMeans.scala:193, took 0.040685 s</strong></span>
<span class="strong"><strong>14/09/02 22:04:38 INFO KMeans: Run 0 finished in 34 iterations</strong></span>
<span class="strong"><strong>14/09/02 22:04:38 INFO KMeans: Iterations took 0.812 seconds.</strong></span>
<span class="strong"><strong>14/09/02 22:04:38 INFO KMeans: KMeans converged in 34 iterations.</strong></span>
<span class="strong"><strong>14/09/02 22:04:38 INFO KMeans: The cost for the best run is 2584.9354332904104.</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>movieClusterModelConverged: org.apache.spark.mllib.clustering.KMeansModel = org.apache.spark.mllib.clustering.KMeansModel@6bb28fb5</strong></span>
</pre></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip41"></a>Tip</h3><p>Notice that when we use a lower number of iterations but use multiple training runs, we typically get a training error (called cost above) that is very similar to the one we obtain by running the model to convergence. Using the multiple runs option can, therefore, be a very effective method to find the best possible model.</p></div><p>Finally, we will also train a K-means model on the user factor vectors:</p><div class="informalexample"><pre class="programlisting">val userClusterModel = KMeans.train(userVectors, numClusters, numIterations, numRuns)</pre></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec50"></a>Making predictions using a clustering model</h2></div></div><hr /></div><p>Using<a id="id704" class="indexterm"></a> the trained K-means model is straightforward<a id="id705" class="indexterm"></a> and similar to the other models we have encountered so far, such as classification and regression. We can make a prediction for a single <code class="literal">Vector</code> instance as follows:</p><div class="informalexample"><pre class="programlisting">val movie1 = movieVectors.first
val movieCluster = movieClusterModel.predict(movie1)
println(movieCluster)</pre></div><p>We can also make predictions for multiple inputs by passing a RDD <code class="literal">[Vector]</code> to the <code class="literal">predict</code> method of the model:</p><div class="informalexample"><pre class="programlisting">val predictions = movieClusterModel.predict(movieVectors)
println(predictions.take(10).mkString(","))</pre></div><p>The resulting output is a cluster assignment for each data point:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>0,0,1,1,2,1,0,1,1,1</strong></span>
</pre></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip42"></a>Tip</h3><p>Note that due to random initialization, the cluster assignments might change from one run of the model to another, so your results might differ from those shown earlier. The cluster ID themselves have no inherent meaning; they are simply arbitrarily labeled, starting from 0.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec72"></a>Interpreting cluster predictions on the MovieLens dataset</h3></div></div></div><p>We <a id="id706" class="indexterm"></a>have covered how to make <a id="id707" class="indexterm"></a>predictions for a set of input vectors, but how do we evaluate how good the predictions are? We will cover performance metrics a little later; however, here, we will see how to manually inspect and interpret the cluster assignments made by our K-means model.</p><p>While unsupervised techniques have the advantage that they do not require us to provide labeled data for training, the disadvantage is that often, the results need to be manually interpreted. Often, we would like to further examine the clusters that are found and possibly try to interpret them and assign some sort of labeling or categorization to them.</p><p>For example, we can examine the clustering of movies we have found to try to see whether there is some meaningful interpretation of each cluster, such as a common genre or theme among the movies in the cluster. There are many approaches we can use, but we will start by taking a few movies in each cluster that are closest to the center of the cluster. These movies, we assume, would be the ones that are least likely to be marginal in terms of their cluster assignment, and so, they should be among the most representative of the <a id="id708" class="indexterm"></a>movies in the cluster. By<a id="id709" class="indexterm"></a> examining these sets of movies, we can see what attributes are shared by the movies in each cluster.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl3sec31"></a>Interpreting the movie clusters</h4></div></div></div><p>To begin, we <a id="id710" class="indexterm"></a>need to decide what we mean by "closest to the center of each cluster". The objective function that is minimized by K-means is the sum of Euclidean distances between each point and the cluster center, summed over all clusters. Therefore, it is natural to use the Euclidean distance as our measure.</p><p>We will<a id="id711" class="indexterm"></a> define this function here. Note that we will need access to certain imports from the <span class="strong"><strong>Breeze</strong></span> library (a dependency of MLlib) for linear algebra and vector-based numerical functions:</p><div class="informalexample"><pre class="programlisting">import breeze.linalg._
import breeze.numerics.pow
def computeDistance(v1: DenseVector[Double], v2: DenseVector[Double]) = pow(v1 - v2, 2).sum</pre></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip43"></a>Tip</h3><p>The preceding <code class="literal">pow</code> function is a Breeze universal function. This function is the same as the <code class="literal">pow</code> function from <code class="literal">scala.math</code>, except that it operates element-wise on the vector that is returned from the minus operation between the two input vectors.</p></div><p>Now, we will use this function to compute, for each movie, the distance of the relevant movie factor vector from the center vector of the assigned cluster. We will also join our cluster assignments and distances data with the movie titles and genres so that we can output the results in a more readable way:</p><div class="informalexample"><pre class="programlisting">val titlesWithFactors = titlesAndGenres.join(movieFactors)
val moviesAssigned = titlesWithFactors.map { case (id, ((title, genres), vector)) =&gt; 
  val pred = movieClusterModel.predict(vector)
  val clusterCentre = movieClusterModel.clusterCenters(pred)
  val dist = computeDistance(DenseVector(clusterCentre.toArray), DenseVector(vector.toArray))
  (id, title, genres.mkString(" "), pred, dist) 
}
val clusterAssignments = moviesAssigned.groupBy { case (id, title, genres, cluster, dist) =&gt; cluster }.collectAsMap </pre></div><p>After <a id="id712" class="indexterm"></a>running the preceding code snippet, we have an RDD that contains a set of key-value pairs for each cluster; here, the key is the numeric cluster identifier, and the value is made up of a set of movies and related information. The movie information we have is the movie ID, title, genres, cluster index, and distance of the movie's factor vector from the cluster center.</p><p>Finally, we will iterate through each cluster and output the top 20 movies, ranked by distance from closest to the cluster center:</p><div class="informalexample"><pre class="programlisting">for ( (k, v) &lt;- clusterAssignments.toSeq.sortBy(_._1)) {
  println(s"Cluster $k:")
  val m = v.toSeq.sortBy(_._5)
  println(m.take(20).map { case (_, title, genres, _, d) =&gt; (title, genres, d) }.mkString("\n")) 
  println("=====\n")
}</pre></div><p>The following screenshot is an example output. Note that your output might differ due to random initializations of both the recommendation and clustering model.</p><div class="mediaobject"><img src="graphics/8519OS_07_06.jpg" /><div class="caption"><p>The first cluster</p></div></div><p>The first cluster, labeled 0, seems to contain a lot of old movies from the 1940s, 1950s, and 1960s, as well as a scattering of recent dramas.</p><div class="mediaobject"><img src="graphics/8519OS_07_07.jpg" /><div class="caption"><p>The second cluster</p></div></div><p>The <a id="id713" class="indexterm"></a>second cluster has a few horror movies in a prominent position, while the rest of the movies are less clear, but dramas are common too.</p><div class="mediaobject"><img src="graphics/8519OS_07_08.jpg" /><div class="caption"><p>The third cluster</p></div></div><p>The third cluster is not clear-cut but has a fair number of comedy and drama movies.</p><div class="mediaobject"><img src="graphics/8519OS_07_09.jpg" /><div class="caption"><p>The fourth cluster</p></div></div><p>The <a id="id714" class="indexterm"></a>next cluster is more clearly associated with dramas and contains some foreign language films in particular.</p><div class="mediaobject"><img src="graphics/8519OS_07_10.jpg" /><div class="caption"><p>The last cluster</p></div></div><p>The final cluster seems to be related predominantly to action and thrillers as well as romance movies, and seems to contain a number of relatively popular movies.</p><p>As you can see, it is not always straightforward to determine exactly what each cluster represents. However, there is some evidence here that the clustering is picking out attributes or commonalities between groups of movies, which might not be immediately obvious <a id="id715" class="indexterm"></a>based only on the movie titles and genres (such as a foreign language segment, a classic movie segment, and so on). If we had more metadata available, such as directors, actors, and so on, we might find out more details about the defining features of each cluster.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip44"></a>Tip</h3><p>We leave it as an exercise for you to perform a similar investigation into the clustering of the user factors. We have already created the input vectors in the <code class="literal">userVectors</code> variable, so you can train a K-means model on these vectors. After that, in order to evaluate the clusters, you would need to investigate the closest users for each cluster center (as we did for movies) and see if some common characteristics can be identified from the movies they have rated or the user metadata available.</p></div></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec51"></a>Evaluating the performance of clustering models</h2></div></div><hr /></div><p>Like<a id="id716" class="indexterm"></a> models such as regression, classification, and recommendation engines, there are many evaluation metrics that can be applied to clustering models to analyze their performance and the goodness of the clustering of the data points. Clustering evaluation is generally divided into either internal or external evaluation. Internal evaluation refers to the case where the same data used to train the model is used for evaluation. External evaluation refers to using data external to the training data for evaluation purposes.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec73"></a>Internal evaluation metrics</h3></div></div></div><p>Common<a id="id717" class="indexterm"></a> internal evaluation <a id="id718" class="indexterm"></a>metrics include the WCSS we covered earlier (which is exactly the K-means objective function), the Davies-Bouldin index, the Dunn Index, and the silhouette coefficient. All these measures tend to reward clusters where elements within a cluster are relatively close together, while elements in different clusters are relatively far away from each other.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note37"></a>Note</h3><p>The Wikipedia<a id="id719" class="indexterm"></a> page on clustering evaluation at <a class="ulink" href="http://en.wikipedia.org/wiki/Cluster_analysis#Internal_evaluation" target="_blank">http://en.wikipedia.org/wiki/Cluster_analysis#Internal_evaluation</a> has more details.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec74"></a>External evaluation metrics</h3></div></div></div><p>Since <a id="id720" class="indexterm"></a>clustering can be thought <a id="id721" class="indexterm"></a>of as unsupervised classification, if we have some form of labeled (or partially labeled) data available, we could use these labels to evaluate a clustering model. We can make predictions of clusters (that is, the class labels) using the model and evaluate the predictions against the true labels using metrics similar to some that we saw for classification evaluation (that is, based on true positive and negative and false positive and negative rates).</p><p>These include the Rand measure, F-measure, Jaccard index, and others.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note38"></a>Note</h3><p>See <a class="ulink" href="http://en.wikipedia.org/wiki/Cluster_analysis#External_evaluation" target="_blank">http://en.wikipedia.org/wiki/Cluster_analysis#External_evaluation</a> for more information on external evaluation for clustering.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec75"></a>Computing performance metrics on the MovieLens dataset</h3></div></div></div><p>MLlib <a id="id722" class="indexterm"></a>provides <a id="id723" class="indexterm"></a>a<a id="id724" class="indexterm"></a> convenient <code class="literal">computeCost</code> function to compute the WCSS objective function given a RDD <code class="literal">[Vector]</code>. We will compute this metric for the following movie and user training data:</p><div class="informalexample"><pre class="programlisting">val movieCost = movieClusterModel.computeCost(movieVectors)
val userCost = userClusterModel.computeCost(userVectors)
println("WCSS for movies: " + movieCost)
println("WCSS for users: " + userCost)</pre></div><p>This should output the result similar to the following one:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>WCSS for movies: 2586.0777166339426</strong></span>
<span class="strong"><strong>WCSS for users: 1403.4137493396831</strong></span>
</pre></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec52"></a>Tuning parameters for clustering models</h2></div></div><hr /></div><p>In<a id="id725" class="indexterm"></a> contrast to<a id="id726" class="indexterm"></a> many of the other models we have come across so far, K-means only has one parameter that can be tuned. This is K, the number of cluster centers chosen.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl2sec76"></a>Selecting K through cross-validation</h3></div></div></div><p>As we<a id="id727" class="indexterm"></a> have done with classification<a id="id728" class="indexterm"></a> and regression models, we can <a id="id729" class="indexterm"></a>apply cross-validation techniques to select the optimal number of clusters for our model. This works in much the same way as for supervised learning methods. We will split the dataset into a training set and a test set. We will then train a model on the training set and compute the evaluation metric of interest on the test set.</p><p>We will do this for the movie clustering using the built-in WCSS evaluation metric provided by MLlib in the following code, using a 60 percent / 40 percent split between the training set and test set:</p><div class="informalexample"><pre class="programlisting">val trainTestSplitMovies = movieVectors.randomSplit(Array(0.6, 0.4), 123)
val trainMovies = trainTestSplitMovies(0)
val testMovies = trainTestSplitMovies(1)
val costsMovies = Seq(2, 3, 4, 5, 10, 20).map { k =&gt; (k, KMeans.train(trainMovies, numIterations, k, numRuns).computeCost(testMovies)) }
println("Movie clustering cross-validation:")
costsMovies.foreach { case (k, cost) =&gt; println(f"WCSS for K=$k id $cost%2.2f") }</pre></div><p>This should give results that look something like the ones shown here.</p><p>The output of movie clustering cross-validation is:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Movie clustering cross-validation</strong></span>
<span class="strong"><strong>WCSS for K=2 id 942.06</strong></span>
<span class="strong"><strong>WCSS for K=3 id 942.67</strong></span>
<span class="strong"><strong>WCSS for K=4 id 950.35</strong></span>
<span class="strong"><strong>WCSS for K=5 id 948.20</strong></span>
<span class="strong"><strong>WCSS for K=10 id 943.26</strong></span>
<span class="strong"><strong>WCSS for K=20 id 947.10</strong></span>
</pre></div><p>We can observe that the WCSS decreases as the number of clusters increases, up to a point. It then begins to increase. Another common pattern observed in the WCSS in cross-validation for K-means is that the metric continues to decrease as K increases, but at a certain point, the rate of decrease flattens out substantially. The value of K at which this occurs is generally selected as the optimal K parameter (this is sometimes called the elbow point, as this is where the line kinks when drawn as a graph).</p><p>In our case, we might select a value of 10 for K, based on the preceding results. Also, note that the clusters that are computed by the model are often used for purposes that require some human interpretation (such as the cases of movie and customer segmentation we mentioned earlier). Therefore, this consideration also impacts the choice of K, as <a id="id730" class="indexterm"></a>although a higher <a id="id731" class="indexterm"></a>value of K might be more optimal from <a id="id732" class="indexterm"></a>the mathematical point of view, it might be more difficult to understand and interpret many clusters.</p><p>For completeness, we will also compute the cross-validation metrics for user clustering:</p><div class="informalexample"><pre class="programlisting">val trainTestSplitUsers = userVectors.randomSplit(Array(0.6, 0.4), 123)
val trainUsers = trainTestSplitUsers(0)
val testUsers = trainTestSplitUsers(1)
val costsUsers = Seq(2, 3, 4, 5, 10, 20).map { k =&gt; (k, KMeans.train(trainUsers, numIterations, k, numRuns).computeCost(testUsers)) }
println("User clustering cross-validation:")
costsUsers.foreach { case (k, cost) =&gt; println(f"WCSS for K=$k id $cost%2.2f") }</pre></div><p>We will see a pattern that is similar to the movie case:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>User clustering cross-validation:</strong></span>
<span class="strong"><strong>WCSS for K=2 id 544.02</strong></span>
<span class="strong"><strong>WCSS for K=3 id 542.18</strong></span>
<span class="strong"><strong>WCSS for K=4 id 542.38</strong></span>
<span class="strong"><strong>WCSS for K=5 id 542.33</strong></span>
<span class="strong"><strong>WCSS for K=10 id 539.68</strong></span>
<span class="strong"><strong>WCSS for K=20 id 541.21</strong></span>
</pre></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip45"></a>Tip</h3><p>Note that your results may differ slightly due to random initialization of the clustering models.</p></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch07lvl1sec53"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we explored a new class of model that learns structure from unlabeled dataâ€”unsupervised learning. We worked through required input data, feature extraction, and saw how to use the output of one model (a recommendation model in our example) as the input to another model (our K-means clustering model). Finally, we evaluated the performance of the clustering model, both using manual interpretation of the cluster assignments and using mathematical performance metrics.</p><p>In the next chapter, we will cover another type of unsupervised learning used to reduce our data down to its most important features or componentsâ€”dimensionality reduction models.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch08"></a>ChapterÂ 8.Â Dimensionality Reduction with Spark</h2></div></div></div><p>Over the course of this chapter, we will continue our exploration of unsupervised learning models<a id="id733" class="indexterm"></a> in the form of <span class="strong"><strong>dimensionality reduction</strong></span>.</p><p>Unlike the models we have covered so far, such as regression, classification, and clustering, dimensionality reduction does not focus on making predictions. Instead, it tries to take a set of input data with a feature dimension <span class="emphasis"><em>D</em></span> (that is, the length of our feature vector) and extract a representation of the data of dimension <span class="emphasis"><em>k</em></span>, where <span class="emphasis"><em>k</em></span> is usually significantly smaller than <span class="emphasis"><em>D</em></span>. It is, therefore, a form of preprocessing or feature transformation rather than a predictive model in its own right.</p><p>It is important that the representation that is extracted should still be able to capture a large proportion of the variability or structure of the original data. The idea behind this is that most data sources will contain some form of underlying structure. This structure is typically unknown (often called latent features or latent factors), but if we can uncover some of this structure, our models could learn this structure and make predictions from it rather than from the data in its raw form, which might be noisy or contain many irrelevant features. In other words, dimensionality reduction throws away some of the noise in the data and keeps the hidden structure that is present.</p><p>In some cases, the dimensionality of the raw data is far higher than the number of data points we have, so without dimensionality reduction, it would be difficult for other machine learning models, such as classification and regression, to learn anything, as they need to fit a number of parameters that is far larger than the number of training examples (in this sense, these methods bear some similarity to the regularization approaches that we have seen used in classification and regression).</p><p>A few<a id="id734" class="indexterm"></a> use cases of dimensionality reduction techniques include:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Exploratory data analysis</p></li><li style="list-style-type: disc"><p>Extracting features to train other machine learning models</p></li><li style="list-style-type: disc"><p>Reducing storage and computation requirements for very large models in the prediction phase (for example, a production system that makes predictions)</p></li><li style="list-style-type: disc"><p>Reducing a large group of text documents down to a set of hidden topics or concepts</p></li><li style="list-style-type: disc"><p>Making <a id="id735" class="indexterm"></a>learning and generalization of models easier when our data has a very large number of features (for example, when working with text, sound, images, or video data, which tends to be very high-dimensional)</p></li></ul></div><p>In this chapter, we will:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Introduce the types of dimensionality reduction models available in MLlib</p></li><li style="list-style-type: disc"><p>Work with images of faces to extract features suitable for dimensionality reduction</p></li><li style="list-style-type: disc"><p>Train a dimensionality reduction model using MLlib</p></li><li style="list-style-type: disc"><p>Visualize and evaluate the results</p></li><li style="list-style-type: disc"><p>Perform parameter selection for our dimensionality reduction model</p></li></ul></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec54"></a>Types of dimensionality reduction</h2></div></div><hr /></div><p>MLlib <a id="id736" class="indexterm"></a>provides two models for dimensionality reduction; these models are closely related to each other. They are <span class="strong"><strong>Principal Components Analysis</strong></span> (<span class="strong"><strong>PCA</strong></span>) and <span class="strong"><strong>Singular Value Decomposition</strong></span> (<span class="strong"><strong>SVD</strong></span>).</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec77"></a>Principal Components Analysis</h3></div></div></div><p>PCA <a id="id737" class="indexterm"></a>operates on a data matrix <span class="emphasis"><em>X</em></span> and seeks to extract a <a id="id738" class="indexterm"></a>set of <span class="emphasis"><em>k</em></span> principal components from <span class="emphasis"><em>X</em></span>. The principal components are each uncorrelated to each other and are computed such that the first principal component accounts for the largest variation in the input data. Each subsequent principal component is, in turn, computed such that it accounts for the largest variation, provided that it is independent of the principal components computed so far.</p><p>In this way, the <span class="emphasis"><em>k</em></span> principal components returned are guaranteed to account for the highest amount of variation in the input data possible. Each principal component, in fact, has the same feature dimensionality as the original data matrix. Hence, a projection step is required in order to actually perform dimensionality reduction, where the original data is projected into the <span class="emphasis"><em>k-dimensional</em></span> space represented by the principal components.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec78"></a>Singular Value Decomposition</h3></div></div></div><p>SVD<a id="id739" class="indexterm"></a> seeks to decompose a matrix <span class="emphasis"><em>X</em></span> of dimension <span class="emphasis"><em>m x n</em></span> into<a id="id740" class="indexterm"></a> three component matrices:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><span class="emphasis"><em>U</em></span> of dimension <span class="emphasis"><em>m x m</em></span></p></li><li style="list-style-type: disc"><p><span class="emphasis"><em>S</em></span>, a diagonal<a id="id741" class="indexterm"></a> matrix of size <span class="emphasis"><em>m x n</em></span>; the entries of <span class="emphasis"><em>S</em></span> are referred to as the <span class="strong"><strong>singular values</strong></span></p></li><li style="list-style-type: disc"><p><span class="emphasis"><em>V<sup>T</sup></em></span> of dimension <span class="emphasis"><em>n x n</em></span></p></li></ul></div><div class="informalexample"><pre class="programlisting">X = U * S * V<sup> T</sup></pre></div><p>Looking at the preceding formula, it appears that we have not reduced the dimensionality of the problem at all, as by multiplying <span class="emphasis"><em>U</em></span>, <span class="emphasis"><em>S</em></span>, and <span class="emphasis"><em>V</em></span>, we reconstruct the original matrix. In practice, the truncated SVD is usually computed. That is, only the top <span class="emphasis"><em>k</em></span> singular values, which represent the most variation in the data, are kept, while the rest are discarded. The formula to reconstruct <span class="emphasis"><em>X</em></span> based on the component matrices is then approximate:</p><div class="informalexample"><pre class="programlisting">X ~ U<sub>k</sub> * S<sub>k</sub> * V<sub>k</sub> <sub> T</sub></pre></div><p>An illustration of the truncated SVD is shown here:</p><div class="mediaobject"><img src="graphics/8519OS_08_01.jpg" /><div class="caption"><p>The truncated Singular Value Decomposition</p></div></div><p>Keeping the top <span class="emphasis"><em>k</em></span> singular values is similar to keeping the top <span class="emphasis"><em>k</em></span> principal components in PCA. In fact, SVD and PCA are directly related, as we will see a little later in this chapter.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note39"></a>Note</h3><p>A detailed mathematical treatment of both PCA and SVD is beyond the scope of this book.</p><p>An <a id="id742" class="indexterm"></a>overview of dimensionality reduction <a id="id743" class="indexterm"></a>can be found in the Spark documentation at <a class="ulink" href="http://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html" target="_blank">http://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html</a>.</p><p>The following links contain a more in-depth mathematical overview of PCA and SVD, respectively: <a class="ulink" href="http://en.wikipedia.org/wiki/Principal_component_analysis" target="_blank">http://en.wikipedia.org/wiki/Principal_component_analysis</a> and <a class="ulink" href="http://en.wikipedia.org/wiki/Singular_value_decomposition" target="_blank">http://en.wikipedia.org/wiki/Singular_value_decomposition</a>.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec79"></a>Relationship with matrix factorization</h3></div></div></div><p>PCA<a id="id744" class="indexterm"></a> and SVD are both matrix factorization techniques, in the sense that they decompose a data matrix into subcomponent matrices, each of which has a lower dimension (or rank) than the original matrix. Many other dimensionality reduction techniques are based on matrix factorization.</p><p>You<a id="id745" class="indexterm"></a> might remember another example of matrix factorization, that is, collaborative filtering, that we have already seen in <a class="link" href="#" linkend="ch04">Chapter 4</a>, <span class="emphasis"><em>Building a Recommendation Engine with Spark</em></span>. Matrix factorization approaches to collaborative filtering work by factorizing the ratings matrix into two components: the user factor matrix and the item factor matrix. Each of these has a lower dimension than the original data, so these methods also act as dimensionality reduction models.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note40"></a>Note</h3><p>Many of the best performing approaches to collaborative filtering include models based on SVD. Simon Funk's approach to the Netflix prize is a famous example. You can look at it at <a class="ulink" href="http://sifter.org/~simon/journal/20061211.html" target="_blank">http://sifter.org/~simon/journal/20061211.html</a>.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec80"></a>Clustering as dimensionality reduction</h3></div></div></div><p>The <a id="id746" class="indexterm"></a>clustering models we covered in the previous chapter can also be used for a form of dimensionality reduction. This works in the following way:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Assume that we cluster our high-dimensional feature vectors using a K-means clustering model, with <span class="emphasis"><em>k</em></span> clusters. The result is a set of <span class="emphasis"><em>k</em></span> cluster centers.</p></li><li style="list-style-type: disc"><p>We can represent each of our original data points in terms of how far it is from each of these cluster centers. That is, we can compute the distance of a data point to each cluster center. The result is a set of <span class="emphasis"><em>k</em></span> distances for each data point.</p></li><li style="list-style-type: disc"><p>These <span class="emphasis"><em>k</em></span> distances can form a new vector of dimension <span class="emphasis"><em>k</em></span>. We can now represent our original data as a new vector of lower dimension, relative to the original feature dimension.</p></li></ul></div><p>Depending on the distance metric used, this can result in both dimensionality reduction and a form of nonlinear transformation of the data, allowing us to learn a more complex model while still benefiting from the speed and scalability of a linear model. For example, using a Gaussian or exponential distance function can approximate a very complex nonlinear feature transformation.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec55"></a>Extracting the right features from your data</h2></div></div><hr /></div><p>As <a id="id747" class="indexterm"></a>with all machine learning models we have explored so far, dimensionality reduction models also operate on a feature vector representation of our data.</p><p>For this<a id="id748" class="indexterm"></a> chapter, we will dive into the world of image processing, using the <span class="strong"><strong>Labeled Faces in the Wild</strong></span> (<span class="strong"><strong>LFW</strong></span>) dataset of facial images. This dataset contains over 13,000 images of faces generally taken from the Internet and belonging to well-known public figures. The faces are labeled with the person's name.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec81"></a>Extracting features from the LFW dataset</h3></div></div></div><p>In <a id="id749" class="indexterm"></a>order to avoid having to download and process<a id="id750" class="indexterm"></a> a very large dataset, we will work with a subset of the images, using people who have names that start with an "A". This dataset can be downloaded from <a class="ulink" href="http://vis-www.cs.umass.edu/lfw/lfw-a.tgz" target="_blank">http://vis-www.cs.umass.edu/lfw/lfw-a.tgz</a>.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note41"></a>Note</h3><p>For more details and other variants of the data, visit <a class="ulink" href="http://vis-www.cs.umass.edu/lfw/" target="_blank">http://vis-www.cs.umass.edu/lfw/</a>.</p><p>The original research paper reference is:</p><p><span class="emphasis"><em>Gary B. Huang</em></span>, <span class="emphasis"><em>Manu Ramesh</em></span>, <span class="emphasis"><em>Tamara Berg</em></span>, and <span class="emphasis"><em>Erik Learned-Miller</em></span>. <span class="emphasis"><em>Labeled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments</em></span>. University of Massachusetts, Amherst, Technical Report 07-49, October, 2007.</p><p>It can be downloaded from <a class="ulink" href="http://vis-www.cs.umass.edu/lfw/lfw.pdf" target="_blank">http://vis-www.cs.umass.edu/lfw/lfw.pdf</a>.</p></div><p>Unzip <a id="id751" class="indexterm"></a>the data using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;tar xfvz lfw-a.tgz</strong></span>
</pre></div><p>This<a id="id752" class="indexterm"></a> will create a folder called <code class="literal">lfw</code>, which contains a number of subfolders, one for each person.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl3sec32"></a>Exploring the face data</h4></div></div></div><p>Start <a id="id753" class="indexterm"></a>up your Spark Scala console by ensuring that you<a id="id754" class="indexterm"></a> allocate sufficient memory, as dimensionality reduction methods can be quite computationally expensive:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;./SPARK_HOME/bin/spark-shell --driver-memory 2g</strong></span>
</pre></div><p>Now that we've unzipped the data, we face a small challenge. Spark provides us with a way to read text files and custom Hadoop input data sources. However, there is no built-in functionality to allow us to read images.</p><p>Spark provides a method called <code class="literal">wholeTextFiles</code>, which allows us to operate on entire files at once, compared to the <code class="literal">textFile</code> method that we have been using so far, which operates on the individual lines within a text file (or multiple files).</p><p>We will use the <code class="literal">wholeTextFiles</code> method to access the location of each file. Using these file paths, we will write custom code to load and process the images. In the following example code, we will use <code class="literal">PATH</code> to refer to the directory in which you extracted the <code class="literal">lfw</code> subdirectory.</p><p>We can use a wildcard path specification (using the <code class="literal">*</code> character highlighted in the following code snippet) to tell Spark to look in each directory under the <code class="literal">lfw</code> directory for files:</p><div class="informalexample"><pre class="programlisting">val path = "/<span class="strong"><strong>PATH</strong></span>/lfw/<span class="strong"><strong>*</strong></span>"
val rdd = sc.wholeTextFiles(path)
val first = rdd.first
println(first)</pre></div><p>Running the <code class="literal">first</code> command might take a little time, as Spark first scans the specified directory structure for all available files. Once completed, you should see output similar to the one shown here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>first: (String, String) =  (file:/PATH/lfw/Aaron_Eckhart/Aaron_Eckhart_0001.jpg,ï¿½ï¿½ï¿½ï¿½??JFIF????? ...</strong></span>
</pre></div><p>You will <a id="id755" class="indexterm"></a>see that <code class="literal">wholeTextFiles</code> returns an RDD that<a id="id756" class="indexterm"></a> contains key-value pairs, where the key is the file location while the value is the content of the entire text file. For our purposes, we only care about the file path, as we cannot work directly with the image data as a string (notice that it is displayed as "binary nonsense" in the shell output).</p><p>Let's extract the file paths from the RDD. Note that earlier, the file path starts with the <code class="literal">file:</code> text. This is used by Spark when reading files in order to differentiate between different filesystems (for example, <code class="literal">file://</code> for the local filesystem, <code class="literal">hdfs://</code> for HDFS, <code class="literal">s3n://</code> for Amazon S3, and so on).</p><p>In our case, we will be using custom code to read the images, so we don't need this part of the path. Thus, we will remove it with the following <code class="literal">map</code> function:</p><div class="informalexample"><pre class="programlisting">val files = rdd.map { case (fileName, content) =&gt; fileName.replace("file:", "") }
println(files.first)</pre></div><p>This should display the file location with the <code class="literal">file:</code> prefix removed:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>/PATH/lfw/Aaron_Eckhart/Aaron_Eckhart_0001.jpg</strong></span>
</pre></div><p>Next, we will see how many files we are dealing with:</p><div class="informalexample"><pre class="programlisting">println(files.count)</pre></div><p>Running these commands creates a lot of noisy output in the Spark shell, as it outputs all the file paths that are read to the console. Ignore this part, but after the command has completed, the<a id="id757" class="indexterm"></a> output should look something like this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>..., /PATH/lfw/Azra_Akin/Azra_Akin_0003.jpg:0+19927, /PATH/lfw/Azra_Akin/Azra_Akin_0004.jpg:0+16030</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>14/09/18 20:36:25 INFO SparkContext: Job finished: count at &lt;console&gt;:19, took 1.151955 s</strong></span>
<span class="strong"><strong>1055</strong></span>
</pre></div><p>So, we can see that we have 1055 images to work with.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl3sec33"></a>Visualizing the face data</h4></div></div></div><p>Although<a id="id758" class="indexterm"></a> there are a few tools available in Scala or <a id="id759" class="indexterm"></a>Java to display images, this is one area where Python and the matplotlib library shine. We will use Scala to process and extract the images and run our models and IPython to display the actual images.</p><p>You can run a separate IPython Notebook by opening a new terminal window and launching a new notebook:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;ipython notebook</strong></span>
</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note42"></a>Note</h3><p>Note that if using Python Notebook, you should first execute the following code snippet to ensure that the images are displayed inline after each notebook cell (including the <code class="literal">%</code> character): <code class="literal">%pylab inline</code>.</p></div><p>Alternatively, you can launch a plain IPython console without the web notebook, enabling the <code class="literal">pylab</code> plotting functionality using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;ipython --pylab</strong></span>
</pre></div><p>The dimensionality reduction techniques in MLlib are only available in Scala or Java at the time of writing this book, so we will continue to use the Scala Spark shell to run the models. Therefore, you won't need to run a PySpark console.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip46"></a>Tip</h3><p>We have provided the full Python code with this chapter as a Python script as well as in the IPython Notebook format. For instructions on installing IPython, see the code bundle.</p></div><p>Let's display the image given by the first path we extracted earlier using matplotlib's <code class="literal">imread</code> and <code class="literal">imshow</code> functions:</p><div class="informalexample"><pre class="programlisting">path = "/PATH/lfw/PATH/lfw/Aaron_Eckhart/Aaron_Eckhart_0001.jpg"
ae = imread(path)
imshow(ae)</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note43"></a>Note</h3><p>You<a id="id760" class="indexterm"></a> should see the image displayed in your Notebook (or in a pop-up window if you are using the standard IPython shell). Note that we have not shown the image here.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl3sec34"></a>Extracting facial images as vectors</h4></div></div></div><p>While a<a id="id761" class="indexterm"></a> full treatment of image processing<a id="id762" class="indexterm"></a> is beyond the scope of this book, we will need to know a few basics to proceed. Each color image can be represented as a three-dimensional array, or matrix, of pixels. The first two dimensions, that is the <span class="emphasis"><em>x</em></span> and <span class="emphasis"><em>y</em></span> axes, represent the position of each pixel, while the third dimension represents the <span class="strong"><strong>red, blue, and green</strong></span> (<span class="strong"><strong>RGB</strong></span>) color values for each pixel.</p><p>A <a id="id763" class="indexterm"></a>grayscale image only requires one value per pixel (there are no RGB values), so it can be represented as a plain two-dimensional matrix. For many image-processing and machine learning tasks related to images, it is common to operate on grayscale images. We will do this here by converting the color images to grayscale first.</p><p>It is also a common practice in machine learning tasks to represent an image as a vector, instead of a matrix. We do this by concatenating each row (or alternatively, each column) of the matrix together to form a long vector (this is known as <span class="strong"><strong>reshaping</strong></span>). In this way, each raw, grayscale <a id="id764" class="indexterm"></a>image matrix is transformed into a feature vector that is usable as input to a machine learning model.</p><p>Fortunately for us, the built-in Java <span class="strong"><strong>Abstract Window Toolkit</strong></span> (<span class="strong"><strong>AWT</strong></span>) contains various basic <a id="id765" class="indexterm"></a>image-processing functions. We will define a few utility functions to perform this processing using the <code class="literal">java.awt</code> classes.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch08lvl4sec17"></a>Loading images</h5></div></div></div><p>The first<a id="id766" class="indexterm"></a> of these is a function to read an<a id="id767" class="indexterm"></a> image from a file:</p><div class="informalexample"><pre class="programlisting">import java.awt.image.BufferedImage
def loadImageFromFile(path: String): BufferedImage = { 
  import javax.imageio.ImageIO
  import java.io.File
  ImageIO.read(new File(path))
}</pre></div><p>This returns an instance of a <code class="literal">java.awt.image.BufferedImage</code> class, which stores the image data and provides a number of useful methods. Let's test it out by loading the first image into our Spark shell:</p><div class="informalexample"><pre class="programlisting">val aePath = "/PATH/lfw/Aaron_Eckhart/Aaron_Eckhart_0001.jpg"
val aeImage = loadImageFromFile(aePath)</pre></div><p>You should see the image details displayed in the shell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>aeImage: java.awt.image.BufferedImage = BufferedImage@f41266e: type = 5 ColorModel: #pixelBits = 24 numComponents = 3 color space = java.awt.color.ICC_ColorSpace@7e420794 transparency = 1 has alpha = false isAlphaPre = false ByteInterleavedRaster: width = 250 height = 250 #numDataElements 3 dataOff[0] = 2</strong></span>
</pre></div><p>There is <a id="id768" class="indexterm"></a>quite a lot of information here. Of particular<a id="id769" class="indexterm"></a> interest to us is that the image width and height are 250 pixels, and as we can see, there are three components (that is, the RGB values) that are highlighted in the preceding output.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch08lvl4sec18"></a>Converting to grayscale and resizing the images</h5></div></div></div><p>The<a id="id770" class="indexterm"></a> next function we will define <a id="id771" class="indexterm"></a>will take the image that we have loaded <a id="id772" class="indexterm"></a>with our preceding function, convert the image from <a id="id773" class="indexterm"></a>color to grayscale, and resize the image's width and height.</p><p>These steps are not strictly necessary, but both steps are done in many cases for efficiency purposes. Using RGB color images instead of grayscale increases the amount of data to be processed by a factor of 3. Similarly, larger images increase the processing and storage overhead significantly. Our raw 250 x 250 images represent 187,500 data points per image using three color components. For a set of 1055 images, this is 197,812,500 data points. Even if stored as integer values, each value stored takes 4 bytes of memory, so just 1055 images represent around 800 MB of memory! As you can see, image-processing tasks can quickly become extremely memory intensive.</p><p>If we convert to grayscale and resize the images to, say, 50 x 50 pixels, we only require 2500 data points per image. For our 1055 images, this equates to 10 MB of memory, which is far more manageable for illustrative purposes.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip47"></a>Tip</h3><p>Another reason to resize is that MLlib's PCA model works best on <span class="emphasis"><em>tall and skinny</em></span> matrices with less than 10,000 columns. We will have 2500 columns (that is, each pixel becomes an entry in our feature vector), so we will come in well below this restriction.</p></div><p>Let's define our processing function. We will do the grayscale conversion and resizing in one step, using the <code class="literal">java.awt.image</code> package:</p><div class="informalexample"><pre class="programlisting">def processImage(image: BufferedImage, width: Int, height: Int): BufferedImage = {
  val bwImage = new BufferedImage(width, height, BufferedImage.TYPE_BYTE_GRAY)
  val g = bwImage.getGraphics()
  g.drawImage(image, 0, 0, width, height, null)
  g.dispose()
  bwImage
}</pre></div><p>The first line of the function creates a new image of the desired width and height and specifies<a id="id774" class="indexterm"></a> a grayscale color model. The <a id="id775" class="indexterm"></a>third line draws the original image <a id="id776" class="indexterm"></a>onto this newly created image. The <code class="literal">drawImage</code> method takes care of the color conversion and resizing for us! Finally, we return <a id="id777" class="indexterm"></a>the new, processed image.</p><p>Let's test this out on our sample image. We will convert it to grayscale and resize it to 100 x 100 pixels:</p><div class="informalexample"><pre class="programlisting">val grayImage = processImage(aeImage, 100, 100)</pre></div><p>You should see the following output in the console:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>grayImage: java.awt.image.BufferedImage = BufferedImage@21f8ea3b: type = 10 ColorModel: #pixelBits = 8 numComponents = 1 color space = java.awt.color.ICC_ColorSpace@5cd9d8e9 transparency = 1 has alpha = false isAlphaPre = false ByteInterleavedRaster: width = 100 height = 100 #numDataElements 1 dataOff[0] = 0</strong></span>
</pre></div><p>As you can see from the highlighted output, the image's width and height are indeed 100, and the number of color components is 1.</p><p>Next, we will save the processed image to a temporary location so that we can read it back and display it in our IPython console:</p><div class="informalexample"><pre class="programlisting">import javax.imageio.ImageIO
import java.io.File
ImageIO.write(grayImage, "jpg", new File("/tmp/aeGray.jpg"))</pre></div><p>You should see a result of <code class="literal">true</code> displayed in your console, indicating that we successfully saved the image to the <code class="literal">aeGray.jpg</code> file in our <code class="literal">/tmp</code> directory.</p><p>Finally, we will read the image in Python and use matplotlib to display the image. Type the following code into your IPython Notebook or shell (remember that this should be open in a new terminal window):</p><div class="informalexample"><pre class="programlisting">tmpPath = "/tmp/aeGray.jpg"
aeGary = imread(tmpPath)
imshow(aeGary, cmap=plt.cm.gray)</pre></div><p>This should display the image (note again, we haven't shown the image here). You should see that it is grayscale and of slightly worse quality as compared to the original image. Furthermore, you <a id="id778" class="indexterm"></a>will notice that the scale of the axes are different, representing<a id="id779" class="indexterm"></a> the new 100 x 100 dimension instead of the original 250 x 250 size.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch08lvl4sec19"></a>Extracting feature vectors</h5></div></div></div><p>The final<a id="id780" class="indexterm"></a> step in the processing<a id="id781" class="indexterm"></a> pipeline is to extract the actual feature vectors that will be the input to our dimensionality reduction model. As we mentioned earlier, the raw grayscale pixel data will be our features. We will form the vectors by flattening out the two-dimensional pixel matrix. The <code class="literal">BufferedImage</code> class provides a utility method to do just this, which we will use in our function:</p><div class="informalexample"><pre class="programlisting">def getPixelsFromImage(image: BufferedImage): Array[Double] = {
  val width = image.getWidth
  val height = image.getHeight
  val pixels = Array.ofDim[Double](width * height)
  image.getData.getPixels(0, 0, width, height, pixels)
}</pre></div><p>We can then combine these three functions into one utility function that takes a file location together with the desired image's width and height and returns the raw <code class="literal">Array[Double]</code> value that contains the pixel data:</p><div class="informalexample"><pre class="programlisting">def extractPixels(path: String, width: Int, height: Int): Array[Double] = {
  val raw = loadImageFromFile(path)
  val processed = processImage(raw, width, height)
  getPixelsFromImage(processed)
}</pre></div><p>Applying this function to each element of the RDD that contains all the image file paths will give us a new RDD that contains the pixel data for each image. Let's do this and inspect the first few elements:</p><div class="informalexample"><pre class="programlisting">val pixels = files.map(f =&gt; extractPixels(f, 50, 50))
println(pixels.take(10).map(_.take(10).mkString("", ",", ", ...")).mkString("\n"))</pre></div><p>You should see output similar to this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0, ...</strong></span>
<span class="strong"><strong>241.0,243.0,245.0,244.0,231.0,205.0,177.0,160.0,150.0,147.0, ...</strong></span>
<span class="strong"><strong>253.0,253.0,253.0,253.0,253.0,253.0,254.0,254.0,253.0,253.0, ...</strong></span>
<span class="strong"><strong>244.0,244.0,243.0,242.0,241.0,240.0,239.0,239.0,237.0,236.0, ...</strong></span>
<span class="strong"><strong>44.0,47.0,47.0,49.0,62.0,116.0,173.0,223.0,232.0,233.0, ...</strong></span>
<span class="strong"><strong>0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0, ...</strong></span>
<span class="strong"><strong>1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,0.0, ...</strong></span>
<span class="strong"><strong>26.0,26.0,27.0,26.0,24.0,24.0,25.0,26.0,27.0,27.0, ...</strong></span>
<span class="strong"><strong>240.0,240.0,240.0,240.0,240.0,240.0,240.0,240.0,240.0,240.0, ...</strong></span>
<span class="strong"><strong>0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0, ...</strong></span>
</pre></div><p>The<a id="id782" class="indexterm"></a> final step is to create an MLlib <code class="literal">Vector</code> instance<a id="id783" class="indexterm"></a> for each image. We will cache the RDD to speed up our later computations:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark.mllib.linalg.Vectors
val vectors = pixels.map(p =&gt; Vectors.dense(p))
vectors.setName("image-vectors")
vectors.cache</pre></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip48"></a>Tip</h3><p>We used the <code class="literal">setName</code> function earlier to assign an RDD a name. In this case, we called it <code class="literal">image-vectors</code>. This is so that we can later identify it more easily when looking at the Spark web interface.</p></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl3sec35"></a>Normalization</h4></div></div></div><p>It is a <a id="id784" class="indexterm"></a>common practice to standardize input data prior to <a id="id785" class="indexterm"></a>running dimensionality reduction models, in particular for PCA. As we did in <a class="link" href="#" linkend="ch05">Chapter 5</a>, <span class="emphasis"><em>Building a Classification Model with Spark</em></span>, we will do this using the built-in <code class="literal">StandardScaler</code> provided by MLlib's <code class="literal">feature</code> package. We will only subtract the mean from the data in this case:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark.mllib.linalg.Matrix
import org.apache.spark.mllib.linalg.distributed.RowMatrix
import org.apache.spark.mllib.feature.StandardScaler
val scaler = new StandardScaler(withMean = true, withStd = false).fit(vectors)</pre></div><p>Calling <code class="literal">fit</code> triggers a computation on our <code class="literal">RDD[Vector]</code>. You should see output similar to the one shown here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>14/09/21 11:46:58 INFO SparkContext: Job finished: reduce at RDDFunctions.scala:111, took 0.495859 s</strong></span>
<span class="strong"><strong>scaler: org.apache.spark.mllib.feature.StandardScalerModel = org.apache.spark.mllib.feature.StandardScalerModel@6bb1a1a1</strong></span>
</pre></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip49"></a>Tip</h3><p>Note that subtracting the mean works for dense input data. However, for sparse vectors, subtracting the mean vector from each input will transform the sparse data into dense data. For very high-dimensional input, this will likely exhaust the available memory resources, so it is not advisable.</p></div><p>Finally, we will use the returned <code class="literal">scaler</code> to transform the raw image vectors to vectors with the column means subtracted:</p><div class="informalexample"><pre class="programlisting">val scaledVectors = vectors.map(v =&gt; scaler.transform(v))</pre></div><p>We mentioned earlier that the resized grayscale images would take up around 10 MB of memory. Indeed, you can take a look at the memory usage in the Spark application monitor storage page by going to <code class="literal">http://localhost:4040/storage/</code> <code class="literal">
</code>in your web browser.</p><p>Since we<a id="id786" class="indexterm"></a> gave our RDD of image vectors a friendly name <a id="id787" class="indexterm"></a>of <code class="literal">image-vectors</code>, you should see something like the following screenshot (note that as we are using <code class="literal">Vector[Double]</code>, each element takes up 8 bytes instead of 4 bytes; hence, we actually use 20 MB of memory):</p><div class="mediaobject"><img src="graphics/8519OS_08_03.jpg" /><div class="caption"><p>Size of image vectors in memory</p></div></div></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec56"></a>Training a dimensionality reduction model</h2></div></div><hr /></div><p>Dimensionality <a id="id788" class="indexterm"></a>reduction models in MLlib require vectors as inputs. However, unlike clustering that operated on an <code class="literal">RDD[Vector]</code>, PCA and SVD computations are provided as methods on a distributed <code class="literal">RowMatrix</code> (this difference is largely down to syntax, as a <code class="literal">RowMatrix</code> is simply a wrapper around an <code class="literal">RDD[Vector]</code>).</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec82"></a>Running PCA on the LFW dataset</h3></div></div></div><p>Now<a id="id789" class="indexterm"></a> that we have <a id="id790" class="indexterm"></a>extracted our image pixel data into vectors, we can<a id="id791" class="indexterm"></a> instantiate a new <code class="literal">RowMatrix</code> and call the <code class="literal">computePrincipalComponents</code> method to compute the top <code class="literal">K</code> principal components of our distributed matrix:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark.mllib.linalg.Matrix
import org.apache.spark.mllib.linalg.distributed.RowMatrix
val matrix = new RowMatrix(scaledVectors)
val K = 10
val pc = matrix.computePrincipalComponents(K)</pre></div><p>You will likely see quite a lot of output in your console while the model runs.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip50"></a>Tip</h3><p>If you see warnings such as <span class="strong"><strong>WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK</strong></span> or <span class="strong"><strong>WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK</strong></span>, you can safely ignore these.</p><p>This means that the underlying linear algebra libraries used by MLlib could not load native routines. In this case, a Java-based fallback will be used, which is slower, but there is nothing to worry about for the purposes of this example.</p></div><p>Once the model training is complete, you should see a result displayed in the console that looks similar to the following one:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>pc: org.apache.spark.mllib.linalg.Matrix = </strong></span>
<span class="strong"><strong>-0.023183157256614906  -0.010622723054037303  ... (10 total)</strong></span>
<span class="strong"><strong>-0.023960537953442107  -0.011495966728461177  ...</strong></span>
<span class="strong"><strong>-0.024397470862198022  -0.013512219690177352  ...</strong></span>
<span class="strong"><strong>-0.02463158818330343   -0.014758658113862178  ...</strong></span>
<span class="strong"><strong>-0.024941633606137027  -0.014878858729655142  ...</strong></span>
<span class="strong"><strong>-0.02525998879466241   -0.014602750644394844  ...</strong></span>
<span class="strong"><strong>-0.025494722450369593  -0.014678013626511024  ...</strong></span>
<span class="strong"><strong>-0.02604194423255582   -0.01439561589951032   ...</strong></span>
<span class="strong"><strong>-0.025942214214865228  -0.013907665261197633  ...</strong></span>
<span class="strong"><strong>-0.026151551334429365  -0.014707035797934148  ...</strong></span>
<span class="strong"><strong>-0.026106572186134578  -0.016701471378568943  ...</strong></span>
<span class="strong"><strong>-0.026242986173995755  -0.016254664123732318  ...</strong></span>
<span class="strong"><strong>-0.02573628754284022   -0.017185663918352894  ...</strong></span>
<span class="strong"><strong>-0.02545319635905169   -0.01653357295561698   ...</strong></span>
<span class="strong"><strong>-0.025325893980995124  -0.0157082218373399...</strong></span>
</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl3sec36"></a>Visualizing the Eigenfaces</h4></div></div></div><p>Now<a id="id792" class="indexterm"></a> that we have trained our PCA model, what is<a id="id793" class="indexterm"></a> the result? Let's inspect the dimensions of the resulting matrix:</p><div class="informalexample"><pre class="programlisting">val rows = pc.numRows
val cols = pc.numCols
println(rows, cols)</pre></div><p>As you should see from your console output, the matrix of principal components has 2500 rows and 10 columns:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>(2500,10)</strong></span>
</pre></div><p>Recall that the dimension of each image is 50 x 50, so here, we have the top 10 principal components, each with a dimension identical to that of the input images. These principal components can be thought of as the set of latent (or hidden) features that capture the greatest variation in the original data.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note44"></a>Note</h3><p>In facial recognition and image processing, these principal components are often referred to <a id="id794" class="indexterm"></a>as <span class="strong"><strong>Eigenfaces</strong></span>, as PCA is closely related to the eigenvalue <a id="id795" class="indexterm"></a>decomposition of the covariance matrix of the original data.</p><p>See <a class="ulink" href="http://en.wikipedia.org/wiki/Eigenface" target="_blank">http://en.wikipedia.org/wiki/Eigenface</a> for more details.</p></div><p>Since each principal component is of the same dimension as the original images, each component can itself be thought of and represented as an image, making it possible to visualize the Eigenfaces as we would the input images.</p><p>As we have often done in this book, we will use functionality from the Breeze linear algebra library as well as Python's numpy and matplotlib to visualize the Eigenfaces.</p><p>First, we will extract the <code class="literal">pc</code> variable (an MLlib matrix) into a Breeze <code class="literal">DenseMatrix</code>:</p><div class="informalexample"><pre class="programlisting">import breeze.linalg.DenseMatrix
val pcBreeze = new DenseMatrix(rows, cols, pc.toArray)</pre></div><p>Breeze<a id="id796" class="indexterm"></a> provides a useful function within the <code class="literal">linalg</code> package to write the matrix out as a CSV file. We will use this to save the principal <a id="id797" class="indexterm"></a>components to a temporary CSV file:</p><div class="informalexample"><pre class="programlisting">import breeze.linalg.csvwrite
csvwrite(new File("/tmp/pc.csv"), pcBreeze)</pre></div><p>Next, we will load the matrix in IPython and visualize the principal components as images. Fortunately, numpy provides a utility function to read the matrix from the CSV file we created:</p><div class="informalexample"><pre class="programlisting">pcs = np.loadtxt("/tmp/pc.csv", delimiter=",")
print(pcs.shape)</pre></div><p>You should see the following output, confirming that the matrix we read has the same dimensions as the one we saved:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>(2500, 10)</strong></span>
</pre></div><p>We will need a utility function to display the images, which we define here:</p><div class="informalexample"><pre class="programlisting">def plot_gallery(images, h, w, n_row=2, n_col=5):
    """Helper function to plot a gallery of portraits"""
    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))
    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)
    for i in range(n_row * n_col):
        plt.subplot(n_row, n_col, i + 1)
        plt.imshow(images[:, i].reshape((h, w)), cmap=plt.cm.gray)
        plt.title("Eigenface %d" % (i + 1), size=12)
        plt.xticks(())
        plt.yticks(())</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note45"></a>Note</h3><p>This function is adapted from the LFW dataset example code in the scikit-learn documentation available at <a class="ulink" href="http://scikit-learn.org/stable/auto_examples/applications/face_recognition.html" target="_blank">http://scikit-learn.org/stable/auto_examples/applications/face_recognition.html</a>.</p></div><p>We will<a id="id798" class="indexterm"></a> now use this function to plot the top 10 Eigenfaces:</p><div class="informalexample"><pre class="programlisting">plot_gallery(pcs, 50, 50)</pre></div><p>This<a id="id799" class="indexterm"></a> should display the following plot:</p><div class="mediaobject"><img src="graphics/8519OS_08_04.jpg" /><div class="caption"><p>Top 10 Eigenfaces</p></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl3sec37"></a>Interpreting the Eigenfaces</h4></div></div></div><p>Looking <a id="id800" class="indexterm"></a>at the preceding images, we can see that <a id="id801" class="indexterm"></a>the PCA model has effectively extracted recurring patterns of variation, which represent various features of the facial images. Each principal component can, as with clustering models, be interpreted. Again, like clustering, it is not always straightforward to interpret precisely what each principal component represents.</p><p>We can see from these images that there appear to be some images that pick up directional factors (for example, images 6 and 9), some hone in on hair patterns (such as images 4, 5, 7, and 10), while others seem to be somewhat more related to facial features such as eyes, nose, and mouth (such as images 1, 7, and 9).</p></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec57"></a>Using a dimensionality reduction model</h2></div></div><hr /></div><p>It is <a id="id802" class="indexterm"></a>interesting to be able to visualize the outcome of a model in this way; however, the overall purpose of using dimensionality reduction is to create a more compact representation of the data that still captures the important features and variability in the raw dataset. To do this, we need to use a trained model to transform our raw data by projecting it into the new, lower-dimensional space represented by the principal components.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec83"></a>Projecting data using PCA on the LFW dataset</h3></div></div></div><p>We <a id="id803" class="indexterm"></a>will illustrate this concept<a id="id804" class="indexterm"></a> by projecting each LFW image into a ten-dimensional <a id="id805" class="indexterm"></a>vector. This is done through a matrix multiplication of the image matrix with the matrix of principal components. As the image matrix is a distributed MLlib <code class="literal">RowMatrix</code>, Spark takes care of distributing this computation for us through the <code class="literal">multiply</code> function:</p><div class="informalexample"><pre class="programlisting">val projected = matrix.multiply(pc)
println(projected.numRows, projected.numCols)</pre></div><p>This will give you the following output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>(1055,10)</strong></span>
</pre></div><p>Observe that each image that was of dimension 2500 has been transformed into a vector of size 10. Let's take a look at the first few vectors:</p><div class="informalexample"><pre class="programlisting">println(projected.rows.take(5).mkString("\n"))</pre></div><p>Here is the output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong> [2648.9455749636277,1340.3713412351376,443.67380716760965,-353.0021423043161,52.53102289832631,423.39861446944354,413.8429065865399,-484.18122999722294,87.98862070273545,-104.62720604921965]</strong></span>
<span class="strong"><strong>[172.67735747311974,663.9154866829355,261.0575622447282,-711.4857925259682,462.7663154755333,167.3082231097332,-71.44832640530836,624.4911488194524,892.3209964031695,-528.0056327351435]</strong></span>
<span class="strong"><strong>[-1063.4562028554978,388.3510869550539,1508.2535609357597,361.2485590837186,282.08588829583596,-554.3804376922453,604.6680021092125,-224.16600191143075,-228.0771984153961,-110.21539201855907]</strong></span>
<span class="strong"><strong>[-4690.549692385103,241.83448841252638,-153.58903325799685,-28.26215061165965,521.8908276360171,-442.0430200747375,-490.1602309367725,-456.78026845649435,-78.79837478503592,70.62925170688868]</strong></span>
<span class="strong"><strong>[-2766.7960144161225,612.8408888724891,-405.76374113178616,-468.56458995613974,863.1136863614743,-925.0935452709143,69.24586949009642,-777.3348492244131,504.54033662376435,257.0263568009851]</strong></span>
</pre></div><p>As <a id="id806" class="indexterm"></a>the projected data is in the <a id="id807" class="indexterm"></a>form of vectors, we can use the projection as input to<a id="id808" class="indexterm"></a> another machine learning model. For example, we could use these projected inputs together with a set of input data generated from various images without faces to train a facial recognition model. Alternatively, we could train a multiclass classifier where each person is a class, thus creating a model that learns to identify the particular person that a face belongs to.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec84"></a>The relationship between PCA and SVD</h3></div></div></div><p>We mentioned earlier that<a id="id809" class="indexterm"></a> there is a close relationship between PCA and SVD. In fact, we <a id="id810" class="indexterm"></a>can recover the same principal components <a id="id811" class="indexterm"></a>and also apply the same projection into the space of principal components using SVD.</p><p>In our example, the right singular vectors derived from computing the SVD will be equivalent to the principal components we have calculated. We can see that this is the case by first computing the SVD on our image matrix and comparing the right singular vectors to the result of PCA. As was the case with PCA, SVD computation is provided as a function on a distributed <code class="literal">RowMatrix</code>:</p><div class="informalexample"><pre class="programlisting">val svd = matrix.computeSVD(10, computeU = true)
println(s"U dimension: (${svd.U.numRows}, ${svd.U.numCols})")
println(s"S dimension: (${svd.s.size}, )")
println(s"V dimension: (${svd.V.numRows}, ${svd.V.numCols})")</pre></div><p>We can see that SVD returns a matrix <code class="literal">U</code> of dimension 1055 x 10, a vector <code class="literal">S</code> of the singular values of length <code class="literal">10</code>, and a matrix <code class="literal">V</code> of the right singular vectors of dimension 2500 x 10:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>U dimension: (1055, 10)</strong></span>
<span class="strong"><strong>S dimension: (10, )</strong></span>
<span class="strong"><strong>V dimension: (2500, 10)</strong></span>
</pre></div><p>The matrix <code class="literal">V</code> is exactly equivalent to the result of PCA (ignoring the sign of the values and floating point tolerance). We can verify this with a utility function to compare the two by approximately<a id="id812" class="indexterm"></a> comparing<a id="id813" class="indexterm"></a> the data arrays of each matrix:</p><div class="informalexample"><pre class="programlisting">def approxEqual(array1: Array[Double], array2: Array[Double], tolerance: Double = 1e-6): Boolean = {
  // note we ignore sign of the principal component / singular vector elements
  val bools = array1.zip(array2).map { case (v1, v2) =&gt; if (math.abs(math.abs(v1) - math.abs(v2)) &gt; 1e-6) false else true }
  bools.fold(true)(_ &amp; _)
} </pre></div><p>We <a id="id814" class="indexterm"></a>will test the function on some test data:</p><div class="informalexample"><pre class="programlisting">println(approxEqual(Array(1.0, 2.0, 3.0), Array(1.0, 2.0, 3.0)))</pre></div><p>This will give you the following output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>true</strong></span>
</pre></div><p>Let's try another test data:</p><div class="informalexample"><pre class="programlisting">println(approxEqual(Array(1.0, 2.0, 3.0), Array(3.0, 2.0, 1.0)))</pre></div><p>This will give you the following output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>false</strong></span>
</pre></div><p>Finally, we can apply our equality function as follows:</p><div class="informalexample"><pre class="programlisting">println(approxEqual(svd.V.toArray, pc.toArray))</pre></div><p>Here is the output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>true</strong></span>
</pre></div><p>The other relationship that holds is that the multiplication of the matrix <code class="literal">U</code> and vector <code class="literal">S</code> (or, strictly speaking, the diagonal matrix <code class="literal">S</code>) is equivalent to the PCA projection of our original image data into the space of the top 10 principal components.</p><p>We will now show that this is indeed the case. We will first use Breeze to multiply each vector in <code class="literal">U</code> by <code class="literal">S</code>, element-wise. We will then compare each vector in our PCA projected vectors <a id="id815" class="indexterm"></a>with the equivalent vector in our SVD projection, and sum up the number of equal cases:</p><div class="informalexample"><pre class="programlisting">val breezeS = breeze.linalg.DenseVector(svd.s.toArray)
val projectedSVD = svd.U.rows.map { v =&gt; 
  val breezeV = breeze.linalg.DenseVector(v.toArray)
  val multV = breezeV <span class="strong"><strong>:*</strong></span> breezeS
  Vectors.dense(multV.data)
}
projected.rows.zip(projectedSVD).map { case (v1, v2) =&gt; approxEqual(v1.toArray, v2.toArray) }.filter(b =&gt; true).count</pre></div><p>This should display a <a id="id816" class="indexterm"></a>result<a id="id817" class="indexterm"></a> of 1055, as we would expect, confirming that each row of <code class="literal">projected</code> is equal to each row of <code class="literal">projectedSVD</code>.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note46"></a>Note</h3><p>Note that the <span class="strong"><strong>:*</strong></span> operator highlighted in the preceding code represents element-wise multiplication of the vectors.</p></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec58"></a>Evaluating dimensionality reduction models</h2></div></div><hr /></div><p>Both<a id="id818" class="indexterm"></a> PCA and SVD are deterministic models. That is, given a certain input dataset, they will always produce the same result. This is in contrast to many of the models we have seen so far, which depend on some random element (most often for the initialization of model weight vectors and so on).</p><p>Both models are also guaranteed to return the top principal components or singular values, and hence, the only parameter is <span class="emphasis"><em>k</em></span>. Like clustering models, increasing <span class="emphasis"><em>k</em></span> always improves the model performance (for clustering, the relevant error function, while for PCA and SVD, the total amount of variability explained by the <span class="emphasis"><em>k</em></span> components). Therefore, selecting a value for <span class="emphasis"><em>k</em></span> is a trade-off between capturing as much structure of the data as possible while keeping the dimensionality of projected data low.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl2sec85"></a>Evaluating k for SVD on the LFW dataset</h3></div></div></div><p>We will <a id="id819" class="indexterm"></a>examine the singular <a id="id820" class="indexterm"></a>values <a id="id821" class="indexterm"></a>obtained from computing the SVD on our image data. We can verify that the singular values are the same for each run and that they are returned in decreasing order, as follows:</p><div class="informalexample"><pre class="programlisting">val sValues = (1 to 5).map { i =&gt; matrix.computeSVD(i, computeU = false).s }
sValues.foreach(println)</pre></div><p>This should show us output similar to the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[54091.00997110354]</strong></span>
<span class="strong"><strong>[54091.00997110358,33757.702867982436]</strong></span>
<span class="strong"><strong>[54091.00997110357,33757.70286798241,24541.193694775946]</strong></span>
<span class="strong"><strong>[54091.00997110358,33757.70286798242,24541.19369477593,23309.58418888302]</strong></span>
<span class="strong"><strong>[54091.00997110358,33757.70286798242,24541.19369477593,23309.584188882982,21803.09841158358]</strong></span>
</pre></div><p>As with evaluating values of <span class="emphasis"><em>k</em></span> for clustering, in the case of SVD (and PCA), it is often useful to plot the singular values for a larger range of <span class="emphasis"><em>k</em></span> and see where the point on the graph is where the amount of additional variance accounted for by each additional singular value starts to flatten out considerably.</p><p>We will do this by first computing the top 300 singular values:</p><div class="informalexample"><pre class="programlisting">val svd300 = matrix.computeSVD(300, computeU = false)
val sMatrix = new DenseMatrix(1, 300, svd300.s.toArray)
csvwrite(new File("/tmp/s.csv"), sMatrix) </pre></div><p>We will write out the vector <code class="literal">S</code> of singular values to a temporary CSV file (as we did for our matrix of Eigenfaces previously) and then read it back in our IPython console, plotting the singular values for each <span class="emphasis"><em>k</em></span>:</p><div class="informalexample"><pre class="programlisting">s = np.loadtxt("/tmp/s.csv", delimiter=",")
print(s.shape)
plot(s)</pre></div><p>You should see an image displayed similar to the one shown here:</p><div class="mediaobject"><img src="graphics/8519OS_08_05.jpg" /><div class="caption"><p>Top 300 singular values</p></div></div><p>A <a id="id822" class="indexterm"></a>similar pattern is seen in the <a id="id823" class="indexterm"></a>cumulative variation accounted for by the<a id="id824" class="indexterm"></a> top 300 singular values (which we will plot on a log scale for the <span class="emphasis"><em>y</em></span> axis):</p><div class="informalexample"><pre class="programlisting">plot(cumsum(s))
plt.yscale('log')</pre></div><div class="mediaobject"><img src="graphics/8519OS_08_06.jpg" /><div class="caption"><p>Cumulative sum of top 300 singular values</p></div></div><p>We <a id="id825" class="indexterm"></a>can see that after a certain <a id="id826" class="indexterm"></a>value range for <span class="emphasis"><em>k</em></span> (around 100 in this case), the <a id="id827" class="indexterm"></a>graph flattens considerably. This indicates that a number of singular values (or principal components) equivalent to this value of <span class="emphasis"><em>k</em></span> probably explains enough of the variation of the original data.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip51"></a>Tip</h3><p>Of course, if we are using dimensionality reduction to help improve the performance of another model, we could use the same evaluation methods used for that model to help us choose a value for <span class="emphasis"><em>k</em></span>.</p><p>For example, we could use the AUC metric, together with cross-validation, to choose both the model parameters for a classification model as well as the value of <span class="emphasis"><em>k</em></span> for our dimensionality reduction model. This does come at the expense of higher computation cost, however, as we would have to recompute the full model training and testing pipeline.</p></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch08lvl1sec59"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we explored two new unsupervised learning methods, PCA and SVD, for dimensionality reduction. We saw how to extract features for and train these models using facial image data. We visualized the results of the model in the form of Eigenfaces, saw how to apply the models to transform our original data into a reduced dimensionality representation, and investigated the close link between PCA and SVD.</p><p>In the next chapter, we will delve more deeply into techniques for text processing and analysis with Spark.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch09"></a>ChapterÂ 9.Â Advanced Text Processing with Spark</h2></div></div></div><p>In <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Obtaining, Processing, and Preparing Data with Spark</em></span>, we covered various topics related to feature extraction and data processing, including the basics of extracting features from text data. In this chapter, we will introduce more advanced text processing techniques available in MLlib to work with large-scale text datasets.</p><p>In this chapter, we will:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Work through detailed examples that illustrate data processing, feature extraction, and the modeling pipeline, as they relate to text data</p></li><li style="list-style-type: disc"><p>Evaluate the similarity between two documents based on the words in the documents</p></li><li style="list-style-type: disc"><p>Use the extracted text features as inputs for a classification model</p></li><li style="list-style-type: disc"><p>Cover a recent development in natural language processing to model words themselves <a id="id828" class="indexterm"></a>as vectors and illustrate the use of Spark's <span class="strong"><strong>Word2Vec</strong></span> model to evaluate the similarity between two words, based on their meaning</p></li></ul></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec60"></a>What's so special about text data?</h2></div></div><hr /></div><p>Text data <a id="id829" class="indexterm"></a>can be complex to work with for two main reasons. First, text and language have an inherent structure that is not easily captured using the raw words as is (for example, meaning, context, different types of words, sentence structure, and different languages, to highlight a few). Therefore, naÃ¯ve feature extraction is usually relatively ineffective.</p><p>Second, the effective dimensionality of text data is extremely large and potentially limitless. Think about the number of words in the English language alone and add all kinds of special words, characters, slang, and so on to this. Then, throw in other languages and all the types of text one might find across the Internet. The dimension of text data can easily exceed tens or even hundreds of millions of words, even in relatively small datasets. For example, the Common Crawl dataset of billions of websites contains over 840 billion individual words.</p><p>To deal <a id="id830" class="indexterm"></a>with these issues, we need ways of extracting more structured features and methods to handle the huge dimensionality of text data.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec61"></a>Extracting the right features from your data</h2></div></div><hr /></div><p>The <a id="id831" class="indexterm"></a>field of <span class="strong"><strong>natural language processing</strong></span> (<span class="strong"><strong>NLP</strong></span>) covers a <a id="id832" class="indexterm"></a>wide range of techniques to work with text, from text processing and feature extraction through to modeling and machine learning. In this chapter, we will focus on two feature extraction techniques available within MLlib: the TF-IDF term weighting scheme and feature hashing.</p><p>Working through an example of TF-IDF, we will also explore the ways in which processing, tokenization, and filtering during feature extraction can help reduce the dimensionality of our input data as well as improve the information content and usefulness of the features we extract.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec86"></a>Term weighting schemes</h3></div></div></div><p>In <a class="link" href="#" linkend="ch03">Chapter 3</a>, <span class="emphasis"><em>Obtaining, Processing, and Preparing Data with Spark</em></span>, we looked at vector representation <a id="id833" class="indexterm"></a>where text features<a id="id834" class="indexterm"></a> are mapped to a simple binary vector called the <span class="strong"><strong>bag-of-words</strong></span> model. Another<a id="id835" class="indexterm"></a> representation used commonly<a id="id836" class="indexterm"></a> in practice is called <span class="strong"><strong>term frequency-inverse document frequency</strong></span> (<span class="strong"><strong>TF-IDF</strong></span>).</p><p>TF-IDF weights each term in a piece of text (referred to as a <span class="strong"><strong>document</strong></span>) based on its frequency in <a id="id837" class="indexterm"></a>the document (the <span class="strong"><strong>term frequency</strong></span>). A global normalization, called the <span class="strong"><strong>inverse document frequency</strong></span>, is then applied based on the frequency <a id="id838" class="indexterm"></a>of this term among all documents (the set <a id="id839" class="indexterm"></a>of documents in a dataset is commonly referred to as a <span class="strong"><strong>corpus</strong></span>). The standard definition of TF-IDF is shown here:</p><div class="informalexample"><pre class="programlisting">tf-idf(t,d) = tf(t,d) x idf(t)</pre></div><p>Here, <span class="emphasis"><em>tf(t,d)</em></span> is the frequency (number of occurrences) of term <span class="emphasis"><em>t</em></span> in document <span class="emphasis"><em>d</em></span> and <span class="emphasis"><em>idf(t)</em></span> is the inverse document frequency of term <span class="emphasis"><em>t</em></span> in the corpus; this is defined as follows:</p><div class="informalexample"><pre class="programlisting">idf(t) = log(N / d)</pre></div><p>Here, <span class="emphasis"><em>N</em></span> is the total number of documents, and <span class="emphasis"><em>d</em></span> is the number of documents in which the term <span class="emphasis"><em>t</em></span> occurs.</p><p>The TF-IDF formulation means that terms occurring many times in a document receive a higher weighting in the vector representation relative to those that occur few times in the document. However, the IDF normalization has the effect of reducing the weight of terms<a id="id840" class="indexterm"></a> that are very common<a id="id841" class="indexterm"></a> across all documents. The end result is that truly rare or important terms should be assigned higher weighting, while more common terms (which are assumed to have less importance) should have less impact in terms of weighting.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note47"></a>Note</h3><p>A <a id="id842" class="indexterm"></a>good resource to learn more about the bag-of-words model (or <span class="strong"><strong>vector space model</strong></span>) is the book <span class="emphasis"><em>Introduction to Information Retrieval</em></span>, <span class="emphasis"><em>Christopher D. Manning, Prabhakar Raghavan and Hinrich SchÃ¼tze</em></span>, <span class="emphasis"><em>Cambridge University Press</em></span> (available in HTML form at <a class="ulink" href="http://nlp.stanford.edu/IR-book/html/htmledition/irbook.html" target="_blank">http://nlp.stanford.edu/IR-book/html/htmledition/irbook.html</a>).</p><p>It contains sections on text processing techniques, including tokenization, stop word removal, stemming, and the vector space model, as well as weighting schemes such as TF-IDF.</p><p>An overview can also be found at <a class="ulink" href="http://en.wikipedia.org/wiki/Tf%E2%80%93idf" target="_blank">http://en.wikipedia.org/wiki/Tf%E2%80%93idf</a>.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec87"></a>Feature hashing</h3></div></div></div><p><span class="strong"><strong>Feature hashing</strong></span> is a technique to deal with high-dimensional data and is often used with text <a id="id843" class="indexterm"></a>and categorical datasets where the features<a id="id844" class="indexterm"></a> can take on many unique values (often many millions of values). In the previous chapters, we often used the <span class="emphasis"><em>1-of-K</em></span> encoding approach for categorical features, including text. While this approach is simple and effective, it can break down in the face of extremely high-dimensional data.</p><p>Building and using <span class="emphasis"><em>1-of-K</em></span> feature encoding requires us to keep a mapping of each possible feature value to an index in a vector. Furthermore, the process of creating the mapping itself requires at least one additional pass through the dataset and can be tricky to do in parallel scenarios. Up until now, we have often used a simple approach of collecting the distinct feature values and zipping this collection with a set of indices to create a map of feature value to index. This mapping is then broadcast (either explicitly in our code or implicitly by Spark) to each worker.</p><p>However, when dealing with huge feature dimensions in the tens of millions or more that are common when working with text, this approach can be slow and can require significant memory and network resources, both on the Spark master (to collect the unique values) and workers (to broadcast the resulting mapping to each worker, which keeps it in memory to allow it to apply the feature encoding to its local piece of the input data).</p><p>Feature<a id="id845" class="indexterm"></a> hashing works by assigning the vector<a id="id846" class="indexterm"></a> index for a feature based on the value obtained by hashing this feature to a number (usually, an integer value) using a hash function. For example, let's say the hash value of a categorical feature for the geolocation of <code class="literal">United States</code> is <code class="literal">342</code>. We will use the hashed value as the vector index, and the value at this index will be <code class="literal">1.0</code> to indicate the presence of the <code class="literal">United States</code> feature. The hash function used must be consistent (that is, for a given input, it returns the same output each time).</p><p>This encoding works the same way as mapping-based encoding, except that we choose a size for our feature vector upfront. As the most common hash functions return values in the entire range of integers, we will use a <span class="emphasis"><em>modulo</em></span> operation to restrict the index values to the size of our vector, which is typically much smaller (a few tens of thousands to a few million, depending on our requirements).</p><p>Feature hashing has the advantage that we do not need to build a mapping and keep it in memory. It is also easy to implement, very fast, and can be done online and in real time, thus not requiring a pass through our dataset first. Finally, because we selected a feature vector dimension that is significantly smaller than the raw dimensionality of our dataset, we bound the memory usage of our model both in training and production; hence, memory usage does not scale with the size and dimensionality of our data.</p><p>However, there are two important drawbacks, which are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>As we don't create a mapping of features to index values, we also cannot do the reverse mapping of feature index to value. This makes it harder to, for example, determine which features are most informative in our models.</p></li><li style="list-style-type: disc"><p>As we are <a id="id847" class="indexterm"></a>restricting the size of our feature vectors, we might experience <span class="strong"><strong>hash collisions</strong></span>. This happens when two different features are hashed into the same index in our feature vector. Surprisingly, this doesn't seem to have a severe impact on model performance as long as we choose a reasonable feature vector dimension relative to the dimension of the input data.</p></li></ul></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note48"></a>Note</h3><p>Further information on hashing can be found at <a class="ulink" href="http://en.wikipedia.org/wiki/Hash_function" target="_blank">http://en.wikipedia.org/wiki/Hash_function</a>.</p><p>A key paper that introduced the use of hashing for feature extraction and machine learning is:</p><p><span class="emphasis"><em>Kilian Weinberger</em></span>, <span class="emphasis"><em>Anirban Dasgupta</em></span>, <span class="emphasis"><em>John Langford</em></span>, <span class="emphasis"><em>Alex Smola</em></span>, and <span class="emphasis"><em>Josh Attenberg</em></span>. <span class="emphasis"><em>Feature Hashing for Large Scale Multitask Learning</em></span>. <span class="emphasis"><em>Proc. ICML 2009</em></span>, which is available at <a class="ulink" href="http://alex.smola.org/papers/2009/Weinbergeretal09.pdf" target="_blank">http://alex.smola.org/papers/2009/Weinbergeretal09.pdf</a>.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec88"></a>Extracting the TF-IDF features from the 20 Newsgroups dataset</h3></div></div></div><p>To<a id="id848" class="indexterm"></a> illustrate the concepts in this chapter, we will use a well-known text dataset called <span class="strong"><strong>20 Newsgroups</strong></span>; this dataset is commonly used for text-classification tasks. This is a <a id="id849" class="indexterm"></a>collection of newsgroup <a id="id850" class="indexterm"></a>messages posted across 20 different topics. There are various forms of data available. For our purposes, we <a id="id851" class="indexterm"></a>will use the <code class="literal">bydate</code> version of the dataset, which is<a id="id852" class="indexterm"></a> available at <a class="ulink" href="http://qwone.com/~jason/20Newsgroups" target="_blank">http://qwone.com/~jason/20Newsgroups</a>.</p><p>This dataset splits up the available data into training and test sets that comprise 60 percent and 40 percent of the original data, respectively. Here, the messages in the test set occur after those in the training set. This dataset also excludes some of the message headers that identify the actual newsgroup; hence, it is an appropriate dataset to test the real-world performance of classification models.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note49"></a>Note</h3><p>Further information on the original dataset can be found in the <span class="emphasis"><em>UCI Machine Learning Repository</em></span> page at <a class="ulink" href="http://kdd.ics.uci.edu/databases/20newsgroups/20newsgroups.data.html" target="_blank">http://kdd.ics.uci.edu/databases/20newsgroups/20newsgroups.data.html</a>.</p></div><p>To get started, download the data and unzip the file using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;tar xfvz 20news-bydate.tar.gz</strong></span>
</pre></div><p>This will create two folders: one called <code class="literal">20news-bydate-train</code> and another one called <code class="literal">20news-bydate-test</code>. Let's take a look at the directory structure under the training dataset folder:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;cd 20news-bydate-train/</strong></span>
<span class="strong"><strong>&gt;ls</strong></span>
</pre></div><p>You will see that it contains a number of subfolders, one for each newsgroup:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>alt.atheism              comp.windows.x           rec.sport.hockey         soc.religion.christian</strong></span>
<span class="strong"><strong>comp.graphics            misc.forsale             sci.crypt                talk.politics.guns</strong></span>
<span class="strong"><strong>comp.os.ms-windows.misc  rec.autos                sci.electronics          talk.politics.mideast</strong></span>
<span class="strong"><strong>comp.sys.ibm.pc.hardware rec.motorcycles          sci.med                  talk.politics.misc</strong></span>
<span class="strong"><strong>comp.sys.mac.hardware    rec.sport.baseball       sci.space                talk.religion.misc</strong></span>
</pre></div><p>There<a id="id853" class="indexterm"></a> are a number of files under each newsgroup folder; each file contains<a id="id854" class="indexterm"></a> an individual message <a id="id855" class="indexterm"></a>posting:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; ls rec.sport.hockey</strong></span>
<span class="strong"><strong>52550 52580 52610 52640 53468 53550 53580 53610 53640 53670 53700 53731 53761 53791</strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><p>We can take a look at a part of one of these messages to see the format:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; head -20 rec.sport.hockey/52550</strong></span>
<span class="strong"><strong>From: dchhabra@stpl.ists.ca (Deepak Chhabra)</strong></span>
<span class="strong"><strong>Subject: Superstars and attendance (was Teemu Selanne, was +/- leaders)</strong></span>
<span class="strong"><strong>Nntp-Posting-Host: stpl.ists.ca</strong></span>
<span class="strong"><strong>Organization: Solar Terresterial Physics Laboratory, ISTS</strong></span>
<span class="strong"><strong>Distribution: na</strong></span>
<span class="strong"><strong>Lines: 115</strong></span>


<span class="strong"><strong>Dean J. Falcione (posting from jrmst+8@pitt.edu) writes:</strong></span>
<span class="strong"><strong>[I wrote:]</strong></span>

<span class="strong"><strong>&gt;&gt;When the Pens got Mario, granted there was big publicity, etc, etc,</strong></span>
<span class="strong"><strong>&gt;&gt;and interest was immediately generated.  Gretzky did the same thing for LA.</strong></span>
<span class="strong"><strong>&gt;&gt;However, imnsho, neither team would have seen a marked improvement in</strong></span>
<span class="strong"><strong>&gt;&gt;attendance if the team record did not improve.  In the year before Lemieux</strong></span>
<span class="strong"><strong>&gt;&gt;came, Pittsburgh finished with 38 points.  Following his arrival, the Pens</strong></span>
<span class="strong"><strong>&gt;&gt;finished with 53, 76, 72, 81, 87, 72, 88, and 87 points, with a couple of</strong></span>
<span class="strong"><strong>                          ^^</strong></span>
<span class="strong"><strong>&gt;&gt;Stanley Cups thrown in.</strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><p>As we <a id="id856" class="indexterm"></a>can see, each<a id="id857" class="indexterm"></a> message contains some header fields that contain the sender, subject, and <a id="id858" class="indexterm"></a>other metadata, followed by the raw content of the message.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl3sec38"></a>Exploring the 20 Newsgroups data</h4></div></div></div><p>Now, we <a id="id859" class="indexterm"></a>will start up our Spark Scala console, ensuring that we make enough memory available:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;./SPARK_HOME/bin/spark-shell --driver-memory 4g</strong></span>
</pre></div><p>Looking at the directory structure, you might recognize that once again, we have data contained in individual text files (one text file per message). Therefore, we will again use Spark's <code class="literal">wholeTextFiles</code> method to read the content of each file into a record in our RDD.</p><p>In the code that follows, <code class="literal">PATH</code> refers to the directory in which you extracted the <code class="literal">20news-bydate</code> ZIP file:</p><div class="informalexample"><pre class="programlisting">val path = "/PATH/20news-bydate-train/*"
val rdd = sc.wholeTextFiles(path)
val text = rdd.map { case (file, text) =&gt; text }
println(text.count)</pre></div><p>The first time you run this command, it might take quite a bit of time, as Spark needs to scan the directory structure. You will also see quite a lot of console output, as Spark logs all the file paths that are being processed. During the processing, you will see the following line displayed, indicating the total number of files that Spark has detected:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>14/10/12 14:27:54 INFO FileInputFormat: Total input paths to process : 11314</strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><p>After the command has finished running, you will see the total record count, which should be the same as the preceding <span class="strong"><strong>Total input paths to process</strong></span> screen output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>11314</strong></span>
</pre></div><p>Next, we will take a look at the newsgroup topics available:</p><div class="informalexample"><pre class="programlisting">val newsgroups = rdd.map { case (file, text) =&gt; file.split("/").takeRight(2).head }
val countByGroup = newsgroups.map(n =&gt; (n, 1)).reduceByKey(_ + _).collect.sortBy(-_._2).mkString("\n")
println(countByGroup)</pre></div><p>This will<a id="id860" class="indexterm"></a> display the following result:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>(rec.sport.hockey,600)</strong></span>
<span class="strong"><strong>(soc.religion.christian,599)</strong></span>
<span class="strong"><strong>(rec.motorcycles,598)</strong></span>
<span class="strong"><strong>(rec.sport.baseball,597)</strong></span>
<span class="strong"><strong>(sci.crypt,595)</strong></span>
<span class="strong"><strong>(rec.autos,594)</strong></span>
<span class="strong"><strong>(sci.med,594)</strong></span>
<span class="strong"><strong>(comp.windows.x,593)</strong></span>
<span class="strong"><strong>(sci.space,593)</strong></span>
<span class="strong"><strong>(sci.electronics,591)</strong></span>
<span class="strong"><strong>(comp.os.ms-windows.misc,591)</strong></span>
<span class="strong"><strong>(comp.sys.ibm.pc.hardware,590)</strong></span>
<span class="strong"><strong>(misc.forsale,585)</strong></span>
<span class="strong"><strong>(comp.graphics,584)</strong></span>
<span class="strong"><strong>(comp.sys.mac.hardware,578)</strong></span>
<span class="strong"><strong>(talk.politics.mideast,564)</strong></span>
<span class="strong"><strong>(talk.politics.guns,546)</strong></span>
<span class="strong"><strong>(alt.atheism,480)</strong></span>
<span class="strong"><strong>(talk.politics.misc,465)</strong></span>
<span class="strong"><strong>(talk.religion.misc,377)</strong></span>
</pre></div><p>We can see that the number of messages is roughly even between the topics.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl3sec39"></a>Applying basic tokenization</h4></div></div></div><p>The first <a id="id861" class="indexterm"></a>step in our text processing pipeline is to split up the raw text content in each document into a collection of terms (also referred to as <span class="strong"><strong>tokens</strong></span>). This is known as <span class="strong"><strong>tokenization</strong></span>. We will start by applying a simple <span class="strong"><strong>whitespace</strong></span> tokenization, together<a id="id862" class="indexterm"></a> with converting each token to lowercase for each document:</p><div class="informalexample"><pre class="programlisting">val text = rdd.map { case (file, text) =&gt; text }
val whiteSpaceSplit = text.flatMap(t =&gt; t.split(" ").map(_.toLowerCase))
println(whiteSpaceSplit.distinct.count)</pre></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip52"></a>Tip</h3><p>In the preceding code, we used the <code class="literal">flatMap</code> function instead of <code class="literal">map</code>, as for now, we want to inspect all the tokens together for exploratory analysis. Later in this chapter, we will apply our tokenization scheme on a per-document basis, so we will use the <code class="literal">map</code> function.</p></div><p>After<a id="id863" class="indexterm"></a> running this code snippet, you will see the total number of unique tokens after applying our tokenization:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>402978</strong></span>
</pre></div><p>As you can see, for even a relatively small set of text, the number of raw tokens (and, therefore, the dimensionality of our feature vectors) can be very high.</p><p>Let's take a look at a randomly selected document:</p><div class="informalexample"><pre class="programlisting">println(whiteSpaceSplit.sample(true, 0.3, 42).take(100).mkString(","))</pre></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip53"></a>Tip</h3><p>Note that we set the third parameter to the <code class="literal">sample</code> function, which is the random seed. We set this function to <code class="literal">42</code> so that we get the same results from each call to <code class="literal">sample</code> so that your results match those in this chapter.</p></div><p>This will display the following result:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>atheist,resources</strong></span>
<span class="strong"><strong>summary:,addresses,,to,atheism</strong></span>
<span class="strong"><strong>keywords:,music,,thu,,11:57:19,11:57:19,gmt</strong></span>
<span class="strong"><strong>distribution:,cambridge.,290</strong></span>

<span class="strong"><strong>archive-name:,atheism/resources</strong></span>
<span class="strong"><strong>alt-atheism-archive-name:,december,,,,,,,,,,,,,,,,,,,,,,addresses,addresses,,,,,,,religion,to:,to:,,p.o.,53701.</strong></span>
<span class="strong"><strong>telephone:,sell,the,,fish,on,their,cars,,with,and,written</strong></span>
<span class="strong"><strong>inside.,3d,plastic,plastic,,evolution,evolution,7119,,,,,san,san,san,mailing,net,who,to,atheist,press</strong></span>

<span class="strong"><strong>aap,various,bible,,and,on.,,,one,book,is:</strong></span>

<span class="strong"><strong>"the,w.p.,american,pp.,,1986.,bible,contains,ball,,based,based,james,of</strong></span>
</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl3sec40"></a>Improving our tokenization</h4></div></div></div><p>The<a id="id864" class="indexterm"></a> preceding simple approach results in a lot of tokens and does not filter out many nonword characters (such as punctuation). Most tokenization schemes will remove these characters. We can do this by splitting each raw document on <span class="strong"><strong>nonword characters</strong></span> using<a id="id865" class="indexterm"></a> a regular expression pattern:</p><div class="informalexample"><pre class="programlisting">val nonWordSplit = text.flatMap(t =&gt; t.split("""\W+""").map(_.toLowerCase))
println(nonWordSplit.distinct.count)</pre></div><p>This reduces the number of unique tokens significantly:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>130126</strong></span>
</pre></div><p>If we inspect the first few tokens, we will see that we have eliminated most of the less useful characters in the text:</p><div class="informalexample"><pre class="programlisting">println(nonWordSplit.distinct.sample(true, 0.3, 42).take(100).mkString(","))</pre></div><p>You will see the following result displayed:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>bone,k29p,w1w3s1,odwyer,dnj33n,bruns,_congressional,mmejv5,mmejv5,artur,125215,entitlements,beleive,1pqd9hinnbmi,</strong></span>
<span class="strong"><strong>jxicaijp,b0vp,underscored,believiing,qsins,1472,urtfi,nauseam,tohc4,kielbasa,ao,wargame,seetex,museum,typeset,pgva4,</strong></span>
<span class="strong"><strong>dcbq,ja_jp,ww4ewa4g,animating,animating,10011100b,10011100b,413,wp3d,wp3d,cannibal,searflame,ets,1qjfnv,6jx,6jx,</strong></span>
<span class="strong"><strong>detergent,yan,aanp,unaskable,9mf,bowdoin,chov,16mb,createwindow,kjznkh,df,classifieds,hour,cfsmo,santiago,santiago,</strong></span>
<span class="strong"><strong>1r1d62,almanac_,almanac_,chq,nowadays,formac,formac,bacteriophage,barking,barking,barking,ipmgocj7b,monger,projector,</strong></span>
<span class="strong"><strong>hama,65e90h8y,homewriter,cl5,1496,zysec,homerific,00ecgillespie,00ecgillespie,mqh0,suspects,steve_mullins,io21087,</strong></span>
<span class="strong"><strong>funded,liberated,canonical,throng,0hnz,exxon,xtappcontext,mcdcup,mcdcup,5seg,biscuits</strong></span>
</pre></div><p>While our nonword pattern to split text works fairly well, we are still left with numbers and tokens that contain numeric characters. In some cases, numbers can be an important part of a corpus. For our purposes, the next step in our pipeline will be to filter out numbers and tokens that are words mixed with numbers.</p><p>We can do this by applying another regular expression pattern and using this to filter out tokens that <span class="emphasis"><em>do not match</em></span> the pattern:</p><div class="informalexample"><pre class="programlisting">val regex = """[^0-9]*""".r
val filterNumbers = nonWordSplit.filter(token =&gt; regex.pattern.matcher(token).matches)
println(filterNumbers.distinct.count)</pre></div><p>This<a id="id866" class="indexterm"></a> further reduces the size of the token set:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>84912</strong></span>
</pre></div><p>Let's take a look at another random sample of the filtered tokens:</p><div class="informalexample"><pre class="programlisting">println(filterNumbers.distinct.sample(true, 0.3, 42).take(100).mkString(","))</pre></div><p>You will see output like the following one:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>reunion,wuair,schwabam,eer,silikian,fuller,sloppiness,crying,crying,beckmans,leymarie,fowl,husky,rlhzrlhz,ignore,</strong></span>
<span class="strong"><strong>loyalists,goofed,arius,isgal,dfuller,neurologists,robin,jxicaijp,majorly,nondiscriminatory,akl,sively,adultery,</strong></span>
<span class="strong"><strong>urtfi,kielbasa,ao,instantaneous,subscriptions,collins,collins,za_,za_,jmckinney,nonmeasurable,nonmeasurable,</strong></span>
<span class="strong"><strong>seetex,kjvar,dcbq,randall_clark,theoreticians,theoreticians,congresswoman,sparcstaton,diccon,nonnemacher,</strong></span>
<span class="strong"><strong>arresed,ets,sganet,internship,bombay,keysym,newsserver,connecters,igpp,aichi,impute,impute,raffle,nixdorf,</strong></span>
<span class="strong"><strong>nixdorf,amazement,butterfield,geosync,geosync,scoliosis,eng,eng,eng,kjznkh,explorers,antisemites,bombardments,</strong></span>
<span class="strong"><strong>abba,caramate,tully,mishandles,wgtn,springer,nkm,nkm,alchoholic,chq,shutdown,bruncati,nowadays,mtearle,eastre,</strong></span>
<span class="strong"><strong>discernible,bacteriophage,paradijs,systematically,rluap,rluap,blown,moderates</strong></span>
</pre></div><p>We can see that we have removed all the numeric characters. This still leaves us with a few strange <span class="emphasis"><em>words</em></span>, but we will not worry about these too much here.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl3sec41"></a>Removing stop words</h4></div></div></div><p><span class="strong"><strong>Stop words</strong></span> refer <a id="id867" class="indexterm"></a>to common words that occur many times across almost all documents in a corpus (and across most corpuses). Examples of typical English stop words include and, but, the, of, and so on. It is a standard practice in text feature extraction to exclude stop words from the extracted tokens.</p><p>When using TF-IDF weighting, the weighting scheme actually takes care of this for us. As stop words have a very low IDF score, they will tend to have very low TF-IDF weightings and thus less importance. In some cases, for information retrieval and search tasks, it might be desirable to include stop words. However, it can still be beneficial to exclude stop words during feature extraction, as it reduces the dimensionality of the final feature vectors as well as the size of the training data.</p><p>We can take a look at some of the tokens in our corpus that have the highest occurrence across all documents to get an idea about some other stop words to exclude:</p><div class="informalexample"><pre class="programlisting">val tokenCounts = filterNumbers.map(t =&gt; (t, 1)).reduceByKey(_ + _)
val oreringDesc = Ordering.by[(String, Int), Int](_._2)
println(tokenCounts.top(20)(oreringDesc).mkString("\n"))</pre></div><p>In the preceding code, we took the tokens after filtering out numeric characters and generated a count of the occurrence of each token across the corpus. We can now use Spark's <code class="literal">top</code> function to retrieve the top 20 tokens by count. Notice that we need to provide the <code class="literal">top</code> function with an ordering that tells Spark how to order the elements of our RDD. In this case, we want to order by the count, so we will specify the second element of our key-value pair.</p><p>Running the preceding code snippet will result in the following top tokens:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>(the,146532)</strong></span>
<span class="strong"><strong>(to,75064)</strong></span>
<span class="strong"><strong>(of,69034)</strong></span>
<span class="strong"><strong>(a,64195)</strong></span>
<span class="strong"><strong>(ax,62406)</strong></span>
<span class="strong"><strong>(and,57957)</strong></span>
<span class="strong"><strong>(i,53036)</strong></span>
<span class="strong"><strong>(in,49402)</strong></span>
<span class="strong"><strong>(is,43480)</strong></span>
<span class="strong"><strong>(that,39264)</strong></span>
<span class="strong"><strong>(it,33638)</strong></span>
<span class="strong"><strong>(for,28600)</strong></span>
<span class="strong"><strong>(you,26682)</strong></span>
<span class="strong"><strong>(from,22670)</strong></span>
<span class="strong"><strong>(s,22337)</strong></span>
<span class="strong"><strong>(edu,21321)</strong></span>
<span class="strong"><strong>(on,20493)</strong></span>
<span class="strong"><strong>(this,20121)</strong></span>
<span class="strong"><strong>(be,19285)</strong></span>
<span class="strong"><strong>(t,18728)</strong></span>
</pre></div><p>As we might expect, there are a lot of common words in this list that we could potentially label as stop words. Let's create a set of stop words with some of these as well as other common <a id="id868" class="indexterm"></a>words. We will then look at the tokens after filtering out these stop words:</p><div class="informalexample"><pre class="programlisting">val stopwords = Set(
  "the","a","an","of","or","in","for","by","on","but", "is", "not", "with", "as", "was", "if",
  "they", "are", "this", "and", "it", "have", "from", "at", "my", "be", "that", "to"
)
val tokenCountsFilteredStopwords = tokenCounts.filter { case (k, v) =&gt; !stopwords.contains(k) }
println(tokenCountsFilteredStopwords.top(20)(oreringDesc).mkString("\n"))</pre></div><p>You will see the following output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>(ax,62406)</strong></span>
<span class="strong"><strong>(i,53036)</strong></span>
<span class="strong"><strong>(you,26682)</strong></span>
<span class="strong"><strong>(s,22337)</strong></span>
<span class="strong"><strong>(edu,21321)</strong></span>
<span class="strong"><strong>(t,18728)</strong></span>
<span class="strong"><strong>(m,12756)</strong></span>
<span class="strong"><strong>(subject,12264)</strong></span>
<span class="strong"><strong>(com,12133)</strong></span>
<span class="strong"><strong>(lines,11835)</strong></span>
<span class="strong"><strong>(can,11355)</strong></span>
<span class="strong"><strong>(organization,11233)</strong></span>
<span class="strong"><strong>(re,10534)</strong></span>
<span class="strong"><strong>(what,9861)</strong></span>
<span class="strong"><strong>(there,9689)</strong></span>
<span class="strong"><strong>(x,9332)</strong></span>
<span class="strong"><strong>(all,9310)</strong></span>
<span class="strong"><strong>(will,9279)</strong></span>
<span class="strong"><strong>(we,9227)</strong></span>
<span class="strong"><strong>(one,9008)</strong></span>
</pre></div><p>You might notice that there are still quite a few common words in this top list. In practice, we might have a much larger set of stop words. However, we will keep a few (partly to illustrate the impact of common words when using TF-IDF weighting a little later).</p><p>One other filtering step that we will use is removing any tokens that are only one character in length. The reasoning behind this is similar to removing stop wordsâ€”these single-character tokens are unlikely to be informative in our text model and can further reduce<a id="id869" class="indexterm"></a> the feature dimension and model size. We will do this with another filtering step:</p><div class="informalexample"><pre class="programlisting">val tokenCountsFilteredSize = tokenCountsFilteredStopwords.filter { case (k, v) =&gt; k.size &gt;= 2 }
println(tokenCountsFilteredSize.top(20)(oreringDesc).mkString("\n"))</pre></div><p>Again, we will examine the tokens remaining after this filtering step:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>(ax,62406)</strong></span>
<span class="strong"><strong>(you,26682)</strong></span>
<span class="strong"><strong>(edu,21321)</strong></span>
<span class="strong"><strong>(subject,12264)</strong></span>
<span class="strong"><strong>(com,12133)</strong></span>
<span class="strong"><strong>(lines,11835)</strong></span>
<span class="strong"><strong>(can,11355)</strong></span>
<span class="strong"><strong>(organization,11233)</strong></span>
<span class="strong"><strong>(re,10534)</strong></span>
<span class="strong"><strong>(what,9861)</strong></span>
<span class="strong"><strong>(there,9689)</strong></span>
<span class="strong"><strong>(all,9310)</strong></span>
<span class="strong"><strong>(will,9279)</strong></span>
<span class="strong"><strong>(we,9227)</strong></span>
<span class="strong"><strong>(one,9008)</strong></span>
<span class="strong"><strong>(would,8905)</strong></span>
<span class="strong"><strong>(do,8674)</strong></span>
<span class="strong"><strong>(he,8441)</strong></span>
<span class="strong"><strong>(about,8336)</strong></span>
<span class="strong"><strong>(writes,7844)</strong></span>
</pre></div><p>Apart from some of the common words that we have not excluded, we see that a few potentially more informative words are starting to appear.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl3sec42"></a>Excluding terms based on frequency</h4></div></div></div><p>It is <a id="id870" class="indexterm"></a>also a common practice to exclude terms during tokenization when their overall occurrence in the corpus is very low. For example, let's examine the least occurring terms in the corpus (notice the different ordering we use here to return the results sorted in ascending order):</p><div class="informalexample"><pre class="programlisting">val oreringAsc = Ordering.by[(String, Int), Int](-_._2)
println(tokenCountsFilteredSize.top(20)(oreringAsc).mkString("\n"))</pre></div><p>You will get the following results:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>(lennips,1)</strong></span>
<span class="strong"><strong>(bluffing,1)</strong></span>
<span class="strong"><strong>(preload,1)</strong></span>
<span class="strong"><strong>(altina,1)</strong></span>
<span class="strong"><strong>(dan_jacobson,1)</strong></span>
<span class="strong"><strong>(vno,1)</strong></span>
<span class="strong"><strong>(actu,1)</strong></span>
<span class="strong"><strong>(donnalyn,1)</strong></span>
<span class="strong"><strong>(ydag,1)</strong></span>
<span class="strong"><strong>(mirosoft,1)</strong></span>
<span class="strong"><strong>(xiconfiywindow,1)</strong></span>
<span class="strong"><strong>(harger,1)</strong></span>
<span class="strong"><strong>(feh,1)</strong></span>
<span class="strong"><strong>(bankruptcies,1)</strong></span>
<span class="strong"><strong>(uncompression,1)</strong></span>
<span class="strong"><strong>(d_nibby,1)</strong></span>
<span class="strong"><strong>(bunuel,1)</strong></span>
<span class="strong"><strong>(odf,1)</strong></span>
<span class="strong"><strong>(swith,1)</strong></span>
<span class="strong"><strong>(lantastic,1)</strong></span>
</pre></div><p>As we can see, there are many terms that only occur once in the entire corpus. Since typically we want to use our extracted features for other tasks such as document similarity or machine learning models, tokens that only occur once are not useful to learn from, as we will not have enough training data relative to these tokens. We can apply another filter to exclude these rare tokens:</p><div class="informalexample"><pre class="programlisting">val rareTokens = tokenCounts.filter{ case (k, v) =&gt; v &lt; 2 }.map { case (k, v) =&gt; k }.collect.toSet
val tokenCountsFilteredAll = tokenCountsFilteredSize.filter { case (k, v) =&gt; !rareTokens.contains(k) }
println(tokenCountsFilteredAll.top(20)(oreringAsc).mkString("\n"))</pre></div><p>We can see that we are left with tokens that occur at least twice in the corpus:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>(sina,2)</strong></span>
<span class="strong"><strong>(akachhy,2)</strong></span>
<span class="strong"><strong>(mvd,2)</strong></span>
<span class="strong"><strong>(hizbolah,2)</strong></span>
<span class="strong"><strong>(wendel_clark,2)</strong></span>
<span class="strong"><strong>(sarkis,2)</strong></span>
<span class="strong"><strong>(purposeful,2)</strong></span>
<span class="strong"><strong>(feagans,2)</strong></span>
<span class="strong"><strong>(wout,2)</strong></span>
<span class="strong"><strong>(uneven,2)</strong></span>
<span class="strong"><strong>(senna,2)</strong></span>
<span class="strong"><strong>(multimeters,2)</strong></span>
<span class="strong"><strong>(bushy,2)</strong></span>
<span class="strong"><strong>(subdivided,2)</strong></span>
<span class="strong"><strong>(coretest,2)</strong></span>
<span class="strong"><strong>(oww,2)</strong></span>
<span class="strong"><strong>(historicity,2)</strong></span>
<span class="strong"><strong>(mmg,2)</strong></span>
<span class="strong"><strong>(margitan,2)</strong></span>
<span class="strong"><strong>(defiance,2)</strong></span>
</pre></div><p>Now, let's<a id="id871" class="indexterm"></a> count the number of unique tokens:</p><div class="informalexample"><pre class="programlisting">println(tokenCountsFilteredAll.count)</pre></div><p>You will see the following output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>51801</strong></span>
</pre></div><p>As we can see, by applying all the filtering steps in our tokenization pipeline, we have reduced the feature dimension from 402,978 to 51,801.</p><p>We can now combine all our filtering logic into one function, which we can apply to each document in our RDD:</p><div class="informalexample"><pre class="programlisting">def tokenize(line: String): Seq[String] = {
  line.split("""\W+""")
    .map(_.toLowerCase)
    .filter(token =&gt; regex.pattern.matcher(token).matches)
    .filterNot(token =&gt; stopwords.contains(token))
    .filterNot(token =&gt; rareTokens.contains(token))
    .filter(token =&gt; token.size &gt;= 2)
    .toSeq
}</pre></div><p>We can check whether this function gives us the same result with the following code snippet:</p><div class="informalexample"><pre class="programlisting">println(text.flatMap(doc =&gt; tokenize(doc)).distinct.count)</pre></div><p>This will output <code class="literal">51801</code>, giving us the same unique token count as our step-by-step pipeline.</p><p>We can<a id="id872" class="indexterm"></a> tokenize each document in our RDD as follows:</p><div class="informalexample"><pre class="programlisting">val tokens = text.map(doc =&gt; tokenize(doc))
println(tokens.first.take(20))</pre></div><p>You will see output similar to the following, showing the first part of the tokenized version of our first document:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>WrappedArray(mathew, mathew, mantis, co, uk, subject, alt, atheism, faq, atheist, resources, summary, books, addresses, music, anything, related, atheism, keywords, faq)</strong></span>
</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl3sec43"></a>A note about stemming</h4></div></div></div><p>A <a id="id873" class="indexterm"></a>common step in text processing and tokenization is <span class="strong"><strong>stemming</strong></span>. This is <a id="id874" class="indexterm"></a>the conversion of whole words to a <span class="strong"><strong>base form</strong></span> (called a <span class="strong"><strong>word stem</strong></span>). For<a id="id875" class="indexterm"></a> example, plurals might be converted to singular (<span class="emphasis"><em>dogs</em></span> becomes <span class="emphasis"><em>dog</em></span>), and forms such as <span class="emphasis"><em>walking</em></span> and <span class="emphasis"><em>walker</em></span> might become <span class="emphasis"><em>walk</em></span>. Stemming can become quite complex and is typically handled with specialized NLP or search engine software (such as NLTK, OpenNLP, and Lucene, for example). We will ignore stemming for the purpose of our example here.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note50"></a>Note</h3><p>A full treatment <a id="id876" class="indexterm"></a>of stemming is beyond the scope of this book. You can find more details at <a class="ulink" href="http://en.wikipedia.org/wiki/Stemming" target="_blank">http://en.wikipedia.org/wiki/Stemming</a>.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl3sec44"></a>Training a TF-IDF model</h4></div></div></div><p>We will<a id="id877" class="indexterm"></a> now use MLlib to transform each document, in the form of processed tokens, into a vector representation. The first step will be to use the <code class="literal">HashingTF</code> implementation, which makes use of feature hashing to map each token in the input text to an index in the vector of term frequencies. Then, we will compute the global IDF and use it to transform the term frequency vectors into TF-IDF vectors.</p><p>For each token, the index will thus be the hash of the token (mapped in turn onto the dimension of the feature vector). The value for each token will be the TF-IDF weighting for that token (that is, the term frequency multiplied by the inverse document frequency).</p><p>First, we will import the classes we need and create our <code class="literal">HashingTF</code> instance, passing in a <code class="literal">dim</code> dimension parameter. While the default feature dimension is 2<sup>20</sup> (or around 1 million), we will choose 2<sup>18</sup> (or around 260,000), since with about 50,000 tokens, we should not experience a significant number of hash collisions, and a smaller dimension will be more memory and processing friendly for illustrative purposes:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark.mllib.linalg.{ SparseVector =&gt; SV }
import org.apache.spark.mllib.feature.HashingTF
import org.apache.spark.mllib.feature.IDF
val dim = math.pow(2, 18).toInt
val hashingTF = new HashingTF(dim)
val tf = hashingTF.transform(tokens)
tf.cache</pre></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip54"></a>Tip</h3><p>Note that we imported MLlib's <code class="literal">SparseVector</code> using an alias of <code class="literal">SV</code>. This is because later, we will use Breeze's <code class="literal">linalg</code> module, which itself also imports <code class="literal">SparseVector</code>. This way, we will avoid namespace collisions.</p></div><p>The <code class="literal">transform</code> function of <code class="literal">HashingTF</code> maps each input document (that is, a sequence of tokens) to an MLlib <code class="literal">Vector</code>. We will also call <code class="literal">cache</code> to pin the data in memory to speed up subsequent operations.</p><p>Let's inspect the first element of our transformed dataset:</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip55"></a>Tip</h3><p>Note that <code class="literal">HashingTF.transform</code> returns an <code class="literal">RDD[Vector]</code>, so we will cast the result returned to an instance of an MLlib <code class="literal">SparseVector</code>.</p><p>The <code class="literal">transform</code> method can also work on an individual document by taking an <code class="literal">Iterable</code> argument (for example, a document as a <code class="literal">Seq[String]</code>). This returns a single vector.</p></div><div class="informalexample"><pre class="programlisting">val v = tf.first.asInstanceOf[SV]
println(v.size)
println(v.values.size)
println(v.values.take(10).toSeq)
println(v.indices.take(10).toSeq)</pre></div><p>You will see the following output displayed:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>262144</strong></span>
<span class="strong"><strong>706</strong></span>
<span class="strong"><strong>WrappedArray(1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1.0)</strong></span>
<span class="strong"><strong>WrappedArray(313, 713, 871, 1202, 1203, 1209, 1795, 1862, 3115, 3166)</strong></span>
</pre></div><p>We can see that the dimension of each sparse vector of term frequencies is 262,144 (or 2<sup>18</sup> as we specified). However, the number on non-zero entries in the vector is only 706. The last two lines of the output show the frequency counts and vector indexes for the first few entries in the vector.</p><p>We will<a id="id878" class="indexterm"></a> now compute the inverse document frequency for each term in the corpus by creating a new <code class="literal">IDF</code> instance and calling <code class="literal">fit</code> with our RDD of term frequency vectors as the input. We will then transform our term frequency vectors to TF-IDF vectors through the <code class="literal">transform</code> function of <code class="literal">IDF</code>:</p><div class="informalexample"><pre class="programlisting">val idf = new IDF().fit(tf)
val tfidf = idf.transform(tf)
val v2 = tfidf.first.asInstanceOf[SV]
println(v2.values.size)
println(v2.values.take(10).toSeq)
println(v2.indices.take(10).toSeq)</pre></div><p>When you examine the first element in the RDD of TF-IDF transformed vectors, you will see output similar to the one shown here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>706</strong></span>
<span class="strong"><strong>WrappedArray(2.3869085659322193, 4.670445463955571, 6.561295835827856, 4.597686109673142,  ...</strong></span>
<span class="strong"><strong>WrappedArray(313, 713, 871, 1202, 1203, 1209, 1795, 1862, 3115, 3166)</strong></span>
</pre></div><p>We can see that the number of non-zero entries hasn't changed (at 706), nor have the vector indices for the terms. What has changed are the values for each term. Earlier, these represented the frequency of each term in the document, but now, the new values represent the frequencies weighted by the <code class="literal">IDF</code>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl3sec45"></a>Analyzing the TF-IDF weightings</h4></div></div></div><p>Next, let's <a id="id879" class="indexterm"></a>investigate the TF-IDF weighting for a few terms to illustrate the impact of the commonality or rarity of a term.</p><p>First, we can compute the minimum and maximum TF-IDF weights across the entire corpus:</p><div class="informalexample"><pre class="programlisting">val minMaxVals = tfidf.map { v =&gt;
  val sv = v.asInstanceOf[SV]
  (sv.values.min, sv.values.max)
}
val globalMinMax = minMaxVals.reduce { case ((min1, max1), (min2, max2)) =&gt;
  (math.min(min1, min2), math.max(max1, max2))
}
println(globalMinMax)</pre></div><p>As we can see, the minimum TF-IDF is zero, while the maximum is significantly larger:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>(0.0,66155.39470409753)</strong></span>
</pre></div><p>We will now explore the TF-IDF weight attached to various terms. In the previous section on stop words, we filtered out many common terms that occur frequently. Recall that we did not remove all such potential stop words. Instead, we kept a few in the corpus so that we could illustrate the impact of applying the TF-IDF weighting scheme on these terms.</p><p>TF-IDF weighting will tend to assign a lower weighting to common terms. To see this, we can compute the TF-IDF representation for a few of the terms that appear in the list of top occurrences that we previously computed, such as <code class="literal">you</code>, <code class="literal">do</code>, and <code class="literal">we</code>:</p><div class="informalexample"><pre class="programlisting">val common = sc.parallelize(Seq(Seq("you", "do", "we")))
val tfCommon = hashingTF.transform(common)
val tfidfCommon = idf.transform(tfCommon)
val commonVector = tfidfCommon.first.asInstanceOf[SV]
println(commonVector.values.toSeq)</pre></div><p>If we form a TF-IDF vector representation of this document, we would see the following values assigned to each term. Note that because of feature hashing, we are not sure exactly which term represents what. However, the values illustrate that the weighting applied to these terms is relatively low:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>WrappedArray(0.9965359935704624, 1.3348773448236835, 0.5457486182039175)</strong></span>
</pre></div><p>Now, let's apply the same transformation to a few less common terms that we might intuitively associate with being more linked to specific topics or concepts:</p><div class="informalexample"><pre class="programlisting">val uncommon = sc.parallelize(Seq(Seq("telescope", "legislation", "investment")))
val tfUncommon = hashingTF.transform(uncommon)
val tfidfUncommon = idf.transform(tfUncommon)
val uncommonVector = tfidfUncommon.first.asInstanceOf[SV]
println(uncommonVector.values.toSeq)</pre></div><p>We can see from the following results that the TF-IDF weightings are indeed significantly <a id="id880" class="indexterm"></a>higher than for the more common terms:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>WrappedArray(5.3265513728351666, 5.308532867332488, 5.483736956357579)</strong></span>
</pre></div></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec62"></a>Using a TF-IDF model</h2></div></div><hr /></div><p>While we <a id="id881" class="indexterm"></a>often refer to training a TF-IDF model, it is actually a feature extraction process or transformation rather than a machine learning model. TF-IDF weighting is often used as a preprocessing step for other models, such as dimensionality reduction, classification, or regression.</p><p>To illustrate the potential uses of TF-IDF weighting, we will explore two examples. The first is using the TF-IDF vectors to compute document similarity, while the second involves training a multilabel classification model with the TF-IDF vectors as input features.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec89"></a>Document similarity with the 20 Newsgroups dataset and TF-IDF features</h3></div></div></div><p>You<a id="id882" class="indexterm"></a> might recall <a id="id883" class="indexterm"></a>from <a class="link" href="#" linkend="ch04">Chapter 4</a>, <span class="emphasis"><em>Building a Recommendation Engine with Spark</em></span>, that the similarity between two vectors can be<a id="id884" class="indexterm"></a> computed using a distance<a id="id885" class="indexterm"></a> metric. The closer two vectors<a id="id886" class="indexterm"></a> are (that is, the lower<a id="id887" class="indexterm"></a> the distance metric), the more similar they are. One such metric that we used to compute similarity between movies is cosine similarity.</p><p>Just like we did for movies, we can also compute the similarity between two documents. Using TF-IDF, we have transformed each document into a vector representation. Hence, we can use the same techniques as we used for movie vectors to compare two documents.</p><p>Intuitively, we might expect two documents to be more similar to each other if they share many terms. Conversely, we might expect two documents to be less similar if they each contain many terms that are different from each other. As we compute cosine similarity by computing a dot product of the two vectors and each vector is made up of the terms in each document, we can see that documents with a high overlap of terms will tend to have a higher cosine similarity.</p><p>Now, we can see TF-IDF at work. We might reasonably expect that even very different documents might contain many overlapping terms that are relatively common (for example, our stop words). However, due to a low TF-IDF weighting, these terms will not have a significant impact on the dot product and, therefore, will not have much impact on the similarity computed.</p><p>For example, we might expect two randomly chosen messages from the <code class="literal">hockey</code> newsgroup to be relatively similar to each other. Let's see if this is the case:</p><div class="informalexample"><pre class="programlisting">val hockeyText = rdd.filter { case (file, text) =&gt; file.contains("hockey") }
val hockeyTF = hockeyText.mapValues(doc =&gt; hashingTF.transform(tokenize(doc)))
val hockeyTfIdf = idf.transform(hockeyTF.map(_._2))</pre></div><p>In <a id="id888" class="indexterm"></a>the preceding <a id="id889" class="indexterm"></a>code, we first<a id="id890" class="indexterm"></a> filtered our raw input RDD <a id="id891" class="indexterm"></a>to keep only the messages within<a id="id892" class="indexterm"></a> the hockey topic. We <a id="id893" class="indexterm"></a>then applied our tokenization and term frequency transformation functions. Note that the <code class="literal">transform</code> method used is the version that works on a single document (in the form of a <code class="literal">Seq[String]</code>) rather than the version that works on an RDD of documents.</p><p>Finally, we applied the <code class="literal">IDF</code> transform (note that we use the same IDF that we have already computed on the whole corpus).</p><p>Once we have our <code class="literal">hockey</code> document vectors, we can select two of these vectors at random and compute the cosine similarity between them (as we did earlier, we will use Breeze for the linear algebra functionality, in particular converting our MLlib vectors to Breeze <code class="literal">SparseVector</code> instances first):</p><div class="informalexample"><pre class="programlisting">import breeze.linalg._
val hockey1 = hockeyTfIdf.sample(true, 0.1, 42).first.asInstanceOf[SV]
val breeze1 = new SparseVector(hockey1.indices, hockey1.values, hockey1.size)
val hockey2 = hockeyTfIdf.sample(true, 0.1, 43).first.asInstanceOf[SV]
val breeze2 = new SparseVector(hockey2.indices, hockey2.values, hockey2.size)
val cosineSim = breeze1.dot(breeze2) / (norm(breeze1) * norm(breeze2))
println(cosineSim)</pre></div><p>We can see that the cosine similarity between the documents is around 0.06:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>0.060250114361164626</strong></span>
</pre></div><p>While this might seem quite low, recall that the effective dimensionality of our features is high due to the large number of unique terms that is typical when dealing with text data. Hence, we can expect that any two documents might have a relatively low overlap of terms even if they are about the same topic, and therefore would have a lower absolute similarity score.</p><p>By<a id="id894" class="indexterm"></a> contrast, we can<a id="id895" class="indexterm"></a> compare this similarity <a id="id896" class="indexterm"></a>score to the one computed<a id="id897" class="indexterm"></a> between one of our <code class="literal">hockey</code> documents<a id="id898" class="indexterm"></a> and another document<a id="id899" class="indexterm"></a> chosen randomly from the <code class="literal">comp.graphics</code> newsgroup, using the same methodology:</p><div class="informalexample"><pre class="programlisting">val graphicsText = rdd.filter { case (file, text) =&gt; file.contains("comp.graphics") }
val graphicsTF = graphicsText.mapValues(doc =&gt; hashingTF.transform(tokenize(doc)))
val graphicsTfIdf = idf.transform(graphicsTF.map(_._2))
val graphics = graphicsTfIdf.sample(true, 0.1, 42).first.asInstanceOf[SV]
val breezeGraphics = new SparseVector(graphics.indices, graphics.values, graphics.size)
val cosineSim2 = breeze1.dot(breezeGraphics) / (norm(breeze1) * norm(breezeGraphics))
println(cosineSim2)</pre></div><p>The cosine similarity is significantly lower at 0.0047:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>0.004664850323792852</strong></span>
</pre></div><p>Finally, it is likely that a document from another sports-related topic might be more similar to our <code class="literal">hockey</code> document than one from a computer-related topic. However, we would probably expect a <code class="literal">baseball</code> document to not be as similar as our <code class="literal">hockey</code> document. Let's see whether this is the case by computing the similarity between a random message from the <code class="literal">baseball</code> newsgroup and our <code class="literal">hockey</code> document:</p><div class="informalexample"><pre class="programlisting">val baseballText = rdd.filter { case (file, text) =&gt; file.contains("baseball") }
val baseballTF = baseballText.mapValues(doc =&gt; hashingTF.transform(tokenize(doc)))
val baseballTfIdf = idf.transform(baseballTF.map(_._2))
val baseball = baseballTfIdf.sample(true, 0.1, 42).first.asInstanceOf[SV]
val breezeBaseball = new SparseVector(baseball.indices, baseball.values, baseball.size)
val cosineSim3 = breeze1.dot(breezeBaseball) / (norm(breeze1) * norm(breezeBaseball))
println(cosineSim3)</pre></div><p>Indeed, as we expected, we found that the <code class="literal">baseball</code> and <code class="literal">hockey</code> documents have a cosine similarity of 0.05, which<a id="id900" class="indexterm"></a> is significantly<a id="id901" class="indexterm"></a> higher<a id="id902" class="indexterm"></a> than the<a id="id903" class="indexterm"></a> <code class="literal">comp.graphics</code> document, but also<a id="id904" class="indexterm"></a> somewhat lower than <a id="id905" class="indexterm"></a>the other <code class="literal">hockey</code> document:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>0.05047395039466008</strong></span>
</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec90"></a>Training a text classifier on the 20 Newsgroups dataset using TF-IDF</h3></div></div></div><p>When <a id="id906" class="indexterm"></a>using TF-IDF<a id="id907" class="indexterm"></a> vectors, we expected that the <a id="id908" class="indexterm"></a>cosine similarity measure would capture <a id="id909" class="indexterm"></a>the similarity between documents, based on the overlap of terms between them. In a similar way, we would expect that a machine learning model, such as a classifier, would be able to learn weightings for individual terms; this would allow it to distinguish between documents from different classes. That is, it should be possible to learn a mapping between the presence (and weighting) of certain terms and a specific topic.</p><p>In the 20 Newsgroups example, each newsgroup topic is a class, and we can train a classifier using our TF-IDF transformed vectors as input.</p><p>Since we are dealing with a multiclass classification problem, we will use the naÃ¯ve Bayes model in MLlib, which supports multiple classes. As the first step, we will import the Spark classes that we will be using:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.classification.NaiveBayes
import org.apache.spark.mllib.evaluation.MulticlassMetrics</pre></div><p>Next, we will need to extract the 20 topics and convert them to class mappings. We can do this in exactly the same way as we might for 1-of-K feature encoding, by assigning a numeric index to each class:</p><div class="informalexample"><pre class="programlisting">val newsgroupsMap = newsgroups.distinct.collect().zipWithIndex.toMap
val zipped = newsgroups.zip(tfidf)
val train = zipped.map { case (topic, vector) =&gt; LabeledPoint(newsgroupsMap(topic), vector) }
train.cache</pre></div><p>In the <a id="id910" class="indexterm"></a>preceding code snippet, we <a id="id911" class="indexterm"></a>took the <code class="literal">newsgroups</code> RDD, where each element <a id="id912" class="indexterm"></a>is the topic, and used the <code class="literal">zip</code> function to combine it with each element in our <code class="literal">tfidf</code> RDD of TF-IDF vectors. We then mapped over each key-value element in our new <code class="literal">zipped</code> RDD and created a <code class="literal">LabeledPoint</code> instance, where <code class="literal">label</code> is the class index and <code class="literal">features</code> is the TF-IDF vector.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip56"></a>Tip</h3><p>Note that the <code class="literal">zip</code> operator assumes that each RDD has the same number of partitions as well as the same number of elements in each partition. It will fail if this is not the case. We can make this assumption here because we have effectively created both our <code class="literal">tfidf</code> RDD and <code class="literal">newsgroups</code> RDD from a series of <code class="literal">map</code> transformations on the same original RDD that preserved the partitioning structure.</p></div><p>Now<a id="id913" class="indexterm"></a> that we have an input RDD in the correct form, we can simply pass it to the naÃ¯ve Bayes <code class="literal">train</code> function:</p><div class="informalexample"><pre class="programlisting">val model = NaiveBayes.train(train, lambda = 0.1)</pre></div><p>Let's evaluate the performance of the model on the test dataset. We will load the raw test data from the <code class="literal">20news-bydate-test</code> directory, again using <code class="literal">wholeTextFiles</code> to read each message into an RDD element. We will then extract the class labels from the file paths in the same way as we did for the <code class="literal">newsgroups</code> RDD:</p><div class="informalexample"><pre class="programlisting">val testPath = "/PATH/20news-bydate-test/*"
val testRDD = sc.wholeTextFiles(testPath)
val testLabels = testRDD.map { case (file, text) =&gt;
  val topic = file.split("/").takeRight(2).head
  newsgroupsMap(topic)
}</pre></div><p>Transforming the text in the test dataset follows the same procedure as for the training dataâ€”we will apply our <code class="literal">tokenize</code> function followed by the term frequency transformation, and we will again use the same IDF computed from the training data to transform <a id="id914" class="indexterm"></a>the TF vectors into TF-IDF <a id="id915" class="indexterm"></a>vectors. Finally, we will zip the test <a id="id916" class="indexterm"></a>class labels with the TF-IDF vectors and create our test <code class="literal">RDD[LabeledPoint]</code>:</p><div class="informalexample"><pre class="programlisting">val testTf = testRDD.map { case (file, text) =&gt; hashingTF.transform(tokenize(text)) }
val testTfIdf = idf.transform(testTf)
val zippedTest = testLabels.zip(testTfIdf)
val test = zippedTest.map { case (topic, vector) =&gt; LabeledPoint(topic, vector) }</pre></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip57"></a>Tip</h3><p>Note that it is important that we use the training set IDF to transform the test data, as this creates a more realistic estimation of model performance on new data, which might potentially contain terms that the model has not yet been trained on. It would be "cheating" to recompute the IDF vector based on the test dataset and, more importantly, would potentially lead to incorrect estimates of optimal model parameters selected through cross-validation.</p></div><p>Now, we're ready to <a id="id917" class="indexterm"></a>compute the predictions and true class labels for our model. We will use this RDD to compute accuracy and the multiclass weighted F-measure for our model:</p><div class="informalexample"><pre class="programlisting">val predictionAndLabel = test.map(p =&gt; (model.predict(p.features), p.label))
val accuracy = 1.0 * predictionAndLabel.filter(x =&gt; x._1 == x._2).count() / test.count()
val metrics = new MulticlassMetrics(predictionAndLabel)
println(accuracy)
println(metrics.weightedFMeasure)</pre></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip58"></a>Tip</h3><p>The weighted F-measure is an overall measure of precision and recall performance (where, like area under an ROC curve, values closer to 1.0 indicate better performance), which is then combined through a weighted averaged across the classes.</p></div><p>We can see that our simple multiclass naÃ¯ve Bayes model has achieved close to 80 percent for both accuracy and F-measure:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>0.7915560276155071</strong></span>
<span class="strong"><strong>0.7810675969031116</strong></span>
</pre></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec63"></a>Evaluating the impact of text processing</h2></div></div><hr /></div><p>Text<a id="id918" class="indexterm"></a> processing and TF-IDF weighting are examples of feature extraction techniques designed to both reduce the dimensionality of and extract some structure from raw text data. We can see the impact of applying these processing techniques by comparing the performance of a model trained on raw text data with one trained on processed and TF-IDF weighted text data.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec91"></a>Comparing raw features with processed TF-IDF features on the 20 Newsgroups dataset</h3></div></div></div><p>In this <a id="id919" class="indexterm"></a>example, we will simply apply the hashing term frequency transformation to the raw text tokens obtained using a simple whitespace splitting of the document text. We will train a model on this data and evaluate the performance on the test set as we did for the model trained with TF-IDF features:</p><div class="informalexample"><pre class="programlisting">val rawTokens = rdd.map { case (file, text) =&gt; text.split(" ") }
val rawTF = texrawTokenst.map(doc =&gt; hashingTF.transform(doc))
val rawTrain = newsgroups.zip(rawTF).map { case (topic, vector) =&gt; LabeledPoint(newsgroupsMap(topic), vector) }
val rawModel = NaiveBayes.train(rawTrain, lambda = 0.1)
val rawTestTF = testRDD.map { case (file, text) =&gt; hashingTF.transform(text.split(" ")) }
val rawZippedTest = testLabels.zip(rawTestTF)
val rawTest = rawZippedTest.map { case (topic, vector) =&gt; LabeledPoint(topic, vector) }
val rawPredictionAndLabel = rawTest.map(p =&gt; (rawModel.predict(p.features), p.label))
val rawAccuracy = 1.0 * rawPredictionAndLabel.filter(x =&gt; x._1 == x._2).count() / rawTest.count()
println(rawAccuracy)
val rawMetrics = new MulticlassMetrics(rawPredictionAndLabel)
println(rawMetrics.weightedFMeasure)</pre></div><p>Perhaps surprisingly, the raw model does quite well, although both accuracy and F-measure <a id="id920" class="indexterm"></a>are a few percentage points lower than those of the TF-IDF model. This is also partly a reflection of the fact that the naÃ¯ve Bayes model is well suited to data in the form of raw frequency counts:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>0.7661975570897503</strong></span>
<span class="strong"><strong>0.7628947184990661</strong></span>
</pre></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec64"></a>Word2Vec models</h2></div></div><hr /></div><p>Until now, we<a id="id921" class="indexterm"></a> have used a bag-of-words vector, optionally with some weighting scheme such as TF-IDF to represent the text in a document. Another recent class of models that has become popular is related to representing individual words as vectors.</p><p>These are generally based in some way on the co-occurrence statistics between the words in a corpus. Once the vector representation is computed, we can use these vectors in ways similar to how we might use TF-IDF vectors (such as using them as features for other machine learning models). One such common use case is computing the similarity between two words with respect to their meanings, based on their vector representations.</p><p>Word2Vec<a id="id922" class="indexterm"></a> refers to a specific implementation of one of these models, often referred to as <span class="strong"><strong>distributed vector representations</strong></span>. The <a id="id923" class="indexterm"></a>MLlib model uses a <span class="strong"><strong>skip-gram</strong></span> model, which seeks to learn vector representations that take into account the contexts in which words occur.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note51"></a>Note</h3><p>While a detailed treatment of Word2Vec is beyond the scope of this book, Spark's documentation at <a class="ulink" href="http://spark.apache.org/docs/latest/mllib-feature-extraction.html#word2vec" target="_blank">http://spark.apache.org/docs/latest/mllib-feature-extraction.html#word2vec</a> contains some further details on the algorithm as well as links to the reference implementation.</p><p>One of the main academic papers underlying Word2Vec is <span class="emphasis"><em>Tomas Mikolov</em></span>, <span class="emphasis"><em>Kai Chen</em></span>, <span class="emphasis"><em>Greg Corrado</em></span>, and <span class="emphasis"><em>Jeffrey Dean</em></span>. <span class="emphasis"><em>Efficient Estimation of Word Representations in Vector Space</em></span>. <span class="emphasis"><em>In Proceedings of Workshop at ICLR</em></span>, <span class="emphasis"><em>2013</em></span>.</p><p>It is available at <a class="ulink" href="http://arxiv.org/pdf/1301.3781.pdf" target="_blank">http://arxiv.org/pdf/1301.3781.pdf</a>.</p><p>Another recent model in the area of word vector representations is GloVe at <a class="ulink" href="http://www-nlp.stanford.edu/projects/glove/" target="_blank">http://www-nlp.stanford.edu/projects/glove/</a>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl2sec92"></a>Word2Vec on the 20 Newsgroups dataset</h3></div></div></div><p>Training<a id="id924" class="indexterm"></a> a Word2Vec model in Spark <a id="id925" class="indexterm"></a>is relatively simple. We will pass in an RDD where each element is a sequence of terms. We can use the RDD of tokenized documents we have already created as input to the model:</p><div class="informalexample"><pre class="programlisting">import org.apache.spark.mllib.feature.Word2Vec
val word2vec = new Word2Vec()
word2vec.setSeed(42)
val word2vecModel = word2vec.fit(tokens)</pre></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip59"></a>Tip</h3><p>Note that we used <code class="literal">setSeed</code> to set the random seed for model training so that you can see the same results each time the model is trained.</p></div><p>You will see some output similar to the following while the model is being trained:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>14/10/25 14:21:59 INFO Word2Vec: wordCount = 2133172, alpha = 0.0011868763094487506</strong></span>
<span class="strong"><strong>14/10/25 14:21:59 INFO Word2Vec: wordCount = 2144172, alpha = 0.0010640806039941193</strong></span>
<span class="strong"><strong>14/10/25 14:21:59 INFO Word2Vec: wordCount = 2155172, alpha = 9.412848985394907E-4</strong></span>
<span class="strong"><strong>14/10/25 14:21:59 INFO Word2Vec: wordCount = 2166172, alpha = 8.184891930848592E-4</strong></span>
<span class="strong"><strong>14/10/25 14:22:00 INFO Word2Vec: wordCount = 2177172, alpha = 6.956934876302307E-4</strong></span>
<span class="strong"><strong>14/10/25 14:22:00 INFO Word2Vec: wordCount = 2188172, alpha = 5.728977821755993E-4</strong></span>
<span class="strong"><strong>14/10/25 14:22:00 INFO Word2Vec: wordCount = 2199172, alpha = 4.501020767209707E-4</strong></span>
<span class="strong"><strong>14/10/25 14:22:00 INFO Word2Vec: wordCount = 2210172, alpha = 3.2730637126634213E-4</strong></span>
<span class="strong"><strong>14/10/25 14:22:01 INFO Word2Vec: wordCount = 2221172, alpha = 2.0451066581171076E-4</strong></span>
<span class="strong"><strong>14/10/25 14:22:01 INFO Word2Vec: wordCount = 2232172, alpha = 8.171496035708214E-5</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>14/10/25 14:22:02 INFO SparkContext: Job finished: collect at Word2Vec.scala:368, took 56.585983 s</strong></span>
<span class="strong"><strong>14/10/25 14:22:02 INFO MappedRDD: Removing RDD 200 from persistence list</strong></span>
<span class="strong"><strong>14/10/25 14:22:02 INFO BlockManager: Removing RDD 200</strong></span>
<span class="strong"><strong>14/10/25 14:22:02 INFO BlockManager: Removing block rdd_200_0</strong></span>
<span class="strong"><strong>14/10/25 14:22:02 INFO MemoryStore: Block rdd_200_0 of size 9008840 dropped from memory (free 1755596828)</strong></span>
<span class="strong"><strong>word2vecModel: org.apache.spark.mllib.feature.Word2VecModel = org.apache.spark.mllib.feature.Word2VecModel@2b94e480</strong></span>
</pre></div><p>Once<a id="id926" class="indexterm"></a> trained, we can easily find the top <a id="id927" class="indexterm"></a>20 synonyms for a given term (that is, the most similar term to the input term, computed by cosine similarity between the word vectors). For example, to find the 20 most similar terms to <span class="emphasis"><em>hockey</em></span>, use the following lines of code:</p><div class="informalexample"><pre class="programlisting">word2vecModel.findSynonyms("hockey", 20).foreach(println)</pre></div><p>As we can see from the following output, most of the terms relate to hockey or other sports topics:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>(sport,0.6828256249427795)</strong></span>
<span class="strong"><strong>(ecac,0.6718048453330994)</strong></span>
<span class="strong"><strong>(hispanic,0.6519884467124939)</strong></span>
<span class="strong"><strong>(glens,0.6447514891624451)</strong></span>
<span class="strong"><strong>(woofers,0.6351765394210815)</strong></span>
<span class="strong"><strong>(boxscores,0.6009076237678528)</strong></span>
<span class="strong"><strong>(tournament,0.6006366014480591)</strong></span>
<span class="strong"><strong>(champs,0.5957855582237244)</strong></span>
<span class="strong"><strong>(aargh,0.584071934223175)</strong></span>
<span class="strong"><strong>(playoff,0.5834275484085083)</strong></span>
<span class="strong"><strong>(ahl,0.5784651637077332)</strong></span>
<span class="strong"><strong>(ncaa,0.5680188536643982)</strong></span>
<span class="strong"><strong>(pool,0.5612311959266663)</strong></span>
<span class="strong"><strong>(olympic,0.5552600026130676)</strong></span>
<span class="strong"><strong>(champion,0.5549421310424805)</strong></span>
<span class="strong"><strong>(filinuk,0.5528956651687622)</strong></span>
<span class="strong"><strong>(yankees,0.5502706170082092)</strong></span>
<span class="strong"><strong>(motorcycles,0.5484763979911804)</strong></span>
<span class="strong"><strong>(calder,0.5481109023094177)</strong></span>
<span class="strong"><strong>(rec,0.5432182550430298)</strong></span>
</pre></div><p>As another example, we can find 20 synonyms for the term <span class="emphasis"><em>legislation</em></span> as follows:</p><div class="informalexample"><pre class="programlisting">word2vecModel.findSynonyms("legislation", 20).foreach(println)</pre></div><p>In this <a id="id928" class="indexterm"></a>case, we observe the terms related<a id="id929" class="indexterm"></a> to <span class="emphasis"><em>regulation</em></span>, <span class="emphasis"><em>politics</em></span>, and <span class="emphasis"><em>business</em></span> feature prominently:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>(accommodates,0.8149217963218689)</strong></span>
<span class="strong"><strong>(briefed,0.7582570314407349)</strong></span>
<span class="strong"><strong>(amended,0.7310371994972229)</strong></span>
<span class="strong"><strong>(telephony,0.7139414548873901)</strong></span>
<span class="strong"><strong>(aclu,0.7080780863761902)</strong></span>
<span class="strong"><strong>(pitted,0.7062571048736572)</strong></span>
<span class="strong"><strong>(licensee,0.6981208324432373)</strong></span>
<span class="strong"><strong>(agency,0.6880651712417603)</strong></span>
<span class="strong"><strong>(policies,0.6828961372375488)</strong></span>
<span class="strong"><strong>(senate,0.6821110844612122)</strong></span>
<span class="strong"><strong>(businesses,0.6814320087432861)</strong></span>
<span class="strong"><strong>(permit,0.6797110438346863)</strong></span>
<span class="strong"><strong>(cpsr,0.6764014959335327)</strong></span>
<span class="strong"><strong>(cooperation,0.6733141541481018)</strong></span>
<span class="strong"><strong>(surveillance,0.6670728325843811)</strong></span>
<span class="strong"><strong>(restricted,0.6666574478149414)</strong></span>
<span class="strong"><strong>(congress,0.6661365628242493)</strong></span>
<span class="strong"><strong>(procure,0.6655452251434326)</strong></span>
<span class="strong"><strong>(industry,0.6650314927101135)</strong></span>
<span class="strong"><strong>(inquiry,0.6644254922866821)</strong></span>
</pre></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch09lvl1sec65"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we took a deeper look into more complex text processing and explored MLlib's text feature extraction capabilities, in particular the TF-IDF term weighting schemes. We covered examples of using the resulting TF-IDF feature vectors to compute document similarity and train a newsgroup topic classification model. Finally, you learned how to use MLlib's cutting-edge Word2Vec model to compute a vector representation of words in a corpus of text and use the trained model to find words with contextual meaning that is similar to a given word.</p><p>In the next chapter, we will take a look at online learning, and you will learn how Spark Streaming relates to online learning models.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="chapter" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title"><a id="ch10"></a>ChapterÂ 10.Â Real-time Machine Learning with Spark Streaming</h2></div></div></div><p>So far<a id="id930" class="indexterm"></a> in this book, we have focused on <span class="strong"><strong>batch</strong></span> data processing. That is, all our analysis, feature extraction, and model training has been applied to a fixed set of data that does not change. This fits neatly into Spark's core abstraction of RDDs, which are immutable distributed datasets. Once created, the data underlying the RDD does not change, although we might create new RDDs from the original RDD through Spark's transformation and action operators.</p><p>Our attention has also been on batch machine learning models where we train a model on a fixed batch of training data that is usually represented as an RDD of feature vectors (and labels, in the case of supervised learning models).</p><p>In this chapter, we will:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Introduce the concept of online learning, where models are trained and updated on new data as it becomes available</p></li><li style="list-style-type: disc"><p>Explore stream processing using Spark Streaming</p></li><li style="list-style-type: disc"><p>See how Spark Streaming fits together with the online learning approach</p></li></ul></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec66"></a>Online learning</h2></div></div><hr /></div><p>The<a id="id931" class="indexterm"></a> batch machine learning methods that we have applied in this book focus on processing an existing fixed set of training data. Typically, these techniques are also iterative, and we have performed multiple passes over our training data in order to converge to an optimal model.</p><p>By contrast, online learning is based on performing only one sequential pass through the training data in a fully incremental fashion (that is, one training example at a time). After seeing each training example, the model makes a prediction for this example and then receives the true outcome (for example, the label for classification or real target for regression). The idea behind online learning is that the model continually updates as new information is received instead of being retrained periodically in batch training.</p><p>In some settings, when data volume is very large or the process that generates the data is changing rapidly, online learning methods can adapt more quickly and in near real time, without needing to be retrained in an expensive batch process.</p><p>However, online <a id="id932" class="indexterm"></a>learning methods do not have to be used in a purely online manner. In fact, we have already seen an example of using an online learning model in the batch setting when we used <span class="strong"><strong>stochastic gradient descent</strong></span> optimization<a id="id933" class="indexterm"></a> to train our classification and regression models. SGD updates the model after each training example. However, we still made use of multiple passes over the training data in order to converge to a better result.</p><p>In the pure online setting, we do not (or perhaps cannot) make multiple passes over the training data; hence, we need to process each input as it arrives. Online methods also include mini-batch methods where, instead of processing one input at a time, we process a small batch of training data.</p><p>Online and batch methods can also be combined in real-world situations. For example, we can periodically retrain our models offline (say, every day) using batch methods. We can then deploy the trained model to production and update it using online methods in real time (that is, during the day, in between batch retraining) to adapt to any changes in the environment.</p><p>As we will see in this chapter, the online learning setting can fit neatly into stream processing and the Spark Streaming framework.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note52"></a>Note</h3><p>See <a class="ulink" href="http://en.wikipedia.org/wiki/Online_machine_learning" target="_blank">http://en.wikipedia.org/wiki/Online_machine_learning</a> for<a id="id934" class="indexterm"></a> more details on online machine learning.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec67"></a>Stream processing</h2></div></div><hr /></div><p>Before <a id="id935" class="indexterm"></a>covering online learning with Spark, we will first explore the basics of stream processing and introduce the Spark Streaming library.</p><p>In addition to the core Spark API and functionality, the Spark project contains another major library (in the same way as MLlib is a major project library) called <span class="strong"><strong>Spark Streaming</strong></span>, which focuses on processing data streams in real time.</p><p>A data stream is a continuous sequence of records. Common examples include activity stream data from a web or mobile application, time-stamped log data, transactional data, and event streams from sensor or device networks.</p><p>The batch processing approach typically involves saving the data stream to an intermediate<a id="id936" class="indexterm"></a> storage system (for example, HDFS or a database) and running a batch process on the saved data. In order to generate up-to-date results, the batch process must be run periodically (for example, daily, hourly, or even every few minutes) on the latest data available.</p><p>By contrast, the stream-based approach applies processing to the data stream as it is generated. This allows near real-time processing (of the order of a subsecond to a few tenths of a second time frames rather than minutes, hours, days, or even weeks with typical batch processing).</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec93"></a>An introduction to Spark Streaming</h3></div></div></div><p>There<a id="id937" class="indexterm"></a> are a few different general techniques to deal <a id="id938" class="indexterm"></a>with stream processing. Two of the most common ones are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Treat each record individually and process it as soon as it is seen.</p></li><li style="list-style-type: disc"><p>Combine multiple records into <span class="strong"><strong>mini-batches</strong></span>. These mini-batches can be delineated<a id="id939" class="indexterm"></a> either by time or by the number of records in a batch.</p></li></ul></div><p>Spark <a id="id940" class="indexterm"></a>Streaming takes the second approach. The core primitive in <a id="id941" class="indexterm"></a>Spark Streaming is the <span class="strong"><strong>discretized stream</strong></span>, or <span class="strong"><strong>DStream</strong></span>. A DStream is a sequence of mini-batches, where each mini-batch is represented as a Spark RDD:</p><div class="mediaobject"><img src="graphics/8519OS_10_01.jpg" /><div class="caption"><p>The discretized stream abstraction</p></div></div><p>A DStream is defined by its input source and a time window called the <span class="strong"><strong>batch interval</strong></span>. The stream is broken up into time periods equal to the batch interval (beginning from the starting time of the application). Each RDD in the stream will contain the records that are received <a id="id942" class="indexterm"></a>by the Spark Streaming application during a <a id="id943" class="indexterm"></a>given batch interval. If no data is present in a given<a id="id944" class="indexterm"></a> interval, the RDD will simply be empty.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch10lvl3sec46"></a>Input sources</h4></div></div></div><p>Spark <a id="id945" class="indexterm"></a>Streaming <span class="strong"><strong>receivers</strong></span> are responsible for receiving data from an <span class="strong"><strong>input source</strong></span> and converting the raw data into a DStream made up of Spark RDDs.</p><p>Spark Streaming supports various input sources, including file-based sources (where the receiver watches for new files arriving at the input location and creates the DStream from the contents read from each new file) and network-based sources (such as receivers that communicate with socket-based sources, the Twitter API stream, Akka actors, or message queues and distributed stream and log transfer frameworks, such Flume, Kafka, and Amazon Kinesis).</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note53"></a>Note</h3><p>See the documentation on input sources at <a class="ulink" href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#input-dstreams" target="_blank">http://spark.apache.org/docs/latest/streaming-programming-guide.html#input-dstreams</a> for more details and for links to various advanced sources.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch10lvl3sec47"></a>Transformations</h4></div></div></div><p>As we<a id="id946" class="indexterm"></a> saw in <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Getting Up and Running with Spark</em></span>, and throughout this book, Spark allows us to apply powerful transformations<a id="id947" class="indexterm"></a> to RDDs. As DStreams are made up of RDDs, Spark Streaming provides a set of transformations available on DStreams; these transformations are similar to those available on RDDs. These include <code class="literal">map</code>, <code class="literal">flatMap</code>, <code class="literal">filter</code>, <code class="literal">join</code>, and <code class="literal">reduceByKey</code>.</p><p>Spark Streaming transformations, such as those applicable to RDDs, operate on each element of a DStream's underlying data. That is, the transformations are effectively applied to each RDD in the DStream, which, in turn, applies the transformation to the elements of the RDD.</p><p>Spark Streaming also provides operators such as <code class="literal">reduce</code> and <code class="literal">count</code>. These operators return a DStream made up of a single element (for example, the count value for each batch). Unlike the equivalent operators on RDDs, these do not trigger computation on DStreams directly. That is, they are not <span class="strong"><strong>actions</strong></span>, but they are still transformations, as they return another DStream.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch10lvl4sec20"></a>Keeping track of state</h5></div></div></div><p>When we <a id="id948" class="indexterm"></a>were dealing with batch processing of RDDs, keeping and updating a state variable was relatively straightforward. We could start with a certain state (for example, a count or sum of values) and then use broadcast variables or accumulators to update this state in parallel. Usually, we would then use an RDD action to collect the updated state to the driver and, in turn, update the global state.</p><p>With DStreams, this is a little more complex, as we need to keep track of states across batches in a fault-tolerant manner. Conveniently, Spark Streaming provides the <code class="literal">updateStateByKey</code> function on a DStream of key-value pairs, which takes care of this for us, allowing us to create a stream of arbitrary state information and update it with each batch of data seen. For example, the state could be a global count of the number of times each key has been seen. The state could, thus, represent the number of visits per web page, clicks per advert, tweets per user, or purchases per product, for example.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h5 class="title"><a id="ch10lvl4sec21"></a>General transformations</h5></div></div></div><p>The <a id="id949" class="indexterm"></a>Spark Streaming API also exposes a general <code class="literal">transform</code> function that gives us access to the underlying RDD for each batch in the stream. That is, where the higher level functions such as <code class="literal">map</code> transform a DStream to another DStream, <code class="literal">transform</code> allows us to apply functions from an RDD to another RDD. For example, we can use the RDD <code class="literal">join</code> operator to join each batch of the stream to an <a id="id950" class="indexterm"></a>existing RDD that we computed separately from our streaming application (perhaps, in Spark or some other system).</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note54"></a>Note</h3><p>The<a id="id951" class="indexterm"></a> full list of transformations and further information on each of them is provided in the Spark documentation at <a class="ulink" href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#transformations-on-dstreams" target="_blank">http://spark.apache.org/docs/latest/streaming-programming-guide.html#transformations-on-dstreams</a>.</p></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch10lvl3sec48"></a>Actions</h4></div></div></div><p>While <a id="id952" class="indexterm"></a>some of the operators we have seen in Spark Streaming, such as <code class="literal">count</code>, are not actions as in the batch RDD case, Spark Streaming has the concept of <span class="strong"><strong>actions</strong></span> on DStreams. Actions are <span class="strong"><strong>output</strong></span> operators that, when invoked, trigger <a id="id953" class="indexterm"></a>computation on the DStream. They are as follows:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><code class="literal">print</code>: This prints the first 10 elements of each batch to the console and is typically used for debugging and testing.</p></li><li style="list-style-type: disc"><p><code class="literal">saveAsObjectFile</code>, <code class="literal">saveAsTextFiles</code>, and <code class="literal">saveAsHadoopFiles</code>: These functions output each batch to a Hadoop-compatible filesystem with a filename (if applicable) derived from the batch start timestamp.</p></li><li style="list-style-type: disc"><p><code class="literal">forEachRDD</code>: This operator is the most generic and allows us to apply any arbitrary processing to the RDDs within each batch of a DStream. It is used to <a id="id954" class="indexterm"></a>apply <span class="emphasis"><em>side effects</em></span>, such as saving data to an external system, printing it for testing, exporting it to a dashboard, and so on.</p></li></ul></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip60"></a>Tip</h3><p>Note that like batch processing with Spark, DStream operators are <span class="strong"><strong>lazy</strong></span>. In the same way in which we need to call an action, such as <code class="literal">count</code>, on an RDD to ensure that processing takes place, we need to call one of the preceding action operators in order to trigger computation on a DStream. Otherwise, our streaming application will not actually perform any computation.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch10lvl3sec49"></a>Window operators</h4></div></div></div><p>As<a id="id955" class="indexterm"></a> Spark Streaming operates on time-ordered<a id="id956" class="indexterm"></a> batched streams of data, it introduces a new concept, which is<a id="id957" class="indexterm"></a> that of <span class="strong"><strong>windowing</strong></span>. A <code class="literal">window</code> function computes a transformation over a sliding window applied to the stream.</p><p>A window is defined by the length of the window and the sliding interval. For example, with a 10-second window and a 5-second sliding interval, we will compute results every 5 seconds, based <a id="id958" class="indexterm"></a>on the latest 10 seconds of data in the DStream. For example, we might wish to calculate the top websites by page view numbers over the last 10 seconds and recompute this metric every 5 seconds using a sliding window.</p><p>The following figure illustrates a windowed DStream:</p><div class="mediaobject"><img src="graphics/8519OS_10_02.jpg" /><div class="caption"><p>A windowed DStream</p></div></div></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec94"></a>Caching and fault tolerance with Spark Streaming</h3></div></div></div><p>Like <a id="id959" class="indexterm"></a>Spark RDDs, DStreams can be <a id="id960" class="indexterm"></a>cached in memory. The use cases for caching are similar to those for RDDsâ€”if we expect to access the data in a DStream multiple times (perhaps performing multiple types of analysis or aggregation or outputting to multiple external systems), we will benefit from caching the data. Stateful operators, which include <code class="literal">window</code> functions and <code class="literal">updateStateByKey</code>, do this automatically for efficiency.</p><p>Recall that RDDs are immutable datasets and are defined by their input data source and <span class="strong"><strong>lineage</strong></span>â€”that is, the set of transformations and actions that are applied to the RDD. Fault tolerance in RDDs works by recreating the RDD (or partition of an RDD) that is lost due to the failure of a worker node.</p><p>As DStreams are themselves batches of RDDs, they can also be recomputed as required to deal with worker node failure. However, this depends on the input data still being available. If the data source itself is fault-tolerant and persistent (such as HDFS or some other fault-tolerant data store), then the DStream can be recomputed.</p><p>If data stream sources are delivered over a network (which is a common case with stream processing), Spark Streaming's default persistence behavior is to replicate data to two worker nodes. This allows network DStreams to be recomputed in the case of failure. Note, however, that any data received by a node but <span class="emphasis"><em>not yet replicated</em></span> might be lost when a node fails.</p><p>Spark <a id="id961" class="indexterm"></a>Streaming also supports recovery<a id="id962" class="indexterm"></a> of the driver node in the event of failure. However, currently, for network-based sources, data in the memory of worker nodes will be lost in this case. Hence, Spark Streaming is not fully fault-tolerant in the face of failure of the driver node or application.</p><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note55"></a>Note</h3><p>See <a class="ulink" href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#cachingâ€”persistence" target="_blank">http://spark.apache.org/docs/latest/streaming-programming-guide.html#cachingâ€”persistence</a> and <a class="ulink" href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#fault-tolerance-properties" target="_blank">http://spark.apache.org/docs/latest/streaming-programming-guide.html#fault-tolerance-properties</a> for more details.</p></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec68"></a>Creating a Spark Streaming application</h2></div></div><hr /></div><p>We <a id="id963" class="indexterm"></a>will now work through creating our first Spark Streaming application to illustrate some of the basic concepts around Spark Streaming that we introduced earlier.</p><p>We will expand on the example applications used in <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Getting Up and Running with Spark</em></span>, where we used a small example dataset of product purchase events. For this example, instead of using a static set of data, we will create a simple producer application that will randomly generate events and send them over a network connection. We will then create a few Spark Streaming consumer applications that will process this event stream.</p><p>The sample project for this chapter contains the code you will need. It is called <code class="literal">scala-spark-streaming-app</code>. It consists of a Scala SBT project definition file, the example application source code, and a <code class="literal">\src\main\resources</code> directory that contains a file called <code class="literal">names.csv</code>.</p><p>The <code class="literal">build.sbt</code> file for the project contains the following project definition:</p><div class="informalexample"><pre class="programlisting">name := "scala-spark-streaming-app"

version := "1.0"

scalaVersion := "2.10.4"

libraryDependencies += "org.apache.spark" %% "spark-mllib" % "1.1.0"

libraryDependencies += "org.apache.spark" %% "spark-streaming" % "1.1.0"</pre></div><p>Note <a id="id964" class="indexterm"></a>that we added a dependency on Spark MLlib and Spark Streaming, which includes the dependency on the Spark core.</p><p>The <code class="literal">names.csv</code> file contains a set of 20 randomly generated user names. We will use these names as part of our data generation function in our producer application:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Miguel,Eric,James,Juan,Shawn,James,Doug,Gary,Frank,Janet,Michael,James,Malinda,Mike,Elaine,Kevin,Janet,Richard,Saul,Manuela</strong></span>
</pre></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec95"></a>The producer application</h3></div></div></div><p>Our <a id="id965" class="indexterm"></a>producer needs to create a network <a id="id966" class="indexterm"></a>connection and generate some random purchase event data to send over this connection. First, we will define our object and main method definition. We will then read the random names from the <code class="literal">names.csv</code> resource and create a set of products with prices, from which we will generate our random product events:</p><div class="informalexample"><pre class="programlisting">/**
 * A producer application that generates random "product events", up to 5 per second, and sends them over a
 * network connection
 */
object StreamingProducer {

  def main(args: Array[String]) {

    val random = new Random()

    // Maximum number of events per second
    val MaxEvents = 6

    // Read the list of possible names
    val namesResource = this.getClass.getResourceAsStream("/names.csv")
    val names = scala.io.Source.fromInputStream(namesResource)
      .getLines()
      .toList
      .head
      .split(",")
      .toSeq

    // Generate a sequence of possible products
    val products = Seq(
      "iPhone Cover" -&gt; 9.99,
      "Headphones" -&gt; 5.49,
      "Samsung Galaxy Cover" -&gt; 8.95,
      "iPad Cover" -&gt; 7.49
    )</pre></div><p>Using <a id="id967" class="indexterm"></a>the list of names and map<a id="id968" class="indexterm"></a> of product name to price, we will create a function that will randomly pick a product and name from these sources, generating a specified number of product events:</p><div class="informalexample"><pre class="programlisting">    /** Generate a number of random product events */
    def generateProductEvents(n: Int) = {
      (1 to n).map { i =&gt;
        val (product, price) = products(random.nextInt(products.size))
        val user = random.shuffle(names).head
        (user, product, price)
      }
    }</pre></div><p>Finally, we will create a network socket and set our producer to listen on this socket. As soon as a connection is made (which will come from our consumer streaming application), the producer will start generating random events at a random rate between 0 and 5 per second:</p><div class="informalexample"><pre class="programlisting">    // create a network producer
    val listener = new ServerSocket(9999)
    println("Listening on port: 9999")

    while (true) {
      val socket = listener.accept()
      new Thread() {
        override def run = {
          println("Got client connected from: " + socket.getInetAddress)
          val out = new PrintWriter(socket.getOutputStream(), true)

          while (true) {
            Thread.sleep(1000)
            val num = random.nextInt(MaxEvents)
            val productEvents = generateProductEvents(num)
            productEvents.foreach{ event =&gt;
              out.write(event.productIterator.mkString(","))
              out.write("\n")
            }
            out.flush()
            println(s"Created $num events...")
          }
          socket.close()
        }
      }.start()
    }
  }
}</pre></div><div class="note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="note56"></a>Note</h3><p>This producer example is based on the <code class="literal">PageViewGenerator</code> example in the Spark Streaming examples.</p></div><p>The<a id="id969" class="indexterm"></a> producer can be run by changing <a id="id970" class="indexterm"></a>into the base directory of <code class="literal">scala-spark-streaming-app</code> and using SBT to run the application, as we did in <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Getting Up and Running with Spark</em></span>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;cd scala-spark-streaming-app</strong></span>
<span class="strong"><strong>&gt;sbt</strong></span>
<span class="strong"><strong>[info] ...</strong></span>
<span class="strong"><strong>&gt;</strong></span>
</pre></div><p>Use the <code class="literal">run</code> command to execute the application:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;run</strong></span>
</pre></div><p>You should see output similar to the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>Multiple main classes detected, select one to run:</strong></span>

<span class="strong"><strong> [1] StreamingProducer</strong></span>
<span class="strong"><strong> [2] SimpleStreamingApp</strong></span>
<span class="strong"><strong> [3] StreamingAnalyticsApp</strong></span>
<span class="strong"><strong> [4] StreamingStateApp</strong></span>
<span class="strong"><strong> [5] StreamingModelProducer</strong></span>
<span class="strong"><strong> [6] SimpleStreamingModel</strong></span>
<span class="strong"><strong> [7] MonitoringStreamingModel</strong></span>

<span class="strong"><strong>Enter number:</strong></span>
</pre></div><p>Select <a id="id971" class="indexterm"></a>the <code class="literal">StreamingProducer</code> option. The application will start running, and you should see the following <a id="id972" class="indexterm"></a>output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[info] Running StreamingProducer</strong></span>
<span class="strong"><strong>Listening on port: 9999</strong></span>
</pre></div><p>We can see that the producer is listening on port <code class="literal">9999</code>, waiting for our consumer application to connect.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec96"></a>Creating a basic streaming application</h3></div></div></div><p>Next, we<a id="id973" class="indexterm"></a> will create<a id="id974" class="indexterm"></a> our first streaming program. We will simply connect to the producer and print out the contents of each batch. Our streaming code looks like this:</p><div class="informalexample"><pre class="programlisting">/**
 * A simple Spark Streaming app in Scala
 */
object SimpleStreamingApp {

  def main(args: Array[String]) {

    val ssc = new StreamingContext("local[2]", "First Streaming App", Seconds(10))
    val stream = ssc.socketTextStream("localhost", 9999)

    // here we simply print out the first few elements of each
    // batch
    stream.print()
    ssc.start()
    ssc.awaitTermination()

  }
}</pre></div><p>It looks fairly simple, and it is mostly due to the fact that Spark Streaming takes care of all the complexity for us. First, we initialized a <code class="literal">StreamingContext</code> (which is the streaming equivalent of the <code class="literal">SparkContext</code> we have used so far), specifying similar configuration options that are used to create a <code class="literal">SparkContext</code>. Notice, however, that here we <a id="id975" class="indexterm"></a>are required<a id="id976" class="indexterm"></a> to provide the batch interval, which we set to 10 seconds.</p><p>We then created our data stream using a predefined streaming source, <code class="literal">socketTextStream</code>, which reads text from a socket host and port and creates a <code class="literal">DStream[String]</code>. We then called the <code class="literal">print</code> function on the DStream; this function prints out the first few elements of each batch.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip61"></a>Tip</h3><p>Calling <code class="literal">print</code> on a DStream is similar to calling <code class="literal">take</code> on an RDD. It displays only the first few elements.</p></div><p>We can run this program using SBT. Open a second terminal window, leaving the producer program running, and run <code class="literal">sbt</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;sbt</strong></span>
<span class="strong"><strong>[info] ...</strong></span>
<span class="strong"><strong>&gt;run</strong></span>
<span class="strong"><strong>....</strong></span>
</pre></div><p>Again, you should see a few options to select:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Multiple main classes detected, select one to run:</strong></span>

<span class="strong"><strong> [1] StreamingProducer</strong></span>
<span class="strong"><strong> [2] SimpleStreamingApp</strong></span>
<span class="strong"><strong> [3] StreamingAnalyticsApp</strong></span>
<span class="strong"><strong> [4] StreamingStateApp</strong></span>
<span class="strong"><strong> [5] StreamingModelProducer</strong></span>
<span class="strong"><strong> [6] SimpleStreamingModel</strong></span>
<span class="strong"><strong> [7] MonitoringStreamingModel</strong></span>
</pre></div><p>Run the <code class="literal">SimpleStreamingApp</code> main class. You should see the streaming program start up, displaying output similar to the one shown here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>14/11/15 21:02:23 INFO scheduler.ReceiverTracker: ReceiverTracker started</strong></span>
<span class="strong"><strong>14/11/15 21:02:23 INFO dstream.ForEachDStream: metadataCleanupDelay = -1</strong></span>
<span class="strong"><strong>14/11/15 21:02:23 INFO dstream.SocketInputDStream: metadataCleanupDelay = -1</strong></span>
<span class="strong"><strong>14/11/15 21:02:23 INFO dstream.SocketInputDStream: Slide time = 10000 ms</strong></span>
<span class="strong"><strong>14/11/15 21:02:23 INFO dstream.SocketInputDStream: Storage level = StorageLevel(false, false, false, false, 1)</strong></span>
<span class="strong"><strong>14/11/15 21:02:23 INFO dstream.SocketInputDStream: Checkpoint interval = null</strong></span>
<span class="strong"><strong>14/11/15 21:02:23 INFO dstream.SocketInputDStream: Remember duration = 10000 ms</strong></span>
<span class="strong"><strong>14/11/15 21:02:23 INFO dstream.SocketInputDStream: Initialized and validated org.apache.spark.streaming.dstream.SocketInputDStream@ff3436d</strong></span>
<span class="strong"><strong>14/11/15 21:02:23 INFO dstream.ForEachDStream: Slide time = 10000 ms</strong></span>
<span class="strong"><strong>14/11/15 21:02:23 INFO dstream.ForEachDStream: Storage level = StorageLevel(false, false, false, false, 1)</strong></span>
<span class="strong"><strong>14/11/15 21:02:23 INFO dstream.ForEachDStream: Checkpoint interval = null</strong></span>
<span class="strong"><strong>14/11/15 21:02:23 INFO dstream.ForEachDStream: Remember duration = 10000 ms</strong></span>
<span class="strong"><strong>14/11/15 21:02:23 INFO dstream.ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@5a10b6e8</strong></span>
<span class="strong"><strong>14/11/15 21:02:23 INFO scheduler.ReceiverTracker: Starting 1 receivers</strong></span>
<span class="strong"><strong>14/11/15 21:02:23 INFO spark.SparkContext: Starting job: runJob at ReceiverTracker.scala:275</strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><p>At<a id="id977" class="indexterm"></a> the same time, you<a id="id978" class="indexterm"></a> should see that the terminal window running the producer displays something like the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>Got client connected from: /127.0.0.1</strong></span>
<span class="strong"><strong>Created 2 events...</strong></span>
<span class="strong"><strong>Created 2 events...</strong></span>
<span class="strong"><strong>Created 3 events...</strong></span>
<span class="strong"><strong>Created 1 events...</strong></span>
<span class="strong"><strong>Created 5 events...</strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><p>After about 10 seconds, which is the time of our streaming batch interval, Spark Streaming will trigger a computation on the stream due to our use of the <code class="literal">print</code> operator. This should display the first few events in the batch, which will look something like the following output:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>14/11/15 21:02:30 INFO spark.SparkContext: Job finished: take at DStream.scala:608, took 0.05596 s</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>Time: 1416078150000 ms</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>Michael,Headphones,5.49</strong></span>
<span class="strong"><strong>Frank,Samsung Galaxy Cover,8.95</strong></span>
<span class="strong"><strong>Eric,Headphones,5.49</strong></span>
<span class="strong"><strong>Malinda,iPad Cover,7.49</strong></span>
<span class="strong"><strong>James,iPhone Cover,9.99</strong></span>
<span class="strong"><strong>James,Headphones,5.49</strong></span>
<span class="strong"><strong>Doug,iPhone Cover,9.99</strong></span>
<span class="strong"><strong>Juan,Headphones,5.49</strong></span>
<span class="strong"><strong>James,iPhone Cover,9.99</strong></span>
<span class="strong"><strong>Richard,iPad Cover,7.49</strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip62"></a>Tip</h3><p>Note that you might see different results, as the producer generates a random number of random events each second.</p></div><p>You<a id="id979" class="indexterm"></a> can terminate <a id="id980" class="indexterm"></a>the streaming app by pressing <span class="emphasis"><em>Ctrl</em></span> + <span class="emphasis"><em>C</em></span>. If you want to, you can also terminate the producer (if you do, you will need to restart it again before starting the next streaming programs that we will create).</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec97"></a>Streaming analytics</h3></div></div></div><p>Next, we<a id="id981" class="indexterm"></a> will create a slightly more<a id="id982" class="indexterm"></a> complex streaming program. In <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Getting Up and Running with Spark</em></span>, we calculated a few metrics on our dataset of product purchases. These included the total number of purchases, the number of unique users, the total revenue, and the most popular product (together with its number of purchases and total revenue).</p><p>In this example, we will compute the same metrics on our stream of purchase events. The key difference is that these metrics will be computed per batch and printed out.</p><p>We will define our streaming application code here:</p><div class="informalexample"><pre class="programlisting">/**
 * A more complex Streaming app, which computes statistics and prints the results for each batch in a DStream
 */
object StreamingAnalyticsApp {

  def main(args: Array[String]) {

    val ssc = new StreamingContext("local[2]", "First Streaming App", Seconds(10))
    val stream = ssc.socketTextStream("localhost", 9999)

    // create stream of events from raw text elements
    val events = stream.map { record =&gt;
      val event = record.split(",")
      (event(0), event(1), event(2))
    }</pre></div><p>First, we created exactly the same <code class="literal">StreamingContext</code> and socket stream as we did earlier. Our next step is to apply a <code class="literal">map</code> transformation to the raw text, where each record is a comma-separated string representing the purchase event. The <code class="literal">map</code> function splits the text and creates a tuple of <code class="literal">(user, product, price)</code>. This illustrates the use of <code class="literal">map</code> on a DStream and how it is the same as if we had been operating on an RDD.</p><p>Next, we will use <code class="literal">foreachRDD</code> to apply arbitrary processing on each RDD in the stream to compute our desired metrics and print them to the console:</p><div class="informalexample"><pre class="programlisting">    /*
      We compute and print out stats for each batch.
      Since each batch is an RDD, we call forEeachRDD on the DStream, and apply the usual RDD functions
      we used in Chapter 1.
     */
    events.foreachRDD { (rdd, time) =&gt;
      val numPurchases = rdd.count()
      val uniqueUsers = rdd.map { case (user, _, _) =&gt; user }.distinct().count()
      val totalRevenue = rdd.map { case (_, _, price) =&gt; price.toDouble }.sum()
      val productsByPopularity = rdd
        .map { case (user, product, price) =&gt; (product, 1) }
        .reduceByKey(_ + _)
        .collect()
        .sortBy(-_._2)
      val mostPopular = productsByPopularity(0)

      val formatter = new SimpleDateFormat
      val dateStr = formatter.format(new Date(time.milliseconds))
      println(s"== Batch start time: $dateStr ==")
      println("Total purchases: " + numPurchases)
      println("Unique users: " + uniqueUsers)
      println("Total revenue: " + totalRevenue)
      println("Most popular product: %s with %d purchases".format(mostPopular._1, mostPopular._2))
    }

    // start the context
    ssc.start()
    ssc.awaitTermination()

  }

}</pre></div><p>If <a id="id983" class="indexterm"></a>you compare the code operating<a id="id984" class="indexterm"></a> on the RDDs inside the preceding <code class="literal">foreachRDD</code> block with that used in <a class="link" href="#" linkend="ch01">Chapter 1</a>, <span class="emphasis"><em>Getting Up and Running with Spark</em></span>, you will notice that it is virtually the same code. This shows that we can apply any RDD-related processing we wish within the streaming setting by operating on the underlying RDDs, as well as using the built-in higher level streaming operations.</p><p>Let's run the streaming program again by calling <code class="literal">sbt run</code> and selecting <code class="literal">StreamingAnalyticsApp</code>.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip63"></a>Tip</h3><p>Remember that you might also need to restart the producer if you previously terminated the program. This should be done before starting the streaming application.</p></div><p>After about 10 seconds, you should see output from the streaming program similar to the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>14/11/15 21:27:30 INFO spark.SparkContext: Job finished: collect at Streaming.scala:125, took 0.071145 s</strong></span>
<span class="strong"><strong>== Batch start time: 2014/11/15 9:27 PM ==</strong></span>
<span class="strong"><strong>Total purchases: 16</strong></span>
<span class="strong"><strong>Unique users: 10</strong></span>
<span class="strong"><strong>Total revenue: 123.72</strong></span>
<span class="strong"><strong>Most popular product: iPad Cover with 6 purchases</strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><p>You <a id="id985" class="indexterm"></a>can again terminate the streaming <a id="id986" class="indexterm"></a>program using <span class="emphasis"><em>Ctrl</em></span> + <span class="emphasis"><em>C</em></span>.</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec98"></a>Stateful streaming</h3></div></div></div><p>As a <a id="id987" class="indexterm"></a>final example, we will apply the<a id="id988" class="indexterm"></a> concept of <span class="strong"><strong>stateful</strong></span> streaming using the <code class="literal">updateStateByKey</code> function to compute a global state of revenue and number of purchases per user, which will be updated with new data from each 10-second batch. Our <code class="literal">StreamingStateApp</code> app is shown here:</p><div class="informalexample"><pre class="programlisting">object StreamingStateApp {
  import org.apache.spark.streaming.StreamingContext._</pre></div><p>We will first define an <code class="literal">updateState</code> function that will compute the new state from the running state value and the new data in the current batch. Our state, in this case, is a tuple of <code class="literal">(number of products, revenue)</code> pairs, which we will keep for each user. We will compute the new state given the set of <code class="literal">(product, revenue)</code> pairs for the current batch and the accumulated state at the current time.</p><p>Notice that we will deal with an <code class="literal">Option</code> value for the current state, as it might be empty (which will be the case for the first batch), and we need to define a default value, which we will do using <code class="literal">getOrElse</code> as shown here:</p><div class="informalexample"><pre class="programlisting">  def updateState(prices: Seq[(String, Double)], currentTotal: Option[(Int, Double)]) = {
    val currentRevenue = prices.map(_._2).sum
    val currentNumberPurchases = prices.size
    val state = currentTotal.getOrElse((0, 0.0))
    Some((currentNumberPurchases + state._1, currentRevenue + state._2))
  }

  def main(args: Array[String]) {

    val ssc = new StreamingContext("local[2]", "First Streaming App", Seconds(10))
    // for stateful operations, we need to set a checkpoint
    // location
    ssc.checkpoint("/tmp/sparkstreaming/")
    val stream = ssc.socketTextStream("localhost", 9999)

    // create stream of events from raw text elements
    val events = stream.map { record =&gt;
      val event = record.split(",")
      (event(0), event(1), event(2).toDouble)
    }

    val users = events.map{ case (user, product, price) =&gt; (user, (product, price)) }
    val revenuePerUser = users.updateStateByKey(updateState)
    revenuePerUser.print()

    // start the context
    ssc.start()
    ssc.awaitTermination()

  }
}</pre></div><p>After <a id="id989" class="indexterm"></a>applying the same string split<a id="id990" class="indexterm"></a> transformation we used in our previous example, we called <code class="literal">updateStateByKey</code> on our DStream, passing in our defined <code class="literal">updateState</code> function. We then printed the results to the console.</p><p>Start the streaming example using <code class="literal">sbt run</code> and by selecting <code class="literal">[4] StreamingStateApp</code> (also restart the producer program if necessary).</p><p>After<a id="id991" class="indexterm"></a> around 10 seconds, you will start to<a id="id992" class="indexterm"></a> see the first set of state output. We will wait another 10 seconds to see the next set of output. You will see the overall global state being updated:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>Time: 1416080440000 ms</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>(Janet,(2,10.98))</strong></span>
<span class="strong"><strong>(Frank,(1,5.49))</strong></span>
<span class="strong"><strong>(James,(2,12.98))</strong></span>
<span class="strong"><strong>(Malinda,(1,9.99))</strong></span>
<span class="strong"><strong>(Elaine,(3,29.97))</strong></span>
<span class="strong"><strong>(Gary,(2,12.98))</strong></span>
<span class="strong"><strong>(Miguel,(3,20.47))</strong></span>
<span class="strong"><strong>(Saul,(1,5.49))</strong></span>
<span class="strong"><strong>(Manuela,(2,18.939999999999998))</strong></span>
<span class="strong"><strong>(Eric,(2,18.939999999999998))</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>Time: 1416080441000 ms</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>(Janet,(6,34.94))</strong></span>
<span class="strong"><strong>(Juan,(4,33.92))</strong></span>
<span class="strong"><strong>(Frank,(2,14.44))</strong></span>
<span class="strong"><strong>(James,(7,48.93000000000001))</strong></span>
<span class="strong"><strong>(Malinda,(1,9.99))</strong></span>
<span class="strong"><strong>(Elaine,(7,61.89))</strong></span>
<span class="strong"><strong>(Gary,(4,28.46))</strong></span>
<span class="strong"><strong>(Michael,(1,8.95))</strong></span>
<span class="strong"><strong>(Richard,(2,16.439999999999998))</strong></span>
<span class="strong"><strong>(Miguel,(5,35.95))</strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><p>We<a id="id993" class="indexterm"></a> can see that the number of purchases <a id="id994" class="indexterm"></a>and revenue totals for each user are added to with each batch of data.</p><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip64"></a>Tip</h3><p>Now, see if you can adapt this example to use Spark Streaming's <code class="literal">window</code> functions. For example, you can compute similar statistics per user over the past minute, sliding every 30 seconds.</p></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec69"></a>Online learning with Spark Streaming</h2></div></div><hr /></div><p>As <a id="id995" class="indexterm"></a>we have seen, Spark Streaming makes it easy to work with data streams in a way that should be familiar to us from working with RDDs. Using Spark's stream processing primitives combined with the online learning capabilities of MLlib's SGD-based methods, we can create real-time machine learning models that we can update on new data in the stream as it arrives.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec99"></a>Streaming regression</h3></div></div></div><p>Spark provides a<a id="id996" class="indexterm"></a> built-in streaming machine learning model in the <code class="literal">StreamingLinearAlgorithm</code> class. Currently, only a linear regression implementation is<a id="id997" class="indexterm"></a> availableâ€”<code class="literal">StreamingLinearRegressionWithSGD</code>â€”but future versions will include classification.</p><p>The streaming regression model provides two methods for usage:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p><code class="literal">trainOn</code>: This<a id="id998" class="indexterm"></a> takes <code class="literal">DStream[LabeledPoint]</code> as its argument. This tells the model to train on every batch in the input DStream. It can be called multiple times to train on different streams.</p></li><li style="list-style-type: disc"><p><code class="literal">predictOn</code>: This also takes <code class="literal">DStream[LabeledPoint]</code>. This tells the model to<a id="id999" class="indexterm"></a> make predictions on the input DStream, returning a new <code class="literal">DStream[Double]</code> that contains the model predictions.</p></li></ul></div><p>Under the hood, the streaming regression model uses <code class="literal">foreachRDD</code> and <code class="literal">map</code> to accomplish this. It also updates the model variable after each batch and exposes the latest trained model, which allows us to use this model in other applications or save it to an external location.</p><p>The streaming regression model can be configured with parameters for step size and number of iterations in the same way as standard batch regressionâ€”the model class used is the same. We can also set the initial model weight vector.</p><p>When <a id="id1000" class="indexterm"></a>we first start training a model, we can set the initial <a id="id1001" class="indexterm"></a>weights to a zero vector, or a random vector, or perhaps load the latest model from the result of an offline batch process. We can also decide to save the model periodically to an external system and use the latest model state as the starting point (for example, in the case of a restart after a node or application failure).</p></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec100"></a>A simple streaming regression program</h3></div></div></div><p>To illustrate the use <a id="id1002" class="indexterm"></a>of streaming regression, we will create a simple example similar to the preceding<a id="id1003" class="indexterm"></a> one, which uses simulated data. We will write a producer program that generates random feature vectors and target variables, given a fixed, known weight vector, and writes each training example to a network stream.</p><p>Our consumer application will run a streaming regression model, training and then testing on our simulated data stream. Our first example consumer will simply print its predictions to the console.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch10lvl3sec50"></a>Creating a streaming data producer</h4></div></div></div><p>The <a id="id1004" class="indexterm"></a>data producer <a id="id1005" class="indexterm"></a>operates in a manner similar to our product event producer example. Recall from <a class="link" href="#" linkend="ch05">Chapter 5</a>, <span class="emphasis"><em>Building a Classification Model with Spark</em></span>, that a linear model is a linear combination (or vector dot product) of a weight vector, <span class="emphasis"><em>w</em></span>, and a feature vector, <span class="emphasis"><em>x</em></span> (that is, <span class="emphasis"><em>wTx</em></span>). Our producer will generate synthetic data using a fixed, known weight vector and randomly generated feature vectors. This data fits the linear model formulation exactly, so we will expect our regression model to learn the true weight vector fairly easily.</p><p>First, we will set up a maximum number of events per second (say, 100) and the number of features in our feature vector (also 100 in this example):</p><div class="informalexample"><pre class="programlisting">/**
 * A producer application that generates random linear regression data.
 */
object StreamingModelProducer {
  import breeze.linalg._

  def main(args: Array[String]) {

    // Maximum number of events per second
    val MaxEvents = 100
    val NumFeatures = 100

    val random = new Random()</pre></div><p>The <code class="literal">generateRandomArray</code> function creates an array of the specified size where the entries are randomly generated from a normal distribution. We will use this function initially to generate our known weight vector, <code class="literal">w</code>, which will be fixed throughout the life of the producer. We will also create a random <code class="literal">intercept</code> value that will also be fixed. The <a id="id1006" class="indexterm"></a>weight vector <a id="id1007" class="indexterm"></a>and <code class="literal">intercept</code> will be used to generate each data point in our stream:</p><div class="informalexample"><pre class="programlisting">    /** Function to generate a normally distributed dense vector */
    def generateRandomArray(n: Int) = Array.tabulate(n)(_ =&gt; random.nextGaussian())

    // Generate a fixed random model weight vector
    val w = new DenseVector(generateRandomArray(NumFeatures))
    val intercept = random.nextGaussian() * 10</pre></div><p>We will also need a function to generate a specified number of random data points. Each event is made up of a random feature vector and the target that we get from computing the dot product of our known weight vector with the random feature vector and adding the <code class="literal">intercept</code> value:</p><div class="informalexample"><pre class="programlisting">    /** Generate a number of random data events*/
    def generateNoisyData(n: Int) = {
      (1 to n).map { i =&gt;
        val x = new DenseVector(generateRandomArray(NumFeatures))
        val y: Double = w.dot(x)
        val noisy = y + intercept
        (noisy, x)
      }
    }</pre></div><p>Finally, we will use code similar to our previous producer to instantiate a network connection and send a random number of data points (between 0 and 100) in text format over the network each second:</p><div class="informalexample"><pre class="programlisting">    // create a network producer
    val listener = new ServerSocket(9999)
    println("Listening on port: 9999")

    while (true) {
      val socket = listener.accept()
      new Thread() {
        override def run = {
          println("Got client connected from: " + socket.getInetAddress)
          val out = new PrintWriter(socket.getOutputStream(), true)

          while (true) {
            Thread.sleep(1000)
            val num = random.nextInt(MaxEvents)
            val data = generateNoisyData(num)
            data.foreach { case (y, x) =&gt;
              val xStr = x.data.mkString(",")
              val eventStr = s"$y\t$xStr"
              out.write(eventStr)
              out.write("\n")
            }
            out.flush()
            println(s"Created $num events...")
          }
          socket.close()
        }
      }.start()
    }
  }
}</pre></div><p>You <a id="id1008" class="indexterm"></a>can start the <a id="id1009" class="indexterm"></a>producer using <code class="literal">sbt run</code>, followed by choosing to execute the <code class="literal">StreamingModelProducer</code> main method. This should result in the following output, thus indicating that the producer program is waiting for connections from our streaming regression application:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[info] Running StreamingModelProducer</strong></span>
<span class="strong"><strong>Listening on port: 9999</strong></span>
</pre></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h4 class="title"><a id="ch10lvl3sec51"></a>Creating a streaming regression model</h4></div></div></div><p>In the<a id="id1010" class="indexterm"></a> next step<a id="id1011" class="indexterm"></a> in our example, we will create a streaming regression program. The basic layout and setup is the same as our previous streaming analytics examples:</p><div class="informalexample"><pre class="programlisting">/**
 * A simple streaming linear regression that prints out predicted value for each batch
 */
object SimpleStreamingModel {

  def main(args: Array[String]) {

    val ssc = new StreamingContext("local[2]", "First Streaming App", Seconds(10))
    val stream = ssc.socketTextStream("localhost", 9999)</pre></div><p>Here, we will set up the number of features to match the records in our input data stream. We will then create a zero vector to use as the initial weight vector of our streaming regression model. Finally, we will select the number of iterations and step size:</p><div class="informalexample"><pre class="programlisting">val NumFeatures = 100
    val zeroVector = DenseVector.zeros[Double](NumFeatures)
    val model = new StreamingLinearRegressionWithSGD()
      .setInitialWeights(Vectors.dense(zeroVector.data))
      .setNumIterations(1)
      .setStepSize(0.01)</pre></div><p>Next, we will again use the <code class="literal">map</code> function to transform the input DStream, where each record is a string representation of our input data, into a <code class="literal">LabeledPoint</code> instance that contains the target value and feature vector:</p><div class="informalexample"><pre class="programlisting">    // create a stream of labeled points
    val labeledStream = stream.map { event =&gt;
      val split = event.split("\t")
      val y = split(0).toDouble
      val features = split(1).split(",").map(_.toDouble)
      LabeledPoint(label = y, features = Vectors.dense(features))
    }</pre></div><p>The final step is to tell the model to train and test on our transformed DStream and also to print out the first few elements of each batch in the DStream of predicted values:</p><div class="informalexample"><pre class="programlisting">    // train and test model on the stream, and print predictions
    // for illustrative purposes
    model.trainOn(labeledStream)
    model.predictOn(labeledStream).print()

    ssc.start()
    ssc.awaitTermination()

  }
}</pre></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip65"></a>Tip</h3><p>Note that because we are using the same MLlib model classes for streaming as we did for batch processing, we can, if we choose, perform multiple iterations over the training data in each batch (which is just an RDD of <code class="literal">LabeledPoint</code> instances).</p><p>Here, we will set the number of iterations to <code class="literal">1</code> to simulate purely online learning. In practice, you can set the number of iterations higher, but note that the training time per batch will go up. If the training time per batch is much higher than the batch interval, the streaming model will start to lag behind the velocity of the data stream.</p><p>This can be handled by decreasing the number of iterations, increasing the batch interval, or increasing the parallelism of our streaming program by adding more Spark workers.</p></div><p>Now, we're ready<a id="id1012" class="indexterm"></a> to run <code class="literal">SimpleStreamingModel</code> in our second terminal window using <code class="literal">sbt run</code> in <a id="id1013" class="indexterm"></a>the same way as we did for the producer (remember to select the correct main method for SBT to execute). Once the streaming program starts running, you should see the following output in the producer console:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Got client connected from: /127.0.0.1</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>Created 10 events...</strong></span>
<span class="strong"><strong>Created 83 events...</strong></span>
<span class="strong"><strong>Created 75 events...</strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><p>After about 10 seconds, you should start seeing the model predictions being printed to the streaming application console, similar to those shown here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>14/11/16 14:54:00 INFO StreamingLinearRegressionWithSGD: Model updated at time 1416142440000 ms</strong></span>
<span class="strong"><strong>14/11/16 14:54:00 INFO StreamingLinearRegressionWithSGD: Current model: weights, [0.05160959387864821,0.05122747155689144,-0.17224086785756998,0.05822993392274008,0.07848094246845688,-0.1298315806501979,0.006059323642394124, ...</strong></span>
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>14/11/16 14:54:00 INFO JobScheduler: Finished job streaming job 1416142440000 ms.0 from job set of time 1416142440000 ms</strong></span>
<span class="strong"><strong>14/11/16 14:54:00 INFO JobScheduler: Starting job streaming job 1416142440000 ms.1 from job set of time 1416142440000 ms</strong></span>
<span class="strong"><strong>14/11/16 14:54:00 INFO SparkContext: Starting job: take at DStream.scala:608</strong></span>
<span class="strong"><strong>14/11/16 14:54:00 INFO DAGScheduler: Got job 3 (take at DStream.scala:608) with 1 output partitions (allowLocal=true)</strong></span>
<span class="strong"><strong>14/11/16 14:54:00 INFO DAGScheduler: Final stage: Stage 3(take at DStream.scala:608)</strong></span>
<span class="strong"><strong>14/11/16 14:54:00 INFO DAGScheduler: Parents of final stage: List()</strong></span>
<span class="strong"><strong>14/11/16 14:54:00 INFO DAGScheduler: Missing parents: List()</strong></span>
<span class="strong"><strong>14/11/16 14:54:00 INFO DAGScheduler: Computing the requested partition locally</strong></span>
<span class="strong"><strong>14/11/16 14:54:00 INFO SparkContext: Job finished: take at DStream.scala:608, took 0.014064 s</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>Time: 1416142440000 ms</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>-2.0851430248312526</strong></span>
<span class="strong"><strong>4.609405228401022</strong></span>
<span class="strong"><strong>2.817934589675725</strong></span>
<span class="strong"><strong>3.3526557917118813</strong></span>
<span class="strong"><strong>4.624236379848475</strong></span>
<span class="strong"><strong>-2.3509098272485156</strong></span>
<span class="strong"><strong>-0.7228551577759544</strong></span>
<span class="strong"><strong>2.914231548990703</strong></span>
<span class="strong"><strong>0.896926579927631</strong></span>
<span class="strong"><strong>1.1968162940541283</strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><p>Congratulations! You've<a id="id1014" class="indexterm"></a> created your first streaming online learning model!</p><p>You<a id="id1015" class="indexterm"></a> can shut down the streaming application (and, optionally, the producer) by pressing <span class="emphasis"><em>Ctrl</em></span> + <span class="emphasis"><em>C</em></span> in each terminal window.</p></div></div><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec101"></a>Streaming K-means</h3></div></div></div><p>MLlib<a id="id1016" class="indexterm"></a> also includes a streaming<a id="id1017" class="indexterm"></a> version of K-means clustering; this is called <code class="literal">StreamingKMeans</code>. This model is an extension of the mini-batch K-means algorithm where the model is updated with each batch based on a combination between the cluster centers computed from the previous batches and the cluster centers computed for the current batch.</p><p><code class="literal">StreamingKMeans</code> supports a <span class="emphasis"><em>forgetfulness</em></span> parameter <span class="emphasis"><em>alpha</em></span> (set using the <code class="literal">setDecayFactor</code> method); this controls how aggressive the model is in giving weight to newer data. An alpha value of 0 means the model will only use new data, while with an alpha value of <code class="literal">1</code>, all data since the beginning of the streaming application will be used.</p><p>We will not cover streaming K-means further here (the Spark documentation at <a class="ulink" href="http://spark.apache.org/docs/latest/mllib-clustering.html#streaming-clustering" target="_blank">http://spark.apache.org/docs/latest/mllib-clustering.html#streaming-clustering</a> contains further detail and an example). However, perhaps you could try to adapt the preceding streaming regression data producer to generate input data for a <code class="literal">StreamingKMeans</code> model. You could also adapt the streaming regression application to use <code class="literal">StreamingKMeans</code>.</p><p>You can create the clustering data producer by first selecting a number of clusters, <span class="emphasis"><em>K</em></span>, and then generating each data point by:</p><div class="itemizedlist"><ul type="bullet"><li style="list-style-type: disc"><p>Randomly selecting a cluster index.</p></li><li style="list-style-type: disc"><p>Generating a random vector using specific normal distribution parameters for each cluster. That is, each of the <span class="emphasis"><em>K</em></span> clusters will have a mean and variance parameter, from which the random vectors will be generated using an approach similar to our preceding <code class="literal">generateRandomArray</code> function.</p></li></ul></div><p>In this way, each data point that belongs to the same cluster will be drawn from the same distribution, so our streaming clustering model should be able to learn the correct cluster centers over time.</p></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec70"></a>Online model evaluation</h2></div></div><hr /></div><p>Combining<a id="id1018" class="indexterm"></a> machine learning with Spark Streaming has many potential applications and use cases, including keeping a model or set of models up to date on new training data as it arrives, thus enabling them to adapt quickly to changing situations or contexts.</p><p>Another useful application is to track and compare the performance of multiple models in an online manner and, possibly, also perform model selection in real time so that the best performing model is always used to generate predictions on live data.</p><p>This can be used to do real-time "A/B testing" of models, or combined with more advanced online selection and learning techniques, such as Bayesian update approaches and bandit algorithms. It can also be used simply to monitor model performance in real time, thus being able to respond or adapt if performance degrades for some reason.</p><p>In this section, we will walk through a simple extension to our streaming regression example. In this example, we will compare the evolving error rate of two models with different parameters as they see more and more data in our input stream.</p><div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h3 class="title"><a id="ch10lvl2sec102"></a>Comparing model performance with Spark Streaming</h3></div></div></div><p>As we have <a id="id1019" class="indexterm"></a>used a known weight vector and intercept to generate the training data in our producer <a id="id1020" class="indexterm"></a>application, we would <a id="id1021" class="indexterm"></a>expect our model to eventually learn this underlying weight vector (in the absence of random noise, which we do not add for this example).</p><p>Therefore, we should see the model's error rate decrease over time, as it sees more and more data. We can also use standard regression error metrics to compare the performance of multiple models.</p><p>In this example, we will create two models with different learning rates, training them both <a id="id1022" class="indexterm"></a>on the same data stream. We will then make predictions<a id="id1023" class="indexterm"></a> for each model and measure the <span class="strong"><strong>mean-squared error</strong></span> (<span class="strong"><strong>MSE</strong></span>) and <span class="strong"><strong>root mean-squared error</strong></span> (<span class="strong"><strong>RMSE</strong></span>) metrics for each batch.</p><p>Our new monitored streaming model code is shown here:</p><div class="informalexample"><pre class="programlisting">/**
 * A streaming regression model that compares the model performance of two models, printing out metrics for
 * each batch
 */
object MonitoringStreamingModel {
  import org.apache.spark.SparkContext._

  def main(args: Array[String]) {

    val ssc = new StreamingContext("local[2]", "First Streaming App", Seconds(10))
    val stream = ssc.socketTextStream("localhost", 9999)

    val NumFeatures = 100
    val zeroVector = DenseVector.zeros[Double](NumFeatures)
    val model1 = new StreamingLinearRegressionWithSGD()
      .setInitialWeights(Vectors.dense(zeroVector.data))
      .setNumIterations(1)
      .setStepSize(0.01)

    val model2 = new StreamingLinearRegressionWithSGD()
      .setInitialWeights(Vectors.dense(zeroVector.data))
      .setNumIterations(1)
      .setStepSize(1.0)
// create a stream of labeled points
    val labeledStream = stream.map { event =&gt;
      val split = event.split("\t")
      val y = split(0).toDouble
      val features = split(1).split(",").map(_.toDouble)
      LabeledPoint(label = y, features = Vectors.dense(features))
    }</pre></div><p>Note that <a id="id1024" class="indexterm"></a>most of the preceding <a id="id1025" class="indexterm"></a>setup code is the same as our simple streaming model example. However, we <a id="id1026" class="indexterm"></a>created two instances of <code class="literal">StreamingLinearRegressionWithSGD</code>: one with a learning rate of <code class="literal">0.01</code> and one with the learning rate set to <code class="literal">1.0</code>.</p><p>Next, we will train each model on our input stream, and using Spark Streaming's <code class="literal">transform</code> function, we will create a new DStream that contains the error rates for each model:</p><div class="informalexample"><pre class="programlisting">    // train both models on the same stream
    model1.trainOn(labeledStream)
    model2.trainOn(labeledStream)

    // use transform to create a stream with model error rates
    val predsAndTrue = labeledStream.transform { rdd =&gt;
      val latest1 = model1.latestModel()
      val latest2 = model2.latestModel()
      rdd.map { point =&gt;
        val pred1 = latest1.predict(point.features)
        val pred2 = latest2.predict(point.features)
        (pred1 - point.label, pred2 - point.label)
      }
    }</pre></div><p>Finally, we will use <code class="literal">foreachRDD</code> to compute the MSE and RMSE metrics for each model and print them to the console:</p><div class="informalexample"><pre class="programlisting">    // print out the MSE and RMSE metrics for each model per batch
    predsAndTrue.foreachRDD { (rdd, time) =&gt;
      val mse1 = rdd.map { case (err1, err2) =&gt; err1 * err1 }.mean()
      val rmse1 = math.sqrt(mse1)
      val mse2 = rdd.map { case (err1, err2) =&gt; err2 * err2 }.mean()
      val rmse2 = math.sqrt(mse2)
      println(
        s"""
           |-------------------------------------------
           |Time: $time
           |-------------------------------------------
         """.stripMargin)
      println(s"MSE current batch: Model 1: $mse1; Model 2: $mse2")
      println(s"RMSE current batch: Model 1: $rmse1; Model 2: $rmse2")
      println("...\n")
    }

    ssc.start()
    ssc.awaitTermination()

  }
}</pre></div><p>If you terminated<a id="id1027" class="indexterm"></a> the producer earlier, start it again by executing <code class="literal">sbt run</code> and selecting <code class="literal">StreamingModelProducer</code>. Once the producer is running again, in your second<a id="id1028" class="indexterm"></a> terminal window, execute <code class="literal">sbt run</code> and choose the main class for <code class="literal">MonitoringStreamingModel</code>.</p><p>You<a id="id1029" class="indexterm"></a> should see the streaming program startup, and after about 10 seconds, the first batch will be processed, printing output similar to the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>14/11/16 14:56:11 INFO SparkContext: Job finished: mean at StreamingModel.scala:159, took 0.09122 s</strong></span>

<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>Time: 1416142570000 ms</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>

<span class="strong"><strong>MSE current batch: Model 1: 97.9475827857361; Model 2: 97.9475827857361</strong></span>
<span class="strong"><strong>RMSE current batch: Model 1: 9.896847113385965; Model 2: 9.896847113385965</strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><p>Since both models start with the same initial weight vector, we see that they both make the same predictions on this first batch and, therefore, have the same error.</p><p>If we leave the streaming program running for a few minutes, we should eventually see that one of the models has started converging, leading to a lower and lower error, while the other model has tended to diverge to a poorer model due to the overly high learning rate:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>14/11/16 14:57:30 INFO SparkContext: Job finished: mean at StreamingModel.scala:159, took 0.069175 s</strong></span>

<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>Time: 1416142650000 ms</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>

<span class="strong"><strong>MSE current batch: Model 1: 75.54543031658632; Model 2: 10318.213926882852</strong></span>
<span class="strong"><strong>RMSE current batch: Model 1: 8.691687426304878; Model 2: 101.57860959317593</strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><p>If you leave the program running for a number of minutes, you should eventually see the first model's error rate getting quite small:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>...</strong></span>
<span class="strong"><strong>14/11/16 17:27:00 INFO SparkContext: Job finished: mean at StreamingModel.scala:159, took 0.037856 s</strong></span>

<span class="strong"><strong>-------------------------------------------</strong></span>
<span class="strong"><strong>Time: 1416151620000 ms</strong></span>
<span class="strong"><strong>-------------------------------------------</strong></span>

<span class="strong"><strong>MSE current batch: Model 1: 6.551475362521364; Model 2: 1.057088005456417E26</strong></span>
<span class="strong"><strong>RMSE current batch: Model 1: 2.559584998104451; Model 2: 1.0281478519436867E13</strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><div class="tip" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a id="tip66"></a>Tip</h3><p>Note<a id="id1030" class="indexterm"></a> again that due<a id="id1031" class="indexterm"></a> to random data generation, you might see different results, but the overall result should be the sameâ€”in the first batch, the models will have the same error, and subsequently, the first model should start to generate to a smaller and smaller error.</p></div></div></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">
<div class="section" lang="en" xml:lang="en"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="ch10lvl1sec71"></a>Summary</h2></div></div><hr /></div><p>In this chapter, we connected some of the dots between online machine learning and streaming data analysis. We introduced the Spark Streaming library and API for continuous processing of data streams based on familiar RDD functionality and worked through examples of streaming analytics applications that illustrate this functionality.</p><p>Finally, we used MLlib's streaming regression model in a streaming application that involves computing and comparing model performance on a stream of input feature vectors.</p></div></div></div></div>
ï»¿<div class="reader-container col-sm-12 col-lg-offset-1 col-lg-10 col-xl-offset-2 col-xl-8"><div class="row"><div style="position:relative;" class="book-content">

    <div id="backindex">
      <h1 class="title">Index</h1>
      <h2>A</h2>
      <ul>
        <li>Abstract Window Toolkit (AWT) / <a href="#ch08lvl1sec55" title="Extracting facial images as vectors" class="link">Extracting facial images as vectors</a></li>
        <li>accumulators / <a href="#ch01lvl1sec10" title="Broadcast variables and accumulators" class="link">Broadcast variables and accumulators</a></li>
        <li>additive smoothing<ul><li>URL / <a href="#ch05lvl1sec38" title="The naÃ¯ve Bayes model" class="link">The naÃ¯ve Bayes model</a></li></ul></li>
        <li>agglomerative clustering<ul><li>about / <a href="#ch07lvl1sec47" title="Hierarchical clustering" class="link">Hierarchical clustering</a></li></ul></li>
        <li>alpha parameter / <a href="#ch04lvl1sec29" title="Training a model using implicit feedback data" class="link">Training a model using implicit feedback data</a></li>
        <li>Alternating Least Squares (ALS) / <a href="#ch04lvl1sec27" title="Alternating least squares" class="link">Alternating least squares</a></li>
        <li>Amazon AWS public datasets<ul><li>URL / <a href="#ch03lvl1sec22" title="Accessing publicly available datasets" class="link">Accessing publicly available datasets</a></li><li>about / <a href="#ch03lvl1sec22" title="Accessing publicly available datasets" class="link">Accessing publicly available datasets</a></li></ul></li>
        <li>Amazon EC2<ul><li>Spark, running on / <a href="#ch01lvl1sec14" title="Getting Spark running on Amazon EC2" class="link">Getting Spark running on Amazon EC2</a></li><li>EC2 Spark cluster, launching / <a href="#ch01lvl1sec14" title="Launching an EC2 Spark cluster" class="link">Launching an EC2 Spark cluster</a></li></ul></li>
        <li>Amazon Web Services account<ul><li>URL / <a href="#ch01lvl1sec14" title="Getting Spark running on Amazon EC2" class="link">Getting Spark running on Amazon EC2</a></li></ul></li>
        <li>Anaconda<ul><li>URL / <a href="#ch03lvl1sec23" title="Exploring and visualizing your data" class="link">Exploring and visualizing your data</a></li></ul></li>
        <li>analytics<ul><li>streaming / <a href="#ch10lvl1sec68" title="Streaming analytics" class="link">Streaming analytics</a></li></ul></li>
        <li>architecture, machine learning system / <a href="#ch02lvl1sec20" title="An architecture for a machine learning system" class="link">An architecture for a machine learning system</a></li>
        <li>area under ROC curve (AUC) / <a href="#ch05lvl1sec37" title="Evaluating the performance of classification models" class="link">Evaluating the performance of classification models</a></li>
        <li>AUC, classification models / <a href="#ch05lvl1sec37" title="ROC curve and AUC" class="link">ROC curve and AUC</a></li>
        <li>AWS console<ul><li>URL / <a href="#ch01lvl1sec14" title="Getting Spark running on Amazon EC2" class="link">Getting Spark running on Amazon EC2</a></li></ul></li>
      </ul>
      <h2>B</h2>
      <ul>
        <li>bad data<ul><li>filling / <a href="#ch03lvl1sec24" title="Filling in bad or missing data" class="link">Filling in bad or missing data</a></li></ul></li>
        <li>bag-of-words model<ul><li>about / <a href="#ch09lvl1sec61" title="Term weighting schemes" class="link">Term weighting schemes</a></li></ul></li>
        <li>base form / <a href="#ch09lvl1sec61" title="A note about stemming" class="link">A note about stemming</a></li>
        <li>basic streaming application<ul><li>creating / <a href="#ch10lvl1sec68" title="Creating a basic streaming application" class="link">Creating a basic streaming application</a></li></ul></li>
        <li>batch interval<ul><li>about / <a href="#ch10lvl1sec67" title="An introduction to Spark Streaming" class="link">An introduction to Spark Streaming</a></li></ul></li>
        <li>bike sharing dataset<ul><li>features, extracting from / <a href="#ch06lvl1sec41" title="Extracting features from the bike sharing dataset" class="link">Extracting features from the bike sharing dataset</a></li><li>regression model, training on / <a href="#ch06lvl1sec42" title="Training a regression model on the bike sharing dataset" class="link">Training a regression model on the bike sharing dataset</a></li><li>performance metrics, computing on / <a href="#ch06lvl1sec43" title="Computing performance metrics on the bike sharing dataset" class="link">Computing performance metrics on the bike sharing dataset</a></li></ul></li>
        <li>Breeze library / <a href="#ch07lvl1sec50" title="Interpreting the movie clusters" class="link">Interpreting the movie clusters</a></li>
        <li>broadcast variable / <a href="#ch01lvl1sec10" title="Broadcast variables and accumulators" class="link">Broadcast variables and accumulators</a></li>
        <li>built-in evaluation functions<ul><li>using / <a href="#ch04lvl1sec31" title="Using MLlib's built-in evaluation functions" class="link">Using MLlib's built-in evaluation functions</a></li><li>RMSE / <a href="#ch04lvl1sec31" title="RMSE and MSE" class="link">RMSE and MSE</a></li><li>MSE / <a href="#ch04lvl1sec31" title="RMSE and MSE" class="link">RMSE and MSE</a></li><li>MAP / <a href="#ch04lvl1sec31" title="MAP" class="link">MAP</a></li></ul></li>
        <li>business use cases, machine learning system<ul><li>about / <a href="#ch02lvl1sec17" title="Business use cases for a machine learning system" class="link">Business use cases for a machine learning system</a></li><li>personalization / <a href="#ch02lvl1sec17" title="Personalization" class="link">Personalization</a></li><li>targeted marketing / <a href="#ch02lvl1sec17" title="Targeted marketing and customer segmentation" class="link">Targeted marketing and customer segmentation</a></li><li>customer segmentation / <a href="#ch02lvl1sec17" title="Targeted marketing and customer segmentation" class="link">Targeted marketing and customer segmentation</a></li><li>predictive modelling and analytics / <a href="#ch02lvl1sec17" title="Predictive modeling and analytics" class="link">Predictive modeling and analytics</a></li></ul></li>
      </ul>
      <h2>C</h2>
      <ul>
        <li>categorical features / <a href="#ch03lvl1sec25" title="Categorical features" class="link">Categorical features</a><ul><li>timestamps, transforming into / <a href="#ch03lvl1sec25" title="Transforming timestamps into categorical features" class="link">Transforming timestamps into categorical features</a></li></ul></li>
        <li>classification model<ul><li>about / <a href="#ch02lvl1sec17" title="Predictive modeling and analytics" class="link">Predictive modeling and analytics</a></li></ul></li>
        <li>classification models<ul><li>types / <a href="#ch05lvl1sec33" title="Types of classification models" class="link">Types of classification models</a></li><li>linear models / <a href="#ch05lvl1sec33" title="Linear models" class="link">Linear models</a></li><li>naÃ¯ve Bayes model / <a href="#ch05lvl1sec33" title="The naÃ¯ve Bayes model" class="link">The naÃ¯ve Bayes model</a></li><li>decision trees / <a href="#ch05lvl1sec33" title="Decision trees" class="link">Decision trees</a></li><li>training / <a href="#ch05lvl1sec35" title="Training classification models" class="link">Training classification models</a></li><li>training, on Kaggle/StumbleUpon evergreen classification dataset / <a href="#ch05lvl1sec35" title="Training a classification model on the Kaggle/StumbleUpon evergreen classification dataset" class="link">Training a classification model on the Kaggle/StumbleUpon evergreen classification dataset</a></li><li>using / <a href="#ch05lvl1sec36" title="Using classification models" class="link">Using classification models</a></li><li>predictions generating, for Kaggle/StumbleUpon evergreen classification dataset / <a href="#ch05lvl1sec36" title="Generating predictions for the Kaggle/StumbleUpon evergreen classification dataset" class="link">Generating predictions for the Kaggle/StumbleUpon evergreen classification dataset</a></li></ul></li>
        <li>clustering evaluation<ul><li>URL / <a href="#ch07lvl1sec51" title="Internal evaluation metrics" class="link">Internal evaluation metrics</a></li></ul></li>
        <li>clustering model<ul><li>training / <a href="#ch07lvl1sec49" title="Training a clustering model" class="link">Training a clustering model</a></li><li>training, on MovieLens dataset / <a href="#ch07lvl1sec49" title="Training a clustering model on the MovieLens dataset" class="link">Training a clustering model on the MovieLens dataset</a></li><li>used, for making predictions / <a href="#ch07lvl1sec50" title="Making predictions using a clustering model" class="link">Making predictions using a clustering model</a></li></ul></li>
        <li>clustering models<ul><li>types / <a href="#ch07lvl1sec47" title="Types of clustering models" class="link">Types of clustering models</a></li><li>K-means clustering / <a href="#ch07lvl1sec47" title="K-means clustering" class="link">K-means clustering</a></li><li>mixture model / <a href="#ch07lvl1sec47" title="Mixture models" class="link">Mixture models</a></li><li>hierarchical clustering / <a href="#ch07lvl1sec47" title="Hierarchical clustering" class="link">Hierarchical clustering</a></li><li>parameters, tuning for / <a href="#ch07lvl1sec52" title="Tuning parameters for clustering models" class="link">Tuning parameters for clustering models</a></li><li>K, selecting through cross-validation / <a href="#ch07lvl1sec52" title="Selecting K through cross-validation" class="link">Selecting K through cross-validation</a></li></ul></li>
        <li>cluster predictions<ul><li>interpreting, on MovieLens dataset / <a href="#ch07lvl1sec50" title="Interpreting cluster predictions on the MovieLens dataset" class="link">Interpreting cluster predictions on the MovieLens dataset</a></li></ul></li>
        <li>collaborative filtering<ul><li>about / <a href="#ch04lvl1sec27" title="Collaborative filtering" class="link">Collaborative filtering</a></li><li>matrix factorization / <a href="#ch04lvl1sec27" title="Matrix factorization" class="link">Matrix factorization</a></li></ul></li>
        <li>comma-separated-value (CSV) / <a href="#ch01lvl1sec11" title="The first step to a Spark program in Scala" class="link">The first step to a Spark program in Scala</a></li>
        <li>components, data-driven machine learning system<ul><li>about / <a href="#ch02lvl1sec19" title="The components of a data-driven machine learning system" class="link">The components of a data-driven machine learning system</a></li><li>data ingestion / <a href="#ch02lvl1sec19" title="Data ingestion and storage" class="link">Data ingestion and storage</a></li><li>data storage / <a href="#ch02lvl1sec19" title="Data ingestion and storage" class="link">Data ingestion and storage</a></li><li>data cleansing / <a href="#ch02lvl1sec19" title="Data cleansing and transformation" class="link">Data cleansing and transformation</a></li><li>data transformation / <a href="#ch02lvl1sec19" title="Data cleansing and transformation" class="link">Data cleansing and transformation</a></li><li>model training / <a href="#ch02lvl1sec19" title="Model training and testing loop" class="link">Model training and testing loop</a></li><li>testing loop / <a href="#ch02lvl1sec19" title="Model training and testing loop" class="link">Model training and testing loop</a></li><li>model deployment / <a href="#ch02lvl1sec19" title="Model deployment and integration" class="link">Model deployment and integration</a></li><li>model integration / <a href="#ch02lvl1sec19" title="Model deployment and integration" class="link">Model deployment and integration</a></li><li>model monitoring / <a href="#ch02lvl1sec19" title="Model monitoring and feedback" class="link">Model monitoring and feedback</a></li><li>model feedback / <a href="#ch02lvl1sec19" title="Model monitoring and feedback" class="link">Model monitoring and feedback</a></li><li>batch, versus real time / <a href="#ch02lvl1sec19" title="Batch versus real time" class="link">Batch versus real time</a></li></ul></li>
        <li>content-based filtering / <a href="#ch04lvl1sec27" title="Content-based filtering" class="link">Content-based filtering</a></li>
        <li>convergence<ul><li>about / <a href="#ch07lvl1sec47" title="K-means clustering" class="link">K-means clustering</a></li></ul></li>
        <li>corpus<ul><li>about / <a href="#ch09lvl1sec61" title="Term weighting schemes" class="link">Term weighting schemes</a></li></ul></li>
        <li>correct form of data<ul><li>using / <a href="#ch05lvl1sec38" title="Using the correct form of data" class="link">Using the correct form of data</a></li></ul></li>
        <li>cross-validation<ul><li>K, selecting through / <a href="#ch07lvl1sec52" title="Selecting K through cross-validation" class="link">Selecting K through cross-validation</a></li></ul></li>
        <li>cross validation<ul><li>about / <a href="#ch02lvl1sec19" title="Model training and testing loop" class="link">Model training and testing loop</a>, <a href="#ch05lvl1sec38" title="Cross-validation" class="link">Cross-validation</a></li><li>URL / <a href="#ch05lvl1sec38" title="Cross-validation" class="link">Cross-validation</a></li></ul></li>
        <li>customer segmentation<ul><li>about / <a href="#ch02lvl1sec17" title="Targeted marketing and customer segmentation" class="link">Targeted marketing and customer segmentation</a></li></ul></li>
      </ul>
      <h2>D</h2>
      <ul>
        <li>data<ul><li>exploring / <a href="#ch03lvl1sec23" title="Exploring and visualizing your data" class="link">Exploring and visualizing your data</a></li><li>visualizing / <a href="#ch03lvl1sec23" title="Exploring and visualizing your data" class="link">Exploring and visualizing your data</a></li><li>user dataset, exploring / <a href="#ch03lvl1sec23" title="Exploring the user dataset" class="link">Exploring the user dataset</a></li><li>movie dataset, exploring / <a href="#ch03lvl1sec23" title="Exploring the movie dataset" class="link">Exploring the movie dataset</a></li><li>rating dataset, exploring / <a href="#ch03lvl1sec23" title="Exploring the rating dataset" class="link">Exploring the rating dataset</a></li><li>processing / <a href="#ch03lvl1sec24" title="Processing and transforming your data" class="link">Processing and transforming your data</a></li><li>transforming / <a href="#ch03lvl1sec24" title="Processing and transforming your data" class="link">Processing and transforming your data</a></li><li>features, extracting from / <a href="#ch03lvl1sec25" title="Extracting useful features from your data" class="link">Extracting useful features from your data</a>, <a href="#ch05lvl1sec34" title="Extracting the right features from your data" class="link">Extracting the right features from your data</a>, <a href="#ch06lvl1sec41" title="Extracting the right features from your data" class="link">Extracting the right features from your data</a>, <a href="#ch07lvl1sec48" title="Extracting the right features from your data" class="link">Extracting the right features from your data</a></li><li>projecting, PCA used / <a href="#ch08lvl1sec57" title="Projecting data using PCA on the LFW dataset" class="link">Projecting data using PCA on the LFW dataset</a></li></ul></li>
        <li>data-driven machine learning system<ul><li>components / <a href="#ch02lvl1sec19" title="The components of a data-driven machine learning system" class="link">The components of a data-driven machine learning system</a>, <a href="#ch02lvl1sec19" title="Data ingestion and storage" class="link">Data ingestion and storage</a>, <a href="#ch02lvl1sec19" title="Data cleansing and transformation" class="link">Data cleansing and transformation</a>, <a href="#ch02lvl1sec19" title="Model training and testing loop" class="link">Model training and testing loop</a>, <a href="#ch02lvl1sec19" title="Model monitoring and feedback" class="link">Model monitoring and feedback</a>, <a href="#ch02lvl1sec19" title="Batch versus real time" class="link">Batch versus real time</a></li></ul></li>
        <li>data cleansing / <a href="#ch02lvl1sec19" title="Data cleansing and transformation" class="link">Data cleansing and transformation</a></li>
        <li>data ingestion / <a href="#ch02lvl1sec19" title="Data ingestion and storage" class="link">Data ingestion and storage</a></li>
        <li>datasets<ul><li>accessing / <a href="#ch03lvl1sec22" title="Accessing publicly available datasets" class="link">Accessing publicly available datasets</a></li><li>MovieLens 100k dataset / <a href="#ch03lvl1sec22" title="The MovieLens 100k dataset" class="link">The MovieLens 100k dataset</a></li></ul></li>
        <li>data sources<ul><li>UCI Machine Learning Repository / <a href="#ch03lvl1sec22" title="Accessing publicly available datasets" class="link">Accessing publicly available datasets</a></li><li>Amazon AWS public datasets / <a href="#ch03lvl1sec22" title="Accessing publicly available datasets" class="link">Accessing publicly available datasets</a></li><li>Kaggle / <a href="#ch03lvl1sec22" title="Accessing publicly available datasets" class="link">Accessing publicly available datasets</a></li><li>KDnuggets / <a href="#ch03lvl1sec22" title="Accessing publicly available datasets" class="link">Accessing publicly available datasets</a></li></ul></li>
        <li>data storage / <a href="#ch02lvl1sec19" title="Data ingestion and storage" class="link">Data ingestion and storage</a></li>
        <li>data transformation / <a href="#ch02lvl1sec19" title="Data cleansing and transformation" class="link">Data cleansing and transformation</a></li>
        <li>decision tree / <a href="#ch06lvl1sec43" title="Decision tree" class="link">Decision tree</a></li>
        <li>decision trees / <a href="#ch05lvl1sec33" title="Decision trees" class="link">Decision trees</a><ul><li>about / <a href="#ch05lvl1sec38" title="Decision trees" class="link">Decision trees</a></li><li>tree depth, tuning / <a href="#ch05lvl1sec38" title="Tuning tree depth and impurity" class="link">Tuning tree depth and impurity</a></li><li>impurity, tuning / <a href="#ch05lvl1sec38" title="Tuning tree depth and impurity" class="link">Tuning tree depth and impurity</a></li><li>used, for regression / <a href="#ch06lvl1sec40" title="Decision trees for regression" class="link">Decision trees for regression</a></li></ul></li>
        <li>derived features<ul><li>about / <a href="#ch03lvl1sec25" title="Derived features" class="link">Derived features</a></li><li>timestamps, transforming into categorical features / <a href="#ch03lvl1sec25" title="Transforming timestamps into categorical features" class="link">Transforming timestamps into categorical features</a></li></ul></li>
        <li>dimensionality reduction<ul><li>types / <a href="#ch08lvl1sec54" title="Types of dimensionality reduction" class="link">Types of dimensionality reduction</a></li><li>PCA / <a href="#ch08lvl1sec54" title="Principal Components Analysis" class="link">Principal Components Analysis</a></li><li>SVD / <a href="#ch08lvl1sec54" title="Singular Value Decomposition" class="link">Singular Value Decomposition</a></li><li>relationship, to matrix factorization / <a href="#ch08lvl1sec54" title="Relationship with matrix factorization" class="link">Relationship with matrix factorization</a></li><li>clustering as / <a href="#ch08lvl1sec54" title="Clustering as dimensionality reduction" class="link">Clustering as dimensionality reduction</a></li></ul></li>
        <li>dimensionality reduction model<ul><li>training / <a href="#ch08lvl1sec56" title="Training a dimensionality reduction model" class="link">Training a dimensionality reduction model</a></li><li>PCA running, on LFW dataset / <a href="#ch08lvl1sec56" title="Running PCA on the LFW dataset" class="link">Running PCA on the LFW dataset</a></li><li>using / <a href="#ch08lvl1sec57" title="Using a dimensionality reduction model" class="link">Using a dimensionality reduction model</a></li><li>data projecting, PCA used / <a href="#ch08lvl1sec57" title="Projecting data using PCA on the LFW dataset" class="link">Projecting data using PCA on the LFW dataset</a></li><li>PCA and SVD, relationship between / <a href="#ch08lvl1sec57" title="The relationship between PCA and SVD" class="link">The relationship between PCA and SVD</a></li></ul></li>
        <li>dimensionality reduction models<ul><li>evaluating / <a href="#ch08lvl1sec58" title="Evaluating dimensionality reduction models" class="link">Evaluating dimensionality reduction models</a></li><li>k, evaluating for SVD / <a href="#ch08lvl1sec58" title="Evaluating k for SVD on the LFW dataset" class="link">Evaluating k for SVD on the LFW dataset</a></li></ul></li>
        <li>discretized stream<ul><li>about / <a href="#ch10lvl1sec67" title="An introduction to Spark Streaming" class="link">An introduction to Spark Streaming</a></li></ul></li>
        <li>distributed vector representations<ul><li>about / <a href="#ch09lvl1sec64" title="Word2Vec models" class="link">Word2Vec models</a></li></ul></li>
        <li>divisive clustering<ul><li>about / <a href="#ch07lvl1sec47" title="Hierarchical clustering" class="link">Hierarchical clustering</a></li></ul></li>
        <li>document similarity<ul><li>with 20 Newsgroups dataset / <a href="#ch09lvl1sec62" title="Document similarity with the 20 Newsgroups dataset and TF-IDF features" class="link">Document similarity with the 20 Newsgroups dataset and TF-IDF features</a></li><li>with TF-IDF features / <a href="#ch09lvl1sec62" title="Document similarity with the 20 Newsgroups dataset and TF-IDF features" class="link">Document similarity with the 20 Newsgroups dataset and TF-IDF features</a></li></ul></li>
        <li>DStream<ul><li>about / <a href="#ch10lvl1sec67" title="An introduction to Spark Streaming" class="link">An introduction to Spark Streaming</a></li><li>actions / <a href="#ch10lvl1sec67" title="Actions" class="link">Actions</a></li></ul></li>
      </ul>
      <h2>E</h2>
      <ul>
        <li>EC2 Spark cluster<ul><li>launching / <a href="#ch01lvl1sec14" title="Launching an EC2 Spark cluster" class="link">Launching an EC2 Spark cluster</a></li></ul></li>
        <li>Eigenfaces<ul><li>visualizing / <a href="#ch08lvl1sec56" title="Visualizing the Eigenfaces" class="link">Visualizing the Eigenfaces</a></li><li>about / <a href="#ch08lvl1sec56" title="Visualizing the Eigenfaces" class="link">Visualizing the Eigenfaces</a></li><li>URL / <a href="#ch08lvl1sec56" title="Visualizing the Eigenfaces" class="link">Visualizing the Eigenfaces</a></li><li>interpreting / <a href="#ch08lvl1sec56" title="Interpreting the Eigenfaces" class="link">Interpreting the Eigenfaces</a></li></ul></li>
        <li>ensemble methods / <a href="#ch02lvl1sec19" title="Model training and testing loop" class="link">Model training and testing loop</a></li>
        <li>evaluation metrics<ul><li>about / <a href="#ch04lvl1sec31" title="Evaluating the performance of recommendation models" class="link">Evaluating the performance of recommendation models</a></li></ul></li>
        <li>explicit matrix factorization / <a href="#ch04lvl1sec27" title="Explicit matrix factorization" class="link">Explicit matrix factorization</a></li>
        <li>external evaluation metrics / <a href="#ch07lvl1sec51" title="External evaluation metrics" class="link">External evaluation metrics</a></li>
      </ul>
      <h2>F</h2>
      <ul>
        <li>face data<ul><li>exploring / <a href="#ch08lvl1sec55" title="Exploring the face data" class="link">Exploring the face data</a></li><li>visualizing / <a href="#ch08lvl1sec55" title="Visualizing the face data" class="link">Visualizing the face data</a></li></ul></li>
        <li>facial images, as vectors<ul><li>extracting / <a href="#ch08lvl1sec55" title="Extracting facial images as vectors" class="link">Extracting facial images as vectors</a></li><li>images, loading / <a href="#ch08lvl1sec55" title="Loading images" class="link">Loading images</a></li><li>grayscale, converting to / <a href="#ch08lvl1sec55" title="Converting to grayscale and resizing the images" class="link">Converting to grayscale and resizing the images</a></li><li>images, resizing / <a href="#ch08lvl1sec55" title="Converting to grayscale and resizing the images" class="link">Converting to grayscale and resizing the images</a></li><li>feature vectors, extracting / <a href="#ch08lvl1sec55" title="Extracting feature vectors" class="link">Extracting feature vectors</a></li></ul></li>
        <li>false positive rate (FPR) / <a href="#ch05lvl1sec37" title="ROC curve and AUC" class="link">ROC curve and AUC</a></li>
        <li>feature extraction<ul><li>packages, used for / <a href="#ch03lvl1sec25" title="Using packages for feature extraction" class="link">Using packages for feature extraction</a></li></ul></li>
        <li>feature extraction techniques<ul><li>term weighting schemes / <a href="#ch09lvl1sec61" title="Term weighting schemes" class="link">Term weighting schemes</a></li><li>feature hashing / <a href="#ch09lvl1sec61" title="Feature hashing" class="link">Feature hashing</a></li><li>TF-IDF features, extracting from 20 Newsgroups dataset / <a href="#ch09lvl1sec61" title="Extracting the TF-IDF features from the 20 Newsgroups dataset" class="link">Extracting the TF-IDF features from the 20 Newsgroups dataset</a></li></ul></li>
        <li>feature hashing / <a href="#ch09lvl1sec61" title="Feature hashing" class="link">Feature hashing</a></li>
        <li>features<ul><li>extracting, from data / <a href="#ch03lvl1sec25" title="Extracting useful features from your data" class="link">Extracting useful features from your data</a>, <a href="#ch05lvl1sec34" title="Extracting the right features from your data" class="link">Extracting the right features from your data</a>, <a href="#ch06lvl1sec41" title="Extracting the right features from your data" class="link">Extracting the right features from your data</a>, <a href="#ch07lvl1sec48" title="Extracting the right features from your data" class="link">Extracting the right features from your data</a>, <a href="#ch08lvl1sec55" title="Extracting the right features from your data" class="link">Extracting the right features from your data</a>, <a href="#ch09lvl1sec61" title="Extracting the right features from your data" class="link">Extracting the right features from your data</a></li><li>about / <a href="#ch03lvl1sec25" title="Extracting useful features from your data" class="link">Extracting useful features from your data</a></li><li>numerical features / <a href="#ch03lvl1sec25" title="Extracting useful features from your data" class="link">Extracting useful features from your data</a>, <a href="#ch03lvl1sec25" title="Numerical features" class="link">Numerical features</a></li><li>categorical features / <a href="#ch03lvl1sec25" title="Extracting useful features from your data" class="link">Extracting useful features from your data</a>, <a href="#ch03lvl1sec25" title="Categorical features" class="link">Categorical features</a></li><li>text features / <a href="#ch03lvl1sec25" title="Extracting useful features from your data" class="link">Extracting useful features from your data</a>, <a href="#ch03lvl1sec25" title="Text features" class="link">Text features</a></li><li>derived features / <a href="#ch03lvl1sec25" title="Derived features" class="link">Derived features</a></li><li>normalizing features / <a href="#ch03lvl1sec25" title="Normalizing features" class="link">Normalizing features</a></li><li>extracting / <a href="#ch04lvl1sec28" title="Extracting the right features from your data" class="link">Extracting the right features from your data</a></li><li>extracting, from MovieLens 100k dataset / <a href="#ch04lvl1sec28" title="Extracting features from the MovieLens 100k dataset" class="link">Extracting features from the MovieLens 100k dataset</a></li><li>extracting, from Kaggle/StumbleUpon evergreen classification dataset / <a href="#ch05lvl1sec34" title="Extracting features from the Kaggle/StumbleUpon evergreen classification dataset" class="link">Extracting features from the Kaggle/StumbleUpon evergreen classification dataset</a></li><li>extracting, from bike sharing dataset / <a href="#ch06lvl1sec41" title="Extracting features from the bike sharing dataset" class="link">Extracting features from the bike sharing dataset</a></li><li>extracting, from MovieLens dataset / <a href="#ch07lvl1sec48" title="Extracting features from the MovieLens dataset" class="link">Extracting features from the MovieLens dataset</a></li><li>extracting, from LFW dataset / <a href="#ch08lvl1sec55" title="Extracting features from the LFW dataset" class="link">Extracting features from the LFW dataset</a></li></ul></li>
        <li>features, extracting<ul><li>feature vectors, creating for linear model / <a href="#ch06lvl1sec41" title="Creating feature vectors for the linear model" class="link">Creating feature vectors for the linear model</a></li><li>feature vectors, creating for decision tree / <a href="#ch06lvl1sec41" title="Creating feature vectors for the decision tree" class="link">Creating feature vectors for the decision tree</a></li></ul></li>
        <li>features, MovieLens dataset<ul><li>movie genre labels, extracting / <a href="#ch07lvl1sec48" title="Extracting movie genre labels" class="link">Extracting movie genre labels</a></li><li>recommendation model, training / <a href="#ch07lvl1sec48" title="Training the recommendation model" class="link">Training the recommendation model</a></li><li>normalization / <a href="#ch07lvl1sec48" title="Normalization" class="link">Normalization</a></li></ul></li>
        <li>feature standardization, model performance / <a href="#ch05lvl1sec38" title="Feature standardization" class="link">Feature standardization</a></li>
        <li>feature vector<ul><li>about / <a href="#ch05lvl1sec34" title="Extracting the right features from your data" class="link">Extracting the right features from your data</a></li></ul></li>
        <li>feature vectors<ul><li>creating, for linear model / <a href="#ch06lvl1sec41" title="Creating feature vectors for the linear model" class="link">Creating feature vectors for the linear model</a></li><li>creating, for decision tree / <a href="#ch06lvl1sec41" title="Creating feature vectors for the decision tree" class="link">Creating feature vectors for the decision tree</a></li><li>extracting / <a href="#ch08lvl1sec55" title="Extracting feature vectors" class="link">Extracting feature vectors</a></li></ul></li>
      </ul>
      <h2>G</h2>
      <ul>
        <li>generalized linear models<ul><li>URL / <a href="#ch05lvl1sec33" title="Linear models" class="link">Linear models</a></li></ul></li>
        <li>general regularization<ul><li>URL / <a href="#ch05lvl1sec38" title="Regularization" class="link">Regularization</a></li></ul></li>
        <li>grayscale<ul><li>converting to / <a href="#ch08lvl1sec55" title="Converting to grayscale and resizing the images" class="link">Converting to grayscale and resizing the images</a></li></ul></li>
      </ul>
      <h2>H</h2>
      <ul>
        <li>Hadoop Distributed File System (HDFS) / <a href="#ch01lvl1sec08" title="Installing and setting up Spark locally" class="link">Installing and setting up Spark locally</a></li>
        <li>hash collisions<ul><li>about / <a href="#ch09lvl1sec61" title="Feature hashing" class="link">Feature hashing</a></li></ul></li>
        <li>hierarchical clustering / <a href="#ch07lvl1sec47" title="Hierarchical clustering" class="link">Hierarchical clustering</a></li>
        <li>hinge loss<ul><li>about / <a href="#ch05lvl1sec33" title="Linear support vector machines" class="link">Linear support vector machines</a></li></ul></li>
      </ul>
      <h2>I</h2>
      <ul>
        <li>images<ul><li>loading / <a href="#ch08lvl1sec55" title="Loading images" class="link">Loading images</a></li><li>resizing / <a href="#ch08lvl1sec55" title="Converting to grayscale and resizing the images" class="link">Converting to grayscale and resizing the images</a></li></ul></li>
        <li>implicit feedback data<ul><li>used, for training model / <a href="#ch04lvl1sec29" title="Training a model using implicit feedback data" class="link">Training a model using implicit feedback data</a></li></ul></li>
        <li>implicit matrix factorization / <a href="#ch04lvl1sec27" title="Implicit matrix factorization" class="link">Implicit matrix factorization</a></li>
        <li>initialization methods, K-means clustering / <a href="#ch07lvl1sec47" title="Initialization methods" class="link">Initialization methods</a></li>
        <li>internal evaluation metrics / <a href="#ch07lvl1sec51" title="Internal evaluation metrics" class="link">Internal evaluation metrics</a></li>
        <li>inverse document frequency<ul><li>about / <a href="#ch09lvl1sec61" title="Term weighting schemes" class="link">Term weighting schemes</a></li></ul></li>
        <li>IPython<ul><li>about / <a href="#ch03lvl1sec23" title="Exploring and visualizing your data" class="link">Exploring and visualizing your data</a></li></ul></li>
        <li>IPython Notebook<ul><li>URL / <a href="#ch03lvl1sec23" title="Exploring and visualizing your data" class="link">Exploring and visualizing your data</a></li></ul></li>
        <li>item recommendations<ul><li>about / <a href="#ch04lvl1sec30" title="Item recommendations" class="link">Item recommendations</a></li><li>similar movies, generating for MovieLens 100K dataset / <a href="#ch04lvl1sec30" title="Generating similar movies for the MovieLens 100k dataset" class="link">Generating similar movies for the MovieLens 100k dataset</a></li></ul></li>
      </ul>
      <h2>J</h2>
      <ul>
        <li>Java<ul><li>Spark program, writing in / <a href="#ch01lvl1sec12" title="The first step to a Spark program in Java" class="link">The first step to a Spark program in Java</a></li></ul></li>
        <li>Java Development Kit (JDK) / <a href="#ch01lvl1sec08" title="Installing and setting up Spark locally" class="link">Installing and setting up Spark locally</a></li>
        <li>Java Runtime Environment (JRE) / <a href="#ch01lvl1sec08" title="Installing and setting up Spark locally" class="link">Installing and setting up Spark locally</a></li>
      </ul>
      <h2>K</h2>
      <ul>
        <li>K<ul><li>selecting, through cross-validation / <a href="#ch07lvl1sec52" title="Selecting K through cross-validation" class="link">Selecting K through cross-validation</a></li></ul></li>
        <li>k<ul><li>evaluating, for SVD on LFW dataset / <a href="#ch08lvl1sec58" title="Evaluating k for SVD on the LFW dataset" class="link">Evaluating k for SVD on the LFW dataset</a></li></ul></li>
        <li>K-means<ul><li>streaming / <a href="#ch10lvl1sec69" title="Streaming K-means" class="link">Streaming K-means</a></li></ul></li>
        <li>K-means clustering<ul><li>about / <a href="#ch07lvl1sec47" title="K-means clustering" class="link">K-means clustering</a></li><li>initialization methods / <a href="#ch07lvl1sec47" title="Initialization methods" class="link">Initialization methods</a></li><li>variants / <a href="#ch07lvl1sec47" title="Variants" class="link">Variants</a></li></ul></li>
        <li>K-means ||<ul><li>about / <a href="#ch07lvl1sec47" title="Initialization methods" class="link">Initialization methods</a></li></ul></li>
        <li>Kaggle<ul><li>about / <a href="#ch03lvl1sec22" title="Accessing publicly available datasets" class="link">Accessing publicly available datasets</a></li><li>URL / <a href="#ch03lvl1sec22" title="Accessing publicly available datasets" class="link">Accessing publicly available datasets</a></li></ul></li>
        <li>Kaggle/StumbleUpon evergreen classification dataset<ul><li>features, extracting from / <a href="#ch05lvl1sec34" title="Extracting features from the Kaggle/StumbleUpon evergreen classification dataset" class="link">Extracting features from the Kaggle/StumbleUpon evergreen classification dataset</a></li><li>URL / <a href="#ch05lvl1sec34" title="Extracting features from the Kaggle/StumbleUpon evergreen classification dataset" class="link">Extracting features from the Kaggle/StumbleUpon evergreen classification dataset</a></li><li>classification models, training on / <a href="#ch05lvl1sec35" title="Training a classification model on the Kaggle/StumbleUpon evergreen classification dataset" class="link">Training a classification model on the Kaggle/StumbleUpon evergreen classification dataset</a></li><li>predictions, generating for / <a href="#ch05lvl1sec36" title="Generating predictions for the Kaggle/StumbleUpon evergreen classification dataset" class="link">Generating predictions for the Kaggle/StumbleUpon evergreen classification dataset</a></li></ul></li>
        <li>Kaggle competition evaluation page<ul><li>URL / <a href="#ch06lvl1sec43" title="Root Mean Squared Log Error" class="link">Root Mean Squared Log Error</a></li></ul></li>
        <li>KDnuggets<ul><li>about / <a href="#ch03lvl1sec22" title="Accessing publicly available datasets" class="link">Accessing publicly available datasets</a></li><li>URL / <a href="#ch03lvl1sec22" title="Accessing publicly available datasets" class="link">Accessing publicly available datasets</a></li></ul></li>
      </ul>
      <h2>L</h2>
      <ul>
        <li>L1 regularization / <a href="#ch06lvl1sec44" title="L1 regularization" class="link">L1 regularization</a></li>
        <li>L2 regularization<ul><li>URL / <a href="#ch05lvl1sec38" title="Regularization" class="link">Regularization</a></li></ul> / <a href="#ch06lvl1sec44" title="L2 regularization" class="link">L2 regularization</a></li>
        <li>label<ul><li>about / <a href="#ch05lvl1sec34" title="Extracting the right features from your data" class="link">Extracting the right features from your data</a></li></ul></li>
        <li>Labeled Faces in the Wild (LFW)<ul><li>about / <a href="#ch08lvl1sec55" title="Extracting the right features from your data" class="link">Extracting the right features from your data</a></li></ul></li>
        <li>lasso<ul><li>about / <a href="#ch06lvl1sec40" title="Least squares regression" class="link">Least squares regression</a></li></ul></li>
        <li>latent feature models<ul><li>about / <a href="#ch04lvl1sec27" title="Explicit matrix factorization" class="link">Explicit matrix factorization</a></li></ul></li>
        <li>Least Squares Regression / <a href="#ch06lvl1sec40" title="Least squares regression" class="link">Least squares regression</a></li>
        <li>LFW dataset<ul><li>features, extracting from / <a href="#ch08lvl1sec55" title="Extracting features from the LFW dataset" class="link">Extracting features from the LFW dataset</a></li><li>face data, exploring / <a href="#ch08lvl1sec55" title="Exploring the face data" class="link">Exploring the face data</a></li><li>face data, visualizing / <a href="#ch08lvl1sec55" title="Visualizing the face data" class="link">Visualizing the face data</a></li><li>facial images, extracting as vectors / <a href="#ch08lvl1sec55" title="Extracting facial images as vectors" class="link">Extracting facial images as vectors</a></li><li>normalization / <a href="#ch08lvl1sec55" title="Normalization" class="link">Normalization</a></li><li>PCA, running on / <a href="#ch08lvl1sec56" title="Running PCA on the LFW dataset" class="link">Running PCA on the LFW dataset</a></li><li>Eigenfaces, visualizing / <a href="#ch08lvl1sec56" title="Visualizing the Eigenfaces" class="link">Visualizing the Eigenfaces</a></li><li>Eigenfaces, interpreting / <a href="#ch08lvl1sec56" title="Interpreting the Eigenfaces" class="link">Interpreting the Eigenfaces</a></li><li>data projecting, PCA used / <a href="#ch08lvl1sec57" title="Projecting data using PCA on the LFW dataset" class="link">Projecting data using PCA on the LFW dataset</a></li><li>k evaluating, for SVD / <a href="#ch08lvl1sec58" title="Evaluating k for SVD on the LFW dataset" class="link">Evaluating k for SVD on the LFW dataset</a></li></ul></li>
        <li>line =&gt; line.size syntax<ul><li>about / <a href="#ch01lvl1sec10" title="Spark operations" class="link">Spark operations</a></li></ul></li>
        <li>linear model / <a href="#ch06lvl1sec43" title="Linear model" class="link">Linear model</a></li>
        <li>linear models<ul><li>about / <a href="#ch05lvl1sec33" title="Linear models" class="link">Linear models</a>, <a href="#ch05lvl1sec38" title="Linear models" class="link">Linear models</a></li><li>logistic regression / <a href="#ch05lvl1sec33" title="Logistic regression" class="link">Logistic regression</a></li><li>linear support vector machines / <a href="#ch05lvl1sec33" title="Linear support vector machines" class="link">Linear support vector machines</a></li><li>iterations / <a href="#ch05lvl1sec38" title="Iterations" class="link">Iterations</a></li><li>step size parameter / <a href="#ch05lvl1sec38" title="Step size" class="link">Step size</a></li><li>regularization / <a href="#ch05lvl1sec38" title="Regularization" class="link">Regularization</a></li></ul></li>
        <li>linear support vector machines / <a href="#ch05lvl1sec33" title="Linear support vector machines" class="link">Linear support vector machines</a></li>
        <li>log-transformed targets<ul><li>training, impact / <a href="#ch06lvl1sec44" title="Impact of training on log-transformed targets" class="link">Impact of training on log-transformed targets</a></li></ul></li>
        <li>logistic regression<ul><li>about / <a href="#ch05lvl1sec33" title="Linear models" class="link">Linear models</a>, <a href="#ch05lvl1sec33" title="Logistic regression" class="link">Logistic regression</a></li></ul></li>
      </ul>
      <h2>M</h2>
      <ul>
        <li>machine learning models, types<ul><li>about / <a href="#ch02lvl1sec18" title="Types of machine learning models" class="link">Types of machine learning models</a></li><li>supervised learning / <a href="#ch02lvl1sec18" title="Types of machine learning models" class="link">Types of machine learning models</a></li><li>unsupervised learning / <a href="#ch02lvl1sec18" title="Types of machine learning models" class="link">Types of machine learning models</a></li></ul></li>
        <li>machine learning system<ul><li>business use cases / <a href="#ch02lvl1sec17" title="Business use cases for a machine learning system" class="link">Business use cases for a machine learning system</a>, <a href="#ch02lvl1sec17" title="Personalization" class="link">Personalization</a>, <a href="#ch02lvl1sec17" title="Targeted marketing and customer segmentation" class="link">Targeted marketing and customer segmentation</a></li><li>architecture / <a href="#ch02lvl1sec20" title="An architecture for a machine learning system" class="link">An architecture for a machine learning system</a></li></ul></li>
        <li>MAE / <a href="#ch06lvl1sec43" title="Mean Absolute Error" class="link">Mean Absolute Error</a></li>
        <li>MAP<ul><li>about / <a href="#ch04lvl1sec31" title="MAP" class="link">MAP</a></li><li>calculating / <a href="#ch04lvl1sec31" title="MAP" class="link">MAP</a></li></ul></li>
        <li>map function / <a href="#ch01lvl1sec10" title="Broadcast variables and accumulators" class="link">Broadcast variables and accumulators</a></li>
        <li>MAPK<ul><li>URL / <a href="#ch04lvl1sec31" title="Mean average precision at K" class="link">Mean average precision at K</a></li></ul></li>
        <li>matrix factorization<ul><li>about / <a href="#ch04lvl1sec27" title="Matrix factorization" class="link">Matrix factorization</a>, <a href="#ch08lvl1sec54" title="Relationship with matrix factorization" class="link">Relationship with matrix factorization</a></li><li>explicit matrix factorization / <a href="#ch04lvl1sec27" title="Explicit matrix factorization" class="link">Explicit matrix factorization</a></li><li>implicit matrix factorization / <a href="#ch04lvl1sec27" title="Implicit matrix factorization" class="link">Implicit matrix factorization</a></li><li>Alternating Least Squares (ALS) / <a href="#ch04lvl1sec27" title="Alternating least squares" class="link">Alternating least squares</a></li></ul></li>
        <li>mean-squared error (MSE) / <a href="#ch10lvl1sec70" title="Comparing model performance with Spark Streaming" class="link">Comparing model performance with Spark Streaming</a></li>
        <li>Mean average precision at K (MAPK) / <a href="#ch04lvl1sec31" title="Mean average precision at K" class="link">Mean average precision at K</a></li>
        <li>Mean Squared Error (MSE) / <a href="#ch04lvl1sec31" title="Mean Squared Error" class="link">Mean Squared Error</a></li>
        <li>mini-batches<ul><li>about / <a href="#ch10lvl1sec67" title="An introduction to Spark Streaming" class="link">An introduction to Spark Streaming</a></li></ul></li>
        <li>missing data<ul><li>filling / <a href="#ch03lvl1sec24" title="Filling in bad or missing data" class="link">Filling in bad or missing data</a></li></ul></li>
        <li>mixture model / <a href="#ch07lvl1sec47" title="Mixture models" class="link">Mixture models</a></li>
        <li>MLlib<ul><li>used, for normalizing features / <a href="#ch03lvl1sec25" title="Using MLlib for feature normalization" class="link">Using MLlib for feature normalization</a></li></ul></li>
        <li>model<ul><li>training, on MovieLens 100k dataset / <a href="#ch04lvl1sec29" title="Training a model on the MovieLens 100k dataset" class="link">Training a model on the MovieLens 100k dataset</a></li><li>training, implicit feedback data used / <a href="#ch04lvl1sec29" title="Training a model using implicit feedback data" class="link">Training a model using implicit feedback data</a></li></ul></li>
        <li>model deployment / <a href="#ch02lvl1sec19" title="Model deployment and integration" class="link">Model deployment and integration</a></li>
        <li>model feedback<ul><li>about / <a href="#ch02lvl1sec19" title="Model monitoring and feedback" class="link">Model monitoring and feedback</a></li></ul></li>
        <li>model fitting<ul><li>about / <a href="#ch05lvl1sec33" title="Linear models" class="link">Linear models</a></li></ul></li>
        <li>model inputs<ul><li>rank / <a href="#ch04lvl1sec29" title="Training a model on the MovieLens 100k dataset" class="link">Training a model on the MovieLens 100k dataset</a></li><li>iterations / <a href="#ch04lvl1sec29" title="Training a model on the MovieLens 100k dataset" class="link">Training a model on the MovieLens 100k dataset</a></li><li>lambda / <a href="#ch04lvl1sec29" title="Training a model on the MovieLens 100k dataset" class="link">Training a model on the MovieLens 100k dataset</a></li></ul></li>
        <li>model integration / <a href="#ch02lvl1sec19" title="Model deployment and integration" class="link">Model deployment and integration</a></li>
        <li>model monitoring / <a href="#ch02lvl1sec19" title="Model monitoring and feedback" class="link">Model monitoring and feedback</a></li>
        <li>model parameters<ul><li>tuning / <a href="#ch05lvl1sec38" title="Tuning model parameters" class="link">Tuning model parameters</a>, <a href="#ch06lvl1sec44" title="Tuning model parameters" class="link">Tuning model parameters</a></li><li>linear models / <a href="#ch05lvl1sec38" title="Linear models" class="link">Linear models</a></li><li>decision trees / <a href="#ch05lvl1sec38" title="Decision trees" class="link">Decision trees</a></li><li>naÃ¯ve Bayes model / <a href="#ch05lvl1sec38" title="The naÃ¯ve Bayes model" class="link">The naÃ¯ve Bayes model</a></li><li>testing set, creating to evaluate parameters / <a href="#ch06lvl1sec44" title="Creating training and testing sets to evaluate parameters" class="link">Creating training and testing sets to evaluate parameters</a></li><li>training set, creating to evaluate parameters / <a href="#ch06lvl1sec44" title="Creating training and testing sets to evaluate parameters" class="link">Creating training and testing sets to evaluate parameters</a></li><li>parameter settings, impact for linear models / <a href="#ch06lvl1sec44" title="The impact of parameter settings for linear models" class="link">The impact of parameter settings for linear models</a></li><li>parameter settings, impact for decision tree / <a href="#ch06lvl1sec44" title="The impact of parameter settings for the decision tree" class="link">The impact of parameter settings for the decision tree</a></li></ul></li>
        <li>model performance<ul><li>improving / <a href="#ch05lvl1sec38" title="Improving model performance and tuning parameters" class="link">Improving model performance and tuning parameters</a>, <a href="#ch06lvl1sec44" title="Improving model performance and tuning parameters" class="link">Improving model performance and tuning parameters</a></li><li>feature standardization / <a href="#ch05lvl1sec38" title="Feature standardization" class="link">Feature standardization</a></li><li>additional features / <a href="#ch05lvl1sec38" title="Additional features" class="link">Additional features</a></li><li>correct form of data, using / <a href="#ch05lvl1sec38" title="Using the correct form of data" class="link">Using the correct form of data</a></li><li>comparing, with Spark Streaming / <a href="#ch10lvl1sec70" title="Comparing model performance with Spark Streaming" class="link">Comparing model performance with Spark Streaming</a></li></ul></li>
        <li>model selection<ul><li>about / <a href="#ch02lvl1sec19" title="Model training and testing loop" class="link">Model training and testing loop</a></li></ul></li>
        <li>model training / <a href="#ch02lvl1sec19" title="Model training and testing loop" class="link">Model training and testing loop</a></li>
        <li>movie clusters<ul><li>interpreting / <a href="#ch07lvl1sec50" title="Interpreting the movie clusters" class="link">Interpreting the movie clusters</a></li></ul></li>
        <li>movie dataset<ul><li>exploring / <a href="#ch03lvl1sec23" title="Exploring the movie dataset" class="link">Exploring the movie dataset</a></li></ul></li>
        <li>movie genre labels<ul><li>extracting / <a href="#ch07lvl1sec48" title="Extracting movie genre labels" class="link">Extracting movie genre labels</a></li></ul></li>
        <li>MovieLens 100K dataset<ul><li>similar movies, generating for / <a href="#ch04lvl1sec30" title="Generating similar movies for the MovieLens 100k dataset" class="link">Generating similar movies for the MovieLens 100k dataset</a></li></ul></li>
        <li>MovieLens 100k dataset / <a href="#ch03lvl1sec22" title="The MovieLens 100k dataset" class="link">The MovieLens 100k dataset</a><ul><li>URL / <a href="#ch03lvl1sec22" title="The MovieLens 100k dataset" class="link">The MovieLens 100k dataset</a></li><li>features, extracting from / <a href="#ch04lvl1sec28" title="Extracting features from the MovieLens 100k dataset" class="link">Extracting features from the MovieLens 100k dataset</a></li><li>movie recommendations, generating from / <a href="#ch04lvl1sec30" title="Generating movie recommendations from the MovieLens 100k dataset" class="link">Generating movie recommendations from the MovieLens 100k dataset</a></li></ul></li>
        <li>MovieLens dataset<ul><li>about / <a href="#ch03lvl1sec22" title="Accessing publicly available datasets" class="link">Accessing publicly available datasets</a></li><li>features, extracting from / <a href="#ch07lvl1sec48" title="Extracting features from the MovieLens dataset" class="link">Extracting features from the MovieLens dataset</a></li><li>clustering model, training on / <a href="#ch07lvl1sec49" title="Training a clustering model on the MovieLens dataset" class="link">Training a clustering model on the MovieLens dataset</a></li><li>cluster predictions, interpreting on / <a href="#ch07lvl1sec50" title="Interpreting cluster predictions on the MovieLens dataset" class="link">Interpreting cluster predictions on the MovieLens dataset</a></li><li>performance metrics, computing on / <a href="#ch07lvl1sec51" title="Computing performance metrics on the MovieLens dataset" class="link">Computing performance metrics on the MovieLens dataset</a></li></ul></li>
        <li>movie recommendations<ul><li>generating, from MovieLens 100k dataset / <a href="#ch04lvl1sec30" title="Generating movie recommendations from the MovieLens 100k dataset" class="link">Generating movie recommendations from the MovieLens 100k dataset</a></li></ul></li>
        <li>MovieStream<ul><li>about / <a href="#ch02lvl1sec16" title="Introducing MovieStream" class="link">Introducing MovieStream</a></li></ul></li>
        <li>MSE / <a href="#ch04lvl1sec31" title="RMSE and MSE" class="link">RMSE and MSE</a>, <a href="#ch06lvl1sec43" title="Mean Squared Error and Root Mean Squared Error" class="link">Mean Squared Error and Root Mean Squared Error</a></li>
      </ul>
      <h2>N</h2>
      <ul>
        <li>20 Newsgroups<ul><li>about / <a href="#ch09lvl1sec61" title="Extracting the TF-IDF features from the 20 Newsgroups dataset" class="link">Extracting the TF-IDF features from the 20 Newsgroups dataset</a></li><li>URL / <a href="#ch09lvl1sec61" title="Extracting the TF-IDF features from the 20 Newsgroups dataset" class="link">Extracting the TF-IDF features from the 20 Newsgroups dataset</a></li></ul></li>
        <li>20 Newsgroups data<ul><li>exploring / <a href="#ch09lvl1sec61" title="Exploring the 20 Newsgroups data" class="link">Exploring the 20 Newsgroups data</a></li></ul></li>
        <li>20 Newsgroups dataset<ul><li>TF-IDF features, extracting from / <a href="#ch09lvl1sec61" title="Extracting the TF-IDF features from the 20 Newsgroups dataset" class="link">Extracting the TF-IDF features from the 20 Newsgroups dataset</a></li><li>document similarity, used with / <a href="#ch09lvl1sec62" title="Document similarity with the 20 Newsgroups dataset and TF-IDF features" class="link">Document similarity with the 20 Newsgroups dataset and TF-IDF features</a></li><li>text classifier, training on / <a href="#ch09lvl1sec62" title="Training a text classifier on the 20 Newsgroups dataset using TF-IDF" class="link">Training a text classifier on the 20 Newsgroups dataset using TF-IDF</a></li><li>Word2Vec models, used on / <a href="#ch09lvl1sec64" title="Word2Vec on the 20 Newsgroups dataset" class="link">Word2Vec on the 20 Newsgroups dataset</a></li></ul></li>
        <li>natural language processing (NLP)<ul><li>about / <a href="#ch09lvl1sec61" title="Extracting the right features from your data" class="link">Extracting the right features from your data</a></li></ul></li>
        <li>naÃ¯ve Bayes model / <a href="#ch05lvl1sec33" title="The naÃ¯ve Bayes model" class="link">The naÃ¯ve Bayes model</a>, <a href="#ch05lvl1sec38" title="The naÃ¯ve Bayes model" class="link">The naÃ¯ve Bayes model</a></li>
        <li>nominal variables<ul><li>about / <a href="#ch03lvl1sec25" title="Categorical features" class="link">Categorical features</a></li></ul></li>
        <li>nonword characters / <a href="#ch09lvl1sec61" title="Improving our tokenization" class="link">Improving our tokenization</a></li>
        <li>normalization<ul><li>normalize a feature / <a href="#ch03lvl1sec25" title="Normalizing features" class="link">Normalizing features</a></li><li>normalize a feature vector / <a href="#ch03lvl1sec25" title="Normalizing features" class="link">Normalizing features</a></li></ul></li>
        <li>normalization, LFW dataset / <a href="#ch08lvl1sec55" title="Normalization" class="link">Normalization</a></li>
        <li>normalization, MovieLens dataset / <a href="#ch07lvl1sec48" title="Normalization" class="link">Normalization</a></li>
        <li>normalizing features<ul><li>about / <a href="#ch03lvl1sec25" title="Normalizing features" class="link">Normalizing features</a></li><li>MLlib, used for / <a href="#ch03lvl1sec25" title="Using MLlib for feature normalization" class="link">Using MLlib for feature normalization</a></li></ul></li>
        <li>numerical features / <a href="#ch03lvl1sec25" title="Numerical features" class="link">Numerical features</a></li>
      </ul>
      <h2>O</h2>
      <ul>
        <li>1-of-k encoding<ul><li>about / <a href="#ch03lvl1sec25" title="Categorical features" class="link">Categorical features</a></li></ul></li>
        <li>online learning / <a href="#ch02lvl1sec19" title="Batch versus real time" class="link">Batch versus real time</a><ul><li>about / <a href="#ch10lvl1sec66" title="Online learning" class="link">Online learning</a></li></ul></li>
        <li>online learning, with Spark Streaming<ul><li>about / <a href="#ch10lvl1sec69" title="Online learning with Spark Streaming" class="link">Online learning with Spark Streaming</a></li><li>streaming regression model / <a href="#ch10lvl1sec69" title="Streaming regression" class="link">Streaming regression</a></li><li>streaming regression program / <a href="#ch10lvl1sec69" title="A simple streaming regression program" class="link">A simple streaming regression program</a></li><li>K-means, streaming / <a href="#ch10lvl1sec69" title="Streaming K-means" class="link">Streaming K-means</a></li></ul></li>
        <li>online machine learning<ul><li>URL / <a href="#ch10lvl1sec66" title="Online learning" class="link">Online learning</a></li></ul></li>
        <li>online model evaluation<ul><li>about / <a href="#ch10lvl1sec70" title="Online model evaluation" class="link">Online model evaluation</a></li><li>model performance, comparing with Spark Streaming / <a href="#ch10lvl1sec70" title="Comparing model performance with Spark Streaming" class="link">Comparing model performance with Spark Streaming</a></li></ul></li>
        <li>optimization<ul><li>about / <a href="#ch05lvl1sec33" title="Linear models" class="link">Linear models</a></li></ul></li>
        <li>options, data transformation<ul><li>about / <a href="#ch03lvl1sec24" title="Processing and transforming your data" class="link">Processing and transforming your data</a></li></ul></li>
        <li>ordinal variables<ul><li>about / <a href="#ch03lvl1sec25" title="Categorical features" class="link">Categorical features</a></li></ul></li>
        <li>Oryx<ul><li>URL / <a href="#ch04lvl1sec27" title="Explicit matrix factorization" class="link">Explicit matrix factorization</a></li></ul></li>
        <li>over-fitting and under-fitting<ul><li>URL / <a href="#ch05lvl1sec38" title="Regularization" class="link">Regularization</a></li></ul></li>
      </ul>
      <h2>P</h2>
      <ul>
        <li>packages<ul><li>used, for feature extraction / <a href="#ch03lvl1sec25" title="Using packages for feature extraction" class="link">Using packages for feature extraction</a></li></ul></li>
        <li>parameters<ul><li>tuning / <a href="#ch05lvl1sec38" title="Improving model performance and tuning parameters" class="link">Improving model performance and tuning parameters</a>, <a href="#ch06lvl1sec44" title="Improving model performance and tuning parameters" class="link">Improving model performance and tuning parameters</a></li><li>tuning, for clustering models / <a href="#ch07lvl1sec52" title="Tuning parameters for clustering models" class="link">Tuning parameters for clustering models</a></li></ul></li>
        <li>parameter settings impact, for decision tree<ul><li>about / <a href="#ch06lvl1sec44" title="The impact of parameter settings for the decision tree" class="link">The impact of parameter settings for the decision tree</a></li><li>tree depth / <a href="#ch06lvl1sec44" title="Tree depth" class="link">Tree depth</a></li><li>maximum bins / <a href="#ch06lvl1sec44" title="Maximum bins" class="link">Maximum bins</a></li></ul></li>
        <li>parameter settings impact, for linear models<ul><li>about / <a href="#ch06lvl1sec44" title="The impact of parameter settings for linear models" class="link">The impact of parameter settings for linear models</a></li><li>iterations / <a href="#ch06lvl1sec44" title="Iterations" class="link">Iterations</a></li><li>step size / <a href="#ch06lvl1sec44" title="Step size" class="link">Step size</a></li><li>L2 regularization / <a href="#ch06lvl1sec44" title="L2 regularization" class="link">L2 regularization</a></li><li>L1 regularization / <a href="#ch06lvl1sec44" title="L1 regularization" class="link">L1 regularization</a></li><li>intercept, using / <a href="#ch06lvl1sec44" title="Intercept" class="link">Intercept</a></li></ul></li>
        <li>PCA / <a href="#ch08lvl1sec54" title="Principal Components Analysis" class="link">Principal Components Analysis</a><ul><li>running, on LFW dataset / <a href="#ch08lvl1sec56" title="Running PCA on the LFW dataset" class="link">Running PCA on the LFW dataset</a></li><li>and SVD, relationship between / <a href="#ch08lvl1sec57" title="The relationship between PCA and SVD" class="link">The relationship between PCA and SVD</a></li></ul></li>
        <li>performance, classification models<ul><li>evaluating / <a href="#ch05lvl1sec37" title="Evaluating the performance of classification models" class="link">Evaluating the performance of classification models</a></li><li>accuracy, calculating / <a href="#ch05lvl1sec37" title="Accuracy and prediction error" class="link">Accuracy and prediction error</a></li><li>prediction error / <a href="#ch05lvl1sec37" title="Accuracy and prediction error" class="link">Accuracy and prediction error</a></li><li>precision / <a href="#ch05lvl1sec37" title="Precision and recall" class="link">Precision and recall</a></li><li>recall / <a href="#ch05lvl1sec37" title="Precision and recall" class="link">Precision and recall</a></li><li>ROC curve / <a href="#ch05lvl1sec37" title="ROC curve and AUC" class="link">ROC curve and AUC</a></li><li>AUC / <a href="#ch05lvl1sec37" title="ROC curve and AUC" class="link">ROC curve and AUC</a></li></ul></li>
        <li>performance, clustering models<ul><li>evaluating / <a href="#ch07lvl1sec51" title="Evaluating the performance of clustering models" class="link">Evaluating the performance of clustering models</a></li><li>internal evaluation metrics / <a href="#ch07lvl1sec51" title="Internal evaluation metrics" class="link">Internal evaluation metrics</a></li><li>external evaluation metrics / <a href="#ch07lvl1sec51" title="External evaluation metrics" class="link">External evaluation metrics</a></li><li>performance metrics, computing on MovieLens dataset / <a href="#ch07lvl1sec51" title="Computing performance metrics on the MovieLens dataset" class="link">Computing performance metrics on the MovieLens dataset</a></li></ul></li>
        <li>performance, recommendation models<ul><li>evaluating / <a href="#ch04lvl1sec31" title="Evaluating the performance of recommendation models" class="link">Evaluating the performance of recommendation models</a></li><li>Mean Squared Error (MSE) / <a href="#ch04lvl1sec31" title="Mean Squared Error" class="link">Mean Squared Error</a></li><li>Mean average precision at K (MAPK) / <a href="#ch04lvl1sec31" title="Mean average precision at K" class="link">Mean average precision at K</a></li><li>built-in evaluation functions, using / <a href="#ch04lvl1sec31" title="Using MLlib's built-in evaluation functions" class="link">Using MLlib's built-in evaluation functions</a></li></ul></li>
        <li>performance, regression models<ul><li>evaluating / <a href="#ch06lvl1sec43" title="Evaluating the performance of regression models" class="link">Evaluating the performance of regression models</a></li><li>MSE / <a href="#ch06lvl1sec43" title="Mean Squared Error and Root Mean Squared Error" class="link">Mean Squared Error and Root Mean Squared Error</a></li><li>RMSE / <a href="#ch06lvl1sec43" title="Mean Squared Error and Root Mean Squared Error" class="link">Mean Squared Error and Root Mean Squared Error</a></li><li>MAE / <a href="#ch06lvl1sec43" title="Mean Absolute Error" class="link">Mean Absolute Error</a></li><li>Root Mean Squared Log Error / <a href="#ch06lvl1sec43" title="Root Mean Squared Log Error" class="link">Root Mean Squared Log Error</a></li><li>R-squared coefficient / <a href="#ch06lvl1sec43" title="The R-squared coefficient" class="link">The R-squared coefficient</a></li><li>performance metrics, computing on bike sharing dataset / <a href="#ch06lvl1sec43" title="Computing performance metrics on the bike sharing dataset" class="link">Computing performance metrics on the bike sharing dataset</a></li></ul></li>
        <li>performance metrics<ul><li>computing, on bike sharing dataset / <a href="#ch06lvl1sec43" title="Computing performance metrics on the bike sharing dataset" class="link">Computing performance metrics on the bike sharing dataset</a></li><li>linear model / <a href="#ch06lvl1sec43" title="Linear model" class="link">Linear model</a></li><li>decision tree / <a href="#ch06lvl1sec43" title="Decision tree" class="link">Decision tree</a></li><li>computing, on MovieLens dataset / <a href="#ch07lvl1sec51" title="Computing performance metrics on the MovieLens dataset" class="link">Computing performance metrics on the MovieLens dataset</a></li></ul></li>
        <li>personalization / <a href="#ch02lvl1sec17" title="Personalization" class="link">Personalization</a></li>
        <li>precision, classification models / <a href="#ch05lvl1sec37" title="Precision and recall" class="link">Precision and recall</a></li>
        <li>precision-recall (PR) curve / <a href="#ch05lvl1sec37" title="Precision and recall" class="link">Precision and recall</a></li>
        <li>Prediction.io<ul><li>URL / <a href="#ch04lvl1sec27" title="Explicit matrix factorization" class="link">Explicit matrix factorization</a></li></ul></li>
        <li>prediction error, classification models / <a href="#ch05lvl1sec37" title="Accuracy and prediction error" class="link">Accuracy and prediction error</a></li>
        <li>predictions<ul><li>generating, for Kaggle/StumbleUpon evergreen / <a href="#ch05lvl1sec36" title="Generating predictions for the Kaggle/StumbleUpon evergreen classification dataset" class="link">Generating predictions for the Kaggle/StumbleUpon evergreen classification dataset</a></li><li>generating, for Kaggle/StumbleUpon evergreen classification dataset / <a href="#ch05lvl1sec36" title="Generating predictions for the Kaggle/StumbleUpon evergreen classification dataset" class="link">Generating predictions for the Kaggle/StumbleUpon evergreen classification dataset</a></li><li>making, clustering model used / <a href="#ch07lvl1sec50" title="Making predictions using a clustering model" class="link">Making predictions using a clustering model</a></li></ul></li>
        <li>predictive modeling<ul><li>about / <a href="#ch02lvl1sec17" title="Predictive modeling and analytics" class="link">Predictive modeling and analytics</a></li></ul></li>
        <li>producer application / <a href="#ch10lvl1sec68" title="The producer application" class="link">The producer application</a></li>
        <li>pylab<ul><li>about / <a href="#ch03lvl1sec23" title="Exploring and visualizing your data" class="link">Exploring and visualizing your data</a></li></ul></li>
        <li>Python<ul><li>Spark program, writing in / <a href="#ch01lvl1sec13" title="The first step to a Spark program in Python" class="link">The first step to a Spark program in Python</a></li></ul></li>
      </ul>
      <h2>R</h2>
      <ul>
        <li>R-squared coefficient / <a href="#ch06lvl1sec43" title="The R-squared coefficient" class="link">The R-squared coefficient</a></li>
        <li>rating dataset<ul><li>exploring / <a href="#ch03lvl1sec23" title="Exploring the rating dataset" class="link">Exploring the rating dataset</a></li></ul></li>
        <li>RDD caching<ul><li>URL / <a href="#ch01lvl1sec10" title="Caching RDDs" class="link">Caching RDDs</a></li></ul></li>
        <li>RDDs<ul><li>about / <a href="#ch01lvl1sec10" title="Resilient Distributed Datasets" class="link">Resilient Distributed Datasets</a></li><li>creating / <a href="#ch01lvl1sec10" title="Creating RDDs" class="link">Creating RDDs</a></li><li>Spark operations / <a href="#ch01lvl1sec10" title="Spark operations" class="link">Spark operations</a></li><li>caching / <a href="#ch01lvl1sec10" title="Caching RDDs" class="link">Caching RDDs</a></li></ul></li>
        <li>Readme.txt file<ul><li>about / <a href="#ch06lvl1sec41" title="Extracting features from the bike sharing dataset" class="link">Extracting features from the bike sharing dataset</a></li><li>variables / <a href="#ch06lvl1sec41" title="Extracting features from the bike sharing dataset" class="link">Extracting features from the bike sharing dataset</a></li></ul></li>
        <li>recall, classification models / <a href="#ch05lvl1sec37" title="Precision and recall" class="link">Precision and recall</a></li>
        <li>receiver operating characteristic (ROC) / <a href="#ch05lvl1sec37" title="Evaluating the performance of classification models" class="link">Evaluating the performance of classification models</a></li>
        <li>recommendation model<ul><li>training / <a href="#ch04lvl1sec29" title="Training the recommendation model" class="link">Training the recommendation model</a>, <a href="#ch07lvl1sec48" title="Training the recommendation model" class="link">Training the recommendation model</a></li><li>model, training on MovieLens 100k dataset / <a href="#ch04lvl1sec29" title="Training a model on the MovieLens 100k dataset" class="link">Training a model on the MovieLens 100k dataset</a></li><li>using / <a href="#ch04lvl1sec30" title="Using the recommendation model" class="link">Using the recommendation model</a></li><li>user recommendations / <a href="#ch04lvl1sec30" title="User recommendations" class="link">User recommendations</a></li><li>item recommendations / <a href="#ch04lvl1sec30" title="Item recommendations" class="link">Item recommendations</a></li></ul></li>
        <li>recommendation models<ul><li>about / <a href="#ch04lvl1sec27" title="Types of recommendation models" class="link">Types of recommendation models</a></li><li>types / <a href="#ch04lvl1sec27" title="Types of recommendation models" class="link">Types of recommendation models</a></li><li>content-based filtering / <a href="#ch04lvl1sec27" title="Content-based filtering" class="link">Content-based filtering</a></li><li>collaborative filtering / <a href="#ch04lvl1sec27" title="Collaborative filtering" class="link">Collaborative filtering</a></li></ul></li>
        <li>recommendations / <a href="#ch02lvl1sec17" title="Personalization" class="link">Personalization</a><ul><li>inspecting / <a href="#ch04lvl1sec30" title="Inspecting the recommendations" class="link">Inspecting the recommendations</a></li></ul></li>
        <li>red, blue, and green (RGB) / <a href="#ch08lvl1sec55" title="Extracting facial images as vectors" class="link">Extracting facial images as vectors</a></li>
        <li>regression model<ul><li>about / <a href="#ch02lvl1sec17" title="Predictive modeling and analytics" class="link">Predictive modeling and analytics</a></li></ul></li>
        <li>regression models<ul><li>types / <a href="#ch06lvl1sec40" title="Types of regression models" class="link">Types of regression models</a></li><li>Least Squares Regression / <a href="#ch06lvl1sec40" title="Least squares regression" class="link">Least squares regression</a></li><li>decision trees, for regression / <a href="#ch06lvl1sec40" title="Decision trees for regression" class="link">Decision trees for regression</a></li><li>training / <a href="#ch06lvl1sec42" title="Training and using regression models" class="link">Training and using regression models</a></li><li>using / <a href="#ch06lvl1sec42" title="Training and using regression models" class="link">Training and using regression models</a></li><li>training, on bike sharing dataset / <a href="#ch06lvl1sec42" title="Training a regression model on the bike sharing dataset" class="link">Training a regression model on the bike sharing dataset</a></li></ul></li>
        <li>regularization forms<ul><li>SimpleUpdater / <a href="#ch05lvl1sec38" title="Regularization" class="link">Regularization</a></li><li>SquaredL2Updater / <a href="#ch05lvl1sec38" title="Regularization" class="link">Regularization</a></li><li>L1Updater / <a href="#ch05lvl1sec38" title="Regularization" class="link">Regularization</a></li></ul></li>
        <li>REPL (Read-Eval-Print-Loop)<ul><li>about / <a href="#ch01lvl1sec10" title="The Spark shell" class="link">The Spark shell</a></li></ul></li>
        <li>reshaping / <a href="#ch08lvl1sec55" title="Extracting facial images as vectors" class="link">Extracting facial images as vectors</a></li>
        <li>RMSE<ul><li>about / <a href="#ch04lvl1sec31" title="Mean Squared Error" class="link">Mean Squared Error</a></li></ul> / <a href="#ch04lvl1sec31" title="RMSE and MSE" class="link">RMSE and MSE</a>, <a href="#ch06lvl1sec43" title="Mean Squared Error and Root Mean Squared Error" class="link">Mean Squared Error and Root Mean Squared Error</a></li>
        <li>ROC curve<ul><li>URL / <a href="#ch05lvl1sec37" title="ROC curve and AUC" class="link">ROC curve and AUC</a></li></ul></li>
        <li>ROC curve, classification models / <a href="#ch05lvl1sec37" title="ROC curve and AUC" class="link">ROC curve and AUC</a></li>
        <li>root mean-squared error (RMSE) / <a href="#ch10lvl1sec70" title="Comparing model performance with Spark Streaming" class="link">Comparing model performance with Spark Streaming</a></li>
        <li>Root Mean Squared Log Error / <a href="#ch06lvl1sec43" title="Root Mean Squared Log Error" class="link">Root Mean Squared Log Error</a></li>
      </ul>
      <h2>S</h2>
      <ul>
        <li>Scala<ul><li>Spark program, writing in / <a href="#ch01lvl1sec11" title="The first step to a Spark program in Scala" class="link">The first step to a Spark program in Scala</a></li></ul></li>
        <li>Scala Build Tool (sbt) / <a href="#ch01lvl1sec11" title="The first step to a Spark program in Scala" class="link">The first step to a Spark program in Scala</a></li>
        <li>similar items<ul><li>inspecting / <a href="#ch04lvl1sec30" title="Inspecting the similar items" class="link">Inspecting the similar items</a></li></ul></li>
        <li>singular values<ul><li>about / <a href="#ch08lvl1sec54" title="Singular Value Decomposition" class="link">Singular Value Decomposition</a></li></ul></li>
        <li>skip-gram model<ul><li>about / <a href="#ch09lvl1sec64" title="Word2Vec models" class="link">Word2Vec models</a></li></ul></li>
        <li>Spark<ul><li>installing / <a href="#ch01lvl1sec08" title="Installing and setting up Spark locally" class="link">Installing and setting up Spark locally</a></li><li>setting up / <a href="#ch01lvl1sec08" title="Installing and setting up Spark locally" class="link">Installing and setting up Spark locally</a></li><li>running, on Amazon EC2 / <a href="#ch01lvl1sec14" title="Getting Spark running on Amazon EC2" class="link">Getting Spark running on Amazon EC2</a></li></ul></li>
        <li>Spark clusters<ul><li>about / <a href="#ch01lvl1sec09" title="Spark clusters" class="link">Spark clusters</a></li><li>URL / <a href="#ch01lvl1sec09" title="Spark clusters" class="link">Spark clusters</a></li></ul></li>
        <li>SparkConf / <a href="#ch01lvl1sec10" title="SparkContext and SparkConf" class="link">SparkContext and SparkConf</a></li>
        <li>SparkContext / <a href="#ch01lvl1sec10" title="SparkContext and SparkConf" class="link">SparkContext and SparkConf</a></li>
        <li>Spark documentation<ul><li>URL / <a href="#ch05lvl1sec33" title="Linear models" class="link">Linear models</a>, <a href="#ch06lvl1sec40" title="Decision trees for regression" class="link">Decision trees for regression</a>, <a href="#ch10lvl1sec67" title="General transformations" class="link">General transformations</a></li></ul></li>
        <li>Spark documentation, for EC2<ul><li>URL / <a href="#ch01lvl1sec14" title="Getting Spark running on Amazon EC2" class="link">Getting Spark running on Amazon EC2</a></li></ul></li>
        <li>Spark operations / <a href="#ch01lvl1sec10" title="Spark operations" class="link">Spark operations</a></li>
        <li>Spark program<ul><li>in Scala / <a href="#ch01lvl1sec11" title="The first step to a Spark program in Scala" class="link">The first step to a Spark program in Scala</a></li><li>in Java / <a href="#ch01lvl1sec12" title="The first step to a Spark program in Java" class="link">The first step to a Spark program in Java</a></li><li>in Python / <a href="#ch01lvl1sec13" title="The first step to a Spark program in Python" class="link">The first step to a Spark program in Python</a></li></ul></li>
        <li>Spark programming guide<ul><li>URL / <a href="#ch01lvl1sec13" title="The first step to a Spark program in Python" class="link">The first step to a Spark program in Python</a></li></ul></li>
        <li>Spark Programming Guide<ul><li>URL / <a href="#ch01lvl1sec10" title="Broadcast variables and accumulators" class="link">Broadcast variables and accumulators</a></li></ul></li>
        <li>Spark programming model<ul><li>about / <a href="#ch01lvl1sec10" title="The Spark programming model" class="link">The Spark programming model</a></li><li>SparkContext / <a href="#ch01lvl1sec10" title="SparkContext and SparkConf" class="link">SparkContext and SparkConf</a></li><li>SparkConf / <a href="#ch01lvl1sec10" title="SparkContext and SparkConf" class="link">SparkContext and SparkConf</a></li><li>Spark shell / <a href="#ch01lvl1sec10" title="The Spark shell" class="link">The Spark shell</a></li><li>RDDs / <a href="#ch01lvl1sec10" title="Resilient Distributed Datasets" class="link">Resilient Distributed Datasets</a></li><li>broadcast variable / <a href="#ch01lvl1sec10" title="Broadcast variables and accumulators" class="link">Broadcast variables and accumulators</a></li><li>accumulators / <a href="#ch01lvl1sec10" title="Broadcast variables and accumulators" class="link">Broadcast variables and accumulators</a></li></ul></li>
        <li>Spark project documentation website<ul><li>URL / <a href="#ch01lvl1sec08" title="Installing and setting up Spark locally" class="link">Installing and setting up Spark locally</a></li></ul></li>
        <li>Spark project website<ul><li>URL / <a href="#ch01lvl1sec08" title="Installing and setting up Spark locally" class="link">Installing and setting up Spark locally</a></li></ul></li>
        <li>Spark Quick Start<ul><li>URL / <a href="#ch01lvl1sec10" title="The Spark programming model" class="link">The Spark programming model</a></li></ul></li>
        <li>Spark shell / <a href="#ch01lvl1sec10" title="The Spark shell" class="link">The Spark shell</a></li>
        <li>Spark Streaming<ul><li>about / <a href="#ch02lvl1sec19" title="Batch versus real time" class="link">Batch versus real time</a>, <a href="#ch10lvl1sec67" title="An introduction to Spark Streaming" class="link">An introduction to Spark Streaming</a></li><li>input sources / <a href="#ch10lvl1sec67" title="Input sources" class="link">Input sources</a></li><li>transformations / <a href="#ch10lvl1sec67" title="Transformations" class="link">Transformations</a></li><li>actions / <a href="#ch10lvl1sec67" title="Actions" class="link">Actions</a></li><li>window operators / <a href="#ch10lvl1sec67" title="Window operators" class="link">Window operators</a></li><li>model performance, comparing with / <a href="#ch10lvl1sec70" title="Comparing model performance with Spark Streaming" class="link">Comparing model performance with Spark Streaming</a></li></ul></li>
        <li>Spark Streaming application<ul><li>creating / <a href="#ch10lvl1sec68" title="Creating a Spark Streaming application" class="link">Creating a Spark Streaming application</a></li><li>producer application / <a href="#ch10lvl1sec68" title="The producer application" class="link">The producer application</a></li><li>basic streaming application, creating / <a href="#ch10lvl1sec68" title="Creating a basic streaming application" class="link">Creating a basic streaming application</a></li><li>analytics, streaming / <a href="#ch10lvl1sec68" title="Streaming analytics" class="link">Streaming analytics</a></li><li>stateful streaming / <a href="#ch10lvl1sec68" title="Stateful streaming" class="link">Stateful streaming</a></li></ul></li>
        <li>stateful streaming / <a href="#ch10lvl1sec68" title="Stateful streaming" class="link">Stateful streaming</a></li>
        <li>stemming<ul><li>about / <a href="#ch09lvl1sec61" title="A note about stemming" class="link">A note about stemming</a></li><li>URL / <a href="#ch09lvl1sec61" title="A note about stemming" class="link">A note about stemming</a></li></ul></li>
        <li>stochastic gradient descent<ul><li>about / <a href="#ch10lvl1sec66" title="Online learning" class="link">Online learning</a></li></ul></li>
        <li>Stochastic Gradient Descent (SGD) / <a href="#ch05lvl1sec38" title="Linear models" class="link">Linear models</a></li>
        <li>stop words<ul><li>removing / <a href="#ch09lvl1sec61" title="Removing stop words" class="link">Removing stop words</a></li></ul></li>
        <li>streaming data producer<ul><li>creating / <a href="#ch10lvl1sec69" title="Creating a streaming data producer" class="link">Creating a streaming data producer</a></li></ul></li>
        <li>streaming regression model / <a href="#ch10lvl1sec69" title="Streaming regression" class="link">Streaming regression</a><ul><li>trainOn method / <a href="#ch10lvl1sec69" title="Streaming regression" class="link">Streaming regression</a></li><li>predictOn method / <a href="#ch10lvl1sec69" title="Streaming regression" class="link">Streaming regression</a></li><li>creating / <a href="#ch10lvl1sec69" title="Creating a streaming regression model" class="link">Creating a streaming regression model</a></li></ul></li>
        <li>streaming regression program<ul><li>about / <a href="#ch10lvl1sec69" title="A simple streaming regression program" class="link">A simple streaming regression program</a></li><li>streaming data producer, creating / <a href="#ch10lvl1sec69" title="Creating a streaming data producer" class="link">Creating a streaming data producer</a></li><li>streaming regression model, creating / <a href="#ch10lvl1sec69" title="Creating a streaming regression model" class="link">Creating a streaming regression model</a></li></ul></li>
        <li>Stream processing<ul><li>about / <a href="#ch10lvl1sec67" title="Stream processing" class="link">Stream processing</a></li><li>Spark Streaming / <a href="#ch10lvl1sec67" title="An introduction to Spark Streaming" class="link">An introduction to Spark Streaming</a></li><li>caching, with Spark Streaming / <a href="#ch10lvl1sec67" title="Caching and fault tolerance with Spark Streaming" class="link">Caching and fault tolerance with Spark Streaming</a></li><li>fault tolerance, with Spark Streaming / <a href="#ch10lvl1sec67" title="Caching and fault tolerance with Spark Streaming" class="link">Caching and fault tolerance with Spark Streaming</a></li></ul></li>
        <li>supervised learning<ul><li>about / <a href="#ch02lvl1sec18" title="Types of machine learning models" class="link">Types of machine learning models</a></li></ul></li>
        <li>Support Vector Machine (SVM)<ul><li>about / <a href="#ch05lvl1sec33" title="Linear models" class="link">Linear models</a></li></ul></li>
        <li>SVD<ul><li>about / <a href="#ch08lvl1sec54" title="Singular Value Decomposition" class="link">Singular Value Decomposition</a></li><li>and PCA, relationship between / <a href="#ch08lvl1sec57" title="The relationship between PCA and SVD" class="link">The relationship between PCA and SVD</a></li></ul></li>
      </ul>
      <h2>T</h2>
      <ul>
        <li>targeted marketing<ul><li>about / <a href="#ch02lvl1sec17" title="Targeted marketing and customer segmentation" class="link">Targeted marketing and customer segmentation</a></li></ul></li>
        <li>target variable<ul><li>transforming / <a href="#ch06lvl1sec44" title="Transforming the target variable" class="link">Transforming the target variable</a></li><li>training on log-transformed targets, impact / <a href="#ch06lvl1sec44" title="Impact of training on log-transformed targets" class="link">Impact of training on log-transformed targets</a></li></ul></li>
        <li>term frequency<ul><li>about / <a href="#ch09lvl1sec61" title="Term weighting schemes" class="link">Term weighting schemes</a></li></ul></li>
        <li>term frequency-inverse document frequency (TF-IDF)<ul><li>about / <a href="#ch09lvl1sec61" title="Term weighting schemes" class="link">Term weighting schemes</a></li></ul></li>
        <li>terms based on frequency<ul><li>excluding / <a href="#ch09lvl1sec61" title="Excluding terms based on frequency" class="link">Excluding terms based on frequency</a></li></ul></li>
        <li>term weighting schemes / <a href="#ch09lvl1sec61" title="Term weighting schemes" class="link">Term weighting schemes</a></li>
        <li>testing loop / <a href="#ch02lvl1sec19" title="Model training and testing loop" class="link">Model training and testing loop</a></li>
        <li>testing set<ul><li>creating, to evaluate parameters / <a href="#ch06lvl1sec44" title="Creating training and testing sets to evaluate parameters" class="link">Creating training and testing sets to evaluate parameters</a></li></ul></li>
        <li>text classifier<ul><li>training, on 20 Newsgroups dataset / <a href="#ch09lvl1sec62" title="Training a text classifier on the 20 Newsgroups dataset using TF-IDF" class="link">Training a text classifier on the 20 Newsgroups dataset using TF-IDF</a></li></ul></li>
        <li>text data<ul><li>about / <a href="#ch09lvl1sec60" title="What's so special about text data?" class="link">What's so special about text data?</a></li></ul></li>
        <li>text features<ul><li>about / <a href="#ch03lvl1sec25" title="Text features" class="link">Text features</a></li><li>extraction / <a href="#ch03lvl1sec25" title="Simple text feature extraction" class="link">Simple text feature extraction</a></li></ul></li>
        <li>text processing impact<ul><li>evaluating / <a href="#ch09lvl1sec63" title="Evaluating the impact of text processing" class="link">Evaluating the impact of text processing</a></li><li>raw features, comparing / <a href="#ch09lvl1sec63" title="Comparing raw features with processed TF-IDF features on the 20 Newsgroups dataset" class="link">Comparing raw features with processed TF-IDF features on the 20 Newsgroups dataset</a></li></ul></li>
        <li>TF-IDF<ul><li>used, for training text classifier / <a href="#ch09lvl1sec62" title="Training a text classifier on the 20 Newsgroups dataset using TF-IDF" class="link">Training a text classifier on the 20 Newsgroups dataset using TF-IDF</a></li></ul></li>
        <li>TF-IDF features<ul><li>extracting, from 20 Newsgroups dataset / <a href="#ch09lvl1sec61" title="Extracting the TF-IDF features from the 20 Newsgroups dataset" class="link">Extracting the TF-IDF features from the 20 Newsgroups dataset</a></li><li>document similarity, used with / <a href="#ch09lvl1sec62" title="Document similarity with the 20 Newsgroups dataset and TF-IDF features" class="link">Document similarity with the 20 Newsgroups dataset and TF-IDF features</a></li></ul></li>
        <li>TF-IDF model<ul><li>training / <a href="#ch09lvl1sec61" title="Training a TF-IDF model" class="link">Training a TF-IDF model</a></li><li>using / <a href="#ch09lvl1sec62" title="Using a TF-IDF model" class="link">Using a TF-IDF model</a></li><li>document similarity, with 20 Newsgroups dataset / <a href="#ch09lvl1sec62" title="Document similarity with the 20 Newsgroups dataset and TF-IDF features" class="link">Document similarity with the 20 Newsgroups dataset and TF-IDF features</a></li><li>document similarity, with TF-IDF features / <a href="#ch09lvl1sec62" title="Document similarity with the 20 Newsgroups dataset and TF-IDF features" class="link">Document similarity with the 20 Newsgroups dataset and TF-IDF features</a></li><li>text classifier, training on 20 Newsgroups dataset / <a href="#ch09lvl1sec62" title="Training a text classifier on the 20 Newsgroups dataset using TF-IDF" class="link">Training a text classifier on the 20 Newsgroups dataset using TF-IDF</a></li></ul></li>
        <li>TF-IDF weightings<ul><li>analyzing / <a href="#ch09lvl1sec61" title="Analyzing the TF-IDF weightings" class="link">Analyzing the TF-IDF weightings</a></li></ul></li>
        <li>timestamps<ul><li>transforming, into categorical features / <a href="#ch03lvl1sec25" title="Transforming timestamps into categorical features" class="link">Transforming timestamps into categorical features</a></li></ul></li>
        <li>tokenization<ul><li>applying / <a href="#ch09lvl1sec61" title="Applying basic tokenization" class="link">Applying basic tokenization</a></li><li>improving / <a href="#ch09lvl1sec61" title="Improving our tokenization" class="link">Improving our tokenization</a></li></ul></li>
        <li>training<ul><li>about / <a href="#ch05lvl1sec33" title="Linear models" class="link">Linear models</a></li></ul></li>
        <li>training set<ul><li>creating, to evaluate parameters / <a href="#ch06lvl1sec44" title="Creating training and testing sets to evaluate parameters" class="link">Creating training and testing sets to evaluate parameters</a></li></ul></li>
        <li>transformations<ul><li>about / <a href="#ch10lvl1sec67" title="Transformations" class="link">Transformations</a></li><li>state, tracking / <a href="#ch10lvl1sec67" title="Keeping track of state" class="link">Keeping track of state</a></li><li>general transformations / <a href="#ch10lvl1sec67" title="General transformations" class="link">General transformations</a></li></ul></li>
        <li>true positive rate (TPR) / <a href="#ch05lvl1sec37" title="ROC curve and AUC" class="link">ROC curve and AUC</a></li>
      </ul>
      <h2>U</h2>
      <ul>
        <li>UCI Machine Learning Repository<ul><li>about / <a href="#ch03lvl1sec22" title="Accessing publicly available datasets" class="link">Accessing publicly available datasets</a></li><li>URL / <a href="#ch03lvl1sec22" title="Accessing publicly available datasets" class="link">Accessing publicly available datasets</a></li></ul></li>
        <li>unsupervised learning<ul><li>about / <a href="#ch02lvl1sec18" title="Types of machine learning models" class="link">Types of machine learning models</a></li></ul></li>
        <li>user dataset<ul><li>exploring / <a href="#ch03lvl1sec23" title="Exploring the user dataset" class="link">Exploring the user dataset</a></li></ul></li>
        <li>user recommendations<ul><li>about / <a href="#ch04lvl1sec30" title="User recommendations" class="link">User recommendations</a></li><li>movie recommendations, generating / <a href="#ch04lvl1sec30" title="Generating movie recommendations from the MovieLens 100k dataset" class="link">Generating movie recommendations from the MovieLens 100k dataset</a></li></ul></li>
      </ul>
      <h2>V</h2>
      <ul>
        <li>variance<ul><li>about / <a href="#ch06lvl1sec40" title="Decision trees for regression" class="link">Decision trees for regression</a></li></ul></li>
        <li>variants, K-means clustering / <a href="#ch07lvl1sec47" title="Variants" class="link">Variants</a></li>
        <li>vector<ul><li>about / <a href="#ch03lvl1sec25" title="Extracting useful features from your data" class="link">Extracting useful features from your data</a></li></ul></li>
        <li>vector space model<ul><li>about / <a href="#ch09lvl1sec61" title="Term weighting schemes" class="link">Term weighting schemes</a></li></ul></li>
      </ul>
      <h2>W</h2>
      <ul>
        <li>whitespace tokenization<ul><li>URL / <a href="#ch09lvl1sec61" title="Applying basic tokenization" class="link">Applying basic tokenization</a></li></ul></li>
        <li>window<ul><li>about / <a href="#ch10lvl1sec67" title="Window operators" class="link">Window operators</a></li></ul></li>
        <li>windowing<ul><li>about / <a href="#ch10lvl1sec67" title="Window operators" class="link">Window operators</a></li></ul></li>
        <li>window operators / <a href="#ch10lvl1sec67" title="Window operators" class="link">Window operators</a></li>
        <li>within cluster sum of squared errors (WCSS) / <a href="#ch07lvl1sec47" title="K-means clustering" class="link">K-means clustering</a></li>
        <li>Word2Vec models<ul><li>about / <a href="#ch09lvl1sec64" title="Word2Vec models" class="link">Word2Vec models</a></li><li>on 20 Newsgroups dataset / <a href="#ch09lvl1sec64" title="Word2Vec on the 20 Newsgroups dataset" class="link">Word2Vec on the 20 Newsgroups dataset</a></li></ul></li>
        <li>word stem / <a href="#ch09lvl1sec61" title="A note about stemming" class="link">A note about stemming</a></li>
      </ul>
    </div>
  </div></div></div>
</div></div></div></body></html>
